{
    "754bd7b8a37f7afd3312fc9e9d4d4c7d726387c0": [
        [
            "GridDhtAtomicCache::lockEntries(GridNearAtomicAbstractUpdateRequest,AffinityTopologyVersion)",
            "2915  \n2916  \n2917  \n2918  \n2919  \n2920  \n2921  \n2922  \n2923  \n2924  \n2925  \n2926  \n2927 -\n2928 -\n2929  \n2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944  \n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  \n2957  \n2958  \n2959  \n2960  \n2961  \n2962  \n2963  \n2964  \n2965  \n2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  ",
            "    /**\n     * Acquires java-level locks on cache entries. Returns collection of locked entries.\n     *\n     * @param req Request with keys to lock.\n     * @param topVer Topology version to lock on.\n     * @return Collection of locked entries.\n     * @throws GridDhtInvalidPartitionException If entry does not belong to local node. If exception is thrown,\n     *      locks are released.\n     */\n    @SuppressWarnings(\"ForLoopReplaceableByForEach\")\n    private List<GridDhtCacheEntry> lockEntries(GridNearAtomicAbstractUpdateRequest req, AffinityTopologyVersion topVer)\n        throws GridDhtInvalidPartitionException {\n        ctx.shared().database().checkpointReadLock();\n\n        if (req.size() == 1) {\n            KeyCacheObject key = req.key(0);\n\n            while (true) {\n                try {\n                    GridDhtCacheEntry entry = entryExx(key, topVer);\n\n                    GridUnsafe.monitorEnter(entry);\n\n                    if (entry.obsolete())\n                        GridUnsafe.monitorExit(entry);\n                    else\n                        return Collections.singletonList(entry);\n                }\n                catch (GridDhtInvalidPartitionException e) {\n                    // Ignore invalid partition exception in CLOCK ordering mode.\n                    if (ctx.config().getAtomicWriteOrderMode() == CLOCK)\n                        return Collections.singletonList(null);\n                    else\n                        throw e;\n                }\n            }\n        }\n        else {\n            List<GridDhtCacheEntry> locked = new ArrayList<>(req.size());\n\n            while (true) {\n                for (int i = 0; i < req.size(); i++) {\n                    try {\n                        GridDhtCacheEntry entry = entryExx(req.key(i), topVer);\n\n                        locked.add(entry);\n                    }\n                    catch (GridDhtInvalidPartitionException e) {\n                        // Ignore invalid partition exception in CLOCK ordering mode.\n                        if (ctx.config().getAtomicWriteOrderMode() == CLOCK)\n                            locked.add(null);\n                        else\n                            throw e;\n                    }\n                }\n\n                boolean retry = false;\n\n                for (int i = 0; i < locked.size(); i++) {\n                    GridCacheMapEntry entry = locked.get(i);\n\n                    if (entry == null)\n                        continue;\n\n                    GridUnsafe.monitorEnter(entry);\n\n                    if (entry.obsolete()) {\n                        // Unlock all locked.\n                        for (int j = 0; j <= i; j++) {\n                            if (locked.get(j) != null)\n                                GridUnsafe.monitorExit(locked.get(j));\n                        }\n\n                        // Clear entries.\n                        locked.clear();\n\n                        // Retry.\n                        retry = true;\n\n                        break;\n                    }\n                }\n\n                if (!retry)\n                    return locked;\n            }\n        }\n    }",
            "2920  \n2921  \n2922  \n2923  \n2924  \n2925  \n2926  \n2927  \n2928  \n2929  \n2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944  \n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  \n2957  \n2958  \n2959  \n2960  \n2961  \n2962  \n2963  \n2964  \n2965  \n2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003  \n3004  \n3005  ",
            "    /**\n     * Acquires java-level locks on cache entries. Returns collection of locked entries.\n     *\n     * @param req Request with keys to lock.\n     * @param topVer Topology version to lock on.\n     * @return Collection of locked entries.\n     * @throws GridDhtInvalidPartitionException If entry does not belong to local node. If exception is thrown,\n     *      locks are released.\n     */\n    @SuppressWarnings(\"ForLoopReplaceableByForEach\")\n    private List<GridDhtCacheEntry> lockEntries(GridNearAtomicAbstractUpdateRequest req, AffinityTopologyVersion topVer)\n        throws GridDhtInvalidPartitionException {\n        if (req.size() == 1) {\n            KeyCacheObject key = req.key(0);\n\n            while (true) {\n                try {\n                    GridDhtCacheEntry entry = entryExx(key, topVer);\n\n                    GridUnsafe.monitorEnter(entry);\n\n                    if (entry.obsolete())\n                        GridUnsafe.monitorExit(entry);\n                    else\n                        return Collections.singletonList(entry);\n                }\n                catch (GridDhtInvalidPartitionException e) {\n                    // Ignore invalid partition exception in CLOCK ordering mode.\n                    if (ctx.config().getAtomicWriteOrderMode() == CLOCK)\n                        return Collections.singletonList(null);\n                    else\n                        throw e;\n                }\n            }\n        }\n        else {\n            List<GridDhtCacheEntry> locked = new ArrayList<>(req.size());\n\n            while (true) {\n                for (int i = 0; i < req.size(); i++) {\n                    try {\n                        GridDhtCacheEntry entry = entryExx(req.key(i), topVer);\n\n                        locked.add(entry);\n                    }\n                    catch (GridDhtInvalidPartitionException e) {\n                        // Ignore invalid partition exception in CLOCK ordering mode.\n                        if (ctx.config().getAtomicWriteOrderMode() == CLOCK)\n                            locked.add(null);\n                        else\n                            throw e;\n                    }\n                }\n\n                boolean retry = false;\n\n                for (int i = 0; i < locked.size(); i++) {\n                    GridCacheMapEntry entry = locked.get(i);\n\n                    if (entry == null)\n                        continue;\n\n                    GridUnsafe.monitorEnter(entry);\n\n                    if (entry.obsolete()) {\n                        // Unlock all locked.\n                        for (int j = 0; j <= i; j++) {\n                            if (locked.get(j) != null)\n                                GridUnsafe.monitorExit(locked.get(j));\n                        }\n\n                        // Clear entries.\n                        locked.clear();\n\n                        // Retry.\n                        retry = true;\n\n                        break;\n                    }\n                }\n\n                if (!retry)\n                    return locked;\n            }\n        }\n    }"
        ],
        [
            "GridCacheProcessor::onExchangeDone(AffinityTopologyVersion,Collection,Throwable)",
            "1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949 -\n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967 -\n1968  \n1969  \n1970  \n1971  \n1972  \n1973  ",
            "    /**\n     * Callback invoked when first exchange future for dynamic cache is completed.\n     *\n     * @param topVer Completed topology version.\n     * @param reqs Change requests.\n     * @param err Error.\n     */\n    @SuppressWarnings(\"unchecked\")\n    public void onExchangeDone(\n        AffinityTopologyVersion topVer,\n        Collection<DynamicCacheChangeRequest> reqs,\n        Throwable err\n    ) {\n        for (GridCacheAdapter<?, ?> cache : caches.values()) {\n            GridCacheContext<?, ?> cacheCtx = cache.context();\n\n            if (F.eq(cacheCtx.startTopologyVersion(), topVer)) {\n                if (cacheCtx.preloader() != null)\n                    cacheCtx.preloader().onInitialExchangeComplete(err);\n\n                String masked = maskNull(cacheCtx.name());\n\n                jCacheProxies.putIfAbsent(masked, new IgniteCacheProxy(cache.context(), cache, null, false));\n            }\n        }\n\n        if (!F.isEmpty(reqs) && err == null) {\n            for (DynamicCacheChangeRequest req : reqs) {\n                String masked = maskNull(req.cacheName());\n\n                if (req.stop()) {\n                    stopGateway(req);\n\n                    prepareCacheStop(req);\n                }\n                else if (req.close() && req.initiatingNodeId().equals(ctx.localNodeId())) {\n                    IgniteCacheProxy<?, ?> proxy = jCacheProxies.remove(masked);\n\n                    if (proxy != null) {\n                        if (proxy.context().affinityNode()) {\n                            GridCacheAdapter<?, ?> cache = caches.get(masked);\n\n                            if (cache != null)\n                                jCacheProxies.putIfAbsent(masked, new IgniteCacheProxy(cache.context(), cache, null, false));\n                        }\n                        else {\n                            if (req.restart())\n                                proxy.restart();\n\n                            proxy.context().gate().onStopped();\n\n                            prepareCacheStop(req);\n                        }\n                    }\n                }\n            }\n        }\n    }",
            "1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939 +\n1940 +\n1941  \n1942  \n1943  \n1944 +\n1945 +\n1946 +\n1947  \n1948  \n1949  \n1950 +\n1951 +\n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969 +\n1970 +\n1971  \n1972  \n1973  \n1974 +\n1975 +\n1976 +\n1977 +\n1978 +\n1979 +\n1980 +\n1981  \n1982 +\n1983 +\n1984 +\n1985  \n1986  ",
            "    /**\n     * Callback invoked when first exchange future for dynamic cache is completed.\n     *\n     * @param topVer Completed topology version.\n     * @param reqs Change requests.\n     * @param err Error.\n     */\n    @SuppressWarnings(\"unchecked\")\n    public void onExchangeDone(\n        AffinityTopologyVersion topVer,\n        Collection<DynamicCacheChangeRequest> reqs,\n        Throwable err\n    ) {\n        for (GridCacheAdapter<?, ?> cache : caches.values()) {\n            GridCacheContext<?, ?> cacheCtx = cache.context();\n\n            if (F.eq(cacheCtx.startTopologyVersion(), topVer)) {\n                if (cacheCtx.preloader() != null)\n                    cacheCtx.preloader().onInitialExchangeComplete(err);\n\n                String masked = maskNull(cacheCtx.name());\n\n                jCacheProxies.putIfAbsent(masked, new IgniteCacheProxy(cache.context(), cache, null, false));\n            }\n        }\n\n        if (!F.isEmpty(reqs) && err == null) {\n            Collection<IgniteBiTuple<GridCacheContext, Boolean>> stopped = null;\n\n            for (DynamicCacheChangeRequest req : reqs) {\n                String masked = maskNull(req.cacheName());\n\n                GridCacheContext<?, ?> stopCtx = null;\n                boolean destroy = false;\n\n                if (req.stop()) {\n                    stopGateway(req);\n\n                    stopCtx = prepareCacheStop(req);\n                    destroy = req.destroy();\n                }\n                else if (req.close() && req.initiatingNodeId().equals(ctx.localNodeId())) {\n                    IgniteCacheProxy<?, ?> proxy = jCacheProxies.remove(masked);\n\n                    if (proxy != null) {\n                        if (proxy.context().affinityNode()) {\n                            GridCacheAdapter<?, ?> cache = caches.get(masked);\n\n                            if (cache != null)\n                                jCacheProxies.putIfAbsent(masked, new IgniteCacheProxy(cache.context(), cache, null, false));\n                        }\n                        else {\n                            if (req.restart())\n                                proxy.restart();\n\n                            proxy.context().gate().onStopped();\n\n                            stopCtx = prepareCacheStop(req);\n                            destroy = req.destroy();\n                        }\n                    }\n                }\n\n                if (stopCtx != null) {\n                    if (stopped == null)\n                        stopped = new ArrayList<>();\n\n                    stopped.add(F.<GridCacheContext, Boolean>t(stopCtx, destroy));\n                }\n            }\n\n            if (stopped != null)\n                sharedCtx.database().onCachesStopped(stopped);\n        }\n    }"
        ],
        [
            "GridDhtAtomicCache::updateAllAsyncInternal0(UUID,GridNearAtomicAbstractUpdateRequest,CI2)",
            "1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  ",
            "    /**\n     * Executes local update after preloader fetched values.\n     *\n     * @param nodeId Node ID.\n     * @param req Update request.\n     * @param completionCb Completion callback.\n     */\n    private void updateAllAsyncInternal0(\n        UUID nodeId,\n        GridNearAtomicAbstractUpdateRequest req,\n        CI2<GridNearAtomicAbstractUpdateRequest, GridNearAtomicUpdateResponse> completionCb\n    ) {\n        GridNearAtomicUpdateResponse res = new GridNearAtomicUpdateResponse(ctx.cacheId(), nodeId, req.futureVersion(),\n            ctx.deploymentEnabled());\n\n        assert !req.returnValue() || (req.operation() == TRANSFORM || req.size() == 1);\n\n        GridDhtAtomicAbstractUpdateFuture dhtFut = null;\n\n        boolean remap = false;\n\n        String taskName = ctx.kernalContext().task().resolveTaskName(req.taskNameHash());\n\n        IgniteCacheExpiryPolicy expiry = null;\n\n        try {\n            // If batch store update is enabled, we need to lock all entries.\n            // First, need to acquire locks on cache entries, then check filter.\n            List<GridDhtCacheEntry> locked = lockEntries(req, req.topologyVersion());\n\n            Collection<IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion>> deleted = null;\n\n            try {\n                GridDhtPartitionTopology top = topology();\n\n                top.readLock();\n\n                try {\n                    if (top.stopping()) {\n                        res.addFailedKeys(req.keys(), new CacheStoppedException(name()));\n\n                        completionCb.apply(req, res);\n\n                        return;\n                    }\n\n                    // Do not check topology version for CLOCK versioning since\n                    // partition exchange will wait for near update future (if future is on server node).\n                    // Also do not check topology version if topology was locked on near node by\n                    // external transaction or explicit lock.\n                    if ((req.fastMap() && !req.clientRequest()) || req.topologyLocked() ||\n                        !needRemap(req.topologyVersion(), top.topologyVersion())) {\n                        ClusterNode node = ctx.discovery().node(nodeId);\n\n                        if (node == null) {\n                            U.warn(msgLog, \"Skip near update request, node originated update request left [\" +\n                                \"futId=\" + req.futureVersion() + \", node=\" + nodeId + ']');\n\n                            return;\n                        }\n\n                        boolean hasNear = ctx.discovery().cacheNearNode(node, name());\n\n                        GridCacheVersion ver = req.updateVersion();\n\n                        if (ver == null) {\n                            // Assign next version for update inside entries lock.\n                            ver = ctx.versions().next(top.topologyVersion());\n\n                            if (hasNear)\n                                res.nearVersion(ver);\n\n                            if (msgLog.isDebugEnabled()) {\n                                msgLog.debug(\"Assigned update version [futId=\" + req.futureVersion() +\n                                    \", writeVer=\" + ver + ']');\n                            }\n                        }\n\n                        assert ver != null : \"Got null version for update request: \" + req;\n\n                        boolean sndPrevVal = !top.rebalanceFinished(req.topologyVersion());\n\n                        dhtFut = createDhtFuture(ver, req, res, completionCb, false);\n\n                        expiry = expiryPolicy(req.expiry());\n\n                        GridCacheReturn retVal = null;\n\n                        if (req.size() > 1 &&                    // Several keys ...\n                            writeThrough() && !req.skipStore() && // and store is enabled ...\n                            !ctx.store().isLocal() &&             // and this is not local store ...\n                                                                  // (conflict resolver should be used for local store)\n                            !ctx.dr().receiveEnabled()            // and no DR.\n                            ) {\n                            // This method can only be used when there are no replicated entries in the batch.\n                            UpdateBatchResult updRes = updateWithBatch(node,\n                                hasNear,\n                                req,\n                                res,\n                                locked,\n                                ver,\n                                dhtFut,\n                                completionCb,\n                                ctx.isDrEnabled(),\n                                taskName,\n                                expiry,\n                                sndPrevVal);\n\n                            deleted = updRes.deleted();\n                            dhtFut = updRes.dhtFuture();\n\n                            if (req.operation() == TRANSFORM)\n                                retVal = updRes.invokeResults();\n                        }\n                        else {\n                            UpdateSingleResult updRes = updateSingle(node,\n                                hasNear,\n                                req,\n                                res,\n                                locked,\n                                ver,\n                                dhtFut,\n                                completionCb,\n                                ctx.isDrEnabled(),\n                                taskName,\n                                expiry,\n                                sndPrevVal);\n\n                            retVal = updRes.returnValue();\n                            deleted = updRes.deleted();\n                            dhtFut = updRes.dhtFuture();\n                        }\n\n                        if (retVal == null)\n                            retVal = new GridCacheReturn(ctx, node.isLocal(), true, null, true);\n\n                        res.returnValue(retVal);\n\n                        if (req.writeSynchronizationMode() != FULL_ASYNC)\n                            req.cleanup(!node.isLocal());\n\n                        if (dhtFut != null)\n                            ctx.mvcc().addAtomicFuture(dhtFut.version(), dhtFut);\n                    }\n                    else\n                        // Should remap all keys.\n                        remap = true;\n                }\n                finally {\n                    top.readUnlock();\n                }\n            }\n            catch (GridCacheEntryRemovedException e) {\n                assert false : \"Entry should not become obsolete while holding lock.\";\n\n                e.printStackTrace();\n            }\n            finally {\n                if (locked != null)\n                    unlockEntries(locked, req.topologyVersion());\n\n                // Enqueue if necessary after locks release.\n                if (deleted != null) {\n                    assert !deleted.isEmpty();\n                    assert ctx.deferredDelete() : this;\n\n                    for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted)\n                        ctx.onDeferredDelete(e.get1(), e.get2());\n                }\n\n                // TODO handle failure: probably drop the node from topology\n                // TODO fire events only after successful fsync\n                if (ctx.shared().wal() != null)\n                    ctx.shared().wal().fsync(null);\n            }\n        }\n        catch (GridDhtInvalidPartitionException ignore) {\n            assert !req.fastMap() || req.clientRequest() : req;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Caught invalid partition exception for cache entry (will remap update request): \" + req);\n\n            remap = true;\n        }\n        catch (Throwable e) {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            U.error(log, \"Unexpected exception during cache update\", e);\n\n            res.addFailedKeys(req.keys(), e);\n\n            completionCb.apply(req, res);\n\n            if (e instanceof Error)\n                throw (Error)e;\n\n            return;\n        }\n\n        if (remap) {\n            assert dhtFut == null;\n\n            res.remapKeys(req.keys());\n\n            completionCb.apply(req, res);\n        }\n        else {\n            // If there are backups, map backup update future.\n            if (dhtFut != null)\n                dhtFut.map();\n                // Otherwise, complete the call.\n            else\n                completionCb.apply(req, res);\n        }\n\n        sendTtlUpdateRequest(expiry);\n    }",
            "1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809 +\n1810 +\n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984 +\n1985 +\n1986 +\n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  ",
            "    /**\n     * Executes local update after preloader fetched values.\n     *\n     * @param nodeId Node ID.\n     * @param req Update request.\n     * @param completionCb Completion callback.\n     */\n    private void updateAllAsyncInternal0(\n        UUID nodeId,\n        GridNearAtomicAbstractUpdateRequest req,\n        CI2<GridNearAtomicAbstractUpdateRequest, GridNearAtomicUpdateResponse> completionCb\n    ) {\n        GridNearAtomicUpdateResponse res = new GridNearAtomicUpdateResponse(ctx.cacheId(), nodeId, req.futureVersion(),\n            ctx.deploymentEnabled());\n\n        assert !req.returnValue() || (req.operation() == TRANSFORM || req.size() == 1);\n\n        GridDhtAtomicAbstractUpdateFuture dhtFut = null;\n\n        boolean remap = false;\n\n        String taskName = ctx.kernalContext().task().resolveTaskName(req.taskNameHash());\n\n        IgniteCacheExpiryPolicy expiry = null;\n\n        ctx.shared().database().checkpointReadLock();\n\n        try {\n            // If batch store update is enabled, we need to lock all entries.\n            // First, need to acquire locks on cache entries, then check filter.\n            List<GridDhtCacheEntry> locked = lockEntries(req, req.topologyVersion());\n\n            Collection<IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion>> deleted = null;\n\n            try {\n                GridDhtPartitionTopology top = topology();\n\n                top.readLock();\n\n                try {\n                    if (top.stopping()) {\n                        res.addFailedKeys(req.keys(), new CacheStoppedException(name()));\n\n                        completionCb.apply(req, res);\n\n                        return;\n                    }\n\n                    // Do not check topology version for CLOCK versioning since\n                    // partition exchange will wait for near update future (if future is on server node).\n                    // Also do not check topology version if topology was locked on near node by\n                    // external transaction or explicit lock.\n                    if ((req.fastMap() && !req.clientRequest()) || req.topologyLocked() ||\n                        !needRemap(req.topologyVersion(), top.topologyVersion())) {\n                        ClusterNode node = ctx.discovery().node(nodeId);\n\n                        if (node == null) {\n                            U.warn(msgLog, \"Skip near update request, node originated update request left [\" +\n                                \"futId=\" + req.futureVersion() + \", node=\" + nodeId + ']');\n\n                            return;\n                        }\n\n                        boolean hasNear = ctx.discovery().cacheNearNode(node, name());\n\n                        GridCacheVersion ver = req.updateVersion();\n\n                        if (ver == null) {\n                            // Assign next version for update inside entries lock.\n                            ver = ctx.versions().next(top.topologyVersion());\n\n                            if (hasNear)\n                                res.nearVersion(ver);\n\n                            if (msgLog.isDebugEnabled()) {\n                                msgLog.debug(\"Assigned update version [futId=\" + req.futureVersion() +\n                                    \", writeVer=\" + ver + ']');\n                            }\n                        }\n\n                        assert ver != null : \"Got null version for update request: \" + req;\n\n                        boolean sndPrevVal = !top.rebalanceFinished(req.topologyVersion());\n\n                        dhtFut = createDhtFuture(ver, req, res, completionCb, false);\n\n                        expiry = expiryPolicy(req.expiry());\n\n                        GridCacheReturn retVal = null;\n\n                        if (req.size() > 1 &&                    // Several keys ...\n                            writeThrough() && !req.skipStore() && // and store is enabled ...\n                            !ctx.store().isLocal() &&             // and this is not local store ...\n                                                                  // (conflict resolver should be used for local store)\n                            !ctx.dr().receiveEnabled()            // and no DR.\n                            ) {\n                            // This method can only be used when there are no replicated entries in the batch.\n                            UpdateBatchResult updRes = updateWithBatch(node,\n                                hasNear,\n                                req,\n                                res,\n                                locked,\n                                ver,\n                                dhtFut,\n                                completionCb,\n                                ctx.isDrEnabled(),\n                                taskName,\n                                expiry,\n                                sndPrevVal);\n\n                            deleted = updRes.deleted();\n                            dhtFut = updRes.dhtFuture();\n\n                            if (req.operation() == TRANSFORM)\n                                retVal = updRes.invokeResults();\n                        }\n                        else {\n                            UpdateSingleResult updRes = updateSingle(node,\n                                hasNear,\n                                req,\n                                res,\n                                locked,\n                                ver,\n                                dhtFut,\n                                completionCb,\n                                ctx.isDrEnabled(),\n                                taskName,\n                                expiry,\n                                sndPrevVal);\n\n                            retVal = updRes.returnValue();\n                            deleted = updRes.deleted();\n                            dhtFut = updRes.dhtFuture();\n                        }\n\n                        if (retVal == null)\n                            retVal = new GridCacheReturn(ctx, node.isLocal(), true, null, true);\n\n                        res.returnValue(retVal);\n\n                        if (req.writeSynchronizationMode() != FULL_ASYNC)\n                            req.cleanup(!node.isLocal());\n\n                        if (dhtFut != null)\n                            ctx.mvcc().addAtomicFuture(dhtFut.version(), dhtFut);\n                    }\n                    else\n                        // Should remap all keys.\n                        remap = true;\n                }\n                finally {\n                    top.readUnlock();\n                }\n            }\n            catch (GridCacheEntryRemovedException e) {\n                assert false : \"Entry should not become obsolete while holding lock.\";\n\n                e.printStackTrace();\n            }\n            finally {\n                if (locked != null)\n                    unlockEntries(locked, req.topologyVersion());\n\n                // Enqueue if necessary after locks release.\n                if (deleted != null) {\n                    assert !deleted.isEmpty();\n                    assert ctx.deferredDelete() : this;\n\n                    for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted)\n                        ctx.onDeferredDelete(e.get1(), e.get2());\n                }\n\n                // TODO handle failure: probably drop the node from topology\n                // TODO fire events only after successful fsync\n                if (ctx.shared().wal() != null)\n                    ctx.shared().wal().fsync(null);\n            }\n        }\n        catch (GridDhtInvalidPartitionException ignore) {\n            assert !req.fastMap() || req.clientRequest() : req;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Caught invalid partition exception for cache entry (will remap update request): \" + req);\n\n            remap = true;\n        }\n        catch (Throwable e) {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            U.error(log, \"Unexpected exception during cache update\", e);\n\n            res.addFailedKeys(req.keys(), e);\n\n            completionCb.apply(req, res);\n\n            if (e instanceof Error)\n                throw (Error)e;\n\n            return;\n        }\n        finally {\n            ctx.shared().database().checkpointReadUnlock();\n        }\n\n        if (remap) {\n            assert dhtFut == null;\n\n            res.remapKeys(req.keys());\n\n            completionCb.apply(req, res);\n        }\n        else {\n            // If there are backups, map backup update future.\n            if (dhtFut != null)\n                dhtFut.map();\n                // Otherwise, complete the call.\n            else\n                completionCb.apply(req, res);\n        }\n\n        sendTtlUpdateRequest(expiry);\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::init()",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538 -\n 539 -\n 540 -\n 541 -\n 542 -\n 543 -\n 544 -\n 545 -\n 546 -\n 547 -\n 548 -\n 549 -\n 550 -\n 551 -\n 552 -\n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  ",
            "    /**\n     * Starts activity.\n     *\n     * @throws IgniteInterruptedCheckedException If interrupted.\n     */\n    public void init() throws IgniteInterruptedCheckedException {\n        if (isDone())\n            return;\n\n        initTs = U.currentTimeMillis();\n\n        U.await(evtLatch);\n\n        assert discoEvt != null : this;\n        assert exchId.nodeId().equals(discoEvt.eventNode().id()) : this;\n        assert !dummy && !forcePreload : this;\n\n        try {\n            AffinityTopologyVersion topVer = topologyVersion();\n\n            srvNodes = new ArrayList<>(cctx.discovery().serverNodes(topVer));\n\n            remaining.addAll(F.nodeIds(F.view(srvNodes, F.remoteNodes(cctx.localNodeId()))));\n\n            crd = srvNodes.isEmpty() ? null : srvNodes.get(0);\n\n            boolean crdNode = crd != null && crd.isLocal();\n\n            skipPreload = cctx.kernalContext().clientNode();\n\n            ExchangeType exchange;\n\n            if (discoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                DiscoveryCustomMessage msg = ((DiscoveryCustomEvent)discoEvt).customMessage();\n\n                if (msg instanceof DynamicCacheChangeBatch){\n                    assert !F.isEmpty(reqs);\n\n                    exchange = onCacheChangeRequest(crdNode);\n                }\n                else if (msg instanceof StartFullSnapshotAckDiscoveryMessage)\n                    exchange = CU.clientNode(discoEvt.eventNode()) ?\n                        onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                else {\n                    assert affChangeMsg != null : this;\n\n                    exchange = onAffinityChangeRequest(crdNode);\n                }\n            }\n            else {\n                if (discoEvt.type() == EVT_NODE_JOINED) {\n                    Collection<DynamicCacheDescriptor> receivedCaches = cctx.cache().startReceivedCaches(topVer);\n\n                    if (!discoEvt.eventNode().isLocal())\n                        cctx.affinity().initStartedCaches(crdNode, this, receivedCaches);\n                }\n\n                exchange = CU.clientNode(discoEvt.eventNode()) ?\n                    onClientNodeEvent(crdNode) :\n                    onServerNodeEvent(crdNode);\n            }\n\n            updateTopologies(crdNode);\n\n            if (!F.isEmpty(reqs)) {\n                boolean hasStop = false;\n\n                for (DynamicCacheChangeRequest req : reqs) {\n                    if (req.stop()) {\n                        hasStop = true;\n\n                        break;\n                    }\n                }\n\n                if (hasStop)\n                    cctx.cache().context().database().beforeCachesStop();\n            }\n\n            switch (exchange) {\n                case ALL: {\n                    distributedExchange();\n\n                    break;\n                }\n\n                case CLIENT: {\n                    initTopologies();\n\n                    clientOnlyExchange();\n\n                    break;\n                }\n\n                case NONE: {\n                    initTopologies();\n\n                    onDone(topVer);\n\n                    break;\n                }\n\n                default:\n                    assert false;\n            }\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            onDone(e);\n\n            throw e;\n        }\n        catch (Throwable e) {\n            U.error(log, \"Failed to reinitialize local partitions (preloading will be stopped): \" + exchId, e);\n\n            onDone(e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n    }",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  ",
            "    /**\n     * Starts activity.\n     *\n     * @throws IgniteInterruptedCheckedException If interrupted.\n     */\n    public void init() throws IgniteInterruptedCheckedException {\n        if (isDone())\n            return;\n\n        initTs = U.currentTimeMillis();\n\n        U.await(evtLatch);\n\n        assert discoEvt != null : this;\n        assert exchId.nodeId().equals(discoEvt.eventNode().id()) : this;\n        assert !dummy && !forcePreload : this;\n\n        try {\n            AffinityTopologyVersion topVer = topologyVersion();\n\n            srvNodes = new ArrayList<>(cctx.discovery().serverNodes(topVer));\n\n            remaining.addAll(F.nodeIds(F.view(srvNodes, F.remoteNodes(cctx.localNodeId()))));\n\n            crd = srvNodes.isEmpty() ? null : srvNodes.get(0);\n\n            boolean crdNode = crd != null && crd.isLocal();\n\n            skipPreload = cctx.kernalContext().clientNode();\n\n            ExchangeType exchange;\n\n            if (discoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                DiscoveryCustomMessage msg = ((DiscoveryCustomEvent)discoEvt).customMessage();\n\n                if (msg instanceof DynamicCacheChangeBatch){\n                    assert !F.isEmpty(reqs);\n\n                    exchange = onCacheChangeRequest(crdNode);\n                }\n                else if (msg instanceof StartFullSnapshotAckDiscoveryMessage)\n                    exchange = CU.clientNode(discoEvt.eventNode()) ?\n                        onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                else {\n                    assert affChangeMsg != null : this;\n\n                    exchange = onAffinityChangeRequest(crdNode);\n                }\n            }\n            else {\n                if (discoEvt.type() == EVT_NODE_JOINED) {\n                    Collection<DynamicCacheDescriptor> receivedCaches = cctx.cache().startReceivedCaches(topVer);\n\n                    if (!discoEvt.eventNode().isLocal())\n                        cctx.affinity().initStartedCaches(crdNode, this, receivedCaches);\n                }\n\n                exchange = CU.clientNode(discoEvt.eventNode()) ?\n                    onClientNodeEvent(crdNode) :\n                    onServerNodeEvent(crdNode);\n            }\n\n            updateTopologies(crdNode);\n\n            switch (exchange) {\n                case ALL: {\n                    distributedExchange();\n\n                    break;\n                }\n\n                case CLIENT: {\n                    initTopologies();\n\n                    clientOnlyExchange();\n\n                    break;\n                }\n\n                case NONE: {\n                    initTopologies();\n\n                    onDone(topVer);\n\n                    break;\n                }\n\n                default:\n                    assert false;\n            }\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            onDone(e);\n\n            throw e;\n        }\n        catch (Throwable e) {\n            U.error(log, \"Failed to reinitialize local partitions (preloading will be stopped): \" + exchId, e);\n\n            onDone(e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n    }"
        ],
        [
            "GridServiceProcessor::onKernalStop(boolean)",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357 -\n 358  \n 359  \n 360  \n 361  \n 362  \n 363  ",
            "    /** {@inheritDoc} */\n    @Override public void onKernalStop(boolean cancel) {\n        busyLock.block();\n\n        try {\n            if (ctx.isDaemon())\n                return;\n\n            if (!ctx.clientNode())\n                ctx.event().removeLocalEventListener(topLsnr);\n\n            Collection<ServiceContextImpl> ctxs = new ArrayList<>();\n\n            synchronized (locSvcs) {\n                for (Collection<ServiceContextImpl> ctxs0 : locSvcs.values())\n                    ctxs.addAll(ctxs0);\n            }\n\n            for (ServiceContextImpl ctx : ctxs) {\n                ctx.setCancelled(true);\n\n                Service svc = ctx.service();\n\n                if (svc != null)\n                    svc.cancel(ctx);\n\n                ctx.executor().shutdownNow();\n            }\n\n            for (ServiceContextImpl ctx : ctxs) {\n                try {\n                    if (log.isInfoEnabled() && !ctxs.isEmpty())\n                        log.info(\"Shutting down distributed service [name=\" + ctx.name() + \", execId8=\" +\n                            U.id8(ctx.executionId()) + ']');\n\n                    ctx.executor().awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n\n                    U.error(log, \"Got interrupted while waiting for service to shutdown (will continue stopping node): \" +\n                        ctx.name());\n                }\n            }\n\n            U.shutdownNow(GridServiceProcessor.class, depExe, log);\n\n            Exception err = new IgniteCheckedException(\"Operation has been cancelled (node is stopping).\");\n\n            cancelFutures(depFuts, err);\n            cancelFutures(undepFuts, err);\n        }finally {\n            busyLock.unblock();\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(\"Stopped service processor.\");\n    }",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357 +\n 358 +\n 359  \n 360  \n 361  \n 362  \n 363  \n 364  ",
            "    /** {@inheritDoc} */\n    @Override public void onKernalStop(boolean cancel) {\n        busyLock.block();\n\n        try {\n            if (ctx.isDaemon())\n                return;\n\n            if (!ctx.clientNode())\n                ctx.event().removeLocalEventListener(topLsnr);\n\n            Collection<ServiceContextImpl> ctxs = new ArrayList<>();\n\n            synchronized (locSvcs) {\n                for (Collection<ServiceContextImpl> ctxs0 : locSvcs.values())\n                    ctxs.addAll(ctxs0);\n            }\n\n            for (ServiceContextImpl ctx : ctxs) {\n                ctx.setCancelled(true);\n\n                Service svc = ctx.service();\n\n                if (svc != null)\n                    svc.cancel(ctx);\n\n                ctx.executor().shutdownNow();\n            }\n\n            for (ServiceContextImpl ctx : ctxs) {\n                try {\n                    if (log.isInfoEnabled() && !ctxs.isEmpty())\n                        log.info(\"Shutting down distributed service [name=\" + ctx.name() + \", execId8=\" +\n                            U.id8(ctx.executionId()) + ']');\n\n                    ctx.executor().awaitTermination(Long.MAX_VALUE, TimeUnit.MILLISECONDS);\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n\n                    U.error(log, \"Got interrupted while waiting for service to shutdown (will continue stopping node): \" +\n                        ctx.name());\n                }\n            }\n\n            U.shutdownNow(GridServiceProcessor.class, depExe, log);\n\n            Exception err = new IgniteCheckedException(\"Operation has been cancelled (node is stopping).\");\n\n            cancelFutures(depFuts, err);\n            cancelFutures(undepFuts, err);\n        }\n        finally {\n            busyLock.unblock();\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(\"Stopped service processor.\");\n    }"
        ],
        [
            "GridDhtAtomicCache::unlockEntries(Collection,AffinityTopologyVersion)",
            "3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044 -\n3045 -\n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  ",
            "    /**\n     * Releases java-level locks on cache entries.\n     *\n     * @param locked Locked entries.\n     * @param topVer Topology version.\n     */\n    private void unlockEntries(Collection<GridDhtCacheEntry> locked, AffinityTopologyVersion topVer) {\n        // Process deleted entries before locks release.\n        assert ctx.deferredDelete() : this;\n\n        // Entries to skip eviction manager notification for.\n        // Enqueue entries while holding locks.\n        Collection<KeyCacheObject> skip = null;\n\n        try {\n            for (GridCacheMapEntry entry : locked) {\n                if (entry != null && entry.deleted()) {\n                    if (skip == null)\n                        skip = new HashSet<>(locked.size(), 1.0f);\n\n                    skip.add(entry.key());\n                }\n            }\n        }\n        finally {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            // That's why releasing locks in the finally block..\n            for (GridCacheMapEntry entry : locked) {\n                if (entry != null)\n                    GridUnsafe.monitorExit(entry);\n            }\n        }\n\n        // Try evict partitions.\n        for (GridDhtCacheEntry entry : locked) {\n            if (entry != null)\n                entry.onUnlock();\n        }\n\n        ctx.shared().database().checkpointReadUnlock();\n\n        if (skip != null && skip.size() == locked.size())\n            // Optimization.\n            return;\n\n        // Must touch all entries since update may have deleted entries.\n        // Eviction manager will remove empty entries.\n        for (GridCacheMapEntry entry : locked) {\n            if (entry != null && (skip == null || !skip.contains(entry.key())))\n                ctx.evicts().touch(entry, topVer);\n        }\n    }",
            "3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  \n3057  ",
            "    /**\n     * Releases java-level locks on cache entries.\n     *\n     * @param locked Locked entries.\n     * @param topVer Topology version.\n     */\n    private void unlockEntries(Collection<GridDhtCacheEntry> locked, AffinityTopologyVersion topVer) {\n        // Process deleted entries before locks release.\n        assert ctx.deferredDelete() : this;\n\n        // Entries to skip eviction manager notification for.\n        // Enqueue entries while holding locks.\n        Collection<KeyCacheObject> skip = null;\n\n        try {\n            for (GridCacheMapEntry entry : locked) {\n                if (entry != null && entry.deleted()) {\n                    if (skip == null)\n                        skip = new HashSet<>(locked.size(), 1.0f);\n\n                    skip.add(entry.key());\n                }\n            }\n        }\n        finally {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            // That's why releasing locks in the finally block..\n            for (GridCacheMapEntry entry : locked) {\n                if (entry != null)\n                    GridUnsafe.monitorExit(entry);\n            }\n        }\n\n        // Try evict partitions.\n        for (GridDhtCacheEntry entry : locked) {\n            if (entry != null)\n                entry.onUnlock();\n        }\n\n        if (skip != null && skip.size() == locked.size())\n            // Optimization.\n            return;\n\n        // Must touch all entries since update may have deleted entries.\n        // Eviction manager will remove empty entries.\n        for (GridCacheMapEntry entry : locked) {\n            if (entry != null && (skip == null || !skip.contains(entry.key())))\n                ctx.evicts().touch(entry, topVer);\n        }\n    }"
        ],
        [
            "GridCacheProcessor::stopCache(GridCacheAdapter,boolean,boolean)",
            "1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251 -\n1252 -\n1253 -\n1254 -\n1255 -\n1256 -\n1257 -\n1258 -\n1259 -\n1260 -\n1261  \n1262  ",
            "    /**\n     * @param cache Cache to stop.\n     * @param cancel Cancel flag.\n     */\n    @SuppressWarnings({\"TypeMayBeWeakened\", \"unchecked\"})\n    private void stopCache(GridCacheAdapter<?, ?> cache, boolean cancel, boolean destroy) {\n        GridCacheContext ctx = cache.context();\n\n        if (!cache.isNear() && ctx.shared().wal() != null) {\n            try {\n                ctx.shared().wal().fsync(null);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to flush write-ahead log on cache stop \" +\n                    \"[cache=\" + ctx.name() + \"]\", e);\n            }\n        }\n\n        sharedCtx.removeCacheContext(ctx);\n\n        cache.stop();\n\n        ctx.kernalContext().query().onCacheStop(ctx);\n\n        if (isNearEnabled(ctx)) {\n            GridDhtCacheAdapter dht = ctx.near().dht();\n\n            // Check whether dht cache has been started.\n            if (dht != null) {\n                dht.stop();\n\n                GridCacheContext<?, ?> dhtCtx = dht.context();\n\n                List<GridCacheManager> dhtMgrs = dhtManagers(dhtCtx);\n\n                for (ListIterator<GridCacheManager> it = dhtMgrs.listIterator(dhtMgrs.size()); it.hasPrevious(); ) {\n                    GridCacheManager mgr = it.previous();\n\n                    mgr.stop(cancel, destroy);\n                }\n            }\n        }\n\n        List<GridCacheManager> mgrs = ctx.managers();\n\n        Collection<GridCacheManager> excludes = dhtExcludes(ctx);\n\n        // Reverse order.\n        for (ListIterator<GridCacheManager> it = mgrs.listIterator(mgrs.size()); it.hasPrevious(); ) {\n            GridCacheManager mgr = it.previous();\n\n            if (!excludes.contains(mgr))\n                mgr.stop(cancel, destroy);\n        }\n\n        ctx.kernalContext().continuous().onCacheStop(ctx);\n\n        ctx.kernalContext().cache().context().database().onCacheStop(ctx);\n\n        U.stopLifecycleAware(log, lifecycleAwares(cache.configuration(), ctx.store().configuredStore()));\n\n        if (log.isInfoEnabled())\n            log.info(\"Stopped cache: \" + cache.name());\n\n        if (sharedCtx.pageStore() != null) {\n            try {\n                sharedCtx.pageStore().shutdownForCache(ctx, destroy);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to gracefully clean page store resources for destroyed cache \" +\n                    \"[cache=\" + ctx.name() + \"]\", e);\n            }\n        }\n\n        cleanup(ctx);\n    }",
            "1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  ",
            "    /**\n     * @param cache Cache to stop.\n     * @param cancel Cancel flag.\n     */\n    @SuppressWarnings({\"TypeMayBeWeakened\", \"unchecked\"})\n    private void stopCache(GridCacheAdapter<?, ?> cache, boolean cancel, boolean destroy) {\n        GridCacheContext ctx = cache.context();\n\n        if (!cache.isNear() && ctx.shared().wal() != null) {\n            try {\n                ctx.shared().wal().fsync(null);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to flush write-ahead log on cache stop \" +\n                    \"[cache=\" + ctx.name() + \"]\", e);\n            }\n        }\n\n        sharedCtx.removeCacheContext(ctx);\n\n        cache.stop();\n\n        ctx.kernalContext().query().onCacheStop(ctx);\n\n        if (isNearEnabled(ctx)) {\n            GridDhtCacheAdapter dht = ctx.near().dht();\n\n            // Check whether dht cache has been started.\n            if (dht != null) {\n                dht.stop();\n\n                GridCacheContext<?, ?> dhtCtx = dht.context();\n\n                List<GridCacheManager> dhtMgrs = dhtManagers(dhtCtx);\n\n                for (ListIterator<GridCacheManager> it = dhtMgrs.listIterator(dhtMgrs.size()); it.hasPrevious(); ) {\n                    GridCacheManager mgr = it.previous();\n\n                    mgr.stop(cancel, destroy);\n                }\n            }\n        }\n\n        List<GridCacheManager> mgrs = ctx.managers();\n\n        Collection<GridCacheManager> excludes = dhtExcludes(ctx);\n\n        // Reverse order.\n        for (ListIterator<GridCacheManager> it = mgrs.listIterator(mgrs.size()); it.hasPrevious(); ) {\n            GridCacheManager mgr = it.previous();\n\n            if (!excludes.contains(mgr))\n                mgr.stop(cancel, destroy);\n        }\n\n        ctx.kernalContext().continuous().onCacheStop(ctx);\n\n        ctx.kernalContext().cache().context().database().onCacheStop(ctx);\n\n        U.stopLifecycleAware(log, lifecycleAwares(cache.configuration(), ctx.store().configuredStore()));\n\n        if (log.isInfoEnabled())\n            log.info(\"Stopped cache: \" + cache.name());\n\n        cleanup(ctx);\n    }"
        ],
        [
            "GridCacheProcessor::checkConsistency()",
            " 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891 -\n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  ",
            "    /**\n     *\n     */\n    private void checkConsistency() throws IgniteCheckedException {\n        if (!ctx.config().isDaemon() && !getBoolean(IGNITE_SKIP_CONFIGURATION_CONSISTENCY_CHECK)) {\n            for (ClusterNode n : ctx.discovery().remoteNodes()) {\n                if (n.attribute(ATTR_CONSISTENCY_CHECK_SKIPPED))\n                    continue;\n\n                checkTransactionConfiguration(n);\n\n                DeploymentMode locDepMode = ctx.config().getDeploymentMode();\n                DeploymentMode rmtDepMode = n.attribute(IgniteNodeAttributes.ATTR_DEPLOYMENT_MODE);\n\n                CU.checkAttributeMismatch(\n                    log, null, n.id(), \"deploymentMode\", \"Deployment mode\",\n                    locDepMode, rmtDepMode, true);\n\n                for (DynamicCacheDescriptor desc : registeredCaches.values()) {\n                    CacheConfiguration rmtCfg = desc.remoteConfiguration(n.id());\n\n                    if (rmtCfg != null) {\n                        CacheConfiguration locCfg = desc.cacheConfiguration();\n\n                        checkCache(locCfg, rmtCfg, n);\n\n                        // Check plugin cache configurations.\n                        CachePluginManager pluginMgr = desc.pluginManager();\n\n                        pluginMgr.validateRemotes(rmtCfg, n);\n                    }\n                }\n            }\n        }\n    }",
            " 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892 +\n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  ",
            "    /**\n     *\n     */\n    private void checkConsistency() throws IgniteCheckedException {\n        if (!ctx.config().isDaemon() && !getBoolean(IGNITE_SKIP_CONFIGURATION_CONSISTENCY_CHECK)) {\n            for (ClusterNode n : ctx.discovery().remoteNodes()) {\n                if (Boolean.TRUE.equals(n.attribute(ATTR_CONSISTENCY_CHECK_SKIPPED)))\n                    continue;\n\n                checkTransactionConfiguration(n);\n\n                DeploymentMode locDepMode = ctx.config().getDeploymentMode();\n                DeploymentMode rmtDepMode = n.attribute(IgniteNodeAttributes.ATTR_DEPLOYMENT_MODE);\n\n                CU.checkAttributeMismatch(\n                    log, null, n.id(), \"deploymentMode\", \"Deployment mode\",\n                    locDepMode, rmtDepMode, true);\n\n                for (DynamicCacheDescriptor desc : registeredCaches.values()) {\n                    CacheConfiguration rmtCfg = desc.remoteConfiguration(n.id());\n\n                    if (rmtCfg != null) {\n                        CacheConfiguration locCfg = desc.cacheConfiguration();\n\n                        checkCache(locCfg, rmtCfg, n);\n\n                        // Check plugin cache configurations.\n                        CachePluginManager pluginMgr = desc.pluginManager();\n\n                        pluginMgr.validateRemotes(rmtCfg, n);\n                    }\n                }\n            }\n        }\n    }"
        ],
        [
            "GridCacheProcessor::prepareCacheStop(DynamicCacheChangeRequest)",
            "1894  \n1895  \n1896  \n1897 -\n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  ",
            "    /**\n     * @param req Stop request.\n     */\n    private void prepareCacheStop(DynamicCacheChangeRequest req) {\n        assert req.stop() || req.close() : req;\n\n        GridCacheAdapter<?, ?> cache = caches.remove(maskNull(req.cacheName()));\n\n        if (cache != null) {\n            GridCacheContext<?, ?> ctx = cache.context();\n\n            sharedCtx.removeCacheContext(ctx);\n\n            assert req.deploymentId().equals(ctx.dynamicDeploymentId()) : \"Different deployment IDs [req=\" + req +\n                \", ctxDepId=\" + ctx.dynamicDeploymentId() + ']';\n\n            onKernalStop(cache, req.destroy());\n\n            stopCache(cache, true, req.destroy());\n        }\n    }",
            "1885  \n1886  \n1887  \n1888  \n1889 +\n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905 +\n1906 +\n1907  \n1908 +\n1909 +\n1910  ",
            "    /**\n     * @param req Stop request.\n     * @return Stopped cache context.\n     */\n    private GridCacheContext<?, ?> prepareCacheStop(DynamicCacheChangeRequest req) {\n        assert req.stop() || req.close() : req;\n\n        GridCacheAdapter<?, ?> cache = caches.remove(maskNull(req.cacheName()));\n\n        if (cache != null) {\n            GridCacheContext<?, ?> ctx = cache.context();\n\n            sharedCtx.removeCacheContext(ctx);\n\n            assert req.deploymentId().equals(ctx.dynamicDeploymentId()) : \"Different deployment IDs [req=\" + req +\n                \", ctxDepId=\" + ctx.dynamicDeploymentId() + ']';\n\n            onKernalStop(cache, req.destroy());\n\n            stopCache(cache, true, req.destroy());\n\n            return ctx;\n        }\n\n        return null;\n    }"
        ]
    ],
    "a14a594d2168c7e5bda56bc0cdc2492c5ffc1f82": [
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testRebalancingOnRestartAfterCheckpoint()",
            " 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204 -\n 205 -\n 206 -\n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245 -\n 246 -\n 247  \n 248  \n 249  \n 250 -\n 251 -\n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "    /**\n     * Test that outdated partitions on restarted nodes are correctly replaced with newer versions.\n     *\n     * @throws Exception If fails.\n     */\n    public void testRebalancingOnRestartAfterCheckpoint() throws Exception {\n        IgniteEx ignite0 = startGrid(0);\n\n        IgniteEx ignite1 = startGrid(1);\n\n        IgniteEx ignite2 = startGrid(2);\n        IgniteEx ignite3 = startGrid(3);\n\n        ignite0.cache(null).rebalance().get();\n        ignite1.cache(null).rebalance().get();\n        ignite2.cache(null).rebalance().get();\n        ignite3.cache(null).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite0.cache(null);\n\n        for (int i = 0; i < 1000; i++)\n            cache1.put(i, i);\n\n        ignite0.context().cache().context().database().waitForCheckpoint(\"test\");\n        ignite1.context().cache().context().database().waitForCheckpoint(\"test\");\n\n        info(\"++++++++++ After checkpoint\");\n\n        ignite2.close();\n        ignite3.close();\n\n        awaitPartitionMapExchange();\n\n        ignite0.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        assert cache1.lostPartitions().isEmpty();\n\n        for (int i = 0; i < 1000; i++)\n            cache1.put(i, i * 2);\n\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n\n        info(\">>> Done puts...\");\n\n        ignite2 = startGrid(2);\n        ignite3 = startGrid(3);\n\n        ignite2.cache(null).rebalance().get();\n        ignite3.cache(null).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(null);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(null);\n\n        for (int i = 0; i < 100; i++) {\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache2.get(i));\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache3.get(i));\n        }\n    }",
            " 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 +\n 209 +\n 210 +\n 211 +\n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249 +\n 250 +\n 251  \n 252  \n 253  \n 254 +\n 255 +\n 256  \n 257  \n 258  \n 259  \n 260  \n 261  ",
            "    /**\n     * Test that outdated partitions on restarted nodes are correctly replaced with newer versions.\n     *\n     * @throws Exception If fails.\n     */\n    public void testRebalancingOnRestartAfterCheckpoint() throws Exception {\n        IgniteEx ignite0 = startGrid(0);\n\n        IgniteEx ignite1 = startGrid(1);\n\n        IgniteEx ignite2 = startGrid(2);\n        IgniteEx ignite3 = startGrid(3);\n\n        ignite0.cache(cacheName).rebalance().get();\n        ignite1.cache(cacheName).rebalance().get();\n        ignite2.cache(cacheName).rebalance().get();\n        ignite3.cache(cacheName).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite0.cache(null);\n\n        for (int i = 0; i < 1000; i++)\n            cache1.put(i, i);\n\n        ignite0.context().cache().context().database().waitForCheckpoint(\"test\");\n        ignite1.context().cache().context().database().waitForCheckpoint(\"test\");\n\n        info(\"++++++++++ After checkpoint\");\n\n        ignite2.close();\n        ignite3.close();\n\n        awaitPartitionMapExchange();\n\n        ignite0.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        assert cache1.lostPartitions().isEmpty();\n\n        for (int i = 0; i < 1000; i++)\n            cache1.put(i, i * 2);\n\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n\n        info(\">>> Done puts...\");\n\n        ignite2 = startGrid(2);\n        ignite3 = startGrid(3);\n\n        ignite2.cache(cacheName).rebalance().get();\n        ignite3.cache(cacheName).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(cacheName);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(cacheName);\n\n        for (int i = 0; i < 100; i++) {\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache2.get(i));\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache3.get(i));\n        }\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testDataCorrectnessAfterRestart()",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 -\n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358 -\n 359 -\n 360 -\n 361 -\n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  ",
            "    /**\n     * Test that all data is correctly restored after non-graceful restart.\n     *\n     * @throws Exception If fails.\n     */\n    public void testDataCorrectnessAfterRestart() throws Exception {\n        IgniteEx ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        IgniteEx ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(null);\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        ignite1.close();\n        ignite2.close();\n        ignite3.close();\n        ignite4.close();\n\n        ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        cache1 = ignite1.cache(null);\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(null);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(null);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(null);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }",
            " 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345 +\n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363 +\n 364 +\n 365 +\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  ",
            "    /**\n     * Test that all data is correctly restored after non-graceful restart.\n     *\n     * @throws Exception If fails.\n     */\n    public void testDataCorrectnessAfterRestart() throws Exception {\n        IgniteEx ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        IgniteEx ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(cacheName);\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        ignite1.close();\n        ignite2.close();\n        ignite3.close();\n        ignite4.close();\n\n        ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        cache1 = ignite1.cache(cacheName);\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(cacheName);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(cacheName);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(cacheName);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testPartitionLossAndRecover()",
            " 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384 -\n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398 -\n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 -\n 408 -\n 409 -\n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "    /**\n     * Test that partitions are marked as lost when all owners leave cluster, but recover after nodes rejoin.\n     *\n     * @throws Exception If fails.\n     */\n    public void testPartitionLossAndRecover() throws Exception {\n        Ignite ignite1 = G.start(getConfiguration(\"test1\"));\n        Ignite ignite2 = G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(\"cache\");\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        ignite1.active(false);\n\n        ignite3.close();\n        ignite4.close();\n\n        ignite1.active(true);\n\n        awaitPartitionMapExchange();\n\n        assert !ignite1.cache(\"cache\").lostPartitions().isEmpty();\n\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        ignite1.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(\"cache\");\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(\"cache\");\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(\"cache\");\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }",
            " 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388 +\n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402 +\n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411 +\n 412 +\n 413 +\n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  ",
            "    /**\n     * Test that partitions are marked as lost when all owners leave cluster, but recover after nodes rejoin.\n     *\n     * @throws Exception If fails.\n     */\n    public void testPartitionLossAndRecover() throws Exception {\n        Ignite ignite1 = G.start(getConfiguration(\"test1\"));\n        Ignite ignite2 = G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(cacheName);\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        ignite1.active(false);\n\n        ignite3.close();\n        ignite4.close();\n\n        ignite1.active(true);\n\n        awaitPartitionMapExchange();\n\n        assert !ignite1.cache(cacheName).lostPartitions().isEmpty();\n\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        ignite1.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(cacheName);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(cacheName);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(cacheName);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::getConfiguration(String)",
            "  52  \n  53  \n  54  \n  55  \n  56 -\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration ccfg1 = cacheConfiguration(\"cache\");\n        ccfg1.setBackups(1);\n        ccfg1.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        CacheConfiguration ccfg2 = cacheConfiguration(\"indexed\");\n        ccfg2.setBackups(1);\n        ccfg2.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        QueryEntity qryEntity = new QueryEntity(Integer.class.getName(), TestValue.class.getName());\n\n        LinkedHashMap<String, String> fields = new LinkedHashMap<>();\n\n        fields.put(\"v1\", Integer.class.getName());\n        fields.put(\"v2\", Integer.class.getName());\n\n        qryEntity.setFields(fields);\n\n        QueryIndex qryIdx = new QueryIndex(\"v1\", true);\n\n        qryEntity.setIndexes(Collections.singleton(qryIdx));\n\n        ccfg2.setQueryEntities(Collections.singleton(qryEntity));\n\n        cfg.setCacheConfiguration(ccfg1, ccfg2);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setConcurrencyLevel(Runtime.getRuntime().availableProcessors() * 4);\n        dbCfg.setPageSize(1024);\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setSize(100 * 1024 * 1024);\n        memPlcCfg.setSwapFilePath(\"db\");\n\n        dbCfg.setMemoryPolicies(memPlcCfg);\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        return cfg;\n    }",
            "  56  \n  57  \n  58  \n  59  \n  60 +\n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration ccfg1 = cacheConfiguration(cacheName);\n        ccfg1.setBackups(1);\n        ccfg1.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        CacheConfiguration ccfg2 = cacheConfiguration(\"indexed\");\n        ccfg2.setBackups(1);\n        ccfg2.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        QueryEntity qryEntity = new QueryEntity(Integer.class.getName(), TestValue.class.getName());\n\n        LinkedHashMap<String, String> fields = new LinkedHashMap<>();\n\n        fields.put(\"v1\", Integer.class.getName());\n        fields.put(\"v2\", Integer.class.getName());\n\n        qryEntity.setFields(fields);\n\n        QueryIndex qryIdx = new QueryIndex(\"v1\", true);\n\n        qryEntity.setIndexes(Collections.singleton(qryIdx));\n\n        ccfg2.setQueryEntities(Collections.singleton(qryEntity));\n\n        cfg.setCacheConfiguration(ccfg1, ccfg2);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setConcurrencyLevel(Runtime.getRuntime().availableProcessors() * 4);\n        dbCfg.setPageSize(1024);\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setSize(100 * 1024 * 1024);\n        memPlcCfg.setSwapFilePath(\"db\");\n\n        dbCfg.setMemoryPolicies(memPlcCfg);\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        return cfg;\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testNoRebalancingOnRestartDeactivated()",
            " 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 -\n 274  \n 275  \n 276  \n 277  \n 278  \n 279 -\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293 -\n 294  \n 295  \n 296  \n 297  \n 298  \n 299 -\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316 -\n 317 -\n 318 -\n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  ",
            "    /**\n     * Test that up-to-date partitions aren't rebalanced after cluster restarts gracefully.\n     *\n     * @throws Exception If fails.\n     */\n    public void testNoRebalancingOnRestartDeactivated() throws Exception {\n        fail();\n        IgniteEx ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        IgniteEx ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(null);\n\n        final Collection<Integer> parts = new HashSet<>();\n\n        for (int i = 0; i < 100; i++) {\n            cache1.put(i, i);\n            parts.add(ignite1.affinity(null).partition(i));\n        }\n\n        ignite1.active(false);\n\n        ignite1.close();\n        ignite2.close();\n        ignite3.close();\n        ignite4.close();\n\n        final AtomicInteger evtCnt = new AtomicInteger();\n\n        ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n\n        cache1 = ignite1.cache(null);\n\n        ignite1.active(false);\n\n        ignite1.events().remoteListen(new IgniteBiPredicate<UUID, CacheRebalancingEvent>() {\n            @Override public boolean apply(UUID uuid, CacheRebalancingEvent evt) {\n                if (evt.cacheName() == null && parts.contains(evt.partition()))\n                    evtCnt.incrementAndGet();\n\n                return true;\n            }\n        }, null, EventType.EVT_CACHE_REBALANCE_PART_LOADED);\n\n        ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        ignite1.active(true);\n\n        awaitPartitionMapExchange();\n\n        assert evtCnt.get() == 0 : evtCnt.get();\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(null);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(null);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(null);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }",
            " 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277 +\n 278  \n 279  \n 280  \n 281  \n 282  \n 283 +\n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297 +\n 298  \n 299  \n 300  \n 301  \n 302  \n 303 +\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 +\n 321 +\n 322 +\n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  ",
            "    /**\n     * Test that up-to-date partitions aren't rebalanced after cluster restarts gracefully.\n     *\n     * @throws Exception If fails.\n     */\n    public void testNoRebalancingOnRestartDeactivated() throws Exception {\n        fail();\n        IgniteEx ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        IgniteEx ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(cacheName);\n\n        final Collection<Integer> parts = new HashSet<>();\n\n        for (int i = 0; i < 100; i++) {\n            cache1.put(i, i);\n            parts.add(ignite1.affinity(cacheName).partition(i));\n        }\n\n        ignite1.active(false);\n\n        ignite1.close();\n        ignite2.close();\n        ignite3.close();\n        ignite4.close();\n\n        final AtomicInteger evtCnt = new AtomicInteger();\n\n        ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n\n        cache1 = ignite1.cache(cacheName);\n\n        ignite1.active(false);\n\n        ignite1.events().remoteListen(new IgniteBiPredicate<UUID, CacheRebalancingEvent>() {\n            @Override public boolean apply(UUID uuid, CacheRebalancingEvent evt) {\n                if (Objects.equals(evt.cacheName(), cacheName) && parts.contains(evt.partition()))\n                    evtCnt.incrementAndGet();\n\n                return true;\n            }\n        }, null, EventType.EVT_CACHE_REBALANCE_PART_LOADED);\n\n        ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        ignite1.active(true);\n\n        awaitPartitionMapExchange();\n\n        assert evtCnt.get() == 0 : evtCnt.get();\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(cacheName);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(cacheName);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(cacheName);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testRebalancingOnRestart()",
            " 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 -\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 -\n 186  \n 187  \n 188  \n 189  ",
            "    /**\n     * Test that outdated partitions on restarted nodes are correctly replaced with newer versions.\n     *\n     * @throws Exception If fails.\n     */\n    public void testRebalancingOnRestart() throws Exception {\n        Ignite ignite0 = startGrid(0);\n\n        startGrid(1);\n\n        IgniteEx ignite2 = startGrid(2);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite0.cache(null);\n\n        for (int i = 0; i < 5000; i++)\n            cache1.put(i, i);\n\n        ignite2.close();\n\n        awaitPartitionMapExchange();\n\n        ignite0.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        assert cache1.lostPartitions().isEmpty();\n\n        for (int i = 0; i < 5000; i++)\n            cache1.put(i, i * 2);\n\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n\n        info(\">>> Done puts...\");\n\n        ignite2 = startGrid(2);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache3 = ignite2.cache(null);\n\n        for (int i = 0; i < 100; i++)\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache3.get(i));\n    }",
            " 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190  \n 191  \n 192  \n 193  ",
            "    /**\n     * Test that outdated partitions on restarted nodes are correctly replaced with newer versions.\n     *\n     * @throws Exception If fails.\n     */\n    public void testRebalancingOnRestart() throws Exception {\n        Ignite ignite0 = startGrid(0);\n\n        startGrid(1);\n\n        IgniteEx ignite2 = startGrid(2);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite0.cache(cacheName);\n\n        for (int i = 0; i < 5000; i++)\n            cache1.put(i, i);\n\n        ignite2.close();\n\n        awaitPartitionMapExchange();\n\n        ignite0.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        assert cache1.lostPartitions().isEmpty();\n\n        for (int i = 0; i < 5000; i++)\n            cache1.put(i, i * 2);\n\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n\n        info(\">>> Done puts...\");\n\n        ignite2 = startGrid(2);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache3 = ignite2.cache(cacheName);\n\n        for (int i = 0; i < 100; i++)\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache3.get(i));\n    }"
        ]
    ],
    "1924873b26d0704eb188faa3d6f4e6a4730ac126": [
        [
            "IgniteDbMultiNodePutGetRestartSelfTest::checkPutGetSql(IgniteEx,boolean)",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167 -\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "    /**\n     * @param ig Ig.\n     * @param write Write.\n     */\n    private void checkPutGetSql(IgniteEx ig, boolean write) {\n        IgniteCache<Integer, DbValue> cache = ig.cache(null);\n\n        if (write) {\n            try (IgniteDataStreamer<Object, Object> streamer = ig.dataStreamer(null)) {\n                for (int i = 0; i < 10_000; i++)\n                    streamer.addData(i, new DbValue(i, \"value-\" + i, i));\n            }\n        }\n\n        List<List<?>> res = cache.query(new SqlFieldsQuery(\"select ival from dbvalue where ival < ? order by ival asc\")\n                .setArgs(10_000)).getAll();\n\n        assertEquals(10_000, res.size());\n\n        for (int i = 0; i < 10_000; i++) {\n            assertEquals(1, res.get(i).size());\n            assertEquals(i, res.get(i).get(0));\n        }\n\n        assertEquals(1, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival = 7899\")).getAll().size());\n        assertEquals(5000, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival >= 5000 and ival < 10000\"))\n                .getAll().size());\n\n        for (int i = 0; i < 10_000; i++)\n            assertEquals(new DbValue(i, \"value-\" + i, i), cache.get(i));\n    }",
            " 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170 +\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  ",
            "    /**\n     * @param ig Ig.\n     * @param write Write.\n     */\n    private void checkPutGetSql(IgniteEx ig, boolean write) {\n        IgniteCache<Integer, DbValue> cache = ig.cache(CACHE_NAME);\n\n        if (write) {\n            try (IgniteDataStreamer<Object, Object> streamer = ig.dataStreamer(CACHE_NAME)) {\n                for (int i = 0; i < 10_000; i++)\n                    streamer.addData(i, new DbValue(i, \"value-\" + i, i));\n            }\n        }\n\n        List<List<?>> res = cache.query(new SqlFieldsQuery(\"select ival from dbvalue where ival < ? order by ival asc\")\n                .setArgs(10_000)).getAll();\n\n        assertEquals(10_000, res.size());\n\n        for (int i = 0; i < 10_000; i++) {\n            assertEquals(1, res.get(i).size());\n            assertEquals(i, res.get(i).get(0));\n        }\n\n        assertEquals(1, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival = 7899\")).getAll().size());\n        assertEquals(5000, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival >= 5000 and ival < 10000\"))\n                .getAll().size());\n\n        for (int i = 0; i < 10_000; i++)\n            assertEquals(new DbValue(i, \"value-\" + i, i), cache.get(i));\n    }"
        ],
        [
            "IgnitePersistentStoreWalTlbSelfTest::getConfiguration(String)",
            "  44  \n  45  \n  46  \n  47  \n  48 -\n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration<Integer, Integer> ccfg = new CacheConfiguration<>();\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n        memCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        cfg.setPersistenceConfiguration(\n            new PersistenceConfiguration()\n                .setCheckpointPageBufferSize(DFLT_CHECKPOINT_PAGE_BUFFER_SIZE + 1)\n        );\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        if (gridName.endsWith(\"1\"))\n            cfg.setClientMode(true);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        return cfg;\n    }",
            "  47  \n  48  \n  49  \n  50  \n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration<Integer, Integer> ccfg = new CacheConfiguration<>(CACHE_NAME);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n        memCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        cfg.setPersistenceConfiguration(\n            new PersistenceConfiguration()\n                .setCheckpointPageBufferSize(DFLT_CHECKPOINT_PAGE_BUFFER_SIZE + 1)\n        );\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        if (gridName.endsWith(\"1\"))\n            cfg.setClientMode(true);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        return cfg;\n    }"
        ],
        [
            "IgniteCachePageStoreIntegrationSelfTest::checkPutGetSql(Ignite,boolean)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186 -\n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 -\n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "    /**\n     * @param ig Ignite instance.\n     * @param write Write flag.\n     */\n    private void checkPutGetSql(Ignite ig, boolean write) {\n        IgniteCache<Integer, DbValue> cache = ig.cache(null);\n\n        int entryCnt = 50_000;\n\n        if (write) {\n            try (IgniteDataStreamer<Object, Object> streamer = ig.dataStreamer(null)) {\n                streamer.allowOverwrite(true);\n\n                for (int i = 0; i < entryCnt; i++)\n                    streamer.addData(i, new DbValue(i, \"value-\" + i, i));\n            }\n        }\n\n        for (int i = 0; i < GRID_CNT; i++) {\n            IgniteEx ignite = grid(i);\n\n            GridCacheAdapter<Object, Object> cache0 = ignite.context().cache().internalCache(null);\n\n            for (int k = 0; k < entryCnt; k++)\n                assertNull(cache0.peekEx(i));\n\n            assertEquals(entryCnt, ignite.cache(null).size());\n        }\n\n        for (int i = 0; i < entryCnt; i++)\n            assertEquals(\"i = \" + i, new DbValue(i, \"value-\" + i, i), cache.get(i));\n\n        List<List<?>> res = cache.query(new SqlFieldsQuery(\"select ival from dbvalue where ival < ? order by ival asc\")\n            .setArgs(10_000)).getAll();\n\n        assertEquals(10_000, res.size());\n\n        for (int i = 0; i < 10_000; i++) {\n            assertEquals(1, res.get(i).size());\n            assertEquals(i, res.get(i).get(0));\n        }\n\n        assertEquals(1, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival = 7899\")).getAll().size());\n        assertEquals(5000, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival >= 5000 and ival < 10000\"))\n            .getAll().size());\n\n        for (int i = 0; i < 10_000; i++)\n            assertEquals(new DbValue(i, \"value-\" + i, i), cache.get(i));\n    }",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190  \n 191  \n 192  \n 193  \n 194 +\n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209  \n 210 +\n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "    /**\n     * @param ig Ignite instance.\n     * @param write Write flag.\n     */\n    private void checkPutGetSql(Ignite ig, boolean write) {\n        IgniteCache<Integer, DbValue> cache = ig.cache(CACHE_NAME);\n\n        int entryCnt = 50_000;\n\n        if (write) {\n            try (IgniteDataStreamer<Object, Object> streamer = ig.dataStreamer(CACHE_NAME)) {\n                streamer.allowOverwrite(true);\n\n                for (int i = 0; i < entryCnt; i++)\n                    streamer.addData(i, new DbValue(i, \"value-\" + i, i));\n            }\n        }\n\n        for (int i = 0; i < GRID_CNT; i++) {\n            IgniteEx ignite = grid(i);\n\n            GridCacheAdapter<Object, Object> cache0 = ignite.context().cache().internalCache(CACHE_NAME);\n\n            for (int k = 0; k < entryCnt; k++)\n                assertNull(cache0.peekEx(i));\n\n            assertEquals(entryCnt, ignite.cache(CACHE_NAME).size());\n        }\n\n        for (int i = 0; i < entryCnt; i++)\n            assertEquals(\"i = \" + i, new DbValue(i, \"value-\" + i, i), cache.get(i));\n\n        List<List<?>> res = cache.query(new SqlFieldsQuery(\"select ival from dbvalue where ival < ? order by ival asc\")\n            .setArgs(10_000)).getAll();\n\n        assertEquals(10_000, res.size());\n\n        for (int i = 0; i < 10_000; i++) {\n            assertEquals(1, res.get(i).size());\n            assertEquals(i, res.get(i).get(0));\n        }\n\n        assertEquals(1, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival = 7899\")).getAll().size());\n        assertEquals(5000, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival >= 5000 and ival < 10000\"))\n            .getAll().size());\n\n        for (int i = 0; i < 10_000; i++)\n            assertEquals(new DbValue(i, \"value-\" + i, i), cache.get(i));\n    }"
        ],
        [
            "RebalancingOnNotStableTopologyTest::test()",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 -\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "    /**\n     * @throws Exception When fails.\n     */\n    public void test() throws Exception {\n        stopAllGrids();\n\n        Ignite ex = startGrid(0);\n\n        startGrid(1);\n\n        final CountDownLatch startLatch = new CountDownLatch(1);\n        final CountDownLatch doneLatch = new CountDownLatch(1);\n\n        final Ignite ex1 = ex;\n\n        final AtomicBoolean stop = new AtomicBoolean();\n        final AtomicInteger keyCnt = new AtomicInteger();\n\n        Thread thread = new Thread(new Runnable() {\n            @Override public void run() {\n                ex1.active(true);\n\n                try {\n                    checkTopology(2);\n\n                    startLatch.countDown();\n\n                    IgniteCache<Object, Object> cache1 = ex1.cache(\"cache1\");\n\n                    int key = keyCnt.get();\n\n                    while (!stop.get()) {\n                        if (key > 0 && (key % 500 == 0)) {\n                            U.sleep(5);\n\n                            System.out.println(\"key = \" + key);\n                        }\n\n                        cache1.put(key, -key);\n\n                        key = keyCnt.incrementAndGet();\n                    }\n                }\n                catch (Throwable th) {\n                    th.printStackTrace();\n                }\n\n                doneLatch.countDown();\n            }\n        });\n\n        thread.setName(\"Data-Loader\");\n        thread.start();\n\n        startLatch.await(60, TimeUnit.SECONDS);\n\n        for (int i = 2; i < CLUSTER_SIZE; i++) {\n            startGrid(i);\n\n            U.sleep(5000);\n        }\n\n        U.sleep(10000);\n\n        IgniteProcessProxy.kill(\"db.RebalancingOnNotStableTopologyTest2\");\n\n        Thread.sleep(5000);\n\n        IgniteProcessProxy.kill(\"db.RebalancingOnNotStableTopologyTest1\");\n\n        assert doneLatch.getCount() > 0;\n\n        stop.set(true);\n\n        doneLatch.await(600, TimeUnit.SECONDS);\n\n        IgniteProcessProxy.killAll();\n\n        stopAllGrids();\n\n        //start cluster. it will cause memory restoration and reading WAL.\n        ex = startGrids(CLUSTER_SIZE);\n\n        ex.active(true);\n\n        checkTopology(CLUSTER_SIZE);\n\n        IgniteCache<Object, Object> cache1 = ex.cache(\"cache1\");\n\n        assert keyCnt.get() > 0;\n\n        for (int i = 0; i < keyCnt.get(); i++)\n            assertEquals(-i, cache1.get(i));\n\n        System.out.println(\"Test finished with total keys count = \" + keyCnt.get());\n    }",
            "  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "    /**\n     * @throws Exception When fails.\n     */\n    public void test() throws Exception {\n        stopAllGrids();\n\n        Ignite ex = startGrid(0);\n\n        startGrid(1);\n\n        final CountDownLatch startLatch = new CountDownLatch(1);\n        final CountDownLatch doneLatch = new CountDownLatch(1);\n\n        final Ignite ex1 = ex;\n\n        final AtomicBoolean stop = new AtomicBoolean();\n        final AtomicInteger keyCnt = new AtomicInteger();\n\n        Thread thread = new Thread(new Runnable() {\n            @Override public void run() {\n                ex1.active(true);\n\n                try {\n                    checkTopology(2);\n\n                    startLatch.countDown();\n\n                    IgniteCache<Object, Object> cache1 = ex1.cache(CACHE_NAME);\n\n                    int key = keyCnt.get();\n\n                    while (!stop.get()) {\n                        if (key > 0 && (key % 500 == 0)) {\n                            U.sleep(5);\n\n                            System.out.println(\"key = \" + key);\n                        }\n\n                        cache1.put(key, -key);\n\n                        key = keyCnt.incrementAndGet();\n                    }\n                }\n                catch (Throwable th) {\n                    th.printStackTrace();\n                }\n\n                doneLatch.countDown();\n            }\n        });\n\n        thread.setName(\"Data-Loader\");\n        thread.start();\n\n        startLatch.await(60, TimeUnit.SECONDS);\n\n        for (int i = 2; i < CLUSTER_SIZE; i++) {\n            startGrid(i);\n\n            U.sleep(5000);\n        }\n\n        U.sleep(10000);\n\n        IgniteProcessProxy.kill(\"db.RebalancingOnNotStableTopologyTest2\");\n\n        Thread.sleep(5000);\n\n        IgniteProcessProxy.kill(\"db.RebalancingOnNotStableTopologyTest1\");\n\n        assert doneLatch.getCount() > 0;\n\n        stop.set(true);\n\n        doneLatch.await(600, TimeUnit.SECONDS);\n\n        IgniteProcessProxy.killAll();\n\n        stopAllGrids();\n\n        //start cluster. it will cause memory restoration and reading WAL.\n        ex = startGrids(CLUSTER_SIZE);\n\n        ex.active(true);\n\n        checkTopology(CLUSTER_SIZE);\n\n        IgniteCache<Object, Object> cache1 = ex.cache(CACHE_NAME);\n\n        assert keyCnt.get() > 0;\n\n        for (int i = 0; i < keyCnt.get(); i++)\n            assertEquals(-i, cache1.get(i));\n\n        System.out.println(\"Test finished with total keys count = \" + keyCnt.get());\n    }"
        ],
        [
            "WalRecoveryTxLogicalRecordsTest::testRebalanceIterator()",
            " 296  \n 297  \n 298  \n 299  \n 300 -\n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 -\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testRebalanceIterator() throws Exception {\n        extraCcfg = new CacheConfiguration(CACHE_NAME + \"2\");\n        extraCcfg.setAffinity(new RendezvousAffinityFunction(false, PARTS));\n\n        Ignite ignite = startGrid();\n\n        try {\n            GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)((IgniteEx)ignite).context()\n                .cache().context().database();\n\n            dbMgr.waitForCheckpoint(\"test\");\n\n            // This number depends on wal history size.\n            int entries = 25;\n\n            IgniteCache<Integer, Integer> cache = ignite.cache(CACHE_NAME);\n            IgniteCache<Integer, Integer> cache2 = ignite.cache(CACHE_NAME + \"2\");\n\n            for (int i = 0; i < entries; i++) {\n                // Put to partition 0.\n                cache.put(i * PARTS, i * PARTS);\n\n                // Put to partition 1.\n                cache.put(i * PARTS + 1, i * PARTS + 1);\n\n                // Put to another cache.\n                cache2.put(i, i);\n\n                dbMgr.waitForCheckpoint(\"test\");\n            }\n\n            for (int i = 0; i < entries; i++) {\n                assertEquals((Integer)(i * PARTS), cache.get(i * PARTS));\n                assertEquals((Integer)(i * PARTS + 1), cache.get(i * PARTS + 1));\n                assertEquals((Integer)(i), cache2.get(i));\n            }\n\n            GridCacheContext<Object, Object> cctx = ((IgniteEx)ignite).context().cache().cache(CACHE_NAME).context();\n            IgniteCacheOffheapManager offh = cctx.offheap();\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            for (int i = 0; i < entries; i++) {\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(0, topVer, (long)i)) {\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    assertNotNull(it);\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(\"i=\" + i + \", j=\" + j, it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(1, topVer, (long)i)) {\n                    assertNotNull(it);\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS + 1, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS + 1, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n            }\n\n            stopAllGrids();\n\n            // Check that iterator is valid after restart.\n            ignite = startGrid();\n\n            cctx = ((IgniteEx)ignite).context().cache().cache(CACHE_NAME).context();\n            offh = cctx.offheap();\n            topVer = cctx.affinity().affinityTopologyVersion();\n\n            for (int i = 0; i < entries; i++) {\n                long start = System.currentTimeMillis();\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(0, topVer, (long)i)) {\n                    long end = System.currentTimeMillis();\n\n                    info(\"Time to get iterator: \" + (end - start));\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    assertNotNull(it);\n\n                    start = System.currentTimeMillis();\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(\"i=\" + i + \", j=\" + j, it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    end = System.currentTimeMillis();\n\n                    info(\"Time to iterate: \" + (end - start));\n\n                    assertFalse(it.hasNext());\n                }\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(1, topVer, (long)i)) {\n                    assertNotNull(it);\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS + 1, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS + 1, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n            }\n        }\n        finally {\n            stopAllGrids();\n        }\n    }",
            " 299  \n 300  \n 301  \n 302  \n 303 +\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318 +\n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testRebalanceIterator() throws Exception {\n        extraCcfg = new CacheConfiguration(CACHE2_NAME);\n        extraCcfg.setAffinity(new RendezvousAffinityFunction(false, PARTS));\n\n        Ignite ignite = startGrid();\n\n        try {\n            GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)((IgniteEx)ignite).context()\n                .cache().context().database();\n\n            dbMgr.waitForCheckpoint(\"test\");\n\n            // This number depends on wal history size.\n            int entries = 25;\n\n            IgniteCache<Integer, Integer> cache = ignite.cache(CACHE_NAME);\n            IgniteCache<Integer, Integer> cache2 = ignite.cache(CACHE2_NAME);\n\n            for (int i = 0; i < entries; i++) {\n                // Put to partition 0.\n                cache.put(i * PARTS, i * PARTS);\n\n                // Put to partition 1.\n                cache.put(i * PARTS + 1, i * PARTS + 1);\n\n                // Put to another cache.\n                cache2.put(i, i);\n\n                dbMgr.waitForCheckpoint(\"test\");\n            }\n\n            for (int i = 0; i < entries; i++) {\n                assertEquals((Integer)(i * PARTS), cache.get(i * PARTS));\n                assertEquals((Integer)(i * PARTS + 1), cache.get(i * PARTS + 1));\n                assertEquals((Integer)(i), cache2.get(i));\n            }\n\n            GridCacheContext<Object, Object> cctx = ((IgniteEx)ignite).context().cache().cache(CACHE_NAME).context();\n            IgniteCacheOffheapManager offh = cctx.offheap();\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            for (int i = 0; i < entries; i++) {\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(0, topVer, (long)i)) {\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    assertNotNull(it);\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(\"i=\" + i + \", j=\" + j, it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(1, topVer, (long)i)) {\n                    assertNotNull(it);\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS + 1, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS + 1, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n            }\n\n            stopAllGrids();\n\n            // Check that iterator is valid after restart.\n            ignite = startGrid();\n\n            cctx = ((IgniteEx)ignite).context().cache().cache(CACHE_NAME).context();\n            offh = cctx.offheap();\n            topVer = cctx.affinity().affinityTopologyVersion();\n\n            for (int i = 0; i < entries; i++) {\n                long start = System.currentTimeMillis();\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(0, topVer, (long)i)) {\n                    long end = System.currentTimeMillis();\n\n                    info(\"Time to get iterator: \" + (end - start));\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    assertNotNull(it);\n\n                    start = System.currentTimeMillis();\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(\"i=\" + i + \", j=\" + j, it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    end = System.currentTimeMillis();\n\n                    info(\"Time to iterate: \" + (end - start));\n\n                    assertFalse(it.hasNext());\n                }\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(1, topVer, (long)i)) {\n                    assertNotNull(it);\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS + 1, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS + 1, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n            }\n        }\n        finally {\n            stopAllGrids();\n        }\n    }"
        ],
        [
            "IgniteNoActualWalHistorySelfTest::getConfiguration(String)",
            "  45  \n  46  \n  47  \n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration<Integer, IndexedObject> ccfg = new CacheConfiguration<>();\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.ATOMIC);\n        ccfg.setRebalanceMode(CacheRebalanceMode.SYNC);\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setPageSize(4 * 1024);\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        PersistenceConfiguration pCfg = new PersistenceConfiguration();\n\n        pCfg.setWalSegmentSize(4 * 1024 * 1024);\n        pCfg.setWalHistorySize(2);\n        pCfg.setWalSegments(10);\n\n        cfg.setPersistenceConfiguration(pCfg);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration binCfg = new BinaryConfiguration();\n\n        binCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(binCfg);\n\n        return cfg;\n    }",
            "  48  \n  49  \n  50  \n  51  \n  52 +\n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration<Integer, IndexedObject> ccfg = new CacheConfiguration<>(CACHE_NAME);\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.ATOMIC);\n        ccfg.setRebalanceMode(CacheRebalanceMode.SYNC);\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setPageSize(4 * 1024);\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        PersistenceConfiguration pCfg = new PersistenceConfiguration();\n\n        pCfg.setWalSegmentSize(4 * 1024 * 1024);\n        pCfg.setWalHistorySize(2);\n        pCfg.setWalSegments(10);\n\n        cfg.setPersistenceConfiguration(pCfg);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration binCfg = new BinaryConfiguration();\n\n        binCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(binCfg);\n\n        return cfg;\n    }"
        ],
        [
            "IgnitePersistentStoreWalTlbSelfTest::testWalDirectOutOfMemory()",
            " 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testWalDirectOutOfMemory() throws Exception {\n        IgniteEx ig = grid(1);\n\n        boolean locked = true;\n\n        try {\n            IgniteDataStreamer<Integer, Integer> streamer = ig.dataStreamer(null);\n\n            for (int i = 0; i < 100_000; i++) {\n                streamer.addData(i, 1);\n\n                if (i > 0 && i % 10_000 == 0)\n                    info(\"Done put: \" + i);\n            }\n        }\n        catch (CacheException ignore) {\n            // expected\n            locked = false;\n        }\n        finally {\n            assertFalse(locked);\n\n            stopAllGrids();\n        }\n    }",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testWalDirectOutOfMemory() throws Exception {\n        IgniteEx ig = grid(1);\n\n        boolean locked = true;\n\n        try {\n            IgniteDataStreamer<Integer, Integer> streamer = ig.dataStreamer(CACHE_NAME);\n\n            for (int i = 0; i < 100_000; i++) {\n                streamer.addData(i, 1);\n\n                if (i > 0 && i % 10_000 == 0)\n                    info(\"Done put: \" + i);\n            }\n        }\n        catch (CacheException ignore) {\n            // expected\n            locked = false;\n        }\n        finally {\n            assertFalse(locked);\n\n            stopAllGrids();\n        }\n    }"
        ],
        [
            "IgniteDbMultiNodePutGetRestartSelfTest::getConfiguration(String)",
            "  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n        dbCfg.setMemoryPolicies(memPlcCfg);\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        CacheConfiguration ccfg = new CacheConfiguration();\n\n        ccfg.setIndexedTypes(Integer.class, DbValue.class);\n\n        ccfg.setRebalanceMode(CacheRebalanceMode.NONE);\n\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration bCfg = new BinaryConfiguration();\n\n        bCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(bCfg);\n\n        return cfg;\n    }",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78 +\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n        dbCfg.setMemoryPolicies(memPlcCfg);\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        CacheConfiguration ccfg = new CacheConfiguration(CACHE_NAME);\n\n        ccfg.setIndexedTypes(Integer.class, DbValue.class);\n\n        ccfg.setRebalanceMode(CacheRebalanceMode.NONE);\n\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration bCfg = new BinaryConfiguration();\n\n        bCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(bCfg);\n\n        return cfg;\n    }"
        ],
        [
            "RebalancingOnNotStableTopologyTest::getConfiguration(String)",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 -\n 156  \n 157 -\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        cfg.setActiveOnStart(false);\n\n        CacheConfiguration<Integer, Integer> ccfg = new CacheConfiguration<>();\n\n        ccfg.setName(\"cache1\");\n        ccfg.setPartitionLossPolicy(PartitionLossPolicy.READ_ONLY_SAFE);\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n        ccfg.setCacheMode(CacheMode.PARTITIONED);\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n        ccfg.setBackups(2);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        PersistenceConfiguration pCfg = new PersistenceConfiguration();\n\n        pCfg.setCheckpointFrequency(CHECKPOINT_FREQUENCY);\n\n        cfg.setPersistenceConfiguration(pCfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(200 * 1024 * 1024);\n        memPlcCfg.setMaxSize(200 * 1024 * 1024);\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n        memCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        return cfg;\n    }",
            " 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        cfg.setActiveOnStart(false);\n\n        CacheConfiguration<Integer, Integer> ccfg = new CacheConfiguration<>(CACHE_NAME);\n\n        ccfg.setPartitionLossPolicy(PartitionLossPolicy.READ_ONLY_SAFE);\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n        ccfg.setCacheMode(CacheMode.PARTITIONED);\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n        ccfg.setBackups(2);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        PersistenceConfiguration pCfg = new PersistenceConfiguration();\n\n        pCfg.setCheckpointFrequency(CHECKPOINT_FREQUENCY);\n\n        cfg.setPersistenceConfiguration(pCfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(200 * 1024 * 1024);\n        memPlcCfg.setMaxSize(200 * 1024 * 1024);\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n        memCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        return cfg;\n    }"
        ],
        [
            "IgniteCachePageStoreIntegrationSelfTest::getConfiguration(String)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 -\n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setConcurrencyLevel(Runtime.getRuntime().availableProcessors() * 4);\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        dbCfg.setMemoryPolicies(memPlcCfg);\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        CacheConfiguration ccfg = new CacheConfiguration();\n\n        ccfg.setIndexedTypes(Integer.class, DbValue.class);\n\n        ccfg.setRebalanceMode(CacheRebalanceMode.NONE);\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL);\n\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        cfg.setCacheConfiguration(ccfg);\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration bCfg = new BinaryConfiguration();\n\n        bCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(bCfg);\n\n        return cfg;\n    }",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setConcurrencyLevel(Runtime.getRuntime().availableProcessors() * 4);\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        dbCfg.setMemoryPolicies(memPlcCfg);\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        CacheConfiguration ccfg = new CacheConfiguration(CACHE_NAME);\n\n        ccfg.setIndexedTypes(Integer.class, DbValue.class);\n\n        ccfg.setRebalanceMode(CacheRebalanceMode.NONE);\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL);\n\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        cfg.setCacheConfiguration(ccfg);\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration bCfg = new BinaryConfiguration();\n\n        bCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(bCfg);\n\n        return cfg;\n    }"
        ],
        [
            "IgniteDbPageEvictionSelfTest::testPageEvictionSql()",
            " 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testPageEvictionSql() throws Exception {\n        IgniteEx ig = grid(0);\n\n        try (IgniteDataStreamer<DbKey, DbValue> streamer = ig.dataStreamer(null)) {\n            for (int i = 0; i < ENTRY_CNT; i++) {\n                streamer.addData(new DbKey(i), new DbValue(i, \"value-\" + i, Long.MAX_VALUE - i));\n\n                if (i > 0 && i % 10_000 == 0)\n                    info(\"Done put: \" + i);\n            }\n        }\n\n        IgniteCache<DbKey, DbValue> cache = ignite(0).cache(null);\n\n        for (int i = 0; i < ENTRY_CNT; i++) {\n            assertEquals(Long.MAX_VALUE - i, cache.get(new DbKey(i)).lVal);\n\n            if (i > 0 && i % 10_000 == 0)\n                info(\"Done get: \" + i);\n        }\n\n        for (int i = 0; i < ENTRY_CNT; i++) {\n            List<List<?>> rows = cache.query(new SqlFieldsQuery(\"select lVal from DbValue where iVal=?\").setArgs(i))\n                .getAll();\n\n            assertEquals(1, rows.size());\n            assertEquals(Long.MAX_VALUE - i, rows.get(0).get(0));\n\n            if (i > 0 && i % 10_000 == 0)\n                info(\"Done SQL query: \" + i);\n        }\n    }",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testPageEvictionSql() throws Exception {\n        IgniteEx ig = grid(0);\n\n        try (IgniteDataStreamer<DbKey, DbValue> streamer = ig.dataStreamer(CACHE_NAME)) {\n            for (int i = 0; i < ENTRY_CNT; i++) {\n                streamer.addData(new DbKey(i), new DbValue(i, \"value-\" + i, Long.MAX_VALUE - i));\n\n                if (i > 0 && i % 10_000 == 0)\n                    info(\"Done put: \" + i);\n            }\n        }\n\n        IgniteCache<DbKey, DbValue> cache = ignite(0).cache(CACHE_NAME);\n\n        for (int i = 0; i < ENTRY_CNT; i++) {\n            assertEquals(Long.MAX_VALUE - i, cache.get(new DbKey(i)).lVal);\n\n            if (i > 0 && i % 10_000 == 0)\n                info(\"Done get: \" + i);\n        }\n\n        for (int i = 0; i < ENTRY_CNT; i++) {\n            List<List<?>> rows = cache.query(new SqlFieldsQuery(\"select lVal from DbValue where iVal=?\").setArgs(i))\n                .getAll();\n\n            assertEquals(1, rows.size());\n            assertEquals(Long.MAX_VALUE - i, rows.get(0).get(0));\n\n            if (i > 0 && i % 10_000 == 0)\n                info(\"Done SQL query: \" + i);\n        }\n    }"
        ],
        [
            "IgniteNoActualWalHistorySelfTest::testWalBig()",
            " 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 -\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testWalBig() throws Exception {\n        try {\n            IgniteEx ignite = startGrid(1);\n\n            IgniteCache<Object, Object> cache = ignite.cache(null);\n\n            Random rnd = new Random();\n\n            Map<Integer, IndexedObject> map = new HashMap<>();\n\n            for (int i = 0; i < 40_000; i++) {\n                if (i % 1000 == 0)\n                    X.println(\" >> \" + i);\n\n                int k = rnd.nextInt(300_000);\n                IndexedObject v = new IndexedObject(rnd.nextInt(10_000));\n\n                cache.put(k, v);\n                map.put(k, v);\n            }\n\n            GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)ignite.context().cache().context()\n                .database();\n\n            // Create many checkpoints to clean up the history.\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n\n            dbMgr.enableCheckpoints(false).get();\n\n            for (int i = 0; i < 50; i++) {\n                int k = rnd.nextInt(300_000);\n                IndexedObject v = new IndexedObject(rnd.nextInt(10_000));\n\n                cache.put(k, v);\n                map.put(k, v);\n            }\n\n            stopGrid(1);\n\n            ignite = startGrid(1);\n\n            cache = ignite.cache(null);\n\n            // Check.\n            for (Integer k : map.keySet())\n                assertEquals(map.get(k), cache.get(k));\n        }\n        finally {\n            stopAllGrids();\n        }\n    }",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testWalBig() throws Exception {\n        try {\n            IgniteEx ignite = startGrid(1);\n\n            IgniteCache<Object, Object> cache = ignite.cache(CACHE_NAME);\n\n            Random rnd = new Random();\n\n            Map<Integer, IndexedObject> map = new HashMap<>();\n\n            for (int i = 0; i < 40_000; i++) {\n                if (i % 1000 == 0)\n                    X.println(\" >> \" + i);\n\n                int k = rnd.nextInt(300_000);\n                IndexedObject v = new IndexedObject(rnd.nextInt(10_000));\n\n                cache.put(k, v);\n                map.put(k, v);\n            }\n\n            GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)ignite.context().cache().context()\n                .database();\n\n            // Create many checkpoints to clean up the history.\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n\n            dbMgr.enableCheckpoints(false).get();\n\n            for (int i = 0; i < 50; i++) {\n                int k = rnd.nextInt(300_000);\n                IndexedObject v = new IndexedObject(rnd.nextInt(10_000));\n\n                cache.put(k, v);\n                map.put(k, v);\n            }\n\n            stopGrid(1);\n\n            ignite = startGrid(1);\n\n            cache = ignite.cache(CACHE_NAME);\n\n            // Check.\n            for (Integer k : map.keySet())\n                assertEquals(map.get(k), cache.get(k));\n        }\n        finally {\n            stopAllGrids();\n        }\n    }"
        ],
        [
            "DbPageEvictionDuringPartitionClearSelfTest::getConfiguration(String)",
            "  47  \n  48  \n  49  \n  50  \n  51 -\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration ccfg = new CacheConfiguration(\"cache\")\n            .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)\n            .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)\n            .setAffinity(new RendezvousAffinityFunction(false, 128))\n            .setRebalanceMode(CacheRebalanceMode.SYNC)\n            .setBackups(1);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        // Intentionally set small page cache size.\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setInitialSize(70 * 1024 * 1024);\n        memPlcCfg.setMaxSize(70 * 1024 * 1024);\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n\n        memCfg.setDefaultMemoryPolicyName(memPlcCfg.getName());\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        return cfg;\n    }",
            "  47  \n  48  \n  49  \n  50  \n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration ccfg = new CacheConfiguration(CACHE_NAME)\n            .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)\n            .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)\n            .setAffinity(new RendezvousAffinityFunction(false, 128))\n            .setRebalanceMode(CacheRebalanceMode.SYNC)\n            .setBackups(1);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        // Intentionally set small page cache size.\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setInitialSize(70 * 1024 * 1024);\n        memPlcCfg.setMaxSize(70 * 1024 * 1024);\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n\n        memCfg.setDefaultMemoryPolicyName(memPlcCfg.getName());\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        return cfg;\n    }"
        ],
        [
            "IgniteCachePageStoreIntegrationSelfTest::testPutMultithreaded()",
            " 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testPutMultithreaded() throws Exception {\n        startGrids(4);\n\n        try {\n            final IgniteEx grid = grid(0);\n\n            GridTestUtils.runMultiThreaded(new Callable<Object>() {\n                @Override public Object call() throws Exception {\n                    for (int i = 0; i < 1000; i++)\n                        grid.cache(null).put(i, i);\n\n                    return null;\n                }\n            }, 8, \"updater\");\n        }\n        finally {\n            stopAllGrids();\n        }\n    }",
            " 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testPutMultithreaded() throws Exception {\n        startGrids(4);\n\n        try {\n            final IgniteEx grid = grid(0);\n\n            GridTestUtils.runMultiThreaded(new Callable<Object>() {\n                @Override public Object call() throws Exception {\n                    for (int i = 0; i < 1000; i++)\n                        grid.cache(CACHE_NAME).put(i, i);\n\n                    return null;\n                }\n            }, 8, \"updater\");\n        }\n        finally {\n            stopAllGrids();\n        }\n    }"
        ]
    ],
    "ab7d7955b5f6ffe170ca5588c6c45d390c84df08": [
        [
            "PersistentStoreExample::main(String)",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  ",
            "    /**\n     * @param args Program arguments, ignored.\n     * @throws Exception If failed.\n     */\n    public static void main(String[] args) throws Exception {\n        Ignition.setClientMode(true);\n\n        try (Ignite ig = Ignition.start(\"examples/config/persistentstore/example-persistent-store.xml\")) {\n\n            IgniteCache<Long, Organization> cache = ig.cache(\"organization\");\n\n            if (UPDATE) {\n                System.out.println(\"Populating the cache...\");\n\n                try (IgniteDataStreamer<Long, Organization> streamer = ig.dataStreamer(\"organization\")) {\n                    streamer.allowOverwrite(true);\n\n                    for (long i = 0; i < 100_000; i++) {\n                        streamer.addData(i, new Organization(i, \"organization-\" + i));\n\n                        if (i > 0 && i % 10_000 == 0)\n                            System.out.println(\"Done: \" + i);\n                    }\n                }\n            }\n\n            // Run SQL without explicitly calling to loadCache().\n            QueryCursor<List<?>> cur = cache.query(\n                    new SqlFieldsQuery(\"select id, name from Organization where name like ?\")\n                            .setArgs(\"organization-54321\"));\n\n            System.out.println(\"SQL Result: \" + cur.getAll());\n\n            // Run get() without explicitly calling to loadCache().\n            Organization org = cache.get(54321l);\n\n            System.out.println(\"GET Result: \" + org);\n        }\n    }",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 +\n  59 +\n  60 +\n  61 +\n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "    /**\n     * @param args Program arguments, ignored.\n     * @throws Exception If failed.\n     */\n    public static void main(String[] args) throws Exception {\n        Ignition.setClientMode(true);\n\n        try (Ignite ig = Ignition.start(\"examples/config/persistentstore/example-persistent-store.xml\")) {\n\n            // Activate the cluster. Required to do if the persistent store is enabled because you might need\n            // to wait while all the nodes, that store a subset of data on disk, join the cluster.\n            ig.active(true);\n\n            IgniteCache<Long, Organization> cache = ig.cache(\"organization\");\n\n            if (UPDATE) {\n                System.out.println(\"Populating the cache...\");\n\n                try (IgniteDataStreamer<Long, Organization> streamer = ig.dataStreamer(\"organization\")) {\n                    streamer.allowOverwrite(true);\n\n                    for (long i = 0; i < 100_000; i++) {\n                        streamer.addData(i, new Organization(i, \"organization-\" + i));\n\n                        if (i > 0 && i % 10_000 == 0)\n                            System.out.println(\"Done: \" + i);\n                    }\n                }\n            }\n\n            // Run SQL without explicitly calling to loadCache().\n            QueryCursor<List<?>> cur = cache.query(\n                    new SqlFieldsQuery(\"select id, name from Organization where name like ?\")\n                            .setArgs(\"organization-54321\"));\n\n            System.out.println(\"SQL Result: \" + cur.getAll());\n\n            // Run get() without explicitly calling to loadCache().\n            Organization org = cache.get(54321l);\n\n            System.out.println(\"GET Result: \" + org);\n        }\n    }"
        ],
        [
            "IgniteKernal::checkClusterState()",
            "3659  \n3660  \n3661  \n3662  \n3663  \n3664  \n3665  \n3666 -\n3667  ",
            "    /**\n     * Validate operation on cluster. Check current cluster state.\n     *\n     * @throws IgniteException if cluster in inActive state\n     */\n    private void checkClusterState() throws IgniteException {\n        if (!ctx.state().active())\n            throw new IgniteException(\"can not perform operation, because cluster inactive\");\n    }",
            "3659  \n3660  \n3661  \n3662  \n3663  \n3664  \n3665  \n3666 +\n3667 +\n3668 +\n3669  ",
            "    /**\n     * Validate operation on cluster. Check current cluster state.\n     *\n     * @throws IgniteException if cluster in inActive state\n     */\n    private void checkClusterState() throws IgniteException {\n        if (!ctx.state().active())\n            throw new IgniteException(\"Can not perform the operation because the cluster is inactive. Note, that \" +\n                \"the cluster is considered inactive by default if Ignite Persistent Store is used to let all the nodes \" +\n                \"join the cluster. To activate the cluster call Ignite.activate(true).\");\n    }"
        ]
    ],
    "32ff1666fe2b5f141d8c93f6aeffc195f6bd80a1": [
        [
            "CacheStopAndDestroySelfTest::testClientClose()",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472 -\n 473 -\n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  ",
            "    /**\n     * Test Client close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testClientClose() throws Exception {\n        fail(\"https://issues.apache.org/jira/browse/IGNITE-5511\");\n\n        startGridsMultiThreaded(gridCount());\n\n        IgniteCache<String, String> cache0 = grid(0).getOrCreateCache(getClientConfig());\n\n        assert cache0.get(KEY_VAL) == null;\n\n        cache0.put(KEY_VAL, KEY_VAL);\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL);\n\n        // DHT Close from client node. Should affect only client node.\n\n        IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_CLIENT);\n        IgniteCache<String, String> cache2 = grid(2).cache(CACHE_NAME_CLIENT);\n\n        assert cache2.get(KEY_VAL).equals(KEY_VAL);\n\n        cache2.close();// Client node.\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL);// Not affected.\n        assert cache1.get(KEY_VAL).equals(KEY_VAL);// Not affected.\n\n        try {\n            cache2.get(KEY_VAL);// Affected.\n\n            assert false;\n        }\n        catch (IllegalStateException ignored) {\n            // No-op\n        }\n\n        // DHT Creation from client node after closed.\n        IgniteCache<String, String> cache2New = grid(2).cache(CACHE_NAME_CLIENT);\n\n        assertNotSame(cache2, cache2New);\n\n        assert cache2New.get(KEY_VAL).equals(KEY_VAL);\n\n        cache0.put(KEY_VAL, KEY_VAL + \"recreated\");\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache2New.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n    }",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  ",
            "    /**\n     * Test Client close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testClientClose() throws Exception {\n        startGridsMultiThreaded(gridCount());\n\n        IgniteCache<String, String> cache0 = grid(0).getOrCreateCache(getClientConfig());\n\n        assert cache0.get(KEY_VAL) == null;\n\n        cache0.put(KEY_VAL, KEY_VAL);\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL);\n\n        // DHT Close from client node. Should affect only client node.\n\n        IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_CLIENT);\n        IgniteCache<String, String> cache2 = grid(2).cache(CACHE_NAME_CLIENT);\n\n        assert cache2.get(KEY_VAL).equals(KEY_VAL);\n\n        cache2.close();// Client node.\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL);// Not affected.\n        assert cache1.get(KEY_VAL).equals(KEY_VAL);// Not affected.\n\n        try {\n            cache2.get(KEY_VAL);// Affected.\n\n            assert false;\n        }\n        catch (IllegalStateException ignored) {\n            // No-op\n        }\n\n        // DHT Creation from client node after closed.\n        IgniteCache<String, String> cache2New = grid(2).cache(CACHE_NAME_CLIENT);\n\n        assertNotSame(cache2, cache2New);\n\n        assert cache2New.get(KEY_VAL).equals(KEY_VAL);\n\n        cache0.put(KEY_VAL, KEY_VAL + \"recreated\");\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache2New.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n    }"
        ],
        [
            "CacheStopAndDestroySelfTest::testNearClose()",
            " 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566 -\n 567 -\n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  ",
            "    /**\n     * Test Near close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testNearClose() throws Exception {\n        fail(\"https://issues.apache.org/jira/browse/IGNITE-5511\");\n\n        startGridsMultiThreaded(gridCount());\n\n        IgniteCache<String, String> cache0 = grid(0).getOrCreateCache(getNearConfig());\n\n        // GridDhtTxPrepareRequest requests to Client node will be counted.\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.nodeFilter = grid(2).context().localNodeId();\n\n        // Near Close from client node.\n\n        IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_NEAR);\n        IgniteCache<String, String> cache2 = grid(2).createNearCache(CACHE_NAME_NEAR, new NearCacheConfiguration());\n\n        assert cache2.get(KEY_VAL) == null;\n\n        // Subscribing to events.\n        cache2.put(KEY_VAL, KEY_VAL);\n\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.set(0);\n\n        cache0.put(KEY_VAL, \"near-test\");\n\n        U.sleep(1000);\n\n        //Ensure near cache was automatically updated.\n        assert CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.get() != 0;\n\n        assert cache2.localPeek(KEY_VAL).equals(\"near-test\");\n\n        cache2.close();\n\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.set(0);\n\n        // Should not produce messages to client node.\n        cache0.put(KEY_VAL, KEY_VAL + 0);\n\n        U.sleep(1000);\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + 0);// Not affected.\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + 0);// Not affected.\n\n        try {\n            cache2.get(KEY_VAL);// Affected.\n\n            assert false;\n        }\n        catch (IllegalArgumentException | IllegalStateException ignored) {\n            // No-op\n        }\n\n        // Near Creation from client node after closed.\n\n        IgniteCache<String, String> cache2New = grid(2).createNearCache(CACHE_NAME_NEAR, new NearCacheConfiguration());\n\n        assertNotSame(cache2, cache2New);\n\n        // Subscribing to events.\n        cache2New.put(KEY_VAL, KEY_VAL);\n\n        assert cache2New.localPeek(KEY_VAL).equals(KEY_VAL);\n\n        cache0.put(KEY_VAL, KEY_VAL + \"recreated\");\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache2New.localPeek(KEY_VAL).equals(KEY_VAL + \"recreated\");\n    }",
            " 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  ",
            "    /**\n     * Test Near close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testNearClose() throws Exception {\n        startGridsMultiThreaded(gridCount());\n\n        IgniteCache<String, String> cache0 = grid(0).getOrCreateCache(getNearConfig());\n\n        // GridDhtTxPrepareRequest requests to Client node will be counted.\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.nodeFilter = grid(2).context().localNodeId();\n\n        // Near Close from client node.\n\n        IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_NEAR);\n        IgniteCache<String, String> cache2 = grid(2).createNearCache(CACHE_NAME_NEAR, new NearCacheConfiguration());\n\n        assert cache2.get(KEY_VAL) == null;\n\n        // Subscribing to events.\n        cache2.put(KEY_VAL, KEY_VAL);\n\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.set(0);\n\n        cache0.put(KEY_VAL, \"near-test\");\n\n        U.sleep(1000);\n\n        //Ensure near cache was automatically updated.\n        assert CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.get() != 0;\n\n        assert cache2.localPeek(KEY_VAL).equals(\"near-test\");\n\n        cache2.close();\n\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.set(0);\n\n        // Should not produce messages to client node.\n        cache0.put(KEY_VAL, KEY_VAL + 0);\n\n        U.sleep(1000);\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + 0);// Not affected.\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + 0);// Not affected.\n\n        try {\n            cache2.get(KEY_VAL);// Affected.\n\n            assert false;\n        }\n        catch (IllegalArgumentException | IllegalStateException ignored) {\n            // No-op\n        }\n\n        // Near Creation from client node after closed.\n\n        IgniteCache<String, String> cache2New = grid(2).createNearCache(CACHE_NAME_NEAR, new NearCacheConfiguration());\n\n        assertNotSame(cache2, cache2New);\n\n        // Subscribing to events.\n        cache2New.put(KEY_VAL, KEY_VAL);\n\n        assert cache2New.localPeek(KEY_VAL).equals(KEY_VAL);\n\n        cache0.put(KEY_VAL, KEY_VAL + \"recreated\");\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache2New.localPeek(KEY_VAL).equals(KEY_VAL + \"recreated\");\n    }"
        ],
        [
            "CacheStopAndDestroySelfTest::testClientCloseWithTry()",
            " 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525 -\n 526 -\n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  ",
            "    /**\n     * Test Client close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testClientCloseWithTry() throws Exception {\n        fail(\"https://issues.apache.org/jira/browse/IGNITE-5511\");\n\n        startGridsMultiThreaded(gridCount());\n\n        String curVal = null;\n\n        for (int i = 0; i < 3; i++) {\n            try (IgniteCache<String, String> cache2 = grid(2).getOrCreateCache(getClientConfig())) {\n                IgniteCache<String, String> cache0 = grid(0).cache(CACHE_NAME_CLIENT);\n                IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_CLIENT);\n\n                if (i == 0) {\n                    assert cache0.get(KEY_VAL) == null;\n                    assert cache1.get(KEY_VAL) == null;\n                    assert cache2.get(KEY_VAL) == null;\n                }\n                else {\n                    assert cache0.get(KEY_VAL).equals(curVal);\n                    assert cache1.get(KEY_VAL).equals(curVal);\n                    assert cache2.get(KEY_VAL).equals(curVal);\n                }\n\n                curVal = KEY_VAL + curVal;\n\n                cache2.put(KEY_VAL, curVal);\n\n                assert cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL).equals(curVal);\n            }\n\n            awaitPartitionMapExchange();\n        }\n    }",
            " 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  ",
            "    /**\n     * Test Client close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testClientCloseWithTry() throws Exception {\n        startGridsMultiThreaded(gridCount());\n\n        String curVal = null;\n\n        for (int i = 0; i < 3; i++) {\n            try (IgniteCache<String, String> cache2 = grid(2).getOrCreateCache(getClientConfig())) {\n                IgniteCache<String, String> cache0 = grid(0).cache(CACHE_NAME_CLIENT);\n                IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_CLIENT);\n\n                if (i == 0) {\n                    assert cache0.get(KEY_VAL) == null;\n                    assert cache1.get(KEY_VAL) == null;\n                    assert cache2.get(KEY_VAL) == null;\n                }\n                else {\n                    assert cache0.get(KEY_VAL).equals(curVal);\n                    assert cache1.get(KEY_VAL).equals(curVal);\n                    assert cache2.get(KEY_VAL).equals(curVal);\n                }\n\n                curVal = KEY_VAL + curVal;\n\n                cache2.put(KEY_VAL, curVal);\n\n                assert cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL).equals(curVal);\n            }\n\n            awaitPartitionMapExchange();\n        }\n    }"
        ],
        [
            "CacheStopAndDestroySelfTest::testNearCloseWithTry()",
            " 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641 -\n 642 -\n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  ",
            "    /**\n     * Test Near close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testNearCloseWithTry() throws Exception {\n        fail(\"https://issues.apache.org/jira/browse/IGNITE-5511\");\n\n        startGridsMultiThreaded(gridCount());\n\n        String curVal = null;\n\n        grid(0).getOrCreateCache(getNearConfig());\n\n        NearCacheConfiguration nearCfg = new NearCacheConfiguration();\n\n        for (int i = 0; i < 3; i++) {\n            try (IgniteCache<String, String> cache2 = grid(2).getOrCreateNearCache(CACHE_NAME_NEAR, nearCfg)) {\n                IgniteCache<String, String> cache0 = grid(0).cache(CACHE_NAME_NEAR);\n                IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_NEAR);\n\n                assert cache2.localPeek(KEY_VAL) == null;\n\n                assert cache0.get(KEY_VAL) == null || cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL) == null || cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL) == null || cache2.get(KEY_VAL).equals(curVal);\n\n                curVal = KEY_VAL + curVal;\n\n                cache2.put(KEY_VAL, curVal);\n\n                assert cache2.localPeek(KEY_VAL).equals(curVal);\n\n                assert cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL).equals(curVal);\n            }\n        }\n    }",
            " 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  ",
            "    /**\n     * Test Near close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testNearCloseWithTry() throws Exception {\n        startGridsMultiThreaded(gridCount());\n\n        String curVal = null;\n\n        grid(0).getOrCreateCache(getNearConfig());\n\n        NearCacheConfiguration nearCfg = new NearCacheConfiguration();\n\n        for (int i = 0; i < 3; i++) {\n            try (IgniteCache<String, String> cache2 = grid(2).getOrCreateNearCache(CACHE_NAME_NEAR, nearCfg)) {\n                IgniteCache<String, String> cache0 = grid(0).cache(CACHE_NAME_NEAR);\n                IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_NEAR);\n\n                assert cache2.localPeek(KEY_VAL) == null;\n\n                assert cache0.get(KEY_VAL) == null || cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL) == null || cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL) == null || cache2.get(KEY_VAL).equals(curVal);\n\n                curVal = KEY_VAL + curVal;\n\n                cache2.put(KEY_VAL, curVal);\n\n                assert cache2.localPeek(KEY_VAL).equals(curVal);\n\n                assert cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL).equals(curVal);\n            }\n        }\n    }"
        ],
        [
            "GridCacheProcessor::closeCache(GridCacheContext,boolean)",
            "2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079 -\n2080  \n2081  \n2082  ",
            "    /**\n     * @param cctx Cache context.\n     * @param destroy Destroy flag.\n     */\n    private void closeCache(GridCacheContext cctx, boolean destroy) {\n        if (cctx.affinityNode()) {\n            GridCacheAdapter<?, ?> cache = caches.get(cctx.name());\n\n            assert cache != null : cctx.name();\n\n            jCacheProxies.put(cctx.name(), new IgniteCacheProxy(cache.context(), cache, null, false));\n        }\n        else {\n            jCacheProxies.remove(cctx.name());\n\n            cctx.gate().onStopped();\n\n            prepareCacheStop(cctx.name(), destroy);\n\n            if (cctx.group().hasCaches())\n                stopCacheGroup(cctx.group().groupId());\n        }\n    }",
            "2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079 +\n2080  \n2081  \n2082  ",
            "    /**\n     * @param cctx Cache context.\n     * @param destroy Destroy flag.\n     */\n    private void closeCache(GridCacheContext cctx, boolean destroy) {\n        if (cctx.affinityNode()) {\n            GridCacheAdapter<?, ?> cache = caches.get(cctx.name());\n\n            assert cache != null : cctx.name();\n\n            jCacheProxies.put(cctx.name(), new IgniteCacheProxy(cache.context(), cache, null, false));\n        }\n        else {\n            jCacheProxies.remove(cctx.name());\n\n            cctx.gate().onStopped();\n\n            prepareCacheStop(cctx.name(), destroy);\n\n            if (!cctx.group().hasCaches())\n                stopCacheGroup(cctx.group().groupId());\n        }\n    }"
        ]
    ],
    "848af3609f9aad462d5e939e708d8284eb5d00e0": [
        [
            "GridDhtPartitionsExchangeFuture::startLocalSnasphotOperation()",
            " 893  \n 894  \n 895  \n 896 -\n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  ",
            "    /**\n\n     */\n    private void startLocalSnasphotOperation() {StartSnapshotOperationAckDiscoveryMessage snapOpMsg= getSnapshotOperationMessage();\n        if (snapOpMsg != null) {\n            SnapshotOperation op = snapOpMsg.snapshotOperation();\n\n            try {\n                IgniteInternalFuture fut = cctx.snapshot()\n                    .startLocalSnapshotOperation(snapOpMsg.initiatorNodeId(), snapOpMsg.snapshotOperation());\n\n                if (fut != null)\n                    fut.get();\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Error while starting snapshot operation\", e);\n            }\n        }\n    }",
            " 894  \n 895  \n 896  \n 897 +\n 898 +\n 899 +\n 900  \n 901  \n 902  \n 903 +\n 904 +\n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  ",
            "    /**\n\n     */\n    private void startLocalSnasphotOperation() {\n        StartSnapshotOperationAckDiscoveryMessage snapOpMsg= getSnapshotOperationMessage();\n\n        if (snapOpMsg != null) {\n            SnapshotOperation op = snapOpMsg.snapshotOperation();\n\n            assert snapOpMsg.needExchange();\n\n            try {\n                IgniteInternalFuture fut = cctx.snapshot()\n                    .startLocalSnapshotOperation(snapOpMsg.initiatorNodeId(), snapOpMsg.snapshotOperation());\n\n                if (fut != null)\n                    fut.get();\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Error while starting snapshot operation\", e);\n            }\n        }\n    }"
        ],
        [
            "GridCacheDatabaseSharedManager::Checkpointer::doCheckpoint()",
            "1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968 -\n1969 -\n1970 -\n1971 -\n1972 -\n1973 -\n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022 -\n2023 -\n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  ",
            "        /**\n         *\n         */\n        private void doCheckpoint() {\n            try {\n                CheckpointMetricsTracker tracker = new CheckpointMetricsTracker();\n\n                Checkpoint chp = markCheckpointBegin(tracker);\n\n                if (chp.cpPages == null){\n                    markCheckpointEnd(chp);\n\n                    return;\n                }\n\n                boolean interrupted = true;\n\n                try {\n                    if (chp.hasDelta()) {\n                        // Identity stores set.\n                        GridConcurrentHashSet<PageStore> updStores = new GridConcurrentHashSet<>();\n\n                        CountDownFuture doneWriteFut = new CountDownFuture(\n                            asyncRunner == null ? 1 : chp.cpPages.collectionsSize());\n\n                        tracker.onPagesWriteStart();\n\n                        if (asyncRunner != null) {\n                            for (int i = 0; i < chp.cpPages.collectionsSize(); i++) {\n                                Runnable write = new WriteCheckpointPages(\n                                    tracker,\n                                    chp.cpPages.innerCollection(i),\n                                    updStores,\n                                    doneWriteFut\n                                );\n\n                                try {\n                                    asyncRunner.execute(write);\n                                }\n                                catch (RejectedExecutionException ignore) {\n                                    // Run the task synchronously.\n                                    write.run();\n                                }\n                            }\n                        }\n                        else {\n                            // Single-threaded checkpoint.\n                            Runnable write = new WriteCheckpointPages(tracker, chp.cpPages, updStores, doneWriteFut);\n\n                            write.run();\n                        }\n\n                        // Wait and check for errors.\n                        doneWriteFut.get();\n\n                        // Must re-check shutdown flag here because threads may have skipped some pages.\n                        // If so, we should not put finish checkpoint mark.\n                        if (shutdownNow) {\n                            chp.progress.cpFinishFut.onDone(new NodeStoppingException(\"Node is stopping.\"));\n\n                            return;\n                        }\n\n                        snapshotMgr.afterCheckpointPageWritten();\n\n                        tracker.onFsyncStart();\n\n                        if (!skipSync) {\n                            for (PageStore updStore : updStores) {\n                                if (shutdownNow) {\n                                    chp.progress.cpFinishFut.onDone(new NodeStoppingException(\"Node is stopping.\"));\n\n                                    return;\n                                }\n\n                                updStore.sync();\n                            }\n                        }\n                    }\n                    else {\n                        tracker.onPagesWriteStart();\n                        tracker.onFsyncStart();\n                    }\n\n                    // Must mark successful checkpoint only if there are no exceptions or interrupts.\n                    interrupted = false;\n                }\n                finally {\n                    if (!interrupted)\n                        markCheckpointEnd(chp);\n                }\n\n                tracker.onEnd();\n\n                if (chp.hasDelta()) {\n                    if (printCheckpointStats) {\n                        if (log.isInfoEnabled())\n                            log.info(String.format(\"Checkpoint finished [cpId=%s, pages=%d, markPos=%s, \" +\n                                    \"walSegmentsCleared=%d, markDuration=%dms, pagesWrite=%dms, fsync=%dms, \" +\n                                    \"total=%dms]\",\n                                chp.cpEntry.checkpointId(),\n                                chp.pagesSize,\n                                chp.cpEntry.checkpointMark(),\n                                chp.walFilesDeleted,\n                                tracker.markDuration(),\n                                tracker.pagesWriteDuration(),\n                                tracker.fsyncDuration(),\n                                tracker.totalDuration()));\n                    }\n\n                    persStoreMetrics.onCheckpoint(\n                        tracker.lockWaitDuration(),\n                        tracker.markDuration(),\n                        tracker.pagesWriteDuration(),\n                        tracker.fsyncDuration(),\n                        tracker.totalDuration(),\n                        chp.pagesSize,\n                        tracker.dataPagesWritten(),\n                        tracker.cowPagesWritten());\n                }\n                else {\n                    persStoreMetrics.onCheckpoint(\n                        tracker.lockWaitDuration(),\n                        tracker.markDuration(),\n                        tracker.pagesWriteDuration(),\n                        tracker.fsyncDuration(),\n                        tracker.totalDuration(),\n                        chp.pagesSize,\n                        tracker.dataPagesWritten(),\n                        tracker.cowPagesWritten());\n                }\n            }\n            catch (IgniteCheckedException e) {\n                // TODO-ignite-db how to handle exception?\n                U.error(log, \"Failed to create checkpoint.\", e);\n            }\n        }",
            "1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036 +\n2037 +\n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  ",
            "        /**\n         *\n         */\n        private void doCheckpoint() {\n            try {\n                CheckpointMetricsTracker tracker = new CheckpointMetricsTracker();\n\n                Checkpoint chp = markCheckpointBegin(tracker);\n\n                boolean interrupted = true;\n\n                try {\n                    if (chp.hasDelta()) {\n                        // Identity stores set.\n                        GridConcurrentHashSet<PageStore> updStores = new GridConcurrentHashSet<>();\n\n                        CountDownFuture doneWriteFut = new CountDownFuture(\n                            asyncRunner == null ? 1 : chp.cpPages.collectionsSize());\n\n                        tracker.onPagesWriteStart();\n\n                        if (asyncRunner != null) {\n                            for (int i = 0; i < chp.cpPages.collectionsSize(); i++) {\n                                Runnable write = new WriteCheckpointPages(\n                                    tracker,\n                                    chp.cpPages.innerCollection(i),\n                                    updStores,\n                                    doneWriteFut\n                                );\n\n                                try {\n                                    asyncRunner.execute(write);\n                                }\n                                catch (RejectedExecutionException ignore) {\n                                    // Run the task synchronously.\n                                    write.run();\n                                }\n                            }\n                        }\n                        else {\n                            // Single-threaded checkpoint.\n                            Runnable write = new WriteCheckpointPages(tracker, chp.cpPages, updStores, doneWriteFut);\n\n                            write.run();\n                        }\n\n                        // Wait and check for errors.\n                        doneWriteFut.get();\n\n                        // Must re-check shutdown flag here because threads may have skipped some pages.\n                        // If so, we should not put finish checkpoint mark.\n                        if (shutdownNow) {\n                            chp.progress.cpFinishFut.onDone(new NodeStoppingException(\"Node is stopping.\"));\n\n                            return;\n                        }\n\n                        tracker.onFsyncStart();\n\n                        if (!skipSync) {\n                            for (PageStore updStore : updStores) {\n                                if (shutdownNow) {\n                                    chp.progress.cpFinishFut.onDone(new NodeStoppingException(\"Node is stopping.\"));\n\n                                    return;\n                                }\n\n                                updStore.sync();\n                            }\n                        }\n                    }\n                    else {\n                        tracker.onPagesWriteStart();\n                        tracker.onFsyncStart();\n                    }\n\n                    snapshotMgr.afterCheckpointPageWritten();\n\n                    // Must mark successful checkpoint only if there are no exceptions or interrupts.\n                    interrupted = false;\n                }\n                finally {\n                    if (!interrupted)\n                        markCheckpointEnd(chp);\n                }\n\n                tracker.onEnd();\n\n                if (chp.hasDelta()) {\n                    if (printCheckpointStats) {\n                        if (log.isInfoEnabled())\n                            log.info(String.format(\"Checkpoint finished [cpId=%s, pages=%d, markPos=%s, \" +\n                                    \"walSegmentsCleared=%d, markDuration=%dms, pagesWrite=%dms, fsync=%dms, \" +\n                                    \"total=%dms]\",\n                                chp.cpEntry.checkpointId(),\n                                chp.pagesSize,\n                                chp.cpEntry.checkpointMark(),\n                                chp.walFilesDeleted,\n                                tracker.markDuration(),\n                                tracker.pagesWriteDuration(),\n                                tracker.fsyncDuration(),\n                                tracker.totalDuration()));\n                    }\n\n                    persStoreMetrics.onCheckpoint(\n                        tracker.lockWaitDuration(),\n                        tracker.markDuration(),\n                        tracker.pagesWriteDuration(),\n                        tracker.fsyncDuration(),\n                        tracker.totalDuration(),\n                        chp.pagesSize,\n                        tracker.dataPagesWritten(),\n                        tracker.cowPagesWritten());\n                }\n                else {\n                    persStoreMetrics.onCheckpoint(\n                        tracker.lockWaitDuration(),\n                        tracker.markDuration(),\n                        tracker.pagesWriteDuration(),\n                        tracker.fsyncDuration(),\n                        tracker.totalDuration(),\n                        chp.pagesSize,\n                        tracker.dataPagesWritten(),\n                        tracker.cowPagesWritten());\n                }\n            }\n            catch (IgniteCheckedException e) {\n                // TODO-ignite-db how to handle exception?\n                U.error(log, \"Failed to create checkpoint.\", e);\n            }\n        }"
        ],
        [
            "GridCacheProcessor::blockGateway(String,boolean,boolean)",
            "1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947 -\n1948 -\n1949 -\n1950 -\n1951  \n1952 -\n1953 -\n1954 -\n1955 -\n1956  \n1957  \n1958  ",
            "    /**\n     * @param cacheName Cache name.\n     * @param stop {@code True} for stop cache, {@code false} for close cache.\n     * @param restart Restart flag.\n     */\n    void blockGateway(String cacheName, boolean stop, boolean restart) {\n        // Break the proxy before exchange future is done.\n        IgniteCacheProxy<?, ?> proxy = jCacheProxies.get(cacheName);\n\n            if (proxy != null) {\n                if (stop){\n                    if (restart)\n                        proxy.restart();\n\n                    proxy.gate().stopped();\n                }\n                else\n                    proxy.closeProxy();\n\n        }\n    }",
            "1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947 +\n1948 +\n1949 +\n1950 +\n1951  \n1952 +\n1953 +\n1954 +\n1955 +\n1956  \n1957  \n1958  ",
            "    /**\n     * @param cacheName Cache name.\n     * @param stop {@code True} for stop cache, {@code false} for close cache.\n     * @param restart Restart flag.\n     */\n    void blockGateway(String cacheName, boolean stop, boolean restart) {\n        // Break the proxy before exchange future is done.\n        IgniteCacheProxy<?, ?> proxy = jCacheProxies.get(cacheName);\n\n        if (proxy != null) {\n            if (stop) {\n                if (restart)\n                    proxy.restart();\n\n                proxy.gate().stopped();\n            }\n            else\n                proxy.closeProxy();\n\n        }\n    }"
        ],
        [
            "PageHandler::writePage(PageMemory,int,long,PageLockListener,PageHandler,PageIO,IgniteWriteAheadLogManager,Boolean,X,int,R)",
            " 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  ",
            "    /**\n     * @param pageMem Page memory.\n     * @param cacheId Cache ID.\n     * @param pageId Page ID.\n     * @param lsnr Lock listener.\n     * @param h Handler.\n     * @param init IO for new page initialization or {@code null} if it is an existing page.\n     * @param wal Write ahead log.\n     * @param walPlc Full page WAL record policy.\n     * @param arg Argument.\n     * @param intArg Argument of type {@code int}.\n     * @param lockFailed Result in case of lock failure due to page recycling.\n     * @return Handler result.\n     * @throws IgniteCheckedException If failed.\n     */\n    public static <X, R> R writePage(\n        PageMemory pageMem,\n        int cacheId,\n        long pageId,\n        PageLockListener lsnr,\n        PageHandler<X, R> h,\n        PageIO init,\n        IgniteWriteAheadLogManager wal,\n        Boolean walPlc,\n        X arg,\n        int intArg,\n        R lockFailed\n    ) throws IgniteCheckedException {\n        boolean releaseAfterWrite = true;\n        long page = pageMem.acquirePage(cacheId, pageId);\n        try {\n            long pageAddr = writeLock(pageMem, cacheId, pageId, page, lsnr, false);\n\n            if (pageAddr == 0L)\n                return lockFailed;\n\n            boolean ok = false;\n\n            try {\n                if (init != null) {\n                    // It is a new page and we have to initialize it.\n                    doInitPage(pageMem, cacheId, pageId, page, pageAddr, init, wal);\n                    walPlc = FALSE;\n                }\n                else {\n                    init = PageIO.getPageIO(pageAddr);\n                }\n\n                R res = h.run(cacheId, pageId, page, pageAddr, init, walPlc, arg, intArg);\n\n                ok = true;\n\n                return res;\n            }\n            finally {\n                assert PageIO.getCrc(pageAddr) == 0; //TODO GG-11480\n\n                if (releaseAfterWrite = h.releaseAfterWrite(cacheId, pageId, page, pageAddr, arg, intArg))\n                    writeUnlock(pageMem, cacheId, pageId, page, pageAddr, lsnr, walPlc, ok);\n            }\n        }\n        finally {\n            if (releaseAfterWrite)\n                pageMem.releasePage(cacheId, pageId, page);\n        }\n    }",
            " 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  ",
            "    /**\n     * @param pageMem Page memory.\n     * @param cacheId Cache ID.\n     * @param pageId Page ID.\n     * @param lsnr Lock listener.\n     * @param h Handler.\n     * @param init IO for new page initialization or {@code null} if it is an existing page.\n     * @param wal Write ahead log.\n     * @param walPlc Full page WAL record policy.\n     * @param arg Argument.\n     * @param intArg Argument of type {@code int}.\n     * @param lockFailed Result in case of lock failure due to page recycling.\n     * @return Handler result.\n     * @throws IgniteCheckedException If failed.\n     */\n    public static <X, R> R writePage(\n        PageMemory pageMem,\n        int cacheId,\n        final long pageId,\n        PageLockListener lsnr,\n        PageHandler<X, R> h,\n        PageIO init,\n        IgniteWriteAheadLogManager wal,\n        Boolean walPlc,\n        X arg,\n        int intArg,\n        R lockFailed\n    ) throws IgniteCheckedException {\n        boolean releaseAfterWrite = true;\n        long page = pageMem.acquirePage(cacheId, pageId);\n        try {\n            long pageAddr = writeLock(pageMem, cacheId, pageId, page, lsnr, false);\n\n            if (pageAddr == 0L)\n                return lockFailed;\n\n            boolean ok = false;\n\n            try {\n                if (init != null) {\n                    // It is a new page and we have to initialize it.\n                    doInitPage(pageMem, cacheId, pageId, page, pageAddr, init, wal);\n                    walPlc = FALSE;\n                }\n                else {\n                    init = PageIO.getPageIO(pageAddr);\n                }\n\n                R res = h.run(cacheId, pageId, page, pageAddr, init, walPlc, arg, intArg);\n\n                ok = true;\n\n                return res;\n            }\n            finally {\n                assert PageIO.getCrc(pageAddr) == 0; //TODO GG-11480\n\n                if (releaseAfterWrite = h.releaseAfterWrite(cacheId, pageId, page, pageAddr, arg, intArg))\n                    writeUnlock(pageMem, cacheId, pageId, page, pageAddr, lsnr, walPlc, ok);\n            }\n        }\n        finally {\n            if (releaseAfterWrite)\n                pageMem.releasePage(cacheId, pageId, page);\n        }\n    }"
        ],
        [
            "GridCacheProcessor::initCacheProxies(AffinityTopologyVersion,Throwable)",
            "2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012 -\n2013 -\n2014  \n2015 -\n2016 -\n2017  \n2018  \n2019  \n2020  \n2021  \n2022  ",
            "    /**\n     * @param startTopVer Cache start version.\n     * @param err Cache start error if any.\n     */\n    void initCacheProxies(\n        AffinityTopologyVersion startTopVer, @Nullable\n        Throwable err) {\n    for (GridCacheAdapter<?, ?> cache : caches.values()) {\n        GridCacheContext<?, ?> cacheCtx = cache.context();\n\n            if (cacheCtx.startTopologyVersion().equals(startTopVer) && !jCacheProxies.containsKey(cacheCtx.name())) {\n                jCacheProxies.putIfAbsent(cacheCtx.name(), new IgniteCacheProxy(cache.context(), cache, null, false));\n\n                if (cacheCtx.preloader() != null)\n                    cacheCtx.preloader().onInitialExchangeComplete(err);\n            }\n        }\n    }",
            "2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012 +\n2013 +\n2014  \n2015 +\n2016 +\n2017 +\n2018  \n2019  \n2020  \n2021  \n2022  \n2023  ",
            "    /**\n     * @param startTopVer Cache start version.\n     * @param err Cache start error if any.\n     */\n    void initCacheProxies(\n        AffinityTopologyVersion startTopVer, @Nullable\n        Throwable err) {\n        for (GridCacheAdapter<?, ?> cache : caches.values()) {\n            GridCacheContext<?, ?> cacheCtx = cache.context();\n\n            if (cacheCtx.startTopologyVersion().equals(startTopVer) ) {\n                if (!jCacheProxies.containsKey(cacheCtx.name()))\n                    jCacheProxies.putIfAbsent(cacheCtx.name(), new IgniteCacheProxy(cache.context(), cache, null, false));\n\n                if (cacheCtx.preloader() != null)\n                    cacheCtx.preloader().onInitialExchangeComplete(err);\n            }\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::getSnapshotOperationMessage()",
            "1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  ",
            "    /**\n     *\n     */\n    private StartSnapshotOperationAckDiscoveryMessage getSnapshotOperationMessage() {\n        // If it's a snapshot operation request, synchronously wait for backup start.\n        if (discoEvt != null && discoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n            DiscoveryCustomMessage customMsg = ((DiscoveryCustomEvent)discoEvt).customMessage();\n\n            if (customMsg instanceof StartSnapshotOperationAckDiscoveryMessage)\n                return  (StartSnapshotOperationAckDiscoveryMessage)customMsg;\n        }\n        return null;\n    }",
            "1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342 +\n1343  \n1344  ",
            "    /**\n     *\n     */\n    private StartSnapshotOperationAckDiscoveryMessage getSnapshotOperationMessage() {\n        // If it's a snapshot operation request, synchronously wait for backup start.\n        if (discoEvt != null && discoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n            DiscoveryCustomMessage customMsg = ((DiscoveryCustomEvent)discoEvt).customMessage();\n\n            if (customMsg instanceof StartSnapshotOperationAckDiscoveryMessage)\n                return  (StartSnapshotOperationAckDiscoveryMessage)customMsg;\n        }\n\n        return null;\n    }"
        ],
        [
            "GridCacheDatabaseSharedManager::Checkpoint::Checkpoint(CheckpointEntry,GridMultiCollectionWrapper,CheckpointProgress)",
            "2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476 -\n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  \n2485 -\n2486  ",
            "        /**\n         * @param cpEntry Checkpoint entry.\n         * @param cpPages Pages to write to the page store.\n         * @param progress Checkpoint progress status.\n         */\n        private Checkpoint(\n            CheckpointEntry cpEntry,\n            GridMultiCollectionWrapper<FullPageId> cpPages,\n            CheckpointProgress progress\n        ) {\n            assert cpEntry == null || cpEntry.initGuard != 0;\n\n            this.cpEntry = cpEntry;\n            this.cpPages = cpPages;\n            this.progress = progress;\n\n            pagesSize = cpPages == null ? 0 : cpPages.size();\n        }",
            "2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471 +\n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480 +\n2481  ",
            "        /**\n         * @param cpEntry Checkpoint entry.\n         * @param cpPages Pages to write to the page store.\n         * @param progress Checkpoint progress status.\n         */\n        private Checkpoint(\n            CheckpointEntry cpEntry,\n            @NotNull GridMultiCollectionWrapper<FullPageId> cpPages,\n            CheckpointProgress progress\n        ) {\n            assert cpEntry == null || cpEntry.initGuard != 0;\n\n            this.cpEntry = cpEntry;\n            this.cpPages = cpPages;\n            this.progress = progress;\n\n            pagesSize = cpPages.size();\n        }"
        ],
        [
            "GridCacheDatabaseSharedManager::Checkpointer::markCheckpointBegin(CheckpointMetricsTracker)",
            "2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270 -\n2271  \n2272  ",
            "        /**\n         *\n         */\n        @SuppressWarnings(\"TooBroadScope\")\n        private Checkpoint markCheckpointBegin(CheckpointMetricsTracker tracker) throws IgniteCheckedException {\n            CheckpointRecord cpRec = new CheckpointRecord(null, false);\n\n            WALPointer cpPtr = null;\n\n            GridMultiCollectionWrapper<FullPageId> cpPages;\n\n            final CheckpointProgress curr;\n\n            tracker.onLockWaitStart();\n\n            checkpointLock.writeLock().lock();\n\n            try {\n                tracker.onMarkStart();\n\n                synchronized (this) {\n                    curr = scheduledCp;\n\n                    curr.started = true;\n\n                    if (curr.reason == null)\n                        curr.reason = \"timeout\";\n\n                    // It is important that we assign a new progress object before checkpoint mark in page memory.\n                    scheduledCp = new CheckpointProgress(U.currentTimeMillis() + checkpointFreq);\n\n                    curCpProgress = curr;\n                }\n\n                final NavigableMap<T2<Integer, Integer>, T2<Integer, Integer>> map =\n                    new TreeMap<>(FullPageIdIterableComparator.INSTANCE);\n\n                DbCheckpointListener.Context ctx0 = new DbCheckpointListener.Context() {\n                    @Override public boolean nextSnapshot() {\n                        return curr.nextSnapshot;\n                    }\n\n                    @Override public Map<T2<Integer, Integer>, T2<Integer, Integer>> partitionStatMap() {\n                        return map;\n                    }\n                };\n\n                // Listeners must be invoked before we write checkpoint record to WAL.\n                for (DbCheckpointListener lsnr : lsnrs)\n                    lsnr.onCheckpointBegin(ctx0);\n\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    List<GridDhtLocalPartition> locParts = new ArrayList<>();\n\n                    for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions())\n                        locParts.add(part);\n\n                    Collections.sort(locParts, ASC_PART_COMPARATOR);\n\n                    CacheState state = new CacheState(locParts.size());\n\n                    for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions())\n                        state.addPartitionState(part.id(), part.dataStore().fullSize(), part.lastAppliedUpdate());\n\n                    cpRec.addCacheGroupState(grp.groupId(), state);\n                }\n\n                if (curr.nextSnapshot)\n                    snapshotMgr.onMarkCheckPointBegin(curr.snapshotOperation, map);\n\n                IgniteBiTuple<Collection<GridMultiCollectionWrapper<FullPageId>>, Integer> tup = beginAllCheckpoints();\n\n                // Todo it maybe more optimally\n                Collection<FullPageId> cpPagesList = new ArrayList<>(tup.get2());\n\n                for (GridMultiCollectionWrapper<FullPageId> col : tup.get1()) {\n                    for (int i = 0; i < col.collectionsSize(); i++)\n                        cpPagesList.addAll(col.innerCollection(i));\n                }\n\n                cpPages = new GridMultiCollectionWrapper<>(cpPagesList);\n\n                if (!F.isEmpty(cpPages)) {\n                    // No page updates for this checkpoint are allowed from now on.\n                    cpPtr = cctx.wal().log(cpRec);\n\n                    if (cpPtr == null)\n                        cpPtr = CheckpointStatus.NULL_PTR;\n                }\n            }\n            finally {\n                checkpointLock.writeLock().unlock();\n\n                tracker.onLockRelease();\n            }\n\n            curr.cpBeginFut.onDone();\n\n            if (!F.isEmpty(cpPages)) {\n                assert cpPtr != null;\n\n                // Sync log outside the checkpoint write lock.\n                cctx.wal().fsync(cpPtr);\n\n                long cpTs = System.currentTimeMillis();\n\n                CheckpointEntry cpEntry = writeCheckpointEntry(\n                    tmpWriteBuf,\n                    cpTs,\n                    cpRec.checkpointId(),\n                    cpPtr,\n                    cpRec,\n                    CheckpointEntryType.START);\n\n                checkpointHist.addCheckpointEntry(cpEntry);\n\n                if (printCheckpointStats)\n                    if (log.isInfoEnabled())\n                        log.info(String.format(\"Checkpoint started [checkpointId=%s, startPtr=%s, checkpointLockWait=%dms, \" +\n                                \"checkpointLockHoldTime=%dms, pages=%d, reason='%s']\",\n                            cpRec.checkpointId(),\n                            cpPtr,\n                            tracker.lockWaitDuration(),\n                            tracker.lockHoldDuration(),\n                            cpPages.size(),\n                            curr.reason)\n                        );\n\n                return new Checkpoint(cpEntry, cpPages, curr);\n            }\n            else {\n                if (printCheckpointStats) {\n                    if (log.isInfoEnabled())\n                        LT.info(log, String.format(\"Skipping checkpoint (no pages were modified) [\" +\n                            \"checkpointLockWait=%dms, checkpointLockHoldTime=%dms, reason='%s']\",\n                            tracker.lockWaitDuration(),\n                            tracker.lockHoldDuration(),\n                            curr.reason));\n                }\n\n                return new Checkpoint(null, null, curr);\n            }\n        }",
            "2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265 +\n2266  \n2267  ",
            "        /**\n         *\n         */\n        @SuppressWarnings(\"TooBroadScope\")\n        private Checkpoint markCheckpointBegin(CheckpointMetricsTracker tracker) throws IgniteCheckedException {\n            CheckpointRecord cpRec = new CheckpointRecord(null, false);\n\n            WALPointer cpPtr = null;\n\n            GridMultiCollectionWrapper<FullPageId> cpPages;\n\n            final CheckpointProgress curr;\n\n            tracker.onLockWaitStart();\n\n            checkpointLock.writeLock().lock();\n\n            try {\n                tracker.onMarkStart();\n\n                synchronized (this) {\n                    curr = scheduledCp;\n\n                    curr.started = true;\n\n                    if (curr.reason == null)\n                        curr.reason = \"timeout\";\n\n                    // It is important that we assign a new progress object before checkpoint mark in page memory.\n                    scheduledCp = new CheckpointProgress(U.currentTimeMillis() + checkpointFreq);\n\n                    curCpProgress = curr;\n                }\n\n                final NavigableMap<T2<Integer, Integer>, T2<Integer, Integer>> map =\n                    new TreeMap<>(FullPageIdIterableComparator.INSTANCE);\n\n                DbCheckpointListener.Context ctx0 = new DbCheckpointListener.Context() {\n                    @Override public boolean nextSnapshot() {\n                        return curr.nextSnapshot;\n                    }\n\n                    @Override public Map<T2<Integer, Integer>, T2<Integer, Integer>> partitionStatMap() {\n                        return map;\n                    }\n                };\n\n                // Listeners must be invoked before we write checkpoint record to WAL.\n                for (DbCheckpointListener lsnr : lsnrs)\n                    lsnr.onCheckpointBegin(ctx0);\n\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    List<GridDhtLocalPartition> locParts = new ArrayList<>();\n\n                    for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions())\n                        locParts.add(part);\n\n                    Collections.sort(locParts, ASC_PART_COMPARATOR);\n\n                    CacheState state = new CacheState(locParts.size());\n\n                    for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions())\n                        state.addPartitionState(part.id(), part.dataStore().fullSize(), part.lastAppliedUpdate());\n\n                    cpRec.addCacheGroupState(grp.groupId(), state);\n                }\n\n                if (curr.nextSnapshot)\n                    snapshotMgr.onMarkCheckPointBegin(curr.snapshotOperation, map);\n\n                IgniteBiTuple<Collection<GridMultiCollectionWrapper<FullPageId>>, Integer> tup = beginAllCheckpoints();\n\n                // Todo it maybe more optimally\n                Collection<FullPageId> cpPagesList = new ArrayList<>(tup.get2());\n\n                for (GridMultiCollectionWrapper<FullPageId> col : tup.get1()) {\n                    for (int i = 0; i < col.collectionsSize(); i++)\n                        cpPagesList.addAll(col.innerCollection(i));\n                }\n\n                cpPages = new GridMultiCollectionWrapper<>(cpPagesList);\n\n                if (!F.isEmpty(cpPages)) {\n                    // No page updates for this checkpoint are allowed from now on.\n                    cpPtr = cctx.wal().log(cpRec);\n\n                    if (cpPtr == null)\n                        cpPtr = CheckpointStatus.NULL_PTR;\n                }\n            }\n            finally {\n                checkpointLock.writeLock().unlock();\n\n                tracker.onLockRelease();\n            }\n\n            curr.cpBeginFut.onDone();\n\n            if (!F.isEmpty(cpPages)) {\n                assert cpPtr != null;\n\n                // Sync log outside the checkpoint write lock.\n                cctx.wal().fsync(cpPtr);\n\n                long cpTs = System.currentTimeMillis();\n\n                CheckpointEntry cpEntry = writeCheckpointEntry(\n                    tmpWriteBuf,\n                    cpTs,\n                    cpRec.checkpointId(),\n                    cpPtr,\n                    cpRec,\n                    CheckpointEntryType.START);\n\n                checkpointHist.addCheckpointEntry(cpEntry);\n\n                if (printCheckpointStats)\n                    if (log.isInfoEnabled())\n                        log.info(String.format(\"Checkpoint started [checkpointId=%s, startPtr=%s, checkpointLockWait=%dms, \" +\n                                \"checkpointLockHoldTime=%dms, pages=%d, reason='%s']\",\n                            cpRec.checkpointId(),\n                            cpPtr,\n                            tracker.lockWaitDuration(),\n                            tracker.lockHoldDuration(),\n                            cpPages.size(),\n                            curr.reason)\n                        );\n\n                return new Checkpoint(cpEntry, cpPages, curr);\n            }\n            else {\n                if (printCheckpointStats) {\n                    if (log.isInfoEnabled())\n                        LT.info(log, String.format(\"Skipping checkpoint (no pages were modified) [\" +\n                            \"checkpointLockWait=%dms, checkpointLockHoldTime=%dms, reason='%s']\",\n                            tracker.lockWaitDuration(),\n                            tracker.lockHoldDuration(),\n                            curr.reason));\n                }\n\n                return new Checkpoint(null, new GridMultiCollectionWrapper<>(new Collection[0]), curr);\n            }\n        }"
        ]
    ],
    "807cab28575efd221a0cba9846549a6ad1273ed6": [
        [
            "GridClientPartitionTopology::update(GridDhtPartitionExchangeId,GridDhtPartitionMap)",
            " 677  \n 678  \n 679 -\n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691 -\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698 -\n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705 -\n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 -\n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765 -\n 766  \n 767  \n 768  \n 769  \n 770  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Nullable @Override public GridDhtPartitionMap update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!cctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return null;\n        }\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return null;\n\n            if (lastExchangeVer != null && exchId != null && lastExchangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for single partition map update (will ignore) [lastExchVer=\" +\n                        lastExchangeVer + \", exchId=\" + exchId + ']');\n\n                return null;\n            }\n\n            if (exchId != null)\n                lastExchangeVer = exchId.topologyVersion();\n\n            if (node2part == null)\n                // Create invalid partition map.\n                node2part = new GridDhtPartitionFullMap();\n\n            GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n            if (cur != null && cur.updateSequence() >= parts.updateSequence()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale update sequence for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curSeq=\" + cur.updateSequence() + \", newSeq=\" + parts.updateSequence() + ']');\n\n                return null;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            node2part = new GridDhtPartitionFullMap(node2part, updateSeq);\n\n            boolean changed = false;\n\n            if (cur == null || !cur.equals(parts))\n                changed = true;\n\n            node2part.put(parts.nodeId(), parts);\n\n            part2node = new HashMap<>(part2node);\n\n            // Add new mappings.\n            for (Integer p : parts.keySet()) {\n                Set<UUID> ids = part2node.get(p);\n\n                if (ids == null)\n                    // Initialize HashSet to size 3 in anticipation that there won't be\n                    // more than 3 nodes per partition.\n                    part2node.put(p, ids = U.newHashSet(3));\n\n                changed |= ids.add(parts.nodeId());\n            }\n\n            // Remove obsolete mappings.\n            if (cur != null) {\n                for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                    Set<UUID> ids = part2node.get(p);\n\n                    if (ids != null)\n                        changed |= ids.remove(parts.nodeId());\n                }\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after single update: \" + fullMapString());\n\n            return changed ? localPartitionMap() : null;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            " 677  \n 678  \n 679 +\n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691 +\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698 +\n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705 +\n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 +\n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765 +\n 766  \n 767  \n 768  \n 769  \n 770  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!cctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return false;\n        }\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return false;\n\n            if (lastExchangeVer != null && exchId != null && lastExchangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for single partition map update (will ignore) [lastExchVer=\" +\n                        lastExchangeVer + \", exchId=\" + exchId + ']');\n\n                return false;\n            }\n\n            if (exchId != null)\n                lastExchangeVer = exchId.topologyVersion();\n\n            if (node2part == null)\n                // Create invalid partition map.\n                node2part = new GridDhtPartitionFullMap();\n\n            GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n            if (cur != null && cur.updateSequence() >= parts.updateSequence()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale update sequence for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curSeq=\" + cur.updateSequence() + \", newSeq=\" + parts.updateSequence() + ']');\n\n                return false;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            node2part = new GridDhtPartitionFullMap(node2part, updateSeq);\n\n            boolean changed = false;\n\n            if (cur == null || !cur.equals(parts))\n                changed = true;\n\n            node2part.put(parts.nodeId(), parts);\n\n            part2node = new HashMap<>(part2node);\n\n            // Add new mappings.\n            for (Integer p : parts.keySet()) {\n                Set<UUID> ids = part2node.get(p);\n\n                if (ids == null)\n                    // Initialize HashSet to size 3 in anticipation that there won't be\n                    // more than 3 nodes per partition.\n                    part2node.put(p, ids = U.newHashSet(3));\n\n                changed |= ids.add(parts.nodeId());\n            }\n\n            // Remove obsolete mappings.\n            if (cur != null) {\n                for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                    Set<UUID> ids = part2node.get(p);\n\n                    if (ids != null)\n                        changed |= ids.remove(parts.nodeId());\n                }\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after single update: \" + fullMapString());\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridCachePartitionExchangeManager::processSinglePartitionUpdate(ClusterNode,GridDhtPartitionsSingleMessage)",
            "1289  \n1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322 -\n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  ",
            "    /**\n     * @param node Node ID.\n     * @param msg Message.\n     */\n    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                        msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    if (grp != null &&\n                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                        continue;\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null) {\n                        updated |= top.update(null, entry.getValue()) != null;\n\n                        cctx.affinity().checkRebalanceState(top, grpId);\n                    }\n                }\n\n                if (updated)\n                    scheduleResendPartitions();\n            }\n            else {\n                if (msg.client()) {\n                    final GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(\n                        msg.exchangeId(),\n                        null,\n                        null,\n                        null,\n                        null);\n\n                    exchFut.listen(new CI1<IgniteInternalFuture<AffinityTopologyVersion>>() {\n                        @Override public void apply(IgniteInternalFuture<AffinityTopologyVersion> fut) {\n                            // Finished future should reply only to sender client node.\n                            exchFut.onReceive(node, msg);\n                        }\n                    });\n                }\n                else\n                    exchangeFuture(msg.exchangeId(), null, null, null, null).onReceive(node, msg);\n            }\n        }\n        finally {\n            leaveBusy();\n        }\n    }",
            "1289  \n1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322 +\n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  ",
            "    /**\n     * @param node Node ID.\n     * @param msg Message.\n     */\n    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                        msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    if (grp != null &&\n                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                        continue;\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null) {\n                        updated |= top.update(null, entry.getValue());\n\n                        cctx.affinity().checkRebalanceState(top, grpId);\n                    }\n                }\n\n                if (updated)\n                    scheduleResendPartitions();\n            }\n            else {\n                if (msg.client()) {\n                    final GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(\n                        msg.exchangeId(),\n                        null,\n                        null,\n                        null,\n                        null);\n\n                    exchFut.listen(new CI1<IgniteInternalFuture<AffinityTopologyVersion>>() {\n                        @Override public void apply(IgniteInternalFuture<AffinityTopologyVersion> fut) {\n                            // Finished future should reply only to sender client node.\n                            exchFut.onReceive(node, msg);\n                        }\n                    });\n                }\n                else\n                    exchangeFuture(msg.exchangeId(), null, null, null, null).onReceive(node, msg);\n            }\n        }\n        finally {\n            leaveBusy();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(AffinityTopologyVersion,GridDhtPartitionFullMap,Map)",
            "1111  \n1112  \n1113 -\n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127 -\n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157 -\n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165 -\n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272 -\n1273  \n1274  \n1275  \n1276  \n1277  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public GridDhtPartitionMap update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable Map<Integer, T2<Long, Long>> cntrMap\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchangeVer + \", parts=\" + fullMapString() + ']');\n\n        assert partMap != null;\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return null;\n\n            if (cntrMap != null) {\n                // update local map partition counters\n                for (Map.Entry<Integer, T2<Long, Long>> e : cntrMap.entrySet()) {\n                    T2<Long, Long> cntr = this.cntrMap.get(e.getKey());\n\n                    if (cntr == null || cntr.get2() < e.getValue().get2())\n                        this.cntrMap.put(e.getKey(), e.getValue());\n                }\n\n                // update local counters in partitions\n                for (int i = 0; i < locParts.length(); i++) {\n                    GridDhtLocalPartition part = locParts.get(i);\n\n                    if (part == null)\n                        continue;\n\n                    T2<Long, Long> cntr = cntrMap.get(part.id());\n\n                    if (cntr != null)\n                        part.updateCounter(cntr.get2());\n                }\n            }\n\n            if (exchangeVer != null && lastExchangeVer != null && lastExchangeVer.compareTo(exchangeVer) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for full partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchangeVer + ']');\n\n                return null;\n            }\n\n            if (node2part != null && node2part.compareTo(partMap) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale partition map for full partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchangeVer + \", curMap=\" + node2part + \", newMap=\" + partMap + ']');\n\n                return null;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            if (exchangeVer != null)\n                lastExchangeVer = exchangeVer;\n\n            if (node2part != null) {\n                for (GridDhtPartitionMap part : node2part.values()) {\n                    GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                    // If for some nodes current partition has a newer map,\n                    // then we keep the newer value.\n                    if (newPart != null &&\n                        (newPart.updateSequence() < part.updateSequence() ||\n                        (grp.localStartVersion().compareTo(newPart.topologyVersion()) > 0))\n                        ) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Overriding partition map in full update map [exch=\" + exchangeVer +\n                                \", curPart=\" + mapString(part) + \", newPart=\" + mapString(newPart) + ']');\n\n                        partMap.put(part.nodeId(), part);\n                    }\n                }\n\n                // Remove entry if node left.\n                for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                    UUID nodeId = it.next();\n\n                    if (!ctx.discovery().alive(nodeId)) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                partMap + ']');\n\n                        it.remove();\n                    }\n                }\n            }\n\n            node2part = partMap;\n\n            Map<Integer, Set<UUID>> p2n = new HashMap<>(grp.affinity().partitions(), 1.0f);\n\n            for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                for (Integer p : e.getValue().keySet()) {\n                    Set<UUID> ids = p2n.get(p);\n\n                    if (ids == null)\n                        // Initialize HashSet to size 3 in anticipation that there won't be\n                        // more than 3 nodes per partitions.\n                        p2n.put(p, ids = U.newHashSet(3));\n\n                    ids.add(e.getKey());\n                }\n            }\n\n            part2node = p2n;\n\n            boolean changed = false;\n\n            AffinityTopologyVersion affVer = grp.affinity().lastVersion();\n\n            GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n            if (nodeMap != null && ctx.database().persistenceEnabled()) {\n                for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                    int p = e.getKey();\n                    GridDhtPartitionState state = e.getValue();\n\n                   if (state == MOVING) {\n                        GridDhtLocalPartition locPart = locParts.get(p);\n\n                        assert locPart != null;\n\n                        if (locPart.state() == OWNING) {\n                            locPart.moving();\n\n                            changed = true;\n                        }\n\n                        if (cntrMap != null) {\n                            T2<Long, Long> cntr = cntrMap.get(p);\n\n                            if (cntr != null && cntr.get2() > locPart.updateCounter())\n                                locPart.updateCounter(cntr.get2());\n                        }\n                    }\n                }\n            }\n\n            if (!affVer.equals(AffinityTopologyVersion.NONE) && affVer.compareTo(topVer) >= 0) {\n                List<List<ClusterNode>> aff = grp.affinity().assignments(topVer);\n\n                changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff);\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after full update: \" + fullMapString());\n\n            if (changed)\n                ctx.exchange().scheduleResendPartitions();\n\n            return changed ? localPartitionMap() : null;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            "1111  \n1112  \n1113 +\n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127 +\n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157 +\n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165 +\n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272 +\n1273  \n1274  \n1275  \n1276  \n1277  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable Map<Integer, T2<Long, Long>> cntrMap\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchangeVer + \", parts=\" + fullMapString() + ']');\n\n        assert partMap != null;\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return false;\n\n            if (cntrMap != null) {\n                // update local map partition counters\n                for (Map.Entry<Integer, T2<Long, Long>> e : cntrMap.entrySet()) {\n                    T2<Long, Long> cntr = this.cntrMap.get(e.getKey());\n\n                    if (cntr == null || cntr.get2() < e.getValue().get2())\n                        this.cntrMap.put(e.getKey(), e.getValue());\n                }\n\n                // update local counters in partitions\n                for (int i = 0; i < locParts.length(); i++) {\n                    GridDhtLocalPartition part = locParts.get(i);\n\n                    if (part == null)\n                        continue;\n\n                    T2<Long, Long> cntr = cntrMap.get(part.id());\n\n                    if (cntr != null)\n                        part.updateCounter(cntr.get2());\n                }\n            }\n\n            if (exchangeVer != null && lastExchangeVer != null && lastExchangeVer.compareTo(exchangeVer) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for full partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchangeVer + ']');\n\n                return false;\n            }\n\n            if (node2part != null && node2part.compareTo(partMap) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale partition map for full partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchangeVer + \", curMap=\" + node2part + \", newMap=\" + partMap + ']');\n\n                return false;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            if (exchangeVer != null)\n                lastExchangeVer = exchangeVer;\n\n            if (node2part != null) {\n                for (GridDhtPartitionMap part : node2part.values()) {\n                    GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                    // If for some nodes current partition has a newer map,\n                    // then we keep the newer value.\n                    if (newPart != null &&\n                        (newPart.updateSequence() < part.updateSequence() ||\n                        (grp.localStartVersion().compareTo(newPart.topologyVersion()) > 0))\n                        ) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Overriding partition map in full update map [exch=\" + exchangeVer +\n                                \", curPart=\" + mapString(part) + \", newPart=\" + mapString(newPart) + ']');\n\n                        partMap.put(part.nodeId(), part);\n                    }\n                }\n\n                // Remove entry if node left.\n                for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                    UUID nodeId = it.next();\n\n                    if (!ctx.discovery().alive(nodeId)) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                partMap + ']');\n\n                        it.remove();\n                    }\n                }\n            }\n\n            node2part = partMap;\n\n            Map<Integer, Set<UUID>> p2n = new HashMap<>(grp.affinity().partitions(), 1.0f);\n\n            for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                for (Integer p : e.getValue().keySet()) {\n                    Set<UUID> ids = p2n.get(p);\n\n                    if (ids == null)\n                        // Initialize HashSet to size 3 in anticipation that there won't be\n                        // more than 3 nodes per partitions.\n                        p2n.put(p, ids = U.newHashSet(3));\n\n                    ids.add(e.getKey());\n                }\n            }\n\n            part2node = p2n;\n\n            boolean changed = false;\n\n            AffinityTopologyVersion affVer = grp.affinity().lastVersion();\n\n            GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n            if (nodeMap != null && ctx.database().persistenceEnabled()) {\n                for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                    int p = e.getKey();\n                    GridDhtPartitionState state = e.getValue();\n\n                   if (state == MOVING) {\n                        GridDhtLocalPartition locPart = locParts.get(p);\n\n                        assert locPart != null;\n\n                        if (locPart.state() == OWNING) {\n                            locPart.moving();\n\n                            changed = true;\n                        }\n\n                        if (cntrMap != null) {\n                            T2<Long, Long> cntr = cntrMap.get(p);\n\n                            if (cntr != null && cntr.get2() > locPart.updateCounter())\n                                locPart.updateCounter(cntr.get2());\n                        }\n                    }\n                }\n            }\n\n            if (!affVer.equals(AffinityTopologyVersion.NONE) && affVer.compareTo(topVer) >= 0) {\n                List<List<ClusterNode>> aff = grp.affinity().assignments(topVer);\n\n                changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff);\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after full update: \" + fullMapString());\n\n            if (changed)\n                ctx.exchange().scheduleResendPartitions();\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(GridDhtPartitionExchangeId,GridDhtPartitionMap)",
            "1314  \n1315  \n1316 -\n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328 -\n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335 -\n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342 -\n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359 -\n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415 -\n1416  \n1417  \n1418  \n1419  \n1420  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Nullable @Override public GridDhtPartitionMap update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!ctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return null;\n        }\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return null;\n\n            if (lastExchangeVer != null && exchId != null && lastExchangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for single partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchId.topologyVersion() + ']');\n\n                return null;\n            }\n\n            if (exchId != null)\n                lastExchangeVer = exchId.topologyVersion();\n\n            if (node2part == null)\n                // Create invalid partition map.\n                node2part = new GridDhtPartitionFullMap();\n\n            GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n            if (cur != null && cur.updateSequence() >= parts.updateSequence()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale update sequence for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curSeq=\" + cur.updateSequence() + \", newSeq=\" + parts.updateSequence() + ']');\n\n                return null;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            node2part = new GridDhtPartitionFullMap(node2part, updateSeq);\n\n            boolean changed = false;\n\n            if (cur == null || !cur.equals(parts))\n                changed = true;\n\n            node2part.put(parts.nodeId(), parts);\n\n            part2node = new HashMap<>(part2node);\n\n            // Add new mappings.\n            for (Integer p : parts.keySet()) {\n                Set<UUID> ids = part2node.get(p);\n\n                if (ids == null)\n                    // Initialize HashSet to size 3 in anticipation that there won't be\n                    // more than 3 nodes per partition.\n                    part2node.put(p, ids = U.newHashSet(3));\n\n                changed |= ids.add(parts.nodeId());\n            }\n\n            // Remove obsolete mappings.\n            if (cur != null) {\n                for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                    Set<UUID> ids = part2node.get(p);\n\n                    if (ids != null)\n                        changed |= ids.remove(parts.nodeId());\n                }\n            }\n\n            AffinityTopologyVersion affVer = grp.affinity().lastVersion();\n\n            if (!affVer.equals(AffinityTopologyVersion.NONE) && affVer.compareTo(topVer) >= 0) {\n                List<List<ClusterNode>> aff = grp.affinity().assignments(topVer);\n\n                changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff);\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after single update: \" + fullMapString());\n\n            if (changed)\n                ctx.exchange().scheduleResendPartitions();\n\n            return changed ? localPartitionMap() : null;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            "1314  \n1315  \n1316 +\n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328 +\n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335 +\n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342 +\n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359 +\n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415 +\n1416  \n1417  \n1418  \n1419  \n1420  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!ctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return false;\n        }\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return false;\n\n            if (lastExchangeVer != null && exchId != null && lastExchangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for single partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchId.topologyVersion() + ']');\n\n                return false;\n            }\n\n            if (exchId != null)\n                lastExchangeVer = exchId.topologyVersion();\n\n            if (node2part == null)\n                // Create invalid partition map.\n                node2part = new GridDhtPartitionFullMap();\n\n            GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n            if (cur != null && cur.updateSequence() >= parts.updateSequence()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale update sequence for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curSeq=\" + cur.updateSequence() + \", newSeq=\" + parts.updateSequence() + ']');\n\n                return false;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            node2part = new GridDhtPartitionFullMap(node2part, updateSeq);\n\n            boolean changed = false;\n\n            if (cur == null || !cur.equals(parts))\n                changed = true;\n\n            node2part.put(parts.nodeId(), parts);\n\n            part2node = new HashMap<>(part2node);\n\n            // Add new mappings.\n            for (Integer p : parts.keySet()) {\n                Set<UUID> ids = part2node.get(p);\n\n                if (ids == null)\n                    // Initialize HashSet to size 3 in anticipation that there won't be\n                    // more than 3 nodes per partition.\n                    part2node.put(p, ids = U.newHashSet(3));\n\n                changed |= ids.add(parts.nodeId());\n            }\n\n            // Remove obsolete mappings.\n            if (cur != null) {\n                for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                    Set<UUID> ids = part2node.get(p);\n\n                    if (ids != null)\n                        changed |= ids.remove(parts.nodeId());\n                }\n            }\n\n            AffinityTopologyVersion affVer = grp.affinity().lastVersion();\n\n            if (!affVer.equals(AffinityTopologyVersion.NONE) && affVer.compareTo(topVer) >= 0) {\n                List<List<ClusterNode>> aff = grp.affinity().assignments(topVer);\n\n                changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff);\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after single update: \" + fullMapString());\n\n            if (changed)\n                ctx.exchange().scheduleResendPartitions();\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridCachePartitionExchangeManager::processFullPartitionUpdate(ClusterNode,GridDhtPartitionsFullMessage)",
            "1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275 -\n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  ",
            "    /**\n     * @param node Node.\n     * @param msg Message.\n     */\n    private void processFullPartitionUpdate(ClusterNode node, GridDhtPartitionsFullMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received full partition update [node=\" + node.id() + \", msg=\" + msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionFullMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null)\n                        updated |= top.update(null, entry.getValue(), null) != null;\n                }\n\n                if (!cctx.kernalContext().clientNode() && updated)\n                    refreshPartitions();\n            }\n            else\n                exchangeFuture(msg.exchangeId(), null, null, null, null).onReceive(node, msg);\n        }\n        finally {\n            leaveBusy();\n        }\n    }",
            "1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275 +\n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  ",
            "    /**\n     * @param node Node.\n     * @param msg Message.\n     */\n    private void processFullPartitionUpdate(ClusterNode node, GridDhtPartitionsFullMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received full partition update [node=\" + node.id() + \", msg=\" + msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionFullMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null)\n                        updated |= top.update(null, entry.getValue(), null);\n                }\n\n                if (!cctx.kernalContext().clientNode() && updated)\n                    refreshPartitions();\n            }\n            else\n                exchangeFuture(msg.exchangeId(), null, null, null, null).onReceive(node, msg);\n        }\n        finally {\n            leaveBusy();\n        }\n    }"
        ],
        [
            "GridClientPartitionTopology::update(AffinityTopologyVersion,GridDhtPartitionFullMap,Map)",
            " 563  \n 564  \n 565 -\n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 -\n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588 -\n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651 -\n 652  \n 653  \n 654  \n 655  \n 656  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Nullable @Override public GridDhtPartitionMap update(\n        @Nullable AffinityTopologyVersion exchVer,\n        GridDhtPartitionFullMap partMap,\n        Map<Integer, T2<Long, Long>> cntrMap) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchVer + \", parts=\" + fullMapString() + ']');\n\n        lock.writeLock().lock();\n\n        try {\n            if (exchVer != null && lastExchangeVer != null && lastExchangeVer.compareTo(exchVer) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for full partition map update (will ignore) [lastExchId=\" +\n                        lastExchangeVer + \", exchVer=\" + exchVer + ']');\n\n                return null;\n            }\n\n            if (node2part != null && node2part.compareTo(partMap) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale partition map for full partition map update (will ignore) [lastExchId=\" +\n                        lastExchangeVer + \", exchVer=\" + exchVer + \", curMap=\" + node2part + \", newMap=\" + partMap + ']');\n\n                return null;\n            }\n\n            updateSeq.incrementAndGet();\n\n            if (exchVer != null)\n                lastExchangeVer = exchVer;\n\n            if (node2part != null) {\n                for (GridDhtPartitionMap part : node2part.values()) {\n                    GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                    // If for some nodes current partition has a newer map,\n                    // then we keep the newer value.\n                    if (newPart != null && newPart.updateSequence() < part.updateSequence()) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Overriding partition map in full update map [exchVer=\" + exchVer + \", curPart=\" +\n                                mapString(part) + \", newPart=\" + mapString(newPart) + ']');\n\n                        partMap.put(part.nodeId(), part);\n                    }\n                }\n\n                for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                    UUID nodeId = it.next();\n\n                    if (!cctx.discovery().alive(nodeId)) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                partMap + ']');\n\n                        it.remove();\n                    }\n                }\n            }\n\n            node2part = partMap;\n\n            Map<Integer, Set<UUID>> p2n = new HashMap<>();\n\n            for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                for (Integer p : e.getValue().keySet()) {\n                    Set<UUID> ids = p2n.get(p);\n\n                    if (ids == null)\n                        // Initialize HashSet to size 3 in anticipation that there won't be\n                        // more than 3 nodes per partitions.\n                        p2n.put(p, ids = U.newHashSet(3));\n\n                    ids.add(e.getKey());\n                }\n            }\n\n            part2node = p2n;\n\n            if (cntrMap != null)\n                this.cntrMap = new HashMap<>(cntrMap);\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after full update: \" + fullMapString());\n\n            return null;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            " 563  \n 564  \n 565 +\n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 +\n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588 +\n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651 +\n 652  \n 653  \n 654  \n 655  \n 656  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchVer,\n        GridDhtPartitionFullMap partMap,\n        Map<Integer, T2<Long, Long>> cntrMap) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchVer + \", parts=\" + fullMapString() + ']');\n\n        lock.writeLock().lock();\n\n        try {\n            if (exchVer != null && lastExchangeVer != null && lastExchangeVer.compareTo(exchVer) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for full partition map update (will ignore) [lastExchId=\" +\n                        lastExchangeVer + \", exchVer=\" + exchVer + ']');\n\n                return false;\n            }\n\n            if (node2part != null && node2part.compareTo(partMap) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale partition map for full partition map update (will ignore) [lastExchId=\" +\n                        lastExchangeVer + \", exchVer=\" + exchVer + \", curMap=\" + node2part + \", newMap=\" + partMap + ']');\n\n                return false;\n            }\n\n            updateSeq.incrementAndGet();\n\n            if (exchVer != null)\n                lastExchangeVer = exchVer;\n\n            if (node2part != null) {\n                for (GridDhtPartitionMap part : node2part.values()) {\n                    GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                    // If for some nodes current partition has a newer map,\n                    // then we keep the newer value.\n                    if (newPart != null && newPart.updateSequence() < part.updateSequence()) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Overriding partition map in full update map [exchVer=\" + exchVer + \", curPart=\" +\n                                mapString(part) + \", newPart=\" + mapString(newPart) + ']');\n\n                        partMap.put(part.nodeId(), part);\n                    }\n                }\n\n                for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                    UUID nodeId = it.next();\n\n                    if (!cctx.discovery().alive(nodeId)) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                partMap + ']');\n\n                        it.remove();\n                    }\n                }\n            }\n\n            node2part = partMap;\n\n            Map<Integer, Set<UUID>> p2n = new HashMap<>();\n\n            for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                for (Integer p : e.getValue().keySet()) {\n                    Set<UUID> ids = p2n.get(p);\n\n                    if (ids == null)\n                        // Initialize HashSet to size 3 in anticipation that there won't be\n                        // more than 3 nodes per partitions.\n                        p2n.put(p, ids = U.newHashSet(3));\n\n                    ids.add(e.getKey());\n                }\n            }\n\n            part2node = p2n;\n\n            if (cntrMap != null)\n                this.cntrMap = new HashMap<>(cntrMap);\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after full update: \" + fullMapString());\n\n            return false;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ]
    ],
    "8445b315663710507e6e3996223f01748f9674a6": [
        [
            "GridDhtPartitionTopologyImpl::beforeExchange(GridDhtPartitionsExchangeFuture,boolean)",
            " 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530 -\n 531 -\n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538 -\n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551 -\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575 -\n 576 -\n 577 -\n 578 -\n 579  \n 580 -\n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut, boolean affReady)\n        throws IgniteCheckedException {\n        DiscoveryEvent discoEvt = exchFut.discoveryEvent();\n\n        ClusterState newState = exchFut.newClusterState();\n\n        treatAllPartAsLoc = (newState != null && newState == ClusterState.ACTIVE)\n            || (ctx.kernalContext().state().active()\n            && discoEvt.type() == EventType.EVT_NODE_JOINED\n            && discoEvt.eventNode().isLocal()\n            && !ctx.kernalContext().clientNode()\n        );\n\n        // Wait for rent outside of checkpoint lock.\n        waitForRent();\n\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        synchronized (ctx.exchange().interruptLock()) {\n            if (Thread.currentThread().isInterrupted())\n                throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n            try {\n                U.writeLock(lock);\n            }\n            catch (IgniteInterruptedCheckedException e) {\n                ctx.database().checkpointReadUnlock();\n\n                throw e;\n            }\n\n            try {\n                GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n                if (stopping)\n                    return;\n\n                assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n                    topVer + \", exchId=\" + exchId + ']';\n\n                if (exchId.isLeft())\n                    removeNode(exchId.nodeId());\n\n                ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                cntrMap.clear();\n\n                // If this is the oldest node.\n                if (oldest != null && (loc.equals(oldest) || exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom()))) {\n                    if (node2part == null) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                                exchId + \", fullMap=\" + fullMapString() + ']');\n                    }\n                    else if (!node2part.valid()) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                                node2part + ']');\n                    }\n                    else if (!node2part.nodeId().equals(loc.id())) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                                exchId + \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n\n                if (affReady)\n                    initPartitions0(exchFut, updateSeq);\n                else {\n                    List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                    createPartitions(aff, updateSeq);\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                        fullMapString() + ']');\n            }\n            finally {\n                lock.writeLock().unlock();\n\n                ctx.database().checkpointReadUnlock();\n            }\n        }\n\n        // Wait for evictions.\n        waitForRent();\n    }",
            " 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542 +\n 543 +\n 544  \n 545  \n 546  \n 547 +\n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559 +\n 560 +\n 561  \n 562 +\n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586 +\n 587 +\n 588 +\n 589 +\n 590 +\n 591 +\n 592 +\n 593  \n 594 +\n 595 +\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut, boolean affReady)\n        throws IgniteCheckedException {\n        DiscoveryEvent discoEvt = exchFut.discoveryEvent();\n\n        ClusterState newState = exchFut.newClusterState();\n\n        treatAllPartAsLoc = (newState != null && newState == ClusterState.ACTIVE)\n            || (ctx.kernalContext().state().active()\n            && discoEvt.type() == EventType.EVT_NODE_JOINED\n            && discoEvt.eventNode().isLocal()\n            && !ctx.kernalContext().clientNode()\n        );\n\n        // Wait for rent outside of checkpoint lock.\n        waitForRent();\n\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        synchronized (ctx.exchange().interruptLock()) {\n            if (Thread.currentThread().isInterrupted())\n                throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n            try {\n                U.writeLock(lock);\n            }\n            catch (IgniteInterruptedCheckedException e) {\n                ctx.database().checkpointReadUnlock();\n\n                throw e;\n            }\n\n            try {\n                if (stopping)\n                    return;\n\n                GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n                assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n                    topVer + \", exchId=\" + exchId + ']';\n\n                if (exchId.isLeft() && exchFut.serverNodeDiscoveryEvent())\n                    removeNode(exchId.nodeId());\n\n                ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                cntrMap.clear();\n\n                boolean grpStarted = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                // If this is the oldest node.\n                if (oldest != null && (loc.equals(oldest) || grpStarted)) {\n                    if (node2part == null) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                                exchId + \", fullMap=\" + fullMapString() + ']');\n                    }\n                    else if (!node2part.valid()) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                                node2part + ']');\n                    }\n                    else if (!node2part.nodeId().equals(loc.id())) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                                exchId + \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n\n                if (grpStarted ||\n                    exchFut.discoveryEvent().type() == EVT_DISCOVERY_CUSTOM_EVT ||\n                    exchFut.serverNodeDiscoveryEvent()) {\n                    if (affReady)\n                        initPartitions0(exchFut, updateSeq);\n                    else {\n                        List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                        createPartitions(aff, updateSeq);\n                    }\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                        fullMapString() + ']');\n            }\n            finally {\n                lock.writeLock().unlock();\n\n                ctx.database().checkpointReadUnlock();\n            }\n        }\n\n        // Wait for evictions.\n        waitForRent();\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::createPartitions(List,long)",
            " 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  ",
            "    /**\n     * @param aff Affinity assignments.\n     * @param updateSeq Update sequence.\n     */\n    private void createPartitions(List<List<ClusterNode>> aff, long updateSeq) {\n        int num = grp.affinity().partitions();\n\n        for (int p = 0; p < num; p++) {\n            if (node2part != null && node2part.valid()) {\n                if (localNode(p, aff)) {\n                    // This will make sure that all non-existing partitions\n                    // will be created in MOVING state.\n                    GridDhtLocalPartition locPart = createPartition(p);\n\n                    updateSeq = updateLocal(p, locPart.state(), updateSeq);\n                }\n            }\n            // If this node's map is empty, we pre-create local partitions,\n            // so local map will be sent correctly during exchange.\n            else if (localNode(p, aff))\n                createPartition(p);\n        }\n    }",
            " 477  \n 478  \n 479  \n 480  \n 481  \n 482 +\n 483 +\n 484 +\n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  ",
            "    /**\n     * @param aff Affinity assignments.\n     * @param updateSeq Update sequence.\n     */\n    private void createPartitions(List<List<ClusterNode>> aff, long updateSeq) {\n        if (!grp.affinityNode())\n            return;\n\n        int num = grp.affinity().partitions();\n\n        for (int p = 0; p < num; p++) {\n            if (node2part != null && node2part.valid()) {\n                if (localNode(p, aff)) {\n                    // This will make sure that all non-existing partitions\n                    // will be created in MOVING state.\n                    GridDhtLocalPartition locPart = createPartition(p);\n\n                    updateSeq = updateLocal(p, locPart.state(), updateSeq);\n                }\n            }\n            // If this node's map is empty, we pre-create local partitions,\n            // so local map will be sent correctly during exchange.\n            else if (localNode(p, aff))\n                createPartition(p);\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::waitForRent()",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  ",
            "    /**\n     * Waits for renting partitions.\n     *\n     * @return {@code True} if mapping was changed.\n     * @throws IgniteCheckedException If failed.\n     */\n    private boolean waitForRent() throws IgniteCheckedException {\n        final long longOpDumpTimeout =\n            IgniteSystemProperties.getLong(IgniteSystemProperties.IGNITE_LONG_OPERATIONS_DUMP_TIMEOUT, 60_000);\n\n        int dumpCnt = 0;\n\n        GridDhtLocalPartition part;\n\n        for (int i = 0; i < locParts.length(); i++) {\n            part = locParts.get(i);\n\n            if (part == null)\n                continue;\n\n            GridDhtPartitionState state = part.state();\n\n            if (state == RENTING || state == EVICTED) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Waiting for renting partition: \" + part);\n\n                part.tryEvictAsync(false);\n\n                // Wait for partition to empty out.\n                if (longOpDumpTimeout > 0) {\n                    while (true) {\n                        try {\n                            part.rent(true).get(longOpDumpTimeout);\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            if (dumpCnt++ < DUMP_PENDING_OBJECTS_THRESHOLD) {\n                                U.warn(log, \"Failed to wait for partition eviction [\" +\n                                    \"topVer=\" + topVer +\n                                    \", group=\" + grp.cacheOrGroupName() +\n                                    \", part=\" + part.id() +\n                                    \", partState=\" + part.state() +\n                                    \", size=\" + part.internalSize() +\n                                    \", reservations=\" + part.reservations() +\n                                    \", grpReservations=\" + part.groupReserved() +\n                                    \", node=\" + ctx.localNodeId() + \"]\");\n\n                                if (IgniteSystemProperties.getBoolean(IGNITE_THREAD_DUMP_ON_EXCHANGE_TIMEOUT, false))\n                                    U.dumpThreads(log);\n                            }\n                        }\n                    }\n                }\n                else\n                    part.rent(true).get();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Finished waiting for renting partition: \" + part);\n            }\n        }\n\n        // Remove evicted partition.\n        lock.writeLock().lock();\n\n        try {\n            boolean changed = false;\n\n            for (int i = 0; i < locParts.length(); i++) {\n                part = locParts.get(i);\n\n                if (part == null)\n                    continue;\n\n                if (part.state() == EVICTED) {\n                    locParts.set(i, null);\n                    changed = true;\n                }\n            }\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 +\n 213 +\n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "    /**\n     * Waits for renting partitions.\n     *\n     * @return {@code True} if mapping was changed.\n     * @throws IgniteCheckedException If failed.\n     */\n    private boolean waitForRent() throws IgniteCheckedException {\n        if (!grp.affinityNode())\n            return false;\n\n        final long longOpDumpTimeout =\n            IgniteSystemProperties.getLong(IgniteSystemProperties.IGNITE_LONG_OPERATIONS_DUMP_TIMEOUT, 60_000);\n\n        int dumpCnt = 0;\n\n        GridDhtLocalPartition part;\n\n        for (int i = 0; i < locParts.length(); i++) {\n            part = locParts.get(i);\n\n            if (part == null)\n                continue;\n\n            GridDhtPartitionState state = part.state();\n\n            if (state == RENTING || state == EVICTED) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Waiting for renting partition: \" + part);\n\n                part.tryEvictAsync(false);\n\n                // Wait for partition to empty out.\n                if (longOpDumpTimeout > 0) {\n                    while (true) {\n                        try {\n                            part.rent(true).get(longOpDumpTimeout);\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            if (dumpCnt++ < DUMP_PENDING_OBJECTS_THRESHOLD) {\n                                U.warn(log, \"Failed to wait for partition eviction [\" +\n                                    \"topVer=\" + topVer +\n                                    \", group=\" + grp.cacheOrGroupName() +\n                                    \", part=\" + part.id() +\n                                    \", partState=\" + part.state() +\n                                    \", size=\" + part.internalSize() +\n                                    \", reservations=\" + part.reservations() +\n                                    \", grpReservations=\" + part.groupReserved() +\n                                    \", node=\" + ctx.localNodeId() + \"]\");\n\n                                if (IgniteSystemProperties.getBoolean(IGNITE_THREAD_DUMP_ON_EXCHANGE_TIMEOUT, false))\n                                    U.dumpThreads(log);\n                            }\n                        }\n                    }\n                }\n                else\n                    part.rent(true).get();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Finished waiting for renting partition: \" + part);\n            }\n        }\n\n        // Remove evicted partition.\n        lock.writeLock().lock();\n\n        try {\n            boolean changed = false;\n\n            for (int i = 0; i < locParts.length(); i++) {\n                part = locParts.get(i);\n\n                if (part == null)\n                    continue;\n\n                if (part.state() == EVICTED) {\n                    locParts.set(i, null);\n                    changed = true;\n                }\n            }\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridClientPartitionTopology::beforeExchange0(ClusterNode,GridDhtPartitionsExchangeFuture)",
            " 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 -\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  ",
            "    /**\n     * @param loc Local node.\n     * @param exchFut Exchange future.\n     */\n    private void beforeExchange0(ClusterNode loc, GridDhtPartitionsExchangeFuture exchFut) {\n        GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n        assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n            topVer + \", exchId=\" + exchId + ']';\n\n        if (!exchId.isJoined())\n            removeNode(exchId.nodeId());\n\n        // In case if node joins, get topology at the time of joining node.\n        ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n        assert oldest != null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n        long updateSeq = this.updateSeq.incrementAndGet();\n\n        // If this is the oldest node.\n        if (oldest.id().equals(loc.id()) || exchFut.dynamicCacheGroupStarted(grpId)) {\n            if (node2part == null) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                        exchId + \", fullMap=\" + fullMapString() + ']');\n            }\n            else if (!node2part.valid()) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                        node2part + ']');\n            }\n            else if (!node2part.nodeId().equals(loc.id())) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                        exchId + \", fullMap=\" + fullMapString() + ']');\n            }\n        }\n\n        consistencyCheck();\n\n        if (log.isDebugEnabled())\n            log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                fullMapString() + ']');\n    }",
            " 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 +\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  ",
            "    /**\n     * @param loc Local node.\n     * @param exchFut Exchange future.\n     */\n    private void beforeExchange0(ClusterNode loc, GridDhtPartitionsExchangeFuture exchFut) {\n        GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n        assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n            topVer + \", exchId=\" + exchId + ']';\n\n        if (exchId.isLeft() && exchFut.serverNodeDiscoveryEvent())\n            removeNode(exchId.nodeId());\n\n        // In case if node joins, get topology at the time of joining node.\n        ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n        assert oldest != null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n        long updateSeq = this.updateSeq.incrementAndGet();\n\n        // If this is the oldest node.\n        if (oldest.id().equals(loc.id()) || exchFut.dynamicCacheGroupStarted(grpId)) {\n            if (node2part == null) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                        exchId + \", fullMap=\" + fullMapString() + ']');\n            }\n            else if (!node2part.valid()) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                        node2part + ']');\n            }\n            else if (!node2part.nodeId().equals(loc.id())) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                        exchId + \", fullMap=\" + fullMapString() + ']');\n            }\n        }\n\n        consistencyCheck();\n\n        if (log.isDebugEnabled())\n            log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                fullMapString() + ']');\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::serverNodeDiscoveryEvent()",
            "1150  \n1151  \n1152  \n1153 -\n1154  \n1155  \n1156  \n1157  ",
            "    /**\n     * @return {@code True} if exchange triggered by server node join or fail.\n     */\n    private boolean serverNodeDiscoveryEvent() {\n        assert discoEvt != null;\n\n        return discoEvt.type() != EVT_DISCOVERY_CUSTOM_EVT && !CU.clientNode(discoEvt.eventNode());\n    }",
            "1150  \n1151  \n1152  \n1153 +\n1154  \n1155  \n1156  \n1157  ",
            "    /**\n     * @return {@code True} if exchange triggered by server node join or fail.\n     */\n    public boolean serverNodeDiscoveryEvent() {\n        assert discoEvt != null;\n\n        return discoEvt.type() != EVT_DISCOVERY_CUSTOM_EVT && !CU.clientNode(discoEvt.eventNode());\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::initPartitions0(GridDhtPartitionsExchangeFuture,long)",
            " 377  \n 378  \n 379  \n 380  \n 381  \n 382 -\n 383  \n 384 -\n 385  \n 386 -\n 387  \n 388 -\n 389 -\n 390 -\n 391 -\n 392 -\n 393 -\n 394 -\n 395 -\n 396 -\n 397 -\n 398  \n 399 -\n 400  \n 401 -\n 402  \n 403 -\n 404 -\n 405  \n 406 -\n 407  \n 408 -\n 409 -\n 410  \n 411 -\n 412 -\n 413 -\n 414  \n 415 -\n 416  \n 417 -\n 418 -\n 419  \n 420 -\n 421 -\n 422  \n 423 -\n 424  \n 425  \n 426  \n 427 -\n 428 -\n 429 -\n 430 -\n 431 -\n 432 -\n 433 -\n 434 -\n 435  \n 436 -\n 437  \n 438 -\n 439 -\n 440 -\n 441  \n 442 -\n 443 -\n 444  \n 445 -\n 446  \n 447 -\n 448 -\n 449 -\n 450  \n 451  \n 452 -\n 453 -\n 454 -\n 455 -\n 456 -\n 457  \n 458 -\n 459  \n 460 -\n 461  \n 462  \n 463 -\n 464  \n 465 -\n 466 -\n 467  \n 468  \n 469  ",
            "    /**\n     * @param exchFut Exchange future.\n     * @param updateSeq Update sequence.\n     */\n    private void initPartitions0(GridDhtPartitionsExchangeFuture exchFut, long updateSeq) {\n        ClusterNode loc = ctx.localNode();\n\n        ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n        GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n        assert topVer.equals(exchFut.topologyVersion()) :\n            \"Invalid topology [topVer=\" + topVer +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", futVer=\" + exchFut.topologyVersion() +\n                \", fut=\" + exchFut + ']';\n        assert grp.affinity().lastVersion().equals(exchFut.topologyVersion()) :\n            \"Invalid affinity [topVer=\" + grp.affinity().lastVersion() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", futVer=\" + exchFut.topologyVersion() +\n                \", fut=\" + exchFut + ']';\n\n        List<List<ClusterNode>> aff = grp.affinity().assignments(exchFut.topologyVersion());\n\n        int num = grp.affinity().partitions();\n\n        if (grp.rebalanceEnabled()) {\n            boolean added = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n            boolean first = added || (loc.equals(oldest) && loc.id().equals(exchId.nodeId()) && exchId.isJoined());\n\n            if (first) {\n                assert exchId.isJoined() || added;\n\n                for (int p = 0; p < num; p++) {\n                    if (localNode(p, aff)) {\n                        GridDhtLocalPartition locPart = createPartition(p);\n\n                        boolean owned = locPart.own();\n\n                        assert owned : \"Failed to own partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                            \", part=\" + locPart + ']';\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Owned partition for oldest node: \" + locPart);\n\n                        updateSeq = updateLocal(p, locPart.state(), updateSeq);\n                    }\n                }\n            }\n            else\n                createPartitions(aff, updateSeq);\n        }\n        else {\n            // If preloader is disabled, then we simply clear out\n            // the partitions this node is not responsible for.\n            for (int p = 0; p < num; p++) {\n                GridDhtLocalPartition locPart = localPartition(p, topVer, false, false);\n\n                boolean belongs = localNode(p, aff);\n\n                if (locPart != null) {\n                    if (!belongs) {\n                        GridDhtPartitionState state = locPart.state();\n\n                        if (state.active()) {\n                            locPart.rent(false);\n\n                            updateSeq = updateLocal(p, locPart.state(), updateSeq);\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Evicting partition with rebalancing disabled \" +\n                                    \"(it does not belong to affinity): \" + locPart);\n                        }\n                    }\n                    else\n                        locPart.own();\n                }\n                else if (belongs) {\n                    locPart = createPartition(p);\n\n                    locPart.own();\n\n                    updateLocal(p, locPart.state(), updateSeq);\n                }\n            }\n        }\n\n        if (node2part != null && node2part.valid())\n            checkEvictions(updateSeq, aff);\n\n        updateRebalanceVersion(aff);\n    }",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386 +\n 387  \n 388 +\n 389 +\n 390  \n 391 +\n 392  \n 393 +\n 394  \n 395 +\n 396 +\n 397 +\n 398 +\n 399 +\n 400 +\n 401 +\n 402 +\n 403 +\n 404 +\n 405  \n 406 +\n 407  \n 408 +\n 409 +\n 410  \n 411 +\n 412  \n 413 +\n 414 +\n 415  \n 416 +\n 417 +\n 418 +\n 419  \n 420 +\n 421  \n 422 +\n 423 +\n 424  \n 425 +\n 426 +\n 427  \n 428 +\n 429 +\n 430  \n 431  \n 432 +\n 433 +\n 434  \n 435 +\n 436 +\n 437 +\n 438 +\n 439 +\n 440  \n 441 +\n 442  \n 443 +\n 444 +\n 445 +\n 446  \n 447 +\n 448 +\n 449  \n 450 +\n 451  \n 452 +\n 453 +\n 454 +\n 455 +\n 456  \n 457 +\n 458 +\n 459  \n 460 +\n 461 +\n 462  \n 463 +\n 464  \n 465 +\n 466 +\n 467  \n 468  \n 469  \n 470 +\n 471 +\n 472 +\n 473  \n 474  \n 475  ",
            "    /**\n     * @param exchFut Exchange future.\n     * @param updateSeq Update sequence.\n     */\n    private void initPartitions0(GridDhtPartitionsExchangeFuture exchFut, long updateSeq) {\n        List<List<ClusterNode>> aff = grp.affinity().assignments(exchFut.topologyVersion());\n\n        if (grp.affinityNode()) {\n            ClusterNode loc = ctx.localNode();\n\n            ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n            GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n            assert topVer.equals(exchFut.topologyVersion()) :\n                \"Invalid topology [topVer=\" + topVer +\n                    \", grp=\" + grp.cacheOrGroupName() +\n                    \", futVer=\" + exchFut.topologyVersion() +\n                    \", fut=\" + exchFut + ']';\n            assert grp.affinity().lastVersion().equals(exchFut.topologyVersion()) :\n                \"Invalid affinity [topVer=\" + grp.affinity().lastVersion() +\n                    \", grp=\" + grp.cacheOrGroupName() +\n                    \", futVer=\" + exchFut.topologyVersion() +\n                    \", fut=\" + exchFut + ']';\n\n            int num = grp.affinity().partitions();\n\n            if (grp.rebalanceEnabled()) {\n                boolean added = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                boolean first = added || (loc.equals(oldest) && loc.id().equals(exchId.nodeId()) && exchId.isJoined());\n\n                if (first) {\n                    assert exchId.isJoined() || added;\n\n                    for (int p = 0; p < num; p++) {\n                        if (localNode(p, aff)) {\n                            GridDhtLocalPartition locPart = createPartition(p);\n\n                            boolean owned = locPart.own();\n\n                            assert owned : \"Failed to own partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                                \", part=\" + locPart + ']';\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Owned partition for oldest node: \" + locPart);\n\n                            updateSeq = updateLocal(p, locPart.state(), updateSeq);\n                        }\n                    }\n                }\n                else\n                    createPartitions(aff, updateSeq);\n            }\n            else {\n                // If preloader is disabled, then we simply clear out\n                // the partitions this node is not responsible for.\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition(p, topVer, false, false);\n\n                    boolean belongs = localNode(p, aff);\n\n                    if (locPart != null) {\n                        if (!belongs) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state.active()) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq);\n\n                                if (log.isDebugEnabled())\n                                    log.debug(\"Evicting partition with rebalancing disabled \" +\n                                        \"(it does not belong to affinity): \" + locPart);\n                            }\n                        }\n                        else\n                            locPart.own();\n                    }\n                    else if (belongs) {\n                        locPart = createPartition(p);\n\n                        locPart.own();\n\n                        updateLocal(p, locPart.state(), updateSeq);\n                    }\n                }\n            }\n\n            if (node2part != null && node2part.valid())\n                checkEvictions(updateSeq, aff);\n        }\n\n        updateRebalanceVersion(aff);\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onDone(AffinityTopologyVersion,Throwable)",
            "1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176 -\n1177 -\n1178 -\n1179  \n1180 -\n1181 -\n1182  \n1183 -\n1184 -\n1185 -\n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202 -\n1203  \n1204 -\n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  ",
            "    /** {@inheritDoc} */\n    @Override public boolean onDone(@Nullable AffinityTopologyVersion res, @Nullable Throwable err) {\n        boolean realExchange = !dummy && !forcePreload;\n\n        if (err == null &&\n            realExchange &&\n            !cctx.kernalContext().clientNode() &&\n            (serverNodeDiscoveryEvent() || affChangeMsg != null)) {\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                if (!cacheCtx.affinityNode() || cacheCtx.isLocal())\n                    continue;\n\n                cacheCtx.continuousQueries().flushBackupQueue(exchId.topologyVersion());\n            }\n       }\n\n        if (err == null && realExchange) {\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (grp.isLocal())\n                    continue;\n\n                try {\n                    if (centralizedAff)\n                        grp.topology().initPartitions(this);\n                }\n                catch (IgniteInterruptedCheckedException e) {\n                    U.error(log, \"Failed to initialize partitions.\", e);\n                }\n            }\n\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                GridCacheContext drCacheCtx = cacheCtx.isNear() ? cacheCtx.near().dht().context() : cacheCtx;\n\n                if (drCacheCtx.isDrEnabled()) {\n                    try {\n                        drCacheCtx.dr().onExchange(topologyVersion(), exchId.isLeft());\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to notify DR: \" + e, e);\n                    }\n                }\n            }\n\n            if (discoEvt.type() == EVT_NODE_LEFT ||\n                discoEvt.type() == EVT_NODE_FAILED ||\n                discoEvt.type() == EVT_NODE_JOINED)\n                detectLostPartitions();\n\n            Map<Integer, CacheValidation> m = U.newHashMap(cctx.cache().cacheGroups().size());\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                m.put(grp.groupId(), validateCacheGroup(grp, discoEvt.topologyNodes()));\n\n            grpValidRes = m;\n        }\n\n        cctx.cache().onExchangeDone(exchId.topologyVersion(), exchActions, err);\n\n        cctx.exchange().onExchangeDone(this, err);\n\n        if (exchActions != null && err == null)\n            exchActions.completeRequestFutures(cctx);\n\n        if (exchangeOnChangeGlobalState && err == null)\n            cctx.kernalContext().state().onExchangeDone();\n\n        if (super.onDone(res, err) && realExchange) {\n            if (log.isDebugEnabled())\n                log.debug(\"Completed partition exchange [localNode=\" + cctx.localNodeId() + \", exchange= \" + this +\n                    \", durationFromInit=\" + (U.currentTimeMillis() - initTs) + ']');\n\n            initFut.onDone(err == null);\n\n            if (exchId.isLeft()) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                    grp.affinityFunction().removeNode(exchId.nodeId());\n            }\n\n            exchActions = null;\n\n            if (discoEvt instanceof DiscoveryCustomEvent)\n                ((DiscoveryCustomEvent)discoEvt).customMessage(null);\n\n            cctx.exchange().lastFinishedFuture(this);\n\n            return true;\n        }\n\n        return dummy;\n    }",
            "1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176 +\n1177 +\n1178 +\n1179 +\n1180  \n1181 +\n1182  \n1183 +\n1184 +\n1185 +\n1186 +\n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203 +\n1204 +\n1205  \n1206 +\n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  ",
            "    /** {@inheritDoc} */\n    @Override public boolean onDone(@Nullable AffinityTopologyVersion res, @Nullable Throwable err) {\n        boolean realExchange = !dummy && !forcePreload;\n\n        if (err == null &&\n            realExchange &&\n            !cctx.kernalContext().clientNode() &&\n            (serverNodeDiscoveryEvent() || affChangeMsg != null)) {\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                if (!cacheCtx.affinityNode() || cacheCtx.isLocal())\n                    continue;\n\n                cacheCtx.continuousQueries().flushBackupQueue(exchId.topologyVersion());\n            }\n       }\n\n        if (err == null && realExchange) {\n            if (centralizedAff) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    try {\n                        grp.topology().initPartitions(this);\n                    }\n                    catch (IgniteInterruptedCheckedException e) {\n                        U.error(log, \"Failed to initialize partitions.\", e);\n                    }\n                }\n            }\n\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                GridCacheContext drCacheCtx = cacheCtx.isNear() ? cacheCtx.near().dht().context() : cacheCtx;\n\n                if (drCacheCtx.isDrEnabled()) {\n                    try {\n                        drCacheCtx.dr().onExchange(topologyVersion(), exchId.isLeft());\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to notify DR: \" + e, e);\n                    }\n                }\n            }\n\n            if (serverNodeDiscoveryEvent() &&\n                (discoEvt.type() == EVT_NODE_LEFT ||\n                discoEvt.type() == EVT_NODE_FAILED ||\n                discoEvt.type() == EVT_NODE_JOINED))\n                detectLostPartitions();\n\n            Map<Integer, CacheValidation> m = U.newHashMap(cctx.cache().cacheGroups().size());\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                m.put(grp.groupId(), validateCacheGroup(grp, discoEvt.topologyNodes()));\n\n            grpValidRes = m;\n        }\n\n        cctx.cache().onExchangeDone(exchId.topologyVersion(), exchActions, err);\n\n        cctx.exchange().onExchangeDone(this, err);\n\n        if (exchActions != null && err == null)\n            exchActions.completeRequestFutures(cctx);\n\n        if (exchangeOnChangeGlobalState && err == null)\n            cctx.kernalContext().state().onExchangeDone();\n\n        if (super.onDone(res, err) && realExchange) {\n            if (log.isDebugEnabled())\n                log.debug(\"Completed partition exchange [localNode=\" + cctx.localNodeId() + \", exchange= \" + this +\n                    \", durationFromInit=\" + (U.currentTimeMillis() - initTs) + ']');\n\n            initFut.onDone(err == null);\n\n            if (exchId.isLeft()) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                    grp.affinityFunction().removeNode(exchId.nodeId());\n            }\n\n            exchActions = null;\n\n            if (discoEvt instanceof DiscoveryCustomEvent)\n                ((DiscoveryCustomEvent)discoEvt).customMessage(null);\n\n            cctx.exchange().lastFinishedFuture(this);\n\n            return true;\n        }\n\n        return dummy;\n    }"
        ]
    ],
    "bebe4d872cd687f793596ee1b2067f777e9ce46e": [
        [
            "GridDhtPartitionsExchangeFuture::finishExchangeOnCoordinator(Collection)",
            "2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198 -\n2199 -\n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351  \n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  \n2386  \n2387  \n2388  \n2389  \n2390  \n2391  \n2392  \n2393  \n2394  \n2395  \n2396  \n2397 -\n2398 -\n2399 -\n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  ",
            "    /**\n     * @param sndResNodes Additional nodes to send finish message to.\n     */\n    private void finishExchangeOnCoordinator(@Nullable Collection<ClusterNode> sndResNodes) {\n        try {\n            AffinityTopologyVersion resTopVer = exchCtx.events().topologyVersion();\n\n            log.info(\"finishExchangeOnCoordinator [topVer=\" + initialVersion() +\n                \", resVer=\" + resTopVer + ']');\n\n            Map<Integer, CacheGroupAffinityMessage> idealAffDiff = null;\n\n            if (exchCtx.mergeExchanges()) {\n                synchronized (mux) {\n                    if (mergedJoinExchMsgs != null) {\n                        for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : mergedJoinExchMsgs.entrySet()) {\n                            msgs.put(e.getKey(), e.getValue());\n\n                            updatePartitionSingleMap(e.getKey(), e.getValue());\n                        }\n                    }\n                }\n\n                assert exchCtx.events().hasServerJoin() || exchCtx.events().hasServerLeft();\n\n                exchCtx.events().processEvents(this);\n\n                if (exchCtx.events().hasServerLeft())\n                    idealAffDiff = cctx.affinity().onServerLeftWithExchangeMergeProtocol(this);\n                else\n                    cctx.affinity().onServerJoinWithExchangeMergeProtocol(this, true);\n\n                for (CacheGroupDescriptor desc : cctx.affinity().cacheGroups().values()) {\n                    if (desc.config().getCacheMode() == CacheMode.LOCAL)\n                        continue;\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(desc.groupId());\n\n                    GridDhtPartitionTopology top = grp != null ? grp.topology() :\n                        cctx.exchange().clientTopology(desc.groupId());\n\n                    top.beforeExchange(this, true, true);\n                }\n            }\n\n            Map<Integer, CacheGroupAffinityMessage> joinedNodeAff = null;\n\n            for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : msgs.entrySet()) {\n                GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                // Apply update counters after all single messages are received.\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    GridDhtPartitionTopology top = grp != null ? grp.topology() :\n                        cctx.exchange().clientTopology(grpId);\n\n                    CachePartitionPartialCountersMap cntrs = msg.partitionUpdateCounters(grpId,\n                        top.partitions());\n\n                    if (cntrs != null)\n                        top.collectUpdateCounters(cntrs);\n                }\n\n                Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n                if (affReq != null) {\n                    joinedNodeAff = CacheGroupAffinityMessage.createAffinityMessages(cctx,\n                        resTopVer,\n                        affReq,\n                        joinedNodeAff);\n                }\n            }\n\n            for (CacheGroupContext grpCtx : cctx.cache().cacheGroups()) {\n                if (!grpCtx.isLocal())\n                    grpCtx.topology().applyUpdateCounters();\n            }\n\n            if (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                assert firstDiscoEvt instanceof DiscoveryCustomEvent;\n\n                if (activateCluster())\n                    assignPartitionsStates();\n\n                if (((DiscoveryCustomEvent)firstDiscoEvt).customMessage() instanceof DynamicCacheChangeBatch) {\n                    if (exchActions != null) {\n                        Set<String> caches = exchActions.cachesToResetLostPartitions();\n\n                        if (!F.isEmpty(caches))\n                            resetLostPartitions(caches);\n                    }\n                }\n            }\n            else {\n                if (exchCtx.events().hasServerJoin())\n                    assignPartitionsStates();\n\n                if (exchCtx.events().hasServerLeft())\n                    detectLostPartitions(resTopVer);\n            }\n\n            updateLastVersion(cctx.versions().last());\n\n            cctx.versions().onExchange(lastVer.get().order());\n\n            IgniteProductVersion minVer = exchCtx.events().discoveryCache().minimumNodeVersion();\n\n            GridDhtPartitionsFullMessage msg = createPartitionsMessage(true,\n                minVer.compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0);\n\n            if (exchCtx.mergeExchanges()) {\n                assert !centralizedAff;\n\n                msg.resultTopologyVersion(resTopVer);\n\n                if (exchCtx.events().hasServerLeft())\n                    msg.idealAffinityDiff(idealAffDiff);\n            }\n\n            msg.prepareMarshal(cctx);\n\n            synchronized (mux) {\n                finishState = new FinishState(crd.id(), resTopVer, msg);\n\n                state = ExchangeLocalState.DONE;\n            }\n\n            if (centralizedAff) {\n                assert !exchCtx.mergeExchanges();\n\n                IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut = cctx.affinity().initAffinityOnNodeLeft(this);\n\n                if (!fut.isDone()) {\n                    fut.listen(new IgniteInClosure<IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>>>() {\n                        @Override public void apply(IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut) {\n                            onAffinityInitialized(fut);\n                        }\n                    });\n                }\n                else\n                    onAffinityInitialized(fut);\n            }\n            else {\n                Set<ClusterNode> nodes;\n\n                Map<UUID, GridDhtPartitionsSingleMessage> mergedJoinExchMsgs0;\n\n                synchronized (mux) {\n                    srvNodes.remove(cctx.localNode());\n\n                    nodes = U.newHashSet(srvNodes.size());\n\n                    nodes.addAll(srvNodes);\n\n                    mergedJoinExchMsgs0 = mergedJoinExchMsgs;\n\n                    if (mergedJoinExchMsgs != null) {\n                        for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : mergedJoinExchMsgs.entrySet()) {\n                            if (e.getValue() != null) {\n                                ClusterNode node = cctx.discovery().node(e.getKey());\n\n                                if (node != null)\n                                    nodes.add(node);\n                            }\n                        }\n                    }\n\n                    if (!F.isEmpty(sndResNodes))\n                        nodes.addAll(sndResNodes);\n                }\n\n                IgniteCheckedException err = null;\n\n                if (stateChangeExchange()) {\n                    StateChangeRequest req = exchActions.stateChangeRequest();\n\n                    assert req != null : exchActions;\n\n                    boolean stateChangeErr = false;\n\n                    if (!F.isEmpty(changeGlobalStateExceptions)) {\n                        stateChangeErr = true;\n\n                        err = new IgniteCheckedException(\"Cluster state change failed.\");\n\n                        cctx.kernalContext().state().onStateChangeError(changeGlobalStateExceptions, req);\n                    }\n\n                    boolean active = !stateChangeErr && req.activate();\n\n                    ChangeGlobalStateFinishMessage stateFinishMsg = new ChangeGlobalStateFinishMessage(\n                        req.requestId(),\n                        active);\n\n                    cctx.discovery().sendCustomEvent(stateFinishMsg);\n                }\n\n                if (!nodes.isEmpty())\n                    sendAllPartitions(msg, nodes, mergedJoinExchMsgs0, joinedNodeAff);\n\n                onDone(exchCtx.events().topologyVersion(), err);\n\n                for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : pendingSingleMsgs.entrySet()) {\n                    log.info(\"Process pending message on coordinator [node=\" + e.getKey() +\n                        \", ver=\" + initialVersion() +\n                        \", resVer=\" + resTopVer + ']');\n\n                    processSingleMessage(e.getKey(), e.getValue());\n                }\n            }\n        }\n        catch (IgniteCheckedException e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else\n                onDone(e);\n        }\n    }",
            "2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216 +\n2217 +\n2218 +\n2219 +\n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351  \n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  \n2386  \n2387  \n2388  \n2389  \n2390  \n2391  \n2392  \n2393  \n2394  \n2395  \n2396  \n2397  \n2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415  \n2416  \n2417 +\n2418 +\n2419 +\n2420 +\n2421 +\n2422  \n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  ",
            "    /**\n     * @param sndResNodes Additional nodes to send finish message to.\n     */\n    private void finishExchangeOnCoordinator(@Nullable Collection<ClusterNode> sndResNodes) {\n        try {\n            AffinityTopologyVersion resTopVer = exchCtx.events().topologyVersion();\n\n            if (log.isInfoEnabled()) {\n                log.info(\"finishExchangeOnCoordinator [topVer=\" + initialVersion() +\n                    \", resVer=\" + resTopVer + ']');\n            }\n\n            Map<Integer, CacheGroupAffinityMessage> idealAffDiff = null;\n\n            if (exchCtx.mergeExchanges()) {\n                synchronized (mux) {\n                    if (mergedJoinExchMsgs != null) {\n                        for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : mergedJoinExchMsgs.entrySet()) {\n                            msgs.put(e.getKey(), e.getValue());\n\n                            updatePartitionSingleMap(e.getKey(), e.getValue());\n                        }\n                    }\n                }\n\n                assert exchCtx.events().hasServerJoin() || exchCtx.events().hasServerLeft();\n\n                exchCtx.events().processEvents(this);\n\n                if (exchCtx.events().hasServerLeft())\n                    idealAffDiff = cctx.affinity().onServerLeftWithExchangeMergeProtocol(this);\n                else\n                    cctx.affinity().onServerJoinWithExchangeMergeProtocol(this, true);\n\n                for (CacheGroupDescriptor desc : cctx.affinity().cacheGroups().values()) {\n                    if (desc.config().getCacheMode() == CacheMode.LOCAL)\n                        continue;\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(desc.groupId());\n\n                    GridDhtPartitionTopology top = grp != null ? grp.topology() :\n                        cctx.exchange().clientTopology(desc.groupId());\n\n                    top.beforeExchange(this, true, true);\n                }\n            }\n\n            Map<Integer, CacheGroupAffinityMessage> joinedNodeAff = null;\n\n            for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : msgs.entrySet()) {\n                GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                // Apply update counters after all single messages are received.\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    GridDhtPartitionTopology top = grp != null ? grp.topology() :\n                        cctx.exchange().clientTopology(grpId);\n\n                    CachePartitionPartialCountersMap cntrs = msg.partitionUpdateCounters(grpId,\n                        top.partitions());\n\n                    if (cntrs != null)\n                        top.collectUpdateCounters(cntrs);\n                }\n\n                Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n                if (affReq != null) {\n                    joinedNodeAff = CacheGroupAffinityMessage.createAffinityMessages(cctx,\n                        resTopVer,\n                        affReq,\n                        joinedNodeAff);\n                }\n            }\n\n            for (CacheGroupContext grpCtx : cctx.cache().cacheGroups()) {\n                if (!grpCtx.isLocal())\n                    grpCtx.topology().applyUpdateCounters();\n            }\n\n            if (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                assert firstDiscoEvt instanceof DiscoveryCustomEvent;\n\n                if (activateCluster())\n                    assignPartitionsStates();\n\n                if (((DiscoveryCustomEvent)firstDiscoEvt).customMessage() instanceof DynamicCacheChangeBatch) {\n                    if (exchActions != null) {\n                        Set<String> caches = exchActions.cachesToResetLostPartitions();\n\n                        if (!F.isEmpty(caches))\n                            resetLostPartitions(caches);\n                    }\n                }\n            }\n            else {\n                if (exchCtx.events().hasServerJoin())\n                    assignPartitionsStates();\n\n                if (exchCtx.events().hasServerLeft())\n                    detectLostPartitions(resTopVer);\n            }\n\n            updateLastVersion(cctx.versions().last());\n\n            cctx.versions().onExchange(lastVer.get().order());\n\n            IgniteProductVersion minVer = exchCtx.events().discoveryCache().minimumNodeVersion();\n\n            GridDhtPartitionsFullMessage msg = createPartitionsMessage(true,\n                minVer.compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0);\n\n            if (exchCtx.mergeExchanges()) {\n                assert !centralizedAff;\n\n                msg.resultTopologyVersion(resTopVer);\n\n                if (exchCtx.events().hasServerLeft())\n                    msg.idealAffinityDiff(idealAffDiff);\n            }\n\n            msg.prepareMarshal(cctx);\n\n            synchronized (mux) {\n                finishState = new FinishState(crd.id(), resTopVer, msg);\n\n                state = ExchangeLocalState.DONE;\n            }\n\n            if (centralizedAff) {\n                assert !exchCtx.mergeExchanges();\n\n                IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut = cctx.affinity().initAffinityOnNodeLeft(this);\n\n                if (!fut.isDone()) {\n                    fut.listen(new IgniteInClosure<IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>>>() {\n                        @Override public void apply(IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut) {\n                            onAffinityInitialized(fut);\n                        }\n                    });\n                }\n                else\n                    onAffinityInitialized(fut);\n            }\n            else {\n                Set<ClusterNode> nodes;\n\n                Map<UUID, GridDhtPartitionsSingleMessage> mergedJoinExchMsgs0;\n\n                synchronized (mux) {\n                    srvNodes.remove(cctx.localNode());\n\n                    nodes = U.newHashSet(srvNodes.size());\n\n                    nodes.addAll(srvNodes);\n\n                    mergedJoinExchMsgs0 = mergedJoinExchMsgs;\n\n                    if (mergedJoinExchMsgs != null) {\n                        for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : mergedJoinExchMsgs.entrySet()) {\n                            if (e.getValue() != null) {\n                                ClusterNode node = cctx.discovery().node(e.getKey());\n\n                                if (node != null)\n                                    nodes.add(node);\n                            }\n                        }\n                    }\n\n                    if (!F.isEmpty(sndResNodes))\n                        nodes.addAll(sndResNodes);\n                }\n\n                IgniteCheckedException err = null;\n\n                if (stateChangeExchange()) {\n                    StateChangeRequest req = exchActions.stateChangeRequest();\n\n                    assert req != null : exchActions;\n\n                    boolean stateChangeErr = false;\n\n                    if (!F.isEmpty(changeGlobalStateExceptions)) {\n                        stateChangeErr = true;\n\n                        err = new IgniteCheckedException(\"Cluster state change failed.\");\n\n                        cctx.kernalContext().state().onStateChangeError(changeGlobalStateExceptions, req);\n                    }\n\n                    boolean active = !stateChangeErr && req.activate();\n\n                    ChangeGlobalStateFinishMessage stateFinishMsg = new ChangeGlobalStateFinishMessage(\n                        req.requestId(),\n                        active);\n\n                    cctx.discovery().sendCustomEvent(stateFinishMsg);\n                }\n\n                if (!nodes.isEmpty())\n                    sendAllPartitions(msg, nodes, mergedJoinExchMsgs0, joinedNodeAff);\n\n                onDone(exchCtx.events().topologyVersion(), err);\n\n                for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : pendingSingleMsgs.entrySet()) {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Process pending message on coordinator [node=\" + e.getKey() +\n                            \", ver=\" + initialVersion() +\n                            \", resVer=\" + resTopVer + ']');\n                    }\n\n                    processSingleMessage(e.getKey(), e.getValue());\n                }\n            }\n        }\n        catch (IgniteCheckedException e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else\n                onDone(e);\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onNodeLeft(ClusterNode)",
            "2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944  \n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  \n2957  \n2958  \n2959  \n2960  \n2961  \n2962  \n2963  \n2964  \n2965  \n2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027 -\n3028 -\n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  \n3057  \n3058  \n3059  \n3060  \n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069  \n3070  \n3071  \n3072 -\n3073 -\n3074 -\n3075 -\n3076  \n3077  \n3078  \n3079  \n3080  \n3081  \n3082  \n3083  \n3084 -\n3085 -\n3086 -\n3087 -\n3088  \n3089  \n3090  \n3091  \n3092  \n3093  \n3094  \n3095  \n3096  \n3097  \n3098  \n3099  \n3100  \n3101  \n3102  \n3103  \n3104  \n3105  \n3106  \n3107  \n3108  ",
            "    /**\n     * Node left callback, processed from the same thread as {@link #onAffinityChangeMessage}.\n     *\n     * @param node Left node.\n     */\n    public void onNodeLeft(final ClusterNode node) {\n        if (isDone() || !enterBusy())\n            return;\n\n        cctx.mvcc().removeExplicitNodeLocks(node.id(), initialVersion());\n\n        try {\n            onDiscoveryEvent(new IgniteRunnable() {\n                @Override public void run() {\n                    if (isDone() || !enterBusy())\n                        return;\n\n                    try {\n                        boolean crdChanged = false;\n                        boolean allReceived = false;\n\n                        ClusterNode crd0;\n\n                        events().discoveryCache().updateAlives(node);\n\n                        InitNewCoordinatorFuture newCrdFut0;\n\n                        synchronized (mux) {\n                            newCrdFut0 = newCrdFut;\n                        }\n\n                        if (newCrdFut0 != null)\n                            newCrdFut0.onNodeLeft(node.id());\n\n                        synchronized (mux) {\n                            if (!srvNodes.remove(node))\n                                return;\n\n                            boolean rmvd = remaining.remove(node.id());\n\n                            if (!rmvd) {\n                                if (mergedJoinExchMsgs != null && mergedJoinExchMsgs.containsKey(node.id())) {\n                                    if (mergedJoinExchMsgs.get(node.id()) == null) {\n                                        mergedJoinExchMsgs.remove(node.id());\n\n                                        rmvd = true;\n                                    }\n                                }\n                            }\n\n                            if (node.equals(crd)) {\n                                crdChanged = true;\n\n                                crd = !srvNodes.isEmpty() ? srvNodes.get(0) : null;\n                            }\n\n                            switch (state) {\n                                case DONE:\n                                    return;\n\n                                case CRD:\n                                    allReceived = rmvd && (remaining.isEmpty() && F.isEmpty(mergedJoinExchMsgs));\n\n                                    break;\n\n                                case SRV:\n                                    assert crd != null;\n\n                                    if (crdChanged && crd.isLocal()) {\n                                        state = ExchangeLocalState.BECOME_CRD;\n\n                                        newCrdFut = new InitNewCoordinatorFuture(cctx);\n                                    }\n\n                                    break;\n                            }\n\n                            crd0 = crd;\n\n                            if (crd0 == null) {\n                                finishState = new FinishState(null, initialVersion(), null);\n                            }\n                        }\n\n                        if (crd0 == null) {\n                            onAllServersLeft();\n\n                            onDone(initialVersion());\n\n                            return;\n                        }\n\n                        if (crd0.isLocal()) {\n                            if (stateChangeExchange() && changeGlobalStateE != null)\n                                changeGlobalStateExceptions.put(crd0.id(), changeGlobalStateE);\n\n                            if (crdChanged) {\n                                log.info(\"Coordinator failed, node is new coordinator [ver=\" + initialVersion() +\n                                    \", prev=\" + node.id() + ']');\n\n                                assert newCrdFut != null;\n\n                                cctx.kernalContext().closure().callLocal(new Callable<Void>() {\n                                    @Override public Void call() throws Exception {\n                                        newCrdFut.init(GridDhtPartitionsExchangeFuture.this);\n\n                                        newCrdFut.listen(new CI1<IgniteInternalFuture>() {\n                                            @Override public void apply(IgniteInternalFuture fut) {\n                                                if (isDone())\n                                                    return;\n\n                                                Lock lock = cctx.io().readLock();\n\n                                                if (lock == null)\n                                                    return;\n\n                                                try {\n                                                    onBecomeCoordinator((InitNewCoordinatorFuture) fut);\n                                                }\n                                                finally {\n                                                    lock.unlock();\n                                                }\n                                            }\n                                        });\n\n                                        return null;\n                                    }\n                                }, GridIoPolicy.SYSTEM_POOL);\n\n                                return;\n                            }\n\n                            if (allReceived) {\n                                awaitSingleMapUpdates();\n\n                                onAllReceived(null);\n                            }\n                        }\n                        else {\n                            if (crdChanged) {\n                                for (Map.Entry<ClusterNode, GridDhtPartitionsFullMessage> m : fullMsgs.entrySet()) {\n                                    if (crd0.equals(m.getKey())) {\n                                        log.info(\"Coordinator changed, process pending full message [\" +\n                                            \"ver=\" + initialVersion() +\n                                            \", crd=\" + node.id() +\n                                            \", pendingMsgNode=\" + m.getKey() + ']');\n\n                                        processFullMessage(true, m.getKey(), m.getValue());\n\n                                        if (isDone())\n                                            return;\n                                    }\n                                }\n\n                                log.info(\"Coordinator changed, send partitions to new coordinator [\" +\n                                    \"ver=\" + initialVersion() +\n                                    \", crd=\" + node.id() +\n                                    \", newCrd=\" + crd0.id() + ']');\n\n                                sendPartitions(crd0);\n                            }\n                        }\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (reconnectOnError(e))\n                            onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n                        else\n                            U.error(log, \"Failed to process node left event: \" + e, e);\n                    }\n                    finally {\n                        leaveBusy();\n                    }\n                }\n            });\n        }\n        finally {\n            leaveBusy();\n        }\n    }",
            "2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  \n3057  \n3058  \n3059  \n3060  \n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069 +\n3070 +\n3071 +\n3072 +\n3073  \n3074  \n3075  \n3076  \n3077  \n3078  \n3079  \n3080  \n3081  \n3082  \n3083  \n3084  \n3085  \n3086  \n3087  \n3088  \n3089  \n3090  \n3091  \n3092  \n3093  \n3094  \n3095  \n3096  \n3097  \n3098  \n3099  \n3100  \n3101  \n3102  \n3103  \n3104  \n3105  \n3106  \n3107  \n3108  \n3109  \n3110  \n3111  \n3112  \n3113  \n3114  \n3115  \n3116 +\n3117 +\n3118 +\n3119 +\n3120 +\n3121 +\n3122  \n3123  \n3124  \n3125  \n3126  \n3127  \n3128  \n3129  \n3130 +\n3131 +\n3132 +\n3133 +\n3134 +\n3135 +\n3136  \n3137  \n3138  \n3139  \n3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  ",
            "    /**\n     * Node left callback, processed from the same thread as {@link #onAffinityChangeMessage}.\n     *\n     * @param node Left node.\n     */\n    public void onNodeLeft(final ClusterNode node) {\n        if (isDone() || !enterBusy())\n            return;\n\n        cctx.mvcc().removeExplicitNodeLocks(node.id(), initialVersion());\n\n        try {\n            onDiscoveryEvent(new IgniteRunnable() {\n                @Override public void run() {\n                    if (isDone() || !enterBusy())\n                        return;\n\n                    try {\n                        boolean crdChanged = false;\n                        boolean allReceived = false;\n\n                        ClusterNode crd0;\n\n                        events().discoveryCache().updateAlives(node);\n\n                        InitNewCoordinatorFuture newCrdFut0;\n\n                        synchronized (mux) {\n                            newCrdFut0 = newCrdFut;\n                        }\n\n                        if (newCrdFut0 != null)\n                            newCrdFut0.onNodeLeft(node.id());\n\n                        synchronized (mux) {\n                            if (!srvNodes.remove(node))\n                                return;\n\n                            boolean rmvd = remaining.remove(node.id());\n\n                            if (!rmvd) {\n                                if (mergedJoinExchMsgs != null && mergedJoinExchMsgs.containsKey(node.id())) {\n                                    if (mergedJoinExchMsgs.get(node.id()) == null) {\n                                        mergedJoinExchMsgs.remove(node.id());\n\n                                        rmvd = true;\n                                    }\n                                }\n                            }\n\n                            if (node.equals(crd)) {\n                                crdChanged = true;\n\n                                crd = !srvNodes.isEmpty() ? srvNodes.get(0) : null;\n                            }\n\n                            switch (state) {\n                                case DONE:\n                                    return;\n\n                                case CRD:\n                                    allReceived = rmvd && (remaining.isEmpty() && F.isEmpty(mergedJoinExchMsgs));\n\n                                    break;\n\n                                case SRV:\n                                    assert crd != null;\n\n                                    if (crdChanged && crd.isLocal()) {\n                                        state = ExchangeLocalState.BECOME_CRD;\n\n                                        newCrdFut = new InitNewCoordinatorFuture(cctx);\n                                    }\n\n                                    break;\n                            }\n\n                            crd0 = crd;\n\n                            if (crd0 == null) {\n                                finishState = new FinishState(null, initialVersion(), null);\n                            }\n                        }\n\n                        if (crd0 == null) {\n                            onAllServersLeft();\n\n                            onDone(initialVersion());\n\n                            return;\n                        }\n\n                        if (crd0.isLocal()) {\n                            if (stateChangeExchange() && changeGlobalStateE != null)\n                                changeGlobalStateExceptions.put(crd0.id(), changeGlobalStateE);\n\n                            if (crdChanged) {\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Coordinator failed, node is new coordinator [ver=\" + initialVersion() +\n                                        \", prev=\" + node.id() + ']');\n                                }\n\n                                assert newCrdFut != null;\n\n                                cctx.kernalContext().closure().callLocal(new Callable<Void>() {\n                                    @Override public Void call() throws Exception {\n                                        newCrdFut.init(GridDhtPartitionsExchangeFuture.this);\n\n                                        newCrdFut.listen(new CI1<IgniteInternalFuture>() {\n                                            @Override public void apply(IgniteInternalFuture fut) {\n                                                if (isDone())\n                                                    return;\n\n                                                Lock lock = cctx.io().readLock();\n\n                                                if (lock == null)\n                                                    return;\n\n                                                try {\n                                                    onBecomeCoordinator((InitNewCoordinatorFuture) fut);\n                                                }\n                                                finally {\n                                                    lock.unlock();\n                                                }\n                                            }\n                                        });\n\n                                        return null;\n                                    }\n                                }, GridIoPolicy.SYSTEM_POOL);\n\n                                return;\n                            }\n\n                            if (allReceived) {\n                                awaitSingleMapUpdates();\n\n                                onAllReceived(null);\n                            }\n                        }\n                        else {\n                            if (crdChanged) {\n                                for (Map.Entry<ClusterNode, GridDhtPartitionsFullMessage> m : fullMsgs.entrySet()) {\n                                    if (crd0.equals(m.getKey())) {\n                                        if (log.isInfoEnabled()) {\n                                            log.info(\"Coordinator changed, process pending full message [\" +\n                                                \"ver=\" + initialVersion() +\n                                                \", crd=\" + node.id() +\n                                                \", pendingMsgNode=\" + m.getKey() + ']');\n                                        }\n\n                                        processFullMessage(true, m.getKey(), m.getValue());\n\n                                        if (isDone())\n                                            return;\n                                    }\n                                }\n\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Coordinator changed, send partitions to new coordinator [\" +\n                                        \"ver=\" + initialVersion() +\n                                        \", crd=\" + node.id() +\n                                        \", newCrd=\" + crd0.id() + ']');\n                                }\n\n                                sendPartitions(crd0);\n                            }\n                        }\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (reconnectOnError(e))\n                            onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n                        else\n                            U.error(log, \"Failed to process node left event: \" + e, e);\n                    }\n                    finally {\n                        leaveBusy();\n                    }\n                }\n            });\n        }\n        finally {\n            leaveBusy();\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::addMergedJoinExchange(ClusterNode,GridDhtPartitionsSingleMessage)",
            "1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591 -\n1592 -\n1593  \n1594  \n1595  \n1596  \n1597  \n1598 -\n1599 -\n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608 -\n1609 -\n1610  \n1611  \n1612  \n1613  \n1614  \n1615  ",
            "    /**\n     * Records that this exchange if merged with another 'node join' exchange.\n     *\n     * @param node Joined node.\n     * @param msg Joined node message if already received.\n     * @return {@code True} if need to wait for message from joined server node.\n     */\n    private boolean addMergedJoinExchange(ClusterNode node, @Nullable GridDhtPartitionsSingleMessage msg) {\n        assert Thread.holdsLock(mux);\n        assert node != null;\n        assert state == ExchangeLocalState.CRD : state;\n\n        if (msg == null && newCrdFut != null)\n            msg = newCrdFut.joinExchangeMessage(node.id());\n\n        UUID nodeId = node.id();\n\n        boolean wait = false;\n\n        if (CU.clientNode(node)) {\n            if (msg != null)\n                waitAndReplyToNode(nodeId, msg);\n        }\n        else {\n            if (mergedJoinExchMsgs == null)\n                mergedJoinExchMsgs = new LinkedHashMap<>();\n\n            if (msg != null) {\n                assert msg.exchangeId().topologyVersion().equals(new AffinityTopologyVersion(node.order()));\n\n                log.info(\"Merge server join exchange, message received [curFut=\" + initialVersion() +\n                    \", node=\" + nodeId + ']');\n\n                mergedJoinExchMsgs.put(nodeId, msg);\n            }\n            else {\n                if (cctx.discovery().alive(nodeId)) {\n                    log.info(\"Merge server join exchange, wait for message [curFut=\" + initialVersion() +\n                        \", node=\" + nodeId + ']');\n\n                    wait = true;\n\n                    mergedJoinExchMsgs.put(nodeId, null);\n\n                    awaitMergedMsgs++;\n                }\n                else {\n                    log.info(\"Merge server join exchange, awaited node left [curFut=\" + initialVersion() +\n                        \", node=\" + nodeId + ']');\n                }\n            }\n        }\n\n        return wait;\n    }",
            "1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594 +\n1595 +\n1596 +\n1597 +\n1598  \n1599  \n1600  \n1601  \n1602  \n1603 +\n1604 +\n1605 +\n1606 +\n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615 +\n1616 +\n1617 +\n1618 +\n1619  \n1620  \n1621  \n1622  \n1623  \n1624  ",
            "    /**\n     * Records that this exchange if merged with another 'node join' exchange.\n     *\n     * @param node Joined node.\n     * @param msg Joined node message if already received.\n     * @return {@code True} if need to wait for message from joined server node.\n     */\n    private boolean addMergedJoinExchange(ClusterNode node, @Nullable GridDhtPartitionsSingleMessage msg) {\n        assert Thread.holdsLock(mux);\n        assert node != null;\n        assert state == ExchangeLocalState.CRD : state;\n\n        if (msg == null && newCrdFut != null)\n            msg = newCrdFut.joinExchangeMessage(node.id());\n\n        UUID nodeId = node.id();\n\n        boolean wait = false;\n\n        if (CU.clientNode(node)) {\n            if (msg != null)\n                waitAndReplyToNode(nodeId, msg);\n        }\n        else {\n            if (mergedJoinExchMsgs == null)\n                mergedJoinExchMsgs = new LinkedHashMap<>();\n\n            if (msg != null) {\n                assert msg.exchangeId().topologyVersion().equals(new AffinityTopologyVersion(node.order()));\n\n                if (log.isInfoEnabled()) {\n                    log.info(\"Merge server join exchange, message received [curFut=\" + initialVersion() +\n                        \", node=\" + nodeId + ']');\n                }\n\n                mergedJoinExchMsgs.put(nodeId, msg);\n            }\n            else {\n                if (cctx.discovery().alive(nodeId)) {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Merge server join exchange, wait for message [curFut=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n\n                    wait = true;\n\n                    mergedJoinExchMsgs.put(nodeId, null);\n\n                    awaitMergedMsgs++;\n                }\n                else {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Merge server join exchange, awaited node left [curFut=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n                }\n            }\n        }\n\n        return wait;\n    }"
        ],
        [
            "InitNewCoordinatorFuture::onNodeLeft(UUID)",
            " 292  \n 293  \n 294  \n 295  \n 296 -\n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "    /**\n     * @param nodeId Failed node ID.\n     */\n    public void onNodeLeft(UUID nodeId) {\n        log.info(\"Init new coordinator, node left [node=\" + nodeId + ']');\n\n        boolean done;\n\n        synchronized (this) {\n            done = awaited.remove(nodeId) && awaited.isEmpty();\n        }\n\n        if (done)\n            restoreStateFut.onDone();\n    }",
            " 307  \n 308  \n 309  \n 310  \n 311 +\n 312 +\n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  ",
            "    /**\n     * @param nodeId Failed node ID.\n     */\n    public void onNodeLeft(UUID nodeId) {\n        if (log.isInfoEnabled())\n            log.info(\"Init new coordinator, node left [node=\" + nodeId + ']');\n\n        boolean done;\n\n        synchronized (this) {\n            done = awaited.remove(nodeId) && awaited.isEmpty();\n        }\n\n        if (done)\n            restoreStateFut.onDone();\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onBecomeCoordinator(InitNewCoordinatorFuture)",
            "3110  \n3111  \n3112  \n3113  \n3114  \n3115  \n3116  \n3117  \n3118  \n3119  \n3120  \n3121  \n3122  \n3123  \n3124 -\n3125 -\n3126  \n3127  \n3128  \n3129  \n3130  \n3131  \n3132  \n3133  \n3134  \n3135  \n3136  \n3137  \n3138  \n3139  \n3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163 -\n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198 -\n3199 -\n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212 -\n3213 -\n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229 -\n3230 -\n3231  \n3232  \n3233  \n3234  \n3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  ",
            "    /**\n     * @param newCrdFut Coordinator initialization future.\n     */\n    private void onBecomeCoordinator(InitNewCoordinatorFuture newCrdFut) {\n        boolean allRcvd = false;\n\n        cctx.exchange().onCoordinatorInitialized();\n\n        if (newCrdFut.restoreState()) {\n            GridDhtPartitionsFullMessage fullMsg = newCrdFut.fullMessage();\n\n            assert msgs.isEmpty() : msgs;\n\n            if (fullMsg != null) {\n                log.info(\"New coordinator restored state [ver=\" + initialVersion() +\n                    \", resVer=\" + fullMsg.resultTopologyVersion() + ']');\n\n                synchronized (mux) {\n                    state = ExchangeLocalState.DONE;\n\n                    finishState = new FinishState(crd.id(), fullMsg.resultTopologyVersion(), fullMsg);\n                }\n\n                fullMsg.exchangeId(exchId);\n\n                processFullMessage(false, null, fullMsg);\n\n                Map<ClusterNode, GridDhtPartitionsSingleMessage> msgs = newCrdFut.messages();\n\n                if (!F.isEmpty(msgs)) {\n                    Map<Integer, CacheGroupAffinityMessage> joinedNodeAff = null;\n\n                    for (Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e : msgs.entrySet()) {\n                        this.msgs.put(e.getKey().id(), e.getValue());\n\n                        GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                        Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n                        if (!F.isEmpty(affReq)) {\n                            joinedNodeAff = CacheGroupAffinityMessage.createAffinityMessages(cctx,\n                                fullMsg.resultTopologyVersion(),\n                                affReq,\n                                joinedNodeAff);\n                        }\n                    }\n\n                    sendAllPartitions(fullMsg, msgs.keySet(), null, joinedNodeAff);\n                }\n\n                return;\n            }\n            else {\n                log.info(\"New coordinator restore state finished [ver=\" + initialVersion() + ']');\n\n                for (Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e : newCrdFut.messages().entrySet()) {\n                    GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                    if (!msg.client()) {\n                        msgs.put(e.getKey().id(), e.getValue());\n\n                        updatePartitionSingleMap(e.getKey().id(), msg);\n                    }\n                }\n            }\n\n            allRcvd = true;\n\n            synchronized (mux) {\n                remaining.clear(); // Do not process messages.\n\n                assert crd != null && crd.isLocal();\n\n                state = ExchangeLocalState.CRD;\n\n                assert mergedJoinExchMsgs == null;\n            }\n        }\n        else {\n            Set<UUID> remaining0 = null;\n\n            synchronized (mux) {\n                assert crd != null && crd.isLocal();\n\n                state = ExchangeLocalState.CRD;\n\n                assert mergedJoinExchMsgs == null;\n\n                log.info(\"New coordinator initialization finished [ver=\" + initialVersion() +\n                    \", remaining=\" + remaining + ']');\n\n                if (!remaining.isEmpty())\n                    remaining0 = new HashSet<>(remaining);\n            }\n\n            if (remaining0 != null) {\n                // It is possible that some nodes finished exchange with previous coordinator.\n                GridDhtPartitionsSingleRequest req = new GridDhtPartitionsSingleRequest(exchId);\n\n                for (UUID nodeId : remaining0) {\n                    try {\n                        if (!pendingSingleMsgs.containsKey(nodeId)) {\n                            log.info(\"New coordinator sends request [ver=\" + initialVersion() +\n                                \", node=\" + nodeId + ']');\n\n                            cctx.io().send(nodeId, req, SYSTEM_POOL);\n                        }\n                    }\n                    catch (ClusterTopologyCheckedException ignored) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Node left during partition exchange [nodeId=\" + nodeId +\n                                \", exchId=\" + exchId + ']');\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to request partitions from node: \" + nodeId, e);\n                    }\n                }\n\n                for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> m : pendingSingleMsgs.entrySet()) {\n                    log.info(\"New coordinator process pending message [ver=\" + initialVersion() +\n                        \", node=\" + m.getKey() + ']');\n\n                    processSingleMessage(m.getKey(), m.getValue());\n                }\n            }\n        }\n\n        if (allRcvd) {\n            awaitSingleMapUpdates();\n\n            onAllReceived(newCrdFut.messages().keySet());\n        }\n    }",
            "3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172 +\n3173 +\n3174 +\n3175 +\n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207 +\n3208 +\n3209 +\n3210 +\n3211 +\n3212 +\n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219 +\n3220 +\n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233  \n3234  \n3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  \n3243  \n3244  \n3245  \n3246  \n3247  \n3248  \n3249  \n3250  \n3251  \n3252  \n3253  \n3254  \n3255 +\n3256 +\n3257 +\n3258 +\n3259  \n3260  \n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269  \n3270  \n3271 +\n3272 +\n3273 +\n3274 +\n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  \n3283  \n3284  \n3285  \n3286  \n3287  \n3288  \n3289  \n3290 +\n3291 +\n3292 +\n3293 +\n3294  \n3295  \n3296  \n3297  \n3298  \n3299  \n3300  \n3301  \n3302  \n3303  \n3304  \n3305  ",
            "    /**\n     * @param newCrdFut Coordinator initialization future.\n     */\n    private void onBecomeCoordinator(InitNewCoordinatorFuture newCrdFut) {\n        boolean allRcvd = false;\n\n        cctx.exchange().onCoordinatorInitialized();\n\n        if (newCrdFut.restoreState()) {\n            GridDhtPartitionsFullMessage fullMsg = newCrdFut.fullMessage();\n\n            assert msgs.isEmpty() : msgs;\n\n            if (fullMsg != null) {\n                if (log.isInfoEnabled()) {\n                    log.info(\"New coordinator restored state [ver=\" + initialVersion() +\n                        \", resVer=\" + fullMsg.resultTopologyVersion() + ']');\n                }\n\n                synchronized (mux) {\n                    state = ExchangeLocalState.DONE;\n\n                    finishState = new FinishState(crd.id(), fullMsg.resultTopologyVersion(), fullMsg);\n                }\n\n                fullMsg.exchangeId(exchId);\n\n                processFullMessage(false, null, fullMsg);\n\n                Map<ClusterNode, GridDhtPartitionsSingleMessage> msgs = newCrdFut.messages();\n\n                if (!F.isEmpty(msgs)) {\n                    Map<Integer, CacheGroupAffinityMessage> joinedNodeAff = null;\n\n                    for (Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e : msgs.entrySet()) {\n                        this.msgs.put(e.getKey().id(), e.getValue());\n\n                        GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                        Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n                        if (!F.isEmpty(affReq)) {\n                            joinedNodeAff = CacheGroupAffinityMessage.createAffinityMessages(cctx,\n                                fullMsg.resultTopologyVersion(),\n                                affReq,\n                                joinedNodeAff);\n                        }\n                    }\n\n                    if (log.isInfoEnabled()) {\n                        log.info(\"New coordinator sends full message [ver=\" + initialVersion() +\n                            \", resVer=\" + fullMsg.resultTopologyVersion() +\n                            \", nodes=\" + F.nodeIds(msgs.keySet()) + ']');\n                    }\n\n                    sendAllPartitions(fullMsg, msgs.keySet(), null, joinedNodeAff);\n                }\n\n                return;\n            }\n            else {\n                if (log.isInfoEnabled())\n                    log.info(\"New coordinator restore state finished [ver=\" + initialVersion() + ']');\n\n                for (Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e : newCrdFut.messages().entrySet()) {\n                    GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                    if (!msg.client()) {\n                        msgs.put(e.getKey().id(), e.getValue());\n\n                        updatePartitionSingleMap(e.getKey().id(), msg);\n                    }\n                }\n            }\n\n            allRcvd = true;\n\n            synchronized (mux) {\n                remaining.clear(); // Do not process messages.\n\n                assert crd != null && crd.isLocal();\n\n                state = ExchangeLocalState.CRD;\n\n                assert mergedJoinExchMsgs == null;\n            }\n        }\n        else {\n            Set<UUID> remaining0 = null;\n\n            synchronized (mux) {\n                assert crd != null && crd.isLocal();\n\n                state = ExchangeLocalState.CRD;\n\n                assert mergedJoinExchMsgs == null;\n\n                if (log.isInfoEnabled()) {\n                    log.info(\"New coordinator initialization finished [ver=\" + initialVersion() +\n                        \", remaining=\" + remaining + ']');\n                }\n\n                if (!remaining.isEmpty())\n                    remaining0 = new HashSet<>(remaining);\n            }\n\n            if (remaining0 != null) {\n                // It is possible that some nodes finished exchange with previous coordinator.\n                GridDhtPartitionsSingleRequest req = new GridDhtPartitionsSingleRequest(exchId);\n\n                for (UUID nodeId : remaining0) {\n                    try {\n                        if (!pendingSingleMsgs.containsKey(nodeId)) {\n                            if (log.isInfoEnabled()) {\n                                log.info(\"New coordinator sends request [ver=\" + initialVersion() +\n                                    \", node=\" + nodeId + ']');\n                            }\n\n                            cctx.io().send(nodeId, req, SYSTEM_POOL);\n                        }\n                    }\n                    catch (ClusterTopologyCheckedException ignored) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Node left during partition exchange [nodeId=\" + nodeId +\n                                \", exchId=\" + exchId + ']');\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to request partitions from node: \" + nodeId, e);\n                    }\n                }\n\n                for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> m : pendingSingleMsgs.entrySet()) {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"New coordinator process pending message [ver=\" + initialVersion() +\n                            \", node=\" + m.getKey() + ']');\n                    }\n\n                    processSingleMessage(m.getKey(), m.getValue());\n                }\n            }\n        }\n\n        if (allRcvd) {\n            awaitSingleMapUpdates();\n\n            onAllReceived(newCrdFut.messages().keySet());\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::init(boolean)",
            " 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680 -\n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  ",
            "    /**\n     * Starts activity.\n     *\n     * @param newCrd {@code True} if node become coordinator on this exchange.\n     * @throws IgniteInterruptedCheckedException If interrupted.\n     */\n    public void init(boolean newCrd) throws IgniteInterruptedCheckedException {\n        if (isDone())\n            return;\n\n        assert !cctx.kernalContext().isDaemon();\n\n        initTs = U.currentTimeMillis();\n\n        U.await(evtLatch);\n\n        assert firstDiscoEvt != null : this;\n        assert exchId.nodeId().equals(firstDiscoEvt.eventNode().id()) : this;\n\n        try {\n            AffinityTopologyVersion topVer = initialVersion();\n\n            srvNodes = new ArrayList<>(firstEvtDiscoCache.serverNodes());\n\n            remaining.addAll(F.nodeIds(F.view(srvNodes, F.remoteNodes(cctx.localNodeId()))));\n\n            crd = srvNodes.isEmpty() ? null : srvNodes.get(0);\n\n            boolean crdNode = crd != null && crd.isLocal();\n\n            exchCtx = new ExchangeContext(crdNode, this);\n\n            assert state == null : state;\n\n            if (crdNode)\n                state = ExchangeLocalState.CRD;\n            else\n                state = cctx.kernalContext().clientNode() ? ExchangeLocalState.CLIENT : ExchangeLocalState.SRV;\n\n            if (exchLog.isInfoEnabled()) {\n                exchLog.info(\"Started exchange init [topVer=\" + topVer +\n                    \", crd=\" + crdNode +\n                    \", evt=\" + IgniteUtils.gridEventName(firstDiscoEvt.type()) +\n                    \", evtNode=\" + firstDiscoEvt.eventNode().id() +\n                    \", customEvt=\" + (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT ? ((DiscoveryCustomEvent)firstDiscoEvt).customMessage() : null) +\n                    \", allowMerge=\" + exchCtx.mergeExchanges() + ']');\n            }\n\n            ExchangeType exchange;\n\n            if (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                assert !exchCtx.mergeExchanges();\n\n                DiscoveryCustomMessage msg = ((DiscoveryCustomEvent)firstDiscoEvt).customMessage();\n\n                if (msg instanceof ChangeGlobalStateMessage) {\n                    assert exchActions != null && !exchActions.empty();\n\n                    exchange = onClusterStateChangeRequest(crdNode);\n                }\n                else if (msg instanceof DynamicCacheChangeBatch) {\n                    assert exchActions != null && !exchActions.empty();\n\n                    exchange = onCacheChangeRequest(crdNode);\n                }\n                else if (msg instanceof SnapshotDiscoveryMessage) {\n                    exchange = CU.clientNode(firstDiscoEvt.eventNode()) ?\n                        onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                }\n                else {\n                    assert affChangeMsg != null : this;\n\n                    exchange = onAffinityChangeRequest(crdNode);\n                }\n\n                initCoordinatorCaches(newCrd);\n            }\n            else {\n                if (firstDiscoEvt.type() == EVT_NODE_JOINED) {\n                    if (!firstDiscoEvt.eventNode().isLocal()) {\n                        Collection<DynamicCacheDescriptor> receivedCaches = cctx.cache().startReceivedCaches(\n                            firstDiscoEvt.eventNode().id(),\n                            topVer);\n\n                        cctx.affinity().initStartedCaches(crdNode, this, receivedCaches);\n                    }\n                    else\n                        initCachesOnLocalJoin();\n                }\n\n                initCoordinatorCaches(newCrd);\n\n                if (exchCtx.mergeExchanges()) {\n                    if (localJoinExchange()) {\n                        if (cctx.kernalContext().clientNode()) {\n                            onClientNodeEvent(crdNode);\n\n                            exchange = ExchangeType.CLIENT;\n                        }\n                        else {\n                            onServerNodeEvent(crdNode);\n\n                            exchange = ExchangeType.ALL;\n                        }\n                    }\n                    else {\n                        if (CU.clientNode(firstDiscoEvt.eventNode()))\n                            exchange = onClientNodeEvent(crdNode);\n                        else\n                            exchange = cctx.kernalContext().clientNode() ? ExchangeType.CLIENT : ExchangeType.ALL;\n                    }\n\n                    if (exchId.isLeft())\n                        onLeft();\n                }\n                else {\n                    exchange = CU.clientNode(firstDiscoEvt.eventNode()) ? onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                }\n            }\n\n            updateTopologies(crdNode);\n\n            switch (exchange) {\n                case ALL: {\n                    distributedExchange();\n\n                    break;\n                }\n\n                case CLIENT: {\n                    if (!exchCtx.mergeExchanges() && exchCtx.fetchAffinityOnJoin())\n                        initTopologies();\n\n                    clientOnlyExchange();\n\n                    break;\n                }\n\n                case NONE: {\n                    initTopologies();\n\n                    onDone(topVer);\n\n                    break;\n                }\n\n                default:\n                    assert false;\n            }\n\n            if (cctx.localNode().isClient())\n                tryToPerformLocalSnapshotOperation();\n\n            exchLog.info(\"Finished exchange init [topVer=\" + topVer + \", crd=\" + crdNode + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            onDone(e);\n\n            throw e;\n        }\n        catch (IgniteNeedReconnectException e) {\n            onDone(e);\n        }\n        catch (Throwable e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else {\n                U.error(log, \"Failed to reinitialize local partitions (preloading will be stopped): \" + exchId, e);\n\n                onDone(e);\n            }\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n    }",
            " 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680 +\n 681 +\n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  ",
            "    /**\n     * Starts activity.\n     *\n     * @param newCrd {@code True} if node become coordinator on this exchange.\n     * @throws IgniteInterruptedCheckedException If interrupted.\n     */\n    public void init(boolean newCrd) throws IgniteInterruptedCheckedException {\n        if (isDone())\n            return;\n\n        assert !cctx.kernalContext().isDaemon();\n\n        initTs = U.currentTimeMillis();\n\n        U.await(evtLatch);\n\n        assert firstDiscoEvt != null : this;\n        assert exchId.nodeId().equals(firstDiscoEvt.eventNode().id()) : this;\n\n        try {\n            AffinityTopologyVersion topVer = initialVersion();\n\n            srvNodes = new ArrayList<>(firstEvtDiscoCache.serverNodes());\n\n            remaining.addAll(F.nodeIds(F.view(srvNodes, F.remoteNodes(cctx.localNodeId()))));\n\n            crd = srvNodes.isEmpty() ? null : srvNodes.get(0);\n\n            boolean crdNode = crd != null && crd.isLocal();\n\n            exchCtx = new ExchangeContext(crdNode, this);\n\n            assert state == null : state;\n\n            if (crdNode)\n                state = ExchangeLocalState.CRD;\n            else\n                state = cctx.kernalContext().clientNode() ? ExchangeLocalState.CLIENT : ExchangeLocalState.SRV;\n\n            if (exchLog.isInfoEnabled()) {\n                exchLog.info(\"Started exchange init [topVer=\" + topVer +\n                    \", crd=\" + crdNode +\n                    \", evt=\" + IgniteUtils.gridEventName(firstDiscoEvt.type()) +\n                    \", evtNode=\" + firstDiscoEvt.eventNode().id() +\n                    \", customEvt=\" + (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT ? ((DiscoveryCustomEvent)firstDiscoEvt).customMessage() : null) +\n                    \", allowMerge=\" + exchCtx.mergeExchanges() + ']');\n            }\n\n            ExchangeType exchange;\n\n            if (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                assert !exchCtx.mergeExchanges();\n\n                DiscoveryCustomMessage msg = ((DiscoveryCustomEvent)firstDiscoEvt).customMessage();\n\n                if (msg instanceof ChangeGlobalStateMessage) {\n                    assert exchActions != null && !exchActions.empty();\n\n                    exchange = onClusterStateChangeRequest(crdNode);\n                }\n                else if (msg instanceof DynamicCacheChangeBatch) {\n                    assert exchActions != null && !exchActions.empty();\n\n                    exchange = onCacheChangeRequest(crdNode);\n                }\n                else if (msg instanceof SnapshotDiscoveryMessage) {\n                    exchange = CU.clientNode(firstDiscoEvt.eventNode()) ?\n                        onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                }\n                else {\n                    assert affChangeMsg != null : this;\n\n                    exchange = onAffinityChangeRequest(crdNode);\n                }\n\n                initCoordinatorCaches(newCrd);\n            }\n            else {\n                if (firstDiscoEvt.type() == EVT_NODE_JOINED) {\n                    if (!firstDiscoEvt.eventNode().isLocal()) {\n                        Collection<DynamicCacheDescriptor> receivedCaches = cctx.cache().startReceivedCaches(\n                            firstDiscoEvt.eventNode().id(),\n                            topVer);\n\n                        cctx.affinity().initStartedCaches(crdNode, this, receivedCaches);\n                    }\n                    else\n                        initCachesOnLocalJoin();\n                }\n\n                initCoordinatorCaches(newCrd);\n\n                if (exchCtx.mergeExchanges()) {\n                    if (localJoinExchange()) {\n                        if (cctx.kernalContext().clientNode()) {\n                            onClientNodeEvent(crdNode);\n\n                            exchange = ExchangeType.CLIENT;\n                        }\n                        else {\n                            onServerNodeEvent(crdNode);\n\n                            exchange = ExchangeType.ALL;\n                        }\n                    }\n                    else {\n                        if (CU.clientNode(firstDiscoEvt.eventNode()))\n                            exchange = onClientNodeEvent(crdNode);\n                        else\n                            exchange = cctx.kernalContext().clientNode() ? ExchangeType.CLIENT : ExchangeType.ALL;\n                    }\n\n                    if (exchId.isLeft())\n                        onLeft();\n                }\n                else {\n                    exchange = CU.clientNode(firstDiscoEvt.eventNode()) ? onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                }\n            }\n\n            updateTopologies(crdNode);\n\n            switch (exchange) {\n                case ALL: {\n                    distributedExchange();\n\n                    break;\n                }\n\n                case CLIENT: {\n                    if (!exchCtx.mergeExchanges() && exchCtx.fetchAffinityOnJoin())\n                        initTopologies();\n\n                    clientOnlyExchange();\n\n                    break;\n                }\n\n                case NONE: {\n                    initTopologies();\n\n                    onDone(topVer);\n\n                    break;\n                }\n\n                default:\n                    assert false;\n            }\n\n            if (cctx.localNode().isClient())\n                tryToPerformLocalSnapshotOperation();\n\n            if (exchLog.isInfoEnabled())\n                exchLog.info(\"Finished exchange init [topVer=\" + topVer + \", crd=\" + crdNode + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            onDone(e);\n\n            throw e;\n        }\n        catch (IgniteNeedReconnectException e) {\n            onDone(e);\n        }\n        catch (Throwable e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else {\n                U.error(log, \"Failed to reinitialize local partitions (preloading will be stopped): \" + exchId, e);\n\n                onDone(e);\n            }\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n    }"
        ],
        [
            "InitNewCoordinatorFuture::onMessage(ClusterNode,GridDhtPartitionsSingleMessage)",
            " 200  \n 201  \n 202  \n 203  \n 204  \n 205 -\n 206 -\n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  ",
            "    /**\n     * @param node Node.\n     * @param msg Message.\n     */\n    public void onMessage(ClusterNode node, GridDhtPartitionsSingleMessage msg) {\n        log.info(\"Init new coordinator, received response [node=\" + node.id() +\n            \", fullMsg=\" + (msg.finishMessage() != null) +\n            \", affReq=\" + !F.isEmpty(msg.cacheGroupsAffinityRequest()) + ']');\n\n        assert msg.restoreState() : msg;\n\n        boolean done = false;\n\n        synchronized (this) {\n            if (awaited.remove(node.id())) {\n                GridDhtPartitionsFullMessage fullMsg0 = msg.finishMessage();\n\n                if (fullMsg0 != null) {\n                    assert fullMsg == null || fullMsg.resultTopologyVersion().equals(fullMsg0.resultTopologyVersion());\n\n                    fullMsg  = fullMsg0;\n                }\n                else\n                    msgs.put(node, msg);\n\n                done = awaited.isEmpty();\n            }\n\n            if (done)\n                onAllReceived();\n        }\n\n        if (done)\n            restoreStateFut.onDone();\n    }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210 +\n 211 +\n 212 +\n 213 +\n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "    /**\n     * @param node Node.\n     * @param msg Message.\n     */\n    public void onMessage(ClusterNode node, GridDhtPartitionsSingleMessage msg) {\n        if (log.isInfoEnabled()) {\n            log.info(\"Init new coordinator, received response [node=\" + node.id() +\n                \", fullMsg=\" + (msg.finishMessage() != null) +\n                \", affReq=\" + !F.isEmpty(msg.cacheGroupsAffinityRequest()) + ']');\n        }\n\n        assert msg.restoreState() : msg;\n\n        boolean done = false;\n\n        synchronized (this) {\n            if (awaited.remove(node.id())) {\n                GridDhtPartitionsFullMessage fullMsg0 = msg.finishMessage();\n\n                if (fullMsg0 != null) {\n                    assert fullMsg == null || fullMsg.resultTopologyVersion().equals(fullMsg0.resultTopologyVersion());\n\n                    fullMsg  = fullMsg0;\n                }\n                else\n                    msgs.put(node, msg);\n\n                done = awaited.isEmpty();\n            }\n\n            if (done)\n                onAllReceived();\n        }\n\n        if (done)\n            restoreStateFut.onDone();\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processSinglePartitionRequest(ClusterNode,GridDhtPartitionsSingleRequest)",
            "2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522 -\n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532 -\n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544 -\n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552 -\n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560 -\n2561 -\n2562  \n2563  \n2564  \n2565  \n2566 -\n2567 -\n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598 -\n2599 -\n2600 -\n2601 -\n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processSinglePartitionRequest(ClusterNode node, GridDhtPartitionsSingleRequest msg) {\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            if (crd == null) {\n                log.info(\"Ignore partitions request, no coordinator [node=\" + node.id() + ']');\n\n                return;\n            }\n\n            switch (state) {\n                case DONE: {\n                    assert finishState != null;\n\n                    if (node.id().equals(finishState.crdId)) {\n                        log.info(\"Ignore partitions request, finished exchange with this coordinator: \" + msg);\n\n                        return;\n                    }\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD:\n                case BECOME_CRD: {\n                    log.info(\"Ignore partitions request, node is coordinator: \" + msg);\n\n                    return;\n                }\n\n                case CLIENT:\n                case SRV: {\n                    if (!cctx.discovery().alive(node)) {\n                        log.info(\"Ignore partitions request, node is not alive [node=\" + node.id() + ']');\n\n                        return;\n                    }\n\n                    if (msg.restoreState()) {\n                        if (!node.equals(crd)) {\n                            if (node.order() > crd.order()) {\n                                log.info(\"Received partitions request, change coordinator [oldCrd=\" + crd.id() +\n                                    \", newCrd=\" + node.id() + ']');\n\n                                crd = node; // Do not allow to process FullMessage from old coordinator.\n                            }\n                            else {\n                                log.info(\"Ignore restore state request, coordinator changed [oldCrd=\" + crd.id() +\n                                    \", newCrd=\" + node.id() + ']');\n\n                                return;\n                            }\n                        }\n                    }\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (msg.restoreState()) {\n            try {\n                assert msg.restoreExchangeId() != null : msg;\n\n                GridDhtPartitionsSingleMessage res = cctx.exchange().createPartitionsSingleMessage(\n                    msg.restoreExchangeId(),\n                    cctx.kernalContext().clientNode(),\n                    true,\n                    node.version().compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0,\n                    exchActions);\n\n                if (localJoinExchange() && finishState0 == null)\n                    res.cacheGroupsAffinityRequest(exchCtx.groupsAffinityRequestOnJoin());\n\n                res.restoreState(true);\n\n                log.info(\"Send restore state response [node=\" + node.id() +\n                    \", exchVer=\" + msg.restoreExchangeId().topologyVersion() +\n                    \", hasState=\" + (finishState0 != null) +\n                    \", affReq=\" + !F.isEmpty(res.cacheGroupsAffinityRequest()) + ']');\n\n                res.finishMessage(finishState0 != null ? finishState0.msg : null);\n\n                cctx.io().send(node, res, SYSTEM_POOL);\n            }\n            catch (ClusterTopologyCheckedException ignored) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Node left during partition exchange [nodeId=\" + node.id() + \", exchId=\" + exchId + ']');\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to send partitions message [node=\" + node + \", msg=\" + msg + ']', e);\n            }\n\n            return;\n        }\n\n        try {\n            sendLocalPartitions(node);\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to send message to coordinator: \" + e);\n        }\n    }",
            "2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544 +\n2545 +\n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555 +\n2556 +\n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568 +\n2569 +\n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577 +\n2578 +\n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586 +\n2587 +\n2588 +\n2589 +\n2590  \n2591  \n2592  \n2593  \n2594 +\n2595 +\n2596 +\n2597 +\n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628 +\n2629 +\n2630 +\n2631 +\n2632 +\n2633 +\n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processSinglePartitionRequest(ClusterNode node, GridDhtPartitionsSingleRequest msg) {\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            if (crd == null) {\n                if (log.isInfoEnabled())\n                    log.info(\"Ignore partitions request, no coordinator [node=\" + node.id() + ']');\n\n                return;\n            }\n\n            switch (state) {\n                case DONE: {\n                    assert finishState != null;\n\n                    if (node.id().equals(finishState.crdId)) {\n                        if (log.isInfoEnabled())\n                            log.info(\"Ignore partitions request, finished exchange with this coordinator: \" + msg);\n\n                        return;\n                    }\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD:\n                case BECOME_CRD: {\n                    if (log.isInfoEnabled())\n                        log.info(\"Ignore partitions request, node is coordinator: \" + msg);\n\n                    return;\n                }\n\n                case CLIENT:\n                case SRV: {\n                    if (!cctx.discovery().alive(node)) {\n                        if (log.isInfoEnabled())\n                            log.info(\"Ignore partitions request, node is not alive [node=\" + node.id() + ']');\n\n                        return;\n                    }\n\n                    if (msg.restoreState()) {\n                        if (!node.equals(crd)) {\n                            if (node.order() > crd.order()) {\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Received partitions request, change coordinator [oldCrd=\" + crd.id() +\n                                        \", newCrd=\" + node.id() + ']');\n                                }\n\n                                crd = node; // Do not allow to process FullMessage from old coordinator.\n                            }\n                            else {\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Ignore restore state request, coordinator changed [oldCrd=\" + crd.id() +\n                                        \", newCrd=\" + node.id() + ']');\n                                }\n\n                                return;\n                            }\n                        }\n                    }\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (msg.restoreState()) {\n            try {\n                assert msg.restoreExchangeId() != null : msg;\n\n                GridDhtPartitionsSingleMessage res = cctx.exchange().createPartitionsSingleMessage(\n                    msg.restoreExchangeId(),\n                    cctx.kernalContext().clientNode(),\n                    true,\n                    node.version().compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0,\n                    exchActions);\n\n                if (localJoinExchange() && finishState0 == null)\n                    res.cacheGroupsAffinityRequest(exchCtx.groupsAffinityRequestOnJoin());\n\n                res.restoreState(true);\n\n                if (log.isInfoEnabled()) {\n                    log.info(\"Send restore state response [node=\" + node.id() +\n                        \", exchVer=\" + msg.restoreExchangeId().topologyVersion() +\n                        \", hasState=\" + (finishState0 != null) +\n                        \", affReq=\" + !F.isEmpty(res.cacheGroupsAffinityRequest()) + ']');\n                }\n\n                res.finishMessage(finishState0 != null ? finishState0.msg : null);\n\n                cctx.io().send(node, res, SYSTEM_POOL);\n            }\n            catch (ClusterTopologyCheckedException ignored) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Node left during partition exchange [nodeId=\" + node.id() + \", exchId=\" + exchId + ']');\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to send partitions message [node=\" + node + \", msg=\" + msg + ']', e);\n            }\n\n            return;\n        }\n\n        try {\n            sendLocalPartitions(node);\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to send message to coordinator: \" + e);\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onAllReceived(Collection)",
            "2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173 -\n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  ",
            "    /**\n     * @param sndResNodes Additional nodes to send finish message to.\n     */\n    private void onAllReceived(@Nullable Collection<ClusterNode> sndResNodes) {\n        try {\n            assert crd.isLocal();\n\n            assert partHistSuppliers.isEmpty() : partHistSuppliers;\n\n            if (!exchCtx.mergeExchanges() && !crd.equals(events().discoveryCache().serverNodes().get(0))) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (!grp.isLocal())\n                        grp.topology().beforeExchange(this, !centralizedAff, false);\n                }\n            }\n\n            if (exchCtx.mergeExchanges()) {\n                log.info(\"Coordinator received all messages, try merge [ver=\" + initialVersion() + ']');\n\n                boolean finish = cctx.exchange().mergeExchangesOnCoordinator(this);\n\n                if (!finish)\n                    return;\n            }\n\n            finishExchangeOnCoordinator(sndResNodes);\n        }\n        catch (IgniteCheckedException e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else\n                onDone(e);\n        }\n    }",
            "2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190 +\n2191 +\n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  ",
            "    /**\n     * @param sndResNodes Additional nodes to send finish message to.\n     */\n    private void onAllReceived(@Nullable Collection<ClusterNode> sndResNodes) {\n        try {\n            assert crd.isLocal();\n\n            assert partHistSuppliers.isEmpty() : partHistSuppliers;\n\n            if (!exchCtx.mergeExchanges() && !crd.equals(events().discoveryCache().serverNodes().get(0))) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (!grp.isLocal())\n                        grp.topology().beforeExchange(this, !centralizedAff, false);\n                }\n            }\n\n            if (exchCtx.mergeExchanges()) {\n                if (log.isInfoEnabled())\n                    log.info(\"Coordinator received all messages, try merge [ver=\" + initialVersion() + ']');\n\n                boolean finish = cctx.exchange().mergeExchangesOnCoordinator(this);\n\n                if (!finish)\n                    return;\n            }\n\n            finishExchangeOnCoordinator(sndResNodes);\n        }\n        catch (IgniteCheckedException e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else\n                onDone(e);\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processSingleMessage(UUID,GridDhtPartitionsSingleMessage)",
            "1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845 -\n1846 -\n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868 -\n1869 -\n1870 -\n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878 -\n1879 -\n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  ",
            "    /**\n     * Note this method performs heavy updatePartitionSingleMap operation, this operation is moved out from the\n     * synchronized block. Only count of such updates {@link #pendingSingleUpdates} is managed under critical section.\n     *\n     * @param nodeId Sender node.\n     * @param msg Partition single message.\n     */\n    private void processSingleMessage(UUID nodeId, GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(nodeId, msg);\n\n            return;\n        }\n\n        boolean allReceived = false; // Received all expected messages.\n        boolean updateSingleMap = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            assert crd != null;\n\n            switch (state) {\n                case DONE: {\n                    log.info(\"Received single message, already done [ver=\" + initialVersion() +\n                        \", node=\" + nodeId + ']');\n\n                    assert finishState != null;\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD: {\n                    assert crd.isLocal() : crd;\n\n                    if (remaining.remove(nodeId)) {\n                        updateSingleMap = true;\n\n                        pendingSingleUpdates++;\n\n                        if (stateChangeExchange() && msg.getError() != null)\n                            changeGlobalStateExceptions.put(nodeId, msg.getError());\n\n                        allReceived = remaining.isEmpty();\n\n                        log.info(\"Coordinator received single message [ver=\" + initialVersion() +\n                            \", node=\" + nodeId +\n                            \", allReceived=\" + allReceived + ']');\n                    }\n\n                    break;\n                }\n\n                case SRV:\n                case BECOME_CRD: {\n                    log.info(\"Non-coordinator received single message [ver=\" + initialVersion() +\n                        \", node=\" + nodeId + \", state=\" + state + ']');\n\n                    pendingSingleMsgs.put(nodeId, msg);\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, nodeId);\n\n            return;\n        }\n\n        if (updateSingleMap) {\n            try {\n                // Do not update partition map, in case cluster transitioning to inactive state.\n                if (!deactivateCluster())\n                    updatePartitionSingleMap(nodeId, msg);\n            }\n            finally {\n                synchronized (mux) {\n                    assert pendingSingleUpdates > 0;\n\n                    pendingSingleUpdates--;\n\n                    if (pendingSingleUpdates == 0)\n                        mux.notifyAll();\n                }\n            }\n        }\n\n        if (allReceived) {\n            if (!awaitSingleMapUpdates())\n                return;\n\n            onAllReceived(null);\n        }\n    }",
            "1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856 +\n1857 +\n1858 +\n1859 +\n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881 +\n1882 +\n1883 +\n1884 +\n1885 +\n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893 +\n1894 +\n1895 +\n1896 +\n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  ",
            "    /**\n     * Note this method performs heavy updatePartitionSingleMap operation, this operation is moved out from the\n     * synchronized block. Only count of such updates {@link #pendingSingleUpdates} is managed under critical section.\n     *\n     * @param nodeId Sender node.\n     * @param msg Partition single message.\n     */\n    private void processSingleMessage(UUID nodeId, GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(nodeId, msg);\n\n            return;\n        }\n\n        boolean allReceived = false; // Received all expected messages.\n        boolean updateSingleMap = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            assert crd != null;\n\n            switch (state) {\n                case DONE: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Received single message, already done [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n\n                    assert finishState != null;\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD: {\n                    assert crd.isLocal() : crd;\n\n                    if (remaining.remove(nodeId)) {\n                        updateSingleMap = true;\n\n                        pendingSingleUpdates++;\n\n                        if (stateChangeExchange() && msg.getError() != null)\n                            changeGlobalStateExceptions.put(nodeId, msg.getError());\n\n                        allReceived = remaining.isEmpty();\n\n                        if (log.isInfoEnabled()) {\n                            log.info(\"Coordinator received single message [ver=\" + initialVersion() +\n                                \", node=\" + nodeId +\n                                \", allReceived=\" + allReceived + ']');\n                        }\n                    }\n\n                    break;\n                }\n\n                case SRV:\n                case BECOME_CRD: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Non-coordinator received single message [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + \", state=\" + state + ']');\n                    }\n\n                    pendingSingleMsgs.put(nodeId, msg);\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, nodeId);\n\n            return;\n        }\n\n        if (updateSingleMap) {\n            try {\n                // Do not update partition map, in case cluster transitioning to inactive state.\n                if (!deactivateCluster())\n                    updatePartitionSingleMap(nodeId, msg);\n            }\n            finally {\n                synchronized (mux) {\n                    assert pendingSingleUpdates > 0;\n\n                    pendingSingleUpdates--;\n\n                    if (pendingSingleUpdates == 0)\n                        mux.notifyAll();\n                }\n            }\n        }\n\n        if (allReceived) {\n            if (!awaitSingleMapUpdates())\n                return;\n\n            onAllReceived(null);\n        }\n    }"
        ],
        [
            "InitNewCoordinatorFuture::onAllReceived()",
            " 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 -\n 252 -\n 253 -\n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 -\n 274 -\n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "    /**\n     *\n     */\n    private void onAllReceived() {\n        if (fullMsg != null) {\n            AffinityTopologyVersion resVer = fullMsg.resultTopologyVersion();\n\n            for (Iterator<Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage>> it = msgs.entrySet().iterator(); it.hasNext();) {\n                Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e = it.next();\n\n                GridDhtPartitionExchangeId msgVer = joinedNodes.get(e.getKey().id());\n\n                if (msgVer != null) {\n                    assert msgVer.topologyVersion().compareTo(initTopVer) > 0 : msgVer;\n\n                    log.info(\"Process joined node message [resVer=\" + resVer +\n                        \", initTopVer=\" + initTopVer +\n                        \", msgVer=\" + msgVer.topologyVersion() + ']');\n\n                    if (msgVer.topologyVersion().compareTo(resVer) > 0)\n                        it.remove();\n                    else\n                        e.getValue().exchangeId(msgVer);\n                }\n            }\n        }\n        else {\n            for (Iterator<Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage>> it = msgs.entrySet().iterator(); it.hasNext();) {\n                Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e = it.next();\n\n                GridDhtPartitionExchangeId msgVer = joinedNodes.get(e.getKey().id());\n\n                if (msgVer != null) {\n                    it.remove();\n\n                    assert msgVer.topologyVersion().compareTo(initTopVer) > 0 : msgVer;\n\n                    log.info(\"Process joined node message [initTopVer=\" + initTopVer +\n                        \", msgVer=\" + msgVer.topologyVersion() + ']');\n\n                    if (joinExchMsgs == null)\n                        joinExchMsgs = new HashMap<>();\n\n                    e.getValue().exchangeId(msgVer);\n\n                    joinExchMsgs.put(e.getKey().id(), e.getValue());\n                }\n            }\n\n        }\n    }",
            " 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 +\n 283 +\n 284 +\n 285 +\n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  ",
            "    /**\n     *\n     */\n    private void onAllReceived() {\n        if (fullMsg != null) {\n            AffinityTopologyVersion resVer = fullMsg.resultTopologyVersion();\n\n            for (Iterator<Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage>> it = msgs.entrySet().iterator(); it.hasNext();) {\n                Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e = it.next();\n\n                GridDhtPartitionExchangeId msgVer = joinedNodes.get(e.getKey().id());\n\n                if (msgVer != null) {\n                    assert msgVer.topologyVersion().compareTo(initTopVer) > 0 : msgVer;\n\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Process joined node message [resVer=\" + resVer +\n                            \", initTopVer=\" + initTopVer +\n                            \", msgVer=\" + msgVer.topologyVersion() + ']');\n                    }\n\n                    if (msgVer.topologyVersion().compareTo(resVer) > 0)\n                        it.remove();\n                    else\n                        e.getValue().exchangeId(msgVer);\n                }\n            }\n        }\n        else {\n            for (Iterator<Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage>> it = msgs.entrySet().iterator(); it.hasNext();) {\n                Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e = it.next();\n\n                GridDhtPartitionExchangeId msgVer = joinedNodes.get(e.getKey().id());\n\n                if (msgVer != null) {\n                    it.remove();\n\n                    assert msgVer.topologyVersion().compareTo(initTopVer) > 0 : msgVer;\n\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Process joined node message [initTopVer=\" + initTopVer +\n                            \", msgVer=\" + msgVer.topologyVersion() + ']');\n                    }\n\n                    if (joinExchMsgs == null)\n                        joinExchMsgs = new HashMap<>();\n\n                    e.getValue().exchangeId(msgVer);\n\n                    joinExchMsgs.put(e.getKey().id(), e.getValue());\n                }\n            }\n\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processMergedMessage(ClusterNode,GridDhtPartitionsSingleMessage)",
            "1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687 -\n1688 -\n1689 -\n1690 -\n1691 -\n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processMergedMessage(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(node.id(), msg);\n\n            return;\n        }\n\n        boolean done = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            if (state == ExchangeLocalState.DONE) {\n                assert finishState != null;\n\n                finishState0 = finishState;\n            }\n            else {\n                boolean process = mergedJoinExchMsgs != null &&\n                    mergedJoinExchMsgs.containsKey(node.id()) &&\n                    mergedJoinExchMsgs.get(node.id()) == null;\n\n                log.info(\"Merge server join exchange, received message [curFut=\" + initialVersion() +\n                    \", node=\" + node.id() +\n                    \", msgVer=\" + msg.exchangeId().topologyVersion() +\n                    \", process=\" + process +\n                    \", awaited=\" + awaitMergedMsgs + ']');\n\n                if (process) {\n                    mergedJoinExchMsgs.put(node.id(), msg);\n\n                    assert awaitMergedMsgs > 0 : awaitMergedMsgs;\n\n                    awaitMergedMsgs--;\n\n                    done = awaitMergedMsgs == 0;\n                }\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, node.id());\n\n            return;\n        }\n\n        if (done)\n            finishExchangeOnCoordinator(null);\n    }",
            "1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696 +\n1697 +\n1698 +\n1699 +\n1700 +\n1701 +\n1702 +\n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processMergedMessage(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(node.id(), msg);\n\n            return;\n        }\n\n        boolean done = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            if (state == ExchangeLocalState.DONE) {\n                assert finishState != null;\n\n                finishState0 = finishState;\n            }\n            else {\n                boolean process = mergedJoinExchMsgs != null &&\n                    mergedJoinExchMsgs.containsKey(node.id()) &&\n                    mergedJoinExchMsgs.get(node.id()) == null;\n\n                if (log.isInfoEnabled()) {\n                    log.info(\"Merge server join exchange, received message [curFut=\" + initialVersion() +\n                        \", node=\" + node.id() +\n                        \", msgVer=\" + msg.exchangeId().topologyVersion() +\n                        \", process=\" + process +\n                        \", awaited=\" + awaitMergedMsgs + ']');\n                }\n\n                if (process) {\n                    mergedJoinExchMsgs.put(node.id(), msg);\n\n                    assert awaitMergedMsgs > 0 : awaitMergedMsgs;\n\n                    awaitMergedMsgs--;\n\n                    done = awaitMergedMsgs == 0;\n                }\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, node.id());\n\n            return;\n        }\n\n        if (done)\n            finishExchangeOnCoordinator(null);\n    }"
        ],
        [
            "InitNewCoordinatorFuture::init(GridDhtPartitionsExchangeFuture)",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 -\n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  ",
            "    /**\n     * @param exchFut Current future.\n     * @throws IgniteCheckedException If failed.\n     */\n    public void init(GridDhtPartitionsExchangeFuture exchFut) throws IgniteCheckedException {\n        initTopVer = exchFut.initialVersion();\n\n        GridCacheSharedContext cctx = exchFut.sharedContext();\n\n        restoreState = exchangeProtocolVersion(exchFut.context().events().discoveryCache().minimumNodeVersion()) > 1;\n\n        boolean newAff = exchFut.localJoinExchange();\n\n        IgniteInternalFuture<?> fut = cctx.affinity().initCoordinatorCaches(exchFut, newAff);\n\n        if (fut != null)\n            add(fut);\n\n        if (restoreState) {\n            DiscoCache curDiscoCache = cctx.discovery().discoCache();\n\n            DiscoCache discoCache = exchFut.events().discoveryCache();\n\n            List<ClusterNode> nodes = new ArrayList<>();\n\n            synchronized (this) {\n                for (ClusterNode node : discoCache.allNodes()) {\n                    if (!node.isLocal() && cctx.discovery().alive(node)) {\n                        awaited.add(node.id());\n\n                        nodes.add(node);\n                    }\n                }\n\n                if (exchFut.context().mergeExchanges() && !curDiscoCache.version().equals(discoCache.version())) {\n                    for (ClusterNode node : curDiscoCache.allNodes()) {\n                        if (discoCache.node(node.id()) == null) {\n                            if (exchangeProtocolVersion(node.version()) == 1)\n                                break;\n\n                            awaited.add(node.id());\n\n                            nodes.add(node);\n\n                            if (joinedNodes == null)\n                                joinedNodes = new HashMap<>();\n\n                            GridDhtPartitionExchangeId exchId = new GridDhtPartitionExchangeId(node.id(),\n                                EVT_NODE_JOINED,\n                                new AffinityTopologyVersion(node.order()));\n\n                            joinedNodes.put(node.id(), exchId);\n                        }\n                    }\n                }\n\n                if (joinedNodes == null)\n                    joinedNodes = Collections.emptyMap();\n\n                if (!awaited.isEmpty()) {\n                    restoreStateFut = new GridFutureAdapter();\n\n                    add(restoreStateFut);\n                }\n            }\n\n            log.info(\"Try restore exchange result [allNodes=\" + awaited +\n                \", joined=\" + joinedNodes.keySet() +  ']');\n\n            if (!nodes.isEmpty()) {\n                GridDhtPartitionsSingleRequest req = GridDhtPartitionsSingleRequest.restoreStateRequest(exchFut.exchangeId(),\n                    exchFut.exchangeId());\n\n                for (ClusterNode node : nodes) {\n                    try {\n                        GridDhtPartitionsSingleRequest sndReq = req;\n\n                        if (joinedNodes.containsKey(node.id())) {\n                            sndReq = GridDhtPartitionsSingleRequest.restoreStateRequest(\n                                joinedNodes.get(node.id()),\n                                exchFut.exchangeId());\n                        }\n\n                        cctx.io().send(node, sndReq, GridIoPolicy.SYSTEM_POOL);\n                    }\n                    catch (ClusterTopologyCheckedException e) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Failed to send partitions request, node failed: \" + node);\n\n                        onNodeLeft(node.id());\n                    }\n                }\n            }\n        }\n\n        markInitialized();\n    }",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 +\n 151 +\n 152 +\n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "    /**\n     * @param exchFut Current future.\n     * @throws IgniteCheckedException If failed.\n     */\n    public void init(GridDhtPartitionsExchangeFuture exchFut) throws IgniteCheckedException {\n        initTopVer = exchFut.initialVersion();\n\n        GridCacheSharedContext cctx = exchFut.sharedContext();\n\n        restoreState = exchangeProtocolVersion(exchFut.context().events().discoveryCache().minimumNodeVersion()) > 1;\n\n        boolean newAff = exchFut.localJoinExchange();\n\n        IgniteInternalFuture<?> fut = cctx.affinity().initCoordinatorCaches(exchFut, newAff);\n\n        if (fut != null)\n            add(fut);\n\n        if (restoreState) {\n            DiscoCache curDiscoCache = cctx.discovery().discoCache();\n\n            DiscoCache discoCache = exchFut.events().discoveryCache();\n\n            List<ClusterNode> nodes = new ArrayList<>();\n\n            synchronized (this) {\n                for (ClusterNode node : discoCache.allNodes()) {\n                    if (!node.isLocal() && cctx.discovery().alive(node)) {\n                        awaited.add(node.id());\n\n                        nodes.add(node);\n                    }\n                }\n\n                if (exchFut.context().mergeExchanges() && !curDiscoCache.version().equals(discoCache.version())) {\n                    for (ClusterNode node : curDiscoCache.allNodes()) {\n                        if (discoCache.node(node.id()) == null) {\n                            if (exchangeProtocolVersion(node.version()) == 1)\n                                break;\n\n                            awaited.add(node.id());\n\n                            nodes.add(node);\n\n                            if (joinedNodes == null)\n                                joinedNodes = new HashMap<>();\n\n                            GridDhtPartitionExchangeId exchId = new GridDhtPartitionExchangeId(node.id(),\n                                EVT_NODE_JOINED,\n                                new AffinityTopologyVersion(node.order()));\n\n                            joinedNodes.put(node.id(), exchId);\n                        }\n                    }\n                }\n\n                if (joinedNodes == null)\n                    joinedNodes = Collections.emptyMap();\n\n                if (!awaited.isEmpty()) {\n                    restoreStateFut = new GridFutureAdapter();\n\n                    add(restoreStateFut);\n                }\n            }\n\n            if (log.isInfoEnabled()) {\n                log.info(\"Try restore exchange result [allNodes=\" + awaited +\n                    \", joined=\" + joinedNodes.keySet() +  ']');\n            }\n\n            if (!nodes.isEmpty()) {\n                GridDhtPartitionsSingleRequest req = GridDhtPartitionsSingleRequest.restoreStateRequest(exchFut.exchangeId(),\n                    exchFut.exchangeId());\n\n                for (ClusterNode node : nodes) {\n                    try {\n                        GridDhtPartitionsSingleRequest sndReq = req;\n\n                        if (joinedNodes.containsKey(node.id())) {\n                            sndReq = GridDhtPartitionsSingleRequest.restoreStateRequest(\n                                joinedNodes.get(node.id()),\n                                exchFut.exchangeId());\n                        }\n\n                        cctx.io().send(node, sndReq, GridIoPolicy.SYSTEM_POOL);\n                    }\n                    catch (ClusterTopologyCheckedException e) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Failed to send partitions request, node failed: \" + node);\n\n                        onNodeLeft(node.id());\n                    }\n                }\n            }\n        }\n\n        markInitialized();\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onDone(AffinityTopologyVersion,Throwable)",
            "1395  \n1396  \n1397  \n1398  \n1399  \n1400 -\n1401 -\n1402 -\n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  ",
            "    /** {@inheritDoc} */\n    @Override public boolean onDone(@Nullable AffinityTopologyVersion res, @Nullable Throwable err) {\n        if (isDone() || !done.compareAndSet(false, true))\n            return false;\n\n        log.info(\"Finish exchange future [startVer=\" + initialVersion() +\n            \", resVer=\" + res +\n            \", err=\" + err + ']');\n\n        assert res != null || err != null;\n\n        if (err == null &&\n            !cctx.kernalContext().clientNode() &&\n            (serverNodeDiscoveryEvent() || affChangeMsg != null)) {\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                if (!cacheCtx.affinityNode() || cacheCtx.isLocal())\n                    continue;\n\n                cacheCtx.continuousQueries().flushBackupQueue(res);\n            }\n       }\n\n        if (err == null) {\n            if (centralizedAff) {\n                assert !exchCtx.mergeExchanges();\n\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    try {\n                        grp.topology().initPartitionsWhenAffinityReady(res, this);\n                    }\n                    catch (IgniteInterruptedCheckedException e) {\n                        U.error(log, \"Failed to initialize partitions.\", e);\n                    }\n                }\n            }\n\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                GridCacheContext drCacheCtx = cacheCtx.isNear() ? cacheCtx.near().dht().context() : cacheCtx;\n\n                if (drCacheCtx.isDrEnabled()) {\n                    try {\n                        drCacheCtx.dr().onExchange(res, exchId.isLeft());\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to notify DR: \" + e, e);\n                    }\n                }\n            }\n\n            if (serverNodeDiscoveryEvent())\n                detectLostPartitions(res);\n\n            Map<Integer, CacheValidation> m = U.newHashMap(cctx.cache().cacheGroups().size());\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                m.put(grp.groupId(), validateCacheGroup(grp, events().lastEvent().topologyNodes()));\n\n            grpValidRes = m;\n        }\n\n        tryToPerformLocalSnapshotOperation();\n\n        cctx.cache().onExchangeDone(initialVersion(), exchActions, err);\n\n        cctx.exchange().onExchangeDone(res, initialVersion(), err);\n\n        if (exchActions != null && err == null)\n            exchActions.completeRequestFutures(cctx);\n\n        if (stateChangeExchange() && err == null)\n            cctx.kernalContext().state().onStateChangeExchangeDone(exchActions.stateChangeRequest());\n\n        Map<T2<Integer, Integer>, Long> localReserved = partHistSuppliers.getReservations(cctx.localNodeId());\n\n        if (localReserved != null) {\n            for (Map.Entry<T2<Integer, Integer>, Long> e : localReserved.entrySet()) {\n                boolean success = cctx.database().reserveHistoryForPreloading(\n                    e.getKey().get1(), e.getKey().get2(), e.getValue());\n\n                if (!success) {\n                    // TODO: how to handle?\n                    err = new IgniteCheckedException(\"Could not reserve history\");\n                }\n            }\n        }\n\n        cctx.database().releaseHistoryForExchange();\n\n        if (err == null) {\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (!grp.isLocal())\n                    grp.topology().onExchangeDone(this, grp.affinity().readyAffinity(res), false);\n            }\n        }\n\n        if (super.onDone(res, err)) {\n            if (log.isDebugEnabled())\n                log.debug(\"Completed partition exchange [localNode=\" + cctx.localNodeId() + \", exchange= \" + this +\n                    \", durationFromInit=\" + (U.currentTimeMillis() - initTs) + ']');\n\n            initFut.onDone(err == null);\n\n            if (exchCtx != null && exchCtx.events().hasServerLeft()) {\n                ExchangeDiscoveryEvents evts = exchCtx.events();\n\n                for (DiscoveryEvent evt : exchCtx.events().events()) {\n                    if (evts.serverLeftEvent(evt)) {\n                        for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                            grp.affinityFunction().removeNode(evt.eventNode().id());\n                    }\n                }\n            }\n\n            exchActions = null;\n\n            if (firstDiscoEvt instanceof DiscoveryCustomEvent)\n                ((DiscoveryCustomEvent)firstDiscoEvt).customMessage(null);\n\n            if (err == null)\n                cctx.exchange().lastFinishedFuture(this);\n\n            return true;\n        }\n\n        return false;\n    }",
            "1396  \n1397  \n1398  \n1399  \n1400  \n1401 +\n1402 +\n1403 +\n1404 +\n1405 +\n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  ",
            "    /** {@inheritDoc} */\n    @Override public boolean onDone(@Nullable AffinityTopologyVersion res, @Nullable Throwable err) {\n        if (isDone() || !done.compareAndSet(false, true))\n            return false;\n\n        if (log.isInfoEnabled()) {\n            log.info(\"Finish exchange future [startVer=\" + initialVersion() +\n                \", resVer=\" + res +\n                \", err=\" + err + ']');\n        }\n\n        assert res != null || err != null;\n\n        if (err == null &&\n            !cctx.kernalContext().clientNode() &&\n            (serverNodeDiscoveryEvent() || affChangeMsg != null)) {\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                if (!cacheCtx.affinityNode() || cacheCtx.isLocal())\n                    continue;\n\n                cacheCtx.continuousQueries().flushBackupQueue(res);\n            }\n       }\n\n        if (err == null) {\n            if (centralizedAff) {\n                assert !exchCtx.mergeExchanges();\n\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    try {\n                        grp.topology().initPartitionsWhenAffinityReady(res, this);\n                    }\n                    catch (IgniteInterruptedCheckedException e) {\n                        U.error(log, \"Failed to initialize partitions.\", e);\n                    }\n                }\n            }\n\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                GridCacheContext drCacheCtx = cacheCtx.isNear() ? cacheCtx.near().dht().context() : cacheCtx;\n\n                if (drCacheCtx.isDrEnabled()) {\n                    try {\n                        drCacheCtx.dr().onExchange(res, exchId.isLeft());\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to notify DR: \" + e, e);\n                    }\n                }\n            }\n\n            if (serverNodeDiscoveryEvent())\n                detectLostPartitions(res);\n\n            Map<Integer, CacheValidation> m = U.newHashMap(cctx.cache().cacheGroups().size());\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                m.put(grp.groupId(), validateCacheGroup(grp, events().lastEvent().topologyNodes()));\n\n            grpValidRes = m;\n        }\n\n        tryToPerformLocalSnapshotOperation();\n\n        cctx.cache().onExchangeDone(initialVersion(), exchActions, err);\n\n        cctx.exchange().onExchangeDone(res, initialVersion(), err);\n\n        if (exchActions != null && err == null)\n            exchActions.completeRequestFutures(cctx);\n\n        if (stateChangeExchange() && err == null)\n            cctx.kernalContext().state().onStateChangeExchangeDone(exchActions.stateChangeRequest());\n\n        Map<T2<Integer, Integer>, Long> localReserved = partHistSuppliers.getReservations(cctx.localNodeId());\n\n        if (localReserved != null) {\n            for (Map.Entry<T2<Integer, Integer>, Long> e : localReserved.entrySet()) {\n                boolean success = cctx.database().reserveHistoryForPreloading(\n                    e.getKey().get1(), e.getKey().get2(), e.getValue());\n\n                if (!success) {\n                    // TODO: how to handle?\n                    err = new IgniteCheckedException(\"Could not reserve history\");\n                }\n            }\n        }\n\n        cctx.database().releaseHistoryForExchange();\n\n        if (err == null) {\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (!grp.isLocal())\n                    grp.topology().onExchangeDone(this, grp.affinity().readyAffinity(res), false);\n            }\n        }\n\n        if (super.onDone(res, err)) {\n            if (log.isDebugEnabled())\n                log.debug(\"Completed partition exchange [localNode=\" + cctx.localNodeId() + \", exchange= \" + this +\n                    \", durationFromInit=\" + (U.currentTimeMillis() - initTs) + ']');\n\n            initFut.onDone(err == null);\n\n            if (exchCtx != null && exchCtx.events().hasServerLeft()) {\n                ExchangeDiscoveryEvents evts = exchCtx.events();\n\n                for (DiscoveryEvent evt : exchCtx.events().events()) {\n                    if (evts.serverLeftEvent(evt)) {\n                        for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                            grp.affinityFunction().removeNode(evt.eventNode().id());\n                    }\n                }\n            }\n\n            exchActions = null;\n\n            if (firstDiscoEvt instanceof DiscoveryCustomEvent)\n                ((DiscoveryCustomEvent)firstDiscoEvt).customMessage(null);\n\n            if (err == null)\n                cctx.exchange().lastFinishedFuture(this);\n\n            return true;\n        }\n\n        return false;\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processFullMessage(boolean,ClusterNode,GridDhtPartitionsFullMessage)",
            "2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640 -\n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648 -\n2649  \n2650  \n2651  \n2652  \n2653  \n2654 -\n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662 -\n2663 -\n2664 -\n2665 -\n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675 -\n2676 -\n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692  \n2693  \n2694  \n2695 -\n2696 -\n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712  \n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processFullMessage(boolean checkCrd, ClusterNode node, GridDhtPartitionsFullMessage msg) {\n        try {\n            assert exchId.equals(msg.exchangeId()) : msg;\n            assert msg.lastVersion() != null : msg;\n\n            if (checkCrd) {\n                assert node != null;\n\n                synchronized (mux) {\n                    if (crd == null) {\n                        log.info(\"Ignore full message, all server nodes left: \" + msg);\n\n                        return;\n                    }\n\n                    switch (state) {\n                        case CRD:\n                        case BECOME_CRD: {\n                            log.info(\"Ignore full message, node is coordinator: \" + msg);\n\n                            return;\n                        }\n\n                        case DONE: {\n                            log.info(\"Ignore full message, future is done: \" + msg);\n\n                            return;\n                        }\n\n                        case SRV:\n                        case CLIENT: {\n                            if (!crd.equals(node)) {\n                                log.info(\"Received full message from non-coordinator [node=\" + node.id() +\n                                    \", nodeOrder=\" + node.order() +\n                                    \", crd=\" + crd.id() +\n                                    \", crdOrder=\" + crd.order() + ']');\n\n                                if (node.order() > crd.order())\n                                    fullMsgs.put(node, msg);\n\n                                return;\n                            }\n                            else {\n                                AffinityTopologyVersion resVer = msg.resultTopologyVersion() != null ? msg.resultTopologyVersion() : initialVersion();\n\n                                log.info(\"Received full message, will finish exchange [node=\" + node.id() +\n                                    \", resVer=\" + resVer + ']');\n\n                                finishState = new FinishState(crd.id(), resVer, msg);\n\n                                state = ExchangeLocalState.DONE;\n\n                                break;\n                            }\n                        }\n                    }\n                }\n            }\n            else\n                assert node == null : node;\n\n            AffinityTopologyVersion resTopVer = initialVersion();\n\n            if (exchCtx.mergeExchanges()) {\n                if (msg.resultTopologyVersion() != null && !initialVersion().equals(msg.resultTopologyVersion())) {\n                    log.info(\"Received full message, need merge [curFut=\" + initialVersion() +\n                        \", resVer=\" + msg.resultTopologyVersion() + ']');\n\n                    resTopVer = msg.resultTopologyVersion();\n\n                    if (cctx.exchange().mergeExchanges(this, msg)) {\n                        assert cctx.kernalContext().isStopping();\n\n                        return; // Node is stopping, no need to further process exchange.\n                    }\n\n                    assert resTopVer.equals(exchCtx.events().topologyVersion()) :  \"Unexpected result version [\" +\n                        \"msgVer=\" + resTopVer +\n                        \", locVer=\" + exchCtx.events().topologyVersion() + ']';\n                }\n\n                exchCtx.events().processEvents(this);\n\n                if (localJoinExchange())\n                    cctx.affinity().onLocalJoin(this, msg, resTopVer);\n                else {\n                    if (exchCtx.events().hasServerLeft())\n                        cctx.affinity().mergeExchangesOnServerLeft(this, msg);\n                    else\n                        cctx.affinity().onServerJoinWithExchangeMergeProtocol(this, false);\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal() || cacheGroupStopping(grp.groupId()))\n                            continue;\n\n                        grp.topology().beforeExchange(this, true, false);\n                    }\n                }\n            }\n            else if (localJoinExchange() && !exchCtx.fetchAffinityOnJoin())\n                cctx.affinity().onLocalJoin(this, msg, resTopVer);\n\n            updatePartitionFullMap(resTopVer, msg);\n\n            IgniteCheckedException err = null;\n\n            if (stateChangeExchange() && !F.isEmpty(msg.getErrorsMap())) {\n                err = new IgniteCheckedException(\"Cluster state change failed\");\n\n                cctx.kernalContext().state().onStateChangeError(msg.getErrorsMap(), exchActions.stateChangeRequest());\n            }\n\n            onDone(resTopVer, err);\n        }\n        catch (IgniteCheckedException e) {\n            onDone(e);\n        }\n    }",
            "2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673 +\n2674 +\n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682 +\n2683 +\n2684  \n2685  \n2686  \n2687  \n2688  \n2689 +\n2690 +\n2691  \n2692  \n2693  \n2694  \n2695  \n2696  \n2697  \n2698 +\n2699 +\n2700 +\n2701 +\n2702 +\n2703 +\n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712  \n2713 +\n2714 +\n2715 +\n2716 +\n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735 +\n2736 +\n2737 +\n2738 +\n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757  \n2758  \n2759  \n2760  \n2761  \n2762  \n2763  \n2764  \n2765  \n2766  \n2767  \n2768  \n2769  \n2770  \n2771  \n2772  \n2773  \n2774  \n2775  \n2776  \n2777  \n2778  \n2779  \n2780  \n2781  \n2782  \n2783  \n2784  \n2785  \n2786  \n2787  \n2788  \n2789  ",
            "    /**\n     * @param checkCrd If {@code true} checks that local node is exchange coordinator.\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processFullMessage(boolean checkCrd, ClusterNode node, GridDhtPartitionsFullMessage msg) {\n        try {\n            assert exchId.equals(msg.exchangeId()) : msg;\n            assert msg.lastVersion() != null : msg;\n\n            if (checkCrd) {\n                assert node != null;\n\n                synchronized (mux) {\n                    if (crd == null) {\n                        if (log.isInfoEnabled())\n                            log.info(\"Ignore full message, all server nodes left: \" + msg);\n\n                        return;\n                    }\n\n                    switch (state) {\n                        case CRD:\n                        case BECOME_CRD: {\n                            if (log.isInfoEnabled())\n                                log.info(\"Ignore full message, node is coordinator: \" + msg);\n\n                            return;\n                        }\n\n                        case DONE: {\n                            if (log.isInfoEnabled())\n                                log.info(\"Ignore full message, future is done: \" + msg);\n\n                            return;\n                        }\n\n                        case SRV:\n                        case CLIENT: {\n                            if (!crd.equals(node)) {\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Received full message from non-coordinator [node=\" + node.id() +\n                                        \", nodeOrder=\" + node.order() +\n                                        \", crd=\" + crd.id() +\n                                        \", crdOrder=\" + crd.order() + ']');\n                                }\n\n                                if (node.order() > crd.order())\n                                    fullMsgs.put(node, msg);\n\n                                return;\n                            }\n                            else {\n                                AffinityTopologyVersion resVer = msg.resultTopologyVersion() != null ? msg.resultTopologyVersion() : initialVersion();\n\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Received full message, will finish exchange [node=\" + node.id() +\n                                        \", resVer=\" + resVer + ']');\n                                }\n\n                                finishState = new FinishState(crd.id(), resVer, msg);\n\n                                state = ExchangeLocalState.DONE;\n\n                                break;\n                            }\n                        }\n                    }\n                }\n            }\n            else\n                assert node == null : node;\n\n            AffinityTopologyVersion resTopVer = initialVersion();\n\n            if (exchCtx.mergeExchanges()) {\n                if (msg.resultTopologyVersion() != null && !initialVersion().equals(msg.resultTopologyVersion())) {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Received full message, need merge [curFut=\" + initialVersion() +\n                            \", resVer=\" + msg.resultTopologyVersion() + ']');\n                    }\n\n                    resTopVer = msg.resultTopologyVersion();\n\n                    if (cctx.exchange().mergeExchanges(this, msg)) {\n                        assert cctx.kernalContext().isStopping();\n\n                        return; // Node is stopping, no need to further process exchange.\n                    }\n\n                    assert resTopVer.equals(exchCtx.events().topologyVersion()) :  \"Unexpected result version [\" +\n                        \"msgVer=\" + resTopVer +\n                        \", locVer=\" + exchCtx.events().topologyVersion() + ']';\n                }\n\n                exchCtx.events().processEvents(this);\n\n                if (localJoinExchange())\n                    cctx.affinity().onLocalJoin(this, msg, resTopVer);\n                else {\n                    if (exchCtx.events().hasServerLeft())\n                        cctx.affinity().mergeExchangesOnServerLeft(this, msg);\n                    else\n                        cctx.affinity().onServerJoinWithExchangeMergeProtocol(this, false);\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal() || cacheGroupStopping(grp.groupId()))\n                            continue;\n\n                        grp.topology().beforeExchange(this, true, false);\n                    }\n                }\n            }\n            else if (localJoinExchange() && !exchCtx.fetchAffinityOnJoin())\n                cctx.affinity().onLocalJoin(this, msg, resTopVer);\n\n            updatePartitionFullMap(resTopVer, msg);\n\n            IgniteCheckedException err = null;\n\n            if (stateChangeExchange() && !F.isEmpty(msg.getErrorsMap())) {\n                err = new IgniteCheckedException(\"Cluster state change failed\");\n\n                cctx.kernalContext().state().onStateChangeError(msg.getErrorsMap(), exchActions.stateChangeRequest());\n            }\n\n            onDone(resTopVer, err);\n        }\n        catch (IgniteCheckedException e) {\n            onDone(e);\n        }\n    }"
        ]
    ],
    "c7b4201e845a69a829f2e81ba2bcd4c33e9785eb": [
        [
            "GridDhtPartitionTopologyImpl::checkEvictions(long,AffinityAssignment)",
            "2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063 -\n2064 -\n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091 -\n2092 -\n2093 -\n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  ",
            "    /**\n     * @param updateSeq Update sequence.\n     * @param aff Affinity assignments.\n     * @return Checks if any of the local partitions need to be evicted.\n     */\n    private boolean checkEvictions(long updateSeq, AffinityAssignment aff) {\n        boolean changed = false;\n\n        UUID locId = ctx.localNodeId();\n\n        for (int p = 0; p < locParts.length(); p++) {\n            GridDhtLocalPartition part = locParts.get(p);\n\n            if (part == null)\n                continue;\n\n            GridDhtPartitionState state = part.state();\n\n            if (state.active()) {\n                List<ClusterNode> affNodes = aff.get(p);\n\n                if (!affNodes.contains(ctx.localNode())) {\n                    List<ClusterNode> nodes = nodes(p, aff.topologyVersion(), OWNING, null);\n                    Collection<UUID> nodeIds = F.nodeIds(nodes);\n\n                    // If all affinity nodes are owners, then evict partition from local node.\n                    if (nodeIds.containsAll(F.nodeIds(affNodes))) {\n                        part.reload(false);\n\n                        part.rent(false);\n\n                        updateSeq = updateLocal(part.id(), part.state(), updateSeq, aff.topologyVersion());\n\n                        changed = true;\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Evicted local partition (all affinity nodes are owners): \" + part);\n                    }\n                    else {\n                        int ownerCnt = nodeIds.size();\n                        int affCnt = affNodes.size();\n\n                        if (ownerCnt > affCnt) {\n                            // Sort by node orders in ascending order.\n                            Collections.sort(nodes, CU.nodeComparator(true));\n\n                            int diff = nodes.size() - affCnt;\n\n                            for (int i = 0; i < diff; i++) {\n                                ClusterNode n = nodes.get(i);\n\n                                if (locId.equals(n.id())) {\n                                    part.reload(false);\n\n                                    part.rent(false);\n\n                                    updateSeq = updateLocal(part.id(),\n                                        part.state(),\n                                        updateSeq,\n                                        aff.topologyVersion());\n\n                                    changed = true;\n\n                                    if (log.isDebugEnabled())\n                                        log.debug(\"Evicted local partition (this node is oldest non-affinity node): \" +\n                                            part);\n\n                                    break;\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        return changed;\n    }",
            "2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099 +\n2100 +\n2101 +\n2102 +\n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129 +\n2130 +\n2131 +\n2132 +\n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  ",
            "    /**\n     * @param updateSeq Update sequence.\n     * @param aff Affinity assignments.\n     * @return Checks if any of the local partitions need to be evicted.\n     */\n    private boolean checkEvictions(long updateSeq, AffinityAssignment aff) {\n        boolean changed = false;\n\n        UUID locId = ctx.localNodeId();\n\n        for (int p = 0; p < locParts.length(); p++) {\n            GridDhtLocalPartition part = locParts.get(p);\n\n            if (part == null)\n                continue;\n\n            GridDhtPartitionState state = part.state();\n\n            if (state.active()) {\n                List<ClusterNode> affNodes = aff.get(p);\n\n                if (!affNodes.contains(ctx.localNode())) {\n                    List<ClusterNode> nodes = nodes(p, aff.topologyVersion(), OWNING, null);\n                    Collection<UUID> nodeIds = F.nodeIds(nodes);\n\n                    // If all affinity nodes are owners, then evict partition from local node.\n                    if (nodeIds.containsAll(F.nodeIds(affNodes))) {\n                        part.reload(false);\n\n                        part.rent(false);\n\n                        updateSeq = updateLocal(part.id(), part.state(), updateSeq, aff.topologyVersion());\n\n                        changed = true;\n\n                        if (log.isDebugEnabled()) {\n                            log.debug(\"Evicted local partition (all affinity nodes are owners) [grp=\" + grp.cacheOrGroupName() +\n                                \", part=\" + part + ']');\n                        }\n                    }\n                    else {\n                        int ownerCnt = nodeIds.size();\n                        int affCnt = affNodes.size();\n\n                        if (ownerCnt > affCnt) {\n                            // Sort by node orders in ascending order.\n                            Collections.sort(nodes, CU.nodeComparator(true));\n\n                            int diff = nodes.size() - affCnt;\n\n                            for (int i = 0; i < diff; i++) {\n                                ClusterNode n = nodes.get(i);\n\n                                if (locId.equals(n.id())) {\n                                    part.reload(false);\n\n                                    part.rent(false);\n\n                                    updateSeq = updateLocal(part.id(),\n                                        part.state(),\n                                        updateSeq,\n                                        aff.topologyVersion());\n\n                                    changed = true;\n\n                                    if (log.isDebugEnabled()) {\n                                        log.debug(\"Evicted local partition (this node is oldest non-affinity node) [\" +\n                                            \"grp=\" + grp.cacheOrGroupName() + \", part=\" + part + ']');\n                                    }\n\n                                    break;\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        return changed;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::collectUpdateCounters(CachePartitionPartialCountersMap)",
            "1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460 -\n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  ",
            "    /** {@inheritDoc} */\n    @Override public void collectUpdateCounters(CachePartitionPartialCountersMap cntrMap) {\n        assert cntrMap != null;\n\n        long now = U.currentTimeMillis();\n\n        lock.writeLock().lock();\n\n        try {\n            long acquired = U.currentTimeMillis();\n\n            if (acquired - now >= 100) {\n                if (timeLog.isInfoEnabled())\n                    timeLog.info(\"Waited too long to acquire topology write lock \" +\n                        \"[cache=\" + grp.groupId() + \", waitTime=\" + (acquired - now) + ']');\n            }\n\n            if (stopping)\n                return;\n\n            for (int i = 0; i < cntrMap.size(); i++) {\n                int pId = cntrMap.partitionAt(i);\n\n                long initialUpdateCntr = cntrMap.initialUpdateCounterAt(i);\n                long updateCntr = cntrMap.updateCounterAt(i);\n\n                if (this.cntrMap.updateCounter(pId) < updateCntr) {\n                    this.cntrMap.initialUpdateCounter(pId, initialUpdateCntr);\n                    this.cntrMap.updateCounter(pId, updateCntr);\n                }\n            }\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            "1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490 +\n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  ",
            "    /** {@inheritDoc} */\n    @Override public void collectUpdateCounters(CachePartitionPartialCountersMap cntrMap) {\n        assert cntrMap != null;\n\n        long now = U.currentTimeMillis();\n\n        lock.writeLock().lock();\n\n        try {\n            long acquired = U.currentTimeMillis();\n\n            if (acquired - now >= 100) {\n                if (timeLog.isInfoEnabled())\n                    timeLog.info(\"Waited too long to acquire topology write lock \" +\n                        \"[grp=\" + grp.cacheOrGroupName() + \", waitTime=\" + (acquired - now) + ']');\n            }\n\n            if (stopping)\n                return;\n\n            for (int i = 0; i < cntrMap.size(); i++) {\n                int pId = cntrMap.partitionAt(i);\n\n                long initialUpdateCntr = cntrMap.initialUpdateCounterAt(i);\n                long updateCntr = cntrMap.updateCounterAt(i);\n\n                if (this.cntrMap.updateCounter(pId) < updateCntr) {\n                    this.cntrMap.initialUpdateCounter(pId, initialUpdateCntr);\n                    this.cntrMap.updateCounter(pId, updateCntr);\n                }\n            }\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::afterExchange(GridDhtPartitionsExchangeFuture)",
            " 586  \n 587 -\n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595 -\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612 -\n 613 -\n 614 -\n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625 -\n 626 -\n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641 -\n 642 -\n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 -\n 659 -\n 660  \n 661  \n 662 -\n 663 -\n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680 -\n 681 -\n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  ",
            "    /** {@inheritDoc} */\n    @Override public boolean afterExchange(GridDhtPartitionsExchangeFuture exchFut) throws IgniteCheckedException {\n        boolean changed = false;\n\n        int num = grp.affinity().partitions();\n\n        AffinityTopologyVersion topVer = exchFut.context().events().topologyVersion();\n\n        assert grp.affinity().lastVersion().equals(topVer) : \"Affinity is not initialized \" +\n            \"[topVer=\" + topVer +\n            \", affVer=\" + grp.affinity().lastVersion() +\n            \", fut=\" + exchFut + ']';\n\n        ctx.database().checkpointReadLock();\n\n        try {\n\n            lock.writeLock().lock();\n\n            try {\n                if (stopping)\n                    return false;\n\n                assert readyTopVer.initialized() : readyTopVer;\n                assert lastTopChangeVer.equals(readyTopVer);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map before afterExchange [exchId=\" + exchFut.exchangeId() + \", fullMap=\" +\n                        fullMapString() + ']');\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition0(p, topVer, false, false, false);\n\n                    if (partitionLocalNode(p, topVer)) {\n                        // This partition will be created during next topology event,\n                        // which obviously has not happened at this point.\n                        if (locPart == null) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Skipping local partition afterExchange (will not create): \" + p);\n\n                            continue;\n                        }\n\n                        GridDhtPartitionState state = locPart.state();\n\n                        if (state == MOVING) {\n                            if (grp.rebalanceEnabled()) {\n                                Collection<ClusterNode> owners = owners(p);\n\n                                // If there are no other owners, then become an owner.\n                                if (F.isEmpty(owners)) {\n                                    boolean owned = locPart.own();\n\n                                    assert owned : \"Failed to own partition [grp=\" + grp.cacheOrGroupName() + \", locPart=\" +\n                                        locPart + ']';\n\n                                    updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n\n                                    changed = true;\n\n                                    if (grp.eventRecordable(EVT_CACHE_REBALANCE_PART_DATA_LOST)) {\n                                        DiscoveryEvent discoEvt = exchFut.events().lastEvent();\n\n                                        grp.addRebalanceEvent(p,\n                                            EVT_CACHE_REBALANCE_PART_DATA_LOST,\n                                            discoEvt.eventNode(),\n                                            discoEvt.type(),\n                                            discoEvt.timestamp());\n                                    }\n\n                                    if (log.isDebugEnabled())\n                                        log.debug(\"Owned partition: \" + locPart);\n                                }\n                                else if (log.isDebugEnabled())\n                                    log.debug(\"Will not own partition (there are owners to rebalance from) [locPart=\" +\n                                        locPart + \", owners = \" + owners + ']');\n                            }\n                            else\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n                        }\n                    }\n                    else {\n                        if (locPart != null) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state == MOVING) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n\n                                changed = true;\n\n                                if (log.isDebugEnabled())\n                                    log.debug(\"Evicting moving partition (it does not belong to affinity): \" + locPart);\n                            }\n                        }\n                    }\n                }\n\n                AffinityAssignment aff = grp.affinity().readyAffinity(topVer);\n\n                if (node2part != null && node2part.valid())\n                    changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff.assignment());\n\n                consistencyCheck();\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        return changed;\n    }",
            " 593  \n 594 +\n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602 +\n 603 +\n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620 +\n 621 +\n 622 +\n 623 +\n 624 +\n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635 +\n 636 +\n 637 +\n 638 +\n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653 +\n 654 +\n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670 +\n 671 +\n 672 +\n 673 +\n 674  \n 675  \n 676 +\n 677 +\n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694 +\n 695 +\n 696 +\n 697 +\n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  ",
            "    /** {@inheritDoc} */\n    @Override public boolean afterExchange(GridDhtPartitionsExchangeFuture exchFut) {\n        boolean changed = false;\n\n        int num = grp.affinity().partitions();\n\n        AffinityTopologyVersion topVer = exchFut.context().events().topologyVersion();\n\n        assert grp.affinity().lastVersion().equals(topVer) : \"Affinity is not initialized \" +\n            \"[grp=\" + grp.cacheOrGroupName() +\n            \", topVer=\" + topVer +\n            \", affVer=\" + grp.affinity().lastVersion() +\n            \", fut=\" + exchFut + ']';\n\n        ctx.database().checkpointReadLock();\n\n        try {\n\n            lock.writeLock().lock();\n\n            try {\n                if (stopping)\n                    return false;\n\n                assert readyTopVer.initialized() : readyTopVer;\n                assert lastTopChangeVer.equals(readyTopVer);\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Partition map before afterExchange [grp=\" + grp.cacheOrGroupName() +\n                        \", exchId=\" + exchFut.exchangeId() +\n                        \", fullMap=\" + fullMapString() + ']');\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition0(p, topVer, false, false, false);\n\n                    if (partitionLocalNode(p, topVer)) {\n                        // This partition will be created during next topology event,\n                        // which obviously has not happened at this point.\n                        if (locPart == null) {\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Skipping local partition afterExchange (will not create) [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() + \", p=\" + p + ']');\n                            }\n\n                            continue;\n                        }\n\n                        GridDhtPartitionState state = locPart.state();\n\n                        if (state == MOVING) {\n                            if (grp.rebalanceEnabled()) {\n                                Collection<ClusterNode> owners = owners(p);\n\n                                // If there are no other owners, then become an owner.\n                                if (F.isEmpty(owners)) {\n                                    boolean owned = locPart.own();\n\n                                    assert owned : \"Failed to own partition [grp=\" + grp.cacheOrGroupName() +\n                                        \", locPart=\" + locPart + ']';\n\n                                    updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n\n                                    changed = true;\n\n                                    if (grp.eventRecordable(EVT_CACHE_REBALANCE_PART_DATA_LOST)) {\n                                        DiscoveryEvent discoEvt = exchFut.events().lastEvent();\n\n                                        grp.addRebalanceEvent(p,\n                                            EVT_CACHE_REBALANCE_PART_DATA_LOST,\n                                            discoEvt.eventNode(),\n                                            discoEvt.type(),\n                                            discoEvt.timestamp());\n                                    }\n\n                                    if (log.isDebugEnabled()) {\n                                        log.debug(\"Owned partition [grp=\" + grp.cacheOrGroupName() +\n                                            \", part=\" + locPart + ']');\n                                    }\n                                }\n                                else if (log.isDebugEnabled())\n                                    log.debug(\"Will not own partition (there are owners to rebalance from) [grp=\" + grp.cacheOrGroupName() +\n                                        \", locPart=\" + locPart + \", owners = \" + owners + ']');\n                            }\n                            else\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n                        }\n                    }\n                    else {\n                        if (locPart != null) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state == MOVING) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n\n                                changed = true;\n\n                                if (log.isDebugEnabled()) {\n                                    log.debug(\"Evicting moving partition (it does not belong to affinity) [\" +\n                                        \"grp=\" + grp.cacheOrGroupName() + \", part=\" + locPart + ']');\n                                }\n                            }\n                        }\n                    }\n                }\n\n                AffinityAssignment aff = grp.affinity().readyAffinity(topVer);\n\n                if (node2part != null && node2part.valid())\n                    changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff.assignment());\n\n                consistencyCheck();\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        return changed;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::updateLocal(int,GridDhtPartitionState,long,AffinityTopologyVersion)",
            "2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  ",
            "    /**\n     * Updates value for single partition.\n     *\n     * @param p Partition.\n     * @param state State.\n     * @param updateSeq Update sequence.\n     * @param affVer Affinity version.\n     * @return Update sequence.\n     */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    private long updateLocal(int p, GridDhtPartitionState state, long updateSeq, AffinityTopologyVersion affVer) {\n        assert lock.isWriteLockedByCurrentThread();\n\n        ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n        assert oldest != null || ctx.kernalContext().clientNode();\n\n        // If this node became the oldest node.\n        if (ctx.localNode().equals(oldest) && node2part != null) {\n            long seq = node2part.updateSequence();\n\n            if (seq != updateSeq) {\n                if (seq > updateSeq) {\n                    long seq0 = this.updateSeq.get();\n\n                    if (seq0 < seq) {\n                        // Update global counter if necessary.\n                        boolean b = this.updateSeq.compareAndSet(seq0, seq + 1);\n\n                        assert b : \"Invalid update sequence [updateSeq=\" + updateSeq +\n                            \", seq=\" + seq +\n                            \", curUpdateSeq=\" + this.updateSeq.get() +\n                            \", node2part=\" + node2part.toFullString() + ']';\n\n                        updateSeq = seq + 1;\n                    }\n                    else\n                        updateSeq = seq;\n                }\n\n                node2part.updateSequence(updateSeq);\n            }\n        }\n\n        if (node2part != null) {\n            UUID locNodeId = ctx.localNodeId();\n\n            GridDhtPartitionMap map = node2part.get(locNodeId);\n\n            if (map == null) {\n                map = new GridDhtPartitionMap(locNodeId,\n                    updateSeq,\n                    affVer,\n                    GridPartitionStateMap.EMPTY,\n                    false);\n\n                node2part.put(locNodeId, map);\n            }\n\n            map.updateSequence(updateSeq, affVer);\n\n            map.put(p, state);\n\n            if (!grp.isReplicated() && (state == MOVING || state == OWNING || state == RENTING)) {\n                AffinityAssignment assignment = grp.affinity().cachedAffinity(diffFromAffinityVer);\n\n                if (!assignment.getIds(p).contains(ctx.localNodeId())) {\n                    Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                    if (diffIds == null)\n                        diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                    diffIds.add(ctx.localNodeId());\n                }\n            }\n        }\n\n        return updateSeq;\n    }",
            "2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176 +\n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  ",
            "    /**\n     * Updates value for single partition.\n     *\n     * @param p Partition.\n     * @param state State.\n     * @param updateSeq Update sequence.\n     * @param affVer Affinity version.\n     * @return Update sequence.\n     */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    private long updateLocal(int p, GridDhtPartitionState state, long updateSeq, AffinityTopologyVersion affVer) {\n        assert lock.isWriteLockedByCurrentThread();\n\n        ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n        assert oldest != null || ctx.kernalContext().clientNode();\n\n        // If this node became the oldest node.\n        if (ctx.localNode().equals(oldest) && node2part != null) {\n            long seq = node2part.updateSequence();\n\n            if (seq != updateSeq) {\n                if (seq > updateSeq) {\n                    long seq0 = this.updateSeq.get();\n\n                    if (seq0 < seq) {\n                        // Update global counter if necessary.\n                        boolean b = this.updateSeq.compareAndSet(seq0, seq + 1);\n\n                        assert b : \"Invalid update sequence [updateSeq=\" + updateSeq +\n                            \", grp=\" + grp.cacheOrGroupName() +\n                            \", seq=\" + seq +\n                            \", curUpdateSeq=\" + this.updateSeq.get() +\n                            \", node2part=\" + node2part.toFullString() + ']';\n\n                        updateSeq = seq + 1;\n                    }\n                    else\n                        updateSeq = seq;\n                }\n\n                node2part.updateSequence(updateSeq);\n            }\n        }\n\n        if (node2part != null) {\n            UUID locNodeId = ctx.localNodeId();\n\n            GridDhtPartitionMap map = node2part.get(locNodeId);\n\n            if (map == null) {\n                map = new GridDhtPartitionMap(locNodeId,\n                    updateSeq,\n                    affVer,\n                    GridPartitionStateMap.EMPTY,\n                    false);\n\n                node2part.put(locNodeId, map);\n            }\n\n            map.updateSequence(updateSeq, affVer);\n\n            map.put(p, state);\n\n            if (!grp.isReplicated() && (state == MOVING || state == OWNING || state == RENTING)) {\n                AffinityAssignment assignment = grp.affinity().cachedAffinity(diffFromAffinityVer);\n\n                if (!assignment.getIds(p).contains(ctx.localNodeId())) {\n                    Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                    if (diffIds == null)\n                        diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                    diffIds.add(ctx.localNodeId());\n                }\n            }\n        }\n\n        return updateSeq;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(AffinityTopologyVersion,GridDhtPartitionFullMap,CachePartitionFullCountersMap,Set,AffinityTopologyVersion)",
            "1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205 -\n1206 -\n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242 -\n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252 -\n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269 -\n1270  \n1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  \n1288  \n1289  \n1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298 -\n1299 -\n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314 -\n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429 -\n1430 -\n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable CachePartitionFullCountersMap incomeCntrMap,\n        Set<Integer> partsToReload,\n        @Nullable AffinityTopologyVersion msgTopVer) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchangeVer + \", parts=\" + fullMapString() + ']');\n\n        assert partMap != null;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (stopping || !lastTopChangeVer.initialized() ||\n                    // Ignore message not-related to exchange if exchange is in progress.\n                    (exchangeVer == null && !lastTopChangeVer.equals(readyTopVer)))\n                    return false;\n\n                if (incomeCntrMap != null) {\n                    // update local counters in partitions\n                    for (int i = 0; i < locParts.length(); i++) {\n                        GridDhtLocalPartition part = locParts.get(i);\n\n                        if (part == null)\n                            continue;\n\n                        if (part.state() == OWNING || part.state() == MOVING) {\n                            long updCntr = incomeCntrMap.updateCounter(part.id());\n\n                            if (updCntr != 0 && updCntr > part.updateCounter())\n                                part.updateCounter(updCntr);\n                        }\n                    }\n                }\n\n                if (exchangeVer != null) {\n                    // Ignore if exchange already finished or new exchange started.\n                    if (readyTopVer.compareTo(exchangeVer) > 0 || lastTopChangeVer.compareTo(exchangeVer) > 0) {\n                        U.warn(log, \"Stale exchange id for full partition map update (will ignore) [\" +\n                            \"lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exchVer=\" + exchangeVer + ']');\n\n                        return false;\n                    }\n                }\n\n                if (msgTopVer != null && lastTopChangeVer.compareTo(msgTopVer) > 0) {\n                    U.warn(log, \"Stale version for full partition map update message (will ignore) [\" +\n                        \"lastTopChange=\" + lastTopChangeVer +\n                        \", readTopVer=\" + readyTopVer +\n                        \", msgVer=\" + msgTopVer + ']');\n\n                    return false;\n                }\n\n                boolean fullMapUpdated = (node2part == null);\n\n                if (node2part != null) {\n                    for (GridDhtPartitionMap part : node2part.values()) {\n                        GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                        if (shouldOverridePartitionMap(part, newPart)) {\n                            fullMapUpdated = true;\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Overriding partition map in full update map [exchVer=\" + exchangeVer +\n                                    \", curPart=\" + mapString(part) +\n                                    \", newPart=\" + mapString(newPart) + ']');\n                            }\n\n                            if (newPart.nodeId().equals(ctx.localNodeId()))\n                                updateSeq.setIfGreater(newPart.updateSequence());\n                        }\n                        else {\n                            // If for some nodes current partition has a newer map,\n                            // then we keep the newer value.\n                            partMap.put(part.nodeId(), part);\n                        }\n                    }\n\n                    // Check that we have new nodes.\n                    for (GridDhtPartitionMap part : partMap.values()) {\n                        if (fullMapUpdated)\n                            break;\n\n                        fullMapUpdated = !node2part.containsKey(part.nodeId());\n                    }\n\n                    // Remove entry if node left.\n                    for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                        UUID nodeId = it.next();\n\n                        if (!ctx.discovery().alive(nodeId)) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                    partMap + ']');\n\n                            it.remove();\n                        }\n                    }\n                }\n                else {\n                    GridDhtPartitionMap locNodeMap = partMap.get(ctx.localNodeId());\n\n                    if (locNodeMap != null)\n                        updateSeq.setIfGreater(locNodeMap.updateSequence());\n                }\n\n                if (!fullMapUpdated) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"No updates for full partition map (will ignore) [lastExch=\" + lastTopChangeVer +\n                            \", exchVer=\" + exchangeVer +\n                            \", curMap=\" + node2part +\n                            \", newMap=\" + partMap + ']');\n                    }\n\n                    return false;\n                }\n\n                if (exchangeVer != null) {\n                    assert exchangeVer.compareTo(readyTopVer) >= 0 && exchangeVer.compareTo(lastTopChangeVer) >= 0;\n\n                    lastTopChangeVer = readyTopVer = exchangeVer;\n                }\n\n                node2part = partMap;\n\n                if (exchangeVer == null && !grp.isReplicated() &&\n                        (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0)) {\n                    AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) {\n                            int p = e0.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e0.getValue() == MOVING || e0.getValue() == OWNING || e0.getValue() == RENTING) &&\n                                !affAssignment.getIds(p).contains(e.getKey())) {\n\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                diffIds.add(e.getKey());\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(e.getKey())) {\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n                    }\n\n                    diffFromAffinityVer = readyTopVer;\n                }\n\n                boolean changed = false;\n\n                GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n                if (nodeMap != null && grp.persistenceEnabled() && readyTopVer.initialized()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                        int p = e.getKey();\n                        GridDhtPartitionState state = e.getValue();\n\n                        if (state == OWNING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            assert locPart != null : grp.cacheOrGroupName();\n\n                            if (locPart.state() == MOVING) {\n                                boolean success = locPart.own();\n\n                                assert success : locPart;\n\n                                changed |= success;\n                            }\n                        }\n                        else if (state == MOVING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            if (locPart == null || locPart.state() == EVICTED)\n                                locPart = createPartition(p);\n\n                            if (locPart.state() == OWNING) {\n                                locPart.moving();\n\n                                changed = true;\n                            }\n                        }\n                        else if (state == RENTING && partsToReload.contains(p)) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            if (locPart == null || locPart.state() == EVICTED) {\n                                createPartition(p);\n\n                                changed = true;\n                            }\n                            else if (locPart.state() == OWNING || locPart.state() == MOVING) {\n                                locPart.reload(true);\n\n                                locPart.rent(false);\n\n                                changed = true;\n                            }\n                            else\n                                locPart.reload(true);\n                        }\n                    }\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchangeVer == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.assignment());\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after full update: \" + fullMapString());\n\n                if (changed)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            "1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225 +\n1226 +\n1227 +\n1228 +\n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264 +\n1265 +\n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275 +\n1276 +\n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  \n1288  \n1289  \n1290  \n1291  \n1292  \n1293 +\n1294 +\n1295 +\n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324 +\n1325 +\n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340 +\n1341 +\n1342 +\n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457 +\n1458 +\n1459 +\n1460 +\n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable CachePartitionFullCountersMap incomeCntrMap,\n        Set<Integer> partsToReload,\n        @Nullable AffinityTopologyVersion msgTopVer) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Updating full partition map [grp=\" + grp.cacheOrGroupName() + \", exchVer=\" + exchangeVer +\n                \", fullMap=\" + fullMapString() + ']');\n        }\n\n        assert partMap != null;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (stopping || !lastTopChangeVer.initialized() ||\n                    // Ignore message not-related to exchange if exchange is in progress.\n                    (exchangeVer == null && !lastTopChangeVer.equals(readyTopVer)))\n                    return false;\n\n                if (incomeCntrMap != null) {\n                    // update local counters in partitions\n                    for (int i = 0; i < locParts.length(); i++) {\n                        GridDhtLocalPartition part = locParts.get(i);\n\n                        if (part == null)\n                            continue;\n\n                        if (part.state() == OWNING || part.state() == MOVING) {\n                            long updCntr = incomeCntrMap.updateCounter(part.id());\n\n                            if (updCntr != 0 && updCntr > part.updateCounter())\n                                part.updateCounter(updCntr);\n                        }\n                    }\n                }\n\n                if (exchangeVer != null) {\n                    // Ignore if exchange already finished or new exchange started.\n                    if (readyTopVer.compareTo(exchangeVer) > 0 || lastTopChangeVer.compareTo(exchangeVer) > 0) {\n                        U.warn(log, \"Stale exchange id for full partition map update (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exchVer=\" + exchangeVer + ']');\n\n                        return false;\n                    }\n                }\n\n                if (msgTopVer != null && lastTopChangeVer.compareTo(msgTopVer) > 0) {\n                    U.warn(log, \"Stale version for full partition map update message (will ignore) [\" +\n                        \"grp=\" + grp.cacheOrGroupName() +\n                        \", lastTopChange=\" + lastTopChangeVer +\n                        \", readTopVer=\" + readyTopVer +\n                        \", msgVer=\" + msgTopVer + ']');\n\n                    return false;\n                }\n\n                boolean fullMapUpdated = (node2part == null);\n\n                if (node2part != null) {\n                    for (GridDhtPartitionMap part : node2part.values()) {\n                        GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                        if (shouldOverridePartitionMap(part, newPart)) {\n                            fullMapUpdated = true;\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Overriding partition map in full update map [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() +\n                                    \", exchVer=\" + exchangeVer +\n                                    \", curPart=\" + mapString(part) +\n                                    \", newPart=\" + mapString(newPart) + ']');\n                            }\n\n                            if (newPart.nodeId().equals(ctx.localNodeId()))\n                                updateSeq.setIfGreater(newPart.updateSequence());\n                        }\n                        else {\n                            // If for some nodes current partition has a newer map,\n                            // then we keep the newer value.\n                            partMap.put(part.nodeId(), part);\n                        }\n                    }\n\n                    // Check that we have new nodes.\n                    for (GridDhtPartitionMap part : partMap.values()) {\n                        if (fullMapUpdated)\n                            break;\n\n                        fullMapUpdated = !node2part.containsKey(part.nodeId());\n                    }\n\n                    // Remove entry if node left.\n                    for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                        UUID nodeId = it.next();\n\n                        if (!ctx.discovery().alive(nodeId)) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Removing left node from full map update [grp=\" + grp.cacheOrGroupName() +\n                                    \", nodeId=\" + nodeId + \", partMap=\" + partMap + ']');\n\n                            it.remove();\n                        }\n                    }\n                }\n                else {\n                    GridDhtPartitionMap locNodeMap = partMap.get(ctx.localNodeId());\n\n                    if (locNodeMap != null)\n                        updateSeq.setIfGreater(locNodeMap.updateSequence());\n                }\n\n                if (!fullMapUpdated) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"No updates for full partition map (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastExch=\" + lastTopChangeVer +\n                            \", exchVer=\" + exchangeVer +\n                            \", curMap=\" + node2part +\n                            \", newMap=\" + partMap + ']');\n                    }\n\n                    return false;\n                }\n\n                if (exchangeVer != null) {\n                    assert exchangeVer.compareTo(readyTopVer) >= 0 && exchangeVer.compareTo(lastTopChangeVer) >= 0;\n\n                    lastTopChangeVer = readyTopVer = exchangeVer;\n                }\n\n                node2part = partMap;\n\n                if (exchangeVer == null && !grp.isReplicated() &&\n                        (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0)) {\n                    AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) {\n                            int p = e0.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e0.getValue() == MOVING || e0.getValue() == OWNING || e0.getValue() == RENTING) &&\n                                !affAssignment.getIds(p).contains(e.getKey())) {\n\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                diffIds.add(e.getKey());\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(e.getKey())) {\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n                    }\n\n                    diffFromAffinityVer = readyTopVer;\n                }\n\n                boolean changed = false;\n\n                GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n                if (nodeMap != null && grp.persistenceEnabled() && readyTopVer.initialized()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                        int p = e.getKey();\n                        GridDhtPartitionState state = e.getValue();\n\n                        if (state == OWNING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            assert locPart != null : grp.cacheOrGroupName();\n\n                            if (locPart.state() == MOVING) {\n                                boolean success = locPart.own();\n\n                                assert success : locPart;\n\n                                changed |= success;\n                            }\n                        }\n                        else if (state == MOVING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            if (locPart == null || locPart.state() == EVICTED)\n                                locPart = createPartition(p);\n\n                            if (locPart.state() == OWNING) {\n                                locPart.moving();\n\n                                changed = true;\n                            }\n                        }\n                        else if (state == RENTING && partsToReload.contains(p)) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            if (locPart == null || locPart.state() == EVICTED) {\n                                createPartition(p);\n\n                                changed = true;\n                            }\n                            else if (locPart.state() == OWNING || locPart.state() == MOVING) {\n                                locPart.reload(true);\n\n                                locPart.rent(false);\n\n                                changed = true;\n                            }\n                            else\n                                locPart.reload(true);\n                        }\n                    }\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchangeVer == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.assignment());\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Partition map after full update [grp=\" + grp.cacheOrGroupName() +\n                        \", map=\" + fullMapString() + ']');\n                }\n\n                if (changed)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::setOwners(int,Set,boolean,boolean)",
            "1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982 -\n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009 -\n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  ",
            "    /** {@inheritDoc} */\n    @Override public Set<UUID> setOwners(int p, Set<UUID> owners, boolean haveHistory, boolean updateSeq) {\n        Set<UUID> result = haveHistory ? Collections.<UUID>emptySet() : new HashSet<UUID>();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                GridDhtLocalPartition locPart = locParts.get(p);\n\n                if (locPart != null) {\n                    if (locPart.state() == OWNING && !owners.contains(ctx.localNodeId())) {\n                        if (haveHistory)\n                            locPart.moving();\n                        else {\n                            locPart.rent(false);\n\n                            locPart.reload(true);\n\n                            result.add(ctx.localNodeId());\n                        }\n\n                        U.warn(log, \"Partition has been scheduled for rebalancing due to outdated update counter \" +\n                            \"[nodeId=\" + ctx.localNodeId() + \", cacheOrGroupName=\" + grp.cacheOrGroupName() +\n                            \", partId=\" + locPart.id() + \", haveHistory=\" + haveHistory + \"]\");\n\n                    }\n                }\n\n                for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) {\n                    GridDhtPartitionMap partMap = e.getValue();\n\n                    if (!partMap.containsKey(p))\n                        continue;\n\n                    if (partMap.get(p) == OWNING && !owners.contains(e.getKey())) {\n                        if (haveHistory)\n                            partMap.put(p, MOVING);\n                        else {\n                            partMap.put(p, RENTING);\n\n                            result.add(e.getKey());\n                        }\n\n                        partMap.updateSequence(partMap.updateSequence() + 1, partMap.topologyVersion());\n\n                        if (partMap.nodeId().equals(ctx.localNodeId()))\n                            this.updateSeq.setIfGreater(partMap.updateSequence());\n\n                        U.warn(log, \"Partition has been scheduled for rebalancing due to outdated update counter \" +\n                            \"[nodeId=\" + e.getKey() + \", cacheOrGroupName=\" + grp.cacheOrGroupName() +\n                            \", partId=\" + p + \", haveHistory=\" + haveHistory + \"]\");\n                    }\n                }\n\n                if (updateSeq)\n                    node2part = new GridDhtPartitionFullMap(node2part, this.updateSeq.incrementAndGet());\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        return result;\n    }",
            "1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018 +\n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045 +\n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  ",
            "    /** {@inheritDoc} */\n    @Override public Set<UUID> setOwners(int p, Set<UUID> owners, boolean haveHistory, boolean updateSeq) {\n        Set<UUID> result = haveHistory ? Collections.<UUID>emptySet() : new HashSet<UUID>();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                GridDhtLocalPartition locPart = locParts.get(p);\n\n                if (locPart != null) {\n                    if (locPart.state() == OWNING && !owners.contains(ctx.localNodeId())) {\n                        if (haveHistory)\n                            locPart.moving();\n                        else {\n                            locPart.rent(false);\n\n                            locPart.reload(true);\n\n                            result.add(ctx.localNodeId());\n                        }\n\n                        U.warn(log, \"Partition has been scheduled for rebalancing due to outdated update counter \" +\n                            \"[nodeId=\" + ctx.localNodeId() + \", grp=\" + grp.cacheOrGroupName() +\n                            \", partId=\" + locPart.id() + \", haveHistory=\" + haveHistory + \"]\");\n\n                    }\n                }\n\n                for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) {\n                    GridDhtPartitionMap partMap = e.getValue();\n\n                    if (!partMap.containsKey(p))\n                        continue;\n\n                    if (partMap.get(p) == OWNING && !owners.contains(e.getKey())) {\n                        if (haveHistory)\n                            partMap.put(p, MOVING);\n                        else {\n                            partMap.put(p, RENTING);\n\n                            result.add(e.getKey());\n                        }\n\n                        partMap.updateSequence(partMap.updateSequence() + 1, partMap.topologyVersion());\n\n                        if (partMap.nodeId().equals(ctx.localNodeId()))\n                            this.updateSeq.setIfGreater(partMap.updateSequence());\n\n                        U.warn(log, \"Partition has been scheduled for rebalancing due to outdated update counter \" +\n                            \"[nodeId=\" + e.getKey() + \", grp=\" + grp.cacheOrGroupName() +\n                            \", partId=\" + p + \", haveHistory=\" + haveHistory + \"]\");\n                    }\n                }\n\n                if (updateSeq)\n                    node2part = new GridDhtPartitionFullMap(node2part, this.updateSeq.incrementAndGet());\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        return result;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::beforeExchange(GridDhtPartitionsExchangeFuture,boolean,boolean)",
            " 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479 -\n 480 -\n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494 -\n 495 -\n 496 -\n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506 -\n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519 -\n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563 -\n 564 -\n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut,\n        boolean affReady,\n        boolean updateMoving)\n        throws IgniteCheckedException {\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            synchronized (ctx.exchange().interruptLock()) {\n                if (Thread.currentThread().isInterrupted())\n                    throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n                U.writeLock(lock);\n\n                try {\n                    if (stopping)\n                        return;\n\n                    assert lastTopChangeVer.equals(exchFut.initialVersion()) : \"Invalid topology version [topVer=\" + lastTopChangeVer +\n                        \", exchId=\" + exchFut.exchangeId() + ']';\n\n                    ExchangeDiscoveryEvents evts = exchFut.context().events();\n\n                    if (affReady) {\n                        assert grp.affinity().lastVersion().equals(evts.topologyVersion()) : \"Invalid affinity version [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", affVer=\" + grp.affinity().lastVersion() +\n                            \", evtsVer=\" + evts.topologyVersion() + ']';\n\n                        lastTopChangeVer = readyTopVer = evts.topologyVersion();\n                    }\n\n                    ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map beforeExchange [exchId=\" + exchFut.exchangeId() +\n                            \", fullMap=\" + fullMapString() + ']');\n                    }\n\n                    long updateSeq = this.updateSeq.incrementAndGet();\n\n                    cntrMap.clear();\n\n                    boolean grpStarted = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                    // If this is the oldest node.\n                    if (oldest != null && (loc.equals(oldest) || grpStarted)) {\n                        if (node2part == null) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                                    exchFut.exchangeId() + \", fullMap=\" + fullMapString() + ']');\n                        }\n                        else if (!node2part.valid()) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Created new full topology map on oldest node [exchId=\" + exchFut.exchangeId() +\n                                    \", fullMap=\" + node2part + ']');\n                            }\n                        }\n                        else if (!node2part.nodeId().equals(loc.id())) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [\" +\n                                    \"exchId=\" + exchFut.exchangeId() + \", fullMap=\" + fullMapString() + ']');\n                            }\n                        }\n                    }\n\n                    if (evts.hasServerLeft()) {\n                        List<DiscoveryEvent> evts0 = evts.events();\n\n                        for (int i = 0; i < evts0.size(); i++) {\n                            DiscoveryEvent evt = evts0.get(i);\n\n                            if (ExchangeDiscoveryEvents.serverLeftEvent(evt))\n                                removeNode(evt.eventNode().id());\n                        }\n                    }\n\n                    if (grp.affinityNode()) {\n                        if (grpStarted ||\n                            exchFut.firstEvent().type() == EVT_DISCOVERY_CUSTOM_EVT ||\n                            exchFut.serverNodeDiscoveryEvent()) {\n                            if (affReady) {\n                                assert grp.affinity().lastVersion().equals(evts.topologyVersion());\n\n                                initPartitions0(evts.topologyVersion(), exchFut, updateSeq);\n                            }\n                            else {\n                                assert !exchFut.context().mergeExchanges();\n\n                                List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                                createPartitions(exchFut.initialVersion(), aff, updateSeq);\n                            }\n                        }\n                    }\n\n                    consistencyCheck();\n\n                    if (updateMoving) {\n                        assert grp.affinity().lastVersion().equals(evts.topologyVersion());\n\n                        createMovingPartitions(grp.affinity().readyAffinity(evts.topologyVersion()));\n                    }\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map after beforeExchange [exchId=\" + exchFut.exchangeId() +\n                            \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n                finally {\n                    lock.writeLock().unlock();\n                }\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            " 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482 +\n 483 +\n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497 +\n 498 +\n 499 +\n 500 +\n 501 +\n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511 +\n 512 +\n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525 +\n 526 +\n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570 +\n 571 +\n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut,\n        boolean affReady,\n        boolean updateMoving)\n        throws IgniteCheckedException {\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            synchronized (ctx.exchange().interruptLock()) {\n                if (Thread.currentThread().isInterrupted())\n                    throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n                U.writeLock(lock);\n\n                try {\n                    if (stopping)\n                        return;\n\n                    assert lastTopChangeVer.equals(exchFut.initialVersion()) : \"Invalid topology version [topVer=\" + lastTopChangeVer +\n                        \", exchId=\" + exchFut.exchangeId() + ']';\n\n                    ExchangeDiscoveryEvents evts = exchFut.context().events();\n\n                    if (affReady) {\n                        assert grp.affinity().lastVersion().equals(evts.topologyVersion()) : \"Invalid affinity version [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", affVer=\" + grp.affinity().lastVersion() +\n                            \", evtsVer=\" + evts.topologyVersion() + ']';\n\n                        lastTopChangeVer = readyTopVer = evts.topologyVersion();\n                    }\n\n                    ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map beforeExchange [grp=\" + grp.cacheOrGroupName() +\n                            \", exchId=\" + exchFut.exchangeId() + \", fullMap=\" + fullMapString() + ']');\n                    }\n\n                    long updateSeq = this.updateSeq.incrementAndGet();\n\n                    cntrMap.clear();\n\n                    boolean grpStarted = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                    // If this is the oldest node.\n                    if (oldest != null && (loc.equals(oldest) || grpStarted)) {\n                        if (node2part == null) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Created brand new full topology map on oldest node [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() + \", exchId=\" + exchFut.exchangeId() +\n                                    \", fullMap=\" + fullMapString() + ']');\n                            }\n                        }\n                        else if (!node2part.valid()) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Created new full topology map on oldest node [\" +\n                                    \"grp=\" +  grp.cacheOrGroupName() + \", exchId=\" + exchFut.exchangeId() +\n                                    \", fullMap=\" + node2part + ']');\n                            }\n                        }\n                        else if (!node2part.nodeId().equals(loc.id())) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() + \", exchId=\" + exchFut.exchangeId() +\n                                    \", fullMap=\" + fullMapString() + ']');\n                            }\n                        }\n                    }\n\n                    if (evts.hasServerLeft()) {\n                        List<DiscoveryEvent> evts0 = evts.events();\n\n                        for (int i = 0; i < evts0.size(); i++) {\n                            DiscoveryEvent evt = evts0.get(i);\n\n                            if (ExchangeDiscoveryEvents.serverLeftEvent(evt))\n                                removeNode(evt.eventNode().id());\n                        }\n                    }\n\n                    if (grp.affinityNode()) {\n                        if (grpStarted ||\n                            exchFut.firstEvent().type() == EVT_DISCOVERY_CUSTOM_EVT ||\n                            exchFut.serverNodeDiscoveryEvent()) {\n                            if (affReady) {\n                                assert grp.affinity().lastVersion().equals(evts.topologyVersion());\n\n                                initPartitions0(evts.topologyVersion(), exchFut, updateSeq);\n                            }\n                            else {\n                                assert !exchFut.context().mergeExchanges();\n\n                                List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                                createPartitions(exchFut.initialVersion(), aff, updateSeq);\n                            }\n                        }\n                    }\n\n                    consistencyCheck();\n\n                    if (updateMoving) {\n                        assert grp.affinity().lastVersion().equals(evts.topologyVersion());\n\n                        createMovingPartitions(grp.affinity().readyAffinity(evts.topologyVersion()));\n                    }\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map after beforeExchange [grp=\" + grp.cacheOrGroupName() + \", \" +\n                            \"exchId=\" + exchFut.exchangeId() + \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n                finally {\n                    lock.writeLock().unlock();\n                }\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::localPartition0(int,AffinityTopologyVersion,boolean,boolean,boolean)",
            " 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837 -\n 838  \n 839  \n 840  \n 841  \n 842 -\n 843 -\n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850 -\n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860 -\n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  ",
            "    /**\n     * @param p Partition number.\n     * @param topVer Topology version.\n     * @param create Create flag.\n     * @param updateSeq Update sequence.\n     * @return Local partition.\n     */\n    @SuppressWarnings(\"TooBroadScope\")\n    private GridDhtLocalPartition localPartition0(int p,\n        AffinityTopologyVersion topVer,\n        boolean create,\n        boolean showRenting,\n        boolean updateSeq) {\n        GridDhtLocalPartition loc;\n\n        loc = locParts.get(p);\n\n        GridDhtPartitionState state = loc != null ? loc.state() : null;\n\n        if (loc != null && state != EVICTED && (state != RENTING || showRenting))\n            return loc;\n\n        if (!create)\n            return null;\n\n        boolean created = false;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                loc = locParts.get(p);\n\n                state = loc != null ? loc.state() : null;\n\n                boolean belongs = partitionLocalNode(p, topVer);\n\n                if (loc != null && state == EVICTED) {\n                    try {\n                        loc.rent(false).get();\n                    }\n                    catch (IgniteCheckedException ex) {\n                        throw new IgniteException(ex);\n                    }\n\n                    locParts.set(p, loc = null);\n\n                    if (!belongs) {\n                        throw new GridDhtInvalidPartitionException(p, \"Adding entry to evicted partition \" +\n                            \"(often may be caused by inconsistent 'key.hashCode()' implementation) \" +\n                            \"[part=\" + p + \", topVer=\" + topVer + \", this.topVer=\" + this.readyTopVer + ']');\n                    }\n                }\n                else if (loc != null && state == RENTING && !showRenting) {\n                    throw new GridDhtInvalidPartitionException(p, \"Adding entry to partition that is concurrently \" +\n                        \"evicted [part=\" + p + \", shouldBeMoving=\" + loc.reload() + \", belongs=\" + belongs +\n                        \", topVer=\" + topVer + \", curTopVer=\" + this.readyTopVer + \"]\");\n                }\n\n                if (loc == null) {\n                    if (!belongs)\n                        throw new GridDhtInvalidPartitionException(p, \"Creating partition which does not belong to \" +\n                            \"local node (often may be caused by inconsistent 'key.hashCode()' implementation) \" +\n                            \"[part=\" + p + \", topVer=\" + topVer + \", this.topVer=\" + this.readyTopVer + ']');\n\n                    locParts.set(p, loc = new GridDhtLocalPartition(ctx, grp, p));\n\n                    if (updateSeq)\n                        this.updateSeq.incrementAndGet();\n\n                    created = true;\n\n                    if (log.isDebugEnabled())\n                        log.debug(\"Created local partition: \" + loc);\n                }\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        if (created && ctx.pageStore() != null) {\n            try {\n                ctx.pageStore().onPartitionCreated(grp.groupId(), p);\n            }\n            catch (IgniteCheckedException e) {\n                // TODO ignite-db\n                throw new IgniteException(e);\n            }\n        }\n\n        return loc;\n    }",
            " 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853 +\n 854 +\n 855  \n 856  \n 857  \n 858  \n 859 +\n 860 +\n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867 +\n 868 +\n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878 +\n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  ",
            "    /**\n     * @param p Partition number.\n     * @param topVer Topology version.\n     * @param create Create flag.\n     * @param updateSeq Update sequence.\n     * @return Local partition.\n     */\n    @SuppressWarnings(\"TooBroadScope\")\n    private GridDhtLocalPartition localPartition0(int p,\n        AffinityTopologyVersion topVer,\n        boolean create,\n        boolean showRenting,\n        boolean updateSeq) {\n        GridDhtLocalPartition loc;\n\n        loc = locParts.get(p);\n\n        GridDhtPartitionState state = loc != null ? loc.state() : null;\n\n        if (loc != null && state != EVICTED && (state != RENTING || showRenting))\n            return loc;\n\n        if (!create)\n            return null;\n\n        boolean created = false;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                loc = locParts.get(p);\n\n                state = loc != null ? loc.state() : null;\n\n                boolean belongs = partitionLocalNode(p, topVer);\n\n                if (loc != null && state == EVICTED) {\n                    try {\n                        loc.rent(false).get();\n                    }\n                    catch (IgniteCheckedException ex) {\n                        throw new IgniteException(ex);\n                    }\n\n                    locParts.set(p, loc = null);\n\n                    if (!belongs) {\n                        throw new GridDhtInvalidPartitionException(p, \"Adding entry to evicted partition \" +\n                            \"(often may be caused by inconsistent 'key.hashCode()' implementation) \" +\n                            \"[grp=\" + grp.cacheOrGroupName() + \", part=\" + p + \", topVer=\" + topVer +\n                            \", this.topVer=\" + this.readyTopVer + ']');\n                    }\n                }\n                else if (loc != null && state == RENTING && !showRenting) {\n                    throw new GridDhtInvalidPartitionException(p, \"Adding entry to partition that is concurrently \" +\n                        \"evicted [grp=\" + grp.cacheOrGroupName() + \", part=\" + p + \", shouldBeMoving=\" +\n                        loc.reload() + \", belongs=\" + belongs + \", topVer=\" + topVer + \", curTopVer=\" + this.readyTopVer + \"]\");\n                }\n\n                if (loc == null) {\n                    if (!belongs)\n                        throw new GridDhtInvalidPartitionException(p, \"Creating partition which does not belong to \" +\n                            \"local node (often may be caused by inconsistent 'key.hashCode()' implementation) \" +\n                            \"[grp=\" + grp.cacheOrGroupName() + \", part=\" + p + \", topVer=\" + topVer +\n                            \", this.topVer=\" + this.readyTopVer + ']');\n\n                    locParts.set(p, loc = new GridDhtLocalPartition(ctx, grp, p));\n\n                    if (updateSeq)\n                        this.updateSeq.incrementAndGet();\n\n                    created = true;\n\n                    if (log.isDebugEnabled())\n                        log.debug(\"Created local partition [grp=\" + grp.cacheOrGroupName() + \", part=\" + loc + ']');\n                }\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        if (created && ctx.pageStore() != null) {\n            try {\n                ctx.pageStore().onPartitionCreated(grp.groupId(), p);\n            }\n            catch (IgniteCheckedException e) {\n                // TODO ignite-db\n                throw new IgniteException(e);\n            }\n        }\n\n        return loc;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::updateRebalanceVersion(List)",
            "2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  \n2434  \n2435  \n2436  \n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444  \n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471 -\n2472  \n2473  ",
            "    /**\n     * @param aff Affinity assignments.\n     */\n    private void updateRebalanceVersion(List<List<ClusterNode>> aff) {\n        if (!rebalancedTopVer.equals(readyTopVer)) {\n            if (node2part == null || !node2part.valid())\n                return;\n\n            for (int i = 0; i < grp.affinity().partitions(); i++) {\n                List<ClusterNode> affNodes = aff.get(i);\n\n                // Topology doesn't contain server nodes (just clients).\n                if (affNodes.isEmpty())\n                    continue;\n\n                Set<ClusterNode> owners = U.newHashSet(affNodes.size());\n\n                for (ClusterNode node : affNodes) {\n                    if (hasState(i, node.id(), OWNING))\n                        owners.add(node);\n                }\n\n                if (!grp.isReplicated()) {\n                    Set<UUID> diff = diffFromAffinity.get(i);\n\n                    if (diff != null) {\n                        for (UUID nodeId : diff) {\n                            if (hasState(i, nodeId, OWNING)) {\n                                ClusterNode node = ctx.discovery().node(nodeId);\n\n                                if (node != null)\n                                    owners.add(node);\n                            }\n                        }\n                    }\n                }\n\n                if (affNodes.size() != owners.size() || !owners.containsAll(affNodes))\n                    return;\n            }\n\n            rebalancedTopVer = readyTopVer;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Updated rebalanced version [cache=\" + grp.cacheOrGroupName() + \", ver=\" + rebalancedTopVer + ']');\n        }\n    }",
            "2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511 +\n2512  \n2513  ",
            "    /**\n     * @param aff Affinity assignments.\n     */\n    private void updateRebalanceVersion(List<List<ClusterNode>> aff) {\n        if (!rebalancedTopVer.equals(readyTopVer)) {\n            if (node2part == null || !node2part.valid())\n                return;\n\n            for (int i = 0; i < grp.affinity().partitions(); i++) {\n                List<ClusterNode> affNodes = aff.get(i);\n\n                // Topology doesn't contain server nodes (just clients).\n                if (affNodes.isEmpty())\n                    continue;\n\n                Set<ClusterNode> owners = U.newHashSet(affNodes.size());\n\n                for (ClusterNode node : affNodes) {\n                    if (hasState(i, node.id(), OWNING))\n                        owners.add(node);\n                }\n\n                if (!grp.isReplicated()) {\n                    Set<UUID> diff = diffFromAffinity.get(i);\n\n                    if (diff != null) {\n                        for (UUID nodeId : diff) {\n                            if (hasState(i, nodeId, OWNING)) {\n                                ClusterNode node = ctx.discovery().node(nodeId);\n\n                                if (node != null)\n                                    owners.add(node);\n                            }\n                        }\n                    }\n                }\n\n                if (affNodes.size() != owners.size() || !owners.containsAll(affNodes))\n                    return;\n            }\n\n            rebalancedTopVer = readyTopVer;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Updated rebalanced version [grp=\" + grp.cacheOrGroupName() + \", ver=\" + rebalancedTopVer + ']');\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(GridDhtPartitionExchangeId,GridDhtPartitionMap,boolean)",
            "1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543 -\n1544 -\n1545  \n1546  \n1547 -\n1548 -\n1549 -\n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566 -\n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585 -\n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662 -\n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts,\n        boolean force\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!ctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return false;\n        }\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (stopping)\n                    return false;\n\n                if (!force) {\n                    if (lastTopChangeVer.initialized() && exchId != null && lastTopChangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                        U.warn(log, \"Stale exchange id for single partition map update (will ignore) [\" +\n                            \"lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exch=\" + exchId.topologyVersion() + ']');\n\n                        return false;\n                    }\n                }\n\n                if (node2part == null)\n                    // Create invalid partition map.\n                    node2part = new GridDhtPartitionFullMap();\n\n                GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n                if (force) {\n                    if (cur != null && cur.topologyVersion().initialized())\n                        parts.updateSequence(cur.updateSequence(), cur.topologyVersion());\n                }\n                else if (isStaleUpdate(cur, parts)) {\n                    U.warn(log, \"Stale update for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curMap=\" + cur +\n                        \", newMap=\" + parts + ']');\n\n                    return false;\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                node2part.newUpdateSequence(updateSeq);\n\n                boolean changed = false;\n\n                if (cur == null || !cur.equals(parts))\n                    changed = true;\n\n                node2part.put(parts.nodeId(), parts);\n\n                // During exchange diff is calculated after all messages are received and affinity initialized.\n                if (exchId == null && !grp.isReplicated()) {\n                    if (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0) {\n                        AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                        // Add new mappings.\n                        for (Map.Entry<Integer, GridDhtPartitionState> e : parts.entrySet()) {\n                            int p = e.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e.getValue() == MOVING || e.getValue() == OWNING || e.getValue() == RENTING)\n                                && !affAssignment.getIds(p).contains(parts.nodeId())) {\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                if (diffIds.add(parts.nodeId()))\n                                    changed = true;\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(parts.nodeId())) {\n                                    changed = true;\n\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n\n                        // Remove obsolete mappings.\n                        if (cur != null) {\n                            for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                                Set<UUID> ids = diffFromAffinity.get(p);\n\n                                if (ids != null && ids.remove(parts.nodeId())) {\n                                    changed = true;\n\n                                    if (ids.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n\n                        diffFromAffinityVer = readyTopVer;\n                    }\n                }\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchId == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.assignment());\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after single update: \" + fullMapString());\n\n                if (changed && exchId == null)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            "1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573 +\n1574 +\n1575 +\n1576 +\n1577  \n1578  \n1579 +\n1580 +\n1581 +\n1582 +\n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599 +\n1600 +\n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619 +\n1620 +\n1621 +\n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698 +\n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts,\n        boolean force\n    ) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Updating single partition map [grp=\" + grp.cacheOrGroupName() + \", exchId=\" + exchId +\n                \", parts=\" + mapString(parts) + ']');\n        }\n\n        if (!ctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled()) {\n                log.debug(\"Received partition update for non-existing node (will ignore) [grp=\" + grp.cacheOrGroupName() +\n                    \", exchId=\" + exchId + \", parts=\" + parts + ']');\n            }\n\n            return false;\n        }\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (stopping)\n                    return false;\n\n                if (!force) {\n                    if (lastTopChangeVer.initialized() && exchId != null && lastTopChangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                        U.warn(log, \"Stale exchange id for single partition map update (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exch=\" + exchId.topologyVersion() + ']');\n\n                        return false;\n                    }\n                }\n\n                if (node2part == null)\n                    // Create invalid partition map.\n                    node2part = new GridDhtPartitionFullMap();\n\n                GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n                if (force) {\n                    if (cur != null && cur.topologyVersion().initialized())\n                        parts.updateSequence(cur.updateSequence(), cur.topologyVersion());\n                }\n                else if (isStaleUpdate(cur, parts)) {\n                    U.warn(log, \"Stale update for single partition map update (will ignore) [\" +\n                        \"grp=\" + grp.cacheOrGroupName() +\n                        \", exchId=\" + exchId +\n                        \", curMap=\" + cur +\n                        \", newMap=\" + parts + ']');\n\n                    return false;\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                node2part.newUpdateSequence(updateSeq);\n\n                boolean changed = false;\n\n                if (cur == null || !cur.equals(parts))\n                    changed = true;\n\n                node2part.put(parts.nodeId(), parts);\n\n                // During exchange diff is calculated after all messages are received and affinity initialized.\n                if (exchId == null && !grp.isReplicated()) {\n                    if (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0) {\n                        AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                        // Add new mappings.\n                        for (Map.Entry<Integer, GridDhtPartitionState> e : parts.entrySet()) {\n                            int p = e.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e.getValue() == MOVING || e.getValue() == OWNING || e.getValue() == RENTING)\n                                && !affAssignment.getIds(p).contains(parts.nodeId())) {\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                if (diffIds.add(parts.nodeId()))\n                                    changed = true;\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(parts.nodeId())) {\n                                    changed = true;\n\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n\n                        // Remove obsolete mappings.\n                        if (cur != null) {\n                            for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                                Set<UUID> ids = diffFromAffinity.get(p);\n\n                                if (ids != null && ids.remove(parts.nodeId())) {\n                                    changed = true;\n\n                                    if (ids.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n\n                        diffFromAffinityVer = readyTopVer;\n                    }\n                }\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchId == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.assignment());\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after single update [grp=\" + grp.cacheOrGroupName() + \", map=\" + fullMapString() + ']');\n\n                if (changed && exchId == null)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::initPartitions0(AffinityTopologyVersion,GridDhtPartitionsExchangeFuture,long)",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365 -\n 366 -\n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392 -\n 393 -\n 394 -\n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  ",
            "    /**\n     * @param affVer Affinity version to use.\n     * @param exchFut Exchange future.\n     * @param updateSeq Update sequence.\n     */\n    private void initPartitions0(AffinityTopologyVersion affVer, GridDhtPartitionsExchangeFuture exchFut, long updateSeq) {\n        List<List<ClusterNode>> aff = grp.affinity().readyAssignments(affVer);\n\n        if (grp.affinityNode()) {\n            ClusterNode loc = ctx.localNode();\n\n            ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n            GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n            assert grp.affinity().lastVersion().equals(affVer) :\n                \"Invalid affinity [topVer=\" + grp.affinity().lastVersion() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", affVer=\" + affVer +\n                \", fut=\" + exchFut + ']';\n\n            int num = grp.affinity().partitions();\n\n            if (grp.rebalanceEnabled()) {\n                boolean added = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                boolean first = added || (loc.equals(oldest) && loc.id().equals(exchId.nodeId()) && exchId.isJoined());\n\n                if (first) {\n                    assert exchId.isJoined() || added;\n\n                    for (int p = 0; p < num; p++) {\n                        if (localNode(p, aff)) {\n                            GridDhtLocalPartition locPart = createPartition(p);\n\n                            boolean owned = locPart.own();\n\n                            assert owned : \"Failed to own partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                                \", part=\" + locPart + ']';\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Owned partition for oldest node: \" + locPart);\n\n                            updateSeq = updateLocal(p, locPart.state(), updateSeq, affVer);\n                        }\n                    }\n                }\n                else\n                    createPartitions(affVer, aff, updateSeq);\n            }\n            else {\n                // If preloader is disabled, then we simply clear out\n                // the partitions this node is not responsible for.\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition0(p, affVer, false, true, false);\n\n                    boolean belongs = localNode(p, aff);\n\n                    if (locPart != null) {\n                        if (!belongs) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state.active()) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, affVer);\n\n                                if (log.isDebugEnabled())\n                                    log.debug(\"Evicting partition with rebalancing disabled \" +\n                                        \"(it does not belong to affinity): \" + locPart);\n                            }\n                        }\n                        else\n                            locPart.own();\n                    }\n                    else if (belongs) {\n                        locPart = createPartition(p);\n\n                        locPart.own();\n\n                        updateLocal(p, locPart.state(), updateSeq, affVer);\n                    }\n                }\n            }\n        }\n\n        updateRebalanceVersion(aff);\n    }",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365 +\n 366 +\n 367 +\n 368 +\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394 +\n 395 +\n 396 +\n 397 +\n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  ",
            "    /**\n     * @param affVer Affinity version to use.\n     * @param exchFut Exchange future.\n     * @param updateSeq Update sequence.\n     */\n    private void initPartitions0(AffinityTopologyVersion affVer, GridDhtPartitionsExchangeFuture exchFut, long updateSeq) {\n        List<List<ClusterNode>> aff = grp.affinity().readyAssignments(affVer);\n\n        if (grp.affinityNode()) {\n            ClusterNode loc = ctx.localNode();\n\n            ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n            GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n            assert grp.affinity().lastVersion().equals(affVer) :\n                \"Invalid affinity [topVer=\" + grp.affinity().lastVersion() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", affVer=\" + affVer +\n                \", fut=\" + exchFut + ']';\n\n            int num = grp.affinity().partitions();\n\n            if (grp.rebalanceEnabled()) {\n                boolean added = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                boolean first = added || (loc.equals(oldest) && loc.id().equals(exchId.nodeId()) && exchId.isJoined());\n\n                if (first) {\n                    assert exchId.isJoined() || added;\n\n                    for (int p = 0; p < num; p++) {\n                        if (localNode(p, aff)) {\n                            GridDhtLocalPartition locPart = createPartition(p);\n\n                            boolean owned = locPart.own();\n\n                            assert owned : \"Failed to own partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                                \", part=\" + locPart + ']';\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Owned partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                                    \", part=\" + locPart + ']');\n                            }\n\n                            updateSeq = updateLocal(p, locPart.state(), updateSeq, affVer);\n                        }\n                    }\n                }\n                else\n                    createPartitions(affVer, aff, updateSeq);\n            }\n            else {\n                // If preloader is disabled, then we simply clear out\n                // the partitions this node is not responsible for.\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition0(p, affVer, false, true, false);\n\n                    boolean belongs = localNode(p, aff);\n\n                    if (locPart != null) {\n                        if (!belongs) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state.active()) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, affVer);\n\n                                if (log.isDebugEnabled()) {\n                                    log.debug(\"Evicting partition with rebalancing disabled (it does not belong to \" +\n                                        \"affinity) [grp=\" + grp.cacheOrGroupName() + \", part=\" + locPart + ']');\n                                }\n                            }\n                        }\n                        else\n                            locPart.own();\n                    }\n                    else if (belongs) {\n                        locPart = createPartition(p);\n\n                        locPart.own();\n\n                        updateLocal(p, locPart.state(), updateSeq, affVer);\n                    }\n                }\n            }\n        }\n\n        updateRebalanceVersion(aff);\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::nodes0(int,AffinityAssignment,List)",
            "1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029 -\n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057 -\n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  ",
            "    /**\n     * @param p Partition.\n     * @param affAssignment Assignments.\n     * @param affNodes Node assigned for given partition by affinity.\n     * @return Nodes responsible for given partition (primary is first).\n     */\n    @Nullable private List<ClusterNode> nodes0(int p, AffinityAssignment affAssignment, List<ClusterNode> affNodes) {\n        if (grp.isReplicated())\n            return affNodes;\n\n        AffinityTopologyVersion topVer = affAssignment.topologyVersion();\n\n        lock.readLock().lock();\n\n        try {\n            assert node2part != null && node2part.valid() : \"Invalid node-to-partitions map [topVer1=\" + topVer +\n                \", topVer2=\" + this.readyTopVer +\n                \", node=\" + ctx.igniteInstanceName() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", node2part=\" + node2part + ']';\n\n            List<ClusterNode> nodes = null;\n\n            if (!topVer.equals(diffFromAffinityVer)) {\n                LT.warn(log, \"Requested topology version does not match calculated diff, will require full iteration to\" +\n                    \"calculate mapping [topVer=\" + topVer + \", diffVer=\" + diffFromAffinityVer + \"]\");\n\n                nodes = new ArrayList<>();\n\n                nodes.addAll(affNodes);\n\n                for (Map.Entry<UUID, GridDhtPartitionMap> entry : node2part.entrySet()) {\n                    GridDhtPartitionState state = entry.getValue().get(p);\n\n                    ClusterNode n = ctx.discovery().node(entry.getKey());\n\n                    if (n != null && state != null && (state == MOVING || state == OWNING || state == RENTING)\n                        && !nodes.contains(n) && (topVer.topologyVersion() < 0 || n.order() <= topVer.topologyVersion())) {\n                        nodes.add(n);\n                    }\n\n                }\n\n                return nodes;\n            }\n\n            Collection<UUID> diffIds = diffFromAffinity.get(p);\n\n            if (!F.isEmpty(diffIds)) {\n                HashSet<UUID> affIds = affAssignment.getIds(p);\n\n                for (UUID nodeId : diffIds) {\n                    if (affIds.contains(nodeId)) {\n                        U.warn(log, \"Node from diff \" + nodeId + \" is affinity node. Skipping it.\");\n\n                        continue;\n                    }\n\n                    if (hasState(p, nodeId, OWNING, MOVING, RENTING)) {\n                        ClusterNode n = ctx.discovery().node(nodeId);\n\n                        if (n != null && (topVer.topologyVersion() < 0 || n.order() <= topVer.topologyVersion())) {\n                            if (nodes == null) {\n                                nodes = new ArrayList<>(affNodes.size() + diffIds.size());\n\n                                nodes.addAll(affNodes);\n                            }\n\n                            nodes.add(n);\n                        }\n                    }\n                }\n            }\n\n            return nodes;\n        }\n        finally {\n            lock.readLock().unlock();\n        }\n    }",
            "1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047 +\n1048 +\n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076 +\n1077 +\n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  ",
            "    /**\n     * @param p Partition.\n     * @param affAssignment Assignments.\n     * @param affNodes Node assigned for given partition by affinity.\n     * @return Nodes responsible for given partition (primary is first).\n     */\n    @Nullable private List<ClusterNode> nodes0(int p, AffinityAssignment affAssignment, List<ClusterNode> affNodes) {\n        if (grp.isReplicated())\n            return affNodes;\n\n        AffinityTopologyVersion topVer = affAssignment.topologyVersion();\n\n        lock.readLock().lock();\n\n        try {\n            assert node2part != null && node2part.valid() : \"Invalid node-to-partitions map [topVer1=\" + topVer +\n                \", topVer2=\" + this.readyTopVer +\n                \", node=\" + ctx.igniteInstanceName() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", node2part=\" + node2part + ']';\n\n            List<ClusterNode> nodes = null;\n\n            if (!topVer.equals(diffFromAffinityVer)) {\n                LT.warn(log, \"Requested topology version does not match calculated diff, will require full iteration to\" +\n                    \"calculate mapping [grp=\" + grp.cacheOrGroupName() + \", topVer=\" + topVer +\n                    \", diffVer=\" + diffFromAffinityVer + \"]\");\n\n                nodes = new ArrayList<>();\n\n                nodes.addAll(affNodes);\n\n                for (Map.Entry<UUID, GridDhtPartitionMap> entry : node2part.entrySet()) {\n                    GridDhtPartitionState state = entry.getValue().get(p);\n\n                    ClusterNode n = ctx.discovery().node(entry.getKey());\n\n                    if (n != null && state != null && (state == MOVING || state == OWNING || state == RENTING)\n                        && !nodes.contains(n) && (topVer.topologyVersion() < 0 || n.order() <= topVer.topologyVersion())) {\n                        nodes.add(n);\n                    }\n\n                }\n\n                return nodes;\n            }\n\n            Collection<UUID> diffIds = diffFromAffinity.get(p);\n\n            if (!F.isEmpty(diffIds)) {\n                HashSet<UUID> affIds = affAssignment.getIds(p);\n\n                for (UUID nodeId : diffIds) {\n                    if (affIds.contains(nodeId)) {\n                        U.warn(log, \"Node from diff is affinity node, skipping it [grp=\" + grp.cacheOrGroupName() +\n                            \", node=\" + nodeId + ']');\n\n                        continue;\n                    }\n\n                    if (hasState(p, nodeId, OWNING, MOVING, RENTING)) {\n                        ClusterNode n = ctx.discovery().node(nodeId);\n\n                        if (n != null && (topVer.topologyVersion() < 0 || n.order() <= topVer.topologyVersion())) {\n                            if (nodes == null) {\n                                nodes = new ArrayList<>(affNodes.size() + diffIds.size());\n\n                                nodes.addAll(affNodes);\n                            }\n\n                            nodes.add(n);\n                        }\n                    }\n                }\n            }\n\n            return nodes;\n        }\n        finally {\n            lock.readLock().unlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::applyUpdateCounters()",
            "1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495 -\n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  ",
            "    /** {@inheritDoc} */\n    @Override public void applyUpdateCounters() {\n        long now = U.currentTimeMillis();\n\n        lock.writeLock().lock();\n\n        try {\n            long acquired = U.currentTimeMillis();\n\n            if (acquired - now >= 100) {\n                if (timeLog.isInfoEnabled())\n                    timeLog.info(\"Waited too long to acquire topology write lock \" +\n                        \"[cache=\" + grp.groupId() + \", waitTime=\" + (acquired - now) + ']');\n            }\n\n            if (stopping)\n                return;\n\n            for (int i = 0; i < locParts.length(); i++) {\n                GridDhtLocalPartition part = locParts.get(i);\n\n                if (part == null)\n                    continue;\n\n                long updCntr = cntrMap.updateCounter(part.id());\n\n                if (updCntr > part.updateCounter())\n                    part.updateCounter(updCntr);\n                else if (part.updateCounter() > 0) {\n                    cntrMap.initialUpdateCounter(part.id(), part.initialUpdateCounter());\n                    cntrMap.updateCounter(part.id(), part.updateCounter());\n                }\n            }\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            "1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525 +\n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  ",
            "    /** {@inheritDoc} */\n    @Override public void applyUpdateCounters() {\n        long now = U.currentTimeMillis();\n\n        lock.writeLock().lock();\n\n        try {\n            long acquired = U.currentTimeMillis();\n\n            if (acquired - now >= 100) {\n                if (timeLog.isInfoEnabled())\n                    timeLog.info(\"Waited too long to acquire topology write lock \" +\n                        \"[grp=\" + grp.cacheOrGroupName() + \", waitTime=\" + (acquired - now) + ']');\n            }\n\n            if (stopping)\n                return;\n\n            for (int i = 0; i < locParts.length(); i++) {\n                GridDhtLocalPartition part = locParts.get(i);\n\n                if (part == null)\n                    continue;\n\n                long updCntr = cntrMap.updateCounter(part.id());\n\n                if (updCntr > part.updateCounter())\n                    part.updateCounter(updCntr);\n                else if (part.updateCounter() > 0) {\n                    cntrMap.initialUpdateCounter(part.id(), part.initialUpdateCounter());\n                    cntrMap.updateCounter(part.id(), part.updateCounter());\n                }\n            }\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ]
    ],
    "7ea5830a5f28bb52db3efa6955f505e731e87f90": [
        [
            "GridCacheAtomicSequenceImpl::get()",
            " 160  \n 161  \n 162  \n 163  \n 164 -\n 165 -\n 166 -\n 167 -\n 168 -\n 169 -\n 170 -\n 171 -\n 172  ",
            "    /** {@inheritDoc} */\n    @Override public long get() {\n        checkRemoved();\n\n        lock.lock();\n\n        try {\n            return locVal;\n        }\n        finally {\n            lock.unlock();\n        }\n    }",
            " 150  \n 151  \n 152  \n 153  \n 154 +\n 155  ",
            "    /** {@inheritDoc} */\n    @Override public long get() {\n        checkRemoved();\n\n        return locVal;\n    }"
        ],
        [
            "GridCacheAtomicSequenceImpl::internalUpdate(long,Callable,boolean)",
            " 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 -\n 239 -\n 240  \n 241 -\n 242  \n 243 -\n 244  \n 245 -\n 246 -\n 247 -\n 248 -\n 249 -\n 250 -\n 251 -\n 252 -\n 253 -\n 254 -\n 255 -\n 256 -\n 257 -\n 258 -\n 259 -\n 260 -\n 261 -\n 262 -\n 263 -\n 264 -\n 265 -\n 266 -\n 267 -\n 268  \n 269 -\n 270 -\n 271  \n 272 -\n 273 -\n 274 -\n 275 -\n 276 -\n 277 -\n 278  \n 279 -\n 280 -\n 281 -\n 282 -\n 283 -\n 284 -\n 285 -\n 286 -\n 287 -\n 288 -\n 289 -\n 290 -\n 291 -\n 292 -\n 293 -\n 294 -\n 295 -\n 296 -\n 297 -\n 298 -\n 299 -\n 300  \n 301  \n 302  ",
            "    /**\n     * Synchronous sequence update operation. Will add given amount to the sequence value.\n     *\n     * @param l Increment amount.\n     * @param updateCall Cache call that will update sequence reservation count in accordance with l.\n     * @param updated If {@code true}, will return sequence value after update, otherwise will return sequence value\n     *      prior to update.\n     * @return Sequence value.\n     * @throws IgniteCheckedException If update failed.\n     */\n    @SuppressWarnings(\"SignalWithoutCorrespondingAwait\")\n    private long internalUpdate(long l, @Nullable Callable<Long> updateCall, boolean updated) throws IgniteCheckedException {\n        checkRemoved();\n\n        assert l > 0;\n\n        lock.lock();\n\n        try {\n            // If reserved range isn't exhausted.\n            if (locVal + l <= upBound) {\n                long curVal = locVal;\n\n                locVal += l;\n\n                return updated ? locVal : curVal;\n            }\n        }\n        finally {\n            lock.unlock();\n        }\n\n        if (updateCall == null)\n            updateCall = internalUpdate(l, updated);\n\n        while (true) {\n            if (updateGuard.compareAndSet(false, true)) {\n                try {\n                    try {\n                        return retryTopologySafe(updateCall);\n                    }\n                    catch (IgniteCheckedException | IgniteException | IllegalStateException e) {\n                        throw e;\n                    }\n                    catch (Exception e) {\n                        throw new IgniteCheckedException(e);\n                    }\n                }\n                finally {\n                    lock.lock();\n\n                    try {\n                        updateGuard.set(false);\n\n                        cond.signalAll();\n                    }\n                    finally {\n                        lock.unlock();\n                    }\n                }\n            }\n            else {\n                lock.lock();\n\n                try {\n                    while (locVal >= upBound && updateGuard.get())\n                        U.await(cond, 500, MILLISECONDS);\n\n                    checkRemoved();\n\n                    // If reserved range isn't exhausted.\n                    if (locVal + l <= upBound) {\n                        long curVal = locVal;\n\n                        locVal += l;\n\n                        return updated ? locVal : curVal;\n                    }\n                }\n                finally {\n                    lock.unlock();\n                }\n            }\n        }\n    }",
            " 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 +\n 222  \n 223 +\n 224 +\n 225  \n 226 +\n 227  \n 228  \n 229 +\n 230 +\n 231  \n 232 +\n 233 +\n 234  \n 235 +\n 236 +\n 237 +\n 238 +\n 239 +\n 240  \n 241  \n 242 +\n 243 +\n 244 +\n 245  ",
            "    /**\n     * Synchronous sequence update operation. Will add given amount to the sequence value.\n     *\n     * @param l Increment amount.\n     * @param updateCall Cache call that will update sequence reservation count in accordance with l.\n     * @param updated If {@code true}, will return sequence value after update, otherwise will return sequence value\n     *      prior to update.\n     * @return Sequence value.\n     * @throws IgniteCheckedException If update failed.\n     */\n    @SuppressWarnings(\"SignalWithoutCorrespondingAwait\")\n    private long internalUpdate(long l, @Nullable Callable<Long> updateCall, boolean updated) throws IgniteCheckedException {\n        checkRemoved();\n\n        assert l > 0;\n\n        lock.lock();\n\n        try {\n            // If reserved range isn't exhausted.\n            long locVal0 = locVal;\n\n            if (locVal0 + l <= upBound) {\n                locVal = locVal0 + l;\n\n                return updated ? locVal0 + l : locVal0;\n            }\n\n            if (updateCall == null)\n                updateCall = internalUpdate(l, updated);\n\n            try {\n                return updateCall.call();\n            }\n            catch (IgniteCheckedException | IgniteException | IllegalStateException e) {\n                throw e;\n            }\n            catch (Exception e) {\n                throw new IgniteCheckedException(e);\n            }\n        }\n        finally {\n            lock.unlock();\n        }\n    }"
        ],
        [
            "GridCacheAtomicSequenceImpl::internalUpdate(long,boolean)",
            " 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425 -\n 426 -\n 427 -\n 428 -\n 429  \n 430 -\n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  ",
            "    /**\n     * Method returns callable for execution all update operations in async and sync mode.\n     *\n     * @param l Value will be added to sequence.\n     * @param updated If {@code true}, will return updated value, if {@code false}, will return previous value.\n     * @return Callable for execution in async and sync mode.\n     */\n    @SuppressWarnings(\"TooBroadScope\")\n    private Callable<Long> internalUpdate(final long l, final boolean updated) {\n        return new Callable<Long>() {\n            @Override public Long call() throws Exception {\n                try (GridNearTxLocal tx = CU.txStartInternal(ctx, seqView, PESSIMISTIC, REPEATABLE_READ)) {\n                    GridCacheAtomicSequenceValue seq = seqView.get(key);\n\n                    checkRemoved();\n\n                    assert seq != null;\n\n                    long curLocVal;\n\n                    long newUpBound;\n\n                    lock.lock();\n\n                    try {\n                        curLocVal = locVal;\n\n                        // If local range was already reserved in another thread.\n                        if (locVal + l <= upBound) {\n                            long retVal = locVal;\n\n                            locVal += l;\n\n                            return updated ? locVal : retVal;\n                        }\n\n                        long curGlobalVal = seq.get();\n\n                        long newLocVal;\n\n                        /* We should use offset because we already reserved left side of range.*/\n                        long off = batchSize > 1 ? batchSize - 1 : 1;\n\n                        // Calculate new values for local counter, global counter and upper bound.\n                        if (curLocVal + l >= curGlobalVal) {\n                            newLocVal = curLocVal + l;\n\n                            newUpBound = newLocVal + off;\n                        }\n                        else {\n                            newLocVal = curGlobalVal;\n\n                            newUpBound = newLocVal + off;\n                        }\n\n                        locVal = newLocVal;\n                        upBound = newUpBound;\n\n                        if (updated)\n                            curLocVal = newLocVal;\n                    }\n                    finally {\n                        lock.unlock();\n                    }\n\n                    // Global counter must be more than reserved upper bound.\n                    seq.set(newUpBound + 1);\n\n                    seqView.put(key, seq);\n\n                    tx.commit();\n\n                    return curLocVal;\n                }\n                catch (Error | Exception e) {\n                    U.error(log, \"Failed to get and add: \" + this, e);\n\n                    throw e;\n                }\n            }\n        };\n    }",
            " 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 +\n 369 +\n 370  \n 371 +\n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  ",
            "    /**\n     * Method returns callable for execution all update operations in async and sync mode.\n     *\n     * @param l Value will be added to sequence.\n     * @param updated If {@code true}, will return updated value, if {@code false}, will return previous value.\n     * @return Callable for execution in async and sync mode.\n     */\n    @SuppressWarnings(\"TooBroadScope\")\n    private Callable<Long> internalUpdate(final long l, final boolean updated) {\n        return new Callable<Long>() {\n            @Override public Long call() throws Exception {\n                try (GridNearTxLocal tx = CU.txStartInternal(ctx, seqView, PESSIMISTIC, REPEATABLE_READ)) {\n                    GridCacheAtomicSequenceValue seq = seqView.get(key);\n\n                    checkRemoved();\n\n                    assert seq != null;\n\n                    long curLocVal;\n\n                    long newUpBound;\n\n                    lock.lock();\n\n                    try {\n                        curLocVal = locVal;\n\n                        // If local range was already reserved in another thread.\n                        if (curLocVal + l <= upBound) {\n                            locVal = curLocVal + l;\n\n                            return updated ? curLocVal + l : curLocVal;\n                        }\n\n                        long curGlobalVal = seq.get();\n\n                        long newLocVal;\n\n                        /* We should use offset because we already reserved left side of range.*/\n                        long off = batchSize > 1 ? batchSize - 1 : 1;\n\n                        // Calculate new values for local counter, global counter and upper bound.\n                        if (curLocVal + l >= curGlobalVal) {\n                            newLocVal = curLocVal + l;\n\n                            newUpBound = newLocVal + off;\n                        }\n                        else {\n                            newLocVal = curGlobalVal;\n\n                            newUpBound = newLocVal + off;\n                        }\n\n                        locVal = newLocVal;\n                        upBound = newUpBound;\n\n                        if (updated)\n                            curLocVal = newLocVal;\n                    }\n                    finally {\n                        lock.unlock();\n                    }\n\n                    // Global counter must be more than reserved upper bound.\n                    seq.set(newUpBound + 1);\n\n                    seqView.put(key, seq);\n\n                    tx.commit();\n\n                    return curLocVal;\n                }\n                catch (Error | Exception e) {\n                    U.error(log, \"Failed to get and add: \" + this, e);\n\n                    throw e;\n                }\n            }\n        };\n    }"
        ]
    ],
    "bc4209bac205a682729d8ab174348bf188356565": [
        [
            "GridCacheDatabaseSharedManager::lock()",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  ",
            "    /** {@inheritDoc} */\n    @Override public void lock() throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"Try to capture file lock [nodeId=\" +\n                cctx.localNodeId() + \" path=\" + fileLockHolder.lockPath() + \"]\");\n\n        fileLockHolder.tryLock(lockWaitTime);\n    }",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480 +\n 481 +\n 482  ",
            "    /** {@inheritDoc} */\n    @Override public void lock() throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"Try to capture file lock [nodeId=\" +\n                cctx.localNodeId() + \" path=\" + fileLockHolder.lockPath() + \"]\");\n\n        fileLockHolder.tryLock(lockWaitTime);\n\n        System.out.println(\"Lock: \" + fileLockHolder.lockPath() + \" node \" + cctx.igniteInstanceName());\n    }"
        ],
        [
            "GridCacheDatabaseSharedManager::onDeActivate(GridKernalContext)",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440 -\n 441  \n 442  \n 443  \n 444  \n 445  \n 446  ",
            "    /** {@inheritDoc} */\n    @Override public void onDeActivate(GridKernalContext kctx) throws IgniteCheckedException {\n        super.onDeActivate(kctx);\n\n        if (log.isDebugEnabled())\n            log.debug(\"DeActivate database manager [id=\" + cctx.localNodeId() +\n                \" topVer=\" + cctx.discovery().topologyVersionEx() + \" ]\");\n\n        onKernalStop0(true);\n\n        /* Must be here, because after deactivate we can invoke activate and file lock must be already configured */\n        stopping = false;\n\n        fileLockHolder = new FileLockHolder(storeMgr.workDir().getPath(), cctx.kernalContext(), log);\n    }",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440 +\n 441  \n 442  \n 443  \n 444  \n 445  \n 446  ",
            "    /** {@inheritDoc} */\n    @Override public void onDeActivate(GridKernalContext kctx) throws IgniteCheckedException {\n        super.onDeActivate(kctx);\n\n        if (log.isDebugEnabled())\n            log.debug(\"DeActivate database manager [id=\" + cctx.localNodeId() +\n                \" topVer=\" + cctx.discovery().topologyVersionEx() + \" ]\");\n\n        onKernalStop0(false);\n\n        /* Must be here, because after deactivate we can invoke activate and file lock must be already configured */\n        stopping = false;\n\n        fileLockHolder = new FileLockHolder(storeMgr.workDir().getPath(), cctx.kernalContext(), log);\n    }"
        ],
        [
            "FilePageStoreManager::onDeActivate(GridKernalContext)",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  ",
            "    /** {@inheritDoc} */\n    @Override public void onDeActivate(GridKernalContext kctx) throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"DeActivate page store manager [id=\" + cctx.localNodeId() +\n                \" topVer=\" + cctx.discovery().topologyVersionEx() + \" ]\");\n\n        stop0(true);\n    }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 +\n 158 +\n 159  ",
            "    /** {@inheritDoc} */\n    @Override public void onDeActivate(GridKernalContext kctx) throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"DeActivate page store manager [id=\" + cctx.localNodeId() +\n                \" topVer=\" + cctx.discovery().topologyVersionEx() + \" ]\");\n\n        stop0(true);\n\n        idxCacheStores.clear();\n    }"
        ]
    ],
    "c20a1f00535f17f6c399874592516c0992ed472d": [
        [
            "CacheBaselineTopologyTest::testPrimaryLeft()",
            " 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testPrimaryLeft() throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ig = grid(0);\n\n        ig.cluster().active(true);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache =\n            ig.createCache(\n                new CacheConfiguration<Integer, Integer>()\n                    .setName(CACHE_NAME)\n                    .setCacheMode(PARTITIONED)\n                    .setBackups(1)\n                    .setPartitionLossPolicy(READ_ONLY_SAFE)\n                    .setReadFromBackup(true)\n                    .setWriteSynchronizationMode(FULL_SYNC)\n                    .setRebalanceDelay(-1)\n            );\n\n        int key = 1;\n\n        List<ClusterNode> affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert affNodes.size() == 2;\n\n        int primaryIdx = -1;\n\n        IgniteEx primary = null;\n        IgniteEx backup = null;\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            if (grid(i).localNode().equals(affNodes.get(0))) {\n                primaryIdx = i;\n                primary = grid(i);\n            }\n            else if (grid(i).localNode().equals(affNodes.get(1)))\n                backup = grid(i);\n        }\n\n        assert primary != null;\n        assert backup != null;\n\n        Integer val1 = 1;\n        Integer val2 = 2;\n\n        cache.put(key, val1);\n\n        assertEquals(val1, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val1, backup.cache(CACHE_NAME).get(key));\n\n        if (ig == primary) {\n            ig = backup;\n\n            cache = ig.cache(CACHE_NAME);\n        }\n\n        primary.close();\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        cache.put(key, val2);\n\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        primary = startGrid(primaryIdx);\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        primary.cache(CACHE_NAME).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        assertEquals(primary.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n    }",
            " 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488 +\n 489 +\n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517 +\n 518 +\n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testPrimaryLeft() throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ig = grid(0);\n\n        ig.cluster().active(true);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache =\n            ig.createCache(\n                new CacheConfiguration<Integer, Integer>()\n                    .setName(CACHE_NAME)\n                    .setCacheMode(PARTITIONED)\n                    .setBackups(1)\n                    .setPartitionLossPolicy(READ_ONLY_SAFE)\n                    .setReadFromBackup(true)\n                    .setWriteSynchronizationMode(FULL_SYNC)\n                    .setRebalanceDelay(-1)\n            );\n\n        int key = 1;\n\n        List<ClusterNode> affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert affNodes.size() == 2;\n\n        int primaryIdx = -1;\n\n        IgniteEx primary = null;\n        IgniteEx backup = null;\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n            if (grid(i).localNode().equals(affNodes.get(0))) {\n                primaryIdx = i;\n                primary = grid(i);\n            }\n            else if (grid(i).localNode().equals(affNodes.get(1)))\n                backup = grid(i);\n        }\n\n        assert primary != null;\n        assert backup != null;\n\n        Integer val1 = 1;\n        Integer val2 = 2;\n\n        cache.put(key, val1);\n\n        assertEquals(val1, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val1, backup.cache(CACHE_NAME).get(key));\n\n        if (ig == primary) {\n            ig = backup;\n\n            cache = ig.cache(CACHE_NAME);\n        }\n\n        primary.close();\n\n        backup.context().cache().context().exchange().affinityReadyFuture(new AffinityTopologyVersion(5, 0)).get();\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        cache.put(key, val2);\n\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        primary = startGrid(primaryIdx);\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        primary.cache(CACHE_NAME).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        assertEquals(primary.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n    }"
        ],
        [
            "CacheBaselineTopologyTest::testBaselineTopologyChanges(boolean)",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    private void testBaselineTopologyChanges(boolean fromClient) throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ignite;\n\n        if (fromClient) {\n            client = true;\n\n            ignite = startGrid(NODE_COUNT + 10);\n\n            client = false;\n        }\n        else\n            ignite = grid(0);\n\n        ignite.cluster().active(true);\n\n        awaitPartitionMapExchange();\n\n        Map<ClusterNode, Ignite> nodes = new HashMap<>();\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            Ignite ig = grid(i);\n\n            nodes.put(ig.cluster().localNode(), ig);\n        }\n\n        ignite.createCache(\n            new CacheConfiguration<Integer, Integer>()\n                .setName(CACHE_NAME)\n                .setCacheMode(PARTITIONED)\n                .setBackups(1)\n                .setPartitionLossPolicy(READ_ONLY_SAFE)\n        );\n\n        int key = -1;\n\n        for (int k = 0; k < 100_000; k++) {\n            if (!ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(k).contains(ignite.localNode())) {\n                key = k;\n                break;\n            }\n        }\n\n        assert key >= 0;\n\n        Collection<ClusterNode> initialMapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping.size() == 2 : initialMapping;\n\n        ignite.cluster().setBaselineTopology(baselineNodes(nodes.keySet()));\n\n        Set<String> stoppedNodeNames = new HashSet<>();\n\n        ClusterNode node = initialMapping.iterator().next();\n\n        stoppedNodeNames.add(nodes.get(node).name());\n\n        nodes.get(node).close();\n\n        nodes.remove(node);\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == 1 : mapping;\n        assert initialMapping.containsAll(mapping);\n\n        Set<ClusterNode> blt2 = new HashSet<>(ignite.cluster().nodes());\n\n        ignite.cluster().setBaselineTopology(baselineNodes(blt2));\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> initialMapping2 = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping2.size() == 2 : initialMapping2;\n\n        Ignite newIgnite = startGrid(NODE_COUNT);\n\n        awaitPartitionMapExchange();\n\n        mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == initialMapping2.size() : mapping;\n        assert mapping.containsAll(initialMapping2);\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length == 0;\n\n        Set<ClusterNode> blt3 = new HashSet<>(ignite.cluster().nodes());\n\n        ignite.cluster().setBaselineTopology(baselineNodes(blt3));\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> initialMapping3 = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping3.size() == 2;\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length > 0;\n\n        newIgnite = startGrid(NODE_COUNT + 1);\n\n        awaitPartitionMapExchange();\n\n        mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == initialMapping3.size() : mapping;\n        assert mapping.containsAll(initialMapping3);\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length == 0;\n\n        ignite.cluster().setBaselineTopology(null);\n\n        awaitPartitionMapExchange();\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length > 0;\n    }",
            " 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364 +\n 365 +\n 366 +\n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    private void testBaselineTopologyChanges(boolean fromClient) throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ignite;\n\n        if (fromClient) {\n            client = true;\n\n            ignite = startGrid(NODE_COUNT + 10);\n\n            client = false;\n        }\n        else\n            ignite = grid(0);\n\n        ignite.cluster().active(true);\n\n        awaitPartitionMapExchange();\n\n        Map<ClusterNode, Ignite> nodes = new HashMap<>();\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            Ignite ig = grid(i);\n\n            nodes.put(ig.cluster().localNode(), ig);\n        }\n\n        ignite.createCache(\n            new CacheConfiguration<Integer, Integer>()\n                .setName(CACHE_NAME)\n                .setCacheMode(PARTITIONED)\n                .setBackups(1)\n                .setPartitionLossPolicy(READ_ONLY_SAFE)\n        );\n\n        for (int i = 0; i < NODE_COUNT; i++)\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n        int key = -1;\n\n        for (int k = 0; k < 100_000; k++) {\n            if (!ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(k).contains(ignite.localNode())) {\n                key = k;\n                break;\n            }\n        }\n\n        assert key >= 0;\n\n        Collection<ClusterNode> initialMapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping.size() == 2 : initialMapping;\n\n        ignite.cluster().setBaselineTopology(baselineNodes(nodes.keySet()));\n\n        Set<String> stoppedNodeNames = new HashSet<>();\n\n        ClusterNode node = initialMapping.iterator().next();\n\n        stoppedNodeNames.add(nodes.get(node).name());\n\n        nodes.get(node).close();\n\n        nodes.remove(node);\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == 1 : mapping;\n        assert initialMapping.containsAll(mapping);\n\n        Set<ClusterNode> blt2 = new HashSet<>(ignite.cluster().nodes());\n\n        ignite.cluster().setBaselineTopology(baselineNodes(blt2));\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> initialMapping2 = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping2.size() == 2 : initialMapping2;\n\n        Ignite newIgnite = startGrid(NODE_COUNT);\n\n        awaitPartitionMapExchange();\n\n        mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == initialMapping2.size() : mapping;\n        assert mapping.containsAll(initialMapping2);\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length == 0;\n\n        Set<ClusterNode> blt3 = new HashSet<>(ignite.cluster().nodes());\n\n        ignite.cluster().setBaselineTopology(baselineNodes(blt3));\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> initialMapping3 = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping3.size() == 2;\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length > 0;\n\n        newIgnite = startGrid(NODE_COUNT + 1);\n\n        awaitPartitionMapExchange();\n\n        mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == initialMapping3.size() : mapping;\n        assert mapping.containsAll(initialMapping3);\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length == 0;\n\n        ignite.cluster().setBaselineTopology(null);\n\n        awaitPartitionMapExchange();\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length > 0;\n    }"
        ],
        [
            "GridCommonAbstractTest::printPartitionState(String,int)",
            " 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853 -\n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  ",
            "    /**\n     * @param cacheName Cache name.\n     * @param firstParts Count partition for print (will be print first count partition).\n     *\n     * Print partitionState for cache.\n     */\n    protected void printPartitionState(String cacheName, int firstParts) {\n        StringBuilder sb = new StringBuilder();\n\n        sb.append(\"----preload sync futures----\\n\");\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteKernal k = ((IgniteKernal)ig);\n\n            IgniteInternalFuture<?> syncFut = k.internalCache(cacheName)\n                .preloader()\n                .syncFuture();\n\n            sb.append(\"nodeId=\")\n                .append(k.context().localNodeId())\n                .append(\" isDone=\")\n                .append(syncFut.isDone())\n                .append(\"\\n\");\n        }\n\n        sb.append(\"----rebalance futures----\\n\");\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteKernal k = ((IgniteKernal)ig);\n\n            IgniteInternalFuture<?> f = k.internalCache(cacheName)\n                .preloader()\n                .rebalanceFuture();\n\n            try {\n                sb.append(\"nodeId=\").append(k.context().localNodeId())\n                    .append(\" isDone=\").append(f.isDone())\n                    .append(\" res=\").append(f.isDone() ? f.get() : \"N/A\")\n                    .append(\" topVer=\")\n                    .append((U.hasField(f, \"topVer\") ?\n                        U.field(f, \"topVer\") : \"[unknown] may be it is finished future\"))\n                    .append(\"\\n\");\n\n                Map<UUID, T2<Long, Collection<Integer>>> remaining = U.field(f, \"remaining\");\n\n                sb.append(\"remaining:\");\n\n                if (remaining.isEmpty())\n                    sb.append(\"empty\\n\");\n                else\n                    for (Map.Entry<UUID, T2<Long, Collection<Integer>>> e : remaining.entrySet())\n                        sb.append(\"\\nuuid=\").append(e.getKey())\n                            .append(\" startTime=\").append(e.getValue().getKey())\n                            .append(\" parts=\").append(Arrays.toString(e.getValue().getValue().toArray()))\n                            .append(\"\\n\");\n\n            }\n            catch (Throwable e) {\n                log.error(e.getMessage());\n            }\n        }\n\n        sb.append(\"----partition state----\\n\");\n\n        for (Ignite g : G.allGrids()) {\n            IgniteKernal g0 = (IgniteKernal)g;\n\n            sb.append(\"localNodeId=\").append(g0.localNode().id())\n                .append(\" grid=\").append(g0.name())\n                .append(\"\\n\");\n\n            IgniteCacheProxy<?, ?> cache = g0.context().cache().jcache(cacheName);\n\n            GridDhtCacheAdapter<?, ?> dht = dht(cache);\n\n            GridDhtPartitionTopology top = dht.topology();\n\n            int parts = firstParts == 0 ? cache.context()\n                .config()\n                .getAffinity()\n                .partitions() : firstParts;\n\n            for (int p = 0; p < parts; p++) {\n                AffinityTopologyVersion readyVer = dht.context().shared().exchange().readyAffinityVersion();\n\n                Collection<UUID> affNodes = F.nodeIds(dht.context()\n                    .affinity()\n                    .assignment(readyVer)\n                    .idealAssignment()\n                    .get(p));\n\n                GridDhtLocalPartition part = top.localPartition(p, AffinityTopologyVersion.NONE, false);\n\n                sb.append(\"local part=\");\n\n                if (part != null)\n                    sb.append(p).append(\" state=\").append(part.state());\n                else\n                    sb.append(p).append(\" is null\");\n\n                sb.append(\" isAffNode=\")\n                    .append(affNodes.contains(g0.localNode().id()))\n                    .append(\"\\n\");\n\n                for (UUID nodeId : F.nodeIds(g0.context().discovery().allNodes())) {\n                    if (!nodeId.equals(g0.localNode().id()))\n                        sb.append(\" nodeId=\")\n                            .append(nodeId)\n                            .append(\" part=\")\n                            .append(p)\n                            .append(\" state=\")\n                            .append(top.partitionState(nodeId, p))\n                            .append(\" isAffNode=\")\n                            .append(affNodes.contains(nodeId))\n                            .append(\"\\n\");\n                }\n            }\n\n            sb.append(\"\\n\");\n        }\n\n        log.info(\"dump partitions state for <\" + cacheName + \">:\\n\" + sb.toString());\n    }",
            " 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853 +\n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  ",
            "    /**\n     * @param cacheName Cache name.\n     * @param firstParts Count partition for print (will be print first count partition).\n     *\n     * Print partitionState for cache.\n     */\n    protected void printPartitionState(String cacheName, int firstParts) {\n        StringBuilder sb = new StringBuilder();\n\n        sb.append(\"----preload sync futures----\\n\");\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteKernal k = ((IgniteKernal)ig);\n\n            IgniteInternalFuture<?> syncFut = k.internalCache(cacheName)\n                .preloader()\n                .syncFuture();\n\n            sb.append(\"nodeId=\")\n                .append(k.context().localNodeId())\n                .append(\" isDone=\")\n                .append(syncFut.isDone())\n                .append(\"\\n\");\n        }\n\n        sb.append(\"----rebalance futures----\\n\");\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteKernal k = ((IgniteKernal)ig);\n\n            IgniteInternalFuture<?> f = k.internalCache(cacheName)\n                .preloader()\n                .rebalanceFuture();\n\n            try {\n                sb.append(\"nodeId=\").append(k.context().localNodeId())\n                    .append(\" isDone=\").append(f.isDone())\n                    .append(\" res=\").append(f.isDone() ? f.get() : \"N/A\")\n                    .append(\" topVer=\")\n                    .append((U.hasField(f, \"topVer\") ?\n                        String.valueOf(U.field(f, \"topVer\")) : \"[unknown] may be it is finished future\"))\n                    .append(\"\\n\");\n\n                Map<UUID, T2<Long, Collection<Integer>>> remaining = U.field(f, \"remaining\");\n\n                sb.append(\"remaining:\");\n\n                if (remaining.isEmpty())\n                    sb.append(\"empty\\n\");\n                else\n                    for (Map.Entry<UUID, T2<Long, Collection<Integer>>> e : remaining.entrySet())\n                        sb.append(\"\\nuuid=\").append(e.getKey())\n                            .append(\" startTime=\").append(e.getValue().getKey())\n                            .append(\" parts=\").append(Arrays.toString(e.getValue().getValue().toArray()))\n                            .append(\"\\n\");\n\n            }\n            catch (Throwable e) {\n                log.error(e.getMessage());\n            }\n        }\n\n        sb.append(\"----partition state----\\n\");\n\n        for (Ignite g : G.allGrids()) {\n            IgniteKernal g0 = (IgniteKernal)g;\n\n            sb.append(\"localNodeId=\").append(g0.localNode().id())\n                .append(\" grid=\").append(g0.name())\n                .append(\"\\n\");\n\n            IgniteCacheProxy<?, ?> cache = g0.context().cache().jcache(cacheName);\n\n            GridDhtCacheAdapter<?, ?> dht = dht(cache);\n\n            GridDhtPartitionTopology top = dht.topology();\n\n            int parts = firstParts == 0 ? cache.context()\n                .config()\n                .getAffinity()\n                .partitions() : firstParts;\n\n            for (int p = 0; p < parts; p++) {\n                AffinityTopologyVersion readyVer = dht.context().shared().exchange().readyAffinityVersion();\n\n                Collection<UUID> affNodes = F.nodeIds(dht.context()\n                    .affinity()\n                    .assignment(readyVer)\n                    .idealAssignment()\n                    .get(p));\n\n                GridDhtLocalPartition part = top.localPartition(p, AffinityTopologyVersion.NONE, false);\n\n                sb.append(\"local part=\");\n\n                if (part != null)\n                    sb.append(p).append(\" state=\").append(part.state());\n                else\n                    sb.append(p).append(\" is null\");\n\n                sb.append(\" isAffNode=\")\n                    .append(affNodes.contains(g0.localNode().id()))\n                    .append(\"\\n\");\n\n                for (UUID nodeId : F.nodeIds(g0.context().discovery().allNodes())) {\n                    if (!nodeId.equals(g0.localNode().id()))\n                        sb.append(\" nodeId=\")\n                            .append(nodeId)\n                            .append(\" part=\")\n                            .append(p)\n                            .append(\" state=\")\n                            .append(top.partitionState(nodeId, p))\n                            .append(\" isAffNode=\")\n                            .append(affNodes.contains(nodeId))\n                            .append(\"\\n\");\n                }\n            }\n\n            sb.append(\"\\n\");\n        }\n\n        log.info(\"dump partitions state for <\" + cacheName + \">:\\n\" + sb.toString());\n    }"
        ],
        [
            "CacheBaselineTopologyTest::testPrimaryLeftAndClusterRestart()",
            " 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testPrimaryLeftAndClusterRestart() throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ig = grid(0);\n\n        ig.cluster().active(true);\n\n        IgniteCache<Integer, Integer> cache =\n            ig.createCache(\n                new CacheConfiguration<Integer, Integer>()\n                    .setName(CACHE_NAME)\n                    .setWriteSynchronizationMode(FULL_SYNC)\n                    .setCacheMode(PARTITIONED)\n                    .setBackups(1)\n                    .setPartitionLossPolicy(READ_ONLY_SAFE)\n                    .setReadFromBackup(true)\n                    .setRebalanceDelay(-1)\n            );\n\n        int key = 1;\n\n        List<ClusterNode> affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert affNodes.size() == 2;\n\n        int primaryIdx = -1;\n        int backupIdx = -1;\n\n        IgniteEx primary = null;\n        IgniteEx backup = null;\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            if (grid(i).localNode().equals(affNodes.get(0))) {\n                primaryIdx = i;\n                primary = grid(i);\n            }\n            else if (grid(i).localNode().equals(affNodes.get(1))) {\n                backupIdx = i;\n                backup = grid(i);\n            }\n        }\n\n        assert primary != null;\n        assert backup != null;\n\n        Integer val1 = 1;\n        Integer val2 = 2;\n\n        cache.put(key, val1);\n\n        assertEquals(val1, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val1, backup.cache(CACHE_NAME).get(key));\n\n        if (ig == primary) {\n            ig = backup;\n\n            cache = ig.cache(CACHE_NAME);\n        }\n\n        stopGrid(primaryIdx, false);\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        cache.put(key, val2);\n\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        stopAllGrids(false);\n\n        startGrids(NODE_COUNT);\n\n        ig = grid(0);\n        primary = grid(primaryIdx);\n        backup = grid(backupIdx);\n\n        boolean activated = GridTestUtils.waitForCondition(() -> {\n            for (int i = 0; i < NODE_COUNT; i++) {\n                if (!grid(i).cluster().active())\n                    return false;\n            }\n\n            return true;\n        }, 10_000);\n\n        assert activated;\n\n//        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        for (int i = 0; i < NODE_COUNT; i++)\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assertEquals(primary.localNode(), affNodes.get(0));\n        assertEquals(backup.localNode(), affNodes.get(1));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n    }",
            " 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574 +\n 575 +\n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testPrimaryLeftAndClusterRestart() throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ig = grid(0);\n\n        ig.cluster().active(true);\n\n        IgniteCache<Integer, Integer> cache =\n            ig.createCache(\n                new CacheConfiguration<Integer, Integer>()\n                    .setName(CACHE_NAME)\n                    .setWriteSynchronizationMode(FULL_SYNC)\n                    .setCacheMode(PARTITIONED)\n                    .setBackups(1)\n                    .setPartitionLossPolicy(READ_ONLY_SAFE)\n                    .setReadFromBackup(true)\n                    .setRebalanceDelay(-1)\n            );\n\n        int key = 1;\n\n        List<ClusterNode> affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert affNodes.size() == 2;\n\n        int primaryIdx = -1;\n        int backupIdx = -1;\n\n        IgniteEx primary = null;\n        IgniteEx backup = null;\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n            if (grid(i).localNode().equals(affNodes.get(0))) {\n                primaryIdx = i;\n                primary = grid(i);\n            }\n            else if (grid(i).localNode().equals(affNodes.get(1))) {\n                backupIdx = i;\n                backup = grid(i);\n            }\n        }\n\n        assert primary != null;\n        assert backup != null;\n\n        Integer val1 = 1;\n        Integer val2 = 2;\n\n        cache.put(key, val1);\n\n        assertEquals(val1, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val1, backup.cache(CACHE_NAME).get(key));\n\n        if (ig == primary) {\n            ig = backup;\n\n            cache = ig.cache(CACHE_NAME);\n        }\n\n        stopGrid(primaryIdx, false);\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        cache.put(key, val2);\n\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        stopAllGrids(false);\n\n        startGrids(NODE_COUNT);\n\n        ig = grid(0);\n        primary = grid(primaryIdx);\n        backup = grid(backupIdx);\n\n        boolean activated = GridTestUtils.waitForCondition(() -> {\n            for (int i = 0; i < NODE_COUNT; i++) {\n                if (!grid(i).cluster().active())\n                    return false;\n            }\n\n            return true;\n        }, 10_000);\n\n        assert activated;\n\n//        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        for (int i = 0; i < NODE_COUNT; i++)\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assertEquals(primary.localNode(), affNodes.get(0));\n        assertEquals(backup.localNode(), affNodes.get(1));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n    }"
        ],
        [
            "CacheBaselineTopologyTest::getConfiguration(String)",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(igniteInstanceName);\n\n        cfg.setConsistentId(igniteInstanceName);\n\n        cfg.setDataStorageConfiguration(\n            new DataStorageConfiguration().setDefaultDataRegionConfiguration(\n                new DataRegionConfiguration()\n                    .setPersistenceEnabled(true)\n                    .setMaxSize(100 * 1024 * 1024)\n                    .setInitialSize(100 * 1024 * 1024)\n            )\n            .setDataRegionConfigurations(\n                new DataRegionConfiguration()\n                .setName(\"memory\")\n                .setPersistenceEnabled(false)\n                .setMaxSize(100 * 1024 * 1024)\n                .setInitialSize(100 * 1024 * 1024)\n            )\n            .setWalMode(WALMode.LOG_ONLY)\n        );\n\n        if (client)\n            cfg.setClientMode(true);\n\n        if (delayRebalance)\n            cfg.setCommunicationSpi(new DelayRebalanceCommunicationSpi());\n\n        return cfg;\n    }",
            " 104  \n 105  \n 106  \n 107  \n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(igniteInstanceName);\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setConsistentId(igniteInstanceName);\n\n        cfg.setDataStorageConfiguration(\n            new DataStorageConfiguration().setDefaultDataRegionConfiguration(\n                new DataRegionConfiguration()\n                    .setPersistenceEnabled(true)\n                    .setMaxSize(100 * 1024 * 1024)\n                    .setInitialSize(100 * 1024 * 1024)\n            )\n            .setDataRegionConfigurations(\n                new DataRegionConfiguration()\n                .setName(\"memory\")\n                .setPersistenceEnabled(false)\n                .setMaxSize(100 * 1024 * 1024)\n                .setInitialSize(100 * 1024 * 1024)\n            )\n            .setWalMode(WALMode.LOG_ONLY)\n        );\n\n        if (client)\n            cfg.setClientMode(true);\n\n        if (delayRebalance)\n            cfg.setCommunicationSpi(new DelayRebalanceCommunicationSpi());\n\n        return cfg;\n    }"
        ]
    ],
    "210ad201a2f6df071e3a2e1c5812e9e28afa1be5": [
        [
            "IgniteAbstractStandByClientReconnectTest::startNodes(CountDownLatch)",
            " 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182 -\n 183 -\n 184 -\n 185  ",
            "    protected void startNodes(CountDownLatch activateLatch) throws Exception {\n        IgniteConfiguration cfg1 = getConfiguration(node1)\n            .setCacheConfiguration(ccfg1static, ccfg1staticWithFilter);\n\n        IgniteConfiguration cfg2 = getConfiguration(node2)\n            .setCacheConfiguration(ccfg2static, ccfg2staticWithFilter);\n\n        IgniteConfiguration cfg3 = getConfiguration(nodeClient)\n            .setCacheConfiguration(ccfg3static, ccfg3staticWithFilter);\n\n        if (activateLatch != null)\n            cfg3.setDiscoverySpi(\n                new AwaitTcpDiscoverySpi(activateLatch)\n                    .setIpFinder(clientIpFinder)\n            );\n\n        cfg3.setClientMode(true);\n\n        IgniteEx ig1 = startGrid(cfg1);\n        IgniteEx ig2 = startGrid(cfg2);\n        IgniteEx client = startGrid(cfg3);\n    }",
            " 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 +\n 226 +\n 227 +\n 228  ",
            "    /**\n     * @param activateLatch Activate latch. Will be fired when cluster is activated.\n     * @throws Exception If failed.\n     */\n    protected void startNodes(CountDownLatch activateLatch) throws Exception {\n        IgniteConfiguration cfg1 = getConfiguration(node1)\n            .setCacheConfiguration(ccfg1static, ccfg1staticWithFilter);\n\n        IgniteConfiguration cfg2 = getConfiguration(node2)\n            .setCacheConfiguration(ccfg2static, ccfg2staticWithFilter);\n\n        IgniteConfiguration cfg3 = getConfiguration(nodeClient)\n            .setCacheConfiguration(ccfg3static, ccfg3staticWithFilter);\n\n        if (activateLatch != null)\n            cfg3.setDiscoverySpi(\n                new AwaitTcpDiscoverySpi(activateLatch)\n                    .setIpFinder(clientIpFinder)\n            );\n\n        cfg3.setClientMode(true);\n\n        startGrid(cfg1);\n        startGrid(cfg2);\n        startGrid(cfg3);\n    }"
        ],
        [
            "IgniteStandByClientReconnectTest::testInActiveClientReconnectToInActiveCluster()",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255 -\n 256 -\n 257 -\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 -\n 273 -\n 274 -\n 275  \n 276 -\n 277  \n 278 -\n 279 -\n 280 -\n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testInActiveClientReconnectToInActiveCluster() throws Exception {\n        startNodes(null);\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        reconnectedLatch.await();\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        client.active(true);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkStaticCaches();\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255 +\n 256 +\n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 +\n 273 +\n 274 +\n 275  \n 276 +\n 277  \n 278 +\n 279 +\n 280 +\n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testInActiveClientReconnectToInActiveCluster() throws Exception {\n        startNodes(null);\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        reconnectedLatch.await();\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        client.cluster().active(true);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkStaticCaches();\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }"
        ],
        [
            "IgniteStandByClientReconnectTest::testActiveClientReconnectToActiveCluster()",
            "  27  \n  28  \n  29  \n  30  \n  31  \n  32  \n  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41 -\n  42 -\n  43 -\n  44  \n  45 -\n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 -\n  62 -\n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86 -\n  87 -\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 -\n  98 -\n  99 -\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testActiveClientReconnectToActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        info(\">>>> star grid\");\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        client.active(true);\n\n        info(\">>>> activate grid\");\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n        checkDescriptors(client, staticCacheNames);\n\n        checkStaticCaches();\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        info(\">>>> dynamic start [\" + ccfgDynamicName + \", \" + ccfgDynamicWithFilterName + \"]\");\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        info(\">>>> stop servers\");\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        info(\">>>> activate new servers\");\n\n        ig1.active(true);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n\n        activateLatch.countDown();\n\n        info(\">>>> reconnect client\");\n\n        reconnectedLatch.await();\n\n        info(\">>>> client reconnected\");\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }",
            "  27  \n  28  \n  29  \n  30  \n  31  \n  32  \n  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41 +\n  42 +\n  43 +\n  44  \n  45 +\n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 +\n  85  \n  86 +\n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 +\n  98 +\n  99 +\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testActiveClientReconnectToActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        info(\">>>> star grid\");\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        client.cluster().active(true);\n\n        info(\">>>> activate grid\");\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n        checkDescriptors(client, staticCacheNames);\n\n        checkStaticCaches();\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        info(\">>>> dynamic start [\" + ccfgDynamicName + \", \" + ccfgDynamicWithFilterName + \"]\");\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        info(\">>>> stop servers\");\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        info(\">>>> activate new servers\");\n\n        ig1.cluster().active(true);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n\n        activateLatch.countDown();\n\n        info(\">>>> reconnect client\");\n\n        reconnectedLatch.await();\n\n        info(\">>>> client reconnected\");\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }"
        ],
        [
            "CacheAffinitySharedManager::onDiscoveryEvent(int,DiscoveryCustomMessage,ClusterNode,AffinityTopologyVersion,DiscoveryDataClusterState)",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 -\n 168 -\n 169 -\n 170  \n 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  ",
            "    /**\n     * Callback invoked from discovery thread when discovery message is received.\n     *\n     * @param type Event type.\n     * @param customMsg Custom message instance.\n     * @param node Event node.\n     * @param topVer Topology version.\n     * @param state Cluster state.\n     */\n    void onDiscoveryEvent(int type,\n        @Nullable DiscoveryCustomMessage customMsg,\n        ClusterNode node,\n        AffinityTopologyVersion topVer,\n        DiscoveryDataClusterState state) {\n        if ((state.transition() || !state.active()) &&\n            !DiscoveryCustomEvent.requiresCentralizedAffinityAssignment(customMsg))\n            return;\n\n        if (type == EVT_NODE_JOINED && node.isLocal())\n            lastAffVer = null;\n\n        if ((!CU.clientNode(node) && (type == EVT_NODE_FAILED || type == EVT_NODE_JOINED || type == EVT_NODE_LEFT)) ||\n            DiscoveryCustomEvent.requiresCentralizedAffinityAssignment(customMsg)) {\n            synchronized (mux) {\n                assert lastAffVer == null || topVer.compareTo(lastAffVer) > 0;\n\n                lastAffVer = topVer;\n            }\n        }\n    }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165 +\n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 +\n 175 +\n 176  \n 177  \n 178  \n 179  \n 180  ",
            "    /**\n     * Callback invoked from discovery thread when discovery message is received.\n     *\n     * @param type Event type.\n     * @param customMsg Custom message instance.\n     * @param node Event node.\n     * @param topVer Topology version.\n     * @param state Cluster state.\n     */\n    void onDiscoveryEvent(int type,\n        @Nullable DiscoveryCustomMessage customMsg,\n        ClusterNode node,\n        AffinityTopologyVersion topVer,\n        DiscoveryDataClusterState state) {\n        if (type == EVT_NODE_JOINED && node.isLocal())\n            lastAffVer = null;\n\n        if ((state.transition() || !state.active()) &&\n            !DiscoveryCustomEvent.requiresCentralizedAffinityAssignment(customMsg))\n            return;\n\n        if ((!CU.clientNode(node) && (type == EVT_NODE_FAILED || type == EVT_NODE_JOINED || type == EVT_NODE_LEFT)) ||\n            DiscoveryCustomEvent.requiresCentralizedAffinityAssignment(customMsg)) {\n            synchronized (mux) {\n                assert lastAffVer == null || topVer.compareTo(lastAffVer) > 0 :\n                    \"lastAffVer=\" + lastAffVer + \", topVer=\" + topVer + \", customMsg=\" + customMsg;\n\n                lastAffVer = topVer;\n            }\n        }\n    }"
        ],
        [
            "IgniteStandByClientReconnectTest::testInActiveClientReconnectToActiveCluster()",
            " 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 -\n 201 -\n 202 -\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 -\n 216  \n 217 -\n 218 -\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 -\n 228 -\n 229 -\n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testInActiveClientReconnectToActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        ig1.active(true);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n\n        activateLatch.countDown();\n\n        reconnectedLatch.await();\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }",
            " 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 +\n 201 +\n 202 +\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 +\n 216  \n 217 +\n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 +\n 228 +\n 229 +\n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testInActiveClientReconnectToActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        ig1.cluster().active(true);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n\n        activateLatch.countDown();\n\n        reconnectedLatch.await();\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }"
        ],
        [
            "IgniteStandByClientReconnectTest::testActiveClientReconnectToInActiveCluster()",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123 -\n 124 -\n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 -\n 147 -\n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 -\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 -\n 172 -\n 173 -\n 174  \n 175 -\n 176  \n 177 -\n 178 -\n 179 -\n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testActiveClientReconnectToInActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        info(\">>>> star grid\");\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        info(\">>>> activate grid\");\n\n        client.active(true);\n\n        checkStaticCaches();\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n        checkDescriptors(client, staticCacheNames);\n\n        info(\">>>> dynamic start [\" + ccfgDynamicName + \", \" + ccfgDynamicWithFilterName + \"]\");\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        info(\">>>> stop \" + node2);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig1.active(false);\n\n        activateLatch.countDown();\n\n        info(\">>>> restart \" + node2);\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        reconnectedLatch.await();\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        client.active(true);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124 +\n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147 +\n 148 +\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 +\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172 +\n 173 +\n 174  \n 175 +\n 176  \n 177 +\n 178 +\n 179 +\n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testActiveClientReconnectToInActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        info(\">>>> star grid\");\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        info(\">>>> activate grid\");\n\n        client.cluster().active(true);\n\n        checkStaticCaches();\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n        checkDescriptors(client, staticCacheNames);\n\n        info(\">>>> dynamic start [\" + ccfgDynamicName + \", \" + ccfgDynamicWithFilterName + \"]\");\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        info(\">>>> stop \" + node2);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig1.cluster().active(false);\n\n        activateLatch.countDown();\n\n        info(\">>>> restart \" + node2);\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        reconnectedLatch.await();\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        client.cluster().active(true);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }"
        ],
        [
            "IgniteAbstractStandByClientReconnectTest::checkOnlySystemCaches()",
            " 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256 -\n 257 -\n 258 -\n 259  ",
            "    protected void checkOnlySystemCaches() {\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        Assert.assertNull(ig1.cache(ccfg1staticName));\n        Assert.assertNull(ig1.cache(ccfg2staticName));\n        Assert.assertNull(ig1.cache(ccfg3staticName));\n\n        Assert.assertNull(ig1.cache(ccfg1staticWithFilterName));\n        Assert.assertNull(ig1.cache(ccfg2staticWithFilterName));\n\n        Assert.assertNull(ig2.cache(ccfg1staticName));\n        Assert.assertNull(ig2.cache(ccfg2staticName));\n        Assert.assertNull(ig2.cache(ccfg3staticName));\n\n        Assert.assertNull(ig2.cache(ccfg3staticWithFilterName));\n        Assert.assertNull(ig2.cache(ccfg2staticWithFilterName));\n\n        Assert.assertNull(client.cache(ccfg1staticName));\n        Assert.assertNull(client.cache(ccfg2staticName));\n        Assert.assertNull(client.cache(ccfg3staticName));\n\n        Assert.assertNull(client.cache(ccfg3staticWithFilterName));\n        Assert.assertNull(client.cache(ccfg1staticWithFilterName));\n\n        checkDescriptors(ig1,Collections.<String>emptySet());\n        checkDescriptors(ig2,Collections.<String>emptySet());\n        checkDescriptors(client, Collections.<String>emptySet());\n    }",
            " 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308 +\n 309 +\n 310 +\n 311  ",
            "    /**\n     *\n     */\n    protected void checkOnlySystemCaches() {\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        Assert.assertNull(ig1.cache(ccfg1staticName));\n        Assert.assertNull(ig1.cache(ccfg2staticName));\n        Assert.assertNull(ig1.cache(ccfg3staticName));\n\n        Assert.assertNull(ig1.cache(ccfg1staticWithFilterName));\n        Assert.assertNull(ig1.cache(ccfg2staticWithFilterName));\n\n        Assert.assertNull(ig2.cache(ccfg1staticName));\n        Assert.assertNull(ig2.cache(ccfg2staticName));\n        Assert.assertNull(ig2.cache(ccfg3staticName));\n\n        Assert.assertNull(ig2.cache(ccfg3staticWithFilterName));\n        Assert.assertNull(ig2.cache(ccfg2staticWithFilterName));\n\n        Assert.assertNull(client.cache(ccfg1staticName));\n        Assert.assertNull(client.cache(ccfg2staticName));\n        Assert.assertNull(client.cache(ccfg3staticName));\n\n        Assert.assertNull(client.cache(ccfg3staticWithFilterName));\n        Assert.assertNull(client.cache(ccfg1staticWithFilterName));\n\n        checkDescriptors(ig1,Collections.emptySet());\n        checkDescriptors(ig2,Collections.emptySet());\n        checkDescriptors(client, Collections.emptySet());\n    }"
        ]
    ],
    "a0b56442365bad1100b078da33f00beb6a844cf0": [
        [
            "IgniteMarshallerCacheFSRestoreTest::afterTest()",
            "  89  \n  90  \n  91  \n  92  ",
            "    /** {@inheritDoc} */\n    @Override protected void afterTest() throws Exception {\n        cleanUpWorkDir();\n    }",
            "  89  \n  90  \n  91  \n  92 +\n  93 +\n  94  ",
            "    /** {@inheritDoc} */\n    @Override protected void afterTest() throws Exception {\n        cleanUpWorkDir();\n\n        stopAllGrids();\n    }"
        ]
    ],
    "79278e074e313e9ca23b5ccc97926e1f5d5cc031": [
        [
            "GridCachePartitionExchangeManager::processSinglePartitionUpdate(ClusterNode,GridDhtPartitionsSingleMessage)",
            "1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555 -\n1556 -\n1557  \n1558  \n1559  \n1560  \n1561  ",
            "    /**\n     * @param node Sender cluster node.\n     * @param msg Message.\n     */\n    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                        msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    if (grp != null &&\n                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                        continue;\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null) {\n                        updated |= top.update(null, entry.getValue(), false);\n\n                        cctx.affinity().checkRebalanceState(top, grpId);\n                    }\n                }\n\n                if (updated)\n                    scheduleResendPartitions();\n            }\n            else\n                exchangeFuture(msg.exchangeId(), null, null, null, null).onReceiveSingleMessage(node, msg);\n        }\n        finally {\n            leaveBusy();\n        }\n    }",
            "1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555 +\n1556 +\n1557 +\n1558 +\n1559 +\n1560 +\n1561 +\n1562 +\n1563  \n1564  \n1565  \n1566  \n1567  ",
            "    /**\n     * @param node Sender cluster node.\n     * @param msg Message.\n     */\n    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                        msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    if (grp != null &&\n                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                        continue;\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null) {\n                        updated |= top.update(null, entry.getValue(), false);\n\n                        cctx.affinity().checkRebalanceState(top, grpId);\n                    }\n                }\n\n                if (updated)\n                    scheduleResendPartitions();\n            }\n            else {\n                GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId(), null, null, null, null);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Notifying exchange future about single message: \" + exchFut);\n\n                exchFut.onReceiveSingleMessage(node, msg);\n            }\n        }\n        finally {\n            leaveBusy();\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onReceiveSingleMessage(ClusterNode,GridDhtPartitionsSingleMessage)",
            "2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  ",
            "    /**\n     * Processing of received single message. Actual processing in future may be delayed if init method was not\n     * completed, see {@link #initDone()}\n     *\n     * @param node Sender node.\n     * @param msg Single partition info.\n     */\n    public void onReceiveSingleMessage(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        assert !node.isDaemon() : node;\n        assert msg != null;\n        assert exchId.equals(msg.exchangeId()) : msg;\n        assert !cctx.kernalContext().clientNode();\n\n        if (msg.restoreState()) {\n            InitNewCoordinatorFuture newCrdFut0;\n\n            synchronized (mux) {\n                assert newCrdFut != null;\n\n                newCrdFut0 = newCrdFut;\n            }\n\n            newCrdFut0.onMessage(node, msg);\n\n            return;\n        }\n\n        if (!msg.client()) {\n            assert msg.lastVersion() != null : msg;\n\n            updateLastVersion(msg.lastVersion());\n        }\n\n        GridDhtPartitionsExchangeFuture mergedWith0 = null;\n\n        synchronized (mux) {\n            if (state == ExchangeLocalState.MERGED) {\n                assert mergedWith != null;\n\n                mergedWith0 = mergedWith;\n            }\n            else {\n                assert state != ExchangeLocalState.CLIENT;\n\n                if (exchangeId().isJoined() && node.id().equals(exchId.nodeId()))\n                    pendingJoinMsg = msg;\n            }\n        }\n\n        if (mergedWith0 != null) {\n            mergedWith0.processMergedMessage(node, msg);\n\n            return;\n        }\n\n        initFut.listen(new CI1<IgniteInternalFuture<Boolean>>() {\n            @Override public void apply(IgniteInternalFuture<Boolean> f) {\n                try {\n                    if (!f.get())\n                        return;\n                }\n                catch (IgniteCheckedException e) {\n                    U.error(log, \"Failed to initialize exchange future: \" + this, e);\n\n                    return;\n                }\n\n                processSingleMessage(node.id(), msg);\n            }\n        });\n    }",
            "2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095 +\n2096 +\n2097 +\n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  ",
            "    /**\n     * Processing of received single message. Actual processing in future may be delayed if init method was not\n     * completed, see {@link #initDone()}\n     *\n     * @param node Sender node.\n     * @param msg Single partition info.\n     */\n    public void onReceiveSingleMessage(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        assert !node.isDaemon() : node;\n        assert msg != null;\n        assert exchId.equals(msg.exchangeId()) : msg;\n        assert !cctx.kernalContext().clientNode();\n\n        if (msg.restoreState()) {\n            InitNewCoordinatorFuture newCrdFut0;\n\n            synchronized (mux) {\n                assert newCrdFut != null;\n\n                newCrdFut0 = newCrdFut;\n            }\n\n            newCrdFut0.onMessage(node, msg);\n\n            return;\n        }\n\n        if (!msg.client()) {\n            assert msg.lastVersion() != null : msg;\n\n            updateLastVersion(msg.lastVersion());\n        }\n\n        GridDhtPartitionsExchangeFuture mergedWith0 = null;\n\n        synchronized (mux) {\n            if (state == ExchangeLocalState.MERGED) {\n                assert mergedWith != null;\n\n                mergedWith0 = mergedWith;\n            }\n            else {\n                assert state != ExchangeLocalState.CLIENT;\n\n                if (exchangeId().isJoined() && node.id().equals(exchId.nodeId()))\n                    pendingJoinMsg = msg;\n            }\n        }\n\n        if (mergedWith0 != null) {\n            mergedWith0.processMergedMessage(node, msg);\n\n            if (log.isDebugEnabled())\n                log.debug(\"Merged message processed, message handling finished: \" + msg);\n\n            return;\n        }\n\n        initFut.listen(new CI1<IgniteInternalFuture<Boolean>>() {\n            @Override public void apply(IgniteInternalFuture<Boolean> f) {\n                try {\n                    if (!f.get())\n                        return;\n                }\n                catch (IgniteCheckedException e) {\n                    U.error(log, \"Failed to initialize exchange future: \" + this, e);\n\n                    return;\n                }\n\n                processSingleMessage(node.id(), msg);\n            }\n        });\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processSingleMessage(UUID,GridDhtPartitionsSingleMessage)",
            "2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  ",
            "    /**\n     * Note this method performs heavy updatePartitionSingleMap operation, this operation is moved out from the\n     * synchronized block. Only count of such updates {@link #pendingSingleUpdates} is managed under critical section.\n     *\n     * @param nodeId Sender node.\n     * @param msg Partition single message.\n     */\n    private void processSingleMessage(UUID nodeId, GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(nodeId, msg);\n\n            return;\n        }\n\n        boolean allReceived = false; // Received all expected messages.\n        boolean updateSingleMap = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            assert crd != null;\n\n            switch (state) {\n                case DONE: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Received single message, already done [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n\n                    assert finishState != null;\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD: {\n                    assert crd.isLocal() : crd;\n\n                    if (remaining.remove(nodeId)) {\n                        updateSingleMap = true;\n\n                        pendingSingleUpdates++;\n\n                        if (stateChangeExchange() && msg.getError() != null)\n                            changeGlobalStateExceptions.put(nodeId, msg.getError());\n\n                        allReceived = remaining.isEmpty();\n\n                        if (log.isInfoEnabled()) {\n                            log.info(\"Coordinator received single message [ver=\" + initialVersion() +\n                                \", node=\" + nodeId +\n                                \", allReceived=\" + allReceived + ']');\n                        }\n                    }\n\n                    break;\n                }\n\n                case SRV:\n                case BECOME_CRD: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Non-coordinator received single message [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + \", state=\" + state + ']');\n                    }\n\n                    pendingSingleMsgs.put(nodeId, msg);\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, nodeId);\n\n            return;\n        }\n\n        if (updateSingleMap) {\n            try {\n                // Do not update partition map, in case cluster transitioning to inactive state.\n                if (!deactivateCluster())\n                    updatePartitionSingleMap(nodeId, msg);\n            }\n            finally {\n                synchronized (mux) {\n                    assert pendingSingleUpdates > 0;\n\n                    pendingSingleUpdates--;\n\n                    if (pendingSingleUpdates == 0)\n                        mux.notifyAll();\n                }\n            }\n        }\n\n        if (allReceived) {\n            if (!awaitSingleMapUpdates())\n                return;\n\n            onAllReceived(null);\n        }\n    }",
            "2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219 +\n2220 +\n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  ",
            "    /**\n     * Note this method performs heavy updatePartitionSingleMap operation, this operation is moved out from the\n     * synchronized block. Only count of such updates {@link #pendingSingleUpdates} is managed under critical section.\n     *\n     * @param nodeId Sender node.\n     * @param msg Partition single message.\n     */\n    private void processSingleMessage(UUID nodeId, GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(nodeId, msg);\n\n            return;\n        }\n\n        boolean allReceived = false; // Received all expected messages.\n        boolean updateSingleMap = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            assert crd != null;\n\n            switch (state) {\n                case DONE: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Received single message, already done [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n\n                    assert finishState != null;\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD: {\n                    assert crd.isLocal() : crd;\n\n                    if (remaining.remove(nodeId)) {\n                        updateSingleMap = true;\n\n                        pendingSingleUpdates++;\n\n                        if (stateChangeExchange() && msg.getError() != null)\n                            changeGlobalStateExceptions.put(nodeId, msg.getError());\n\n                        allReceived = remaining.isEmpty();\n\n                        if (log.isInfoEnabled()) {\n                            log.info(\"Coordinator received single message [ver=\" + initialVersion() +\n                                \", node=\" + nodeId +\n                                \", allReceived=\" + allReceived + ']');\n                        }\n                    }\n                    else if (log.isDebugEnabled())\n                        log.debug(\"Coordinator received single message it didn't expect to receive: \" + msg);\n\n                    break;\n                }\n\n                case SRV:\n                case BECOME_CRD: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Non-coordinator received single message [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + \", state=\" + state + ']');\n                    }\n\n                    pendingSingleMsgs.put(nodeId, msg);\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, nodeId);\n\n            return;\n        }\n\n        if (updateSingleMap) {\n            try {\n                // Do not update partition map, in case cluster transitioning to inactive state.\n                if (!deactivateCluster())\n                    updatePartitionSingleMap(nodeId, msg);\n            }\n            finally {\n                synchronized (mux) {\n                    assert pendingSingleUpdates > 0;\n\n                    pendingSingleUpdates--;\n\n                    if (pendingSingleUpdates == 0)\n                        mux.notifyAll();\n                }\n            }\n        }\n\n        if (allReceived) {\n            if (!awaitSingleMapUpdates())\n                return;\n\n            onAllReceived(null);\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::sendAllPartitionsToNode(FinishState,GridDhtPartitionsSingleMessage,UUID)",
            "2851  \n2852  \n2853  \n2854  \n2855  \n2856  \n2857  \n2858  \n2859  \n2860  \n2861  \n2862  \n2863  \n2864  \n2865  \n2866  \n2867  \n2868  \n2869  \n2870  \n2871  \n2872  \n2873  \n2874  \n2875  \n2876  \n2877  \n2878  \n2879  \n2880  \n2881  \n2882  \n2883  \n2884  \n2885  \n2886  \n2887  \n2888  \n2889  \n2890  \n2891  \n2892  \n2893  \n2894  ",
            "    /**\n     * @param finishState State.\n     * @param msg Request.\n     * @param nodeId Node ID.\n     */\n    private void sendAllPartitionsToNode(FinishState finishState, GridDhtPartitionsSingleMessage msg, UUID nodeId) {\n        ClusterNode node = cctx.node(nodeId);\n\n        if (node != null) {\n            GridDhtPartitionsFullMessage fullMsg = finishState.msg.copy();\n\n            Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n            if (affReq != null) {\n                Map<Integer, CacheGroupAffinityMessage> aff = CacheGroupAffinityMessage.createAffinityMessages(\n                    cctx,\n                    finishState.resTopVer,\n                    affReq,\n                    null);\n\n                fullMsg.joinedNodeAffinity(aff);\n            }\n\n            if (!fullMsg.exchangeId().equals(msg.exchangeId())) {\n                fullMsg = fullMsg.copy();\n\n                fullMsg.exchangeId(msg.exchangeId());\n            }\n\n            try {\n                cctx.io().send(node, fullMsg, SYSTEM_POOL);\n            }\n            catch (ClusterTopologyCheckedException e) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Failed to send partitions, node failed: \" + node);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to send partitions [node=\" + node + ']', e);\n            }\n        }\n        else if (log.isDebugEnabled())\n            log.debug(\"Failed to send partitions, node failed: \" + nodeId);\n\n    }",
            "2868  \n2869  \n2870  \n2871  \n2872  \n2873  \n2874  \n2875  \n2876  \n2877  \n2878  \n2879  \n2880  \n2881  \n2882  \n2883  \n2884  \n2885  \n2886  \n2887  \n2888  \n2889  \n2890  \n2891  \n2892  \n2893  \n2894  \n2895  \n2896  \n2897  \n2898  \n2899 +\n2900 +\n2901 +\n2902 +\n2903 +\n2904 +\n2905 +\n2906  \n2907  \n2908  \n2909  \n2910  \n2911  \n2912  \n2913  \n2914  \n2915  \n2916  \n2917  \n2918  ",
            "    /**\n     * @param finishState State.\n     * @param msg Request.\n     * @param nodeId Node ID.\n     */\n    private void sendAllPartitionsToNode(FinishState finishState, GridDhtPartitionsSingleMessage msg, UUID nodeId) {\n        ClusterNode node = cctx.node(nodeId);\n\n        if (node != null) {\n            GridDhtPartitionsFullMessage fullMsg = finishState.msg.copy();\n\n            Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n            if (affReq != null) {\n                Map<Integer, CacheGroupAffinityMessage> aff = CacheGroupAffinityMessage.createAffinityMessages(\n                    cctx,\n                    finishState.resTopVer,\n                    affReq,\n                    null);\n\n                fullMsg.joinedNodeAffinity(aff);\n            }\n\n            if (!fullMsg.exchangeId().equals(msg.exchangeId())) {\n                fullMsg = fullMsg.copy();\n\n                fullMsg.exchangeId(msg.exchangeId());\n            }\n\n            try {\n                cctx.io().send(node, fullMsg, SYSTEM_POOL);\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Full message was sent to node: \" +\n                        node +\n                        \", fullMsg: \" + fullMsg\n                    );\n                }\n            }\n            catch (ClusterTopologyCheckedException e) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Failed to send partitions, node failed: \" + node);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to send partitions [node=\" + node + ']', e);\n            }\n        }\n        else if (log.isDebugEnabled())\n            log.debug(\"Failed to send partitions, node failed: \" + nodeId);\n\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::waitAndReplyToNode(UUID,GridDhtPartitionsSingleMessage)",
            "2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136 -\n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  ",
            "    /**\n     * @param nodeId Node ID.\n     * @param msg Client's message.\n     */\n    public void waitAndReplyToNode(final UUID nodeId, final GridDhtPartitionsSingleMessage msg) {\n        listen(new CI1<IgniteInternalFuture<AffinityTopologyVersion>>() {\n            @Override public void apply(IgniteInternalFuture<AffinityTopologyVersion> fut) {\n                if (cctx.kernalContext().isStopping())\n                    return;\n\n                FinishState finishState0;\n\n                synchronized (mux) {\n                    finishState0 = finishState;\n                }\n\n                if (finishState0 == null) {\n                    assert firstDiscoEvt.type() == EVT_NODE_JOINED && CU.clientNode(firstDiscoEvt.eventNode()) : this;\n\n                    ClusterNode node = cctx.node(nodeId);\n\n                    if (node == null)\n                        return;\n\n                    finishState0 = new FinishState(cctx.localNodeId(),\n                        initialVersion(),\n                        createPartitionsMessage(true, node.version().compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0));\n                }\n\n                sendAllPartitionsToNode(finishState0, msg, nodeId);\n            }\n        });\n    }",
            "2118  \n2119  \n2120  \n2121  \n2122  \n2123 +\n2124 +\n2125 +\n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142 +\n2143 +\n2144 +\n2145 +\n2146 +\n2147 +\n2148 +\n2149 +\n2150 +\n2151  \n2152 +\n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  ",
            "    /**\n     * @param nodeId Node ID.\n     * @param msg Client's message.\n     */\n    public void waitAndReplyToNode(final UUID nodeId, final GridDhtPartitionsSingleMessage msg) {\n        if (log.isDebugEnabled())\n            log.debug(\"Single message will be handled on completion of exchange future: \" + this);\n\n        listen(new CI1<IgniteInternalFuture<AffinityTopologyVersion>>() {\n            @Override public void apply(IgniteInternalFuture<AffinityTopologyVersion> fut) {\n                if (cctx.kernalContext().isStopping())\n                    return;\n\n                FinishState finishState0;\n\n                synchronized (mux) {\n                    finishState0 = finishState;\n                }\n\n                if (finishState0 == null) {\n                    assert firstDiscoEvt.type() == EVT_NODE_JOINED && CU.clientNode(firstDiscoEvt.eventNode()) : this;\n\n                    ClusterNode node = cctx.node(nodeId);\n\n                    if (node == null) {\n                        if (log.isDebugEnabled()) {\n                            log.debug(\"No node found for nodeId: \" +\n                                nodeId +\n                                \", handling of single message will be stopped: \" +\n                                msg\n                            );\n                        }\n\n                        return;\n                    }\n\n                    finishState0 = new FinishState(cctx.localNodeId(),\n                        initialVersion(),\n                        createPartitionsMessage(true, node.version().compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0));\n                }\n\n                sendAllPartitionsToNode(finishState0, msg, nodeId);\n            }\n        });\n    }"
        ]
    ],
    "6ed872b4e49c2c767dad975aa9613fc881726027": [
        [
            "TcpCommunicationSpi::nodeAddresses(ClusterNode)",
            "2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044 -\n3045  \n3046  \n3047  \n3048  ",
            "    /**\n     * @param node Node.\n     * @return Node addresses.\n     * @throws IgniteCheckedException If node does not have addresses.\n     */\n    private LinkedHashSet<InetSocketAddress> nodeAddresses(ClusterNode node) throws IgniteCheckedException {\n        Collection<String> rmtAddrs0 = node.attribute(createSpiAttributeName(ATTR_ADDRS));\n        Collection<String> rmtHostNames0 = node.attribute(createSpiAttributeName(ATTR_HOST_NAMES));\n        Integer boundPort = node.attribute(createSpiAttributeName(ATTR_PORT));\n        Collection<InetSocketAddress> extAddrs = node.attribute(createSpiAttributeName(ATTR_EXT_ADDRS));\n\n        boolean isRmtAddrsExist = (!F.isEmpty(rmtAddrs0) && boundPort != null);\n        boolean isExtAddrsExist = !F.isEmpty(extAddrs);\n\n        if (!isRmtAddrsExist && !isExtAddrsExist)\n            throw new IgniteCheckedException(\"Failed to send message to the destination node. Node doesn't have any \" +\n                \"TCP communication addresses or mapped external addresses. Check configuration and make sure \" +\n                \"that you use the same communication SPI on all nodes. Remote node id: \" + node.id());\n\n        LinkedHashSet<InetSocketAddress> addrs;\n\n        // Try to connect first on bound addresses.\n        if (isRmtAddrsExist) {\n            List<InetSocketAddress> addrs0 = new ArrayList<>(U.toSocketAddresses(rmtAddrs0, rmtHostNames0, boundPort));\n\n            boolean sameHost = U.sameMacs(getSpiContext().localNode(), node);\n\n            Collections.sort(addrs0, U.inetAddressesComparator(sameHost));\n\n            addrs = new LinkedHashSet<>(addrs0);\n        }\n        else\n            addrs = new LinkedHashSet<>();\n\n        // Then on mapped external addresses.\n        if (isExtAddrsExist)\n            addrs.addAll(extAddrs);\n\n        if (filterReachableAddresses) {\n            Set<InetAddress> allInetAddrs = U.newHashSet(addrs.size());\n\n            for (InetSocketAddress addr : addrs) {\n                // Skip unresolved as addr.getAddress() can return null.\n                if (!addr.isUnresolved())\n                    allInetAddrs.add(addr.getAddress());\n            }\n\n            List<InetAddress> reachableInetAddrs = U.filterReachable(allInetAddrs);\n\n            if (reachableInetAddrs.size() < allInetAddrs.size()) {\n                LinkedHashSet<InetSocketAddress> addrs0 = U.newLinkedHashSet(addrs.size());\n\n                List<InetSocketAddress> unreachableInetAddr = new ArrayList<>(allInetAddrs.size() - reachableInetAddrs.size());\n\n                for (InetSocketAddress addr : addrs) {\n                    if (reachableInetAddrs.contains(addr.getAddress()))\n                        addrs0.add(addr);\n                    else\n                        unreachableInetAddr.add(addr);\n                }\n\n                addrs0.addAll(unreachableInetAddr);\n\n                addrs = addrs0;\n            }\n\n            if (log.isDebugEnabled())\n                log.debug(\"Addresses to connect for node [rmtNode=\" + node.id() + \", addrs=\" + addrs.toString() + ']');\n        }\n\n        return addrs;\n    }",
            "2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015 +\n3016 +\n3017 +\n3018 +\n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048 +\n3049  \n3050  \n3051  \n3052  ",
            "    /**\n     * @param node Node.\n     * @return Node addresses.\n     * @throws IgniteCheckedException If node does not have addresses.\n     */\n    private LinkedHashSet<InetSocketAddress> nodeAddresses(ClusterNode node) throws IgniteCheckedException {\n        Collection<String> rmtAddrs0 = node.attribute(createSpiAttributeName(ATTR_ADDRS));\n        Collection<String> rmtHostNames0 = node.attribute(createSpiAttributeName(ATTR_HOST_NAMES));\n        Integer boundPort = node.attribute(createSpiAttributeName(ATTR_PORT));\n        Collection<InetSocketAddress> extAddrs = node.attribute(createSpiAttributeName(ATTR_EXT_ADDRS));\n\n        boolean isRmtAddrsExist = (!F.isEmpty(rmtAddrs0) && boundPort != null);\n        boolean isExtAddrsExist = !F.isEmpty(extAddrs);\n\n        if (!isRmtAddrsExist && !isExtAddrsExist)\n            throw new IgniteCheckedException(\"Failed to send message to the destination node. Node doesn't have any \" +\n                \"TCP communication addresses or mapped external addresses. Check configuration and make sure \" +\n                \"that you use the same communication SPI on all nodes. Remote node id: \" + node.id());\n\n        LinkedHashSet<InetSocketAddress> addrs;\n\n        // Try to connect first on bound addresses.\n        if (isRmtAddrsExist) {\n            List<InetSocketAddress> addrs0 = new ArrayList<>(U.toSocketAddresses(rmtAddrs0, rmtHostNames0, boundPort));\n\n            boolean sameHost = U.sameMacs(getSpiContext().localNode(), node);\n\n            Collections.sort(addrs0, U.inetAddressesComparator(sameHost));\n\n            addrs = new LinkedHashSet<>(addrs0);\n        }\n        else\n            addrs = new LinkedHashSet<>();\n\n        // Then on mapped external addresses.\n        if (isExtAddrsExist)\n            addrs.addAll(extAddrs);\n\n        if (log.isDebugEnabled())\n            log.debug(\"Addresses resolved from attributes [rmtNode=\" + node.id() + \", addrs=\" + addrs +\n                \", isRmtAddrsExist=\" + isRmtAddrsExist + ']');\n\n        if (filterReachableAddresses) {\n            Set<InetAddress> allInetAddrs = U.newHashSet(addrs.size());\n\n            for (InetSocketAddress addr : addrs) {\n                // Skip unresolved as addr.getAddress() can return null.\n                if (!addr.isUnresolved())\n                    allInetAddrs.add(addr.getAddress());\n            }\n\n            List<InetAddress> reachableInetAddrs = U.filterReachable(allInetAddrs);\n\n            if (reachableInetAddrs.size() < allInetAddrs.size()) {\n                LinkedHashSet<InetSocketAddress> addrs0 = U.newLinkedHashSet(addrs.size());\n\n                List<InetSocketAddress> unreachableInetAddr = new ArrayList<>(allInetAddrs.size() - reachableInetAddrs.size());\n\n                for (InetSocketAddress addr : addrs) {\n                    if (reachableInetAddrs.contains(addr.getAddress()))\n                        addrs0.add(addr);\n                    else\n                        unreachableInetAddr.add(addr);\n                }\n\n                addrs0.addAll(unreachableInetAddr);\n\n                addrs = addrs0;\n            }\n\n            if (log.isDebugEnabled())\n                log.debug(\"Addresses to connect for node [rmtNode=\" + node.id() + \", addrs=\" + addrs + ']');\n        }\n\n        return addrs;\n    }"
        ],
        [
            "TcpCommunicationSpi::createTcpClient(ClusterNode,int)",
            "3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  \n3057  \n3058  \n3059  \n3060  \n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069  \n3070  \n3071  \n3072  \n3073  \n3074  \n3075  \n3076  \n3077  \n3078  \n3079  \n3080  \n3081  \n3082  \n3083  \n3084  \n3085  \n3086  \n3087  \n3088  \n3089  \n3090  \n3091  \n3092  \n3093  \n3094  \n3095  \n3096  \n3097  \n3098  \n3099  \n3100  \n3101  \n3102  \n3103  \n3104  \n3105  \n3106  \n3107  \n3108  \n3109  \n3110  \n3111  \n3112  \n3113  \n3114  \n3115  \n3116  \n3117  \n3118  \n3119  \n3120  \n3121  \n3122  \n3123  \n3124  \n3125  \n3126  \n3127  \n3128  \n3129  \n3130  \n3131  \n3132  \n3133  \n3134  \n3135  \n3136  \n3137  \n3138  \n3139  \n3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212  \n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233  \n3234  \n3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  \n3243  \n3244  \n3245  \n3246  \n3247  \n3248  \n3249  \n3250  \n3251  \n3252  \n3253  \n3254  \n3255  \n3256  \n3257  \n3258  \n3259  \n3260  \n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269  \n3270  \n3271  \n3272  \n3273  \n3274  \n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  \n3283  \n3284  \n3285  \n3286  \n3287  \n3288  \n3289  \n3290  \n3291  \n3292  \n3293  \n3294  \n3295  \n3296  \n3297  \n3298  \n3299  \n3300  \n3301  \n3302  \n3303  \n3304  \n3305  \n3306  \n3307  \n3308  \n3309  \n3310  \n3311  \n3312  \n3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331  \n3332  \n3333  ",
            "    /**\n     * Establish TCP connection to remote node and returns client.\n     *\n     * @param node Remote node.\n     * @param connIdx Connection index.\n     * @return Client.\n     * @throws IgniteCheckedException If failed.\n     */\n    protected GridCommunicationClient createTcpClient(ClusterNode node, int connIdx) throws IgniteCheckedException {\n        LinkedHashSet<InetSocketAddress> addrs = nodeAddresses(node);\n\n        GridCommunicationClient client = null;\n        IgniteCheckedException errs = null;\n\n        int connectAttempts = 1;\n\n        for (InetSocketAddress addr : addrs) {\n            long connTimeout0 = connTimeout;\n\n            int attempt = 1;\n\n            IgniteSpiOperationTimeoutHelper timeoutHelper = new IgniteSpiOperationTimeoutHelper(this,\n                !node.isClient());\n\n            int lastWaitingTimeout = 1;\n\n            while (client == null) { // Reconnection on handshake timeout.\n                boolean needWait = false;\n\n                try {\n                    SocketChannel ch = SocketChannel.open();\n\n                    ch.configureBlocking(true);\n\n                    ch.socket().setTcpNoDelay(tcpNoDelay);\n                    ch.socket().setKeepAlive(true);\n\n                    if (sockRcvBuf > 0)\n                        ch.socket().setReceiveBufferSize(sockRcvBuf);\n\n                    if (sockSndBuf > 0)\n                        ch.socket().setSendBufferSize(sockSndBuf);\n\n                    if (getSpiContext().node(node.id()) == null) {\n                        U.closeQuiet(ch);\n\n                        throw new ClusterTopologyCheckedException(\"Failed to send message \" +\n                            \"(node left topology): \" + node);\n                    }\n\n                    ConnectionKey connKey = new ConnectionKey(node.id(), connIdx, -1);\n\n                    GridNioRecoveryDescriptor recoveryDesc = outRecoveryDescriptor(node, connKey);\n\n                    if (!recoveryDesc.reserve()) {\n                        U.closeQuiet(ch);\n\n                        return null;\n                    }\n\n                    Long rcvCnt;\n\n                    Map<Integer, Object> meta = new HashMap<>();\n\n                    GridSslMeta sslMeta = null;\n\n                    try {\n                        ch.socket().connect(addr, (int)timeoutHelper.nextTimeoutChunk(connTimeout));\n\n                        if (isSslEnabled()) {\n                            meta.put(SSL_META.ordinal(), sslMeta = new GridSslMeta());\n\n                            SSLEngine sslEngine = ignite.configuration().getSslContextFactory().create().createSSLEngine();\n\n                            sslEngine.setUseClientMode(true);\n\n                            sslMeta.sslEngine(sslEngine);\n                        }\n\n                        Integer handshakeConnIdx = connIdx;\n\n                        rcvCnt = safeTcpHandshake(ch,\n                            recoveryDesc,\n                            node.id(),\n                            timeoutHelper.nextTimeoutChunk(connTimeout0),\n                            sslMeta,\n                            handshakeConnIdx);\n\n                        if (rcvCnt == ALREADY_CONNECTED) {\n                            return null;\n                        }\n                        else if (rcvCnt == NODE_STOPPING) {\n                            throw new ClusterTopologyCheckedException(\"Remote node started stop procedure: \" + node.id());\n                        }\n                        else if (rcvCnt == NEED_WAIT) {\n                            needWait = true;\n\n                            continue;\n                        }\n\n                        meta.put(CONN_IDX_META, connKey);\n\n                        if (recoveryDesc != null) {\n                            recoveryDesc.onHandshake(rcvCnt);\n\n                            meta.put(GridNioServer.RECOVERY_DESC_META_KEY, recoveryDesc);\n                        }\n\n                        GridNioSession ses = nioSrvr.createSession(ch, meta, false, null).get();\n\n                        client = new GridTcpNioCommunicationClient(connIdx, ses, log);\n                    }\n                    finally {\n                        if (client == null) {\n                            U.closeQuiet(ch);\n\n                            if (recoveryDesc != null)\n                                recoveryDesc.release();\n\n                            if (needWait) {\n                                if (lastWaitingTimeout < 60000)\n                                    lastWaitingTimeout *= 2;\n\n                                U.sleep(lastWaitingTimeout);\n                            }\n                        }\n                    }\n                }\n                catch (HandshakeTimeoutException | IgniteSpiOperationTimeoutException e) {\n                    if (client != null) {\n                        client.forceClose();\n\n                        client = null;\n                    }\n\n                    if (failureDetectionTimeoutEnabled() && (e instanceof HandshakeTimeoutException ||\n                        X.hasCause(e, SocketException.class) ||\n                        timeoutHelper.checkFailureTimeoutReached(e))) {\n\n                        String msg = \"Handshake timed out (failure detection timeout is reached) \" +\n                            \"[failureDetectionTimeout=\" + failureDetectionTimeout() + \", addr=\" + addr + ']';\n\n                        onException(msg, e);\n\n                        if (log.isDebugEnabled())\n                            log.debug(msg);\n\n                        if (errs == null)\n                            errs = new IgniteCheckedException(\"Failed to connect to node (is node still alive?). \" +\n                                \"Make sure that each ComputeTask and cache Transaction has a timeout set \" +\n                                \"in order to prevent parties from waiting forever in case of network issues \" +\n                                \"[nodeId=\" + node.id() + \", addrs=\" + addrs + ']');\n\n                        errs.addSuppressed(new IgniteCheckedException(\"Failed to connect to address: \" + addr, e));\n\n                        break;\n                    }\n\n                    assert !failureDetectionTimeoutEnabled();\n\n                    onException(\"Handshake timed out (will retry with increased timeout) [timeout=\" + connTimeout0 +\n                        \", addr=\" + addr + ']', e);\n\n                    if (log.isDebugEnabled())\n                        log.debug(\n                            \"Handshake timed out (will retry with increased timeout) [timeout=\" + connTimeout0 +\n                                \", addr=\" + addr + \", err=\" + e + ']');\n\n                    if (attempt == reconCnt || connTimeout0 > maxConnTimeout) {\n                        U.warn(log, \"Handshake timedout (will stop attempts to perform the handshake) \" +\n                            \"[node=\" + node.id() + \", timeout=\" + connTimeout0 +\n                            \", maxConnTimeout=\" + maxConnTimeout +\n                            \", attempt=\" + attempt + \", reconCnt=\" + reconCnt +\n                            \", err=\" + e.getMessage() + \", addr=\" + addr + ']');\n\n                        if (errs == null)\n                            errs = new IgniteCheckedException(\"Failed to connect to node (is node still alive?). \" +\n                                \"Make sure that each ComputeTask and cache Transaction has a timeout set \" +\n                                \"in order to prevent parties from waiting forever in case of network issues \" +\n                                \"[nodeId=\" + node.id() + \", addrs=\" + addrs + ']');\n\n                        errs.addSuppressed(new IgniteCheckedException(\"Failed to connect to address: \" + addr, e));\n\n                        break;\n                    }\n                    else {\n                        attempt++;\n\n                        connTimeout0 *= 2;\n\n                        // Continue loop.\n                    }\n                }\n                catch (ClusterTopologyCheckedException e) {\n                    throw e;\n                }\n                catch (Exception e) {\n                    if (client != null) {\n                        client.forceClose();\n\n                        client = null;\n                    }\n\n                    onException(\"Client creation failed [addr=\" + addr + \", err=\" + e + ']', e);\n\n                    if (log.isDebugEnabled())\n                        log.debug(\"Client creation failed [addr=\" + addr + \", err=\" + e + ']');\n\n                    boolean failureDetThrReached = timeoutHelper.checkFailureTimeoutReached(e);\n\n                    if (enableTroubleshootingLog)\n                        U.error(log, \"Failed to establish connection to a remote node [node=\" + node +\n                            \", addr=\" + addr + \", connectAttempts=\" + connectAttempts +\n                            \", failureDetThrReached=\" + failureDetThrReached + ']', e);\n\n                    if (failureDetThrReached)\n                        LT.warn(log, \"Connect timed out (consider increasing 'failureDetectionTimeout' \" +\n                            \"configuration property) [addr=\" + addr + \", failureDetectionTimeout=\" +\n                            failureDetectionTimeout() + ']');\n                    else if (X.hasCause(e, SocketTimeoutException.class))\n                        LT.warn(log, \"Connect timed out (consider increasing 'connTimeout' \" +\n                            \"configuration property) [addr=\" + addr + \", connTimeout=\" + connTimeout + ']');\n\n                    if (errs == null)\n                        errs = new IgniteCheckedException(\"Failed to connect to node (is node still alive?). \" +\n                            \"Make sure that each ComputeTask and cache Transaction has a timeout set \" +\n                            \"in order to prevent parties from waiting forever in case of network issues \" +\n                            \"[nodeId=\" + node.id() + \", addrs=\" + addrs + ']');\n\n                    errs.addSuppressed(new IgniteCheckedException(\"Failed to connect to address \" +\n                        \"[addr=\" + addr + \", err=\" + e.getMessage() + ']', e));\n\n                    // Reconnect for the second time, if connection is not established.\n                    if (!failureDetThrReached && connectAttempts < 5 &&\n                        (X.hasCause(e, ConnectException.class, HandshakeException.class, SocketTimeoutException.class))) {\n                        U.sleep(200);\n\n                        connectAttempts++;\n\n                        continue;\n                    }\n\n                    break;\n                }\n            }\n\n            if (client != null)\n                break;\n        }\n\n        if (client == null) {\n            assert errs != null;\n\n            if (X.hasCause(errs, ConnectException.class))\n                LT.warn(log, \"Failed to connect to a remote node \" +\n                    \"(make sure that destination node is alive and \" +\n                    \"operating system firewall is disabled on local and remote hosts) \" +\n                    \"[addrs=\" + addrs + ']');\n\n            if (enableForcibleNodeKill) {\n                if (getSpiContext().node(node.id()) != null\n                    && (CU.clientNode(node) ||  !CU.clientNode(getLocalNode())) &&\n                    connectionError(errs)) {\n                    String msg = \"TcpCommunicationSpi failed to establish connection to node, node will be dropped from \" +\n                        \"cluster [\" + \"rmtNode=\" + node + ']';\n\n                    if (enableTroubleshootingLog)\n                        U.error(log, msg, errs);\n                    else\n                        U.warn(log, msg);\n\n                    getSpiContext().failNode(node.id(), \"TcpCommunicationSpi failed to establish connection to node [\" +\n                        \"rmtNode=\" + node +\n                        \", errs=\" + errs +\n                        \", connectErrs=\" + Arrays.toString(errs.getSuppressed()) + ']');\n                }\n            }\n\n            if (X.hasCause(errs, ConnectException.class, HandshakeException.class))\n                throw errs;\n        }\n\n        return client;\n    }",
            "3054  \n3055  \n3056  \n3057  \n3058  \n3059  \n3060  \n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069  \n3070  \n3071  \n3072  \n3073  \n3074  \n3075  \n3076  \n3077  \n3078  \n3079  \n3080  \n3081 +\n3082 +\n3083 +\n3084 +\n3085 +\n3086 +\n3087 +\n3088 +\n3089  \n3090  \n3091  \n3092  \n3093  \n3094  \n3095  \n3096  \n3097  \n3098  \n3099  \n3100  \n3101  \n3102  \n3103  \n3104  \n3105  \n3106  \n3107  \n3108  \n3109  \n3110  \n3111  \n3112  \n3113  \n3114  \n3115  \n3116  \n3117  \n3118  \n3119  \n3120  \n3121  \n3122  \n3123  \n3124  \n3125  \n3126  \n3127  \n3128  \n3129  \n3130  \n3131  \n3132  \n3133  \n3134  \n3135  \n3136  \n3137  \n3138  \n3139  \n3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212  \n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233  \n3234  \n3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  \n3243  \n3244  \n3245  \n3246  \n3247  \n3248  \n3249  \n3250  \n3251  \n3252  \n3253  \n3254  \n3255  \n3256  \n3257  \n3258  \n3259  \n3260  \n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269  \n3270  \n3271  \n3272  \n3273  \n3274  \n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  \n3283  \n3284  \n3285  \n3286  \n3287  \n3288  \n3289  \n3290  \n3291  \n3292  \n3293  \n3294  \n3295  \n3296  \n3297  \n3298  \n3299  \n3300  \n3301  \n3302  \n3303  \n3304  \n3305  \n3306  \n3307  \n3308  \n3309  \n3310  \n3311  \n3312  \n3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331  \n3332  \n3333  \n3334  \n3335  \n3336  \n3337  \n3338  \n3339  \n3340  \n3341  \n3342  \n3343  \n3344  \n3345  ",
            "    /**\n     * Establish TCP connection to remote node and returns client.\n     *\n     * @param node Remote node.\n     * @param connIdx Connection index.\n     * @return Client.\n     * @throws IgniteCheckedException If failed.\n     */\n    protected GridCommunicationClient createTcpClient(ClusterNode node, int connIdx) throws IgniteCheckedException {\n        LinkedHashSet<InetSocketAddress> addrs = nodeAddresses(node);\n\n        GridCommunicationClient client = null;\n        IgniteCheckedException errs = null;\n\n        int connectAttempts = 1;\n\n        for (InetSocketAddress addr : addrs) {\n            long connTimeout0 = connTimeout;\n\n            int attempt = 1;\n\n            IgniteSpiOperationTimeoutHelper timeoutHelper = new IgniteSpiOperationTimeoutHelper(this,\n                !node.isClient());\n\n            int lastWaitingTimeout = 1;\n\n            while (client == null) { // Reconnection on handshake timeout.\n                if (addr.getAddress().isLoopbackAddress() && addr.getPort() == boundTcpPort) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Skipping local address [addr=\" + addr +\n                            \", locAddrs=\" + node.attribute(createSpiAttributeName(ATTR_ADDRS)) +\n                            \", node=\" + node + ']');\n                    continue;\n                }\n\n                boolean needWait = false;\n\n                try {\n                    SocketChannel ch = SocketChannel.open();\n\n                    ch.configureBlocking(true);\n\n                    ch.socket().setTcpNoDelay(tcpNoDelay);\n                    ch.socket().setKeepAlive(true);\n\n                    if (sockRcvBuf > 0)\n                        ch.socket().setReceiveBufferSize(sockRcvBuf);\n\n                    if (sockSndBuf > 0)\n                        ch.socket().setSendBufferSize(sockSndBuf);\n\n                    if (getSpiContext().node(node.id()) == null) {\n                        U.closeQuiet(ch);\n\n                        throw new ClusterTopologyCheckedException(\"Failed to send message \" +\n                            \"(node left topology): \" + node);\n                    }\n\n                    ConnectionKey connKey = new ConnectionKey(node.id(), connIdx, -1);\n\n                    GridNioRecoveryDescriptor recoveryDesc = outRecoveryDescriptor(node, connKey);\n\n                    if (!recoveryDesc.reserve()) {\n                        U.closeQuiet(ch);\n\n                        return null;\n                    }\n\n                    Long rcvCnt;\n\n                    Map<Integer, Object> meta = new HashMap<>();\n\n                    GridSslMeta sslMeta = null;\n\n                    try {\n                        ch.socket().connect(addr, (int)timeoutHelper.nextTimeoutChunk(connTimeout));\n\n                        if (isSslEnabled()) {\n                            meta.put(SSL_META.ordinal(), sslMeta = new GridSslMeta());\n\n                            SSLEngine sslEngine = ignite.configuration().getSslContextFactory().create().createSSLEngine();\n\n                            sslEngine.setUseClientMode(true);\n\n                            sslMeta.sslEngine(sslEngine);\n                        }\n\n                        Integer handshakeConnIdx = connIdx;\n\n                        rcvCnt = safeTcpHandshake(ch,\n                            recoveryDesc,\n                            node.id(),\n                            timeoutHelper.nextTimeoutChunk(connTimeout0),\n                            sslMeta,\n                            handshakeConnIdx);\n\n                        if (rcvCnt == ALREADY_CONNECTED) {\n                            return null;\n                        }\n                        else if (rcvCnt == NODE_STOPPING) {\n                            throw new ClusterTopologyCheckedException(\"Remote node started stop procedure: \" + node.id());\n                        }\n                        else if (rcvCnt == NEED_WAIT) {\n                            needWait = true;\n\n                            continue;\n                        }\n\n                        meta.put(CONN_IDX_META, connKey);\n\n                        if (recoveryDesc != null) {\n                            recoveryDesc.onHandshake(rcvCnt);\n\n                            meta.put(GridNioServer.RECOVERY_DESC_META_KEY, recoveryDesc);\n                        }\n\n                        GridNioSession ses = nioSrvr.createSession(ch, meta, false, null).get();\n\n                        client = new GridTcpNioCommunicationClient(connIdx, ses, log);\n                    }\n                    finally {\n                        if (client == null) {\n                            U.closeQuiet(ch);\n\n                            if (recoveryDesc != null)\n                                recoveryDesc.release();\n\n                            if (needWait) {\n                                if (lastWaitingTimeout < 60000)\n                                    lastWaitingTimeout *= 2;\n\n                                U.sleep(lastWaitingTimeout);\n                            }\n                        }\n                    }\n                }\n                catch (HandshakeTimeoutException | IgniteSpiOperationTimeoutException e) {\n                    if (client != null) {\n                        client.forceClose();\n\n                        client = null;\n                    }\n\n                    if (failureDetectionTimeoutEnabled() && (e instanceof HandshakeTimeoutException ||\n                        X.hasCause(e, SocketException.class) ||\n                        timeoutHelper.checkFailureTimeoutReached(e))) {\n\n                        String msg = \"Handshake timed out (failure detection timeout is reached) \" +\n                            \"[failureDetectionTimeout=\" + failureDetectionTimeout() + \", addr=\" + addr + ']';\n\n                        onException(msg, e);\n\n                        if (log.isDebugEnabled())\n                            log.debug(msg);\n\n                        if (errs == null)\n                            errs = new IgniteCheckedException(\"Failed to connect to node (is node still alive?). \" +\n                                \"Make sure that each ComputeTask and cache Transaction has a timeout set \" +\n                                \"in order to prevent parties from waiting forever in case of network issues \" +\n                                \"[nodeId=\" + node.id() + \", addrs=\" + addrs + ']');\n\n                        errs.addSuppressed(new IgniteCheckedException(\"Failed to connect to address: \" + addr, e));\n\n                        break;\n                    }\n\n                    assert !failureDetectionTimeoutEnabled();\n\n                    onException(\"Handshake timed out (will retry with increased timeout) [timeout=\" + connTimeout0 +\n                        \", addr=\" + addr + ']', e);\n\n                    if (log.isDebugEnabled())\n                        log.debug(\n                            \"Handshake timed out (will retry with increased timeout) [timeout=\" + connTimeout0 +\n                                \", addr=\" + addr + \", err=\" + e + ']');\n\n                    if (attempt == reconCnt || connTimeout0 > maxConnTimeout) {\n                        U.warn(log, \"Handshake timedout (will stop attempts to perform the handshake) \" +\n                            \"[node=\" + node.id() + \", timeout=\" + connTimeout0 +\n                            \", maxConnTimeout=\" + maxConnTimeout +\n                            \", attempt=\" + attempt + \", reconCnt=\" + reconCnt +\n                            \", err=\" + e.getMessage() + \", addr=\" + addr + ']');\n\n                        if (errs == null)\n                            errs = new IgniteCheckedException(\"Failed to connect to node (is node still alive?). \" +\n                                \"Make sure that each ComputeTask and cache Transaction has a timeout set \" +\n                                \"in order to prevent parties from waiting forever in case of network issues \" +\n                                \"[nodeId=\" + node.id() + \", addrs=\" + addrs + ']');\n\n                        errs.addSuppressed(new IgniteCheckedException(\"Failed to connect to address: \" + addr, e));\n\n                        break;\n                    }\n                    else {\n                        attempt++;\n\n                        connTimeout0 *= 2;\n\n                        // Continue loop.\n                    }\n                }\n                catch (ClusterTopologyCheckedException e) {\n                    throw e;\n                }\n                catch (Exception e) {\n                    if (client != null) {\n                        client.forceClose();\n\n                        client = null;\n                    }\n\n                    onException(\"Client creation failed [addr=\" + addr + \", err=\" + e + ']', e);\n\n                    if (log.isDebugEnabled())\n                        log.debug(\"Client creation failed [addr=\" + addr + \", err=\" + e + ']');\n\n                    boolean failureDetThrReached = timeoutHelper.checkFailureTimeoutReached(e);\n\n                    if (enableTroubleshootingLog)\n                        U.error(log, \"Failed to establish connection to a remote node [node=\" + node +\n                            \", addr=\" + addr + \", connectAttempts=\" + connectAttempts +\n                            \", failureDetThrReached=\" + failureDetThrReached + ']', e);\n\n                    if (failureDetThrReached)\n                        LT.warn(log, \"Connect timed out (consider increasing 'failureDetectionTimeout' \" +\n                            \"configuration property) [addr=\" + addr + \", failureDetectionTimeout=\" +\n                            failureDetectionTimeout() + ']');\n                    else if (X.hasCause(e, SocketTimeoutException.class))\n                        LT.warn(log, \"Connect timed out (consider increasing 'connTimeout' \" +\n                            \"configuration property) [addr=\" + addr + \", connTimeout=\" + connTimeout + ']');\n\n                    if (errs == null)\n                        errs = new IgniteCheckedException(\"Failed to connect to node (is node still alive?). \" +\n                            \"Make sure that each ComputeTask and cache Transaction has a timeout set \" +\n                            \"in order to prevent parties from waiting forever in case of network issues \" +\n                            \"[nodeId=\" + node.id() + \", addrs=\" + addrs + ']');\n\n                    errs.addSuppressed(new IgniteCheckedException(\"Failed to connect to address \" +\n                        \"[addr=\" + addr + \", err=\" + e.getMessage() + ']', e));\n\n                    // Reconnect for the second time, if connection is not established.\n                    if (!failureDetThrReached && connectAttempts < 5 &&\n                        (X.hasCause(e, ConnectException.class, HandshakeException.class, SocketTimeoutException.class))) {\n                        U.sleep(200);\n\n                        connectAttempts++;\n\n                        continue;\n                    }\n\n                    break;\n                }\n            }\n\n            if (client != null)\n                break;\n        }\n\n        if (client == null) {\n            assert errs != null;\n\n            if (X.hasCause(errs, ConnectException.class))\n                LT.warn(log, \"Failed to connect to a remote node \" +\n                    \"(make sure that destination node is alive and \" +\n                    \"operating system firewall is disabled on local and remote hosts) \" +\n                    \"[addrs=\" + addrs + ']');\n\n            if (enableForcibleNodeKill) {\n                if (getSpiContext().node(node.id()) != null\n                    && (CU.clientNode(node) ||  !CU.clientNode(getLocalNode())) &&\n                    connectionError(errs)) {\n                    String msg = \"TcpCommunicationSpi failed to establish connection to node, node will be dropped from \" +\n                        \"cluster [\" + \"rmtNode=\" + node + ']';\n\n                    if (enableTroubleshootingLog)\n                        U.error(log, msg, errs);\n                    else\n                        U.warn(log, msg);\n\n                    getSpiContext().failNode(node.id(), \"TcpCommunicationSpi failed to establish connection to node [\" +\n                        \"rmtNode=\" + node +\n                        \", errs=\" + errs +\n                        \", connectErrs=\" + Arrays.toString(errs.getSuppressed()) + ']');\n                }\n            }\n\n            if (X.hasCause(errs, ConnectException.class, HandshakeException.class))\n                throw errs;\n        }\n\n        return client;\n    }"
        ]
    ],
    "f8e29b73253c5f78435f9e7c173f81694e7d3dbc": [
        [
            "IgniteCachePartitionLossPolicySelfTest::noPrimaryOrBackupPartition(List)",
            " 383  \n 384  \n 385  \n 386  \n 387 -\n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404 -\n 405 -\n 406  \n 407  \n 408 -\n 409  ",
            "    /**\n     * @param nodes List of nodes to find partition.\n     * @return Partition id that isn't primary or backup for specified nodes.\n     */\n    protected Integer noPrimaryOrBackupPartition(List<Integer> nodes) {\n        Affinity<Object> aff = ignite(4).affinity(CACHE_NAME);\n\n        Integer part;\n\n        for (int i = 0; i < aff.partitions(); i++) {\n            part = i;\n\n            for (Integer id : nodes) {\n                if (aff.isPrimaryOrBackup(grid(id).cluster().localNode(), i)) {\n                    part = null;\n\n                    break;\n                }\n            }\n\n            if (part != null)\n                return part;\n\n        }\n\n        return null;\n    }",
            " 384  \n 385  \n 386  \n 387  \n 388 +\n 389  \n 390  \n 391 +\n 392 +\n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 +\n 408  \n 409  \n 410 +\n 411  ",
            "    /**\n     * @param nodes List of nodes to find partition.\n     * @return List of partitions that aren't primary or backup for specified nodes.\n     */\n    protected List<Integer> noPrimaryOrBackupPartition(List<Integer> nodes) {\n        Affinity<Object> aff = ignite(4).affinity(CACHE_NAME);\n\n        List<Integer> parts = new ArrayList<>();\n\n        Integer part;\n\n        for (int i = 0; i < aff.partitions(); i++) {\n            part = i;\n\n            for (Integer id : nodes) {\n                if (aff.isPrimaryOrBackup(grid(id).cluster().localNode(), i)) {\n                    part = null;\n\n                    break;\n                }\n            }\n\n            if (part != null)\n                parts.add(i);\n        }\n\n        return parts;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(AffinityTopologyVersion,GridDhtPartitionFullMap,CachePartitionFullCountersMap,Set,Map,AffinityTopologyVersion)",
            "1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432 -\n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable CachePartitionFullCountersMap incomeCntrMap,\n        Set<Integer> partsToReload,\n        @Nullable Map<Integer, Long> partSizes,\n        @Nullable AffinityTopologyVersion msgTopVer) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Updating full partition map [grp=\" + grp.cacheOrGroupName() + \", exchVer=\" + exchangeVer +\n                \", fullMap=\" + fullMapString() + ']');\n        }\n\n        assert partMap != null;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (log.isTraceEnabled() && exchangeVer != null) {\n                    log.trace(\"Partition states before full update [grp=\" + grp.cacheOrGroupName()\n                        + \", exchVer=\" + exchangeVer + \", states=\" + dumpPartitionStates() + ']');\n                }\n\n                if (stopping || !lastTopChangeVer.initialized() ||\n                    // Ignore message not-related to exchange if exchange is in progress.\n                    (exchangeVer == null && !lastTopChangeVer.equals(readyTopVer)))\n                    return false;\n\n                if (incomeCntrMap != null) {\n                    // update local counters in partitions\n                    for (int i = 0; i < locParts.length(); i++) {\n                        cntrMap.updateCounter(i, incomeCntrMap.updateCounter(i));\n\n                        GridDhtLocalPartition part = locParts.get(i);\n\n                        if (part == null)\n                            continue;\n\n                        if (part.state() == OWNING || part.state() == MOVING) {\n                            long updCntr = incomeCntrMap.updateCounter(part.id());\n\n                            if (updCntr != 0 && updCntr > part.updateCounter())\n                                part.updateCounter(updCntr);\n                        }\n                    }\n                }\n\n                if (exchangeVer != null) {\n                    // Ignore if exchange already finished or new exchange started.\n                    if (readyTopVer.compareTo(exchangeVer) > 0 || lastTopChangeVer.compareTo(exchangeVer) > 0) {\n                        U.warn(log, \"Stale exchange id for full partition map update (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exchVer=\" + exchangeVer + ']');\n\n                        return false;\n                    }\n                }\n\n                if (msgTopVer != null && lastTopChangeVer.compareTo(msgTopVer) > 0) {\n                    U.warn(log, \"Stale version for full partition map update message (will ignore) [\" +\n                        \"grp=\" + grp.cacheOrGroupName() +\n                        \", lastTopChange=\" + lastTopChangeVer +\n                        \", readTopVer=\" + readyTopVer +\n                        \", msgVer=\" + msgTopVer + ']');\n\n                    return false;\n                }\n\n                boolean fullMapUpdated = (node2part == null);\n\n                if (node2part != null) {\n                    for (GridDhtPartitionMap part : node2part.values()) {\n                        GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                        if (shouldOverridePartitionMap(part, newPart)) {\n                            fullMapUpdated = true;\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Overriding partition map in full update map [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() +\n                                    \", exchVer=\" + exchangeVer +\n                                    \", curPart=\" + mapString(part) +\n                                    \", newPart=\" + mapString(newPart) + ']');\n                            }\n\n                            if (newPart.nodeId().equals(ctx.localNodeId()))\n                                updateSeq.setIfGreater(newPart.updateSequence());\n                        }\n                        else {\n                            // If for some nodes current partition has a newer map,\n                            // then we keep the newer value.\n                            partMap.put(part.nodeId(), part);\n                        }\n                    }\n\n                    // Check that we have new nodes.\n                    for (GridDhtPartitionMap part : partMap.values()) {\n                        if (fullMapUpdated)\n                            break;\n\n                        fullMapUpdated = !node2part.containsKey(part.nodeId());\n                    }\n\n                    // Remove entry if node left.\n                    for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                        UUID nodeId = it.next();\n\n                        if (!ctx.discovery().alive(nodeId)) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Removing left node from full map update [grp=\" + grp.cacheOrGroupName() +\n                                    \", nodeId=\" + nodeId + \", partMap=\" + partMap + ']');\n\n                            leftNode2Part.put(nodeId, partMap.get(nodeId));\n\n                            it.remove();\n                        }\n                    }\n                }\n                else {\n                    GridDhtPartitionMap locNodeMap = partMap.get(ctx.localNodeId());\n\n                    if (locNodeMap != null)\n                        updateSeq.setIfGreater(locNodeMap.updateSequence());\n                }\n\n                if (!fullMapUpdated) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"No updates for full partition map (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastExch=\" + lastTopChangeVer +\n                            \", exchVer=\" + exchangeVer +\n                            \", curMap=\" + node2part +\n                            \", newMap=\" + partMap + ']');\n                    }\n\n                    return false;\n                }\n\n                if (exchangeVer != null) {\n                    assert exchangeVer.compareTo(readyTopVer) >= 0 && exchangeVer.compareTo(lastTopChangeVer) >= 0;\n\n                    lastTopChangeVer = readyTopVer = exchangeVer;\n                }\n\n                node2part = partMap;\n\n                if (exchangeVer == null && !grp.isReplicated() &&\n                        (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0)) {\n                    AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) {\n                            int p = e0.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e0.getValue() == MOVING || e0.getValue() == OWNING || e0.getValue() == RENTING) &&\n                                !affAssignment.getIds(p).contains(e.getKey())) {\n\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                diffIds.add(e.getKey());\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(e.getKey())) {\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n                    }\n\n                    diffFromAffinityVer = readyTopVer;\n                }\n\n                boolean changed = false;\n\n                GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n                // Only in real exchange occurred.\n                if (exchangeVer != null &&\n                    nodeMap != null &&\n                    grp.persistenceEnabled() &&\n                    readyTopVer.initialized()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                        int p = e.getKey();\n                        GridDhtPartitionState state = e.getValue();\n\n                        if (state == OWNING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            assert locPart != null : grp.cacheOrGroupName();\n\n                            if (locPart.state() == MOVING) {\n                                boolean success = locPart.own();\n\n                                assert success : locPart;\n\n                                changed |= success;\n                            }\n                        }\n                        else if (state == MOVING) {\n                            boolean haveHistory = !partsToReload.contains(p);\n\n                            rebalancePartition(p, haveHistory);\n\n                            changed = true;\n                        }\n                    }\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchangeVer == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.topologyVersion(), aff.assignment());\n                }\n\n                if (partSizes != null)\n                    this.globalPartSizes = partSizes;\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Partition map after full update [grp=\" + grp.cacheOrGroupName() +\n                        \", map=\" + fullMapString() + ']');\n                }\n\n                if (log.isTraceEnabled() && exchangeVer != null) {\n                    log.trace(\"Partition states after full update [grp=\" + grp.cacheOrGroupName()\n                        + \", exchVer=\" + exchangeVer + \", states=\" + dumpPartitionStates() + ']');\n                }\n\n                if (changed)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            } finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            "1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432 +\n1433 +\n1434 +\n1435 +\n1436 +\n1437 +\n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable CachePartitionFullCountersMap incomeCntrMap,\n        Set<Integer> partsToReload,\n        @Nullable Map<Integer, Long> partSizes,\n        @Nullable AffinityTopologyVersion msgTopVer) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Updating full partition map [grp=\" + grp.cacheOrGroupName() + \", exchVer=\" + exchangeVer +\n                \", fullMap=\" + fullMapString() + ']');\n        }\n\n        assert partMap != null;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (log.isTraceEnabled() && exchangeVer != null) {\n                    log.trace(\"Partition states before full update [grp=\" + grp.cacheOrGroupName()\n                        + \", exchVer=\" + exchangeVer + \", states=\" + dumpPartitionStates() + ']');\n                }\n\n                if (stopping || !lastTopChangeVer.initialized() ||\n                    // Ignore message not-related to exchange if exchange is in progress.\n                    (exchangeVer == null && !lastTopChangeVer.equals(readyTopVer)))\n                    return false;\n\n                if (incomeCntrMap != null) {\n                    // update local counters in partitions\n                    for (int i = 0; i < locParts.length(); i++) {\n                        cntrMap.updateCounter(i, incomeCntrMap.updateCounter(i));\n\n                        GridDhtLocalPartition part = locParts.get(i);\n\n                        if (part == null)\n                            continue;\n\n                        if (part.state() == OWNING || part.state() == MOVING) {\n                            long updCntr = incomeCntrMap.updateCounter(part.id());\n\n                            if (updCntr != 0 && updCntr > part.updateCounter())\n                                part.updateCounter(updCntr);\n                        }\n                    }\n                }\n\n                if (exchangeVer != null) {\n                    // Ignore if exchange already finished or new exchange started.\n                    if (readyTopVer.compareTo(exchangeVer) > 0 || lastTopChangeVer.compareTo(exchangeVer) > 0) {\n                        U.warn(log, \"Stale exchange id for full partition map update (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exchVer=\" + exchangeVer + ']');\n\n                        return false;\n                    }\n                }\n\n                if (msgTopVer != null && lastTopChangeVer.compareTo(msgTopVer) > 0) {\n                    U.warn(log, \"Stale version for full partition map update message (will ignore) [\" +\n                        \"grp=\" + grp.cacheOrGroupName() +\n                        \", lastTopChange=\" + lastTopChangeVer +\n                        \", readTopVer=\" + readyTopVer +\n                        \", msgVer=\" + msgTopVer + ']');\n\n                    return false;\n                }\n\n                boolean fullMapUpdated = (node2part == null);\n\n                if (node2part != null) {\n                    for (GridDhtPartitionMap part : node2part.values()) {\n                        GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                        if (shouldOverridePartitionMap(part, newPart)) {\n                            fullMapUpdated = true;\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Overriding partition map in full update map [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() +\n                                    \", exchVer=\" + exchangeVer +\n                                    \", curPart=\" + mapString(part) +\n                                    \", newPart=\" + mapString(newPart) + ']');\n                            }\n\n                            if (newPart.nodeId().equals(ctx.localNodeId()))\n                                updateSeq.setIfGreater(newPart.updateSequence());\n                        }\n                        else {\n                            // If for some nodes current partition has a newer map,\n                            // then we keep the newer value.\n                            partMap.put(part.nodeId(), part);\n                        }\n                    }\n\n                    // Check that we have new nodes.\n                    for (GridDhtPartitionMap part : partMap.values()) {\n                        if (fullMapUpdated)\n                            break;\n\n                        fullMapUpdated = !node2part.containsKey(part.nodeId());\n                    }\n\n                    // Remove entry if node left.\n                    for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                        UUID nodeId = it.next();\n\n                        if (!ctx.discovery().alive(nodeId)) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Removing left node from full map update [grp=\" + grp.cacheOrGroupName() +\n                                    \", nodeId=\" + nodeId + \", partMap=\" + partMap + ']');\n\n                            if (node2part.containsKey(nodeId)) {\n                                GridDhtPartitionMap map = partMap.get(nodeId);\n\n                                if (map != null)\n                                    leftNode2Part.put(nodeId, map);\n                            }\n\n                            it.remove();\n                        }\n                    }\n                }\n                else {\n                    GridDhtPartitionMap locNodeMap = partMap.get(ctx.localNodeId());\n\n                    if (locNodeMap != null)\n                        updateSeq.setIfGreater(locNodeMap.updateSequence());\n                }\n\n                if (!fullMapUpdated) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"No updates for full partition map (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastExch=\" + lastTopChangeVer +\n                            \", exchVer=\" + exchangeVer +\n                            \", curMap=\" + node2part +\n                            \", newMap=\" + partMap + ']');\n                    }\n\n                    return false;\n                }\n\n                if (exchangeVer != null) {\n                    assert exchangeVer.compareTo(readyTopVer) >= 0 && exchangeVer.compareTo(lastTopChangeVer) >= 0;\n\n                    lastTopChangeVer = readyTopVer = exchangeVer;\n                }\n\n                node2part = partMap;\n\n                if (exchangeVer == null && !grp.isReplicated() &&\n                        (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0)) {\n                    AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) {\n                            int p = e0.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e0.getValue() == MOVING || e0.getValue() == OWNING || e0.getValue() == RENTING) &&\n                                !affAssignment.getIds(p).contains(e.getKey())) {\n\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                diffIds.add(e.getKey());\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(e.getKey())) {\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n                    }\n\n                    diffFromAffinityVer = readyTopVer;\n                }\n\n                boolean changed = false;\n\n                GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n                // Only in real exchange occurred.\n                if (exchangeVer != null &&\n                    nodeMap != null &&\n                    grp.persistenceEnabled() &&\n                    readyTopVer.initialized()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                        int p = e.getKey();\n                        GridDhtPartitionState state = e.getValue();\n\n                        if (state == OWNING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            assert locPart != null : grp.cacheOrGroupName();\n\n                            if (locPart.state() == MOVING) {\n                                boolean success = locPart.own();\n\n                                assert success : locPart;\n\n                                changed |= success;\n                            }\n                        }\n                        else if (state == MOVING) {\n                            boolean haveHistory = !partsToReload.contains(p);\n\n                            rebalancePartition(p, haveHistory);\n\n                            changed = true;\n                        }\n                    }\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchangeVer == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.topologyVersion(), aff.assignment());\n                }\n\n                if (partSizes != null)\n                    this.globalPartSizes = partSizes;\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Partition map after full update [grp=\" + grp.cacheOrGroupName() +\n                        \", map=\" + fullMapString() + ']');\n                }\n\n                if (log.isTraceEnabled() && exchangeVer != null) {\n                    log.trace(\"Partition states after full update [grp=\" + grp.cacheOrGroupName()\n                        + \", exchVer=\" + exchangeVer + \", states=\" + dumpPartitionStates() + ']');\n                }\n\n                if (changed)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            } finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ],
        [
            "IgniteCachePartitionLossPolicySelfTest::checkLostPartition(boolean,boolean,TopologyChanger)",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 -\n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  ",
            "    /**\n     * @param canWrite {@code True} if writes are allowed.\n     * @param safe {@code True} if lost partition should trigger exception.\n     * @param topChanger topology changer.\n     * @throws Exception if failed.\n     */\n    private void checkLostPartition(boolean canWrite, boolean safe, TopologyChanger topChanger) throws Exception {\n        assert partLossPlc != null;\n\n        int part = topChanger.changeTopology();\n\n        // Wait for all grids (servers and client) have same topology version\n        // to make sure that all nodes received map with lost partition.\n        GridTestUtils.waitForCondition(() -> {\n            AffinityTopologyVersion last = null;\n            for (Ignite ig : G.allGrids()) {\n                AffinityTopologyVersion ver = ((IgniteEx) ig).context().cache().context().exchange().readyAffinityVersion();\n\n                if (last != null && !last.equals(ver))\n                    return false;\n\n                last = ver;\n            }\n\n            return true;\n        }, 10000);\n\n        for (Ignite ig : G.allGrids()) {\n            info(\"Checking node: \" + ig.cluster().localNode().id());\n\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            verifyCacheOps(canWrite, safe, part, ig);\n\n            // Check we can read and write to lost partition in recovery mode.\n            IgniteCache<Integer, Integer> recoverCache = cache.withPartitionRecover();\n\n            for (int lostPart : recoverCache.lostPartitions()) {\n                recoverCache.get(lostPart);\n                recoverCache.put(lostPart, lostPart);\n            }\n\n            // Check that writing in recover mode does not clear partition state.\n            verifyCacheOps(canWrite, safe, part, ig);\n        }\n\n        // Check that partition state does not change after we start a new node.\n        IgniteEx grd = startGrid(3);\n\n        info(\"Newly started node: \" + grd.cluster().localNode().id());\n\n        for (Ignite ig : G.allGrids())\n            verifyCacheOps(canWrite, safe, part, ig);\n\n        ignite(4).resetLostPartitions(Collections.singletonList(CACHE_NAME));\n\n        awaitPartitionMapExchange(true, true, null);\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            assertTrue(cache.lostPartitions().isEmpty());\n\n            int parts = ig.affinity(CACHE_NAME).partitions();\n\n            for (int i = 0; i < parts; i++) {\n                cache.get(i);\n\n                cache.put(i, i);\n            }\n        }\n    }",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262 +\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  ",
            "    /**\n     * @param canWrite {@code True} if writes are allowed.\n     * @param safe {@code True} if lost partition should trigger exception.\n     * @param topChanger topology changer.\n     * @throws Exception if failed.\n     */\n    private void checkLostPartition(boolean canWrite, boolean safe, TopologyChanger topChanger) throws Exception {\n        assert partLossPlc != null;\n\n        int part = topChanger.changeTopology().get(0);\n\n        // Wait for all grids (servers and client) have same topology version\n        // to make sure that all nodes received map with lost partition.\n        GridTestUtils.waitForCondition(() -> {\n            AffinityTopologyVersion last = null;\n            for (Ignite ig : G.allGrids()) {\n                AffinityTopologyVersion ver = ((IgniteEx) ig).context().cache().context().exchange().readyAffinityVersion();\n\n                if (last != null && !last.equals(ver))\n                    return false;\n\n                last = ver;\n            }\n\n            return true;\n        }, 10000);\n\n        for (Ignite ig : G.allGrids()) {\n            info(\"Checking node: \" + ig.cluster().localNode().id());\n\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            verifyCacheOps(canWrite, safe, part, ig);\n\n            // Check we can read and write to lost partition in recovery mode.\n            IgniteCache<Integer, Integer> recoverCache = cache.withPartitionRecover();\n\n            for (int lostPart : recoverCache.lostPartitions()) {\n                recoverCache.get(lostPart);\n                recoverCache.put(lostPart, lostPart);\n            }\n\n            // Check that writing in recover mode does not clear partition state.\n            verifyCacheOps(canWrite, safe, part, ig);\n        }\n\n        // Check that partition state does not change after we start a new node.\n        IgniteEx grd = startGrid(3);\n\n        info(\"Newly started node: \" + grd.cluster().localNode().id());\n\n        for (Ignite ig : G.allGrids())\n            verifyCacheOps(canWrite, safe, part, ig);\n\n        ignite(4).resetLostPartitions(Collections.singletonList(CACHE_NAME));\n\n        awaitPartitionMapExchange(true, true, null);\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            assertTrue(cache.lostPartitions().isEmpty());\n\n            int parts = ig.affinity(CACHE_NAME).partitions();\n\n            for (int i = 0; i < parts; i++) {\n                cache.get(i);\n\n                cache.put(i, i);\n            }\n        }\n    }"
        ],
        [
            "IgniteCachePartitionLossPolicySelfTest::TopologyChanger::changeTopology()",
            " 439  \n 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462 -\n 463  \n 464 -\n 465  \n 466  \n 467 -\n 468  \n 469  \n 470 -\n 471 -\n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479 -\n 480 -\n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507 -\n 508 -\n 509  \n 510 -\n 511 -\n 512  \n 513 -\n 514  ",
            "        /**\n         * @return Lost partition ID.\n         * @throws Exception If failed.\n         */\n        protected int changeTopology() throws Exception {\n            startGrids(4);\n\n            Affinity<Object> aff = ignite(0).affinity(CACHE_NAME);\n\n            for (int i = 0; i < aff.partitions(); i++)\n                ignite(0).cache(CACHE_NAME).put(i, i);\n\n            client = true;\n\n            startGrid(4);\n\n            client = false;\n\n            for (int i = 0; i < 5; i++)\n                info(\">>> Node [idx=\" + i + \", nodeId=\" + ignite(i).cluster().localNode().id() + ']');\n\n            awaitPartitionMapExchange();\n\n            final Integer part = noPrimaryOrBackupPartition(aliveNodes);\n\n            if (part == null)\n                throw new IllegalStateException(\"No partition on nodes: \" + killNodes);\n\n            final List<Semaphore> partLost = new ArrayList<>();\n\n            for (int i : aliveNodes) {\n                final Semaphore sem = new Semaphore(0);\n                partLost.add(sem);\n\n                grid(i).events().localListen(new P1<Event>() {\n                    @Override public boolean apply(Event evt) {\n                        assert evt.type() == EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST;\n\n                        CacheRebalancingEvent cacheEvt = (CacheRebalancingEvent)evt;\n\n                        if (cacheEvt.partition() == part && F.eq(CACHE_NAME, cacheEvt.cacheName()))\n                            sem.release();\n\n                        return true;\n                    }\n                }, EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST);\n\n            }\n\n            if (delayExchange)\n                delayPartExchange.set(true);\n\n            ExecutorService executor = Executors.newFixedThreadPool(killNodes.size());\n\n            for (Integer node : killNodes) {\n                executor.submit(new Runnable() {\n                    @Override public void run() {\n                        grid(node).close();\n                    }\n                });\n\n                Thread.sleep(stopDelay);\n            }\n\n            executor.shutdown();\n\n            delayPartExchange.set(false);\n\n            for (Semaphore sem : partLost)\n                assertTrue(\"Failed to wait for partition LOST event\", sem.tryAcquire(1, 10L, TimeUnit.SECONDS));\n\n            for (Semaphore sem : partLost)\n                assertFalse(\"Partition LOST event raised twice\", sem.tryAcquire(1, 1L, TimeUnit.SECONDS));\n\n            return part;\n        }",
            " 441  \n 442  \n 443  \n 444  \n 445 +\n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464 +\n 465  \n 466 +\n 467  \n 468  \n 469 +\n 470  \n 471  \n 472 +\n 473 +\n 474 +\n 475 +\n 476 +\n 477 +\n 478 +\n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486 +\n 487 +\n 488 +\n 489 +\n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516 +\n 517 +\n 518 +\n 519 +\n 520 +\n 521 +\n 522  \n 523 +\n 524 +\n 525 +\n 526 +\n 527  \n 528 +\n 529  ",
            "        /**\n         * @return Lost partition ID.\n         * @throws Exception If failed.\n         */\n        protected List<Integer> changeTopology() throws Exception {\n            startGrids(4);\n\n            Affinity<Object> aff = ignite(0).affinity(CACHE_NAME);\n\n            for (int i = 0; i < aff.partitions(); i++)\n                ignite(0).cache(CACHE_NAME).put(i, i);\n\n            client = true;\n\n            startGrid(4);\n\n            client = false;\n\n            for (int i = 0; i < 5; i++)\n                info(\">>> Node [idx=\" + i + \", nodeId=\" + ignite(i).cluster().localNode().id() + ']');\n\n            awaitPartitionMapExchange();\n\n            final List<Integer> parts = noPrimaryOrBackupPartition(aliveNodes);\n\n            if (parts.size() == 0)\n                throw new IllegalStateException(\"No partition on nodes: \" + killNodes);\n\n            final List<Map<Integer, Semaphore>> lostMap = new ArrayList<>();\n\n            for (int i : aliveNodes) {\n                HashMap<Integer, Semaphore> semaphoreMap = new HashMap<>();\n\n                for (Integer part : parts)\n                    semaphoreMap.put(part, new Semaphore(0));\n\n                lostMap.add(semaphoreMap);\n\n\n                grid(i).events().localListen(new P1<Event>() {\n                    @Override public boolean apply(Event evt) {\n                        assert evt.type() == EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST;\n\n                        CacheRebalancingEvent cacheEvt = (CacheRebalancingEvent)evt;\n\n                        if (F.eq(CACHE_NAME, cacheEvt.cacheName())) {\n                            if (semaphoreMap.containsKey(cacheEvt.partition()))\n                                semaphoreMap.get(cacheEvt.partition()).release();\n                        }\n\n                        return true;\n                    }\n                }, EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST);\n\n            }\n\n            if (delayExchange)\n                delayPartExchange.set(true);\n\n            ExecutorService executor = Executors.newFixedThreadPool(killNodes.size());\n\n            for (Integer node : killNodes) {\n                executor.submit(new Runnable() {\n                    @Override public void run() {\n                        grid(node).close();\n                    }\n                });\n\n                Thread.sleep(stopDelay);\n            }\n\n            executor.shutdown();\n\n            delayPartExchange.set(false);\n\n            Thread.sleep(5_000L);\n\n            for (Map<Integer, Semaphore> map : lostMap) {\n                for (Map.Entry<Integer, Semaphore> entry : map.entrySet())\n                    assertTrue(\"Failed to wait for partition LOST event for partition:\" + entry.getKey(), entry.getValue().tryAcquire(1));\n            }\n\n            for (Map<Integer, Semaphore> map : lostMap) {\n                for (Map.Entry<Integer, Semaphore> entry : map.entrySet())\n                    assertFalse(\"Partition LOST event raised twice for partition:\" + entry.getKey(), entry.getValue().tryAcquire(1));\n            }\n\n            return parts;\n        }"
        ]
    ],
    "6e7b86e8ed127904e3799144527c90ff5818a674": [
        [
            "FileWriteAheadLogManager::FileCompressor::run()",
            "2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107 -\n2108  \n2109 -\n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            init();\n\n            while (!Thread.currentThread().isInterrupted() && !stopped) {\n                long currReservedSegment = -1;\n\n                try {\n                    deleteObsoleteRawSegments();\n\n                    currReservedSegment = tryReserveNextSegmentOrWait();\n                    if (currReservedSegment == -1)\n                        continue;\n\n                    File tmpZip = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment) + \".zip\" + \".tmp\");\n\n                    File zip = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment) + \".zip\");\n\n                    File raw = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment));\n                    if (!Files.exists(raw.toPath()))\n                        throw new IgniteCheckedException(\"WAL archive segment is missing: \" + raw);\n\n                    compressSegmentToFile(currReservedSegment, raw, tmpZip);\n\n                    Files.move(tmpZip.toPath(), zip.toPath());\n\n                    if (mode != WALMode.NONE) {\n                        try (FileIO f0 = ioFactory.create(zip, CREATE, READ, WRITE)) {\n                            f0.force();\n                        }\n\n                        if (evt.isRecordable(EVT_WAL_SEGMENT_COMPACTED)) {\n                            evt.record(new WalSegmentCompactedEvent(\n                                cctx.discovery().localNode(),\n                                currReservedSegment,\n                                zip.getAbsoluteFile())\n                            );\n                        }\n                    }\n\n                    lastCompressedIdx = currReservedSegment;\n                }\n                catch (IgniteCheckedException | IOException e) {\n                    U.error(log, \"Compression of WAL segment [idx=\" + currReservedSegment +\n                        \"] was skipped due to unexpected error\", e);\n\n                    lastCompressedIdx++;\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n                }\n                finally {\n                    if (currReservedSegment != -1)\n                        release(new FileWALPointer(currReservedSegment, 0, 0));\n                }\n            }\n        }",
            "2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108 +\n2109 +\n2110  \n2111 +\n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            init();\n\n            while (!Thread.currentThread().isInterrupted() && !stopped) {\n                long currReservedSegment = -1;\n\n                try {\n                    deleteObsoleteRawSegments();\n\n                    currReservedSegment = tryReserveNextSegmentOrWait();\n                    if (currReservedSegment == -1)\n                        continue;\n\n                    File tmpZip = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment)\n                        + FilePageStoreManager.ZIP_SUFFIX + FilePageStoreManager.TMP_SUFFIX);\n\n                    File zip = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment) + FilePageStoreManager.ZIP_SUFFIX);\n\n                    File raw = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment));\n                    if (!Files.exists(raw.toPath()))\n                        throw new IgniteCheckedException(\"WAL archive segment is missing: \" + raw);\n\n                    compressSegmentToFile(currReservedSegment, raw, tmpZip);\n\n                    Files.move(tmpZip.toPath(), zip.toPath());\n\n                    if (mode != WALMode.NONE) {\n                        try (FileIO f0 = ioFactory.create(zip, CREATE, READ, WRITE)) {\n                            f0.force();\n                        }\n\n                        if (evt.isRecordable(EVT_WAL_SEGMENT_COMPACTED)) {\n                            evt.record(new WalSegmentCompactedEvent(\n                                cctx.discovery().localNode(),\n                                currReservedSegment,\n                                zip.getAbsoluteFile())\n                            );\n                        }\n                    }\n\n                    lastCompressedIdx = currReservedSegment;\n                }\n                catch (IgniteCheckedException | IOException e) {\n                    U.error(log, \"Compression of WAL segment [idx=\" + currReservedSegment +\n                        \"] was skipped due to unexpected error\", e);\n\n                    lastCompressedIdx++;\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n                }\n                finally {\n                    if (currReservedSegment != -1)\n                        release(new FileWALPointer(currReservedSegment, 0, 0));\n                }\n            }\n        }"
        ],
        [
            "IgniteNodeStoppedDuringDisableWALTest::testStopNodeWithDisableWAL(NodeStopPoint)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 -\n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "    /**\n     * @param nodeStopPoint Stop point.\n     * @throws Exception If failed.\n     */\n    private void testStopNodeWithDisableWAL(NodeStopPoint nodeStopPoint) throws Exception {\n        log.info(\"Start test crash \" + nodeStopPoint);\n\n        IgniteEx ig0 = startGrid(0);\n\n        GridCacheSharedContext<Object, Object> sharedContext = ig0.context().cache().context();\n\n        GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)sharedContext.database();\n        IgniteWriteAheadLogManager WALmgr = sharedContext.wal();\n\n        WALDisableContext walDisableContext = new WALDisableContext(dbMgr, sharedContext.pageStore(), log) {\n            @Override protected void writeMetaStoreDisableWALFlag() throws IgniteCheckedException {\n                if (nodeStopPoint == NodeStopPoint.BEFORE_WRITE_KEY_TO_META_STORE)\n                    failNode(nodeStopPoint);\n\n                super.writeMetaStoreDisableWALFlag();\n\n                if (nodeStopPoint == NodeStopPoint.AFTER_WRITE_KEY_TO_META_STORE)\n                    failNode(nodeStopPoint);\n            }\n\n            @Override protected void removeMetaStoreDisableWALFlag() throws IgniteCheckedException {\n                if (nodeStopPoint == NodeStopPoint.AFTER_CHECKPOINT_AFTER_ENABLE_WAL)\n                    failNode(nodeStopPoint);\n\n                super.removeMetaStoreDisableWALFlag();\n\n                if (nodeStopPoint == NodeStopPoint.AFTER_REMOVE_KEY_TO_META_STORE)\n                    failNode(nodeStopPoint);\n            }\n\n            @Override protected void disableWAL(boolean disable) throws IgniteCheckedException {\n                if (disable) {\n                    if (nodeStopPoint == NodeStopPoint.AFTER_CHECKPOINT_BEFORE_DISABLE_WAL)\n                        failNode(nodeStopPoint);\n\n                    super.disableWAL(disable);\n\n                    if (nodeStopPoint == NodeStopPoint.AFTER_DISABLE_WAL)\n                        failNode(nodeStopPoint);\n\n                }\n                else {\n                    super.disableWAL(disable);\n\n                    if (nodeStopPoint == NodeStopPoint.AFTER_ENABLE_WAL)\n                        failNode(nodeStopPoint);\n                }\n            }\n        };\n\n        setFieldValue(sharedContext.walState(), \"walDisableContext\", walDisableContext);\n\n        setFieldValue(WALmgr, \"walDisableContext\", walDisableContext);\n\n        ig0.context().internalSubscriptionProcessor().registerMetastorageListener(walDisableContext);\n\n        ig0.cluster().active(true);\n\n        try (IgniteDataStreamer<Integer, Integer> st = ig0.dataStreamer(DEFAULT_CACHE_NAME)) {\n            st.allowOverwrite(true);\n\n            for (int i = 0; i < 10_000; i++)\n                st.addData(i, -i);\n        }\n\n        boolean fail = false;\n\n        try (WALIterator it = sharedContext.wal().replay(null)) {\n            dbMgr.applyUpdatesOnRecovery(it, (tup) -> true, (entry) -> true, new HashMap<>());\n        }\n        catch (IgniteCheckedException e) {\n            if (nodeStopPoint.needCleanUp)\n                fail = true;\n        }\n\n        Assert.assertEquals(nodeStopPoint.needCleanUp, fail);\n\n        Ignite ig1 = startGrid(0);\n\n        String msg = nodeStopPoint.toString();\n\n        if (nodeStopPoint.needCleanUp) {\n            PdsFoldersResolver foldersResolver = ((IgniteEx)ig1).context().pdsFolderResolver();\n\n            File root = foldersResolver.resolveFolders().persistentStoreRootPath();\n\n            walkFileTree(root.toPath(), new SimpleFileVisitor<Path>() {\n                @Override public FileVisitResult visitFile(Path path, BasicFileAttributes attrs) throws IOException {\n                    String name = path.toFile().getName();\n\n                    String filePath = path.toString();\n\n                    if (path.toFile().getParentFile().getName().equals(META_STORAGE_NAME))\n                        return CONTINUE;\n\n                    if (WAL_NAME_PATTERN.matcher(name).matches() || WAL_TEMP_NAME_PATTERN.matcher(name).matches())\n                        return CONTINUE;\n\n                    boolean failed = false;\n\n                    if (name.endsWith(FILE_TMP_SUFFIX))\n                        failed = true;\n\n                    if (CP_FILE_NAME_PATTERN.matcher(name).matches())\n                        failed = true;\n\n                    if (name.startsWith(PART_FILE_PREFIX))\n                        failed = true;\n\n                    if (name.startsWith(INDEX_FILE_NAME))\n                        failed = true;\n\n                    if (failed)\n                        fail(msg + \" \" + filePath);\n\n                    return CONTINUE;\n                }\n            });\n        }\n    }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 +\n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "    /**\n     * @param nodeStopPoint Stop point.\n     * @throws Exception If failed.\n     */\n    private void testStopNodeWithDisableWAL(NodeStopPoint nodeStopPoint) throws Exception {\n        log.info(\"Start test crash \" + nodeStopPoint);\n\n        IgniteEx ig0 = startGrid(0);\n\n        GridCacheSharedContext<Object, Object> sharedContext = ig0.context().cache().context();\n\n        GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)sharedContext.database();\n        IgniteWriteAheadLogManager WALmgr = sharedContext.wal();\n\n        WALDisableContext walDisableContext = new WALDisableContext(dbMgr, sharedContext.pageStore(), log) {\n            @Override protected void writeMetaStoreDisableWALFlag() throws IgniteCheckedException {\n                if (nodeStopPoint == NodeStopPoint.BEFORE_WRITE_KEY_TO_META_STORE)\n                    failNode(nodeStopPoint);\n\n                super.writeMetaStoreDisableWALFlag();\n\n                if (nodeStopPoint == NodeStopPoint.AFTER_WRITE_KEY_TO_META_STORE)\n                    failNode(nodeStopPoint);\n            }\n\n            @Override protected void removeMetaStoreDisableWALFlag() throws IgniteCheckedException {\n                if (nodeStopPoint == NodeStopPoint.AFTER_CHECKPOINT_AFTER_ENABLE_WAL)\n                    failNode(nodeStopPoint);\n\n                super.removeMetaStoreDisableWALFlag();\n\n                if (nodeStopPoint == NodeStopPoint.AFTER_REMOVE_KEY_TO_META_STORE)\n                    failNode(nodeStopPoint);\n            }\n\n            @Override protected void disableWAL(boolean disable) throws IgniteCheckedException {\n                if (disable) {\n                    if (nodeStopPoint == NodeStopPoint.AFTER_CHECKPOINT_BEFORE_DISABLE_WAL)\n                        failNode(nodeStopPoint);\n\n                    super.disableWAL(disable);\n\n                    if (nodeStopPoint == NodeStopPoint.AFTER_DISABLE_WAL)\n                        failNode(nodeStopPoint);\n\n                }\n                else {\n                    super.disableWAL(disable);\n\n                    if (nodeStopPoint == NodeStopPoint.AFTER_ENABLE_WAL)\n                        failNode(nodeStopPoint);\n                }\n            }\n        };\n\n        setFieldValue(sharedContext.walState(), \"walDisableContext\", walDisableContext);\n\n        setFieldValue(WALmgr, \"walDisableContext\", walDisableContext);\n\n        ig0.context().internalSubscriptionProcessor().registerMetastorageListener(walDisableContext);\n\n        ig0.cluster().active(true);\n\n        try (IgniteDataStreamer<Integer, Integer> st = ig0.dataStreamer(DEFAULT_CACHE_NAME)) {\n            st.allowOverwrite(true);\n\n            for (int i = 0; i < 10_000; i++)\n                st.addData(i, -i);\n        }\n\n        boolean fail = false;\n\n        try (WALIterator it = sharedContext.wal().replay(null)) {\n            dbMgr.applyUpdatesOnRecovery(it, (tup) -> true, (entry) -> true, new HashMap<>());\n        }\n        catch (IgniteCheckedException e) {\n            if (nodeStopPoint.needCleanUp)\n                fail = true;\n        }\n\n        Assert.assertEquals(nodeStopPoint.needCleanUp, fail);\n\n        Ignite ig1 = startGrid(0);\n\n        String msg = nodeStopPoint.toString();\n\n        if (nodeStopPoint.needCleanUp) {\n            PdsFoldersResolver foldersResolver = ((IgniteEx)ig1).context().pdsFolderResolver();\n\n            File root = foldersResolver.resolveFolders().persistentStoreRootPath();\n\n            walkFileTree(root.toPath(), new SimpleFileVisitor<Path>() {\n                @Override public FileVisitResult visitFile(Path path, BasicFileAttributes attrs) throws IOException {\n                    String name = path.toFile().getName();\n\n                    String filePath = path.toString();\n\n                    if (path.toFile().getParentFile().getName().equals(META_STORAGE_NAME))\n                        return CONTINUE;\n\n                    if (WAL_NAME_PATTERN.matcher(name).matches() || WAL_TEMP_NAME_PATTERN.matcher(name).matches())\n                        return CONTINUE;\n\n                    boolean failed = false;\n\n                    if (name.endsWith(FilePageStoreManager.TMP_SUFFIX))\n                        failed = true;\n\n                    if (CP_FILE_NAME_PATTERN.matcher(name).matches())\n                        failed = true;\n\n                    if (name.startsWith(PART_FILE_PREFIX))\n                        failed = true;\n\n                    if (name.startsWith(INDEX_FILE_NAME))\n                        failed = true;\n\n                    if (failed)\n                        fail(msg + \" \" + filePath);\n\n                    return CONTINUE;\n                }\n            });\n        }\n    }"
        ],
        [
            "FileWriteAheadLogManager::FileDecompressor::body()",
            "2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268 -\n2269 -\n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  ",
            "        /** {@inheritDoc} */\n        @Override protected void body() {\n            Throwable err = null;\n\n            try {\n                while (!isCancelled()) {\n                    long segmentToDecompress = -1L;\n\n                    try {\n                        segmentToDecompress = segmentsQueue.take();\n\n                        if (isCancelled())\n                            break;\n\n                        File zip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".zip\");\n                        File unzipTmp = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".tmp\");\n                        File unzip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress));\n\n                        try (ZipInputStream zis = new ZipInputStream(new BufferedInputStream(new FileInputStream(zip)));\n                             FileIO io = ioFactory.create(unzipTmp)) {\n                            zis.getNextEntry();\n\n                            while (io.writeFully(arr, 0, zis.read(arr)) > 0)\n                                ;\n                        }\n\n                        try {\n                            Files.move(unzipTmp.toPath(), unzip.toPath());\n                        }\n                        catch (FileAlreadyExistsException e) {\n                            U.error(log, \"Can't rename temporary unzipped segment: raw segment is already present \" +\n                                \"[tmp=\" + unzipTmp + \", raw=\" + unzip + \"]\", e);\n\n                            if (!unzipTmp.delete())\n                                U.error(log, \"Can't delete temporary unzipped segment [tmp=\" + unzipTmp + \"]\");\n                        }\n\n                        synchronized (this) {\n                            decompressionFutures.remove(segmentToDecompress).onDone();\n                        }\n                    }\n                    catch (IOException ex) {\n                        if (!isCancelled && segmentToDecompress != -1L) {\n                            IgniteCheckedException e = new IgniteCheckedException(\"Error during WAL segment \" +\n                                \"decompression [segmentIdx=\" + segmentToDecompress + \"]\", ex);\n\n                            synchronized (this) {\n                                decompressionFutures.remove(segmentToDecompress).onDone(e);\n                            }\n                        }\n                    }\n                }\n            }\n            catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n\n                if (!isCancelled)\n                    err = e;\n            }\n            catch (Throwable t) {\n                err = t;\n            }\n            finally {\n                if (err == null && !isCancelled)\n                    err = new IllegalStateException(\"Worker \" + name() + \" is terminated unexpectedly\");\n\n                if (err instanceof OutOfMemoryError)\n                    failureProcessor.process(new FailureContext(CRITICAL_ERROR, err));\n                else if (err != null)\n                    failureProcessor.process(new FailureContext(SYSTEM_WORKER_TERMINATION, err));\n            }\n        }",
            "2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270 +\n2271 +\n2272 +\n2273 +\n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  ",
            "        /** {@inheritDoc} */\n        @Override protected void body() {\n            Throwable err = null;\n\n            try {\n                while (!isCancelled()) {\n                    long segmentToDecompress = -1L;\n\n                    try {\n                        segmentToDecompress = segmentsQueue.take();\n\n                        if (isCancelled())\n                            break;\n\n                        File zip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress)\n                            + FilePageStoreManager.ZIP_SUFFIX);\n                        File unzipTmp = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress)\n                            + FilePageStoreManager.TMP_SUFFIX);\n                        File unzip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress));\n\n                        try (ZipInputStream zis = new ZipInputStream(new BufferedInputStream(new FileInputStream(zip)));\n                             FileIO io = ioFactory.create(unzipTmp)) {\n                            zis.getNextEntry();\n\n                            while (io.writeFully(arr, 0, zis.read(arr)) > 0)\n                                ;\n                        }\n\n                        try {\n                            Files.move(unzipTmp.toPath(), unzip.toPath());\n                        }\n                        catch (FileAlreadyExistsException e) {\n                            U.error(log, \"Can't rename temporary unzipped segment: raw segment is already present \" +\n                                \"[tmp=\" + unzipTmp + \", raw=\" + unzip + \"]\", e);\n\n                            if (!unzipTmp.delete())\n                                U.error(log, \"Can't delete temporary unzipped segment [tmp=\" + unzipTmp + \"]\");\n                        }\n\n                        synchronized (this) {\n                            decompressionFutures.remove(segmentToDecompress).onDone();\n                        }\n                    }\n                    catch (IOException ex) {\n                        if (!isCancelled && segmentToDecompress != -1L) {\n                            IgniteCheckedException e = new IgniteCheckedException(\"Error during WAL segment \" +\n                                \"decompression [segmentIdx=\" + segmentToDecompress + \"]\", ex);\n\n                            synchronized (this) {\n                                decompressionFutures.remove(segmentToDecompress).onDone(e);\n                            }\n                        }\n                    }\n                }\n            }\n            catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n\n                if (!isCancelled)\n                    err = e;\n            }\n            catch (Throwable t) {\n                err = t;\n            }\n            finally {\n                if (err == null && !isCancelled)\n                    err = new IllegalStateException(\"Worker \" + name() + \" is terminated unexpectedly\");\n\n                if (err instanceof OutOfMemoryError)\n                    failureProcessor.process(new FailureContext(CRITICAL_ERROR, err));\n                else if (err != null)\n                    failureProcessor.process(new FailureContext(SYSTEM_WORKER_TERMINATION, err));\n            }\n        }"
        ],
        [
            "StartCommand::zip(String)",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 -\n 101  \n 102  \n 103  \n 104  ",
            "    /**\n     * Archives specified folder or file into zip archive.\n     *\n     * @param jobArchivePath Path to folder to be archived.\n     * @return Byte array representing zip archive.\n     * @throws IOException In case of input/output exception.\n     */\n    private byte[] zip(String jobArchivePath) throws IOException {\n        Path path = Paths.get(jobArchivePath);\n        File file = path.toFile();\n\n        if (!file.exists())\n            throw new IllegalArgumentException(\"File doesn't exist [name=\" + jobArchivePath + \"]\");\n\n        if (file.isDirectory())\n            return zipDirectory(file);\n        else if (jobArchivePath.endsWith(\".zip\"))\n            return zipArchive(file);\n        else\n            return zipFile(file);\n    }",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102  \n 103  \n 104  \n 105  ",
            "    /**\n     * Archives specified folder or file into zip archive.\n     *\n     * @param jobArchivePath Path to folder to be archived.\n     * @return Byte array representing zip archive.\n     * @throws IOException In case of input/output exception.\n     */\n    private byte[] zip(String jobArchivePath) throws IOException {\n        Path path = Paths.get(jobArchivePath);\n        File file = path.toFile();\n\n        if (!file.exists())\n            throw new IllegalArgumentException(\"File doesn't exist [name=\" + jobArchivePath + \"]\");\n\n        if (file.isDirectory())\n            return zipDirectory(file);\n        else if (jobArchivePath.endsWith(FilePageStoreManager.ZIP_SUFFIX))\n            return zipArchive(file);\n        else\n            return zipFile(file);\n    }"
        ],
        [
            "FsyncModeFileWriteAheadLogManager::getAndReserveWalFiles(FileWALPointer,FileWALPointer)",
            " 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621 -\n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  ",
            "    /**\n     *  Collect wal segment files from low pointer (include) to high pointer (not include) and reserve low pointer.\n     *\n     * @param low Low bound.\n     * @param high High bound.\n     */\n    public Collection<File> getAndReserveWalFiles(FileWALPointer low, FileWALPointer high) throws IgniteCheckedException {\n        final long awaitIdx = high.index() - 1;\n\n        while (archiver.lastArchivedAbsoluteIndex() < awaitIdx)\n            LockSupport.parkNanos(Thread.currentThread(), 1_000_000);\n\n        if (!reserve(low))\n            throw new IgniteCheckedException(\"WAL archive segment has been deleted [idx=\" + low.index() + \"]\");\n\n        List<File> res = new ArrayList<>();\n\n        for (long i = low.index(); i < high.index(); i++) {\n            String segmentName = FileDescriptor.fileName(i);\n\n            File file = new File(walArchiveDir, segmentName);\n            File fileZip = new File(walArchiveDir, segmentName + \".zip\");\n\n            if (file.exists())\n                res.add(file);\n            else if (fileZip.exists())\n                res.add(fileZip);\n            else {\n                if (log.isInfoEnabled()) {\n                    log.info(\"Segment not found: \" + file.getName() + \"/\" + fileZip.getName());\n\n                    log.info(\"Stopped iteration on idx: \" + i);\n                }\n\n                break;\n            }\n        }\n\n        return res;\n    }",
            " 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622 +\n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  ",
            "    /**\n     *  Collect wal segment files from low pointer (include) to high pointer (not include) and reserve low pointer.\n     *\n     * @param low Low bound.\n     * @param high High bound.\n     */\n    public Collection<File> getAndReserveWalFiles(FileWALPointer low, FileWALPointer high) throws IgniteCheckedException {\n        final long awaitIdx = high.index() - 1;\n\n        while (archiver.lastArchivedAbsoluteIndex() < awaitIdx)\n            LockSupport.parkNanos(Thread.currentThread(), 1_000_000);\n\n        if (!reserve(low))\n            throw new IgniteCheckedException(\"WAL archive segment has been deleted [idx=\" + low.index() + \"]\");\n\n        List<File> res = new ArrayList<>();\n\n        for (long i = low.index(); i < high.index(); i++) {\n            String segmentName = FileDescriptor.fileName(i);\n\n            File file = new File(walArchiveDir, segmentName);\n            File fileZip = new File(walArchiveDir, segmentName + FilePageStoreManager.ZIP_SUFFIX);\n\n            if (file.exists())\n                res.add(file);\n            else if (fileZip.exists())\n                res.add(fileZip);\n            else {\n                if (log.isInfoEnabled()) {\n                    log.info(\"Segment not found: \" + file.getName() + \"/\" + fileZip.getName());\n\n                    log.info(\"Stopped iteration on idx: \" + i);\n                }\n\n                break;\n            }\n        }\n\n        return res;\n    }"
        ],
        [
            "FileWriteAheadLogManager::createFile(File)",
            "1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499 -\n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  ",
            "    /**\n     * Creates a file atomically with temp file.\n     *\n     * @param file File to create.\n     * @throws StorageException If failed.\n     */\n    private void createFile(File file) throws StorageException {\n        if (log.isDebugEnabled())\n            log.debug(\"Creating new file [exists=\" + file.exists() + \", file=\" + file.getAbsolutePath() + ']');\n\n        File tmp = new File(file.getParent(), file.getName() + \".tmp\");\n\n        formatFile(tmp);\n\n        try {\n            Files.move(tmp.toPath(), file.toPath());\n        }\n        catch (IOException e) {\n            throw new StorageException(\"Failed to move temp file to a regular WAL segment file: \" +\n                file.getAbsolutePath(), e);\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(\"Created WAL segment [file=\" + file.getAbsolutePath() + \", size=\" + file.length() + ']');\n    }",
            "1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500 +\n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  ",
            "    /**\n     * Creates a file atomically with temp file.\n     *\n     * @param file File to create.\n     * @throws StorageException If failed.\n     */\n    private void createFile(File file) throws StorageException {\n        if (log.isDebugEnabled())\n            log.debug(\"Creating new file [exists=\" + file.exists() + \", file=\" + file.getAbsolutePath() + ']');\n\n        File tmp = new File(file.getParent(), file.getName() + FilePageStoreManager.TMP_SUFFIX);\n\n        formatFile(tmp);\n\n        try {\n            Files.move(tmp.toPath(), file.toPath());\n        }\n        catch (IOException e) {\n            throw new StorageException(\"Failed to move temp file to a regular WAL segment file: \" +\n                file.getAbsolutePath(), e);\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(\"Created WAL segment [file=\" + file.getAbsolutePath() + \", size=\" + file.length() + ']');\n    }"
        ],
        [
            "FileWriteAheadLogManager::FileArchiver::archiveSegment(long)",
            "1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914 -\n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  ",
            "        /**\n         * Moves WAL segment from work folder to archive folder. Temp file is used to do movement\n         *\n         * @param absIdx Absolute index to archive.\n         */\n        private SegmentArchiveResult archiveSegment(long absIdx) throws StorageException {\n            long segIdx = absIdx % dsCfg.getWalSegments();\n\n            File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n            String name = FileDescriptor.fileName(absIdx);\n\n            File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n            File dstFile = new File(walArchiveDir, name);\n\n            if (log.isInfoEnabled())\n                log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n                    \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n            try {\n                Files.deleteIfExists(dstTmpFile.toPath());\n\n                Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n                Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n                if (mode != WALMode.NONE) {\n                    try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                        f0.force();\n                    }\n                }\n            }\n            catch (IOException e) {\n                throw new StorageException(\"Failed to archive WAL segment [\" +\n                    \"srcFile=\" + origFile.getAbsolutePath() +\n                    \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n            }\n\n            if (log.isInfoEnabled())\n                log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n                    \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n            return new SegmentArchiveResult(absIdx, origFile, dstFile);\n        }",
            "1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915 +\n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  ",
            "        /**\n         * Moves WAL segment from work folder to archive folder. Temp file is used to do movement\n         *\n         * @param absIdx Absolute index to archive.\n         */\n        private SegmentArchiveResult archiveSegment(long absIdx) throws StorageException {\n            long segIdx = absIdx % dsCfg.getWalSegments();\n\n            File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n            String name = FileDescriptor.fileName(absIdx);\n\n            File dstTmpFile = new File(walArchiveDir, name + FilePageStoreManager.TMP_SUFFIX);\n\n            File dstFile = new File(walArchiveDir, name);\n\n            if (log.isInfoEnabled())\n                log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n                    \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n            try {\n                Files.deleteIfExists(dstTmpFile.toPath());\n\n                Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n                Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n                if (mode != WALMode.NONE) {\n                    try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                        f0.force();\n                    }\n                }\n            }\n            catch (IOException e) {\n                throw new StorageException(\"Failed to archive WAL segment [\" +\n                    \"srcFile=\" + origFile.getAbsolutePath() +\n                    \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n            }\n\n            if (log.isInfoEnabled())\n                log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n                    \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n            return new SegmentArchiveResult(absIdx, origFile, dstFile);\n        }"
        ],
        [
            "WalCompactionTest::testSeekingStartInCompactedSegment()",
            " 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 -\n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testSeekingStartInCompactedSegment() throws Exception {\n        IgniteEx ig = (IgniteEx)startGrids(3);\n        ig.cluster().active(true);\n\n        IgniteCache<Integer, byte[]> cache = ig.cache(\"cache\");\n\n        for (int i = 0; i < 100; i++) {\n            final byte[] val = new byte[20000];\n\n            val[i] = 1;\n\n            cache.put(i, val);\n        }\n\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n\n        String nodeFolderName = ig.context().pdsFolderResolver().resolveFolders().folderName();\n\n        File dbDir = U.resolveWorkDirectory(U.defaultWorkDirectory(), \"db\", false);\n        File nodeLfsDir = new File(dbDir, nodeFolderName);\n        File cpMarkersDir = new File(nodeLfsDir, \"cp\");\n\n        final File[] cpMarkersToSave = cpMarkersDir.listFiles();\n\n        assert cpMarkersToSave != null;\n        assertTrue(cpMarkersToSave.length >= 2);\n\n        Arrays.sort(cpMarkersToSave, new Comparator<File>() {\n            @Override public int compare(File o1, File o2) {\n                return o1.getName().compareTo(o2.getName());\n            }\n        });\n\n        for (int i = 100; i < ENTRIES; i++) { // At least 20MB of raw data in total.\n            final byte[] val = new byte[20000];\n\n            val[i] = 1;\n\n            cache.put(i, val);\n        }\n\n        // Spam WAL to move all data records to compressible WAL zone.\n        for (int i = 0; i < WAL_SEGMENT_SIZE / DFLT_PAGE_SIZE * 2; i++)\n            ig.context().cache().context().wal().log(new PageSnapshot(new FullPageId(-1, -1), new byte[DFLT_PAGE_SIZE]));\n\n        // WAL archive segment is allowed to be compressed when it's at least one checkpoint away from current WAL head.\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n\n        Thread.sleep(15_000); // Allow compressor to compress WAL segments.\n\n        File walDir = new File(dbDir, \"wal\");\n        File archiveDir = new File(walDir, \"archive\");\n        File nodeArchiveDir = new File(archiveDir, nodeFolderName);\n        File walSegment = new File(nodeArchiveDir, FileDescriptor.fileName(0) + \".zip\");\n\n        assertTrue(walSegment.exists());\n        assertTrue(walSegment.length() < WAL_SEGMENT_SIZE / 2); // Should be compressed at least in half.\n\n        stopAllGrids();\n\n        File[] cpMarkers = cpMarkersDir.listFiles(new FilenameFilter() {\n            @Override public boolean accept(File dir, String name) {\n                return !(\n                    name.equals(cpMarkersToSave[0].getName()) ||\n                    name.equals(cpMarkersToSave[1].getName()) ||\n                    name.equals(cpMarkersToSave[2].getName()) ||\n                    name.equals(cpMarkersToSave[3].getName()) ||\n                    name.equals(cpMarkersToSave[4].getName())\n                );\n            }\n        });\n\n        assertNotNull(cpMarkers);\n        assertTrue(cpMarkers.length > 0);\n\n        File cacheDir = new File(nodeLfsDir, \"cache-\" + CACHE_NAME);\n        File[] lfsFiles = cacheDir.listFiles();\n\n        assertNotNull(lfsFiles);\n        assertTrue(lfsFiles.length > 0);\n\n        // Enforce reading WAL from the very beginning at the next start.\n        for (File f : cpMarkers)\n            f.delete();\n\n        for (File f : lfsFiles)\n            f.delete();\n\n        ig = (IgniteEx)startGrids(3);\n\n        awaitPartitionMapExchange();\n\n        cache = ig.cache(CACHE_NAME);\n\n        int missing = 0;\n\n        for (int i = 0; i < 100; i++) {\n            if (!cache.containsKey(i))\n                missing++;\n        }\n\n        System.out.println(\">>> Missing \" + missing + \" entries logged before WAL iteration start\");\n        assertTrue(missing > 0);\n\n        boolean fail = false;\n\n        // Check that all data is recovered from compacted WAL.\n        for (int i = 100; i < ENTRIES; i++) {\n            byte[] arr = cache.get(i);\n\n            if (arr == null) {\n                System.out.println(\">>> Missing: \" + i);\n\n                fail = true;\n            }\n            else if (arr[i] != 1) {\n                System.out.println(\">>> Corrupted: \" + i);\n\n                fail = true;\n            }\n        }\n\n        assertFalse(fail);\n    }",
            " 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testSeekingStartInCompactedSegment() throws Exception {\n        IgniteEx ig = (IgniteEx)startGrids(3);\n        ig.cluster().active(true);\n\n        IgniteCache<Integer, byte[]> cache = ig.cache(\"cache\");\n\n        for (int i = 0; i < 100; i++) {\n            final byte[] val = new byte[20000];\n\n            val[i] = 1;\n\n            cache.put(i, val);\n        }\n\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n\n        String nodeFolderName = ig.context().pdsFolderResolver().resolveFolders().folderName();\n\n        File dbDir = U.resolveWorkDirectory(U.defaultWorkDirectory(), \"db\", false);\n        File nodeLfsDir = new File(dbDir, nodeFolderName);\n        File cpMarkersDir = new File(nodeLfsDir, \"cp\");\n\n        final File[] cpMarkersToSave = cpMarkersDir.listFiles();\n\n        assert cpMarkersToSave != null;\n        assertTrue(cpMarkersToSave.length >= 2);\n\n        Arrays.sort(cpMarkersToSave, new Comparator<File>() {\n            @Override public int compare(File o1, File o2) {\n                return o1.getName().compareTo(o2.getName());\n            }\n        });\n\n        for (int i = 100; i < ENTRIES; i++) { // At least 20MB of raw data in total.\n            final byte[] val = new byte[20000];\n\n            val[i] = 1;\n\n            cache.put(i, val);\n        }\n\n        // Spam WAL to move all data records to compressible WAL zone.\n        for (int i = 0; i < WAL_SEGMENT_SIZE / DFLT_PAGE_SIZE * 2; i++)\n            ig.context().cache().context().wal().log(new PageSnapshot(new FullPageId(-1, -1), new byte[DFLT_PAGE_SIZE]));\n\n        // WAL archive segment is allowed to be compressed when it's at least one checkpoint away from current WAL head.\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n\n        Thread.sleep(15_000); // Allow compressor to compress WAL segments.\n\n        File walDir = new File(dbDir, \"wal\");\n        File archiveDir = new File(walDir, \"archive\");\n        File nodeArchiveDir = new File(archiveDir, nodeFolderName);\n        File walSegment = new File(nodeArchiveDir, FileDescriptor.fileName(0) + FilePageStoreManager.ZIP_SUFFIX);\n\n        assertTrue(walSegment.exists());\n        assertTrue(walSegment.length() < WAL_SEGMENT_SIZE / 2); // Should be compressed at least in half.\n\n        stopAllGrids();\n\n        File[] cpMarkers = cpMarkersDir.listFiles(new FilenameFilter() {\n            @Override public boolean accept(File dir, String name) {\n                return !(\n                    name.equals(cpMarkersToSave[0].getName()) ||\n                    name.equals(cpMarkersToSave[1].getName()) ||\n                    name.equals(cpMarkersToSave[2].getName()) ||\n                    name.equals(cpMarkersToSave[3].getName()) ||\n                    name.equals(cpMarkersToSave[4].getName())\n                );\n            }\n        });\n\n        assertNotNull(cpMarkers);\n        assertTrue(cpMarkers.length > 0);\n\n        File cacheDir = new File(nodeLfsDir, \"cache-\" + CACHE_NAME);\n        File[] lfsFiles = cacheDir.listFiles();\n\n        assertNotNull(lfsFiles);\n        assertTrue(lfsFiles.length > 0);\n\n        // Enforce reading WAL from the very beginning at the next start.\n        for (File f : cpMarkers)\n            f.delete();\n\n        for (File f : lfsFiles)\n            f.delete();\n\n        ig = (IgniteEx)startGrids(3);\n\n        awaitPartitionMapExchange();\n\n        cache = ig.cache(CACHE_NAME);\n\n        int missing = 0;\n\n        for (int i = 0; i < 100; i++) {\n            if (!cache.containsKey(i))\n                missing++;\n        }\n\n        System.out.println(\">>> Missing \" + missing + \" entries logged before WAL iteration start\");\n        assertTrue(missing > 0);\n\n        boolean fail = false;\n\n        // Check that all data is recovered from compacted WAL.\n        for (int i = 100; i < ENTRIES; i++) {\n            byte[] arr = cache.get(i);\n\n            if (arr == null) {\n                System.out.println(\">>> Missing: \" + i);\n\n                fail = true;\n            }\n            else if (arr[i] != 1) {\n                System.out.println(\">>> Corrupted: \" + i);\n\n                fail = true;\n            }\n        }\n\n        assertFalse(fail);\n    }"
        ],
        [
            "IgnitePdsDiskErrorsRecoveringTest::testRecoveringOnCheckpointBeginFail()",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  ",
            "    /**\n     * Test node stopping & recovering on checkpoint begin fail.\n     *\n     * @throws Exception If test failed.\n     */\n    public void testRecoveringOnCheckpointBeginFail() throws Exception {\n        // Fail to write checkpoint start marker tmp file at the second checkpoint. Pass only initial checkpoint.\n        ioFactory = new FilteringFileIOFactory(\"START.bin\" + GridCacheDatabaseSharedManager.FILE_TMP_SUFFIX, new LimitedSizeFileIOFactory(new RandomAccessFileIOFactory(), 20));\n\n        final IgniteEx grid = startGrid(0);\n        grid.cluster().active(true);\n\n        for (int i = 0; i < 1000; i++) {\n            byte payload = (byte) i;\n            byte[] data = new byte[2048];\n            Arrays.fill(data, payload);\n\n            grid.cache(CACHE_NAME).put(i, data);\n        }\n\n        String errMsg = \"Failed to write checkpoint entry\";\n\n        boolean checkpointFailed = false;\n        try {\n            forceCheckpoint();\n        }\n        catch (IgniteCheckedException e) {\n            if (e.getMessage().contains(errMsg))\n                checkpointFailed = true;\n        }\n\n        Assert.assertTrue(\"Checkpoint must be failed by IgniteCheckedException: \" + errMsg, checkpointFailed);\n\n        // Grid should be automatically stopped after checkpoint fail.\n        awaitStop(grid);\n\n        // Grid should be successfully recovered after stopping.\n        ioFactory = null;\n\n        IgniteEx recoveredGrid = startGrid(0);\n        recoveredGrid.cluster().active(true);\n\n        for (int i = 0; i < 1000; i++) {\n            byte payload = (byte) i;\n            byte[] data = new byte[2048];\n            Arrays.fill(data, payload);\n\n            byte[] actualData = (byte[]) recoveredGrid.cache(CACHE_NAME).get(i);\n            Assert.assertArrayEquals(data, actualData);\n        }\n    }",
            " 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 +\n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  ",
            "    /**\n     * Test node stopping & recovering on checkpoint begin fail.\n     *\n     * @throws Exception If test failed.\n     */\n    public void testRecoveringOnCheckpointBeginFail() throws Exception {\n        // Fail to write checkpoint start marker tmp file at the second checkpoint. Pass only initial checkpoint.\n        ioFactory = new FilteringFileIOFactory(\"START.bin\" + FilePageStoreManager.TMP_SUFFIX, new LimitedSizeFileIOFactory(new RandomAccessFileIOFactory(), 20));\n\n        final IgniteEx grid = startGrid(0);\n        grid.cluster().active(true);\n\n        for (int i = 0; i < 1000; i++) {\n            byte payload = (byte) i;\n            byte[] data = new byte[2048];\n            Arrays.fill(data, payload);\n\n            grid.cache(CACHE_NAME).put(i, data);\n        }\n\n        String errMsg = \"Failed to write checkpoint entry\";\n\n        boolean checkpointFailed = false;\n        try {\n            forceCheckpoint();\n        }\n        catch (IgniteCheckedException e) {\n            if (e.getMessage().contains(errMsg))\n                checkpointFailed = true;\n        }\n\n        Assert.assertTrue(\"Checkpoint must be failed by IgniteCheckedException: \" + errMsg, checkpointFailed);\n\n        // Grid should be automatically stopped after checkpoint fail.\n        awaitStop(grid);\n\n        // Grid should be successfully recovered after stopping.\n        ioFactory = null;\n\n        IgniteEx recoveredGrid = startGrid(0);\n        recoveredGrid.cluster().active(true);\n\n        for (int i = 0; i < 1000; i++) {\n            byte payload = (byte) i;\n            byte[] data = new byte[2048];\n            Arrays.fill(data, payload);\n\n            byte[] actualData = (byte[]) recoveredGrid.cache(CACHE_NAME).get(i);\n            Assert.assertArrayEquals(data, actualData);\n        }\n    }"
        ],
        [
            "FileWriteAheadLogManager::getAndReserveWalFiles(FileWALPointer,FileWALPointer)",
            " 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537 -\n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  ",
            "    /**\n     *  Collect wal segment files from low pointer (include) to high pointer (not include) and reserve low pointer.\n     *\n     * @param low Low bound.\n     * @param high High bound.\n     */\n    public Collection<File> getAndReserveWalFiles(FileWALPointer low, FileWALPointer high) throws IgniteCheckedException {\n        final long awaitIdx = high.index() - 1;\n\n        archivedMonitor.awaitSegmentArchived(awaitIdx);\n\n        if (!reserve(low))\n            throw new IgniteCheckedException(\"WAL archive segment has been deleted [idx=\" + low.index() + \"]\");\n\n        List<File> res = new ArrayList<>();\n\n        for (long i = low.index(); i < high.index(); i++) {\n            String segmentName = FileDescriptor.fileName(i);\n\n            File file = new File(walArchiveDir, segmentName);\n            File fileZip = new File(walArchiveDir, segmentName + \".zip\");\n\n            if (file.exists())\n                res.add(file);\n            else if (fileZip.exists())\n                res.add(fileZip);\n            else {\n                if (log.isInfoEnabled()) {\n                    log.info(\"Segment not found: \" + file.getName() + \"/\" + fileZip.getName());\n\n                    log.info(\"Stopped iteration on idx: \" + i);\n                }\n\n                break;\n            }\n        }\n\n        return res;\n    }",
            " 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538 +\n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  ",
            "    /**\n     *  Collect wal segment files from low pointer (include) to high pointer (not include) and reserve low pointer.\n     *\n     * @param low Low bound.\n     * @param high High bound.\n     */\n    public Collection<File> getAndReserveWalFiles(FileWALPointer low, FileWALPointer high) throws IgniteCheckedException {\n        final long awaitIdx = high.index() - 1;\n\n        archivedMonitor.awaitSegmentArchived(awaitIdx);\n\n        if (!reserve(low))\n            throw new IgniteCheckedException(\"WAL archive segment has been deleted [idx=\" + low.index() + \"]\");\n\n        List<File> res = new ArrayList<>();\n\n        for (long i = low.index(); i < high.index(); i++) {\n            String segmentName = FileDescriptor.fileName(i);\n\n            File file = new File(walArchiveDir, segmentName);\n            File fileZip = new File(walArchiveDir, segmentName + FilePageStoreManager.ZIP_SUFFIX);\n\n            if (file.exists())\n                res.add(file);\n            else if (fileZip.exists())\n                res.add(fileZip);\n            else {\n                if (log.isInfoEnabled()) {\n                    log.info(\"Segment not found: \" + file.getName() + \"/\" + fileZip.getName());\n\n                    log.info(\"Stopped iteration on idx: \" + i);\n                }\n\n                break;\n            }\n        }\n\n        return res;\n    }"
        ],
        [
            "FsyncModeFileWriteAheadLogManager::FileCompressor::run()",
            "1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937 -\n1938  \n1939 -\n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            init();\n\n            while (!Thread.currentThread().isInterrupted() && !stopped) {\n                long currReservedSegment = -1;\n\n                try {\n                    deleteObsoleteRawSegments();\n\n                    currReservedSegment = tryReserveNextSegmentOrWait();\n                    if (currReservedSegment == -1)\n                        continue;\n\n                    File tmpZip = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment) + \".zip\" + \".tmp\");\n\n                    File zip = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment) + \".zip\");\n\n                    File raw = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment));\n                    if (!Files.exists(raw.toPath()))\n                        throw new IgniteCheckedException(\"WAL archive segment is missing: \" + raw);\n\n                    compressSegmentToFile(currReservedSegment, raw, tmpZip);\n\n                    Files.move(tmpZip.toPath(), zip.toPath());\n\n                    if (mode != WALMode.NONE) {\n                        try (FileIO f0 = ioFactory.create(zip, CREATE, READ, WRITE)) {\n                            f0.force();\n                        }\n\n                        if (evt.isRecordable(EVT_WAL_SEGMENT_COMPACTED)) {\n                            evt.record(new WalSegmentCompactedEvent(\n                                cctx.discovery().localNode(),\n                                currReservedSegment,\n                                zip.getAbsoluteFile())\n                            );\n                        }\n                    }\n\n                    lastCompressedIdx = currReservedSegment;\n                }\n                catch (IgniteCheckedException | IOException e) {\n                    U.error(log, \"Compression of WAL segment [idx=\" + currReservedSegment +\n                        \"] was skipped due to unexpected error\", e);\n\n                    lastCompressedIdx++;\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n                }\n                finally {\n                    try {\n                        if (currReservedSegment != -1)\n                            release(new FileWALPointer(currReservedSegment, 0, 0));\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Can't release raw WAL segment [idx=\" + currReservedSegment +\n                            \"] after compression\", e);\n                    }\n                }\n            }\n        }",
            "1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938 +\n1939 +\n1940  \n1941 +\n1942 +\n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            init();\n\n            while (!Thread.currentThread().isInterrupted() && !stopped) {\n                long currReservedSegment = -1;\n\n                try {\n                    deleteObsoleteRawSegments();\n\n                    currReservedSegment = tryReserveNextSegmentOrWait();\n                    if (currReservedSegment == -1)\n                        continue;\n\n                    File tmpZip = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment)\n                        + FilePageStoreManager.ZIP_SUFFIX + FilePageStoreManager.TMP_SUFFIX);\n\n                    File zip = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment)\n                        + FilePageStoreManager.ZIP_SUFFIX);\n\n                    File raw = new File(walArchiveDir, FileDescriptor.fileName(currReservedSegment));\n                    if (!Files.exists(raw.toPath()))\n                        throw new IgniteCheckedException(\"WAL archive segment is missing: \" + raw);\n\n                    compressSegmentToFile(currReservedSegment, raw, tmpZip);\n\n                    Files.move(tmpZip.toPath(), zip.toPath());\n\n                    if (mode != WALMode.NONE) {\n                        try (FileIO f0 = ioFactory.create(zip, CREATE, READ, WRITE)) {\n                            f0.force();\n                        }\n\n                        if (evt.isRecordable(EVT_WAL_SEGMENT_COMPACTED)) {\n                            evt.record(new WalSegmentCompactedEvent(\n                                cctx.discovery().localNode(),\n                                currReservedSegment,\n                                zip.getAbsoluteFile())\n                            );\n                        }\n                    }\n\n                    lastCompressedIdx = currReservedSegment;\n                }\n                catch (IgniteCheckedException | IOException e) {\n                    U.error(log, \"Compression of WAL segment [idx=\" + currReservedSegment +\n                        \"] was skipped due to unexpected error\", e);\n\n                    lastCompressedIdx++;\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n                }\n                finally {\n                    try {\n                        if (currReservedSegment != -1)\n                            release(new FileWALPointer(currReservedSegment, 0, 0));\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Can't release raw WAL segment [idx=\" + currReservedSegment +\n                            \"] after compression\", e);\n                    }\n                }\n            }\n        }"
        ],
        [
            "FsyncModeFileWriteAheadLogManager::hasIndex(long)",
            " 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818 -\n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  ",
            "    /**\n     * @param absIdx Absolulte index to check.\n     * @return {@code true} if has this index.\n     */\n    private boolean hasIndex(long absIdx) {\n        String segmentName = FileDescriptor.fileName(absIdx);\n\n        String zipSegmentName = FileDescriptor.fileName(absIdx) + \".zip\";\n\n        boolean inArchive = new File(walArchiveDir, segmentName).exists() ||\n            new File(walArchiveDir, zipSegmentName).exists();\n\n        if (inArchive)\n            return true;\n\n        if (absIdx <= lastArchivedIndex())\n            return false;\n\n        FileWriteHandle cur = currentHnd;\n\n        return cur != null && cur.idx >= absIdx;\n    }",
            " 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819 +\n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  ",
            "    /**\n     * @param absIdx Absolulte index to check.\n     * @return {@code true} if has this index.\n     */\n    private boolean hasIndex(long absIdx) {\n        String segmentName = FileDescriptor.fileName(absIdx);\n\n        String zipSegmentName = FileDescriptor.fileName(absIdx) + FilePageStoreManager.ZIP_SUFFIX;\n\n        boolean inArchive = new File(walArchiveDir, segmentName).exists() ||\n            new File(walArchiveDir, zipSegmentName).exists();\n\n        if (inArchive)\n            return true;\n\n        if (absIdx <= lastArchivedIndex())\n            return false;\n\n        FileWriteHandle cur = currentHnd;\n\n        return cur != null && cur.idx >= absIdx;\n    }"
        ],
        [
            "HadoopV2JobResourceManager::processFiles(File,Object,boolean,boolean,Collection,String)",
            " 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237 -\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  ",
            "    /**\n     * Process list of resources.\n     *\n     * @param jobLocDir Job working directory.\n     * @param files Array of {@link URI} or {@link org.apache.hadoop.fs.Path} to process resources.\n     * @param download {@code true}, if need to download. Process class path only else.\n     * @param extract {@code true}, if need to extract archive.\n     * @param clsPathUrls Collection to add resource as classpath resource.\n     * @param rsrcNameProp Property for resource name array setting.\n     * @throws IOException If failed.\n     */\n    private void processFiles(File jobLocDir, @Nullable Object[] files, boolean download, boolean extract,\n        @Nullable Collection<URL> clsPathUrls, @Nullable String rsrcNameProp) throws IOException {\n        if (F.isEmptyOrNulls(files))\n            return;\n\n        Collection<String> res = new ArrayList<>();\n\n        for (Object pathObj : files) {\n            Path srcPath;\n\n            if (pathObj instanceof URI) {\n                URI uri = (URI)pathObj;\n\n                srcPath = new Path(uri);\n            }\n            else\n                srcPath = (Path)pathObj;\n\n            String locName = srcPath.getName();\n\n            File dstPath = new File(jobLocDir.getAbsolutePath(), locName);\n\n            res.add(locName);\n\n            rsrcSet.add(dstPath);\n\n            if (clsPathUrls != null)\n                clsPathUrls.add(dstPath.toURI().toURL());\n\n            if (!download)\n                continue;\n\n            JobConf cfg = ctx.getJobConf();\n\n            FileSystem dstFs = FileSystem.getLocal(cfg);\n\n            FileSystem srcFs = job.fileSystem(srcPath.toUri(), cfg);\n\n            if (extract) {\n                File archivesPath = new File(jobLocDir.getAbsolutePath(), \".cached-archives\");\n\n                if (!archivesPath.exists() && !archivesPath.mkdir())\n                    throw new IOException(\"Failed to create directory \" +\n                        \"[path=\" + archivesPath + \", jobId=\" + jobId + ']');\n\n                File archiveFile = new File(archivesPath, locName);\n\n                FileUtil.copy(srcFs, srcPath, dstFs, new Path(archiveFile.toString()), false, cfg);\n\n                String archiveNameLC = archiveFile.getName().toLowerCase();\n\n                if (archiveNameLC.endsWith(\".jar\"))\n                    RunJar.unJar(archiveFile, dstPath);\n                else if (archiveNameLC.endsWith(\".zip\"))\n                    FileUtil.unZip(archiveFile, dstPath);\n                else if (archiveNameLC.endsWith(\".tar.gz\") ||\n                    archiveNameLC.endsWith(\".tgz\") ||\n                    archiveNameLC.endsWith(\".tar\"))\n                    FileUtil.unTar(archiveFile, dstPath);\n                else\n                    throw new IOException(\"Cannot unpack archive [path=\" + srcPath + \", jobId=\" + jobId + ']');\n            }\n            else\n                FileUtil.copy(srcFs, srcPath, dstFs, new Path(dstPath.toString()), false, cfg);\n        }\n\n        if (!res.isEmpty() && rsrcNameProp != null)\n            ctx.getJobConf().setStrings(rsrcNameProp, res.toArray(new String[res.size()]));\n    }",
            " 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 +\n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  ",
            "    /**\n     * Process list of resources.\n     *\n     * @param jobLocDir Job working directory.\n     * @param files Array of {@link URI} or {@link org.apache.hadoop.fs.Path} to process resources.\n     * @param download {@code true}, if need to download. Process class path only else.\n     * @param extract {@code true}, if need to extract archive.\n     * @param clsPathUrls Collection to add resource as classpath resource.\n     * @param rsrcNameProp Property for resource name array setting.\n     * @throws IOException If failed.\n     */\n    private void processFiles(File jobLocDir, @Nullable Object[] files, boolean download, boolean extract,\n        @Nullable Collection<URL> clsPathUrls, @Nullable String rsrcNameProp) throws IOException {\n        if (F.isEmptyOrNulls(files))\n            return;\n\n        Collection<String> res = new ArrayList<>();\n\n        for (Object pathObj : files) {\n            Path srcPath;\n\n            if (pathObj instanceof URI) {\n                URI uri = (URI)pathObj;\n\n                srcPath = new Path(uri);\n            }\n            else\n                srcPath = (Path)pathObj;\n\n            String locName = srcPath.getName();\n\n            File dstPath = new File(jobLocDir.getAbsolutePath(), locName);\n\n            res.add(locName);\n\n            rsrcSet.add(dstPath);\n\n            if (clsPathUrls != null)\n                clsPathUrls.add(dstPath.toURI().toURL());\n\n            if (!download)\n                continue;\n\n            JobConf cfg = ctx.getJobConf();\n\n            FileSystem dstFs = FileSystem.getLocal(cfg);\n\n            FileSystem srcFs = job.fileSystem(srcPath.toUri(), cfg);\n\n            if (extract) {\n                File archivesPath = new File(jobLocDir.getAbsolutePath(), \".cached-archives\");\n\n                if (!archivesPath.exists() && !archivesPath.mkdir())\n                    throw new IOException(\"Failed to create directory \" +\n                        \"[path=\" + archivesPath + \", jobId=\" + jobId + ']');\n\n                File archiveFile = new File(archivesPath, locName);\n\n                FileUtil.copy(srcFs, srcPath, dstFs, new Path(archiveFile.toString()), false, cfg);\n\n                String archiveNameLC = archiveFile.getName().toLowerCase();\n\n                if (archiveNameLC.endsWith(\".jar\"))\n                    RunJar.unJar(archiveFile, dstPath);\n                else if (archiveNameLC.endsWith(FilePageStoreManager.ZIP_SUFFIX))\n                    FileUtil.unZip(archiveFile, dstPath);\n                else if (archiveNameLC.endsWith(\".tar.gz\") ||\n                    archiveNameLC.endsWith(\".tgz\") ||\n                    archiveNameLC.endsWith(\".tar\"))\n                    FileUtil.unTar(archiveFile, dstPath);\n                else\n                    throw new IOException(\"Cannot unpack archive [path=\" + srcPath + \", jobId=\" + jobId + ']');\n            }\n            else\n                FileUtil.copy(srcFs, srcPath, dstFs, new Path(dstPath.toString()), false, cfg);\n        }\n\n        if (!res.isEmpty() && rsrcNameProp != null)\n            ctx.getJobConf().setStrings(rsrcNameProp, res.toArray(new String[res.size()]));\n    }"
        ],
        [
            "FilePageStoreManager::storeCacheData(StoredCacheData,boolean)",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344 -\n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  ",
            "    /** {@inheritDoc} */\n    @Override public void storeCacheData(StoredCacheData cacheData, boolean overwrite) throws IgniteCheckedException {\n        File cacheWorkDir = cacheWorkDir(cacheData.config());\n        File file;\n\n        checkAndInitCacheWorkDir(cacheWorkDir);\n\n        assert cacheWorkDir.exists() : \"Work directory does not exist: \" + cacheWorkDir;\n\n        if (cacheData.config().getGroupName() != null)\n            file = new File(cacheWorkDir, cacheData.config().getName() + CACHE_DATA_FILENAME);\n        else\n            file = new File(cacheWorkDir, CACHE_DATA_FILENAME);\n\n        if (overwrite || !file.exists() || file.length() == 0) {\n            try {\n                File tmp = new File(file.getParent(), file.getName() + \".tmp\");\n\n                tmp.createNewFile();\n\n                // Pre-existing file will be truncated upon stream open.\n                try (OutputStream stream = new BufferedOutputStream(new FileOutputStream(tmp))) {\n                    marshaller.marshal(cacheData, stream);\n                }\n\n                if (file.exists())\n                    file.delete();\n\n                Files.move(tmp.toPath(), file.toPath());\n            }\n            catch (IOException ex) {\n                throw new IgniteCheckedException(\"Failed to persist cache configuration: \" + cacheData.config().getName(), ex);\n            }\n        }\n    }",
            " 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350 +\n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  ",
            "    /** {@inheritDoc} */\n    @Override public void storeCacheData(StoredCacheData cacheData, boolean overwrite) throws IgniteCheckedException {\n        File cacheWorkDir = cacheWorkDir(cacheData.config());\n        File file;\n\n        checkAndInitCacheWorkDir(cacheWorkDir);\n\n        assert cacheWorkDir.exists() : \"Work directory does not exist: \" + cacheWorkDir;\n\n        if (cacheData.config().getGroupName() != null)\n            file = new File(cacheWorkDir, cacheData.config().getName() + CACHE_DATA_FILENAME);\n        else\n            file = new File(cacheWorkDir, CACHE_DATA_FILENAME);\n\n        if (overwrite || !file.exists() || file.length() == 0) {\n            try {\n                File tmp = new File(file.getParent(), file.getName() + TMP_SUFFIX);\n\n                tmp.createNewFile();\n\n                // Pre-existing file will be truncated upon stream open.\n                try (OutputStream stream = new BufferedOutputStream(new FileOutputStream(tmp))) {\n                    marshaller.marshal(cacheData, stream);\n                }\n\n                if (file.exists())\n                    file.delete();\n\n                Files.move(tmp.toPath(), file.toPath());\n            }\n            catch (IOException ex) {\n                throw new IgniteCheckedException(\"Failed to persist cache configuration: \" + cacheData.config().getName(), ex);\n            }\n        }\n    }"
        ],
        [
            "FsyncModeFileWriteAheadLogManager::createFile(File)",
            "1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348 -\n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  ",
            "    /**\n     * Creates a file atomically with temp file.\n     *\n     * @param file File to create.\n     * @throws StorageException If failed.\n     */\n    private void createFile(File file) throws StorageException {\n        if (log.isDebugEnabled())\n            log.debug(\"Creating new file [exists=\" + file.exists() + \", file=\" + file.getAbsolutePath() + ']');\n\n        File tmp = new File(file.getParent(), file.getName() + \".tmp\");\n\n        formatFile(tmp);\n\n        try {\n            Files.move(tmp.toPath(), file.toPath());\n        }\n        catch (IOException e) {\n            throw new StorageException(\"Failed to move temp file to a regular WAL segment file: \" +\n                file.getAbsolutePath(), e);\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(\"Created WAL segment [file=\" + file.getAbsolutePath() + \", size=\" + file.length() + ']');\n    }",
            "1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349 +\n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  ",
            "    /**\n     * Creates a file atomically with temp file.\n     *\n     * @param file File to create.\n     * @throws StorageException If failed.\n     */\n    private void createFile(File file) throws StorageException {\n        if (log.isDebugEnabled())\n            log.debug(\"Creating new file [exists=\" + file.exists() + \", file=\" + file.getAbsolutePath() + ']');\n\n        File tmp = new File(file.getParent(), file.getName() + FilePageStoreManager.TMP_SUFFIX);\n\n        formatFile(tmp);\n\n        try {\n            Files.move(tmp.toPath(), file.toPath());\n        }\n        catch (IOException e) {\n            throw new StorageException(\"Failed to move temp file to a regular WAL segment file: \" +\n                file.getAbsolutePath(), e);\n        }\n\n        if (log.isDebugEnabled())\n            log.debug(\"Created WAL segment [file=\" + file.getAbsolutePath() + \", size=\" + file.length() + ']');\n    }"
        ],
        [
            "FileDescriptor::isCompressed()",
            " 122  \n 123  \n 124 -\n 125  ",
            "    /** {@inheritDoc} */\n    @Override public boolean isCompressed() {\n        return file.getName().endsWith(\".zip\");\n    }",
            " 123  \n 124  \n 125 +\n 126  ",
            "    /** {@inheritDoc} */\n    @Override public boolean isCompressed() {\n        return file.getName().endsWith(FilePageStoreManager.ZIP_SUFFIX);\n    }"
        ],
        [
            "GridCacheDatabaseSharedManager::cleanupTempCheckpointDirectory()",
            " 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507 -\n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  ",
            "    /**\n     * Cleanup checkpoint directory from all temporary files {@link #FILE_TMP_SUFFIX}.\n     */\n    @Override public void cleanupTempCheckpointDirectory() throws IgniteCheckedException {\n        try {\n            try (DirectoryStream<Path> files = Files.newDirectoryStream(\n                cpDir.toPath(),\n                path -> path.endsWith(FILE_TMP_SUFFIX))\n            ) {\n                for (Path path : files)\n                    Files.delete(path);\n            }\n        }\n        catch (IOException e) {\n            throw new IgniteCheckedException(\"Failed to cleanup checkpoint directory from temporary files: \" + cpDir, e);\n        }\n    }",
            " 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504 +\n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  ",
            "    /**\n     * Cleanup checkpoint directory from all temporary files.\n     */\n    @Override public void cleanupTempCheckpointDirectory() throws IgniteCheckedException {\n        try {\n            try (DirectoryStream<Path> files = Files.newDirectoryStream(\n                cpDir.toPath(),\n                path -> path.endsWith(FilePageStoreManager.TMP_SUFFIX))\n            ) {\n                for (Path path : files)\n                    Files.delete(path);\n            }\n        }\n        catch (IOException e) {\n            throw new IgniteCheckedException(\"Failed to cleanup checkpoint directory from temporary files: \" + cpDir, e);\n        }\n    }"
        ],
        [
            "FileWriteAheadLogManager::RecordsIterator::initReadHandle(AbstractFileDescriptor,FileWALPointer)",
            "3055  \n3056  \n3057  \n3058  \n3059  \n3060  \n3061  \n3062  \n3063  \n3064 -\n3065  \n3066  \n3067  \n3068  \n3069  \n3070  \n3071  \n3072  \n3073  \n3074  \n3075  \n3076  \n3077  \n3078  ",
            "        /** {@inheritDoc} */\n        @Override protected ReadFileHandle initReadHandle(\n            @NotNull AbstractFileDescriptor desc,\n            @Nullable FileWALPointer start\n        ) throws IgniteCheckedException, FileNotFoundException {\n            AbstractFileDescriptor currDesc = desc;\n\n            if (!desc.file().exists()) {\n                FileDescriptor zipFile = new FileDescriptor(\n                    new File(walArchiveDir, FileDescriptor.fileName(desc.idx()) + \".zip\"));\n\n                if (!zipFile.file.exists()) {\n                    throw new FileNotFoundException(\"Both compressed and raw segment files are missing in archive \" +\n                        \"[segmentIdx=\" + desc.idx() + \"]\");\n                }\n\n                if (decompressor != null)\n                    decompressor.decompressFile(desc.idx()).get();\n                else\n                    currDesc = zipFile;\n            }\n\n            return (ReadFileHandle) super.initReadHandle(currDesc, start);\n        }",
            "3059  \n3060  \n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068 +\n3069 +\n3070  \n3071  \n3072  \n3073  \n3074  \n3075  \n3076  \n3077  \n3078  \n3079  \n3080  \n3081  \n3082  \n3083  ",
            "        /** {@inheritDoc} */\n        @Override protected ReadFileHandle initReadHandle(\n            @NotNull AbstractFileDescriptor desc,\n            @Nullable FileWALPointer start\n        ) throws IgniteCheckedException, FileNotFoundException {\n            AbstractFileDescriptor currDesc = desc;\n\n            if (!desc.file().exists()) {\n                FileDescriptor zipFile = new FileDescriptor(\n                    new File(walArchiveDir, FileDescriptor.fileName(desc.idx())\n                        + FilePageStoreManager.ZIP_SUFFIX));\n\n                if (!zipFile.file.exists()) {\n                    throw new FileNotFoundException(\"Both compressed and raw segment files are missing in archive \" +\n                        \"[segmentIdx=\" + desc.idx() + \"]\");\n                }\n\n                if (decompressor != null)\n                    decompressor.decompressFile(desc.idx()).get();\n                else\n                    currDesc = zipFile;\n            }\n\n            return (ReadFileHandle) super.initReadHandle(currDesc, start);\n        }"
        ],
        [
            "WalCompactionTest::testApplyingUpdatesFromCompactedWal(boolean)",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 -\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "    /**\n     * @param switchOffCompressor Switch off compressor after restart.\n     * @throws Exception If failed.\n     */\n    private void testApplyingUpdatesFromCompactedWal(boolean switchOffCompressor) throws Exception {\n        IgniteEx ig = (IgniteEx)startGrids(3);\n        ig.cluster().active(true);\n\n        IgniteCache<Integer, byte[]> cache = ig.cache(\"cache\");\n\n        for (int i = 0; i < ENTRIES; i++) { // At least 20MB of raw data in total.\n            final byte[] val = new byte[20000];\n\n            val[i] = 1;\n\n            cache.put(i, val);\n        }\n\n        // Spam WAL to move all data records to compressible WAL zone.\n        for (int i = 0; i < WAL_SEGMENT_SIZE / DFLT_PAGE_SIZE * 2; i++)\n            ig.context().cache().context().wal().log(new PageSnapshot(new FullPageId(-1, -1), new byte[DFLT_PAGE_SIZE]));\n\n        // WAL archive segment is allowed to be compressed when it's at least one checkpoint away from current WAL head.\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n\n        Thread.sleep(15_000); // Allow compressor to compress WAL segments.\n\n        String nodeFolderName = ig.context().pdsFolderResolver().resolveFolders().folderName();\n\n        File dbDir = U.resolveWorkDirectory(U.defaultWorkDirectory(), \"db\", false);\n        File walDir = new File(dbDir, \"wal\");\n        File archiveDir = new File(walDir, \"archive\");\n        File nodeArchiveDir = new File(archiveDir, nodeFolderName);\n        File walSegment = new File(nodeArchiveDir, FileDescriptor.fileName(0) + \".zip\");\n\n        assertTrue(walSegment.exists());\n        assertTrue(walSegment.length() < WAL_SEGMENT_SIZE / 2); // Should be compressed at least in half.\n\n        stopAllGrids();\n\n        File nodeLfsDir = new File(dbDir, nodeFolderName);\n        File cpMarkersDir = new File(nodeLfsDir, \"cp\");\n\n        File[] cpMarkers = cpMarkersDir.listFiles();\n\n        assertNotNull(cpMarkers);\n        assertTrue(cpMarkers.length > 0);\n\n        File cacheDir = new File(nodeLfsDir, \"cache-\" + CACHE_NAME);\n        File[] lfsFiles = cacheDir.listFiles();\n\n        assertNotNull(lfsFiles);\n        assertTrue(lfsFiles.length > 0);\n\n        // Enforce reading WAL from the very beginning at the next start.\n        for (File f : cpMarkers)\n            f.delete();\n\n        for (File f : lfsFiles)\n            f.delete();\n\n        compactionEnabled = !switchOffCompressor;\n\n        ig = (IgniteEx)startGrids(3);\n\n        awaitPartitionMapExchange();\n\n        cache = ig.cache(CACHE_NAME);\n\n        boolean fail = false;\n\n        // Check that all data is recovered from compacted WAL.\n        for (int i = 0; i < ENTRIES; i++) {\n            byte[] arr = cache.get(i);\n\n            if (arr == null) {\n                System.out.println(\">>> Missing: \" + i);\n\n                fail = true;\n            }\n            else if (arr[i] != 1) {\n                System.out.println(\">>> Corrupted: \" + i);\n\n                fail = true;\n            }\n        }\n\n        assertFalse(fail);\n    }",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  ",
            "    /**\n     * @param switchOffCompressor Switch off compressor after restart.\n     * @throws Exception If failed.\n     */\n    private void testApplyingUpdatesFromCompactedWal(boolean switchOffCompressor) throws Exception {\n        IgniteEx ig = (IgniteEx)startGrids(3);\n        ig.cluster().active(true);\n\n        IgniteCache<Integer, byte[]> cache = ig.cache(\"cache\");\n\n        for (int i = 0; i < ENTRIES; i++) { // At least 20MB of raw data in total.\n            final byte[] val = new byte[20000];\n\n            val[i] = 1;\n\n            cache.put(i, val);\n        }\n\n        // Spam WAL to move all data records to compressible WAL zone.\n        for (int i = 0; i < WAL_SEGMENT_SIZE / DFLT_PAGE_SIZE * 2; i++)\n            ig.context().cache().context().wal().log(new PageSnapshot(new FullPageId(-1, -1), new byte[DFLT_PAGE_SIZE]));\n\n        // WAL archive segment is allowed to be compressed when it's at least one checkpoint away from current WAL head.\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n        ig.context().cache().context().database().wakeupForCheckpoint(\"Forced checkpoint\").get();\n\n        Thread.sleep(15_000); // Allow compressor to compress WAL segments.\n\n        String nodeFolderName = ig.context().pdsFolderResolver().resolveFolders().folderName();\n\n        File dbDir = U.resolveWorkDirectory(U.defaultWorkDirectory(), \"db\", false);\n        File walDir = new File(dbDir, \"wal\");\n        File archiveDir = new File(walDir, \"archive\");\n        File nodeArchiveDir = new File(archiveDir, nodeFolderName);\n        File walSegment = new File(nodeArchiveDir, FileDescriptor.fileName(0) + FilePageStoreManager.ZIP_SUFFIX);\n\n        assertTrue(walSegment.exists());\n        assertTrue(walSegment.length() < WAL_SEGMENT_SIZE / 2); // Should be compressed at least in half.\n\n        stopAllGrids();\n\n        File nodeLfsDir = new File(dbDir, nodeFolderName);\n        File cpMarkersDir = new File(nodeLfsDir, \"cp\");\n\n        File[] cpMarkers = cpMarkersDir.listFiles();\n\n        assertNotNull(cpMarkers);\n        assertTrue(cpMarkers.length > 0);\n\n        File cacheDir = new File(nodeLfsDir, \"cache-\" + CACHE_NAME);\n        File[] lfsFiles = cacheDir.listFiles();\n\n        assertNotNull(lfsFiles);\n        assertTrue(lfsFiles.length > 0);\n\n        // Enforce reading WAL from the very beginning at the next start.\n        for (File f : cpMarkers)\n            f.delete();\n\n        for (File f : lfsFiles)\n            f.delete();\n\n        compactionEnabled = !switchOffCompressor;\n\n        ig = (IgniteEx)startGrids(3);\n\n        awaitPartitionMapExchange();\n\n        cache = ig.cache(CACHE_NAME);\n\n        boolean fail = false;\n\n        // Check that all data is recovered from compacted WAL.\n        for (int i = 0; i < ENTRIES; i++) {\n            byte[] arr = cache.get(i);\n\n            if (arr == null) {\n                System.out.println(\">>> Missing: \" + i);\n\n                fail = true;\n            }\n            else if (arr[i] != 1) {\n                System.out.println(\">>> Corrupted: \" + i);\n\n                fail = true;\n            }\n        }\n\n        assertFalse(fail);\n    }"
        ],
        [
            "FsyncModeFileWriteAheadLogManager::RecordsIterator::initReadHandle(AbstractFileDescriptor,FileWALPointer)",
            "3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149 -\n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  ",
            "        /** {@inheritDoc} */\n        @Override protected ReadFileHandle initReadHandle(\n            @NotNull AbstractFileDescriptor desc,\n            @Nullable FileWALPointer start\n        ) throws IgniteCheckedException, FileNotFoundException {\n            AbstractFileDescriptor currDesc = desc;\n\n            if (!desc.file().exists()) {\n                FileDescriptor zipFile = new FileDescriptor(\n                        new File(walArchiveDir, FileDescriptor.fileName(desc.idx()) + \".zip\"));\n\n                if (!zipFile.file.exists()) {\n                    throw new FileNotFoundException(\"Both compressed and raw segment files are missing in archive \" +\n                            \"[segmentIdx=\" + desc.idx() + \"]\");\n                }\n\n                if (decompressor != null)\n                    decompressor.decompressFile(desc.idx()).get();\n                else\n                    currDesc = zipFile;\n            }\n\n            return (ReadFileHandle) super.initReadHandle(currDesc, start);\n        }",
            "3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154 +\n3155 +\n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  ",
            "        /** {@inheritDoc} */\n        @Override protected ReadFileHandle initReadHandle(\n            @NotNull AbstractFileDescriptor desc,\n            @Nullable FileWALPointer start\n        ) throws IgniteCheckedException, FileNotFoundException {\n            AbstractFileDescriptor currDesc = desc;\n\n            if (!desc.file().exists()) {\n                FileDescriptor zipFile = new FileDescriptor(\n                        new File(walArchiveDir, FileDescriptor.fileName(desc.idx())\n                            + FilePageStoreManager.ZIP_SUFFIX));\n\n                if (!zipFile.file.exists()) {\n                    throw new FileNotFoundException(\"Both compressed and raw segment files are missing in archive \" +\n                            \"[segmentIdx=\" + desc.idx() + \"]\");\n                }\n\n                if (decompressor != null)\n                    decompressor.decompressFile(desc.idx()).get();\n                else\n                    currDesc = zipFile;\n            }\n\n            return (ReadFileHandle) super.initReadHandle(currDesc, start);\n        }"
        ],
        [
            "GridCacheDatabaseSharedManager::writeCheckpointEntry(ByteBuffer,CheckpointEntry,CheckpointEntryType)",
            "2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669 -\n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692  ",
            "    /**\n     * Writes checkpoint entry buffer {@code entryBuf} to specified checkpoint file with 2-phase protocol.\n     *\n     * @param entryBuf Checkpoint entry buffer to write.\n     * @param cp Checkpoint entry.\n     * @param type Checkpoint entry type.\n     * @throws StorageException If failed to write checkpoint entry.\n     */\n    public void writeCheckpointEntry(ByteBuffer entryBuf, CheckpointEntry cp, CheckpointEntryType type) throws StorageException {\n        String fileName = checkpointFileName(cp, type);\n        String tmpFileName = fileName + FILE_TMP_SUFFIX;\n\n        try {\n            try (FileIO io = ioFactory.create(Paths.get(cpDir.getAbsolutePath(), skipSync ? fileName : tmpFileName).toFile(),\n                StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE)) {\n\n                io.writeFully(entryBuf);\n\n                entryBuf.clear();\n\n                if (!skipSync)\n                    io.force(true);\n            }\n\n            if (!skipSync)\n                Files.move(Paths.get(cpDir.getAbsolutePath(), tmpFileName), Paths.get(cpDir.getAbsolutePath(), fileName));\n        }\n        catch (IOException e) {\n            throw new StorageException(\"Failed to write checkpoint entry [ptr=\" + cp.checkpointMark()\n                + \", cpTs=\" + cp.timestamp()\n                + \", cpId=\" + cp.checkpointId()\n                + \", type=\" + type + \"]\", e);\n        }\n    }",
            "2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666 +\n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  ",
            "    /**\n     * Writes checkpoint entry buffer {@code entryBuf} to specified checkpoint file with 2-phase protocol.\n     *\n     * @param entryBuf Checkpoint entry buffer to write.\n     * @param cp Checkpoint entry.\n     * @param type Checkpoint entry type.\n     * @throws StorageException If failed to write checkpoint entry.\n     */\n    public void writeCheckpointEntry(ByteBuffer entryBuf, CheckpointEntry cp, CheckpointEntryType type) throws StorageException {\n        String fileName = checkpointFileName(cp, type);\n        String tmpFileName = fileName + FilePageStoreManager.TMP_SUFFIX;\n\n        try {\n            try (FileIO io = ioFactory.create(Paths.get(cpDir.getAbsolutePath(), skipSync ? fileName : tmpFileName).toFile(),\n                StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE)) {\n\n                io.writeFully(entryBuf);\n\n                entryBuf.clear();\n\n                if (!skipSync)\n                    io.force(true);\n            }\n\n            if (!skipSync)\n                Files.move(Paths.get(cpDir.getAbsolutePath(), tmpFileName), Paths.get(cpDir.getAbsolutePath(), fileName));\n        }\n        catch (IOException e) {\n            throw new StorageException(\"Failed to write checkpoint entry [ptr=\" + cp.checkpointMark()\n                + \", cpTs=\" + cp.timestamp()\n                + \", cpId=\" + cp.checkpointId()\n                + \", type=\" + type + \"]\", e);\n        }\n    }"
        ],
        [
            "GridCacheDatabaseSharedManager::nodeStart(WALPointer)",
            " 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889 -\n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  ",
            "    /**\n     * Creates file with current timestamp and specific \"node-started.bin\" suffix\n     * and writes into memory recovery pointer.\n     *\n     * @param ptr Memory recovery wal pointer.\n     */\n    private void nodeStart(WALPointer ptr) throws IgniteCheckedException {\n        FileWALPointer p = (FileWALPointer)ptr;\n\n        String fileName = U.currentTimeMillis() + NODE_STARTED_FILE_NAME_SUFFIX;\n        String tmpFileName = fileName + FILE_TMP_SUFFIX;\n\n        ByteBuffer buf = ByteBuffer.allocate(FileWALPointer.POINTER_SIZE);\n        buf.order(ByteOrder.nativeOrder());\n\n        try {\n            try (FileIO io = ioFactory.create(Paths.get(cpDir.getAbsolutePath(), tmpFileName).toFile(),\n                    StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE)) {\n                buf.putLong(p.index());\n\n                buf.putInt(p.fileOffset());\n\n                buf.putInt(p.length());\n\n                buf.flip();\n\n                io.writeFully(buf);\n\n                buf.clear();\n\n                io.force(true);\n            }\n\n            Files.move(Paths.get(cpDir.getAbsolutePath(), tmpFileName), Paths.get(cpDir.getAbsolutePath(), fileName));\n        }\n        catch (IOException e) {\n            throw new StorageException(\"Failed to write node start marker: \" + ptr, e);\n        }\n    }",
            " 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886 +\n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  ",
            "    /**\n     * Creates file with current timestamp and specific \"node-started.bin\" suffix\n     * and writes into memory recovery pointer.\n     *\n     * @param ptr Memory recovery wal pointer.\n     */\n    private void nodeStart(WALPointer ptr) throws IgniteCheckedException {\n        FileWALPointer p = (FileWALPointer)ptr;\n\n        String fileName = U.currentTimeMillis() + NODE_STARTED_FILE_NAME_SUFFIX;\n        String tmpFileName = fileName + FilePageStoreManager.TMP_SUFFIX;\n\n        ByteBuffer buf = ByteBuffer.allocate(FileWALPointer.POINTER_SIZE);\n        buf.order(ByteOrder.nativeOrder());\n\n        try {\n            try (FileIO io = ioFactory.create(Paths.get(cpDir.getAbsolutePath(), tmpFileName).toFile(),\n                    StandardOpenOption.CREATE_NEW, StandardOpenOption.WRITE)) {\n                buf.putLong(p.index());\n\n                buf.putInt(p.fileOffset());\n\n                buf.putInt(p.length());\n\n                buf.flip();\n\n                io.writeFully(buf);\n\n                buf.clear();\n\n                io.force(true);\n            }\n\n            Files.move(Paths.get(cpDir.getAbsolutePath(), tmpFileName), Paths.get(cpDir.getAbsolutePath(), fileName));\n        }\n        catch (IOException e) {\n            throw new StorageException(\"Failed to write node start marker: \" + ptr, e);\n        }\n    }"
        ],
        [
            "FileWriteAheadLogManager::hasIndex(long)",
            " 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912 -\n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  ",
            "    /**\n     * @param absIdx Absolulte index to check.\n     * @return {@code true} if has this index.\n     */\n    private boolean hasIndex(long absIdx) {\n        String segmentName = FileDescriptor.fileName(absIdx);\n\n        String zipSegmentName = FileDescriptor.fileName(absIdx) + \".zip\";\n\n        boolean inArchive = new File(walArchiveDir, segmentName).exists() ||\n            new File(walArchiveDir, zipSegmentName).exists();\n\n        if (inArchive)\n            return true;\n\n        if (absIdx <= lastArchivedIndex())\n            return false;\n\n        FileWriteHandle cur = currHnd;\n\n        return cur != null && cur.idx >= absIdx;\n    }",
            " 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913 +\n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  ",
            "    /**\n     * @param absIdx Absolulte index to check.\n     * @return {@code true} if has this index.\n     */\n    private boolean hasIndex(long absIdx) {\n        String segmentName = FileDescriptor.fileName(absIdx);\n\n        String zipSegmentName = FileDescriptor.fileName(absIdx) + FilePageStoreManager.ZIP_SUFFIX;\n\n        boolean inArchive = new File(walArchiveDir, segmentName).exists() ||\n            new File(walArchiveDir, zipSegmentName).exists();\n\n        if (inArchive)\n            return true;\n\n        if (absIdx <= lastArchivedIndex())\n            return false;\n\n        FileWriteHandle cur = currHnd;\n\n        return cur != null && cur.idx >= absIdx;\n    }"
        ],
        [
            "IgnitePdsDiskErrorsRecoveringTest::testRecoveringOnNodeStartMarkerWriteFail()",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 -\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "    /**\n     * Test node stopping & recovering on start marker writing fail during activation.\n     *\n     * @throws Exception If test failed.\n     */\n    public void testRecoveringOnNodeStartMarkerWriteFail() throws Exception {\n        // Fail to write node start marker tmp file at the second checkpoint. Pass only initial checkpoint.\n        ioFactory = new FilteringFileIOFactory(\"started.bin\" + GridCacheDatabaseSharedManager.FILE_TMP_SUFFIX, new LimitedSizeFileIOFactory(new RandomAccessFileIOFactory(), 20));\n\n        IgniteEx grid = startGrid(0);\n        grid.cluster().active(true);\n\n        for (int i = 0; i < 1000; i++) {\n            byte payload = (byte) i;\n            byte[] data = new byte[2048];\n            Arrays.fill(data, payload);\n\n            grid.cache(CACHE_NAME).put(i, data);\n        }\n\n        stopAllGrids();\n\n        boolean activationFailed = false;\n        try {\n            grid = startGrid(0);\n            grid.cluster().active(true);\n        }\n        catch (IgniteException e) {\n            log.warning(\"Activation test exception\", e);\n\n            activationFailed = true;\n        }\n\n        Assert.assertTrue(\"Activation must be failed\", activationFailed);\n\n        // Grid should be automatically stopped after checkpoint fail.\n        awaitStop(grid);\n\n        // Grid should be successfully recovered after stopping.\n        ioFactory = null;\n\n        IgniteEx recoveredGrid = startGrid(0);\n        recoveredGrid.cluster().active(true);\n\n        for (int i = 0; i < 1000; i++) {\n            byte payload = (byte) i;\n            byte[] data = new byte[2048];\n            Arrays.fill(data, payload);\n\n            byte[] actualData = (byte[]) recoveredGrid.cache(CACHE_NAME).get(i);\n            Assert.assertArrayEquals(data, actualData);\n        }\n    }",
            " 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  ",
            "    /**\n     * Test node stopping & recovering on start marker writing fail during activation.\n     *\n     * @throws Exception If test failed.\n     */\n    public void testRecoveringOnNodeStartMarkerWriteFail() throws Exception {\n        // Fail to write node start marker tmp file at the second checkpoint. Pass only initial checkpoint.\n        ioFactory = new FilteringFileIOFactory(\"started.bin\" + FilePageStoreManager.TMP_SUFFIX, new LimitedSizeFileIOFactory(new RandomAccessFileIOFactory(), 20));\n\n        IgniteEx grid = startGrid(0);\n        grid.cluster().active(true);\n\n        for (int i = 0; i < 1000; i++) {\n            byte payload = (byte) i;\n            byte[] data = new byte[2048];\n            Arrays.fill(data, payload);\n\n            grid.cache(CACHE_NAME).put(i, data);\n        }\n\n        stopAllGrids();\n\n        boolean activationFailed = false;\n        try {\n            grid = startGrid(0);\n            grid.cluster().active(true);\n        }\n        catch (IgniteException e) {\n            log.warning(\"Activation test exception\", e);\n\n            activationFailed = true;\n        }\n\n        Assert.assertTrue(\"Activation must be failed\", activationFailed);\n\n        // Grid should be automatically stopped after checkpoint fail.\n        awaitStop(grid);\n\n        // Grid should be successfully recovered after stopping.\n        ioFactory = null;\n\n        IgniteEx recoveredGrid = startGrid(0);\n        recoveredGrid.cluster().active(true);\n\n        for (int i = 0; i < 1000; i++) {\n            byte payload = (byte) i;\n            byte[] data = new byte[2048];\n            Arrays.fill(data, payload);\n\n            byte[] actualData = (byte[]) recoveredGrid.cache(CACHE_NAME).get(i);\n            Assert.assertArrayEquals(data, actualData);\n        }\n    }"
        ],
        [
            "FsyncModeFileWriteAheadLogManager::FileArchiver::archiveSegment(long)",
            "1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743 -\n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  ",
            "        /**\n         * Moves WAL segment from work folder to archive folder.\n         * Temp file is used to do movement\n         *\n         * @param absIdx Absolute index to archive.\n         */\n        private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n            long segIdx = absIdx % dsCfg.getWalSegments();\n\n            File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n            String name = FileDescriptor.fileName(absIdx);\n\n            File dstTmpFile = new File(walArchiveDir, name + \".tmp\");\n\n            File dstFile = new File(walArchiveDir, name);\n\n            if (log.isInfoEnabled())\n                log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n                    \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n            try {\n                Files.deleteIfExists(dstTmpFile.toPath());\n\n                Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n                Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n                if (mode == WALMode.FSYNC) {\n                    try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                        f0.force();\n                    }\n                }\n            }\n            catch (IOException e) {\n                throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n                    \"srcFile=\" + origFile.getAbsolutePath() +\n                    \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n            }\n\n            if (log.isInfoEnabled())\n                log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n                    \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n            return new SegmentArchiveResult(absIdx, origFile, dstFile);\n        }",
            "1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744 +\n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  ",
            "        /**\n         * Moves WAL segment from work folder to archive folder.\n         * Temp file is used to do movement\n         *\n         * @param absIdx Absolute index to archive.\n         */\n        private SegmentArchiveResult archiveSegment(long absIdx) throws IgniteCheckedException {\n            long segIdx = absIdx % dsCfg.getWalSegments();\n\n            File origFile = new File(walWorkDir, FileDescriptor.fileName(segIdx));\n\n            String name = FileDescriptor.fileName(absIdx);\n\n            File dstTmpFile = new File(walArchiveDir, name + FilePageStoreManager.TMP_SUFFIX);\n\n            File dstFile = new File(walArchiveDir, name);\n\n            if (log.isInfoEnabled())\n                log.info(\"Starting to copy WAL segment [absIdx=\" + absIdx + \", segIdx=\" + segIdx +\n                    \", origFile=\" + origFile.getAbsolutePath() + \", dstFile=\" + dstFile.getAbsolutePath() + ']');\n\n            try {\n                Files.deleteIfExists(dstTmpFile.toPath());\n\n                Files.copy(origFile.toPath(), dstTmpFile.toPath());\n\n                Files.move(dstTmpFile.toPath(), dstFile.toPath());\n\n                if (mode == WALMode.FSYNC) {\n                    try (FileIO f0 = ioFactory.create(dstFile, CREATE, READ, WRITE)) {\n                        f0.force();\n                    }\n                }\n            }\n            catch (IOException e) {\n                throw new IgniteCheckedException(\"Failed to archive WAL segment [\" +\n                    \"srcFile=\" + origFile.getAbsolutePath() +\n                    \", dstFile=\" + dstTmpFile.getAbsolutePath() + ']', e);\n            }\n\n            if (log.isInfoEnabled())\n                log.info(\"Copied file [src=\" + origFile.getAbsolutePath() +\n                    \", dst=\" + dstFile.getAbsolutePath() + ']');\n\n            return new SegmentArchiveResult(absIdx, origFile, dstFile);\n        }"
        ],
        [
            "FilePageStoreManager::checkAndInitCacheWorkDir(File)",
            " 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599 -\n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  ",
            "    /**\n     * @param cacheWorkDir Cache work directory.\n     */\n    private boolean checkAndInitCacheWorkDir(File cacheWorkDir) throws IgniteCheckedException {\n        boolean dirExisted = false;\n\n        if (!cacheWorkDir.exists()) {\n            boolean res = cacheWorkDir.mkdirs();\n\n            if (!res)\n                throw new IgniteCheckedException(\"Failed to initialize cache working directory \" +\n                    \"(failed to create, make sure the work folder has correct permissions): \" +\n                    cacheWorkDir.getAbsolutePath());\n        }\n        else {\n            if (cacheWorkDir.isFile())\n                throw new IgniteCheckedException(\"Failed to initialize cache working directory \" +\n                    \"(a file with the same name already exists): \" + cacheWorkDir.getAbsolutePath());\n\n            File lockF = new File(cacheWorkDir, IgniteCacheSnapshotManager.SNAPSHOT_RESTORE_STARTED_LOCK_FILENAME);\n\n            Path cacheWorkDirPath = cacheWorkDir.toPath();\n\n            Path tmp = cacheWorkDirPath.getParent().resolve(cacheWorkDir.getName() + \".tmp\");\n\n            if (Files.exists(tmp) && Files.isDirectory(tmp) &&\n                    Files.exists(tmp.resolve(IgniteCacheSnapshotManager.TEMP_FILES_COMPLETENESS_MARKER))) {\n\n                U.warn(log, \"Ignite node crashed during the snapshot restore process \" +\n                    \"(there is a snapshot restore lock file left for cache). But old version of cache was saved. \" +\n                    \"Trying to restore it. Cache - [\" + cacheWorkDir.getAbsolutePath() + ']');\n\n                U.delete(cacheWorkDir);\n\n                try {\n                    Files.move(tmp, cacheWorkDirPath, StandardCopyOption.ATOMIC_MOVE);\n\n                    cacheWorkDirPath.resolve(IgniteCacheSnapshotManager.TEMP_FILES_COMPLETENESS_MARKER).toFile().delete();\n                }\n                catch (IOException e) {\n                    throw new IgniteCheckedException(e);\n                }\n            }\n            else if (lockF.exists()) {\n                U.warn(log, \"Ignite node crashed during the snapshot restore process \" +\n                    \"(there is a snapshot restore lock file left for cache). Will remove both the lock file and \" +\n                    \"incomplete cache directory [cacheDir=\" + cacheWorkDir.getAbsolutePath() + ']');\n\n                boolean deleted = U.delete(cacheWorkDir);\n\n                if (!deleted)\n                    throw new IgniteCheckedException(\"Failed to remove obsolete cache working directory \" +\n                        \"(remove the directory manually and make sure the work folder has correct permissions): \" +\n                        cacheWorkDir.getAbsolutePath());\n\n                cacheWorkDir.mkdirs();\n            }\n            else\n                dirExisted = true;\n\n            if (!cacheWorkDir.exists())\n                throw new IgniteCheckedException(\"Failed to initialize cache working directory \" +\n                    \"(failed to create, make sure the work folder has correct permissions): \" +\n                    cacheWorkDir.getAbsolutePath());\n\n            if (Files.exists(tmp))\n                U.delete(tmp);\n        }\n\n        return dirExisted;\n    }",
            " 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605 +\n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  ",
            "    /**\n     * @param cacheWorkDir Cache work directory.\n     */\n    private boolean checkAndInitCacheWorkDir(File cacheWorkDir) throws IgniteCheckedException {\n        boolean dirExisted = false;\n\n        if (!cacheWorkDir.exists()) {\n            boolean res = cacheWorkDir.mkdirs();\n\n            if (!res)\n                throw new IgniteCheckedException(\"Failed to initialize cache working directory \" +\n                    \"(failed to create, make sure the work folder has correct permissions): \" +\n                    cacheWorkDir.getAbsolutePath());\n        }\n        else {\n            if (cacheWorkDir.isFile())\n                throw new IgniteCheckedException(\"Failed to initialize cache working directory \" +\n                    \"(a file with the same name already exists): \" + cacheWorkDir.getAbsolutePath());\n\n            File lockF = new File(cacheWorkDir, IgniteCacheSnapshotManager.SNAPSHOT_RESTORE_STARTED_LOCK_FILENAME);\n\n            Path cacheWorkDirPath = cacheWorkDir.toPath();\n\n            Path tmp = cacheWorkDirPath.getParent().resolve(cacheWorkDir.getName() + TMP_SUFFIX);\n\n            if (Files.exists(tmp) && Files.isDirectory(tmp) &&\n                    Files.exists(tmp.resolve(IgniteCacheSnapshotManager.TEMP_FILES_COMPLETENESS_MARKER))) {\n\n                U.warn(log, \"Ignite node crashed during the snapshot restore process \" +\n                    \"(there is a snapshot restore lock file left for cache). But old version of cache was saved. \" +\n                    \"Trying to restore it. Cache - [\" + cacheWorkDir.getAbsolutePath() + ']');\n\n                U.delete(cacheWorkDir);\n\n                try {\n                    Files.move(tmp, cacheWorkDirPath, StandardCopyOption.ATOMIC_MOVE);\n\n                    cacheWorkDirPath.resolve(IgniteCacheSnapshotManager.TEMP_FILES_COMPLETENESS_MARKER).toFile().delete();\n                }\n                catch (IOException e) {\n                    throw new IgniteCheckedException(e);\n                }\n            }\n            else if (lockF.exists()) {\n                U.warn(log, \"Ignite node crashed during the snapshot restore process \" +\n                    \"(there is a snapshot restore lock file left for cache). Will remove both the lock file and \" +\n                    \"incomplete cache directory [cacheDir=\" + cacheWorkDir.getAbsolutePath() + ']');\n\n                boolean deleted = U.delete(cacheWorkDir);\n\n                if (!deleted)\n                    throw new IgniteCheckedException(\"Failed to remove obsolete cache working directory \" +\n                        \"(remove the directory manually and make sure the work folder has correct permissions): \" +\n                        cacheWorkDir.getAbsolutePath());\n\n                cacheWorkDir.mkdirs();\n            }\n            else\n                dirExisted = true;\n\n            if (!cacheWorkDir.exists())\n                throw new IgniteCheckedException(\"Failed to initialize cache working directory \" +\n                    \"(failed to create, make sure the work folder has correct permissions): \" +\n                    cacheWorkDir.getAbsolutePath());\n\n            if (Files.exists(tmp))\n                U.delete(tmp);\n        }\n\n        return dirExisted;\n    }"
        ],
        [
            "FsyncModeFileWriteAheadLogManager::FileDecompressor::body()",
            "2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076 -\n2077 -\n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  ",
            "        /** {@inheritDoc} */\n        @Override protected void body() {\n            Throwable err = null;\n\n            try {\n                while (!isCancelled()) {\n                    long segmentToDecompress = -1L;\n\n                    try {\n                        segmentToDecompress = segmentsQueue.take();\n\n                        if (isCancelled())\n                            break;\n\n                        File zip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".zip\");\n                        File unzipTmp = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".tmp\");\n                        File unzip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress));\n\n                        try (ZipInputStream zis = new ZipInputStream(new BufferedInputStream(new FileInputStream(zip)));\n                             FileIO io = ioFactory.create(unzipTmp)) {\n                            zis.getNextEntry();\n\n                            while (io.writeFully(arr, 0, zis.read(arr)) > 0)\n                                ;\n                        }\n\n                        try {\n                            Files.move(unzipTmp.toPath(), unzip.toPath());\n                        }\n                        catch (FileAlreadyExistsException e) {\n                            U.error(log, \"Can't rename temporary unzipped segment: raw segment is already present \" +\n                                \"[tmp=\" + unzipTmp + \", raw=\" + unzip + ']', e);\n\n                            if (!unzipTmp.delete())\n                                U.error(log, \"Can't delete temporary unzipped segment [tmp=\" + unzipTmp + ']');\n                        }\n\n                        synchronized (this) {\n                            decompressionFutures.remove(segmentToDecompress).onDone();\n                        }\n                    }\n                    catch (IOException ex) {\n                        if (!isCancelled && segmentToDecompress != -1L) {\n                            IgniteCheckedException e = new IgniteCheckedException(\"Error during WAL segment \" +\n                                \"decompression [segmentIdx=\" + segmentToDecompress + ']', ex);\n\n                            synchronized (this) {\n                                decompressionFutures.remove(segmentToDecompress).onDone(e);\n                            }\n                        }\n                    }\n                }\n            }\n            catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n\n                if (!isCancelled)\n                    err = e;\n            }\n            catch (Throwable t) {\n                err = t;\n            }\n            finally {\n                if (err == null && !isCancelled)\n                    err = new IllegalStateException(\"Worker \" + name() + \" is terminated unexpectedly\");\n\n                if (err instanceof OutOfMemoryError)\n                    cctx.kernalContext().failure().process(new FailureContext(CRITICAL_ERROR, err));\n                else if (err != null)\n                    cctx.kernalContext().failure().process(new FailureContext(SYSTEM_WORKER_TERMINATION, err));\n            }\n        }",
            "2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079 +\n2080 +\n2081 +\n2082 +\n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  ",
            "        /** {@inheritDoc} */\n        @Override protected void body() {\n            Throwable err = null;\n\n            try {\n                while (!isCancelled()) {\n                    long segmentToDecompress = -1L;\n\n                    try {\n                        segmentToDecompress = segmentsQueue.take();\n\n                        if (isCancelled())\n                            break;\n\n                        File zip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress)\n                            + FilePageStoreManager.ZIP_SUFFIX);\n                        File unzipTmp = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress)\n                            + FilePageStoreManager.TMP_SUFFIX);\n                        File unzip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress));\n\n                        try (ZipInputStream zis = new ZipInputStream(new BufferedInputStream(new FileInputStream(zip)));\n                             FileIO io = ioFactory.create(unzipTmp)) {\n                            zis.getNextEntry();\n\n                            while (io.writeFully(arr, 0, zis.read(arr)) > 0)\n                                ;\n                        }\n\n                        try {\n                            Files.move(unzipTmp.toPath(), unzip.toPath());\n                        }\n                        catch (FileAlreadyExistsException e) {\n                            U.error(log, \"Can't rename temporary unzipped segment: raw segment is already present \" +\n                                \"[tmp=\" + unzipTmp + \", raw=\" + unzip + ']', e);\n\n                            if (!unzipTmp.delete())\n                                U.error(log, \"Can't delete temporary unzipped segment [tmp=\" + unzipTmp + ']');\n                        }\n\n                        synchronized (this) {\n                            decompressionFutures.remove(segmentToDecompress).onDone();\n                        }\n                    }\n                    catch (IOException ex) {\n                        if (!isCancelled && segmentToDecompress != -1L) {\n                            IgniteCheckedException e = new IgniteCheckedException(\"Error during WAL segment \" +\n                                \"decompression [segmentIdx=\" + segmentToDecompress + ']', ex);\n\n                            synchronized (this) {\n                                decompressionFutures.remove(segmentToDecompress).onDone(e);\n                            }\n                        }\n                    }\n                }\n            }\n            catch (InterruptedException e) {\n                Thread.currentThread().interrupt();\n\n                if (!isCancelled)\n                    err = e;\n            }\n            catch (Throwable t) {\n                err = t;\n            }\n            finally {\n                if (err == null && !isCancelled)\n                    err = new IllegalStateException(\"Worker \" + name() + \" is terminated unexpectedly\");\n\n                if (err instanceof OutOfMemoryError)\n                    cctx.kernalContext().failure().process(new FailureContext(CRITICAL_ERROR, err));\n                else if (err != null)\n                    cctx.kernalContext().failure().process(new FailureContext(SYSTEM_WORKER_TERMINATION, err));\n            }\n        }"
        ]
    ],
    "d6972e95367d93c1015b6bab8dd4dc78ae2fc1e6": [
        [
            "GridCachePartitionExchangeManager::dumpDebugInfo(GridDhtPartitionsExchangeFuture)",
            "1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405 -\n1406  \n1407 -\n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  ",
            "    /**\n     * @param exchFut Optional current exchange future.\n     * @throws Exception If failed.\n     */\n    public void dumpDebugInfo(@Nullable GridDhtPartitionsExchangeFuture exchFut) throws Exception {\n        AffinityTopologyVersion exchTopVer = exchFut != null ? exchFut.topologyVersion() : null;\n\n        U.warn(diagnosticLog, \"Ready affinity version: \" + readyTopVer.get());\n\n        U.warn(diagnosticLog, \"Last exchange future: \" + lastInitializedFut);\n\n        exchWorker.dumpExchangeDebugInfo();\n\n        if (!readyFuts.isEmpty()) {\n            U.warn(diagnosticLog, \"Pending affinity ready futures:\");\n\n            for (AffinityReadyFuture fut : readyFuts.values())\n                U.warn(diagnosticLog, \">>> \" + fut);\n        }\n\n        IgniteDiagnosticPrepareContext diagCtx = cctx.kernalContext().cluster().diagnosticEnabled() ?\n            new IgniteDiagnosticPrepareContext(cctx.localNodeId()) : null;\n\n        if (diagCtx != null && exchFut != null)\n            exchFut.addDiagnosticRequest(diagCtx);\n\n        ExchangeFutureSet exchFuts = this.exchFuts;\n\n        if (exchFuts != null) {\n            U.warn(diagnosticLog, \"Last 10 exchange futures (total: \" + exchFuts.size() + \"):\");\n\n            int cnt = 0;\n\n            for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {\n                U.warn(diagnosticLog, \">>> \" + fut.shortInfo());\n\n                if (++cnt == 10)\n                    break;\n            }\n        }\n\n        dumpPendingObjects(exchTopVer, diagCtx);\n\n        for (CacheGroupContext grp : cctx.cache().cacheGroups())\n            grp.preloader().dumpDebugInfo();\n\n        cctx.affinity().dumpDebugInfo();\n\n        cctx.io().dumpPendingMessages();\n\n        if (IgniteSystemProperties.getBoolean(IGNITE_IO_DUMP_ON_TIMEOUT, false))\n            cctx.gridIO().dumpStats();\n\n        if (IgniteSystemProperties.getBoolean(IGNITE_THREAD_DUMP_ON_EXCHANGE_TIMEOUT, false))\n            U.dumpThreads(diagnosticLog);\n\n        if (diagCtx != null)\n            diagCtx.send(cctx.kernalContext(), null);\n    }",
            "1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406 +\n1407 +\n1408 +\n1409  \n1410 +\n1411  \n1412 +\n1413 +\n1414 +\n1415 +\n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  ",
            "    /**\n     * @param exchFut Optional current exchange future.\n     * @throws Exception If failed.\n     */\n    public void dumpDebugInfo(@Nullable GridDhtPartitionsExchangeFuture exchFut) throws Exception {\n        AffinityTopologyVersion exchTopVer = exchFut != null ? exchFut.topologyVersion() : null;\n\n        U.warn(diagnosticLog, \"Ready affinity version: \" + readyTopVer.get());\n\n        U.warn(diagnosticLog, \"Last exchange future: \" + lastInitializedFut);\n\n        exchWorker.dumpExchangeDebugInfo();\n\n        if (!readyFuts.isEmpty()) {\n            U.warn(diagnosticLog, \"First 5 pending affinity ready futures [total=\" + readyFuts.size() + ']');\n\n            int cnt = 0;\n\n            for (AffinityReadyFuture fut : readyFuts.values()) {\n                U.warn(diagnosticLog, \">>> \" + fut);\n\n                if (++cnt == 5)\n                    break;\n            }\n        }\n\n        IgniteDiagnosticPrepareContext diagCtx = cctx.kernalContext().cluster().diagnosticEnabled() ?\n            new IgniteDiagnosticPrepareContext(cctx.localNodeId()) : null;\n\n        if (diagCtx != null && exchFut != null)\n            exchFut.addDiagnosticRequest(diagCtx);\n\n        ExchangeFutureSet exchFuts = this.exchFuts;\n\n        if (exchFuts != null) {\n            U.warn(diagnosticLog, \"Last 10 exchange futures (total: \" + exchFuts.size() + \"):\");\n\n            int cnt = 0;\n\n            for (GridDhtPartitionsExchangeFuture fut : exchFuts.values()) {\n                U.warn(diagnosticLog, \">>> \" + fut.shortInfo());\n\n                if (++cnt == 10)\n                    break;\n            }\n        }\n\n        dumpPendingObjects(exchTopVer, diagCtx);\n\n        for (CacheGroupContext grp : cctx.cache().cacheGroups())\n            grp.preloader().dumpDebugInfo();\n\n        cctx.affinity().dumpDebugInfo();\n\n        cctx.io().dumpPendingMessages();\n\n        if (IgniteSystemProperties.getBoolean(IGNITE_IO_DUMP_ON_TIMEOUT, false))\n            cctx.gridIO().dumpStats();\n\n        if (IgniteSystemProperties.getBoolean(IGNITE_THREAD_DUMP_ON_EXCHANGE_TIMEOUT, false))\n            U.dumpThreads(diagnosticLog);\n\n        if (diagCtx != null)\n            diagCtx.send(cctx.kernalContext(), null);\n    }"
        ],
        [
            "GridCachePartitionExchangeManager::ExchangeWorker::dumpExchangeDebugInfo()",
            "1774  \n1775  \n1776  \n1777  \n1778 -\n1779  \n1780 -\n1781 -\n1782  \n1783  \n1784  ",
            "        /**\n         * Dump debug info.\n         */\n        void dumpExchangeDebugInfo() {\n            U.warn(log, \"Pending exchange futures:\");\n\n            for (CachePartitionExchangeWorkerTask task: futQ) {\n                if (isExchangeTask(task))\n                    U.warn(log, \">>> \" + ((GridDhtPartitionsExchangeFuture)task).shortInfo());\n            }\n        }",
            "1781  \n1782  \n1783  \n1784  \n1785 +\n1786  \n1787 +\n1788 +\n1789 +\n1790 +\n1791  \n1792 +\n1793 +\n1794 +\n1795 +\n1796  \n1797  ",
            "        /**\n         * Dump debug info.\n         */\n        void dumpExchangeDebugInfo() {\n            U.warn(log, \"First 10 pending exchange futures [total=\" + futQ.size() + ']');\n\n            int cnt = 0;\n\n            for (CachePartitionExchangeWorkerTask task : futQ) {\n                if (isExchangeTask(task)) {\n                    U.warn(log, \">>> \" + ((GridDhtPartitionsExchangeFuture)task).shortInfo());\n\n                    if (++cnt == 10)\n                        break;\n                }\n            }\n        }"
        ],
        [
            "GridAffinityAssignmentCache::dumpDebugInfo()",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437 -\n 438  \n 439 -\n 440  \n 441  \n 442  ",
            "    /**\n     * Dumps debug information.\n     */\n    public void dumpDebugInfo() {\n        if (!readyFuts.isEmpty()) {\n            U.warn(log, \"Pending affinity ready futures [grp=\" + cacheOrGrpName + \", lastVer=\" + lastVersion() + \"]:\");\n\n            for (AffinityReadyFuture fut : readyFuts.values())\n                U.warn(log, \">>> \" + fut);\n        }\n    }",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437 +\n 438 +\n 439 +\n 440  \n 441 +\n 442 +\n 443 +\n 444  \n 445 +\n 446 +\n 447 +\n 448 +\n 449  \n 450  ",
            "    /**\n     * Dumps debug information.\n     */\n    public void dumpDebugInfo() {\n        if (!readyFuts.isEmpty()) {\n            U.warn(log, \"First 3 pending affinity ready futures [grp=\" + cacheOrGrpName +\n                \", total=\" + readyFuts.size() +\n                \", lastVer=\" + lastVersion() + \"]:\");\n\n            int cnt = 0;\n\n            for (AffinityReadyFuture fut : readyFuts.values()) {\n                U.warn(log, \">>> \" + fut);\n\n                if (++cnt == 3)\n                    break;\n            }\n        }\n    }"
        ]
    ],
    "91d77a79c4805e501d8637fc1263866d5e914fba": [
        [
            "GridDhtPartitionTopologyImpl::detectLostPartitions(AffinityTopologyVersion,DiscoveryEvent)",
            "2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  ",
            "    /** {@inheritDoc} */\n    @Override public boolean detectLostPartitions(AffinityTopologyVersion resTopVer, DiscoveryEvent discoEvt) {\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (node2part == null)\n                    return false;\n\n                int parts = grp.affinity().partitions();\n\n                Set<Integer> lost = new HashSet<>(parts);\n\n                for (int p = 0; p < parts; p++)\n                    lost.add(p);\n\n                for (GridDhtPartitionMap partMap : node2part.values()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : partMap.entrySet()) {\n                        if (e.getValue() == OWNING) {\n                            lost.remove(e.getKey());\n\n                            if (lost.isEmpty())\n                                break;\n                        }\n                    }\n                }\n\n                boolean changed = false;\n\n                if (!F.isEmpty(lost)) {\n                    PartitionLossPolicy plc = grp.config().getPartitionLossPolicy();\n\n                    assert plc != null;\n\n                    Set<Integer> recentlyLost = new HashSet<>();\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> leftEntry : leftNode2Part.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> entry : leftEntry.getValue().entrySet()) {\n                            if (entry.getValue() == OWNING)\n                                recentlyLost.add(entry.getKey());\n                        }\n                    }\n\n                    // Update partition state on all nodes.\n                    for (Integer part : lost) {\n                        long updSeq = updateSeq.incrementAndGet();\n\n                        GridDhtLocalPartition locPart = localPartition(part, resTopVer, false, true);\n\n                        if (locPart != null) {\n                            if (locPart.state() == LOST)\n                                continue;\n\n                            boolean marked = plc == PartitionLossPolicy.IGNORE ? locPart.own() : locPart.markLost();\n\n                            if (marked)\n                                updateLocal(locPart.id(), locPart.state(), updSeq, resTopVer);\n\n                            changed |= marked;\n                        }\n                        // Update map for remote node.\n                        else if (plc != PartitionLossPolicy.IGNORE) {\n                            for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) {\n                                if (e.getKey().equals(ctx.localNodeId()))\n                                    continue;\n\n                                if (e.getValue().get(part) != EVICTED)\n                                    e.getValue().put(part, LOST);\n                            }\n                        }\n\n                        if (recentlyLost.contains(part) && grp.eventRecordable(EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST)) {\n                            grp.addRebalanceEvent(part,\n                                EVT_CACHE_REBALANCE_PART_DATA_LOST,\n                                discoEvt.eventNode(),\n                                discoEvt.type(),\n                                discoEvt.timestamp());\n                        }\n                    }\n\n                    if (plc != PartitionLossPolicy.IGNORE)\n                        grp.needsRecovery(true);\n                }\n\n                leftNode2Part.clear();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            "2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054 +\n2055 +\n2056 +\n2057 +\n2058 +\n2059 +\n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  ",
            "    /** {@inheritDoc} */\n    @Override public boolean detectLostPartitions(AffinityTopologyVersion resTopVer, DiscoveryEvent discoEvt) {\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (node2part == null)\n                    return false;\n\n                int parts = grp.affinity().partitions();\n\n                Set<Integer> lost = new HashSet<>(parts);\n\n                for (int p = 0; p < parts; p++)\n                    lost.add(p);\n\n                for (GridDhtPartitionMap partMap : node2part.values()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : partMap.entrySet()) {\n                        if (e.getValue() == OWNING) {\n                            lost.remove(e.getKey());\n\n                            if (lost.isEmpty())\n                                break;\n                        }\n                    }\n                }\n\n                boolean changed = false;\n\n                if (!F.isEmpty(lost)) {\n                    PartitionLossPolicy plc = grp.config().getPartitionLossPolicy();\n\n                    assert plc != null;\n\n                    Set<Integer> recentlyLost = new HashSet<>();\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> leftEntry : leftNode2Part.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> entry : leftEntry.getValue().entrySet()) {\n                            if (entry.getValue() == OWNING)\n                                recentlyLost.add(entry.getKey());\n                        }\n                    }\n\n                    if (!recentlyLost.isEmpty()) {\n                        U.warn(log, \"Detected lost partitions [grp=\" + grp.cacheOrGroupName()\n                            + \", parts=\" + S.compact(recentlyLost)\n                            + \", plc=\" + plc + \"]\");\n                    }\n\n                    // Update partition state on all nodes.\n                    for (Integer part : lost) {\n                        long updSeq = updateSeq.incrementAndGet();\n\n                        GridDhtLocalPartition locPart = localPartition(part, resTopVer, false, true);\n\n                        if (locPart != null) {\n                            if (locPart.state() == LOST)\n                                continue;\n\n                            boolean marked = plc == PartitionLossPolicy.IGNORE ? locPart.own() : locPart.markLost();\n\n                            if (marked)\n                                updateLocal(locPart.id(), locPart.state(), updSeq, resTopVer);\n\n                            changed |= marked;\n                        }\n                        // Update map for remote node.\n                        else if (plc != PartitionLossPolicy.IGNORE) {\n                            for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) {\n                                if (e.getKey().equals(ctx.localNodeId()))\n                                    continue;\n\n                                if (e.getValue().get(part) != EVICTED)\n                                    e.getValue().put(part, LOST);\n                            }\n                        }\n\n                        if (recentlyLost.contains(part) && grp.eventRecordable(EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST)) {\n                            grp.addRebalanceEvent(part,\n                                EVT_CACHE_REBALANCE_PART_DATA_LOST,\n                                discoEvt.eventNode(),\n                                discoEvt.type(),\n                                discoEvt.timestamp());\n                        }\n                    }\n\n                    if (plc != PartitionLossPolicy.IGNORE)\n                        grp.needsRecovery(true);\n                }\n\n                leftNode2Part.clear();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ]
    ],
    "4924ed430f0adfd9385414d7e341393a00577d1a": [
        [
            "TxRollbackAsyncTest::testRollbackOnTopologyLockPessimistic()",
            " 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903 -\n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  ",
            "    /**\n     *\n     */\n    public void testRollbackOnTopologyLockPessimistic() throws Exception {\n        final Ignite client = startClient();\n\n        Ignite crd = grid(0);\n\n        List<Integer> keys = primaryKeys(grid(1).cache(CACHE_NAME), 1);\n\n        assertTrue(crd.cluster().localNode().order() == 1);\n\n        CountDownLatch txLatch = new CountDownLatch(1);\n        CountDownLatch tx2Latch = new CountDownLatch(1);\n        CountDownLatch commitLatch = new CountDownLatch(1);\n\n        // Start tx holding topology.\n        IgniteInternalFuture txFut = runAsync(new Runnable() {\n            @Override public void run() {\n                List<Integer> keys = primaryKeys(grid(0).cache(CACHE_NAME), 1);\n\n                try (Transaction tx = client.transactions().txStart()) {\n                    client.cache(CACHE_NAME).put(keys.get(0), 0);\n\n                    txLatch.countDown();\n\n                    U.awaitQuiet(commitLatch);\n\n                    tx.commit();\n\n                    fail();\n                }\n                catch (Exception e) {\n                    // Expected.\n                }\n            }\n        });\n\n        U.awaitQuiet(txLatch);\n\n        crd.events().localListen(new IgnitePredicate<Event>() {\n            @Override public boolean apply(Event evt) {\n                runAsync(new Runnable() {\n                    @Override public void run() {\n                        try(Transaction tx = crd.transactions().withLabel(\"testLbl\").txStart()) {\n                            // Wait for node start.\n                            waitForCondition(new GridAbsPredicate() {\n                                @Override public boolean apply() {\n                                    return crd.cluster().topologyVersion() != GRID_CNT +\n                                        /** client node */ 1  + /** stop server node */ 1 + /** start server node */ 1;\n                                }\n                            }, 10_000);\n\n                            tx2Latch.countDown();\n\n                            crd.cache(CACHE_NAME).put(keys.get(0), 0);\n\n                            tx.commit();\n\n                            fail();\n                        }\n                        catch (Exception e) {\n                            // Expected.\n                        }\n                    }\n                });\n\n                return false;\n            }\n        }, EventType.EVT_NODE_FAILED, EventType.EVT_NODE_LEFT);\n\n        IgniteInternalFuture restartFut = runAsync(new Runnable() {\n            @Override public void run() {\n                stopGrid(2);\n\n                try {\n                    startGrid(2);\n                }\n                catch (Exception e) {\n                    fail();\n                }\n            }\n        });\n\n        U.awaitQuiet(tx2Latch);\n\n        // Rollback tx using kill task.\n        VisorTxTaskArg arg =\n            new VisorTxTaskArg(VisorTxOperation.KILL, null, null, null, null, null, null, null, null, null);\n\n        Map<ClusterNode, VisorTxTaskResult> res = client.compute(client.cluster().forPredicate(F.alwaysTrue())).\n            execute(new VisorTxTask(), new VisorTaskArgument<>(client.cluster().localNode().id(), arg, false));\n\n        int expCnt = 0;\n\n        for (Map.Entry<ClusterNode, VisorTxTaskResult> entry : res.entrySet()) {\n            if (entry.getValue().getInfos().isEmpty())\n                continue;\n\n            for (VisorTxInfo info : entry.getValue().getInfos()) {\n                log.info(info.toUserString());\n\n                expCnt++;\n            }\n        }\n\n        assertEquals(\"Expecting 2 transactions\", 2, expCnt);\n\n        commitLatch.countDown();\n\n        txFut.get();\n        restartFut.get();\n\n        checkFutures();\n    }",
            " 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904 +\n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935 +\n 936 +\n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  ",
            "    /**\n     *\n     */\n    public void testRollbackOnTopologyLockPessimistic() throws Exception {\n        final Ignite client = startClient();\n\n        Ignite crd = grid(0);\n\n        List<Integer> keys = primaryKeys(grid(1).cache(CACHE_NAME), 1);\n\n        assertTrue(crd.cluster().localNode().order() == 1);\n\n        CountDownLatch txLatch = new CountDownLatch(1);\n        CountDownLatch tx2Latch = new CountDownLatch(1);\n        CountDownLatch commitLatch = new CountDownLatch(1);\n\n        // Start tx holding topology.\n        IgniteInternalFuture txFut = runAsync(new Runnable() {\n            @Override public void run() {\n                List<Integer> keys = primaryKeys(grid(0).cache(CACHE_NAME), 1);\n\n                try (Transaction tx = client.transactions().txStart()) {\n                    client.cache(CACHE_NAME).put(keys.get(0), 0);\n\n                    txLatch.countDown();\n\n                    assertTrue(U.await(commitLatch, 10, TimeUnit.SECONDS));\n\n                    tx.commit();\n\n                    fail();\n                }\n                catch (Exception e) {\n                    // Expected.\n                }\n            }\n        });\n\n        U.awaitQuiet(txLatch);\n\n        crd.events().localListen(new IgnitePredicate<Event>() {\n            @Override public boolean apply(Event evt) {\n                runAsync(new Runnable() {\n                    @Override public void run() {\n                        try(Transaction tx = crd.transactions().withLabel(\"testLbl\").txStart()) {\n                            // Wait for node start.\n                            waitForCondition(new GridAbsPredicate() {\n                                @Override public boolean apply() {\n                                    return crd.cluster().topologyVersion() != GRID_CNT +\n                                        /** client node */ 1  + /** stop server node */ 1 + /** start server node */ 1;\n                                }\n                            }, 10_000);\n\n                            tx2Latch.countDown();\n\n                            crd.cache(CACHE_NAME).put(keys.get(0), 0);\n\n                            assertTrue(U.await(commitLatch, 10, TimeUnit.SECONDS));\n\n                            tx.commit();\n\n                            fail();\n                        }\n                        catch (Exception e) {\n                            // Expected.\n                        }\n                    }\n                });\n\n                return false;\n            }\n        }, EventType.EVT_NODE_FAILED, EventType.EVT_NODE_LEFT);\n\n        IgniteInternalFuture restartFut = runAsync(new Runnable() {\n            @Override public void run() {\n                stopGrid(2);\n\n                try {\n                    startGrid(2);\n                }\n                catch (Exception e) {\n                    fail();\n                }\n            }\n        });\n\n        U.awaitQuiet(tx2Latch);\n\n        // Rollback tx using kill task.\n        VisorTxTaskArg arg =\n            new VisorTxTaskArg(VisorTxOperation.KILL, null, null, null, null, null, null, null, null, null);\n\n        Map<ClusterNode, VisorTxTaskResult> res = client.compute(client.cluster().forPredicate(F.alwaysTrue())).\n            execute(new VisorTxTask(), new VisorTaskArgument<>(client.cluster().localNode().id(), arg, false));\n\n        int expCnt = 0;\n\n        for (Map.Entry<ClusterNode, VisorTxTaskResult> entry : res.entrySet()) {\n            if (entry.getValue().getInfos().isEmpty())\n                continue;\n\n            for (VisorTxInfo info : entry.getValue().getInfos()) {\n                log.info(info.toUserString());\n\n                expCnt++;\n            }\n        }\n\n        assertEquals(\"Expecting 2 transactions\", 2, expCnt);\n\n        commitLatch.countDown();\n\n        txFut.get();\n        restartFut.get();\n\n        checkFutures();\n    }"
        ]
    ],
    "1039c8a7921858c7d577816ef1994de969d18e74": [
        [
            "TxTopologyVersionFuture::init()",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "    /** */\n    private void init() {\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = fut.validateCache(cctx, false, false, null, null);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            onDone(topVer);\n\n            topLocked = true;\n\n            return;\n        }\n\n        acquireTopologyVersion();\n    }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 +\n  74 +\n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81 +\n  82 +\n  83 +\n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    /** */\n    private void init() {\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = null;\n\n                    // Before cache validation, make sure that this topology future is already completed.\n                    try {\n                        fut.get();\n                    }\n                    catch (IgniteCheckedException e) {\n                        err = fut.error();\n                    }\n\n                    if (err == null)\n                        err = fut.validateCache(cctx, false, false, null, null);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            onDone(topVer);\n\n            topLocked = true;\n\n            return;\n        }\n\n        acquireTopologyVersion();\n    }"
        ],
        [
            "GridDhtAtomicCache::updateAllAsyncInternal0(ClusterNode,GridNearAtomicAbstractUpdateRequest,UpdateReplyClosure)",
            "1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725 -\n1726 -\n1727 -\n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  ",
            "    /**\n     * Executes local update after preloader fetched values.\n     *\n     * @param node Node.\n     * @param req Update request.\n     * @param completionCb Completion callback.\n     */\n    private void updateAllAsyncInternal0(\n        ClusterNode node,\n        GridNearAtomicAbstractUpdateRequest req,\n        UpdateReplyClosure completionCb\n    ) {\n        GridNearAtomicUpdateResponse res = new GridNearAtomicUpdateResponse(ctx.cacheId(),\n            node.id(),\n            req.futureId(),\n            req.partition(),\n            false,\n            ctx.deploymentEnabled());\n\n        assert !req.returnValue() || (req.operation() == TRANSFORM || req.size() == 1);\n\n        GridDhtAtomicAbstractUpdateFuture dhtFut = null;\n\n        IgniteCacheExpiryPolicy expiry = null;\n\n        ctx.shared().database().checkpointReadLock();\n\n        try {\n            ctx.shared().database().ensureFreeSpace(ctx.dataRegion());\n\n            // If batch store update is enabled, we need to lock all entries.\n            // First, need to acquire locks on cache entries, then check filter.\n            List<GridDhtCacheEntry> locked = lockEntries(req, req.topologyVersion());;\n\n            Collection<IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion>> deleted = null;\n\n            DhtAtomicUpdateResult  updDhtRes = new DhtAtomicUpdateResult();\n\n            try {\n                while (true) {\n                    try {\n                        GridDhtPartitionTopology top = topology();\n\n                        top.readLock();\n\n                        try {\n                            if (top.stopping()) {\n                                res.addFailedKeys(req.keys(), new CacheStoppedException(name()));\n\n                                completionCb.apply(req, res);\n\n                                return;\n                            }\n\n                            boolean remap = false;\n\n                            // Do not check topology version if topology was locked on near node by\n                            // external transaction or explicit lock.\n                            if (!req.topologyLocked()) {\n                                // Can not wait for topology future since it will break\n                                // GridNearAtomicCheckUpdateRequest processing.\n                                remap = !top.topologyVersionFuture().isDone() ||\n                                    needRemap(req.topologyVersion(), top.readyTopologyVersion(), req.keys());\n                            }\n\n                            if (!remap) {\n                                boolean validateCache = needCacheValidation(node);\n\n                                if (validateCache) {\n                                    GridDhtTopologyFuture topFut = top.topologyVersionFuture();\n\n                                    assert topFut.isDone() : topFut;\n\n                                    Throwable err = topFut.validateCache(ctx, req.recovery(), false, null, null);\n\n                                    if (err != null) {\n                                        IgniteCheckedException e = new IgniteCheckedException(err);\n\n                                        res.error(e);\n\n                                        completionCb.apply(req, res);\n\n                                        return;\n                                    }\n                                }\n\n                                update(node, locked, req, res, updDhtRes);\n\n                                dhtFut = updDhtRes.dhtFuture();\n                                deleted = updDhtRes.deleted();\n                                expiry = updDhtRes.expiryPolicy();\n                            }\n                            else\n                                // Should remap all keys.\n                                res.remapTopologyVersion(top.lastTopologyChangeVersion());\n                        }\n                        finally {\n                            top.readUnlock();\n                        }\n\n                        // This call will convert entry processor invocation results to cache object instances.\n                        // Must be done outside topology read lock to avoid deadlocks.\n                        if (res.returnValue() != null)\n                            res.returnValue().marshalResult(ctx);\n\n                        break;\n                    }\n                    catch (UnregisteredClassException ex) {\n                        IgniteCacheObjectProcessor cacheObjProc = ctx.cacheObjects();\n\n                        assert cacheObjProc instanceof CacheObjectBinaryProcessorImpl;\n\n                        ((CacheObjectBinaryProcessorImpl)cacheObjProc)\n                            .binaryContext().descriptorForClass(ex.cls(), false, false);\n                    }\n                    catch (UnregisteredBinaryTypeException ex) {\n                        IgniteCacheObjectProcessor cacheObjProc = ctx.cacheObjects();\n\n                        assert cacheObjProc instanceof CacheObjectBinaryProcessorImpl;\n\n                        ((CacheObjectBinaryProcessorImpl)cacheObjProc)\n                            .binaryContext().updateMetadata(ex.typeId(), ex.binaryMetadata(), false);\n                    }\n                }\n            }\n            catch (GridCacheEntryRemovedException e) {\n                assert false : \"Entry should not become obsolete while holding lock.\";\n\n                e.printStackTrace();\n            }\n            finally {\n                if (locked != null)\n                    unlockEntries(locked, req.topologyVersion());\n\n                // Enqueue if necessary after locks release.\n                if (deleted != null) {\n                    assert !deleted.isEmpty();\n                    assert ctx.deferredDelete() : this;\n\n                    for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted)\n                        ctx.onDeferredDelete(e.get1(), e.get2());\n                }\n\n                // TODO handle failure: probably drop the node from topology\n                // TODO fire events only after successful fsync\n                if (ctx.shared().wal() != null)\n                    ctx.shared().wal().flush(null, false);\n            }\n        }\n        catch (GridDhtInvalidPartitionException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Caught invalid partition exception for cache entry (will remap update request): \" + req);\n\n            res.remapTopologyVersion(ctx.topology().lastTopologyChangeVersion());\n        }\n        catch (Throwable e) {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            U.error(log, \"Unexpected exception during cache update\", e);\n\n            res.addFailedKeys(req.keys(), e);\n\n            completionCb.apply(req, res);\n\n            if (e instanceof Error)\n                throw (Error)e;\n\n            return;\n        }\n        finally {\n            ctx.shared().database().checkpointReadUnlock();\n        }\n\n        if (res.remapTopologyVersion() != null) {\n            assert dhtFut == null;\n\n            completionCb.apply(req, res);\n        }\n        else {\n            if (dhtFut != null)\n                dhtFut.map(node, res.returnValue(), res, completionCb);\n        }\n\n        if (req.writeSynchronizationMode() != FULL_ASYNC)\n            req.cleanup(!node.isLocal());\n\n        sendTtlUpdateRequest(expiry);\n    }",
            "1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726 +\n1727 +\n1728 +\n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789 +\n1790 +\n1791 +\n1792 +\n1793 +\n1794 +\n1795 +\n1796 +\n1797 +\n1798 +\n1799 +\n1800 +\n1801 +\n1802 +\n1803 +\n1804 +\n1805 +\n1806 +\n1807 +\n1808 +\n1809 +\n1810 +\n1811 +\n1812 +\n1813 +\n1814 +\n1815 +\n1816 +\n1817 +\n1818 +\n1819 +\n1820 +\n1821 +\n1822 +\n1823 +\n1824 +\n1825 +\n1826 +\n1827 +\n1828 +\n1829 +\n1830 +\n1831 +\n1832 +\n1833 +\n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  ",
            "    /**\n     * Executes local update after preloader fetched values.\n     *\n     * @param node Node.\n     * @param req Update request.\n     * @param completionCb Completion callback.\n     */\n    private void updateAllAsyncInternal0(\n        final ClusterNode node,\n        final GridNearAtomicAbstractUpdateRequest req,\n        final UpdateReplyClosure completionCb\n    ) {\n        GridNearAtomicUpdateResponse res = new GridNearAtomicUpdateResponse(ctx.cacheId(),\n            node.id(),\n            req.futureId(),\n            req.partition(),\n            false,\n            ctx.deploymentEnabled());\n\n        assert !req.returnValue() || (req.operation() == TRANSFORM || req.size() == 1);\n\n        GridDhtAtomicAbstractUpdateFuture dhtFut = null;\n\n        IgniteCacheExpiryPolicy expiry = null;\n\n        ctx.shared().database().checkpointReadLock();\n\n        try {\n            ctx.shared().database().ensureFreeSpace(ctx.dataRegion());\n\n            // If batch store update is enabled, we need to lock all entries.\n            // First, need to acquire locks on cache entries, then check filter.\n            List<GridDhtCacheEntry> locked = lockEntries(req, req.topologyVersion());;\n\n            Collection<IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion>> deleted = null;\n\n            DhtAtomicUpdateResult  updDhtRes = new DhtAtomicUpdateResult();\n\n            try {\n                while (true) {\n                    try {\n                        GridDhtPartitionTopology top = topology();\n\n                        top.readLock();\n\n                        try {\n                            if (top.stopping()) {\n                                res.addFailedKeys(req.keys(), new CacheStoppedException(name()));\n\n                                completionCb.apply(req, res);\n\n                                return;\n                            }\n\n                            boolean remap = false;\n\n                            // Do not check topology version if topology was locked on near node by\n                            // external transaction or explicit lock.\n                            if (!req.topologyLocked()) {\n                                // Can not wait for topology future since it will break\n                                // GridNearAtomicCheckUpdateRequest processing.\n                                remap = !top.topologyVersionFuture().isDone() ||\n                                    needRemap(req.topologyVersion(), top.readyTopologyVersion(), req.keys());\n                            }\n\n                            if (!remap) {\n                                boolean validateCache = needCacheValidation(node);\n\n                                if (validateCache) {\n                                    GridDhtTopologyFuture topFut = top.topologyVersionFuture();\n\n                                    // Cache validation should use topology version from the update request\n                                    // in case of the topology version was locked on near node.\n                                    if (req.topologyLocked()) {\n                                        // affinityReadyFuture() can return GridFinishedFuture under some circumstances\n                                        // and therefore it cannot be used for validation.\n                                        IgniteInternalFuture<AffinityTopologyVersion> affFut =\n                                            ctx.shared().exchange().affinityReadyFuture(req.topologyVersion());\n\n                                        if (affFut.isDone()) {\n                                            List<GridDhtPartitionsExchangeFuture> futs =\n                                                ctx.shared().exchange().exchangeFutures();\n\n                                            boolean found = false;\n\n                                            for (int i = 0; i < futs.size(); ++i) {\n                                                GridDhtPartitionsExchangeFuture fut = futs.get(i);\n\n                                                // We have to check fut.exchangeDone() here -\n                                                // otherwise attempt to get topVer will throw error.\n                                                // We won't skip needed future as per affinity ready future is done.\n                                                if (fut.exchangeDone() &&\n                                                    fut.topologyVersion().equals(req.topologyVersion())) {\n                                                    topFut = fut;\n\n                                                    found = true;\n\n                                                    break;\n                                                }\n                                            }\n\n                                            assert found: \"The requested topology future cannot be found [topVer=\"\n                                                + req.topologyVersion() + ']';\n                                        }\n                                        else {\n                                            affFut.listen(f -> updateAllAsyncInternal0(node, req, completionCb));\n\n                                            return;\n                                        }\n\n                                        assert req.topologyVersion().equals(topFut.topologyVersion()) :\n                                            \"The requested topology version cannot be found [\" +\n                                                \"reqTopFut=\" + req.topologyVersion()\n                                                + \", topFut=\" + topFut + ']';\n                                    }\n\n                                    assert topFut.isDone() : topFut;\n\n                                    Throwable err = topFut.validateCache(ctx, req.recovery(), false, null, null);\n\n                                    if (err != null) {\n                                        IgniteCheckedException e = new IgniteCheckedException(err);\n\n                                        res.error(e);\n\n                                        completionCb.apply(req, res);\n\n                                        return;\n                                    }\n                                }\n\n                                update(node, locked, req, res, updDhtRes);\n\n                                dhtFut = updDhtRes.dhtFuture();\n                                deleted = updDhtRes.deleted();\n                                expiry = updDhtRes.expiryPolicy();\n                            }\n                            else\n                                // Should remap all keys.\n                                res.remapTopologyVersion(top.lastTopologyChangeVersion());\n                        }\n                        finally {\n                            top.readUnlock();\n                        }\n\n                        // This call will convert entry processor invocation results to cache object instances.\n                        // Must be done outside topology read lock to avoid deadlocks.\n                        if (res.returnValue() != null)\n                            res.returnValue().marshalResult(ctx);\n\n                        break;\n                    }\n                    catch (UnregisteredClassException ex) {\n                        IgniteCacheObjectProcessor cacheObjProc = ctx.cacheObjects();\n\n                        assert cacheObjProc instanceof CacheObjectBinaryProcessorImpl;\n\n                        ((CacheObjectBinaryProcessorImpl)cacheObjProc)\n                            .binaryContext().descriptorForClass(ex.cls(), false, false);\n                    }\n                    catch (UnregisteredBinaryTypeException ex) {\n                        IgniteCacheObjectProcessor cacheObjProc = ctx.cacheObjects();\n\n                        assert cacheObjProc instanceof CacheObjectBinaryProcessorImpl;\n\n                        ((CacheObjectBinaryProcessorImpl)cacheObjProc)\n                            .binaryContext().updateMetadata(ex.typeId(), ex.binaryMetadata(), false);\n                    }\n                }\n            }\n            catch (GridCacheEntryRemovedException e) {\n                assert false : \"Entry should not become obsolete while holding lock.\";\n\n                e.printStackTrace();\n            }\n            finally {\n                if (locked != null)\n                    unlockEntries(locked, req.topologyVersion());\n\n                // Enqueue if necessary after locks release.\n                if (deleted != null) {\n                    assert !deleted.isEmpty();\n                    assert ctx.deferredDelete() : this;\n\n                    for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted)\n                        ctx.onDeferredDelete(e.get1(), e.get2());\n                }\n\n                // TODO handle failure: probably drop the node from topology\n                // TODO fire events only after successful fsync\n                if (ctx.shared().wal() != null)\n                    ctx.shared().wal().flush(null, false);\n            }\n        }\n        catch (GridDhtInvalidPartitionException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Caught invalid partition exception for cache entry (will remap update request): \" + req);\n\n            res.remapTopologyVersion(ctx.topology().lastTopologyChangeVersion());\n        }\n        catch (Throwable e) {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            U.error(log, \"Unexpected exception during cache update\", e);\n\n            res.addFailedKeys(req.keys(), e);\n\n            completionCb.apply(req, res);\n\n            if (e instanceof Error)\n                throw (Error)e;\n\n            return;\n        }\n        finally {\n            ctx.shared().database().checkpointReadUnlock();\n        }\n\n        if (res.remapTopologyVersion() != null) {\n            assert dhtFut == null;\n\n            completionCb.apply(req, res);\n        }\n        else {\n            if (dhtFut != null)\n                dhtFut.map(node, res.returnValue(), res, completionCb);\n        }\n\n        if (req.writeSynchronizationMode() != FULL_ASYNC)\n            req.cleanup(!node.isLocal());\n\n        sendTtlUpdateRequest(expiry);\n    }"
        ],
        [
            "GridNearLockFuture::map()",
            " 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848 -\n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  ",
            "    /**\n     * Basically, future mapping consists from two parts. First, we must determine the topology version this future\n     * will map on. Locking is performed within a user transaction, we must continue to map keys on the same\n     * topology version as it started. If topology version is undefined, we get current topology future and wait\n     * until it completes so the topology is ready to use.\n     * <p/>\n     * During the second part we map keys to primary nodes using topology snapshot we obtained during the first\n     * part. Note that if primary node leaves grid, the future will fail and transaction will be rolled back.\n     */\n    void map() {\n        if (isDone()) // Possible due to async rollback.\n            return;\n\n        if (timeout > 0) {\n            timeoutObj = new LockTimeoutObject();\n\n            cctx.time().addTimeoutObject(timeoutObj);\n        }\n\n        boolean added = cctx.mvcc().addFuture(this);\n\n        assert added : this;\n\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx != null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null && tx != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null && tx != null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)){\n                    Throwable err = fut.validateCache(cctx, recovery, read, null, keys);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            // Continue mapping on the same topology version as it was before.\n            if (this.topVer == null)\n                this.topVer = topVer;\n\n            map(keys, false, true);\n\n            markInitialized();\n\n            return;\n        }\n\n        // Must get topology snapshot and map on that version.\n        mapOnTopology(false);\n    }",
            " 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848 +\n 849 +\n 850 +\n 851 +\n 852 +\n 853 +\n 854 +\n 855 +\n 856 +\n 857 +\n 858 +\n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  ",
            "    /**\n     * Basically, future mapping consists from two parts. First, we must determine the topology version this future\n     * will map on. Locking is performed within a user transaction, we must continue to map keys on the same\n     * topology version as it started. If topology version is undefined, we get current topology future and wait\n     * until it completes so the topology is ready to use.\n     * <p/>\n     * During the second part we map keys to primary nodes using topology snapshot we obtained during the first\n     * part. Note that if primary node leaves grid, the future will fail and transaction will be rolled back.\n     */\n    void map() {\n        if (isDone()) // Possible due to async rollback.\n            return;\n\n        if (timeout > 0) {\n            timeoutObj = new LockTimeoutObject();\n\n            cctx.time().addTimeoutObject(timeoutObj);\n        }\n\n        boolean added = cctx.mvcc().addFuture(this);\n\n        assert added : this;\n\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx != null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null && tx != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null && tx != null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)){\n                    Throwable err = null;\n\n                    // Before cache validation, make sure that this topology future is already completed.\n                    try {\n                        fut.get();\n                    }\n                    catch (IgniteCheckedException e) {\n                        err = fut.error();\n                    }\n\n                    err = (err == null)? fut.validateCache(cctx, recovery, read, null, keys): err;\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            // Continue mapping on the same topology version as it was before.\n            if (this.topVer == null)\n                this.topVer = topVer;\n\n            map(keys, false, true);\n\n            markInitialized();\n\n            return;\n        }\n\n        // Must get topology snapshot and map on that version.\n        mapOnTopology(false);\n    }"
        ],
        [
            "GridDhtColocatedLockFuture::map()",
            " 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780 -\n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  ",
            "    /**\n     * Basically, future mapping consists from two parts. First, we must determine the topology version this future\n     * will map on. Locking is performed within a user transaction, we must continue to map keys on the same\n     * topology version as it started. If topology version is undefined, we get current topology future and wait\n     * until it completes so the topology is ready to use.\n     * <p/>\n     * During the second part we map keys to primary nodes using topology snapshot we obtained during the first\n     * part. Note that if primary node leaves grid, the future will fail and transaction will be rolled back.\n     */\n    void map() {\n        if (isDone()) // Possible due to async rollback.\n            return;\n\n        if (timeout > 0) {\n            timeoutObj = new LockTimeoutObject();\n\n            cctx.time().addTimeoutObject(timeoutObj);\n        }\n\n        // Obtain the topology version to use.\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx != null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(Thread.currentThread().getId(), tx);\n\n        if (topVer != null && tx != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null && tx != null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = fut.validateCache(cctx, recovery, read, null, keys);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            // Continue mapping on the same topology version as it was before.\n            synchronized (this) {\n                if (this.topVer == null)\n                    this.topVer = topVer;\n            }\n\n            map(keys, false, true);\n\n            markInitialized();\n\n            return;\n        }\n\n        // Must get topology snapshot and map on that version.\n        mapOnTopology(false, null);\n    }",
            " 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780 +\n 781 +\n 782 +\n 783 +\n 784 +\n 785 +\n 786 +\n 787 +\n 788 +\n 789 +\n 790 +\n 791 +\n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  ",
            "    /**\n     * Basically, future mapping consists from two parts. First, we must determine the topology version this future\n     * will map on. Locking is performed within a user transaction, we must continue to map keys on the same\n     * topology version as it started. If topology version is undefined, we get current topology future and wait\n     * until it completes so the topology is ready to use.\n     * <p/>\n     * During the second part we map keys to primary nodes using topology snapshot we obtained during the first\n     * part. Note that if primary node leaves grid, the future will fail and transaction will be rolled back.\n     */\n    void map() {\n        if (isDone()) // Possible due to async rollback.\n            return;\n\n        if (timeout > 0) {\n            timeoutObj = new LockTimeoutObject();\n\n            cctx.time().addTimeoutObject(timeoutObj);\n        }\n\n        // Obtain the topology version to use.\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx != null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(Thread.currentThread().getId(), tx);\n\n        if (topVer != null && tx != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null && tx != null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = null;\n\n                    // Before cache validation, make sure that this topology future is already completed.\n                    try {\n                        fut.get();\n                    }\n                    catch (IgniteCheckedException e) {\n                        err = fut.error();\n                    }\n\n                    if (err == null)\n                        err = fut.validateCache(cctx, recovery, read, null, keys);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            // Continue mapping on the same topology version as it was before.\n            synchronized (this) {\n                if (this.topVer == null)\n                    this.topVer = topVer;\n            }\n\n            map(keys, false, true);\n\n            markInitialized();\n\n            return;\n        }\n\n        // Must get topology snapshot and map on that version.\n        mapOnTopology(false, null);\n    }"
        ],
        [
            "GridNearTxAbstractEnlistFuture::init()",
            " 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226 -\n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  ",
            "    /**\n     *\n     */\n    public void init() {\n        if (timeout < 0) {\n            // Time is out.\n            onDone(timeoutException());\n\n            return;\n        }\n        else if (timeout > 0)\n            timeoutObj = new LockTimeoutObject();\n\n        while (true) {\n            IgniteInternalFuture<?> fut = tx.lockFuture();\n\n            if (fut == GridDhtTxLocalAdapter.ROLLBACK_FUT) {\n                onDone(tx.timedOut() ? tx.timeoutException() : tx.rollbackException());\n\n                return;\n            }\n            else if (fut != null) {\n                // Wait for previous future.\n                assert fut instanceof GridNearTxAbstractEnlistFuture\n                    || fut instanceof GridDhtTxAbstractEnlistFuture\n                    || fut instanceof CompoundLockFuture\n                    || fut instanceof GridNearTxSelectForUpdateFuture : fut;\n\n                // Terminate this future if parent future is terminated by rollback.\n                if (!fut.isDone()) {\n                    fut.listen(new IgniteInClosure<IgniteInternalFuture>() {\n                        @Override public void apply(IgniteInternalFuture fut) {\n                            if (fut.error() != null)\n                                onDone(fut.error());\n                        }\n                    });\n                }\n                else if (fut.error() != null)\n                    onDone(fut.error());\n\n                break;\n            }\n            else if (tx.updateLockFuture(null, this))\n                break;\n        }\n\n        boolean added = cctx.mvcc().addFuture(this);\n\n        assert added : this;\n\n        if (isDone()) {\n            cctx.mvcc().removeFuture(futId);\n\n            return;\n        }\n\n        try {\n            tx.addActiveCache(cctx, false);\n        }\n        catch (IgniteCheckedException e) {\n            onDone(e);\n\n            return;\n        }\n\n        if (timeoutObj != null)\n            cctx.time().addTimeoutObject(timeoutObj);\n\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = fut.validateCache(cctx, false, false, null, null);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            if (this.topVer == null)\n                this.topVer = topVer;\n\n            map(true);\n\n            return;\n        }\n\n        mapOnTopology();\n    }",
            " 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226 +\n 227 +\n 228 +\n 229 +\n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  ",
            "    /**\n     *\n     */\n    public void init() {\n        if (timeout < 0) {\n            // Time is out.\n            onDone(timeoutException());\n\n            return;\n        }\n        else if (timeout > 0)\n            timeoutObj = new LockTimeoutObject();\n\n        while (true) {\n            IgniteInternalFuture<?> fut = tx.lockFuture();\n\n            if (fut == GridDhtTxLocalAdapter.ROLLBACK_FUT) {\n                onDone(tx.timedOut() ? tx.timeoutException() : tx.rollbackException());\n\n                return;\n            }\n            else if (fut != null) {\n                // Wait for previous future.\n                assert fut instanceof GridNearTxAbstractEnlistFuture\n                    || fut instanceof GridDhtTxAbstractEnlistFuture\n                    || fut instanceof CompoundLockFuture\n                    || fut instanceof GridNearTxSelectForUpdateFuture : fut;\n\n                // Terminate this future if parent future is terminated by rollback.\n                if (!fut.isDone()) {\n                    fut.listen(new IgniteInClosure<IgniteInternalFuture>() {\n                        @Override public void apply(IgniteInternalFuture fut) {\n                            if (fut.error() != null)\n                                onDone(fut.error());\n                        }\n                    });\n                }\n                else if (fut.error() != null)\n                    onDone(fut.error());\n\n                break;\n            }\n            else if (tx.updateLockFuture(null, this))\n                break;\n        }\n\n        boolean added = cctx.mvcc().addFuture(this);\n\n        assert added : this;\n\n        if (isDone()) {\n            cctx.mvcc().removeFuture(futId);\n\n            return;\n        }\n\n        try {\n            tx.addActiveCache(cctx, false);\n        }\n        catch (IgniteCheckedException e) {\n            onDone(e);\n\n            return;\n        }\n\n        if (timeoutObj != null)\n            cctx.time().addTimeoutObject(timeoutObj);\n\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = null;\n\n                    // Before cache validation, make sure that this topology future is already completed.\n                    try {\n                        fut.get();\n                    }\n                    catch (IgniteCheckedException e) {\n                        err = fut.error();\n                    }\n\n                    if (err == null)\n                        err = fut.validateCache(cctx, false, false, null, null);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            if (this.topVer == null)\n                this.topVer = topVer;\n\n            map(true);\n\n            return;\n        }\n\n        mapOnTopology();\n    }"
        ]
    ]
}