{
    "754bd7b8a37f7afd3312fc9e9d4d4c7d726387c0": [
        [
            "GridCacheProcessor::prepareCacheStop(DynamicCacheChangeRequest)",
            "1894  \n1895  \n1896  \n1897 -\n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  ",
            "    /**\n     * @param req Stop request.\n     */\n    private void prepareCacheStop(DynamicCacheChangeRequest req) {\n        assert req.stop() || req.close() : req;\n\n        GridCacheAdapter<?, ?> cache = caches.remove(maskNull(req.cacheName()));\n\n        if (cache != null) {\n            GridCacheContext<?, ?> ctx = cache.context();\n\n            sharedCtx.removeCacheContext(ctx);\n\n            assert req.deploymentId().equals(ctx.dynamicDeploymentId()) : \"Different deployment IDs [req=\" + req +\n                \", ctxDepId=\" + ctx.dynamicDeploymentId() + ']';\n\n            onKernalStop(cache, req.destroy());\n\n            stopCache(cache, true, req.destroy());\n        }\n    }",
            "1885  \n1886  \n1887  \n1888  \n1889 +\n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905 +\n1906 +\n1907  \n1908 +\n1909 +\n1910  ",
            "    /**\n     * @param req Stop request.\n     * @return Stopped cache context.\n     */\n    private GridCacheContext<?, ?> prepareCacheStop(DynamicCacheChangeRequest req) {\n        assert req.stop() || req.close() : req;\n\n        GridCacheAdapter<?, ?> cache = caches.remove(maskNull(req.cacheName()));\n\n        if (cache != null) {\n            GridCacheContext<?, ?> ctx = cache.context();\n\n            sharedCtx.removeCacheContext(ctx);\n\n            assert req.deploymentId().equals(ctx.dynamicDeploymentId()) : \"Different deployment IDs [req=\" + req +\n                \", ctxDepId=\" + ctx.dynamicDeploymentId() + ']';\n\n            onKernalStop(cache, req.destroy());\n\n            stopCache(cache, true, req.destroy());\n\n            return ctx;\n        }\n\n        return null;\n    }"
        ],
        [
            "GridDhtAtomicCache::updateAllAsyncInternal0(UUID,GridNearAtomicAbstractUpdateRequest,CI2)",
            "1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  ",
            "    /**\n     * Executes local update after preloader fetched values.\n     *\n     * @param nodeId Node ID.\n     * @param req Update request.\n     * @param completionCb Completion callback.\n     */\n    private void updateAllAsyncInternal0(\n        UUID nodeId,\n        GridNearAtomicAbstractUpdateRequest req,\n        CI2<GridNearAtomicAbstractUpdateRequest, GridNearAtomicUpdateResponse> completionCb\n    ) {\n        GridNearAtomicUpdateResponse res = new GridNearAtomicUpdateResponse(ctx.cacheId(), nodeId, req.futureVersion(),\n            ctx.deploymentEnabled());\n\n        assert !req.returnValue() || (req.operation() == TRANSFORM || req.size() == 1);\n\n        GridDhtAtomicAbstractUpdateFuture dhtFut = null;\n\n        boolean remap = false;\n\n        String taskName = ctx.kernalContext().task().resolveTaskName(req.taskNameHash());\n\n        IgniteCacheExpiryPolicy expiry = null;\n\n        try {\n            // If batch store update is enabled, we need to lock all entries.\n            // First, need to acquire locks on cache entries, then check filter.\n            List<GridDhtCacheEntry> locked = lockEntries(req, req.topologyVersion());\n\n            Collection<IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion>> deleted = null;\n\n            try {\n                GridDhtPartitionTopology top = topology();\n\n                top.readLock();\n\n                try {\n                    if (top.stopping()) {\n                        res.addFailedKeys(req.keys(), new CacheStoppedException(name()));\n\n                        completionCb.apply(req, res);\n\n                        return;\n                    }\n\n                    // Do not check topology version for CLOCK versioning since\n                    // partition exchange will wait for near update future (if future is on server node).\n                    // Also do not check topology version if topology was locked on near node by\n                    // external transaction or explicit lock.\n                    if ((req.fastMap() && !req.clientRequest()) || req.topologyLocked() ||\n                        !needRemap(req.topologyVersion(), top.topologyVersion())) {\n                        ClusterNode node = ctx.discovery().node(nodeId);\n\n                        if (node == null) {\n                            U.warn(msgLog, \"Skip near update request, node originated update request left [\" +\n                                \"futId=\" + req.futureVersion() + \", node=\" + nodeId + ']');\n\n                            return;\n                        }\n\n                        boolean hasNear = ctx.discovery().cacheNearNode(node, name());\n\n                        GridCacheVersion ver = req.updateVersion();\n\n                        if (ver == null) {\n                            // Assign next version for update inside entries lock.\n                            ver = ctx.versions().next(top.topologyVersion());\n\n                            if (hasNear)\n                                res.nearVersion(ver);\n\n                            if (msgLog.isDebugEnabled()) {\n                                msgLog.debug(\"Assigned update version [futId=\" + req.futureVersion() +\n                                    \", writeVer=\" + ver + ']');\n                            }\n                        }\n\n                        assert ver != null : \"Got null version for update request: \" + req;\n\n                        boolean sndPrevVal = !top.rebalanceFinished(req.topologyVersion());\n\n                        dhtFut = createDhtFuture(ver, req, res, completionCb, false);\n\n                        expiry = expiryPolicy(req.expiry());\n\n                        GridCacheReturn retVal = null;\n\n                        if (req.size() > 1 &&                    // Several keys ...\n                            writeThrough() && !req.skipStore() && // and store is enabled ...\n                            !ctx.store().isLocal() &&             // and this is not local store ...\n                                                                  // (conflict resolver should be used for local store)\n                            !ctx.dr().receiveEnabled()            // and no DR.\n                            ) {\n                            // This method can only be used when there are no replicated entries in the batch.\n                            UpdateBatchResult updRes = updateWithBatch(node,\n                                hasNear,\n                                req,\n                                res,\n                                locked,\n                                ver,\n                                dhtFut,\n                                completionCb,\n                                ctx.isDrEnabled(),\n                                taskName,\n                                expiry,\n                                sndPrevVal);\n\n                            deleted = updRes.deleted();\n                            dhtFut = updRes.dhtFuture();\n\n                            if (req.operation() == TRANSFORM)\n                                retVal = updRes.invokeResults();\n                        }\n                        else {\n                            UpdateSingleResult updRes = updateSingle(node,\n                                hasNear,\n                                req,\n                                res,\n                                locked,\n                                ver,\n                                dhtFut,\n                                completionCb,\n                                ctx.isDrEnabled(),\n                                taskName,\n                                expiry,\n                                sndPrevVal);\n\n                            retVal = updRes.returnValue();\n                            deleted = updRes.deleted();\n                            dhtFut = updRes.dhtFuture();\n                        }\n\n                        if (retVal == null)\n                            retVal = new GridCacheReturn(ctx, node.isLocal(), true, null, true);\n\n                        res.returnValue(retVal);\n\n                        if (req.writeSynchronizationMode() != FULL_ASYNC)\n                            req.cleanup(!node.isLocal());\n\n                        if (dhtFut != null)\n                            ctx.mvcc().addAtomicFuture(dhtFut.version(), dhtFut);\n                    }\n                    else\n                        // Should remap all keys.\n                        remap = true;\n                }\n                finally {\n                    top.readUnlock();\n                }\n            }\n            catch (GridCacheEntryRemovedException e) {\n                assert false : \"Entry should not become obsolete while holding lock.\";\n\n                e.printStackTrace();\n            }\n            finally {\n                if (locked != null)\n                    unlockEntries(locked, req.topologyVersion());\n\n                // Enqueue if necessary after locks release.\n                if (deleted != null) {\n                    assert !deleted.isEmpty();\n                    assert ctx.deferredDelete() : this;\n\n                    for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted)\n                        ctx.onDeferredDelete(e.get1(), e.get2());\n                }\n\n                // TODO handle failure: probably drop the node from topology\n                // TODO fire events only after successful fsync\n                if (ctx.shared().wal() != null)\n                    ctx.shared().wal().fsync(null);\n            }\n        }\n        catch (GridDhtInvalidPartitionException ignore) {\n            assert !req.fastMap() || req.clientRequest() : req;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Caught invalid partition exception for cache entry (will remap update request): \" + req);\n\n            remap = true;\n        }\n        catch (Throwable e) {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            U.error(log, \"Unexpected exception during cache update\", e);\n\n            res.addFailedKeys(req.keys(), e);\n\n            completionCb.apply(req, res);\n\n            if (e instanceof Error)\n                throw (Error)e;\n\n            return;\n        }\n\n        if (remap) {\n            assert dhtFut == null;\n\n            res.remapKeys(req.keys());\n\n            completionCb.apply(req, res);\n        }\n        else {\n            // If there are backups, map backup update future.\n            if (dhtFut != null)\n                dhtFut.map();\n                // Otherwise, complete the call.\n            else\n                completionCb.apply(req, res);\n        }\n\n        sendTtlUpdateRequest(expiry);\n    }",
            "1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809 +\n1810 +\n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984 +\n1985 +\n1986 +\n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  ",
            "    /**\n     * Executes local update after preloader fetched values.\n     *\n     * @param nodeId Node ID.\n     * @param req Update request.\n     * @param completionCb Completion callback.\n     */\n    private void updateAllAsyncInternal0(\n        UUID nodeId,\n        GridNearAtomicAbstractUpdateRequest req,\n        CI2<GridNearAtomicAbstractUpdateRequest, GridNearAtomicUpdateResponse> completionCb\n    ) {\n        GridNearAtomicUpdateResponse res = new GridNearAtomicUpdateResponse(ctx.cacheId(), nodeId, req.futureVersion(),\n            ctx.deploymentEnabled());\n\n        assert !req.returnValue() || (req.operation() == TRANSFORM || req.size() == 1);\n\n        GridDhtAtomicAbstractUpdateFuture dhtFut = null;\n\n        boolean remap = false;\n\n        String taskName = ctx.kernalContext().task().resolveTaskName(req.taskNameHash());\n\n        IgniteCacheExpiryPolicy expiry = null;\n\n        ctx.shared().database().checkpointReadLock();\n\n        try {\n            // If batch store update is enabled, we need to lock all entries.\n            // First, need to acquire locks on cache entries, then check filter.\n            List<GridDhtCacheEntry> locked = lockEntries(req, req.topologyVersion());\n\n            Collection<IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion>> deleted = null;\n\n            try {\n                GridDhtPartitionTopology top = topology();\n\n                top.readLock();\n\n                try {\n                    if (top.stopping()) {\n                        res.addFailedKeys(req.keys(), new CacheStoppedException(name()));\n\n                        completionCb.apply(req, res);\n\n                        return;\n                    }\n\n                    // Do not check topology version for CLOCK versioning since\n                    // partition exchange will wait for near update future (if future is on server node).\n                    // Also do not check topology version if topology was locked on near node by\n                    // external transaction or explicit lock.\n                    if ((req.fastMap() && !req.clientRequest()) || req.topologyLocked() ||\n                        !needRemap(req.topologyVersion(), top.topologyVersion())) {\n                        ClusterNode node = ctx.discovery().node(nodeId);\n\n                        if (node == null) {\n                            U.warn(msgLog, \"Skip near update request, node originated update request left [\" +\n                                \"futId=\" + req.futureVersion() + \", node=\" + nodeId + ']');\n\n                            return;\n                        }\n\n                        boolean hasNear = ctx.discovery().cacheNearNode(node, name());\n\n                        GridCacheVersion ver = req.updateVersion();\n\n                        if (ver == null) {\n                            // Assign next version for update inside entries lock.\n                            ver = ctx.versions().next(top.topologyVersion());\n\n                            if (hasNear)\n                                res.nearVersion(ver);\n\n                            if (msgLog.isDebugEnabled()) {\n                                msgLog.debug(\"Assigned update version [futId=\" + req.futureVersion() +\n                                    \", writeVer=\" + ver + ']');\n                            }\n                        }\n\n                        assert ver != null : \"Got null version for update request: \" + req;\n\n                        boolean sndPrevVal = !top.rebalanceFinished(req.topologyVersion());\n\n                        dhtFut = createDhtFuture(ver, req, res, completionCb, false);\n\n                        expiry = expiryPolicy(req.expiry());\n\n                        GridCacheReturn retVal = null;\n\n                        if (req.size() > 1 &&                    // Several keys ...\n                            writeThrough() && !req.skipStore() && // and store is enabled ...\n                            !ctx.store().isLocal() &&             // and this is not local store ...\n                                                                  // (conflict resolver should be used for local store)\n                            !ctx.dr().receiveEnabled()            // and no DR.\n                            ) {\n                            // This method can only be used when there are no replicated entries in the batch.\n                            UpdateBatchResult updRes = updateWithBatch(node,\n                                hasNear,\n                                req,\n                                res,\n                                locked,\n                                ver,\n                                dhtFut,\n                                completionCb,\n                                ctx.isDrEnabled(),\n                                taskName,\n                                expiry,\n                                sndPrevVal);\n\n                            deleted = updRes.deleted();\n                            dhtFut = updRes.dhtFuture();\n\n                            if (req.operation() == TRANSFORM)\n                                retVal = updRes.invokeResults();\n                        }\n                        else {\n                            UpdateSingleResult updRes = updateSingle(node,\n                                hasNear,\n                                req,\n                                res,\n                                locked,\n                                ver,\n                                dhtFut,\n                                completionCb,\n                                ctx.isDrEnabled(),\n                                taskName,\n                                expiry,\n                                sndPrevVal);\n\n                            retVal = updRes.returnValue();\n                            deleted = updRes.deleted();\n                            dhtFut = updRes.dhtFuture();\n                        }\n\n                        if (retVal == null)\n                            retVal = new GridCacheReturn(ctx, node.isLocal(), true, null, true);\n\n                        res.returnValue(retVal);\n\n                        if (req.writeSynchronizationMode() != FULL_ASYNC)\n                            req.cleanup(!node.isLocal());\n\n                        if (dhtFut != null)\n                            ctx.mvcc().addAtomicFuture(dhtFut.version(), dhtFut);\n                    }\n                    else\n                        // Should remap all keys.\n                        remap = true;\n                }\n                finally {\n                    top.readUnlock();\n                }\n            }\n            catch (GridCacheEntryRemovedException e) {\n                assert false : \"Entry should not become obsolete while holding lock.\";\n\n                e.printStackTrace();\n            }\n            finally {\n                if (locked != null)\n                    unlockEntries(locked, req.topologyVersion());\n\n                // Enqueue if necessary after locks release.\n                if (deleted != null) {\n                    assert !deleted.isEmpty();\n                    assert ctx.deferredDelete() : this;\n\n                    for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted)\n                        ctx.onDeferredDelete(e.get1(), e.get2());\n                }\n\n                // TODO handle failure: probably drop the node from topology\n                // TODO fire events only after successful fsync\n                if (ctx.shared().wal() != null)\n                    ctx.shared().wal().fsync(null);\n            }\n        }\n        catch (GridDhtInvalidPartitionException ignore) {\n            assert !req.fastMap() || req.clientRequest() : req;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Caught invalid partition exception for cache entry (will remap update request): \" + req);\n\n            remap = true;\n        }\n        catch (Throwable e) {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            U.error(log, \"Unexpected exception during cache update\", e);\n\n            res.addFailedKeys(req.keys(), e);\n\n            completionCb.apply(req, res);\n\n            if (e instanceof Error)\n                throw (Error)e;\n\n            return;\n        }\n        finally {\n            ctx.shared().database().checkpointReadUnlock();\n        }\n\n        if (remap) {\n            assert dhtFut == null;\n\n            res.remapKeys(req.keys());\n\n            completionCb.apply(req, res);\n        }\n        else {\n            // If there are backups, map backup update future.\n            if (dhtFut != null)\n                dhtFut.map();\n                // Otherwise, complete the call.\n            else\n                completionCb.apply(req, res);\n        }\n\n        sendTtlUpdateRequest(expiry);\n    }"
        ],
        [
            "GridDhtAtomicCache::unlockEntries(Collection,AffinityTopologyVersion)",
            "3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044 -\n3045 -\n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  ",
            "    /**\n     * Releases java-level locks on cache entries.\n     *\n     * @param locked Locked entries.\n     * @param topVer Topology version.\n     */\n    private void unlockEntries(Collection<GridDhtCacheEntry> locked, AffinityTopologyVersion topVer) {\n        // Process deleted entries before locks release.\n        assert ctx.deferredDelete() : this;\n\n        // Entries to skip eviction manager notification for.\n        // Enqueue entries while holding locks.\n        Collection<KeyCacheObject> skip = null;\n\n        try {\n            for (GridCacheMapEntry entry : locked) {\n                if (entry != null && entry.deleted()) {\n                    if (skip == null)\n                        skip = new HashSet<>(locked.size(), 1.0f);\n\n                    skip.add(entry.key());\n                }\n            }\n        }\n        finally {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            // That's why releasing locks in the finally block..\n            for (GridCacheMapEntry entry : locked) {\n                if (entry != null)\n                    GridUnsafe.monitorExit(entry);\n            }\n        }\n\n        // Try evict partitions.\n        for (GridDhtCacheEntry entry : locked) {\n            if (entry != null)\n                entry.onUnlock();\n        }\n\n        ctx.shared().database().checkpointReadUnlock();\n\n        if (skip != null && skip.size() == locked.size())\n            // Optimization.\n            return;\n\n        // Must touch all entries since update may have deleted entries.\n        // Eviction manager will remove empty entries.\n        for (GridCacheMapEntry entry : locked) {\n            if (entry != null && (skip == null || !skip.contains(entry.key())))\n                ctx.evicts().touch(entry, topVer);\n        }\n    }",
            "3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  \n3057  ",
            "    /**\n     * Releases java-level locks on cache entries.\n     *\n     * @param locked Locked entries.\n     * @param topVer Topology version.\n     */\n    private void unlockEntries(Collection<GridDhtCacheEntry> locked, AffinityTopologyVersion topVer) {\n        // Process deleted entries before locks release.\n        assert ctx.deferredDelete() : this;\n\n        // Entries to skip eviction manager notification for.\n        // Enqueue entries while holding locks.\n        Collection<KeyCacheObject> skip = null;\n\n        try {\n            for (GridCacheMapEntry entry : locked) {\n                if (entry != null && entry.deleted()) {\n                    if (skip == null)\n                        skip = new HashSet<>(locked.size(), 1.0f);\n\n                    skip.add(entry.key());\n                }\n            }\n        }\n        finally {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            // That's why releasing locks in the finally block..\n            for (GridCacheMapEntry entry : locked) {\n                if (entry != null)\n                    GridUnsafe.monitorExit(entry);\n            }\n        }\n\n        // Try evict partitions.\n        for (GridDhtCacheEntry entry : locked) {\n            if (entry != null)\n                entry.onUnlock();\n        }\n\n        if (skip != null && skip.size() == locked.size())\n            // Optimization.\n            return;\n\n        // Must touch all entries since update may have deleted entries.\n        // Eviction manager will remove empty entries.\n        for (GridCacheMapEntry entry : locked) {\n            if (entry != null && (skip == null || !skip.contains(entry.key())))\n                ctx.evicts().touch(entry, topVer);\n        }\n    }"
        ],
        [
            "GridCacheProcessor::checkConsistency()",
            " 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891 -\n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  ",
            "    /**\n     *\n     */\n    private void checkConsistency() throws IgniteCheckedException {\n        if (!ctx.config().isDaemon() && !getBoolean(IGNITE_SKIP_CONFIGURATION_CONSISTENCY_CHECK)) {\n            for (ClusterNode n : ctx.discovery().remoteNodes()) {\n                if (n.attribute(ATTR_CONSISTENCY_CHECK_SKIPPED))\n                    continue;\n\n                checkTransactionConfiguration(n);\n\n                DeploymentMode locDepMode = ctx.config().getDeploymentMode();\n                DeploymentMode rmtDepMode = n.attribute(IgniteNodeAttributes.ATTR_DEPLOYMENT_MODE);\n\n                CU.checkAttributeMismatch(\n                    log, null, n.id(), \"deploymentMode\", \"Deployment mode\",\n                    locDepMode, rmtDepMode, true);\n\n                for (DynamicCacheDescriptor desc : registeredCaches.values()) {\n                    CacheConfiguration rmtCfg = desc.remoteConfiguration(n.id());\n\n                    if (rmtCfg != null) {\n                        CacheConfiguration locCfg = desc.cacheConfiguration();\n\n                        checkCache(locCfg, rmtCfg, n);\n\n                        // Check plugin cache configurations.\n                        CachePluginManager pluginMgr = desc.pluginManager();\n\n                        pluginMgr.validateRemotes(rmtCfg, n);\n                    }\n                }\n            }\n        }\n    }",
            " 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892 +\n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  ",
            "    /**\n     *\n     */\n    private void checkConsistency() throws IgniteCheckedException {\n        if (!ctx.config().isDaemon() && !getBoolean(IGNITE_SKIP_CONFIGURATION_CONSISTENCY_CHECK)) {\n            for (ClusterNode n : ctx.discovery().remoteNodes()) {\n                if (Boolean.TRUE.equals(n.attribute(ATTR_CONSISTENCY_CHECK_SKIPPED)))\n                    continue;\n\n                checkTransactionConfiguration(n);\n\n                DeploymentMode locDepMode = ctx.config().getDeploymentMode();\n                DeploymentMode rmtDepMode = n.attribute(IgniteNodeAttributes.ATTR_DEPLOYMENT_MODE);\n\n                CU.checkAttributeMismatch(\n                    log, null, n.id(), \"deploymentMode\", \"Deployment mode\",\n                    locDepMode, rmtDepMode, true);\n\n                for (DynamicCacheDescriptor desc : registeredCaches.values()) {\n                    CacheConfiguration rmtCfg = desc.remoteConfiguration(n.id());\n\n                    if (rmtCfg != null) {\n                        CacheConfiguration locCfg = desc.cacheConfiguration();\n\n                        checkCache(locCfg, rmtCfg, n);\n\n                        // Check plugin cache configurations.\n                        CachePluginManager pluginMgr = desc.pluginManager();\n\n                        pluginMgr.validateRemotes(rmtCfg, n);\n                    }\n                }\n            }\n        }\n    }"
        ],
        [
            "GridDhtAtomicCache::lockEntries(GridNearAtomicAbstractUpdateRequest,AffinityTopologyVersion)",
            "2915  \n2916  \n2917  \n2918  \n2919  \n2920  \n2921  \n2922  \n2923  \n2924  \n2925  \n2926  \n2927 -\n2928 -\n2929  \n2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944  \n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  \n2957  \n2958  \n2959  \n2960  \n2961  \n2962  \n2963  \n2964  \n2965  \n2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  ",
            "    /**\n     * Acquires java-level locks on cache entries. Returns collection of locked entries.\n     *\n     * @param req Request with keys to lock.\n     * @param topVer Topology version to lock on.\n     * @return Collection of locked entries.\n     * @throws GridDhtInvalidPartitionException If entry does not belong to local node. If exception is thrown,\n     *      locks are released.\n     */\n    @SuppressWarnings(\"ForLoopReplaceableByForEach\")\n    private List<GridDhtCacheEntry> lockEntries(GridNearAtomicAbstractUpdateRequest req, AffinityTopologyVersion topVer)\n        throws GridDhtInvalidPartitionException {\n        ctx.shared().database().checkpointReadLock();\n\n        if (req.size() == 1) {\n            KeyCacheObject key = req.key(0);\n\n            while (true) {\n                try {\n                    GridDhtCacheEntry entry = entryExx(key, topVer);\n\n                    GridUnsafe.monitorEnter(entry);\n\n                    if (entry.obsolete())\n                        GridUnsafe.monitorExit(entry);\n                    else\n                        return Collections.singletonList(entry);\n                }\n                catch (GridDhtInvalidPartitionException e) {\n                    // Ignore invalid partition exception in CLOCK ordering mode.\n                    if (ctx.config().getAtomicWriteOrderMode() == CLOCK)\n                        return Collections.singletonList(null);\n                    else\n                        throw e;\n                }\n            }\n        }\n        else {\n            List<GridDhtCacheEntry> locked = new ArrayList<>(req.size());\n\n            while (true) {\n                for (int i = 0; i < req.size(); i++) {\n                    try {\n                        GridDhtCacheEntry entry = entryExx(req.key(i), topVer);\n\n                        locked.add(entry);\n                    }\n                    catch (GridDhtInvalidPartitionException e) {\n                        // Ignore invalid partition exception in CLOCK ordering mode.\n                        if (ctx.config().getAtomicWriteOrderMode() == CLOCK)\n                            locked.add(null);\n                        else\n                            throw e;\n                    }\n                }\n\n                boolean retry = false;\n\n                for (int i = 0; i < locked.size(); i++) {\n                    GridCacheMapEntry entry = locked.get(i);\n\n                    if (entry == null)\n                        continue;\n\n                    GridUnsafe.monitorEnter(entry);\n\n                    if (entry.obsolete()) {\n                        // Unlock all locked.\n                        for (int j = 0; j <= i; j++) {\n                            if (locked.get(j) != null)\n                                GridUnsafe.monitorExit(locked.get(j));\n                        }\n\n                        // Clear entries.\n                        locked.clear();\n\n                        // Retry.\n                        retry = true;\n\n                        break;\n                    }\n                }\n\n                if (!retry)\n                    return locked;\n            }\n        }\n    }",
            "2920  \n2921  \n2922  \n2923  \n2924  \n2925  \n2926  \n2927  \n2928  \n2929  \n2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944  \n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  \n2957  \n2958  \n2959  \n2960  \n2961  \n2962  \n2963  \n2964  \n2965  \n2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003  \n3004  \n3005  ",
            "    /**\n     * Acquires java-level locks on cache entries. Returns collection of locked entries.\n     *\n     * @param req Request with keys to lock.\n     * @param topVer Topology version to lock on.\n     * @return Collection of locked entries.\n     * @throws GridDhtInvalidPartitionException If entry does not belong to local node. If exception is thrown,\n     *      locks are released.\n     */\n    @SuppressWarnings(\"ForLoopReplaceableByForEach\")\n    private List<GridDhtCacheEntry> lockEntries(GridNearAtomicAbstractUpdateRequest req, AffinityTopologyVersion topVer)\n        throws GridDhtInvalidPartitionException {\n        if (req.size() == 1) {\n            KeyCacheObject key = req.key(0);\n\n            while (true) {\n                try {\n                    GridDhtCacheEntry entry = entryExx(key, topVer);\n\n                    GridUnsafe.monitorEnter(entry);\n\n                    if (entry.obsolete())\n                        GridUnsafe.monitorExit(entry);\n                    else\n                        return Collections.singletonList(entry);\n                }\n                catch (GridDhtInvalidPartitionException e) {\n                    // Ignore invalid partition exception in CLOCK ordering mode.\n                    if (ctx.config().getAtomicWriteOrderMode() == CLOCK)\n                        return Collections.singletonList(null);\n                    else\n                        throw e;\n                }\n            }\n        }\n        else {\n            List<GridDhtCacheEntry> locked = new ArrayList<>(req.size());\n\n            while (true) {\n                for (int i = 0; i < req.size(); i++) {\n                    try {\n                        GridDhtCacheEntry entry = entryExx(req.key(i), topVer);\n\n                        locked.add(entry);\n                    }\n                    catch (GridDhtInvalidPartitionException e) {\n                        // Ignore invalid partition exception in CLOCK ordering mode.\n                        if (ctx.config().getAtomicWriteOrderMode() == CLOCK)\n                            locked.add(null);\n                        else\n                            throw e;\n                    }\n                }\n\n                boolean retry = false;\n\n                for (int i = 0; i < locked.size(); i++) {\n                    GridCacheMapEntry entry = locked.get(i);\n\n                    if (entry == null)\n                        continue;\n\n                    GridUnsafe.monitorEnter(entry);\n\n                    if (entry.obsolete()) {\n                        // Unlock all locked.\n                        for (int j = 0; j <= i; j++) {\n                            if (locked.get(j) != null)\n                                GridUnsafe.monitorExit(locked.get(j));\n                        }\n\n                        // Clear entries.\n                        locked.clear();\n\n                        // Retry.\n                        retry = true;\n\n                        break;\n                    }\n                }\n\n                if (!retry)\n                    return locked;\n            }\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::init()",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538 -\n 539 -\n 540 -\n 541 -\n 542 -\n 543 -\n 544 -\n 545 -\n 546 -\n 547 -\n 548 -\n 549 -\n 550 -\n 551 -\n 552 -\n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  ",
            "    /**\n     * Starts activity.\n     *\n     * @throws IgniteInterruptedCheckedException If interrupted.\n     */\n    public void init() throws IgniteInterruptedCheckedException {\n        if (isDone())\n            return;\n\n        initTs = U.currentTimeMillis();\n\n        U.await(evtLatch);\n\n        assert discoEvt != null : this;\n        assert exchId.nodeId().equals(discoEvt.eventNode().id()) : this;\n        assert !dummy && !forcePreload : this;\n\n        try {\n            AffinityTopologyVersion topVer = topologyVersion();\n\n            srvNodes = new ArrayList<>(cctx.discovery().serverNodes(topVer));\n\n            remaining.addAll(F.nodeIds(F.view(srvNodes, F.remoteNodes(cctx.localNodeId()))));\n\n            crd = srvNodes.isEmpty() ? null : srvNodes.get(0);\n\n            boolean crdNode = crd != null && crd.isLocal();\n\n            skipPreload = cctx.kernalContext().clientNode();\n\n            ExchangeType exchange;\n\n            if (discoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                DiscoveryCustomMessage msg = ((DiscoveryCustomEvent)discoEvt).customMessage();\n\n                if (msg instanceof DynamicCacheChangeBatch){\n                    assert !F.isEmpty(reqs);\n\n                    exchange = onCacheChangeRequest(crdNode);\n                }\n                else if (msg instanceof StartFullSnapshotAckDiscoveryMessage)\n                    exchange = CU.clientNode(discoEvt.eventNode()) ?\n                        onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                else {\n                    assert affChangeMsg != null : this;\n\n                    exchange = onAffinityChangeRequest(crdNode);\n                }\n            }\n            else {\n                if (discoEvt.type() == EVT_NODE_JOINED) {\n                    Collection<DynamicCacheDescriptor> receivedCaches = cctx.cache().startReceivedCaches(topVer);\n\n                    if (!discoEvt.eventNode().isLocal())\n                        cctx.affinity().initStartedCaches(crdNode, this, receivedCaches);\n                }\n\n                exchange = CU.clientNode(discoEvt.eventNode()) ?\n                    onClientNodeEvent(crdNode) :\n                    onServerNodeEvent(crdNode);\n            }\n\n            updateTopologies(crdNode);\n\n            if (!F.isEmpty(reqs)) {\n                boolean hasStop = false;\n\n                for (DynamicCacheChangeRequest req : reqs) {\n                    if (req.stop()) {\n                        hasStop = true;\n\n                        break;\n                    }\n                }\n\n                if (hasStop)\n                    cctx.cache().context().database().beforeCachesStop();\n            }\n\n            switch (exchange) {\n                case ALL: {\n                    distributedExchange();\n\n                    break;\n                }\n\n                case CLIENT: {\n                    initTopologies();\n\n                    clientOnlyExchange();\n\n                    break;\n                }\n\n                case NONE: {\n                    initTopologies();\n\n                    onDone(topVer);\n\n                    break;\n                }\n\n                default:\n                    assert false;\n            }\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            onDone(e);\n\n            throw e;\n        }\n        catch (Throwable e) {\n            U.error(log, \"Failed to reinitialize local partitions (preloading will be stopped): \" + exchId, e);\n\n            onDone(e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n    }",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  ",
            "    /**\n     * Starts activity.\n     *\n     * @throws IgniteInterruptedCheckedException If interrupted.\n     */\n    public void init() throws IgniteInterruptedCheckedException {\n        if (isDone())\n            return;\n\n        initTs = U.currentTimeMillis();\n\n        U.await(evtLatch);\n\n        assert discoEvt != null : this;\n        assert exchId.nodeId().equals(discoEvt.eventNode().id()) : this;\n        assert !dummy && !forcePreload : this;\n\n        try {\n            AffinityTopologyVersion topVer = topologyVersion();\n\n            srvNodes = new ArrayList<>(cctx.discovery().serverNodes(topVer));\n\n            remaining.addAll(F.nodeIds(F.view(srvNodes, F.remoteNodes(cctx.localNodeId()))));\n\n            crd = srvNodes.isEmpty() ? null : srvNodes.get(0);\n\n            boolean crdNode = crd != null && crd.isLocal();\n\n            skipPreload = cctx.kernalContext().clientNode();\n\n            ExchangeType exchange;\n\n            if (discoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                DiscoveryCustomMessage msg = ((DiscoveryCustomEvent)discoEvt).customMessage();\n\n                if (msg instanceof DynamicCacheChangeBatch){\n                    assert !F.isEmpty(reqs);\n\n                    exchange = onCacheChangeRequest(crdNode);\n                }\n                else if (msg instanceof StartFullSnapshotAckDiscoveryMessage)\n                    exchange = CU.clientNode(discoEvt.eventNode()) ?\n                        onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                else {\n                    assert affChangeMsg != null : this;\n\n                    exchange = onAffinityChangeRequest(crdNode);\n                }\n            }\n            else {\n                if (discoEvt.type() == EVT_NODE_JOINED) {\n                    Collection<DynamicCacheDescriptor> receivedCaches = cctx.cache().startReceivedCaches(topVer);\n\n                    if (!discoEvt.eventNode().isLocal())\n                        cctx.affinity().initStartedCaches(crdNode, this, receivedCaches);\n                }\n\n                exchange = CU.clientNode(discoEvt.eventNode()) ?\n                    onClientNodeEvent(crdNode) :\n                    onServerNodeEvent(crdNode);\n            }\n\n            updateTopologies(crdNode);\n\n            switch (exchange) {\n                case ALL: {\n                    distributedExchange();\n\n                    break;\n                }\n\n                case CLIENT: {\n                    initTopologies();\n\n                    clientOnlyExchange();\n\n                    break;\n                }\n\n                case NONE: {\n                    initTopologies();\n\n                    onDone(topVer);\n\n                    break;\n                }\n\n                default:\n                    assert false;\n            }\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            onDone(e);\n\n            throw e;\n        }\n        catch (Throwable e) {\n            U.error(log, \"Failed to reinitialize local partitions (preloading will be stopped): \" + exchId, e);\n\n            onDone(e);\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n    }"
        ],
        [
            "GridCacheProcessor::onExchangeDone(AffinityTopologyVersion,Collection,Throwable)",
            "1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949 -\n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967 -\n1968  \n1969  \n1970  \n1971  \n1972  \n1973  ",
            "    /**\n     * Callback invoked when first exchange future for dynamic cache is completed.\n     *\n     * @param topVer Completed topology version.\n     * @param reqs Change requests.\n     * @param err Error.\n     */\n    @SuppressWarnings(\"unchecked\")\n    public void onExchangeDone(\n        AffinityTopologyVersion topVer,\n        Collection<DynamicCacheChangeRequest> reqs,\n        Throwable err\n    ) {\n        for (GridCacheAdapter<?, ?> cache : caches.values()) {\n            GridCacheContext<?, ?> cacheCtx = cache.context();\n\n            if (F.eq(cacheCtx.startTopologyVersion(), topVer)) {\n                if (cacheCtx.preloader() != null)\n                    cacheCtx.preloader().onInitialExchangeComplete(err);\n\n                String masked = maskNull(cacheCtx.name());\n\n                jCacheProxies.putIfAbsent(masked, new IgniteCacheProxy(cache.context(), cache, null, false));\n            }\n        }\n\n        if (!F.isEmpty(reqs) && err == null) {\n            for (DynamicCacheChangeRequest req : reqs) {\n                String masked = maskNull(req.cacheName());\n\n                if (req.stop()) {\n                    stopGateway(req);\n\n                    prepareCacheStop(req);\n                }\n                else if (req.close() && req.initiatingNodeId().equals(ctx.localNodeId())) {\n                    IgniteCacheProxy<?, ?> proxy = jCacheProxies.remove(masked);\n\n                    if (proxy != null) {\n                        if (proxy.context().affinityNode()) {\n                            GridCacheAdapter<?, ?> cache = caches.get(masked);\n\n                            if (cache != null)\n                                jCacheProxies.putIfAbsent(masked, new IgniteCacheProxy(cache.context(), cache, null, false));\n                        }\n                        else {\n                            if (req.restart())\n                                proxy.restart();\n\n                            proxy.context().gate().onStopped();\n\n                            prepareCacheStop(req);\n                        }\n                    }\n                }\n            }\n        }\n    }",
            "1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939 +\n1940 +\n1941  \n1942  \n1943  \n1944 +\n1945 +\n1946 +\n1947  \n1948  \n1949  \n1950 +\n1951 +\n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969 +\n1970 +\n1971  \n1972  \n1973  \n1974 +\n1975 +\n1976 +\n1977 +\n1978 +\n1979 +\n1980 +\n1981  \n1982 +\n1983 +\n1984 +\n1985  \n1986  ",
            "    /**\n     * Callback invoked when first exchange future for dynamic cache is completed.\n     *\n     * @param topVer Completed topology version.\n     * @param reqs Change requests.\n     * @param err Error.\n     */\n    @SuppressWarnings(\"unchecked\")\n    public void onExchangeDone(\n        AffinityTopologyVersion topVer,\n        Collection<DynamicCacheChangeRequest> reqs,\n        Throwable err\n    ) {\n        for (GridCacheAdapter<?, ?> cache : caches.values()) {\n            GridCacheContext<?, ?> cacheCtx = cache.context();\n\n            if (F.eq(cacheCtx.startTopologyVersion(), topVer)) {\n                if (cacheCtx.preloader() != null)\n                    cacheCtx.preloader().onInitialExchangeComplete(err);\n\n                String masked = maskNull(cacheCtx.name());\n\n                jCacheProxies.putIfAbsent(masked, new IgniteCacheProxy(cache.context(), cache, null, false));\n            }\n        }\n\n        if (!F.isEmpty(reqs) && err == null) {\n            Collection<IgniteBiTuple<GridCacheContext, Boolean>> stopped = null;\n\n            for (DynamicCacheChangeRequest req : reqs) {\n                String masked = maskNull(req.cacheName());\n\n                GridCacheContext<?, ?> stopCtx = null;\n                boolean destroy = false;\n\n                if (req.stop()) {\n                    stopGateway(req);\n\n                    stopCtx = prepareCacheStop(req);\n                    destroy = req.destroy();\n                }\n                else if (req.close() && req.initiatingNodeId().equals(ctx.localNodeId())) {\n                    IgniteCacheProxy<?, ?> proxy = jCacheProxies.remove(masked);\n\n                    if (proxy != null) {\n                        if (proxy.context().affinityNode()) {\n                            GridCacheAdapter<?, ?> cache = caches.get(masked);\n\n                            if (cache != null)\n                                jCacheProxies.putIfAbsent(masked, new IgniteCacheProxy(cache.context(), cache, null, false));\n                        }\n                        else {\n                            if (req.restart())\n                                proxy.restart();\n\n                            proxy.context().gate().onStopped();\n\n                            stopCtx = prepareCacheStop(req);\n                            destroy = req.destroy();\n                        }\n                    }\n                }\n\n                if (stopCtx != null) {\n                    if (stopped == null)\n                        stopped = new ArrayList<>();\n\n                    stopped.add(F.<GridCacheContext, Boolean>t(stopCtx, destroy));\n                }\n            }\n\n            if (stopped != null)\n                sharedCtx.database().onCachesStopped(stopped);\n        }\n    }"
        ],
        [
            "GridCacheProcessor::stopCache(GridCacheAdapter,boolean,boolean)",
            "1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251 -\n1252 -\n1253 -\n1254 -\n1255 -\n1256 -\n1257 -\n1258 -\n1259 -\n1260 -\n1261  \n1262  ",
            "    /**\n     * @param cache Cache to stop.\n     * @param cancel Cancel flag.\n     */\n    @SuppressWarnings({\"TypeMayBeWeakened\", \"unchecked\"})\n    private void stopCache(GridCacheAdapter<?, ?> cache, boolean cancel, boolean destroy) {\n        GridCacheContext ctx = cache.context();\n\n        if (!cache.isNear() && ctx.shared().wal() != null) {\n            try {\n                ctx.shared().wal().fsync(null);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to flush write-ahead log on cache stop \" +\n                    \"[cache=\" + ctx.name() + \"]\", e);\n            }\n        }\n\n        sharedCtx.removeCacheContext(ctx);\n\n        cache.stop();\n\n        ctx.kernalContext().query().onCacheStop(ctx);\n\n        if (isNearEnabled(ctx)) {\n            GridDhtCacheAdapter dht = ctx.near().dht();\n\n            // Check whether dht cache has been started.\n            if (dht != null) {\n                dht.stop();\n\n                GridCacheContext<?, ?> dhtCtx = dht.context();\n\n                List<GridCacheManager> dhtMgrs = dhtManagers(dhtCtx);\n\n                for (ListIterator<GridCacheManager> it = dhtMgrs.listIterator(dhtMgrs.size()); it.hasPrevious(); ) {\n                    GridCacheManager mgr = it.previous();\n\n                    mgr.stop(cancel, destroy);\n                }\n            }\n        }\n\n        List<GridCacheManager> mgrs = ctx.managers();\n\n        Collection<GridCacheManager> excludes = dhtExcludes(ctx);\n\n        // Reverse order.\n        for (ListIterator<GridCacheManager> it = mgrs.listIterator(mgrs.size()); it.hasPrevious(); ) {\n            GridCacheManager mgr = it.previous();\n\n            if (!excludes.contains(mgr))\n                mgr.stop(cancel, destroy);\n        }\n\n        ctx.kernalContext().continuous().onCacheStop(ctx);\n\n        ctx.kernalContext().cache().context().database().onCacheStop(ctx);\n\n        U.stopLifecycleAware(log, lifecycleAwares(cache.configuration(), ctx.store().configuredStore()));\n\n        if (log.isInfoEnabled())\n            log.info(\"Stopped cache: \" + cache.name());\n\n        if (sharedCtx.pageStore() != null) {\n            try {\n                sharedCtx.pageStore().shutdownForCache(ctx, destroy);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to gracefully clean page store resources for destroyed cache \" +\n                    \"[cache=\" + ctx.name() + \"]\", e);\n            }\n        }\n\n        cleanup(ctx);\n    }",
            "1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  ",
            "    /**\n     * @param cache Cache to stop.\n     * @param cancel Cancel flag.\n     */\n    @SuppressWarnings({\"TypeMayBeWeakened\", \"unchecked\"})\n    private void stopCache(GridCacheAdapter<?, ?> cache, boolean cancel, boolean destroy) {\n        GridCacheContext ctx = cache.context();\n\n        if (!cache.isNear() && ctx.shared().wal() != null) {\n            try {\n                ctx.shared().wal().fsync(null);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to flush write-ahead log on cache stop \" +\n                    \"[cache=\" + ctx.name() + \"]\", e);\n            }\n        }\n\n        sharedCtx.removeCacheContext(ctx);\n\n        cache.stop();\n\n        ctx.kernalContext().query().onCacheStop(ctx);\n\n        if (isNearEnabled(ctx)) {\n            GridDhtCacheAdapter dht = ctx.near().dht();\n\n            // Check whether dht cache has been started.\n            if (dht != null) {\n                dht.stop();\n\n                GridCacheContext<?, ?> dhtCtx = dht.context();\n\n                List<GridCacheManager> dhtMgrs = dhtManagers(dhtCtx);\n\n                for (ListIterator<GridCacheManager> it = dhtMgrs.listIterator(dhtMgrs.size()); it.hasPrevious(); ) {\n                    GridCacheManager mgr = it.previous();\n\n                    mgr.stop(cancel, destroy);\n                }\n            }\n        }\n\n        List<GridCacheManager> mgrs = ctx.managers();\n\n        Collection<GridCacheManager> excludes = dhtExcludes(ctx);\n\n        // Reverse order.\n        for (ListIterator<GridCacheManager> it = mgrs.listIterator(mgrs.size()); it.hasPrevious(); ) {\n            GridCacheManager mgr = it.previous();\n\n            if (!excludes.contains(mgr))\n                mgr.stop(cancel, destroy);\n        }\n\n        ctx.kernalContext().continuous().onCacheStop(ctx);\n\n        ctx.kernalContext().cache().context().database().onCacheStop(ctx);\n\n        U.stopLifecycleAware(log, lifecycleAwares(cache.configuration(), ctx.store().configuredStore()));\n\n        if (log.isInfoEnabled())\n            log.info(\"Stopped cache: \" + cache.name());\n\n        cleanup(ctx);\n    }"
        ]
    ],
    "8613c1634091576ee309643d2febefc39bf9b333": [
        [
            "JavaStandaloneIgniteRDDSelfTest::testQueryFieldsFromIgnite()",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 -\n 220  \n 221  \n 222  \n 223  \n 224 -\n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234  \n 235  \n 236  \n 237 -\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testQueryFieldsFromIgnite() throws Exception {\n        JavaSparkContext sc = new JavaSparkContext(\"local[*]\", \"test\");\n\n        try {\n            JavaIgniteContext<String, Entity> ic = new JavaIgniteContext<>(sc, new IgniteConfigProvider());\n\n            JavaIgniteRDD<String, Entity> cache = ic.fromCache(PARTITIONED_CACHE_NAME);\n\n            cache.savePairs(sc.parallelize(F.range(0, 1001), 2).mapToPair(INT_TO_ENTITY_F));\n\n            DataFrame df =\n                cache.sql(\"select id, name, salary from Entity where name = ? and salary = ?\", \"name50\", 5000);\n\n            df.printSchema();\n\n            Row[] res = df.collect();\n\n            assertEquals(\"Invalid result length\", 1, res.length);\n            assertEquals(\"Invalid result\", 50, res[0].get(0));\n            assertEquals(\"Invalid result\", \"name50\", res[0].get(1));\n            assertEquals(\"Invalid result\", 5000, res[0].get(2));\n\n            Column exp = new Column(\"NAME\").equalTo(\"name50\").and(new Column(\"SALARY\").equalTo(5000));\n\n            DataFrame df0 = cache.sql(\"select id, name, salary from Entity\").where(exp);\n\n            df.printSchema();\n\n            Row[] res0 = df0.collect();\n\n            assertEquals(\"Invalid result length\", 1, res0.length);\n            assertEquals(\"Invalid result\", 50, res0[0].get(0));\n            assertEquals(\"Invalid result\", \"name50\", res0[0].get(1));\n            assertEquals(\"Invalid result\", 5000, res0[0].get(2));\n\n            assertEquals(\"Invalid count\", 500, cache.sql(\"select id from Entity where id > 500\").count());\n        }\n        finally {\n            sc.stop();\n        }\n    }",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220  \n 221  \n 222  \n 223  \n 224 +\n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 +\n 234  \n 235  \n 236  \n 237 +\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testQueryFieldsFromIgnite() throws Exception {\n        JavaSparkContext sc = new JavaSparkContext(\"local[*]\", \"test\");\n\n        try {\n            JavaIgniteContext<String, Entity> ic = new JavaIgniteContext<>(sc, new IgniteConfigProvider());\n\n            JavaIgniteRDD<String, Entity> cache = ic.fromCache(PARTITIONED_CACHE_NAME);\n\n            cache.savePairs(sc.parallelize(F.range(0, 1001), 2).mapToPair(INT_TO_ENTITY_F));\n\n            Dataset<Row> df =\n                cache.sql(\"select id, name, salary from Entity where name = ? and salary = ?\", \"name50\", 5000);\n\n            df.printSchema();\n\n            Row[] res = (Row[])df.collect();\n\n            assertEquals(\"Invalid result length\", 1, res.length);\n            assertEquals(\"Invalid result\", 50, res[0].get(0));\n            assertEquals(\"Invalid result\", \"name50\", res[0].get(1));\n            assertEquals(\"Invalid result\", 5000, res[0].get(2));\n\n            Column exp = new Column(\"NAME\").equalTo(\"name50\").and(new Column(\"SALARY\").equalTo(5000));\n\n            Dataset<Row> df0 = cache.sql(\"select id, name, salary from Entity\").where(exp);\n\n            df.printSchema();\n\n            Row[] res0 = (Row[])df0.collect();\n\n            assertEquals(\"Invalid result length\", 1, res0.length);\n            assertEquals(\"Invalid result\", 50, res0[0].get(0));\n            assertEquals(\"Invalid result\", \"name50\", res0[0].get(1));\n            assertEquals(\"Invalid result\", 5000, res0[0].get(2));\n\n            assertEquals(\"Invalid count\", 500, cache.sql(\"select id from Entity where id > 500\").count());\n        }\n        finally {\n            sc.stop();\n        }\n    }"
        ],
        [
            "JavaEmbeddedIgniteRDDSelfTest::testQueryFieldsFromIgnite()",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 -\n 241  \n 242  \n 243  \n 244  \n 245 -\n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 -\n 255  \n 256  \n 257  \n 258 -\n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testQueryFieldsFromIgnite() throws Exception {\n        JavaSparkContext sc = createContext();\n\n        JavaIgniteContext<String, Entity> ic = null;\n\n        try {\n            ic = new JavaIgniteContext<>(sc, new IgniteConfigProvider(), false);\n\n            JavaIgniteRDD<String, Entity> cache = ic.fromCache(PARTITIONED_CACHE_NAME);\n\n            cache.savePairs(sc.parallelize(F.range(0, 1001), GRID_CNT).mapToPair(INT_TO_ENTITY_F), true);\n\n            DataFrame df =\n                cache.sql(\"select id, name, salary from Entity where name = ? and salary = ?\", \"name50\", 5000);\n\n            df.printSchema();\n\n            Row[] res = df.collect();\n\n            assertEquals(\"Invalid result length\", 1, res.length);\n            assertEquals(\"Invalid result\", 50, res[0].get(0));\n            assertEquals(\"Invalid result\", \"name50\", res[0].get(1));\n            assertEquals(\"Invalid result\", 5000, res[0].get(2));\n\n            Column exp = new Column(\"NAME\").equalTo(\"name50\").and(new Column(\"SALARY\").equalTo(5000));\n\n            DataFrame df0 = cache.sql(\"select id, name, salary from Entity\").where(exp);\n\n            df.printSchema();\n\n            Row[] res0 = df0.collect();\n\n            assertEquals(\"Invalid result length\", 1, res0.length);\n            assertEquals(\"Invalid result\", 50, res0[0].get(0));\n            assertEquals(\"Invalid result\", \"name50\", res0[0].get(1));\n            assertEquals(\"Invalid result\", 5000, res0[0].get(2));\n\n            assertEquals(\"Invalid count\", 500, cache.sql(\"select id from Entity where id > 500\").count());\n        }\n        finally {\n            if (ic != null)\n                ic.close(true);\n\n            sc.stop();\n        }\n    }",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 +\n 241  \n 242  \n 243  \n 244  \n 245 +\n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 +\n 255  \n 256  \n 257  \n 258 +\n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testQueryFieldsFromIgnite() throws Exception {\n        JavaSparkContext sc = createContext();\n\n        JavaIgniteContext<String, Entity> ic = null;\n\n        try {\n            ic = new JavaIgniteContext<>(sc, new IgniteConfigProvider(), false);\n\n            JavaIgniteRDD<String, Entity> cache = ic.fromCache(PARTITIONED_CACHE_NAME);\n\n            cache.savePairs(sc.parallelize(F.range(0, 1001), GRID_CNT).mapToPair(INT_TO_ENTITY_F), true);\n\n            Dataset<Row> df =\n                cache.sql(\"select id, name, salary from Entity where name = ? and salary = ?\", \"name50\", 5000);\n\n            df.printSchema();\n\n            Row[] res = (Row[])df.collect();\n\n            assertEquals(\"Invalid result length\", 1, res.length);\n            assertEquals(\"Invalid result\", 50, res[0].get(0));\n            assertEquals(\"Invalid result\", \"name50\", res[0].get(1));\n            assertEquals(\"Invalid result\", 5000, res[0].get(2));\n\n            Column exp = new Column(\"NAME\").equalTo(\"name50\").and(new Column(\"SALARY\").equalTo(5000));\n\n            Dataset<Row> df0 = cache.sql(\"select id, name, salary from Entity\").where(exp);\n\n            df.printSchema();\n\n            Row[] res0 = (Row[])df0.collect();\n\n            assertEquals(\"Invalid result length\", 1, res0.length);\n            assertEquals(\"Invalid result\", 50, res0[0].get(0));\n            assertEquals(\"Invalid result\", \"name50\", res0[0].get(1));\n            assertEquals(\"Invalid result\", 5000, res0[0].get(2));\n\n            assertEquals(\"Invalid count\", 500, cache.sql(\"select id from Entity where id > 500\").count());\n        }\n        finally {\n            if (ic != null)\n                ic.close(true);\n\n            sc.stop();\n        }\n    }"
        ],
        [
            "SharedRDDExample::main(String)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "    /**\n     * Executes the example.\n     * @param args Command line arguments, none required.\n     */\n    public static void main(String args[]) {\n        // Spark Configuration.\n        SparkConf sparkConf = new SparkConf()\n            .setAppName(\"JavaIgniteRDDExample\")\n            .setMaster(\"local\")\n            .set(\"spark.executor.instances\", \"2\");\n\n        // Spark context.\n        JavaSparkContext sparkContext = new JavaSparkContext(sparkConf);\n\n        // Adjust the logger to exclude the logs of no interest.\n        Logger.getRootLogger().setLevel(Level.ERROR);\n        Logger.getLogger(\"org.apache.ignite\").setLevel(Level.INFO);\n\n        // Creates Ignite context with specific configuration and runs Ignite in the embedded mode.\n        JavaIgniteContext<Integer, Integer> igniteContext = new JavaIgniteContext<Integer, Integer>(\n            sparkContext,\"examples/config/spark/example-shared-rdd.xml\", false);\n\n        // Create a Java Ignite RDD of Type (Int,Int) Integer Pair.\n        JavaIgniteRDD<Integer, Integer> sharedRDD = igniteContext.<Integer, Integer>fromCache(\"sharedRDD\");\n\n        // Define data to be stored in the Ignite RDD (cache).\n        List<Integer> data = IntStream.range(0, 20).boxed().collect(Collectors.toList());\n\n        // Preparing a Java RDD.\n        JavaRDD<Integer> javaRDD = sparkContext.<Integer>parallelize(data);\n\n        // Fill the Ignite RDD in with Int pairs. Here Pairs are represented as Scala Tuple2.\n        sharedRDD.savePairs(javaRDD.<Integer, Integer>mapToPair(new PairFunction<Integer, Integer, Integer>() {\n            @Override public Tuple2<Integer, Integer> call(Integer val) throws Exception {\n                return new Tuple2<Integer, Integer>(val, val);\n            }\n        }));\n\n        System.out.println(\">>> Iterating over Ignite Shared RDD...\");\n\n        // Iterate over the Ignite RDD.\n        sharedRDD.foreach((x) -> System.out.println(\"(\" + x._1 + \",\" + x._2 + \")\"));\n\n        System.out.println(\">>> Transforming values stored in Ignite Shared RDD...\");\n\n        // Filter out even values as a transformed RDD.\n        JavaPairRDD<Integer, Integer> transformedValues =\n            sharedRDD.filter((Tuple2<Integer, Integer> pair) -> pair._2() % 2 == 0);\n\n        // Print out the transformed values.\n        transformedValues.foreach((x) -> System.out.println(\"(\" + x._1 + \",\" + x._2 + \")\"));\n\n        System.out.println(\">>> Executing SQL query over Ignite Shared RDD...\");\n\n        // Execute SQL query over the Ignite RDD.\n        DataFrame df = sharedRDD.sql(\"select _val from Integer where _key < 9\");\n\n        // Show the result of the execution.\n        df.show();\n\n        // Close IgniteContext on all the workers.\n        igniteContext.close(true);\n    }",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "    /**\n     * Executes the example.\n     * @param args Command line arguments, none required.\n     */\n    public static void main(String args[]) {\n        // Spark Configuration.\n        SparkConf sparkConf = new SparkConf()\n            .setAppName(\"JavaIgniteRDDExample\")\n            .setMaster(\"local\")\n            .set(\"spark.executor.instances\", \"2\");\n\n        // Spark context.\n        JavaSparkContext sparkContext = new JavaSparkContext(sparkConf);\n\n        // Adjust the logger to exclude the logs of no interest.\n        Logger.getRootLogger().setLevel(Level.ERROR);\n        Logger.getLogger(\"org.apache.ignite\").setLevel(Level.INFO);\n\n        // Creates Ignite context with specific configuration and runs Ignite in the embedded mode.\n        JavaIgniteContext<Integer, Integer> igniteContext = new JavaIgniteContext<Integer, Integer>(\n            sparkContext,\"examples/config/spark/example-shared-rdd.xml\", false);\n\n        // Create a Java Ignite RDD of Type (Int,Int) Integer Pair.\n        JavaIgniteRDD<Integer, Integer> sharedRDD = igniteContext.<Integer, Integer>fromCache(\"sharedRDD\");\n\n        // Define data to be stored in the Ignite RDD (cache).\n        List<Integer> data = IntStream.range(0, 20).boxed().collect(Collectors.toList());\n\n        // Preparing a Java RDD.\n        JavaRDD<Integer> javaRDD = sparkContext.<Integer>parallelize(data);\n\n        // Fill the Ignite RDD in with Int pairs. Here Pairs are represented as Scala Tuple2.\n        sharedRDD.savePairs(javaRDD.<Integer, Integer>mapToPair(new PairFunction<Integer, Integer, Integer>() {\n            @Override public Tuple2<Integer, Integer> call(Integer val) throws Exception {\n                return new Tuple2<Integer, Integer>(val, val);\n            }\n        }));\n\n        System.out.println(\">>> Iterating over Ignite Shared RDD...\");\n\n        // Iterate over the Ignite RDD.\n        sharedRDD.foreach((x) -> System.out.println(\"(\" + x._1 + \",\" + x._2 + \")\"));\n\n        System.out.println(\">>> Transforming values stored in Ignite Shared RDD...\");\n\n        // Filter out even values as a transformed RDD.\n        JavaPairRDD<Integer, Integer> transformedValues =\n            sharedRDD.filter((Tuple2<Integer, Integer> pair) -> pair._2() % 2 == 0);\n\n        // Print out the transformed values.\n        transformedValues.foreach((x) -> System.out.println(\"(\" + x._1 + \",\" + x._2 + \")\"));\n\n        System.out.println(\">>> Executing SQL query over Ignite Shared RDD...\");\n\n        // Execute SQL query over the Ignite RDD.\n        Dataset df = sharedRDD.sql(\"select _val from Integer where _key < 9\");\n\n        // Show the result of the execution.\n        df.show();\n\n        // Close IgniteContext on all the workers.\n        igniteContext.close(true);\n    }"
        ],
        [
            "JavaStandaloneIgniteRDDSelfTest::testAllFieldsTypes()",
            " 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 -\n 273  \n 274  \n 275  \n 276  \n 277 -\n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284 -\n 285  \n 286  \n 287  \n 288 -\n 289 -\n 290 -\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testAllFieldsTypes() throws Exception {\n        JavaSparkContext sc = new JavaSparkContext(\"local[*]\", \"test\");\n\n        final int cnt = 100;\n\n        try {\n            JavaIgniteContext<String, EntityTestAllTypeFields> ic = new JavaIgniteContext<>(sc, new IgniteConfigProvider());\n\n            JavaIgniteRDD<String, EntityTestAllTypeFields> cache = ic.fromCache(PARTITIONED_CACHE_NAME);\n\n            cache.savePairs(sc.parallelize(F.range(0, cnt), 2).mapToPair(INT_TO_ENTITY_ALL_FIELDS_F));\n\n            EntityTestAllTypeFields e = new EntityTestAllTypeFields(cnt / 2);\n            for(Field f : EntityTestAllTypeFields.class.getDeclaredFields()) {\n                String fieldName = f.getName();\n\n                Object val = GridTestUtils.getFieldValue(e, fieldName);\n\n                DataFrame df = cache.sql(\n                    String.format(\"select %s from EntityTestAllTypeFields where %s = ?\", fieldName, fieldName),\n                    val);\n\n                if (val instanceof BigDecimal) {\n                    Object res = df.collect()[0].get(0);\n\n                    assertTrue(String.format(\"+++ Fail on %s field\", fieldName),\n                        ((Comparable<BigDecimal>)val).compareTo((BigDecimal)res) == 0);\n                }\n                else if (val instanceof java.sql.Date)\n                    assertEquals(String.format(\"+++ Fail on %s field\", fieldName),\n                        val.toString(), df.collect()[0].get(0).toString());\n                else if (val.getClass().isArray())\n                    assertTrue(String.format(\"+++ Fail on %s field\", fieldName), 1 <= df.count());\n                else {\n                    assertTrue(String.format(\"+++ Fail on %s field\", fieldName), df.collect().length > 0);\n                    assertTrue(String.format(\"+++ Fail on %s field\", fieldName), df.collect()[0].size() > 0);\n                    assertEquals(String.format(\"+++ Fail on %s field\", fieldName), val, df.collect()[0].get(0));\n                }\n\n                info(String.format(\"+++ Query on the filed: %s : %s passed\", fieldName, f.getType().getSimpleName()));\n            }\n        }\n        finally {\n            sc.stop();\n        }\n    }",
            " 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 +\n 273  \n 274  \n 275  \n 276  \n 277 +\n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284 +\n 285  \n 286  \n 287  \n 288 +\n 289 +\n 290 +\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testAllFieldsTypes() throws Exception {\n        JavaSparkContext sc = new JavaSparkContext(\"local[*]\", \"test\");\n\n        final int cnt = 100;\n\n        try {\n            JavaIgniteContext<String, EntityTestAllTypeFields> ic = new JavaIgniteContext<>(sc, new IgniteConfigProvider());\n\n            JavaIgniteRDD<String, EntityTestAllTypeFields> cache = ic.fromCache(PARTITIONED_CACHE_NAME);\n\n            cache.savePairs(sc.parallelize(F.range(0, cnt), 2).mapToPair(INT_TO_ENTITY_ALL_FIELDS_F));\n\n            EntityTestAllTypeFields e = new EntityTestAllTypeFields(cnt / 2);\n            for(Field f : EntityTestAllTypeFields.class.getDeclaredFields()) {\n                String fieldName = f.getName();\n\n                Object val = GridTestUtils.getFieldValue(e, fieldName);\n\n                Dataset<Row> df = cache.sql(\n                    String.format(\"select %s from EntityTestAllTypeFields where %s = ?\", fieldName, fieldName),\n                    val);\n\n                if (val instanceof BigDecimal) {\n                    Object res = ((Row[])df.collect())[0].get(0);\n\n                    assertTrue(String.format(\"+++ Fail on %s field\", fieldName),\n                        ((Comparable<BigDecimal>)val).compareTo((BigDecimal)res) == 0);\n                }\n                else if (val instanceof java.sql.Date)\n                    assertEquals(String.format(\"+++ Fail on %s field\", fieldName),\n                        val.toString(), ((Row[])df.collect())[0].get(0).toString());\n                else if (val.getClass().isArray())\n                    assertTrue(String.format(\"+++ Fail on %s field\", fieldName), 1 <= df.count());\n                else {\n                    assertTrue(String.format(\"+++ Fail on %s field\", fieldName), ((Row[])df.collect()).length > 0);\n                    assertTrue(String.format(\"+++ Fail on %s field\", fieldName), ((Row[])df.collect())[0].size() > 0);\n                    assertEquals(String.format(\"+++ Fail on %s field\", fieldName), val, ((Row[])df.collect())[0].get(0));\n                }\n\n                info(String.format(\"+++ Query on the filed: %s : %s passed\", fieldName, f.getType().getSimpleName()));\n            }\n        }\n        finally {\n            sc.stop();\n        }\n    }"
        ]
    ],
    "1924873b26d0704eb188faa3d6f4e6a4730ac126": [
        [
            "RebalancingOnNotStableTopologyTest::test()",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 -\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "    /**\n     * @throws Exception When fails.\n     */\n    public void test() throws Exception {\n        stopAllGrids();\n\n        Ignite ex = startGrid(0);\n\n        startGrid(1);\n\n        final CountDownLatch startLatch = new CountDownLatch(1);\n        final CountDownLatch doneLatch = new CountDownLatch(1);\n\n        final Ignite ex1 = ex;\n\n        final AtomicBoolean stop = new AtomicBoolean();\n        final AtomicInteger keyCnt = new AtomicInteger();\n\n        Thread thread = new Thread(new Runnable() {\n            @Override public void run() {\n                ex1.active(true);\n\n                try {\n                    checkTopology(2);\n\n                    startLatch.countDown();\n\n                    IgniteCache<Object, Object> cache1 = ex1.cache(\"cache1\");\n\n                    int key = keyCnt.get();\n\n                    while (!stop.get()) {\n                        if (key > 0 && (key % 500 == 0)) {\n                            U.sleep(5);\n\n                            System.out.println(\"key = \" + key);\n                        }\n\n                        cache1.put(key, -key);\n\n                        key = keyCnt.incrementAndGet();\n                    }\n                }\n                catch (Throwable th) {\n                    th.printStackTrace();\n                }\n\n                doneLatch.countDown();\n            }\n        });\n\n        thread.setName(\"Data-Loader\");\n        thread.start();\n\n        startLatch.await(60, TimeUnit.SECONDS);\n\n        for (int i = 2; i < CLUSTER_SIZE; i++) {\n            startGrid(i);\n\n            U.sleep(5000);\n        }\n\n        U.sleep(10000);\n\n        IgniteProcessProxy.kill(\"db.RebalancingOnNotStableTopologyTest2\");\n\n        Thread.sleep(5000);\n\n        IgniteProcessProxy.kill(\"db.RebalancingOnNotStableTopologyTest1\");\n\n        assert doneLatch.getCount() > 0;\n\n        stop.set(true);\n\n        doneLatch.await(600, TimeUnit.SECONDS);\n\n        IgniteProcessProxy.killAll();\n\n        stopAllGrids();\n\n        //start cluster. it will cause memory restoration and reading WAL.\n        ex = startGrids(CLUSTER_SIZE);\n\n        ex.active(true);\n\n        checkTopology(CLUSTER_SIZE);\n\n        IgniteCache<Object, Object> cache1 = ex.cache(\"cache1\");\n\n        assert keyCnt.get() > 0;\n\n        for (int i = 0; i < keyCnt.get(); i++)\n            assertEquals(-i, cache1.get(i));\n\n        System.out.println(\"Test finished with total keys count = \" + keyCnt.get());\n    }",
            "  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "    /**\n     * @throws Exception When fails.\n     */\n    public void test() throws Exception {\n        stopAllGrids();\n\n        Ignite ex = startGrid(0);\n\n        startGrid(1);\n\n        final CountDownLatch startLatch = new CountDownLatch(1);\n        final CountDownLatch doneLatch = new CountDownLatch(1);\n\n        final Ignite ex1 = ex;\n\n        final AtomicBoolean stop = new AtomicBoolean();\n        final AtomicInteger keyCnt = new AtomicInteger();\n\n        Thread thread = new Thread(new Runnable() {\n            @Override public void run() {\n                ex1.active(true);\n\n                try {\n                    checkTopology(2);\n\n                    startLatch.countDown();\n\n                    IgniteCache<Object, Object> cache1 = ex1.cache(CACHE_NAME);\n\n                    int key = keyCnt.get();\n\n                    while (!stop.get()) {\n                        if (key > 0 && (key % 500 == 0)) {\n                            U.sleep(5);\n\n                            System.out.println(\"key = \" + key);\n                        }\n\n                        cache1.put(key, -key);\n\n                        key = keyCnt.incrementAndGet();\n                    }\n                }\n                catch (Throwable th) {\n                    th.printStackTrace();\n                }\n\n                doneLatch.countDown();\n            }\n        });\n\n        thread.setName(\"Data-Loader\");\n        thread.start();\n\n        startLatch.await(60, TimeUnit.SECONDS);\n\n        for (int i = 2; i < CLUSTER_SIZE; i++) {\n            startGrid(i);\n\n            U.sleep(5000);\n        }\n\n        U.sleep(10000);\n\n        IgniteProcessProxy.kill(\"db.RebalancingOnNotStableTopologyTest2\");\n\n        Thread.sleep(5000);\n\n        IgniteProcessProxy.kill(\"db.RebalancingOnNotStableTopologyTest1\");\n\n        assert doneLatch.getCount() > 0;\n\n        stop.set(true);\n\n        doneLatch.await(600, TimeUnit.SECONDS);\n\n        IgniteProcessProxy.killAll();\n\n        stopAllGrids();\n\n        //start cluster. it will cause memory restoration and reading WAL.\n        ex = startGrids(CLUSTER_SIZE);\n\n        ex.active(true);\n\n        checkTopology(CLUSTER_SIZE);\n\n        IgniteCache<Object, Object> cache1 = ex.cache(CACHE_NAME);\n\n        assert keyCnt.get() > 0;\n\n        for (int i = 0; i < keyCnt.get(); i++)\n            assertEquals(-i, cache1.get(i));\n\n        System.out.println(\"Test finished with total keys count = \" + keyCnt.get());\n    }"
        ],
        [
            "IgniteDbPageEvictionSelfTest::testPageEvictionSql()",
            " 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testPageEvictionSql() throws Exception {\n        IgniteEx ig = grid(0);\n\n        try (IgniteDataStreamer<DbKey, DbValue> streamer = ig.dataStreamer(null)) {\n            for (int i = 0; i < ENTRY_CNT; i++) {\n                streamer.addData(new DbKey(i), new DbValue(i, \"value-\" + i, Long.MAX_VALUE - i));\n\n                if (i > 0 && i % 10_000 == 0)\n                    info(\"Done put: \" + i);\n            }\n        }\n\n        IgniteCache<DbKey, DbValue> cache = ignite(0).cache(null);\n\n        for (int i = 0; i < ENTRY_CNT; i++) {\n            assertEquals(Long.MAX_VALUE - i, cache.get(new DbKey(i)).lVal);\n\n            if (i > 0 && i % 10_000 == 0)\n                info(\"Done get: \" + i);\n        }\n\n        for (int i = 0; i < ENTRY_CNT; i++) {\n            List<List<?>> rows = cache.query(new SqlFieldsQuery(\"select lVal from DbValue where iVal=?\").setArgs(i))\n                .getAll();\n\n            assertEquals(1, rows.size());\n            assertEquals(Long.MAX_VALUE - i, rows.get(0).get(0));\n\n            if (i > 0 && i % 10_000 == 0)\n                info(\"Done SQL query: \" + i);\n        }\n    }",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testPageEvictionSql() throws Exception {\n        IgniteEx ig = grid(0);\n\n        try (IgniteDataStreamer<DbKey, DbValue> streamer = ig.dataStreamer(CACHE_NAME)) {\n            for (int i = 0; i < ENTRY_CNT; i++) {\n                streamer.addData(new DbKey(i), new DbValue(i, \"value-\" + i, Long.MAX_VALUE - i));\n\n                if (i > 0 && i % 10_000 == 0)\n                    info(\"Done put: \" + i);\n            }\n        }\n\n        IgniteCache<DbKey, DbValue> cache = ignite(0).cache(CACHE_NAME);\n\n        for (int i = 0; i < ENTRY_CNT; i++) {\n            assertEquals(Long.MAX_VALUE - i, cache.get(new DbKey(i)).lVal);\n\n            if (i > 0 && i % 10_000 == 0)\n                info(\"Done get: \" + i);\n        }\n\n        for (int i = 0; i < ENTRY_CNT; i++) {\n            List<List<?>> rows = cache.query(new SqlFieldsQuery(\"select lVal from DbValue where iVal=?\").setArgs(i))\n                .getAll();\n\n            assertEquals(1, rows.size());\n            assertEquals(Long.MAX_VALUE - i, rows.get(0).get(0));\n\n            if (i > 0 && i % 10_000 == 0)\n                info(\"Done SQL query: \" + i);\n        }\n    }"
        ],
        [
            "IgniteCachePageStoreIntegrationSelfTest::getConfiguration(String)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 -\n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setConcurrencyLevel(Runtime.getRuntime().availableProcessors() * 4);\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        dbCfg.setMemoryPolicies(memPlcCfg);\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        CacheConfiguration ccfg = new CacheConfiguration();\n\n        ccfg.setIndexedTypes(Integer.class, DbValue.class);\n\n        ccfg.setRebalanceMode(CacheRebalanceMode.NONE);\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL);\n\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        cfg.setCacheConfiguration(ccfg);\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration bCfg = new BinaryConfiguration();\n\n        bCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(bCfg);\n\n        return cfg;\n    }",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setConcurrencyLevel(Runtime.getRuntime().availableProcessors() * 4);\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        dbCfg.setMemoryPolicies(memPlcCfg);\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        CacheConfiguration ccfg = new CacheConfiguration(CACHE_NAME);\n\n        ccfg.setIndexedTypes(Integer.class, DbValue.class);\n\n        ccfg.setRebalanceMode(CacheRebalanceMode.NONE);\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL);\n\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        cfg.setCacheConfiguration(ccfg);\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration bCfg = new BinaryConfiguration();\n\n        bCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(bCfg);\n\n        return cfg;\n    }"
        ],
        [
            "IgniteNoActualWalHistorySelfTest::getConfiguration(String)",
            "  45  \n  46  \n  47  \n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration<Integer, IndexedObject> ccfg = new CacheConfiguration<>();\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.ATOMIC);\n        ccfg.setRebalanceMode(CacheRebalanceMode.SYNC);\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setPageSize(4 * 1024);\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        PersistenceConfiguration pCfg = new PersistenceConfiguration();\n\n        pCfg.setWalSegmentSize(4 * 1024 * 1024);\n        pCfg.setWalHistorySize(2);\n        pCfg.setWalSegments(10);\n\n        cfg.setPersistenceConfiguration(pCfg);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration binCfg = new BinaryConfiguration();\n\n        binCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(binCfg);\n\n        return cfg;\n    }",
            "  48  \n  49  \n  50  \n  51  \n  52 +\n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration<Integer, IndexedObject> ccfg = new CacheConfiguration<>(CACHE_NAME);\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.ATOMIC);\n        ccfg.setRebalanceMode(CacheRebalanceMode.SYNC);\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setPageSize(4 * 1024);\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        PersistenceConfiguration pCfg = new PersistenceConfiguration();\n\n        pCfg.setWalSegmentSize(4 * 1024 * 1024);\n        pCfg.setWalHistorySize(2);\n        pCfg.setWalSegments(10);\n\n        cfg.setPersistenceConfiguration(pCfg);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration binCfg = new BinaryConfiguration();\n\n        binCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(binCfg);\n\n        return cfg;\n    }"
        ],
        [
            "IgnitePersistentStoreWalTlbSelfTest::getConfiguration(String)",
            "  44  \n  45  \n  46  \n  47  \n  48 -\n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration<Integer, Integer> ccfg = new CacheConfiguration<>();\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n        memCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        cfg.setPersistenceConfiguration(\n            new PersistenceConfiguration()\n                .setCheckpointPageBufferSize(DFLT_CHECKPOINT_PAGE_BUFFER_SIZE + 1)\n        );\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        if (gridName.endsWith(\"1\"))\n            cfg.setClientMode(true);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        return cfg;\n    }",
            "  47  \n  48  \n  49  \n  50  \n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration<Integer, Integer> ccfg = new CacheConfiguration<>(CACHE_NAME);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n        memCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        cfg.setPersistenceConfiguration(\n            new PersistenceConfiguration()\n                .setCheckpointPageBufferSize(DFLT_CHECKPOINT_PAGE_BUFFER_SIZE + 1)\n        );\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        if (gridName.endsWith(\"1\"))\n            cfg.setClientMode(true);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        return cfg;\n    }"
        ],
        [
            "RebalancingOnNotStableTopologyTest::getConfiguration(String)",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 -\n 156  \n 157 -\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        cfg.setActiveOnStart(false);\n\n        CacheConfiguration<Integer, Integer> ccfg = new CacheConfiguration<>();\n\n        ccfg.setName(\"cache1\");\n        ccfg.setPartitionLossPolicy(PartitionLossPolicy.READ_ONLY_SAFE);\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n        ccfg.setCacheMode(CacheMode.PARTITIONED);\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n        ccfg.setBackups(2);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        PersistenceConfiguration pCfg = new PersistenceConfiguration();\n\n        pCfg.setCheckpointFrequency(CHECKPOINT_FREQUENCY);\n\n        cfg.setPersistenceConfiguration(pCfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(200 * 1024 * 1024);\n        memPlcCfg.setMaxSize(200 * 1024 * 1024);\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n        memCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        return cfg;\n    }",
            " 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        cfg.setActiveOnStart(false);\n\n        CacheConfiguration<Integer, Integer> ccfg = new CacheConfiguration<>(CACHE_NAME);\n\n        ccfg.setPartitionLossPolicy(PartitionLossPolicy.READ_ONLY_SAFE);\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n        ccfg.setCacheMode(CacheMode.PARTITIONED);\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n        ccfg.setBackups(2);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        PersistenceConfiguration pCfg = new PersistenceConfiguration();\n\n        pCfg.setCheckpointFrequency(CHECKPOINT_FREQUENCY);\n\n        cfg.setPersistenceConfiguration(pCfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(200 * 1024 * 1024);\n        memPlcCfg.setMaxSize(200 * 1024 * 1024);\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n        memCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        return cfg;\n    }"
        ],
        [
            "IgniteDbMultiNodePutGetRestartSelfTest::getConfiguration(String)",
            "  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n        dbCfg.setMemoryPolicies(memPlcCfg);\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        CacheConfiguration ccfg = new CacheConfiguration();\n\n        ccfg.setIndexedTypes(Integer.class, DbValue.class);\n\n        ccfg.setRebalanceMode(CacheRebalanceMode.NONE);\n\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration bCfg = new BinaryConfiguration();\n\n        bCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(bCfg);\n\n        return cfg;\n    }",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78 +\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setInitialSize(100 * 1024 * 1024);\n        memPlcCfg.setMaxSize(100 * 1024 * 1024);\n\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n        dbCfg.setMemoryPolicies(memPlcCfg);\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        CacheConfiguration ccfg = new CacheConfiguration(CACHE_NAME);\n\n        ccfg.setIndexedTypes(Integer.class, DbValue.class);\n\n        ccfg.setRebalanceMode(CacheRebalanceMode.NONE);\n\n        ccfg.setAffinity(new RendezvousAffinityFunction(false, 32));\n\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setMarshaller(null);\n\n        BinaryConfiguration bCfg = new BinaryConfiguration();\n\n        bCfg.setCompactFooter(false);\n\n        cfg.setBinaryConfiguration(bCfg);\n\n        return cfg;\n    }"
        ],
        [
            "WalRecoveryTxLogicalRecordsTest::testRebalanceIterator()",
            " 296  \n 297  \n 298  \n 299  \n 300 -\n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 -\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testRebalanceIterator() throws Exception {\n        extraCcfg = new CacheConfiguration(CACHE_NAME + \"2\");\n        extraCcfg.setAffinity(new RendezvousAffinityFunction(false, PARTS));\n\n        Ignite ignite = startGrid();\n\n        try {\n            GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)((IgniteEx)ignite).context()\n                .cache().context().database();\n\n            dbMgr.waitForCheckpoint(\"test\");\n\n            // This number depends on wal history size.\n            int entries = 25;\n\n            IgniteCache<Integer, Integer> cache = ignite.cache(CACHE_NAME);\n            IgniteCache<Integer, Integer> cache2 = ignite.cache(CACHE_NAME + \"2\");\n\n            for (int i = 0; i < entries; i++) {\n                // Put to partition 0.\n                cache.put(i * PARTS, i * PARTS);\n\n                // Put to partition 1.\n                cache.put(i * PARTS + 1, i * PARTS + 1);\n\n                // Put to another cache.\n                cache2.put(i, i);\n\n                dbMgr.waitForCheckpoint(\"test\");\n            }\n\n            for (int i = 0; i < entries; i++) {\n                assertEquals((Integer)(i * PARTS), cache.get(i * PARTS));\n                assertEquals((Integer)(i * PARTS + 1), cache.get(i * PARTS + 1));\n                assertEquals((Integer)(i), cache2.get(i));\n            }\n\n            GridCacheContext<Object, Object> cctx = ((IgniteEx)ignite).context().cache().cache(CACHE_NAME).context();\n            IgniteCacheOffheapManager offh = cctx.offheap();\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            for (int i = 0; i < entries; i++) {\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(0, topVer, (long)i)) {\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    assertNotNull(it);\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(\"i=\" + i + \", j=\" + j, it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(1, topVer, (long)i)) {\n                    assertNotNull(it);\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS + 1, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS + 1, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n            }\n\n            stopAllGrids();\n\n            // Check that iterator is valid after restart.\n            ignite = startGrid();\n\n            cctx = ((IgniteEx)ignite).context().cache().cache(CACHE_NAME).context();\n            offh = cctx.offheap();\n            topVer = cctx.affinity().affinityTopologyVersion();\n\n            for (int i = 0; i < entries; i++) {\n                long start = System.currentTimeMillis();\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(0, topVer, (long)i)) {\n                    long end = System.currentTimeMillis();\n\n                    info(\"Time to get iterator: \" + (end - start));\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    assertNotNull(it);\n\n                    start = System.currentTimeMillis();\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(\"i=\" + i + \", j=\" + j, it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    end = System.currentTimeMillis();\n\n                    info(\"Time to iterate: \" + (end - start));\n\n                    assertFalse(it.hasNext());\n                }\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(1, topVer, (long)i)) {\n                    assertNotNull(it);\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS + 1, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS + 1, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n            }\n        }\n        finally {\n            stopAllGrids();\n        }\n    }",
            " 299  \n 300  \n 301  \n 302  \n 303 +\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318 +\n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testRebalanceIterator() throws Exception {\n        extraCcfg = new CacheConfiguration(CACHE2_NAME);\n        extraCcfg.setAffinity(new RendezvousAffinityFunction(false, PARTS));\n\n        Ignite ignite = startGrid();\n\n        try {\n            GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)((IgniteEx)ignite).context()\n                .cache().context().database();\n\n            dbMgr.waitForCheckpoint(\"test\");\n\n            // This number depends on wal history size.\n            int entries = 25;\n\n            IgniteCache<Integer, Integer> cache = ignite.cache(CACHE_NAME);\n            IgniteCache<Integer, Integer> cache2 = ignite.cache(CACHE2_NAME);\n\n            for (int i = 0; i < entries; i++) {\n                // Put to partition 0.\n                cache.put(i * PARTS, i * PARTS);\n\n                // Put to partition 1.\n                cache.put(i * PARTS + 1, i * PARTS + 1);\n\n                // Put to another cache.\n                cache2.put(i, i);\n\n                dbMgr.waitForCheckpoint(\"test\");\n            }\n\n            for (int i = 0; i < entries; i++) {\n                assertEquals((Integer)(i * PARTS), cache.get(i * PARTS));\n                assertEquals((Integer)(i * PARTS + 1), cache.get(i * PARTS + 1));\n                assertEquals((Integer)(i), cache2.get(i));\n            }\n\n            GridCacheContext<Object, Object> cctx = ((IgniteEx)ignite).context().cache().cache(CACHE_NAME).context();\n            IgniteCacheOffheapManager offh = cctx.offheap();\n            AffinityTopologyVersion topVer = cctx.affinity().affinityTopologyVersion();\n\n            for (int i = 0; i < entries; i++) {\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(0, topVer, (long)i)) {\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    assertNotNull(it);\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(\"i=\" + i + \", j=\" + j, it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(1, topVer, (long)i)) {\n                    assertNotNull(it);\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS + 1, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS + 1, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n            }\n\n            stopAllGrids();\n\n            // Check that iterator is valid after restart.\n            ignite = startGrid();\n\n            cctx = ((IgniteEx)ignite).context().cache().cache(CACHE_NAME).context();\n            offh = cctx.offheap();\n            topVer = cctx.affinity().affinityTopologyVersion();\n\n            for (int i = 0; i < entries; i++) {\n                long start = System.currentTimeMillis();\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(0, topVer, (long)i)) {\n                    long end = System.currentTimeMillis();\n\n                    info(\"Time to get iterator: \" + (end - start));\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    assertNotNull(it);\n\n                    start = System.currentTimeMillis();\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(\"i=\" + i + \", j=\" + j, it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    end = System.currentTimeMillis();\n\n                    info(\"Time to iterate: \" + (end - start));\n\n                    assertFalse(it.hasNext());\n                }\n\n                try (IgniteRebalanceIterator it = offh.rebalanceIterator(1, topVer, (long)i)) {\n                    assertNotNull(it);\n\n                    assertTrue(\"Not historical for iteration: \" + i, it.historical());\n\n                    for (int j = i; j < entries; j++) {\n                        assertTrue(it.hasNextX());\n\n                        CacheDataRow row = it.next();\n\n                        assertEquals(j * PARTS + 1, (int)row.key().value(cctx.cacheObjectContext(), false));\n                        assertEquals(j * PARTS + 1, (int)row.value().value(cctx.cacheObjectContext(), false));\n                    }\n\n                    assertFalse(it.hasNext());\n                }\n            }\n        }\n        finally {\n            stopAllGrids();\n        }\n    }"
        ],
        [
            "IgniteDbMultiNodePutGetRestartSelfTest::checkPutGetSql(IgniteEx,boolean)",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167 -\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "    /**\n     * @param ig Ig.\n     * @param write Write.\n     */\n    private void checkPutGetSql(IgniteEx ig, boolean write) {\n        IgniteCache<Integer, DbValue> cache = ig.cache(null);\n\n        if (write) {\n            try (IgniteDataStreamer<Object, Object> streamer = ig.dataStreamer(null)) {\n                for (int i = 0; i < 10_000; i++)\n                    streamer.addData(i, new DbValue(i, \"value-\" + i, i));\n            }\n        }\n\n        List<List<?>> res = cache.query(new SqlFieldsQuery(\"select ival from dbvalue where ival < ? order by ival asc\")\n                .setArgs(10_000)).getAll();\n\n        assertEquals(10_000, res.size());\n\n        for (int i = 0; i < 10_000; i++) {\n            assertEquals(1, res.get(i).size());\n            assertEquals(i, res.get(i).get(0));\n        }\n\n        assertEquals(1, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival = 7899\")).getAll().size());\n        assertEquals(5000, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival >= 5000 and ival < 10000\"))\n                .getAll().size());\n\n        for (int i = 0; i < 10_000; i++)\n            assertEquals(new DbValue(i, \"value-\" + i, i), cache.get(i));\n    }",
            " 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170 +\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  ",
            "    /**\n     * @param ig Ig.\n     * @param write Write.\n     */\n    private void checkPutGetSql(IgniteEx ig, boolean write) {\n        IgniteCache<Integer, DbValue> cache = ig.cache(CACHE_NAME);\n\n        if (write) {\n            try (IgniteDataStreamer<Object, Object> streamer = ig.dataStreamer(CACHE_NAME)) {\n                for (int i = 0; i < 10_000; i++)\n                    streamer.addData(i, new DbValue(i, \"value-\" + i, i));\n            }\n        }\n\n        List<List<?>> res = cache.query(new SqlFieldsQuery(\"select ival from dbvalue where ival < ? order by ival asc\")\n                .setArgs(10_000)).getAll();\n\n        assertEquals(10_000, res.size());\n\n        for (int i = 0; i < 10_000; i++) {\n            assertEquals(1, res.get(i).size());\n            assertEquals(i, res.get(i).get(0));\n        }\n\n        assertEquals(1, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival = 7899\")).getAll().size());\n        assertEquals(5000, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival >= 5000 and ival < 10000\"))\n                .getAll().size());\n\n        for (int i = 0; i < 10_000; i++)\n            assertEquals(new DbValue(i, \"value-\" + i, i), cache.get(i));\n    }"
        ],
        [
            "DbPageEvictionDuringPartitionClearSelfTest::getConfiguration(String)",
            "  47  \n  48  \n  49  \n  50  \n  51 -\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration ccfg = new CacheConfiguration(\"cache\")\n            .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)\n            .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)\n            .setAffinity(new RendezvousAffinityFunction(false, 128))\n            .setRebalanceMode(CacheRebalanceMode.SYNC)\n            .setBackups(1);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        // Intentionally set small page cache size.\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setInitialSize(70 * 1024 * 1024);\n        memPlcCfg.setMaxSize(70 * 1024 * 1024);\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n\n        memCfg.setDefaultMemoryPolicyName(memPlcCfg.getName());\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        return cfg;\n    }",
            "  47  \n  48  \n  49  \n  50  \n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration ccfg = new CacheConfiguration(CACHE_NAME)\n            .setAtomicityMode(CacheAtomicityMode.TRANSACTIONAL)\n            .setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC)\n            .setAffinity(new RendezvousAffinityFunction(false, 128))\n            .setRebalanceMode(CacheRebalanceMode.SYNC)\n            .setBackups(1);\n\n        cfg.setCacheConfiguration(ccfg);\n\n        MemoryConfiguration memCfg = new MemoryConfiguration();\n\n        // Intentionally set small page cache size.\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setInitialSize(70 * 1024 * 1024);\n        memPlcCfg.setMaxSize(70 * 1024 * 1024);\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n\n        memCfg.setMemoryPolicies(memPlcCfg);\n\n        memCfg.setDefaultMemoryPolicyName(memPlcCfg.getName());\n\n        cfg.setMemoryConfiguration(memCfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        return cfg;\n    }"
        ],
        [
            "IgniteNoActualWalHistorySelfTest::testWalBig()",
            " 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 -\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testWalBig() throws Exception {\n        try {\n            IgniteEx ignite = startGrid(1);\n\n            IgniteCache<Object, Object> cache = ignite.cache(null);\n\n            Random rnd = new Random();\n\n            Map<Integer, IndexedObject> map = new HashMap<>();\n\n            for (int i = 0; i < 40_000; i++) {\n                if (i % 1000 == 0)\n                    X.println(\" >> \" + i);\n\n                int k = rnd.nextInt(300_000);\n                IndexedObject v = new IndexedObject(rnd.nextInt(10_000));\n\n                cache.put(k, v);\n                map.put(k, v);\n            }\n\n            GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)ignite.context().cache().context()\n                .database();\n\n            // Create many checkpoints to clean up the history.\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n\n            dbMgr.enableCheckpoints(false).get();\n\n            for (int i = 0; i < 50; i++) {\n                int k = rnd.nextInt(300_000);\n                IndexedObject v = new IndexedObject(rnd.nextInt(10_000));\n\n                cache.put(k, v);\n                map.put(k, v);\n            }\n\n            stopGrid(1);\n\n            ignite = startGrid(1);\n\n            cache = ignite.cache(null);\n\n            // Check.\n            for (Integer k : map.keySet())\n                assertEquals(map.get(k), cache.get(k));\n        }\n        finally {\n            stopAllGrids();\n        }\n    }",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testWalBig() throws Exception {\n        try {\n            IgniteEx ignite = startGrid(1);\n\n            IgniteCache<Object, Object> cache = ignite.cache(CACHE_NAME);\n\n            Random rnd = new Random();\n\n            Map<Integer, IndexedObject> map = new HashMap<>();\n\n            for (int i = 0; i < 40_000; i++) {\n                if (i % 1000 == 0)\n                    X.println(\" >> \" + i);\n\n                int k = rnd.nextInt(300_000);\n                IndexedObject v = new IndexedObject(rnd.nextInt(10_000));\n\n                cache.put(k, v);\n                map.put(k, v);\n            }\n\n            GridCacheDatabaseSharedManager dbMgr = (GridCacheDatabaseSharedManager)ignite.context().cache().context()\n                .database();\n\n            // Create many checkpoints to clean up the history.\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n            dbMgr.wakeupForCheckpoint(\"test\").get();\n\n            dbMgr.enableCheckpoints(false).get();\n\n            for (int i = 0; i < 50; i++) {\n                int k = rnd.nextInt(300_000);\n                IndexedObject v = new IndexedObject(rnd.nextInt(10_000));\n\n                cache.put(k, v);\n                map.put(k, v);\n            }\n\n            stopGrid(1);\n\n            ignite = startGrid(1);\n\n            cache = ignite.cache(CACHE_NAME);\n\n            // Check.\n            for (Integer k : map.keySet())\n                assertEquals(map.get(k), cache.get(k));\n        }\n        finally {\n            stopAllGrids();\n        }\n    }"
        ],
        [
            "IgniteCachePageStoreIntegrationSelfTest::checkPutGetSql(Ignite,boolean)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186 -\n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 -\n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "    /**\n     * @param ig Ignite instance.\n     * @param write Write flag.\n     */\n    private void checkPutGetSql(Ignite ig, boolean write) {\n        IgniteCache<Integer, DbValue> cache = ig.cache(null);\n\n        int entryCnt = 50_000;\n\n        if (write) {\n            try (IgniteDataStreamer<Object, Object> streamer = ig.dataStreamer(null)) {\n                streamer.allowOverwrite(true);\n\n                for (int i = 0; i < entryCnt; i++)\n                    streamer.addData(i, new DbValue(i, \"value-\" + i, i));\n            }\n        }\n\n        for (int i = 0; i < GRID_CNT; i++) {\n            IgniteEx ignite = grid(i);\n\n            GridCacheAdapter<Object, Object> cache0 = ignite.context().cache().internalCache(null);\n\n            for (int k = 0; k < entryCnt; k++)\n                assertNull(cache0.peekEx(i));\n\n            assertEquals(entryCnt, ignite.cache(null).size());\n        }\n\n        for (int i = 0; i < entryCnt; i++)\n            assertEquals(\"i = \" + i, new DbValue(i, \"value-\" + i, i), cache.get(i));\n\n        List<List<?>> res = cache.query(new SqlFieldsQuery(\"select ival from dbvalue where ival < ? order by ival asc\")\n            .setArgs(10_000)).getAll();\n\n        assertEquals(10_000, res.size());\n\n        for (int i = 0; i < 10_000; i++) {\n            assertEquals(1, res.get(i).size());\n            assertEquals(i, res.get(i).get(0));\n        }\n\n        assertEquals(1, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival = 7899\")).getAll().size());\n        assertEquals(5000, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival >= 5000 and ival < 10000\"))\n            .getAll().size());\n\n        for (int i = 0; i < 10_000; i++)\n            assertEquals(new DbValue(i, \"value-\" + i, i), cache.get(i));\n    }",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190  \n 191  \n 192  \n 193  \n 194 +\n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209  \n 210 +\n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "    /**\n     * @param ig Ignite instance.\n     * @param write Write flag.\n     */\n    private void checkPutGetSql(Ignite ig, boolean write) {\n        IgniteCache<Integer, DbValue> cache = ig.cache(CACHE_NAME);\n\n        int entryCnt = 50_000;\n\n        if (write) {\n            try (IgniteDataStreamer<Object, Object> streamer = ig.dataStreamer(CACHE_NAME)) {\n                streamer.allowOverwrite(true);\n\n                for (int i = 0; i < entryCnt; i++)\n                    streamer.addData(i, new DbValue(i, \"value-\" + i, i));\n            }\n        }\n\n        for (int i = 0; i < GRID_CNT; i++) {\n            IgniteEx ignite = grid(i);\n\n            GridCacheAdapter<Object, Object> cache0 = ignite.context().cache().internalCache(CACHE_NAME);\n\n            for (int k = 0; k < entryCnt; k++)\n                assertNull(cache0.peekEx(i));\n\n            assertEquals(entryCnt, ignite.cache(CACHE_NAME).size());\n        }\n\n        for (int i = 0; i < entryCnt; i++)\n            assertEquals(\"i = \" + i, new DbValue(i, \"value-\" + i, i), cache.get(i));\n\n        List<List<?>> res = cache.query(new SqlFieldsQuery(\"select ival from dbvalue where ival < ? order by ival asc\")\n            .setArgs(10_000)).getAll();\n\n        assertEquals(10_000, res.size());\n\n        for (int i = 0; i < 10_000; i++) {\n            assertEquals(1, res.get(i).size());\n            assertEquals(i, res.get(i).get(0));\n        }\n\n        assertEquals(1, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival = 7899\")).getAll().size());\n        assertEquals(5000, cache.query(new SqlFieldsQuery(\"select lval from dbvalue where ival >= 5000 and ival < 10000\"))\n            .getAll().size());\n\n        for (int i = 0; i < 10_000; i++)\n            assertEquals(new DbValue(i, \"value-\" + i, i), cache.get(i));\n    }"
        ],
        [
            "IgniteCachePageStoreIntegrationSelfTest::testPutMultithreaded()",
            " 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testPutMultithreaded() throws Exception {\n        startGrids(4);\n\n        try {\n            final IgniteEx grid = grid(0);\n\n            GridTestUtils.runMultiThreaded(new Callable<Object>() {\n                @Override public Object call() throws Exception {\n                    for (int i = 0; i < 1000; i++)\n                        grid.cache(null).put(i, i);\n\n                    return null;\n                }\n            }, 8, \"updater\");\n        }\n        finally {\n            stopAllGrids();\n        }\n    }",
            " 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testPutMultithreaded() throws Exception {\n        startGrids(4);\n\n        try {\n            final IgniteEx grid = grid(0);\n\n            GridTestUtils.runMultiThreaded(new Callable<Object>() {\n                @Override public Object call() throws Exception {\n                    for (int i = 0; i < 1000; i++)\n                        grid.cache(CACHE_NAME).put(i, i);\n\n                    return null;\n                }\n            }, 8, \"updater\");\n        }\n        finally {\n            stopAllGrids();\n        }\n    }"
        ],
        [
            "IgnitePersistentStoreWalTlbSelfTest::testWalDirectOutOfMemory()",
            " 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testWalDirectOutOfMemory() throws Exception {\n        IgniteEx ig = grid(1);\n\n        boolean locked = true;\n\n        try {\n            IgniteDataStreamer<Integer, Integer> streamer = ig.dataStreamer(null);\n\n            for (int i = 0; i < 100_000; i++) {\n                streamer.addData(i, 1);\n\n                if (i > 0 && i % 10_000 == 0)\n                    info(\"Done put: \" + i);\n            }\n        }\n        catch (CacheException ignore) {\n            // expected\n            locked = false;\n        }\n        finally {\n            assertFalse(locked);\n\n            stopAllGrids();\n        }\n    }",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "    /**\n     * @throws Exception if failed.\n     */\n    public void testWalDirectOutOfMemory() throws Exception {\n        IgniteEx ig = grid(1);\n\n        boolean locked = true;\n\n        try {\n            IgniteDataStreamer<Integer, Integer> streamer = ig.dataStreamer(CACHE_NAME);\n\n            for (int i = 0; i < 100_000; i++) {\n                streamer.addData(i, 1);\n\n                if (i > 0 && i % 10_000 == 0)\n                    info(\"Done put: \" + i);\n            }\n        }\n        catch (CacheException ignore) {\n            // expected\n            locked = false;\n        }\n        finally {\n            assertFalse(locked);\n\n            stopAllGrids();\n        }\n    }"
        ]
    ],
    "32ff1666fe2b5f141d8c93f6aeffc195f6bd80a1": [
        [
            "CacheStopAndDestroySelfTest::testNearClose()",
            " 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566 -\n 567 -\n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  ",
            "    /**\n     * Test Near close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testNearClose() throws Exception {\n        fail(\"https://issues.apache.org/jira/browse/IGNITE-5511\");\n\n        startGridsMultiThreaded(gridCount());\n\n        IgniteCache<String, String> cache0 = grid(0).getOrCreateCache(getNearConfig());\n\n        // GridDhtTxPrepareRequest requests to Client node will be counted.\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.nodeFilter = grid(2).context().localNodeId();\n\n        // Near Close from client node.\n\n        IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_NEAR);\n        IgniteCache<String, String> cache2 = grid(2).createNearCache(CACHE_NAME_NEAR, new NearCacheConfiguration());\n\n        assert cache2.get(KEY_VAL) == null;\n\n        // Subscribing to events.\n        cache2.put(KEY_VAL, KEY_VAL);\n\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.set(0);\n\n        cache0.put(KEY_VAL, \"near-test\");\n\n        U.sleep(1000);\n\n        //Ensure near cache was automatically updated.\n        assert CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.get() != 0;\n\n        assert cache2.localPeek(KEY_VAL).equals(\"near-test\");\n\n        cache2.close();\n\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.set(0);\n\n        // Should not produce messages to client node.\n        cache0.put(KEY_VAL, KEY_VAL + 0);\n\n        U.sleep(1000);\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + 0);// Not affected.\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + 0);// Not affected.\n\n        try {\n            cache2.get(KEY_VAL);// Affected.\n\n            assert false;\n        }\n        catch (IllegalArgumentException | IllegalStateException ignored) {\n            // No-op\n        }\n\n        // Near Creation from client node after closed.\n\n        IgniteCache<String, String> cache2New = grid(2).createNearCache(CACHE_NAME_NEAR, new NearCacheConfiguration());\n\n        assertNotSame(cache2, cache2New);\n\n        // Subscribing to events.\n        cache2New.put(KEY_VAL, KEY_VAL);\n\n        assert cache2New.localPeek(KEY_VAL).equals(KEY_VAL);\n\n        cache0.put(KEY_VAL, KEY_VAL + \"recreated\");\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache2New.localPeek(KEY_VAL).equals(KEY_VAL + \"recreated\");\n    }",
            " 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  ",
            "    /**\n     * Test Near close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testNearClose() throws Exception {\n        startGridsMultiThreaded(gridCount());\n\n        IgniteCache<String, String> cache0 = grid(0).getOrCreateCache(getNearConfig());\n\n        // GridDhtTxPrepareRequest requests to Client node will be counted.\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.nodeFilter = grid(2).context().localNodeId();\n\n        // Near Close from client node.\n\n        IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_NEAR);\n        IgniteCache<String, String> cache2 = grid(2).createNearCache(CACHE_NAME_NEAR, new NearCacheConfiguration());\n\n        assert cache2.get(KEY_VAL) == null;\n\n        // Subscribing to events.\n        cache2.put(KEY_VAL, KEY_VAL);\n\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.set(0);\n\n        cache0.put(KEY_VAL, \"near-test\");\n\n        U.sleep(1000);\n\n        //Ensure near cache was automatically updated.\n        assert CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.get() != 0;\n\n        assert cache2.localPeek(KEY_VAL).equals(\"near-test\");\n\n        cache2.close();\n\n        CountingTxRequestsToClientNodeTcpCommunicationSpi.cnt.set(0);\n\n        // Should not produce messages to client node.\n        cache0.put(KEY_VAL, KEY_VAL + 0);\n\n        U.sleep(1000);\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + 0);// Not affected.\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + 0);// Not affected.\n\n        try {\n            cache2.get(KEY_VAL);// Affected.\n\n            assert false;\n        }\n        catch (IllegalArgumentException | IllegalStateException ignored) {\n            // No-op\n        }\n\n        // Near Creation from client node after closed.\n\n        IgniteCache<String, String> cache2New = grid(2).createNearCache(CACHE_NAME_NEAR, new NearCacheConfiguration());\n\n        assertNotSame(cache2, cache2New);\n\n        // Subscribing to events.\n        cache2New.put(KEY_VAL, KEY_VAL);\n\n        assert cache2New.localPeek(KEY_VAL).equals(KEY_VAL);\n\n        cache0.put(KEY_VAL, KEY_VAL + \"recreated\");\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache2New.localPeek(KEY_VAL).equals(KEY_VAL + \"recreated\");\n    }"
        ],
        [
            "CacheStopAndDestroySelfTest::testClientClose()",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472 -\n 473 -\n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  ",
            "    /**\n     * Test Client close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testClientClose() throws Exception {\n        fail(\"https://issues.apache.org/jira/browse/IGNITE-5511\");\n\n        startGridsMultiThreaded(gridCount());\n\n        IgniteCache<String, String> cache0 = grid(0).getOrCreateCache(getClientConfig());\n\n        assert cache0.get(KEY_VAL) == null;\n\n        cache0.put(KEY_VAL, KEY_VAL);\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL);\n\n        // DHT Close from client node. Should affect only client node.\n\n        IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_CLIENT);\n        IgniteCache<String, String> cache2 = grid(2).cache(CACHE_NAME_CLIENT);\n\n        assert cache2.get(KEY_VAL).equals(KEY_VAL);\n\n        cache2.close();// Client node.\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL);// Not affected.\n        assert cache1.get(KEY_VAL).equals(KEY_VAL);// Not affected.\n\n        try {\n            cache2.get(KEY_VAL);// Affected.\n\n            assert false;\n        }\n        catch (IllegalStateException ignored) {\n            // No-op\n        }\n\n        // DHT Creation from client node after closed.\n        IgniteCache<String, String> cache2New = grid(2).cache(CACHE_NAME_CLIENT);\n\n        assertNotSame(cache2, cache2New);\n\n        assert cache2New.get(KEY_VAL).equals(KEY_VAL);\n\n        cache0.put(KEY_VAL, KEY_VAL + \"recreated\");\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache2New.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n    }",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  ",
            "    /**\n     * Test Client close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testClientClose() throws Exception {\n        startGridsMultiThreaded(gridCount());\n\n        IgniteCache<String, String> cache0 = grid(0).getOrCreateCache(getClientConfig());\n\n        assert cache0.get(KEY_VAL) == null;\n\n        cache0.put(KEY_VAL, KEY_VAL);\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL);\n\n        // DHT Close from client node. Should affect only client node.\n\n        IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_CLIENT);\n        IgniteCache<String, String> cache2 = grid(2).cache(CACHE_NAME_CLIENT);\n\n        assert cache2.get(KEY_VAL).equals(KEY_VAL);\n\n        cache2.close();// Client node.\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL);// Not affected.\n        assert cache1.get(KEY_VAL).equals(KEY_VAL);// Not affected.\n\n        try {\n            cache2.get(KEY_VAL);// Affected.\n\n            assert false;\n        }\n        catch (IllegalStateException ignored) {\n            // No-op\n        }\n\n        // DHT Creation from client node after closed.\n        IgniteCache<String, String> cache2New = grid(2).cache(CACHE_NAME_CLIENT);\n\n        assertNotSame(cache2, cache2New);\n\n        assert cache2New.get(KEY_VAL).equals(KEY_VAL);\n\n        cache0.put(KEY_VAL, KEY_VAL + \"recreated\");\n\n        assert cache0.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache1.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n        assert cache2New.get(KEY_VAL).equals(KEY_VAL + \"recreated\");\n    }"
        ],
        [
            "CacheStopAndDestroySelfTest::testClientCloseWithTry()",
            " 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525 -\n 526 -\n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  ",
            "    /**\n     * Test Client close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testClientCloseWithTry() throws Exception {\n        fail(\"https://issues.apache.org/jira/browse/IGNITE-5511\");\n\n        startGridsMultiThreaded(gridCount());\n\n        String curVal = null;\n\n        for (int i = 0; i < 3; i++) {\n            try (IgniteCache<String, String> cache2 = grid(2).getOrCreateCache(getClientConfig())) {\n                IgniteCache<String, String> cache0 = grid(0).cache(CACHE_NAME_CLIENT);\n                IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_CLIENT);\n\n                if (i == 0) {\n                    assert cache0.get(KEY_VAL) == null;\n                    assert cache1.get(KEY_VAL) == null;\n                    assert cache2.get(KEY_VAL) == null;\n                }\n                else {\n                    assert cache0.get(KEY_VAL).equals(curVal);\n                    assert cache1.get(KEY_VAL).equals(curVal);\n                    assert cache2.get(KEY_VAL).equals(curVal);\n                }\n\n                curVal = KEY_VAL + curVal;\n\n                cache2.put(KEY_VAL, curVal);\n\n                assert cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL).equals(curVal);\n            }\n\n            awaitPartitionMapExchange();\n        }\n    }",
            " 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  ",
            "    /**\n     * Test Client close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testClientCloseWithTry() throws Exception {\n        startGridsMultiThreaded(gridCount());\n\n        String curVal = null;\n\n        for (int i = 0; i < 3; i++) {\n            try (IgniteCache<String, String> cache2 = grid(2).getOrCreateCache(getClientConfig())) {\n                IgniteCache<String, String> cache0 = grid(0).cache(CACHE_NAME_CLIENT);\n                IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_CLIENT);\n\n                if (i == 0) {\n                    assert cache0.get(KEY_VAL) == null;\n                    assert cache1.get(KEY_VAL) == null;\n                    assert cache2.get(KEY_VAL) == null;\n                }\n                else {\n                    assert cache0.get(KEY_VAL).equals(curVal);\n                    assert cache1.get(KEY_VAL).equals(curVal);\n                    assert cache2.get(KEY_VAL).equals(curVal);\n                }\n\n                curVal = KEY_VAL + curVal;\n\n                cache2.put(KEY_VAL, curVal);\n\n                assert cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL).equals(curVal);\n            }\n\n            awaitPartitionMapExchange();\n        }\n    }"
        ],
        [
            "GridCacheProcessor::closeCache(GridCacheContext,boolean)",
            "2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079 -\n2080  \n2081  \n2082  ",
            "    /**\n     * @param cctx Cache context.\n     * @param destroy Destroy flag.\n     */\n    private void closeCache(GridCacheContext cctx, boolean destroy) {\n        if (cctx.affinityNode()) {\n            GridCacheAdapter<?, ?> cache = caches.get(cctx.name());\n\n            assert cache != null : cctx.name();\n\n            jCacheProxies.put(cctx.name(), new IgniteCacheProxy(cache.context(), cache, null, false));\n        }\n        else {\n            jCacheProxies.remove(cctx.name());\n\n            cctx.gate().onStopped();\n\n            prepareCacheStop(cctx.name(), destroy);\n\n            if (cctx.group().hasCaches())\n                stopCacheGroup(cctx.group().groupId());\n        }\n    }",
            "2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079 +\n2080  \n2081  \n2082  ",
            "    /**\n     * @param cctx Cache context.\n     * @param destroy Destroy flag.\n     */\n    private void closeCache(GridCacheContext cctx, boolean destroy) {\n        if (cctx.affinityNode()) {\n            GridCacheAdapter<?, ?> cache = caches.get(cctx.name());\n\n            assert cache != null : cctx.name();\n\n            jCacheProxies.put(cctx.name(), new IgniteCacheProxy(cache.context(), cache, null, false));\n        }\n        else {\n            jCacheProxies.remove(cctx.name());\n\n            cctx.gate().onStopped();\n\n            prepareCacheStop(cctx.name(), destroy);\n\n            if (!cctx.group().hasCaches())\n                stopCacheGroup(cctx.group().groupId());\n        }\n    }"
        ],
        [
            "CacheStopAndDestroySelfTest::testNearCloseWithTry()",
            " 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641 -\n 642 -\n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  ",
            "    /**\n     * Test Near close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testNearCloseWithTry() throws Exception {\n        fail(\"https://issues.apache.org/jira/browse/IGNITE-5511\");\n\n        startGridsMultiThreaded(gridCount());\n\n        String curVal = null;\n\n        grid(0).getOrCreateCache(getNearConfig());\n\n        NearCacheConfiguration nearCfg = new NearCacheConfiguration();\n\n        for (int i = 0; i < 3; i++) {\n            try (IgniteCache<String, String> cache2 = grid(2).getOrCreateNearCache(CACHE_NAME_NEAR, nearCfg)) {\n                IgniteCache<String, String> cache0 = grid(0).cache(CACHE_NAME_NEAR);\n                IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_NEAR);\n\n                assert cache2.localPeek(KEY_VAL) == null;\n\n                assert cache0.get(KEY_VAL) == null || cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL) == null || cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL) == null || cache2.get(KEY_VAL).equals(curVal);\n\n                curVal = KEY_VAL + curVal;\n\n                cache2.put(KEY_VAL, curVal);\n\n                assert cache2.localPeek(KEY_VAL).equals(curVal);\n\n                assert cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL).equals(curVal);\n            }\n        }\n    }",
            " 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  ",
            "    /**\n     * Test Near close.\n     *\n     * @throws Exception If failed.\n     */\n    public void testNearCloseWithTry() throws Exception {\n        startGridsMultiThreaded(gridCount());\n\n        String curVal = null;\n\n        grid(0).getOrCreateCache(getNearConfig());\n\n        NearCacheConfiguration nearCfg = new NearCacheConfiguration();\n\n        for (int i = 0; i < 3; i++) {\n            try (IgniteCache<String, String> cache2 = grid(2).getOrCreateNearCache(CACHE_NAME_NEAR, nearCfg)) {\n                IgniteCache<String, String> cache0 = grid(0).cache(CACHE_NAME_NEAR);\n                IgniteCache<String, String> cache1 = grid(1).cache(CACHE_NAME_NEAR);\n\n                assert cache2.localPeek(KEY_VAL) == null;\n\n                assert cache0.get(KEY_VAL) == null || cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL) == null || cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL) == null || cache2.get(KEY_VAL).equals(curVal);\n\n                curVal = KEY_VAL + curVal;\n\n                cache2.put(KEY_VAL, curVal);\n\n                assert cache2.localPeek(KEY_VAL).equals(curVal);\n\n                assert cache0.get(KEY_VAL).equals(curVal);\n                assert cache1.get(KEY_VAL).equals(curVal);\n                assert cache2.get(KEY_VAL).equals(curVal);\n            }\n        }\n    }"
        ]
    ],
    "848af3609f9aad462d5e939e708d8284eb5d00e0": [
        [
            "PageHandler::writePage(PageMemory,int,long,PageLockListener,PageHandler,PageIO,IgniteWriteAheadLogManager,Boolean,X,int,R)",
            " 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  ",
            "    /**\n     * @param pageMem Page memory.\n     * @param cacheId Cache ID.\n     * @param pageId Page ID.\n     * @param lsnr Lock listener.\n     * @param h Handler.\n     * @param init IO for new page initialization or {@code null} if it is an existing page.\n     * @param wal Write ahead log.\n     * @param walPlc Full page WAL record policy.\n     * @param arg Argument.\n     * @param intArg Argument of type {@code int}.\n     * @param lockFailed Result in case of lock failure due to page recycling.\n     * @return Handler result.\n     * @throws IgniteCheckedException If failed.\n     */\n    public static <X, R> R writePage(\n        PageMemory pageMem,\n        int cacheId,\n        long pageId,\n        PageLockListener lsnr,\n        PageHandler<X, R> h,\n        PageIO init,\n        IgniteWriteAheadLogManager wal,\n        Boolean walPlc,\n        X arg,\n        int intArg,\n        R lockFailed\n    ) throws IgniteCheckedException {\n        boolean releaseAfterWrite = true;\n        long page = pageMem.acquirePage(cacheId, pageId);\n        try {\n            long pageAddr = writeLock(pageMem, cacheId, pageId, page, lsnr, false);\n\n            if (pageAddr == 0L)\n                return lockFailed;\n\n            boolean ok = false;\n\n            try {\n                if (init != null) {\n                    // It is a new page and we have to initialize it.\n                    doInitPage(pageMem, cacheId, pageId, page, pageAddr, init, wal);\n                    walPlc = FALSE;\n                }\n                else {\n                    init = PageIO.getPageIO(pageAddr);\n                }\n\n                R res = h.run(cacheId, pageId, page, pageAddr, init, walPlc, arg, intArg);\n\n                ok = true;\n\n                return res;\n            }\n            finally {\n                assert PageIO.getCrc(pageAddr) == 0; //TODO GG-11480\n\n                if (releaseAfterWrite = h.releaseAfterWrite(cacheId, pageId, page, pageAddr, arg, intArg))\n                    writeUnlock(pageMem, cacheId, pageId, page, pageAddr, lsnr, walPlc, ok);\n            }\n        }\n        finally {\n            if (releaseAfterWrite)\n                pageMem.releasePage(cacheId, pageId, page);\n        }\n    }",
            " 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  ",
            "    /**\n     * @param pageMem Page memory.\n     * @param cacheId Cache ID.\n     * @param pageId Page ID.\n     * @param lsnr Lock listener.\n     * @param h Handler.\n     * @param init IO for new page initialization or {@code null} if it is an existing page.\n     * @param wal Write ahead log.\n     * @param walPlc Full page WAL record policy.\n     * @param arg Argument.\n     * @param intArg Argument of type {@code int}.\n     * @param lockFailed Result in case of lock failure due to page recycling.\n     * @return Handler result.\n     * @throws IgniteCheckedException If failed.\n     */\n    public static <X, R> R writePage(\n        PageMemory pageMem,\n        int cacheId,\n        final long pageId,\n        PageLockListener lsnr,\n        PageHandler<X, R> h,\n        PageIO init,\n        IgniteWriteAheadLogManager wal,\n        Boolean walPlc,\n        X arg,\n        int intArg,\n        R lockFailed\n    ) throws IgniteCheckedException {\n        boolean releaseAfterWrite = true;\n        long page = pageMem.acquirePage(cacheId, pageId);\n        try {\n            long pageAddr = writeLock(pageMem, cacheId, pageId, page, lsnr, false);\n\n            if (pageAddr == 0L)\n                return lockFailed;\n\n            boolean ok = false;\n\n            try {\n                if (init != null) {\n                    // It is a new page and we have to initialize it.\n                    doInitPage(pageMem, cacheId, pageId, page, pageAddr, init, wal);\n                    walPlc = FALSE;\n                }\n                else {\n                    init = PageIO.getPageIO(pageAddr);\n                }\n\n                R res = h.run(cacheId, pageId, page, pageAddr, init, walPlc, arg, intArg);\n\n                ok = true;\n\n                return res;\n            }\n            finally {\n                assert PageIO.getCrc(pageAddr) == 0; //TODO GG-11480\n\n                if (releaseAfterWrite = h.releaseAfterWrite(cacheId, pageId, page, pageAddr, arg, intArg))\n                    writeUnlock(pageMem, cacheId, pageId, page, pageAddr, lsnr, walPlc, ok);\n            }\n        }\n        finally {\n            if (releaseAfterWrite)\n                pageMem.releasePage(cacheId, pageId, page);\n        }\n    }"
        ],
        [
            "GridCacheProcessor::initCacheProxies(AffinityTopologyVersion,Throwable)",
            "2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012 -\n2013 -\n2014  \n2015 -\n2016 -\n2017  \n2018  \n2019  \n2020  \n2021  \n2022  ",
            "    /**\n     * @param startTopVer Cache start version.\n     * @param err Cache start error if any.\n     */\n    void initCacheProxies(\n        AffinityTopologyVersion startTopVer, @Nullable\n        Throwable err) {\n    for (GridCacheAdapter<?, ?> cache : caches.values()) {\n        GridCacheContext<?, ?> cacheCtx = cache.context();\n\n            if (cacheCtx.startTopologyVersion().equals(startTopVer) && !jCacheProxies.containsKey(cacheCtx.name())) {\n                jCacheProxies.putIfAbsent(cacheCtx.name(), new IgniteCacheProxy(cache.context(), cache, null, false));\n\n                if (cacheCtx.preloader() != null)\n                    cacheCtx.preloader().onInitialExchangeComplete(err);\n            }\n        }\n    }",
            "2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012 +\n2013 +\n2014  \n2015 +\n2016 +\n2017 +\n2018  \n2019  \n2020  \n2021  \n2022  \n2023  ",
            "    /**\n     * @param startTopVer Cache start version.\n     * @param err Cache start error if any.\n     */\n    void initCacheProxies(\n        AffinityTopologyVersion startTopVer, @Nullable\n        Throwable err) {\n        for (GridCacheAdapter<?, ?> cache : caches.values()) {\n            GridCacheContext<?, ?> cacheCtx = cache.context();\n\n            if (cacheCtx.startTopologyVersion().equals(startTopVer) ) {\n                if (!jCacheProxies.containsKey(cacheCtx.name()))\n                    jCacheProxies.putIfAbsent(cacheCtx.name(), new IgniteCacheProxy(cache.context(), cache, null, false));\n\n                if (cacheCtx.preloader() != null)\n                    cacheCtx.preloader().onInitialExchangeComplete(err);\n            }\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::startLocalSnasphotOperation()",
            " 893  \n 894  \n 895  \n 896 -\n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  ",
            "    /**\n\n     */\n    private void startLocalSnasphotOperation() {StartSnapshotOperationAckDiscoveryMessage snapOpMsg= getSnapshotOperationMessage();\n        if (snapOpMsg != null) {\n            SnapshotOperation op = snapOpMsg.snapshotOperation();\n\n            try {\n                IgniteInternalFuture fut = cctx.snapshot()\n                    .startLocalSnapshotOperation(snapOpMsg.initiatorNodeId(), snapOpMsg.snapshotOperation());\n\n                if (fut != null)\n                    fut.get();\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Error while starting snapshot operation\", e);\n            }\n        }\n    }",
            " 894  \n 895  \n 896  \n 897 +\n 898 +\n 899 +\n 900  \n 901  \n 902  \n 903 +\n 904 +\n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  ",
            "    /**\n\n     */\n    private void startLocalSnasphotOperation() {\n        StartSnapshotOperationAckDiscoveryMessage snapOpMsg= getSnapshotOperationMessage();\n\n        if (snapOpMsg != null) {\n            SnapshotOperation op = snapOpMsg.snapshotOperation();\n\n            assert snapOpMsg.needExchange();\n\n            try {\n                IgniteInternalFuture fut = cctx.snapshot()\n                    .startLocalSnapshotOperation(snapOpMsg.initiatorNodeId(), snapOpMsg.snapshotOperation());\n\n                if (fut != null)\n                    fut.get();\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Error while starting snapshot operation\", e);\n            }\n        }\n    }"
        ],
        [
            "GridCacheDatabaseSharedManager::Checkpoint::Checkpoint(CheckpointEntry,GridMultiCollectionWrapper,CheckpointProgress)",
            "2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476 -\n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  \n2485 -\n2486  ",
            "        /**\n         * @param cpEntry Checkpoint entry.\n         * @param cpPages Pages to write to the page store.\n         * @param progress Checkpoint progress status.\n         */\n        private Checkpoint(\n            CheckpointEntry cpEntry,\n            GridMultiCollectionWrapper<FullPageId> cpPages,\n            CheckpointProgress progress\n        ) {\n            assert cpEntry == null || cpEntry.initGuard != 0;\n\n            this.cpEntry = cpEntry;\n            this.cpPages = cpPages;\n            this.progress = progress;\n\n            pagesSize = cpPages == null ? 0 : cpPages.size();\n        }",
            "2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471 +\n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480 +\n2481  ",
            "        /**\n         * @param cpEntry Checkpoint entry.\n         * @param cpPages Pages to write to the page store.\n         * @param progress Checkpoint progress status.\n         */\n        private Checkpoint(\n            CheckpointEntry cpEntry,\n            @NotNull GridMultiCollectionWrapper<FullPageId> cpPages,\n            CheckpointProgress progress\n        ) {\n            assert cpEntry == null || cpEntry.initGuard != 0;\n\n            this.cpEntry = cpEntry;\n            this.cpPages = cpPages;\n            this.progress = progress;\n\n            pagesSize = cpPages.size();\n        }"
        ],
        [
            "GridCacheDatabaseSharedManager::Checkpointer::markCheckpointBegin(CheckpointMetricsTracker)",
            "2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270 -\n2271  \n2272  ",
            "        /**\n         *\n         */\n        @SuppressWarnings(\"TooBroadScope\")\n        private Checkpoint markCheckpointBegin(CheckpointMetricsTracker tracker) throws IgniteCheckedException {\n            CheckpointRecord cpRec = new CheckpointRecord(null, false);\n\n            WALPointer cpPtr = null;\n\n            GridMultiCollectionWrapper<FullPageId> cpPages;\n\n            final CheckpointProgress curr;\n\n            tracker.onLockWaitStart();\n\n            checkpointLock.writeLock().lock();\n\n            try {\n                tracker.onMarkStart();\n\n                synchronized (this) {\n                    curr = scheduledCp;\n\n                    curr.started = true;\n\n                    if (curr.reason == null)\n                        curr.reason = \"timeout\";\n\n                    // It is important that we assign a new progress object before checkpoint mark in page memory.\n                    scheduledCp = new CheckpointProgress(U.currentTimeMillis() + checkpointFreq);\n\n                    curCpProgress = curr;\n                }\n\n                final NavigableMap<T2<Integer, Integer>, T2<Integer, Integer>> map =\n                    new TreeMap<>(FullPageIdIterableComparator.INSTANCE);\n\n                DbCheckpointListener.Context ctx0 = new DbCheckpointListener.Context() {\n                    @Override public boolean nextSnapshot() {\n                        return curr.nextSnapshot;\n                    }\n\n                    @Override public Map<T2<Integer, Integer>, T2<Integer, Integer>> partitionStatMap() {\n                        return map;\n                    }\n                };\n\n                // Listeners must be invoked before we write checkpoint record to WAL.\n                for (DbCheckpointListener lsnr : lsnrs)\n                    lsnr.onCheckpointBegin(ctx0);\n\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    List<GridDhtLocalPartition> locParts = new ArrayList<>();\n\n                    for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions())\n                        locParts.add(part);\n\n                    Collections.sort(locParts, ASC_PART_COMPARATOR);\n\n                    CacheState state = new CacheState(locParts.size());\n\n                    for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions())\n                        state.addPartitionState(part.id(), part.dataStore().fullSize(), part.lastAppliedUpdate());\n\n                    cpRec.addCacheGroupState(grp.groupId(), state);\n                }\n\n                if (curr.nextSnapshot)\n                    snapshotMgr.onMarkCheckPointBegin(curr.snapshotOperation, map);\n\n                IgniteBiTuple<Collection<GridMultiCollectionWrapper<FullPageId>>, Integer> tup = beginAllCheckpoints();\n\n                // Todo it maybe more optimally\n                Collection<FullPageId> cpPagesList = new ArrayList<>(tup.get2());\n\n                for (GridMultiCollectionWrapper<FullPageId> col : tup.get1()) {\n                    for (int i = 0; i < col.collectionsSize(); i++)\n                        cpPagesList.addAll(col.innerCollection(i));\n                }\n\n                cpPages = new GridMultiCollectionWrapper<>(cpPagesList);\n\n                if (!F.isEmpty(cpPages)) {\n                    // No page updates for this checkpoint are allowed from now on.\n                    cpPtr = cctx.wal().log(cpRec);\n\n                    if (cpPtr == null)\n                        cpPtr = CheckpointStatus.NULL_PTR;\n                }\n            }\n            finally {\n                checkpointLock.writeLock().unlock();\n\n                tracker.onLockRelease();\n            }\n\n            curr.cpBeginFut.onDone();\n\n            if (!F.isEmpty(cpPages)) {\n                assert cpPtr != null;\n\n                // Sync log outside the checkpoint write lock.\n                cctx.wal().fsync(cpPtr);\n\n                long cpTs = System.currentTimeMillis();\n\n                CheckpointEntry cpEntry = writeCheckpointEntry(\n                    tmpWriteBuf,\n                    cpTs,\n                    cpRec.checkpointId(),\n                    cpPtr,\n                    cpRec,\n                    CheckpointEntryType.START);\n\n                checkpointHist.addCheckpointEntry(cpEntry);\n\n                if (printCheckpointStats)\n                    if (log.isInfoEnabled())\n                        log.info(String.format(\"Checkpoint started [checkpointId=%s, startPtr=%s, checkpointLockWait=%dms, \" +\n                                \"checkpointLockHoldTime=%dms, pages=%d, reason='%s']\",\n                            cpRec.checkpointId(),\n                            cpPtr,\n                            tracker.lockWaitDuration(),\n                            tracker.lockHoldDuration(),\n                            cpPages.size(),\n                            curr.reason)\n                        );\n\n                return new Checkpoint(cpEntry, cpPages, curr);\n            }\n            else {\n                if (printCheckpointStats) {\n                    if (log.isInfoEnabled())\n                        LT.info(log, String.format(\"Skipping checkpoint (no pages were modified) [\" +\n                            \"checkpointLockWait=%dms, checkpointLockHoldTime=%dms, reason='%s']\",\n                            tracker.lockWaitDuration(),\n                            tracker.lockHoldDuration(),\n                            curr.reason));\n                }\n\n                return new Checkpoint(null, null, curr);\n            }\n        }",
            "2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265 +\n2266  \n2267  ",
            "        /**\n         *\n         */\n        @SuppressWarnings(\"TooBroadScope\")\n        private Checkpoint markCheckpointBegin(CheckpointMetricsTracker tracker) throws IgniteCheckedException {\n            CheckpointRecord cpRec = new CheckpointRecord(null, false);\n\n            WALPointer cpPtr = null;\n\n            GridMultiCollectionWrapper<FullPageId> cpPages;\n\n            final CheckpointProgress curr;\n\n            tracker.onLockWaitStart();\n\n            checkpointLock.writeLock().lock();\n\n            try {\n                tracker.onMarkStart();\n\n                synchronized (this) {\n                    curr = scheduledCp;\n\n                    curr.started = true;\n\n                    if (curr.reason == null)\n                        curr.reason = \"timeout\";\n\n                    // It is important that we assign a new progress object before checkpoint mark in page memory.\n                    scheduledCp = new CheckpointProgress(U.currentTimeMillis() + checkpointFreq);\n\n                    curCpProgress = curr;\n                }\n\n                final NavigableMap<T2<Integer, Integer>, T2<Integer, Integer>> map =\n                    new TreeMap<>(FullPageIdIterableComparator.INSTANCE);\n\n                DbCheckpointListener.Context ctx0 = new DbCheckpointListener.Context() {\n                    @Override public boolean nextSnapshot() {\n                        return curr.nextSnapshot;\n                    }\n\n                    @Override public Map<T2<Integer, Integer>, T2<Integer, Integer>> partitionStatMap() {\n                        return map;\n                    }\n                };\n\n                // Listeners must be invoked before we write checkpoint record to WAL.\n                for (DbCheckpointListener lsnr : lsnrs)\n                    lsnr.onCheckpointBegin(ctx0);\n\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    List<GridDhtLocalPartition> locParts = new ArrayList<>();\n\n                    for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions())\n                        locParts.add(part);\n\n                    Collections.sort(locParts, ASC_PART_COMPARATOR);\n\n                    CacheState state = new CacheState(locParts.size());\n\n                    for (GridDhtLocalPartition part : grp.topology().currentLocalPartitions())\n                        state.addPartitionState(part.id(), part.dataStore().fullSize(), part.lastAppliedUpdate());\n\n                    cpRec.addCacheGroupState(grp.groupId(), state);\n                }\n\n                if (curr.nextSnapshot)\n                    snapshotMgr.onMarkCheckPointBegin(curr.snapshotOperation, map);\n\n                IgniteBiTuple<Collection<GridMultiCollectionWrapper<FullPageId>>, Integer> tup = beginAllCheckpoints();\n\n                // Todo it maybe more optimally\n                Collection<FullPageId> cpPagesList = new ArrayList<>(tup.get2());\n\n                for (GridMultiCollectionWrapper<FullPageId> col : tup.get1()) {\n                    for (int i = 0; i < col.collectionsSize(); i++)\n                        cpPagesList.addAll(col.innerCollection(i));\n                }\n\n                cpPages = new GridMultiCollectionWrapper<>(cpPagesList);\n\n                if (!F.isEmpty(cpPages)) {\n                    // No page updates for this checkpoint are allowed from now on.\n                    cpPtr = cctx.wal().log(cpRec);\n\n                    if (cpPtr == null)\n                        cpPtr = CheckpointStatus.NULL_PTR;\n                }\n            }\n            finally {\n                checkpointLock.writeLock().unlock();\n\n                tracker.onLockRelease();\n            }\n\n            curr.cpBeginFut.onDone();\n\n            if (!F.isEmpty(cpPages)) {\n                assert cpPtr != null;\n\n                // Sync log outside the checkpoint write lock.\n                cctx.wal().fsync(cpPtr);\n\n                long cpTs = System.currentTimeMillis();\n\n                CheckpointEntry cpEntry = writeCheckpointEntry(\n                    tmpWriteBuf,\n                    cpTs,\n                    cpRec.checkpointId(),\n                    cpPtr,\n                    cpRec,\n                    CheckpointEntryType.START);\n\n                checkpointHist.addCheckpointEntry(cpEntry);\n\n                if (printCheckpointStats)\n                    if (log.isInfoEnabled())\n                        log.info(String.format(\"Checkpoint started [checkpointId=%s, startPtr=%s, checkpointLockWait=%dms, \" +\n                                \"checkpointLockHoldTime=%dms, pages=%d, reason='%s']\",\n                            cpRec.checkpointId(),\n                            cpPtr,\n                            tracker.lockWaitDuration(),\n                            tracker.lockHoldDuration(),\n                            cpPages.size(),\n                            curr.reason)\n                        );\n\n                return new Checkpoint(cpEntry, cpPages, curr);\n            }\n            else {\n                if (printCheckpointStats) {\n                    if (log.isInfoEnabled())\n                        LT.info(log, String.format(\"Skipping checkpoint (no pages were modified) [\" +\n                            \"checkpointLockWait=%dms, checkpointLockHoldTime=%dms, reason='%s']\",\n                            tracker.lockWaitDuration(),\n                            tracker.lockHoldDuration(),\n                            curr.reason));\n                }\n\n                return new Checkpoint(null, new GridMultiCollectionWrapper<>(new Collection[0]), curr);\n            }\n        }"
        ],
        [
            "GridCacheDatabaseSharedManager::Checkpointer::doCheckpoint()",
            "1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968 -\n1969 -\n1970 -\n1971 -\n1972 -\n1973 -\n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022 -\n2023 -\n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  ",
            "        /**\n         *\n         */\n        private void doCheckpoint() {\n            try {\n                CheckpointMetricsTracker tracker = new CheckpointMetricsTracker();\n\n                Checkpoint chp = markCheckpointBegin(tracker);\n\n                if (chp.cpPages == null){\n                    markCheckpointEnd(chp);\n\n                    return;\n                }\n\n                boolean interrupted = true;\n\n                try {\n                    if (chp.hasDelta()) {\n                        // Identity stores set.\n                        GridConcurrentHashSet<PageStore> updStores = new GridConcurrentHashSet<>();\n\n                        CountDownFuture doneWriteFut = new CountDownFuture(\n                            asyncRunner == null ? 1 : chp.cpPages.collectionsSize());\n\n                        tracker.onPagesWriteStart();\n\n                        if (asyncRunner != null) {\n                            for (int i = 0; i < chp.cpPages.collectionsSize(); i++) {\n                                Runnable write = new WriteCheckpointPages(\n                                    tracker,\n                                    chp.cpPages.innerCollection(i),\n                                    updStores,\n                                    doneWriteFut\n                                );\n\n                                try {\n                                    asyncRunner.execute(write);\n                                }\n                                catch (RejectedExecutionException ignore) {\n                                    // Run the task synchronously.\n                                    write.run();\n                                }\n                            }\n                        }\n                        else {\n                            // Single-threaded checkpoint.\n                            Runnable write = new WriteCheckpointPages(tracker, chp.cpPages, updStores, doneWriteFut);\n\n                            write.run();\n                        }\n\n                        // Wait and check for errors.\n                        doneWriteFut.get();\n\n                        // Must re-check shutdown flag here because threads may have skipped some pages.\n                        // If so, we should not put finish checkpoint mark.\n                        if (shutdownNow) {\n                            chp.progress.cpFinishFut.onDone(new NodeStoppingException(\"Node is stopping.\"));\n\n                            return;\n                        }\n\n                        snapshotMgr.afterCheckpointPageWritten();\n\n                        tracker.onFsyncStart();\n\n                        if (!skipSync) {\n                            for (PageStore updStore : updStores) {\n                                if (shutdownNow) {\n                                    chp.progress.cpFinishFut.onDone(new NodeStoppingException(\"Node is stopping.\"));\n\n                                    return;\n                                }\n\n                                updStore.sync();\n                            }\n                        }\n                    }\n                    else {\n                        tracker.onPagesWriteStart();\n                        tracker.onFsyncStart();\n                    }\n\n                    // Must mark successful checkpoint only if there are no exceptions or interrupts.\n                    interrupted = false;\n                }\n                finally {\n                    if (!interrupted)\n                        markCheckpointEnd(chp);\n                }\n\n                tracker.onEnd();\n\n                if (chp.hasDelta()) {\n                    if (printCheckpointStats) {\n                        if (log.isInfoEnabled())\n                            log.info(String.format(\"Checkpoint finished [cpId=%s, pages=%d, markPos=%s, \" +\n                                    \"walSegmentsCleared=%d, markDuration=%dms, pagesWrite=%dms, fsync=%dms, \" +\n                                    \"total=%dms]\",\n                                chp.cpEntry.checkpointId(),\n                                chp.pagesSize,\n                                chp.cpEntry.checkpointMark(),\n                                chp.walFilesDeleted,\n                                tracker.markDuration(),\n                                tracker.pagesWriteDuration(),\n                                tracker.fsyncDuration(),\n                                tracker.totalDuration()));\n                    }\n\n                    persStoreMetrics.onCheckpoint(\n                        tracker.lockWaitDuration(),\n                        tracker.markDuration(),\n                        tracker.pagesWriteDuration(),\n                        tracker.fsyncDuration(),\n                        tracker.totalDuration(),\n                        chp.pagesSize,\n                        tracker.dataPagesWritten(),\n                        tracker.cowPagesWritten());\n                }\n                else {\n                    persStoreMetrics.onCheckpoint(\n                        tracker.lockWaitDuration(),\n                        tracker.markDuration(),\n                        tracker.pagesWriteDuration(),\n                        tracker.fsyncDuration(),\n                        tracker.totalDuration(),\n                        chp.pagesSize,\n                        tracker.dataPagesWritten(),\n                        tracker.cowPagesWritten());\n                }\n            }\n            catch (IgniteCheckedException e) {\n                // TODO-ignite-db how to handle exception?\n                U.error(log, \"Failed to create checkpoint.\", e);\n            }\n        }",
            "1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036 +\n2037 +\n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  ",
            "        /**\n         *\n         */\n        private void doCheckpoint() {\n            try {\n                CheckpointMetricsTracker tracker = new CheckpointMetricsTracker();\n\n                Checkpoint chp = markCheckpointBegin(tracker);\n\n                boolean interrupted = true;\n\n                try {\n                    if (chp.hasDelta()) {\n                        // Identity stores set.\n                        GridConcurrentHashSet<PageStore> updStores = new GridConcurrentHashSet<>();\n\n                        CountDownFuture doneWriteFut = new CountDownFuture(\n                            asyncRunner == null ? 1 : chp.cpPages.collectionsSize());\n\n                        tracker.onPagesWriteStart();\n\n                        if (asyncRunner != null) {\n                            for (int i = 0; i < chp.cpPages.collectionsSize(); i++) {\n                                Runnable write = new WriteCheckpointPages(\n                                    tracker,\n                                    chp.cpPages.innerCollection(i),\n                                    updStores,\n                                    doneWriteFut\n                                );\n\n                                try {\n                                    asyncRunner.execute(write);\n                                }\n                                catch (RejectedExecutionException ignore) {\n                                    // Run the task synchronously.\n                                    write.run();\n                                }\n                            }\n                        }\n                        else {\n                            // Single-threaded checkpoint.\n                            Runnable write = new WriteCheckpointPages(tracker, chp.cpPages, updStores, doneWriteFut);\n\n                            write.run();\n                        }\n\n                        // Wait and check for errors.\n                        doneWriteFut.get();\n\n                        // Must re-check shutdown flag here because threads may have skipped some pages.\n                        // If so, we should not put finish checkpoint mark.\n                        if (shutdownNow) {\n                            chp.progress.cpFinishFut.onDone(new NodeStoppingException(\"Node is stopping.\"));\n\n                            return;\n                        }\n\n                        tracker.onFsyncStart();\n\n                        if (!skipSync) {\n                            for (PageStore updStore : updStores) {\n                                if (shutdownNow) {\n                                    chp.progress.cpFinishFut.onDone(new NodeStoppingException(\"Node is stopping.\"));\n\n                                    return;\n                                }\n\n                                updStore.sync();\n                            }\n                        }\n                    }\n                    else {\n                        tracker.onPagesWriteStart();\n                        tracker.onFsyncStart();\n                    }\n\n                    snapshotMgr.afterCheckpointPageWritten();\n\n                    // Must mark successful checkpoint only if there are no exceptions or interrupts.\n                    interrupted = false;\n                }\n                finally {\n                    if (!interrupted)\n                        markCheckpointEnd(chp);\n                }\n\n                tracker.onEnd();\n\n                if (chp.hasDelta()) {\n                    if (printCheckpointStats) {\n                        if (log.isInfoEnabled())\n                            log.info(String.format(\"Checkpoint finished [cpId=%s, pages=%d, markPos=%s, \" +\n                                    \"walSegmentsCleared=%d, markDuration=%dms, pagesWrite=%dms, fsync=%dms, \" +\n                                    \"total=%dms]\",\n                                chp.cpEntry.checkpointId(),\n                                chp.pagesSize,\n                                chp.cpEntry.checkpointMark(),\n                                chp.walFilesDeleted,\n                                tracker.markDuration(),\n                                tracker.pagesWriteDuration(),\n                                tracker.fsyncDuration(),\n                                tracker.totalDuration()));\n                    }\n\n                    persStoreMetrics.onCheckpoint(\n                        tracker.lockWaitDuration(),\n                        tracker.markDuration(),\n                        tracker.pagesWriteDuration(),\n                        tracker.fsyncDuration(),\n                        tracker.totalDuration(),\n                        chp.pagesSize,\n                        tracker.dataPagesWritten(),\n                        tracker.cowPagesWritten());\n                }\n                else {\n                    persStoreMetrics.onCheckpoint(\n                        tracker.lockWaitDuration(),\n                        tracker.markDuration(),\n                        tracker.pagesWriteDuration(),\n                        tracker.fsyncDuration(),\n                        tracker.totalDuration(),\n                        chp.pagesSize,\n                        tracker.dataPagesWritten(),\n                        tracker.cowPagesWritten());\n                }\n            }\n            catch (IgniteCheckedException e) {\n                // TODO-ignite-db how to handle exception?\n                U.error(log, \"Failed to create checkpoint.\", e);\n            }\n        }"
        ]
    ],
    "807cab28575efd221a0cba9846549a6ad1273ed6": [
        [
            "GridCachePartitionExchangeManager::processFullPartitionUpdate(ClusterNode,GridDhtPartitionsFullMessage)",
            "1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275 -\n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  ",
            "    /**\n     * @param node Node.\n     * @param msg Message.\n     */\n    private void processFullPartitionUpdate(ClusterNode node, GridDhtPartitionsFullMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received full partition update [node=\" + node.id() + \", msg=\" + msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionFullMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null)\n                        updated |= top.update(null, entry.getValue(), null) != null;\n                }\n\n                if (!cctx.kernalContext().clientNode() && updated)\n                    refreshPartitions();\n            }\n            else\n                exchangeFuture(msg.exchangeId(), null, null, null, null).onReceive(node, msg);\n        }\n        finally {\n            leaveBusy();\n        }\n    }",
            "1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275 +\n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  ",
            "    /**\n     * @param node Node.\n     * @param msg Message.\n     */\n    private void processFullPartitionUpdate(ClusterNode node, GridDhtPartitionsFullMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received full partition update [node=\" + node.id() + \", msg=\" + msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionFullMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null)\n                        updated |= top.update(null, entry.getValue(), null);\n                }\n\n                if (!cctx.kernalContext().clientNode() && updated)\n                    refreshPartitions();\n            }\n            else\n                exchangeFuture(msg.exchangeId(), null, null, null, null).onReceive(node, msg);\n        }\n        finally {\n            leaveBusy();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(GridDhtPartitionExchangeId,GridDhtPartitionMap)",
            "1314  \n1315  \n1316 -\n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328 -\n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335 -\n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342 -\n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359 -\n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415 -\n1416  \n1417  \n1418  \n1419  \n1420  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Nullable @Override public GridDhtPartitionMap update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!ctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return null;\n        }\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return null;\n\n            if (lastExchangeVer != null && exchId != null && lastExchangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for single partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchId.topologyVersion() + ']');\n\n                return null;\n            }\n\n            if (exchId != null)\n                lastExchangeVer = exchId.topologyVersion();\n\n            if (node2part == null)\n                // Create invalid partition map.\n                node2part = new GridDhtPartitionFullMap();\n\n            GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n            if (cur != null && cur.updateSequence() >= parts.updateSequence()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale update sequence for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curSeq=\" + cur.updateSequence() + \", newSeq=\" + parts.updateSequence() + ']');\n\n                return null;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            node2part = new GridDhtPartitionFullMap(node2part, updateSeq);\n\n            boolean changed = false;\n\n            if (cur == null || !cur.equals(parts))\n                changed = true;\n\n            node2part.put(parts.nodeId(), parts);\n\n            part2node = new HashMap<>(part2node);\n\n            // Add new mappings.\n            for (Integer p : parts.keySet()) {\n                Set<UUID> ids = part2node.get(p);\n\n                if (ids == null)\n                    // Initialize HashSet to size 3 in anticipation that there won't be\n                    // more than 3 nodes per partition.\n                    part2node.put(p, ids = U.newHashSet(3));\n\n                changed |= ids.add(parts.nodeId());\n            }\n\n            // Remove obsolete mappings.\n            if (cur != null) {\n                for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                    Set<UUID> ids = part2node.get(p);\n\n                    if (ids != null)\n                        changed |= ids.remove(parts.nodeId());\n                }\n            }\n\n            AffinityTopologyVersion affVer = grp.affinity().lastVersion();\n\n            if (!affVer.equals(AffinityTopologyVersion.NONE) && affVer.compareTo(topVer) >= 0) {\n                List<List<ClusterNode>> aff = grp.affinity().assignments(topVer);\n\n                changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff);\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after single update: \" + fullMapString());\n\n            if (changed)\n                ctx.exchange().scheduleResendPartitions();\n\n            return changed ? localPartitionMap() : null;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            "1314  \n1315  \n1316 +\n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328 +\n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335 +\n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342 +\n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359 +\n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415 +\n1416  \n1417  \n1418  \n1419  \n1420  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!ctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return false;\n        }\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return false;\n\n            if (lastExchangeVer != null && exchId != null && lastExchangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for single partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchId.topologyVersion() + ']');\n\n                return false;\n            }\n\n            if (exchId != null)\n                lastExchangeVer = exchId.topologyVersion();\n\n            if (node2part == null)\n                // Create invalid partition map.\n                node2part = new GridDhtPartitionFullMap();\n\n            GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n            if (cur != null && cur.updateSequence() >= parts.updateSequence()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale update sequence for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curSeq=\" + cur.updateSequence() + \", newSeq=\" + parts.updateSequence() + ']');\n\n                return false;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            node2part = new GridDhtPartitionFullMap(node2part, updateSeq);\n\n            boolean changed = false;\n\n            if (cur == null || !cur.equals(parts))\n                changed = true;\n\n            node2part.put(parts.nodeId(), parts);\n\n            part2node = new HashMap<>(part2node);\n\n            // Add new mappings.\n            for (Integer p : parts.keySet()) {\n                Set<UUID> ids = part2node.get(p);\n\n                if (ids == null)\n                    // Initialize HashSet to size 3 in anticipation that there won't be\n                    // more than 3 nodes per partition.\n                    part2node.put(p, ids = U.newHashSet(3));\n\n                changed |= ids.add(parts.nodeId());\n            }\n\n            // Remove obsolete mappings.\n            if (cur != null) {\n                for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                    Set<UUID> ids = part2node.get(p);\n\n                    if (ids != null)\n                        changed |= ids.remove(parts.nodeId());\n                }\n            }\n\n            AffinityTopologyVersion affVer = grp.affinity().lastVersion();\n\n            if (!affVer.equals(AffinityTopologyVersion.NONE) && affVer.compareTo(topVer) >= 0) {\n                List<List<ClusterNode>> aff = grp.affinity().assignments(topVer);\n\n                changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff);\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after single update: \" + fullMapString());\n\n            if (changed)\n                ctx.exchange().scheduleResendPartitions();\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(AffinityTopologyVersion,GridDhtPartitionFullMap,Map)",
            "1111  \n1112  \n1113 -\n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127 -\n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157 -\n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165 -\n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272 -\n1273  \n1274  \n1275  \n1276  \n1277  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public GridDhtPartitionMap update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable Map<Integer, T2<Long, Long>> cntrMap\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchangeVer + \", parts=\" + fullMapString() + ']');\n\n        assert partMap != null;\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return null;\n\n            if (cntrMap != null) {\n                // update local map partition counters\n                for (Map.Entry<Integer, T2<Long, Long>> e : cntrMap.entrySet()) {\n                    T2<Long, Long> cntr = this.cntrMap.get(e.getKey());\n\n                    if (cntr == null || cntr.get2() < e.getValue().get2())\n                        this.cntrMap.put(e.getKey(), e.getValue());\n                }\n\n                // update local counters in partitions\n                for (int i = 0; i < locParts.length(); i++) {\n                    GridDhtLocalPartition part = locParts.get(i);\n\n                    if (part == null)\n                        continue;\n\n                    T2<Long, Long> cntr = cntrMap.get(part.id());\n\n                    if (cntr != null)\n                        part.updateCounter(cntr.get2());\n                }\n            }\n\n            if (exchangeVer != null && lastExchangeVer != null && lastExchangeVer.compareTo(exchangeVer) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for full partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchangeVer + ']');\n\n                return null;\n            }\n\n            if (node2part != null && node2part.compareTo(partMap) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale partition map for full partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchangeVer + \", curMap=\" + node2part + \", newMap=\" + partMap + ']');\n\n                return null;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            if (exchangeVer != null)\n                lastExchangeVer = exchangeVer;\n\n            if (node2part != null) {\n                for (GridDhtPartitionMap part : node2part.values()) {\n                    GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                    // If for some nodes current partition has a newer map,\n                    // then we keep the newer value.\n                    if (newPart != null &&\n                        (newPart.updateSequence() < part.updateSequence() ||\n                        (grp.localStartVersion().compareTo(newPart.topologyVersion()) > 0))\n                        ) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Overriding partition map in full update map [exch=\" + exchangeVer +\n                                \", curPart=\" + mapString(part) + \", newPart=\" + mapString(newPart) + ']');\n\n                        partMap.put(part.nodeId(), part);\n                    }\n                }\n\n                // Remove entry if node left.\n                for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                    UUID nodeId = it.next();\n\n                    if (!ctx.discovery().alive(nodeId)) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                partMap + ']');\n\n                        it.remove();\n                    }\n                }\n            }\n\n            node2part = partMap;\n\n            Map<Integer, Set<UUID>> p2n = new HashMap<>(grp.affinity().partitions(), 1.0f);\n\n            for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                for (Integer p : e.getValue().keySet()) {\n                    Set<UUID> ids = p2n.get(p);\n\n                    if (ids == null)\n                        // Initialize HashSet to size 3 in anticipation that there won't be\n                        // more than 3 nodes per partitions.\n                        p2n.put(p, ids = U.newHashSet(3));\n\n                    ids.add(e.getKey());\n                }\n            }\n\n            part2node = p2n;\n\n            boolean changed = false;\n\n            AffinityTopologyVersion affVer = grp.affinity().lastVersion();\n\n            GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n            if (nodeMap != null && ctx.database().persistenceEnabled()) {\n                for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                    int p = e.getKey();\n                    GridDhtPartitionState state = e.getValue();\n\n                   if (state == MOVING) {\n                        GridDhtLocalPartition locPart = locParts.get(p);\n\n                        assert locPart != null;\n\n                        if (locPart.state() == OWNING) {\n                            locPart.moving();\n\n                            changed = true;\n                        }\n\n                        if (cntrMap != null) {\n                            T2<Long, Long> cntr = cntrMap.get(p);\n\n                            if (cntr != null && cntr.get2() > locPart.updateCounter())\n                                locPart.updateCounter(cntr.get2());\n                        }\n                    }\n                }\n            }\n\n            if (!affVer.equals(AffinityTopologyVersion.NONE) && affVer.compareTo(topVer) >= 0) {\n                List<List<ClusterNode>> aff = grp.affinity().assignments(topVer);\n\n                changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff);\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after full update: \" + fullMapString());\n\n            if (changed)\n                ctx.exchange().scheduleResendPartitions();\n\n            return changed ? localPartitionMap() : null;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            "1111  \n1112  \n1113 +\n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127 +\n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157 +\n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165 +\n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272 +\n1273  \n1274  \n1275  \n1276  \n1277  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable Map<Integer, T2<Long, Long>> cntrMap\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchangeVer + \", parts=\" + fullMapString() + ']');\n\n        assert partMap != null;\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return false;\n\n            if (cntrMap != null) {\n                // update local map partition counters\n                for (Map.Entry<Integer, T2<Long, Long>> e : cntrMap.entrySet()) {\n                    T2<Long, Long> cntr = this.cntrMap.get(e.getKey());\n\n                    if (cntr == null || cntr.get2() < e.getValue().get2())\n                        this.cntrMap.put(e.getKey(), e.getValue());\n                }\n\n                // update local counters in partitions\n                for (int i = 0; i < locParts.length(); i++) {\n                    GridDhtLocalPartition part = locParts.get(i);\n\n                    if (part == null)\n                        continue;\n\n                    T2<Long, Long> cntr = cntrMap.get(part.id());\n\n                    if (cntr != null)\n                        part.updateCounter(cntr.get2());\n                }\n            }\n\n            if (exchangeVer != null && lastExchangeVer != null && lastExchangeVer.compareTo(exchangeVer) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for full partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchangeVer + ']');\n\n                return false;\n            }\n\n            if (node2part != null && node2part.compareTo(partMap) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale partition map for full partition map update (will ignore) [lastExch=\" +\n                        lastExchangeVer + \", exch=\" + exchangeVer + \", curMap=\" + node2part + \", newMap=\" + partMap + ']');\n\n                return false;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            if (exchangeVer != null)\n                lastExchangeVer = exchangeVer;\n\n            if (node2part != null) {\n                for (GridDhtPartitionMap part : node2part.values()) {\n                    GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                    // If for some nodes current partition has a newer map,\n                    // then we keep the newer value.\n                    if (newPart != null &&\n                        (newPart.updateSequence() < part.updateSequence() ||\n                        (grp.localStartVersion().compareTo(newPart.topologyVersion()) > 0))\n                        ) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Overriding partition map in full update map [exch=\" + exchangeVer +\n                                \", curPart=\" + mapString(part) + \", newPart=\" + mapString(newPart) + ']');\n\n                        partMap.put(part.nodeId(), part);\n                    }\n                }\n\n                // Remove entry if node left.\n                for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                    UUID nodeId = it.next();\n\n                    if (!ctx.discovery().alive(nodeId)) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                partMap + ']');\n\n                        it.remove();\n                    }\n                }\n            }\n\n            node2part = partMap;\n\n            Map<Integer, Set<UUID>> p2n = new HashMap<>(grp.affinity().partitions(), 1.0f);\n\n            for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                for (Integer p : e.getValue().keySet()) {\n                    Set<UUID> ids = p2n.get(p);\n\n                    if (ids == null)\n                        // Initialize HashSet to size 3 in anticipation that there won't be\n                        // more than 3 nodes per partitions.\n                        p2n.put(p, ids = U.newHashSet(3));\n\n                    ids.add(e.getKey());\n                }\n            }\n\n            part2node = p2n;\n\n            boolean changed = false;\n\n            AffinityTopologyVersion affVer = grp.affinity().lastVersion();\n\n            GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n            if (nodeMap != null && ctx.database().persistenceEnabled()) {\n                for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                    int p = e.getKey();\n                    GridDhtPartitionState state = e.getValue();\n\n                   if (state == MOVING) {\n                        GridDhtLocalPartition locPart = locParts.get(p);\n\n                        assert locPart != null;\n\n                        if (locPart.state() == OWNING) {\n                            locPart.moving();\n\n                            changed = true;\n                        }\n\n                        if (cntrMap != null) {\n                            T2<Long, Long> cntr = cntrMap.get(p);\n\n                            if (cntr != null && cntr.get2() > locPart.updateCounter())\n                                locPart.updateCounter(cntr.get2());\n                        }\n                    }\n                }\n            }\n\n            if (!affVer.equals(AffinityTopologyVersion.NONE) && affVer.compareTo(topVer) >= 0) {\n                List<List<ClusterNode>> aff = grp.affinity().assignments(topVer);\n\n                changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff);\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after full update: \" + fullMapString());\n\n            if (changed)\n                ctx.exchange().scheduleResendPartitions();\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridCachePartitionExchangeManager::processSinglePartitionUpdate(ClusterNode,GridDhtPartitionsSingleMessage)",
            "1289  \n1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322 -\n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  ",
            "    /**\n     * @param node Node ID.\n     * @param msg Message.\n     */\n    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                        msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    if (grp != null &&\n                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                        continue;\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null) {\n                        updated |= top.update(null, entry.getValue()) != null;\n\n                        cctx.affinity().checkRebalanceState(top, grpId);\n                    }\n                }\n\n                if (updated)\n                    scheduleResendPartitions();\n            }\n            else {\n                if (msg.client()) {\n                    final GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(\n                        msg.exchangeId(),\n                        null,\n                        null,\n                        null,\n                        null);\n\n                    exchFut.listen(new CI1<IgniteInternalFuture<AffinityTopologyVersion>>() {\n                        @Override public void apply(IgniteInternalFuture<AffinityTopologyVersion> fut) {\n                            // Finished future should reply only to sender client node.\n                            exchFut.onReceive(node, msg);\n                        }\n                    });\n                }\n                else\n                    exchangeFuture(msg.exchangeId(), null, null, null, null).onReceive(node, msg);\n            }\n        }\n        finally {\n            leaveBusy();\n        }\n    }",
            "1289  \n1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322 +\n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  ",
            "    /**\n     * @param node Node ID.\n     * @param msg Message.\n     */\n    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                        msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    if (grp != null &&\n                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                        continue;\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null) {\n                        updated |= top.update(null, entry.getValue());\n\n                        cctx.affinity().checkRebalanceState(top, grpId);\n                    }\n                }\n\n                if (updated)\n                    scheduleResendPartitions();\n            }\n            else {\n                if (msg.client()) {\n                    final GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(\n                        msg.exchangeId(),\n                        null,\n                        null,\n                        null,\n                        null);\n\n                    exchFut.listen(new CI1<IgniteInternalFuture<AffinityTopologyVersion>>() {\n                        @Override public void apply(IgniteInternalFuture<AffinityTopologyVersion> fut) {\n                            // Finished future should reply only to sender client node.\n                            exchFut.onReceive(node, msg);\n                        }\n                    });\n                }\n                else\n                    exchangeFuture(msg.exchangeId(), null, null, null, null).onReceive(node, msg);\n            }\n        }\n        finally {\n            leaveBusy();\n        }\n    }"
        ],
        [
            "GridClientPartitionTopology::update(GridDhtPartitionExchangeId,GridDhtPartitionMap)",
            " 677  \n 678  \n 679 -\n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691 -\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698 -\n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705 -\n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 -\n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765 -\n 766  \n 767  \n 768  \n 769  \n 770  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Nullable @Override public GridDhtPartitionMap update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!cctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return null;\n        }\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return null;\n\n            if (lastExchangeVer != null && exchId != null && lastExchangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for single partition map update (will ignore) [lastExchVer=\" +\n                        lastExchangeVer + \", exchId=\" + exchId + ']');\n\n                return null;\n            }\n\n            if (exchId != null)\n                lastExchangeVer = exchId.topologyVersion();\n\n            if (node2part == null)\n                // Create invalid partition map.\n                node2part = new GridDhtPartitionFullMap();\n\n            GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n            if (cur != null && cur.updateSequence() >= parts.updateSequence()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale update sequence for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curSeq=\" + cur.updateSequence() + \", newSeq=\" + parts.updateSequence() + ']');\n\n                return null;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            node2part = new GridDhtPartitionFullMap(node2part, updateSeq);\n\n            boolean changed = false;\n\n            if (cur == null || !cur.equals(parts))\n                changed = true;\n\n            node2part.put(parts.nodeId(), parts);\n\n            part2node = new HashMap<>(part2node);\n\n            // Add new mappings.\n            for (Integer p : parts.keySet()) {\n                Set<UUID> ids = part2node.get(p);\n\n                if (ids == null)\n                    // Initialize HashSet to size 3 in anticipation that there won't be\n                    // more than 3 nodes per partition.\n                    part2node.put(p, ids = U.newHashSet(3));\n\n                changed |= ids.add(parts.nodeId());\n            }\n\n            // Remove obsolete mappings.\n            if (cur != null) {\n                for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                    Set<UUID> ids = part2node.get(p);\n\n                    if (ids != null)\n                        changed |= ids.remove(parts.nodeId());\n                }\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after single update: \" + fullMapString());\n\n            return changed ? localPartitionMap() : null;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            " 677  \n 678  \n 679 +\n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691 +\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698 +\n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705 +\n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 +\n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765 +\n 766  \n 767  \n 768  \n 769  \n 770  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!cctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return false;\n        }\n\n        lock.writeLock().lock();\n\n        try {\n            if (stopping)\n                return false;\n\n            if (lastExchangeVer != null && exchId != null && lastExchangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for single partition map update (will ignore) [lastExchVer=\" +\n                        lastExchangeVer + \", exchId=\" + exchId + ']');\n\n                return false;\n            }\n\n            if (exchId != null)\n                lastExchangeVer = exchId.topologyVersion();\n\n            if (node2part == null)\n                // Create invalid partition map.\n                node2part = new GridDhtPartitionFullMap();\n\n            GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n            if (cur != null && cur.updateSequence() >= parts.updateSequence()) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale update sequence for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curSeq=\" + cur.updateSequence() + \", newSeq=\" + parts.updateSequence() + ']');\n\n                return false;\n            }\n\n            long updateSeq = this.updateSeq.incrementAndGet();\n\n            node2part = new GridDhtPartitionFullMap(node2part, updateSeq);\n\n            boolean changed = false;\n\n            if (cur == null || !cur.equals(parts))\n                changed = true;\n\n            node2part.put(parts.nodeId(), parts);\n\n            part2node = new HashMap<>(part2node);\n\n            // Add new mappings.\n            for (Integer p : parts.keySet()) {\n                Set<UUID> ids = part2node.get(p);\n\n                if (ids == null)\n                    // Initialize HashSet to size 3 in anticipation that there won't be\n                    // more than 3 nodes per partition.\n                    part2node.put(p, ids = U.newHashSet(3));\n\n                changed |= ids.add(parts.nodeId());\n            }\n\n            // Remove obsolete mappings.\n            if (cur != null) {\n                for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                    Set<UUID> ids = part2node.get(p);\n\n                    if (ids != null)\n                        changed |= ids.remove(parts.nodeId());\n                }\n            }\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after single update: \" + fullMapString());\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridClientPartitionTopology::update(AffinityTopologyVersion,GridDhtPartitionFullMap,Map)",
            " 563  \n 564  \n 565 -\n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 -\n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588 -\n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651 -\n 652  \n 653  \n 654  \n 655  \n 656  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Nullable @Override public GridDhtPartitionMap update(\n        @Nullable AffinityTopologyVersion exchVer,\n        GridDhtPartitionFullMap partMap,\n        Map<Integer, T2<Long, Long>> cntrMap) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchVer + \", parts=\" + fullMapString() + ']');\n\n        lock.writeLock().lock();\n\n        try {\n            if (exchVer != null && lastExchangeVer != null && lastExchangeVer.compareTo(exchVer) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for full partition map update (will ignore) [lastExchId=\" +\n                        lastExchangeVer + \", exchVer=\" + exchVer + ']');\n\n                return null;\n            }\n\n            if (node2part != null && node2part.compareTo(partMap) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale partition map for full partition map update (will ignore) [lastExchId=\" +\n                        lastExchangeVer + \", exchVer=\" + exchVer + \", curMap=\" + node2part + \", newMap=\" + partMap + ']');\n\n                return null;\n            }\n\n            updateSeq.incrementAndGet();\n\n            if (exchVer != null)\n                lastExchangeVer = exchVer;\n\n            if (node2part != null) {\n                for (GridDhtPartitionMap part : node2part.values()) {\n                    GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                    // If for some nodes current partition has a newer map,\n                    // then we keep the newer value.\n                    if (newPart != null && newPart.updateSequence() < part.updateSequence()) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Overriding partition map in full update map [exchVer=\" + exchVer + \", curPart=\" +\n                                mapString(part) + \", newPart=\" + mapString(newPart) + ']');\n\n                        partMap.put(part.nodeId(), part);\n                    }\n                }\n\n                for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                    UUID nodeId = it.next();\n\n                    if (!cctx.discovery().alive(nodeId)) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                partMap + ']');\n\n                        it.remove();\n                    }\n                }\n            }\n\n            node2part = partMap;\n\n            Map<Integer, Set<UUID>> p2n = new HashMap<>();\n\n            for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                for (Integer p : e.getValue().keySet()) {\n                    Set<UUID> ids = p2n.get(p);\n\n                    if (ids == null)\n                        // Initialize HashSet to size 3 in anticipation that there won't be\n                        // more than 3 nodes per partitions.\n                        p2n.put(p, ids = U.newHashSet(3));\n\n                    ids.add(e.getKey());\n                }\n            }\n\n            part2node = p2n;\n\n            if (cntrMap != null)\n                this.cntrMap = new HashMap<>(cntrMap);\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after full update: \" + fullMapString());\n\n            return null;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            " 563  \n 564  \n 565 +\n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 +\n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588 +\n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651 +\n 652  \n 653  \n 654  \n 655  \n 656  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchVer,\n        GridDhtPartitionFullMap partMap,\n        Map<Integer, T2<Long, Long>> cntrMap) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchVer + \", parts=\" + fullMapString() + ']');\n\n        lock.writeLock().lock();\n\n        try {\n            if (exchVer != null && lastExchangeVer != null && lastExchangeVer.compareTo(exchVer) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale exchange id for full partition map update (will ignore) [lastExchId=\" +\n                        lastExchangeVer + \", exchVer=\" + exchVer + ']');\n\n                return false;\n            }\n\n            if (node2part != null && node2part.compareTo(partMap) >= 0) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Stale partition map for full partition map update (will ignore) [lastExchId=\" +\n                        lastExchangeVer + \", exchVer=\" + exchVer + \", curMap=\" + node2part + \", newMap=\" + partMap + ']');\n\n                return false;\n            }\n\n            updateSeq.incrementAndGet();\n\n            if (exchVer != null)\n                lastExchangeVer = exchVer;\n\n            if (node2part != null) {\n                for (GridDhtPartitionMap part : node2part.values()) {\n                    GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                    // If for some nodes current partition has a newer map,\n                    // then we keep the newer value.\n                    if (newPart != null && newPart.updateSequence() < part.updateSequence()) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Overriding partition map in full update map [exchVer=\" + exchVer + \", curPart=\" +\n                                mapString(part) + \", newPart=\" + mapString(newPart) + ']');\n\n                        partMap.put(part.nodeId(), part);\n                    }\n                }\n\n                for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                    UUID nodeId = it.next();\n\n                    if (!cctx.discovery().alive(nodeId)) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                partMap + ']');\n\n                        it.remove();\n                    }\n                }\n            }\n\n            node2part = partMap;\n\n            Map<Integer, Set<UUID>> p2n = new HashMap<>();\n\n            for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                for (Integer p : e.getValue().keySet()) {\n                    Set<UUID> ids = p2n.get(p);\n\n                    if (ids == null)\n                        // Initialize HashSet to size 3 in anticipation that there won't be\n                        // more than 3 nodes per partitions.\n                        p2n.put(p, ids = U.newHashSet(3));\n\n                    ids.add(e.getKey());\n                }\n            }\n\n            part2node = p2n;\n\n            if (cntrMap != null)\n                this.cntrMap = new HashMap<>(cntrMap);\n\n            consistencyCheck();\n\n            if (log.isDebugEnabled())\n                log.debug(\"Partition map after full update: \" + fullMapString());\n\n            return false;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ]
    ],
    "8445b315663710507e6e3996223f01748f9674a6": [
        [
            "GridDhtPartitionsExchangeFuture::serverNodeDiscoveryEvent()",
            "1150  \n1151  \n1152  \n1153 -\n1154  \n1155  \n1156  \n1157  ",
            "    /**\n     * @return {@code True} if exchange triggered by server node join or fail.\n     */\n    private boolean serverNodeDiscoveryEvent() {\n        assert discoEvt != null;\n\n        return discoEvt.type() != EVT_DISCOVERY_CUSTOM_EVT && !CU.clientNode(discoEvt.eventNode());\n    }",
            "1150  \n1151  \n1152  \n1153 +\n1154  \n1155  \n1156  \n1157  ",
            "    /**\n     * @return {@code True} if exchange triggered by server node join or fail.\n     */\n    public boolean serverNodeDiscoveryEvent() {\n        assert discoEvt != null;\n\n        return discoEvt.type() != EVT_DISCOVERY_CUSTOM_EVT && !CU.clientNode(discoEvt.eventNode());\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onDone(AffinityTopologyVersion,Throwable)",
            "1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176 -\n1177 -\n1178 -\n1179  \n1180 -\n1181 -\n1182  \n1183 -\n1184 -\n1185 -\n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202 -\n1203  \n1204 -\n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  ",
            "    /** {@inheritDoc} */\n    @Override public boolean onDone(@Nullable AffinityTopologyVersion res, @Nullable Throwable err) {\n        boolean realExchange = !dummy && !forcePreload;\n\n        if (err == null &&\n            realExchange &&\n            !cctx.kernalContext().clientNode() &&\n            (serverNodeDiscoveryEvent() || affChangeMsg != null)) {\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                if (!cacheCtx.affinityNode() || cacheCtx.isLocal())\n                    continue;\n\n                cacheCtx.continuousQueries().flushBackupQueue(exchId.topologyVersion());\n            }\n       }\n\n        if (err == null && realExchange) {\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (grp.isLocal())\n                    continue;\n\n                try {\n                    if (centralizedAff)\n                        grp.topology().initPartitions(this);\n                }\n                catch (IgniteInterruptedCheckedException e) {\n                    U.error(log, \"Failed to initialize partitions.\", e);\n                }\n            }\n\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                GridCacheContext drCacheCtx = cacheCtx.isNear() ? cacheCtx.near().dht().context() : cacheCtx;\n\n                if (drCacheCtx.isDrEnabled()) {\n                    try {\n                        drCacheCtx.dr().onExchange(topologyVersion(), exchId.isLeft());\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to notify DR: \" + e, e);\n                    }\n                }\n            }\n\n            if (discoEvt.type() == EVT_NODE_LEFT ||\n                discoEvt.type() == EVT_NODE_FAILED ||\n                discoEvt.type() == EVT_NODE_JOINED)\n                detectLostPartitions();\n\n            Map<Integer, CacheValidation> m = U.newHashMap(cctx.cache().cacheGroups().size());\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                m.put(grp.groupId(), validateCacheGroup(grp, discoEvt.topologyNodes()));\n\n            grpValidRes = m;\n        }\n\n        cctx.cache().onExchangeDone(exchId.topologyVersion(), exchActions, err);\n\n        cctx.exchange().onExchangeDone(this, err);\n\n        if (exchActions != null && err == null)\n            exchActions.completeRequestFutures(cctx);\n\n        if (exchangeOnChangeGlobalState && err == null)\n            cctx.kernalContext().state().onExchangeDone();\n\n        if (super.onDone(res, err) && realExchange) {\n            if (log.isDebugEnabled())\n                log.debug(\"Completed partition exchange [localNode=\" + cctx.localNodeId() + \", exchange= \" + this +\n                    \", durationFromInit=\" + (U.currentTimeMillis() - initTs) + ']');\n\n            initFut.onDone(err == null);\n\n            if (exchId.isLeft()) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                    grp.affinityFunction().removeNode(exchId.nodeId());\n            }\n\n            exchActions = null;\n\n            if (discoEvt instanceof DiscoveryCustomEvent)\n                ((DiscoveryCustomEvent)discoEvt).customMessage(null);\n\n            cctx.exchange().lastFinishedFuture(this);\n\n            return true;\n        }\n\n        return dummy;\n    }",
            "1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176 +\n1177 +\n1178 +\n1179 +\n1180  \n1181 +\n1182  \n1183 +\n1184 +\n1185 +\n1186 +\n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194  \n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203 +\n1204 +\n1205  \n1206 +\n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  ",
            "    /** {@inheritDoc} */\n    @Override public boolean onDone(@Nullable AffinityTopologyVersion res, @Nullable Throwable err) {\n        boolean realExchange = !dummy && !forcePreload;\n\n        if (err == null &&\n            realExchange &&\n            !cctx.kernalContext().clientNode() &&\n            (serverNodeDiscoveryEvent() || affChangeMsg != null)) {\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                if (!cacheCtx.affinityNode() || cacheCtx.isLocal())\n                    continue;\n\n                cacheCtx.continuousQueries().flushBackupQueue(exchId.topologyVersion());\n            }\n       }\n\n        if (err == null && realExchange) {\n            if (centralizedAff) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    try {\n                        grp.topology().initPartitions(this);\n                    }\n                    catch (IgniteInterruptedCheckedException e) {\n                        U.error(log, \"Failed to initialize partitions.\", e);\n                    }\n                }\n            }\n\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                GridCacheContext drCacheCtx = cacheCtx.isNear() ? cacheCtx.near().dht().context() : cacheCtx;\n\n                if (drCacheCtx.isDrEnabled()) {\n                    try {\n                        drCacheCtx.dr().onExchange(topologyVersion(), exchId.isLeft());\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to notify DR: \" + e, e);\n                    }\n                }\n            }\n\n            if (serverNodeDiscoveryEvent() &&\n                (discoEvt.type() == EVT_NODE_LEFT ||\n                discoEvt.type() == EVT_NODE_FAILED ||\n                discoEvt.type() == EVT_NODE_JOINED))\n                detectLostPartitions();\n\n            Map<Integer, CacheValidation> m = U.newHashMap(cctx.cache().cacheGroups().size());\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                m.put(grp.groupId(), validateCacheGroup(grp, discoEvt.topologyNodes()));\n\n            grpValidRes = m;\n        }\n\n        cctx.cache().onExchangeDone(exchId.topologyVersion(), exchActions, err);\n\n        cctx.exchange().onExchangeDone(this, err);\n\n        if (exchActions != null && err == null)\n            exchActions.completeRequestFutures(cctx);\n\n        if (exchangeOnChangeGlobalState && err == null)\n            cctx.kernalContext().state().onExchangeDone();\n\n        if (super.onDone(res, err) && realExchange) {\n            if (log.isDebugEnabled())\n                log.debug(\"Completed partition exchange [localNode=\" + cctx.localNodeId() + \", exchange= \" + this +\n                    \", durationFromInit=\" + (U.currentTimeMillis() - initTs) + ']');\n\n            initFut.onDone(err == null);\n\n            if (exchId.isLeft()) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                    grp.affinityFunction().removeNode(exchId.nodeId());\n            }\n\n            exchActions = null;\n\n            if (discoEvt instanceof DiscoveryCustomEvent)\n                ((DiscoveryCustomEvent)discoEvt).customMessage(null);\n\n            cctx.exchange().lastFinishedFuture(this);\n\n            return true;\n        }\n\n        return dummy;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::createPartitions(List,long)",
            " 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  ",
            "    /**\n     * @param aff Affinity assignments.\n     * @param updateSeq Update sequence.\n     */\n    private void createPartitions(List<List<ClusterNode>> aff, long updateSeq) {\n        int num = grp.affinity().partitions();\n\n        for (int p = 0; p < num; p++) {\n            if (node2part != null && node2part.valid()) {\n                if (localNode(p, aff)) {\n                    // This will make sure that all non-existing partitions\n                    // will be created in MOVING state.\n                    GridDhtLocalPartition locPart = createPartition(p);\n\n                    updateSeq = updateLocal(p, locPart.state(), updateSeq);\n                }\n            }\n            // If this node's map is empty, we pre-create local partitions,\n            // so local map will be sent correctly during exchange.\n            else if (localNode(p, aff))\n                createPartition(p);\n        }\n    }",
            " 477  \n 478  \n 479  \n 480  \n 481  \n 482 +\n 483 +\n 484 +\n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  ",
            "    /**\n     * @param aff Affinity assignments.\n     * @param updateSeq Update sequence.\n     */\n    private void createPartitions(List<List<ClusterNode>> aff, long updateSeq) {\n        if (!grp.affinityNode())\n            return;\n\n        int num = grp.affinity().partitions();\n\n        for (int p = 0; p < num; p++) {\n            if (node2part != null && node2part.valid()) {\n                if (localNode(p, aff)) {\n                    // This will make sure that all non-existing partitions\n                    // will be created in MOVING state.\n                    GridDhtLocalPartition locPart = createPartition(p);\n\n                    updateSeq = updateLocal(p, locPart.state(), updateSeq);\n                }\n            }\n            // If this node's map is empty, we pre-create local partitions,\n            // so local map will be sent correctly during exchange.\n            else if (localNode(p, aff))\n                createPartition(p);\n        }\n    }"
        ],
        [
            "GridClientPartitionTopology::beforeExchange0(ClusterNode,GridDhtPartitionsExchangeFuture)",
            " 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 -\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  ",
            "    /**\n     * @param loc Local node.\n     * @param exchFut Exchange future.\n     */\n    private void beforeExchange0(ClusterNode loc, GridDhtPartitionsExchangeFuture exchFut) {\n        GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n        assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n            topVer + \", exchId=\" + exchId + ']';\n\n        if (!exchId.isJoined())\n            removeNode(exchId.nodeId());\n\n        // In case if node joins, get topology at the time of joining node.\n        ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n        assert oldest != null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n        long updateSeq = this.updateSeq.incrementAndGet();\n\n        // If this is the oldest node.\n        if (oldest.id().equals(loc.id()) || exchFut.dynamicCacheGroupStarted(grpId)) {\n            if (node2part == null) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                        exchId + \", fullMap=\" + fullMapString() + ']');\n            }\n            else if (!node2part.valid()) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                        node2part + ']');\n            }\n            else if (!node2part.nodeId().equals(loc.id())) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                        exchId + \", fullMap=\" + fullMapString() + ']');\n            }\n        }\n\n        consistencyCheck();\n\n        if (log.isDebugEnabled())\n            log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                fullMapString() + ']');\n    }",
            " 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 +\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  ",
            "    /**\n     * @param loc Local node.\n     * @param exchFut Exchange future.\n     */\n    private void beforeExchange0(ClusterNode loc, GridDhtPartitionsExchangeFuture exchFut) {\n        GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n        assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n            topVer + \", exchId=\" + exchId + ']';\n\n        if (exchId.isLeft() && exchFut.serverNodeDiscoveryEvent())\n            removeNode(exchId.nodeId());\n\n        // In case if node joins, get topology at the time of joining node.\n        ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n        assert oldest != null;\n\n        if (log.isDebugEnabled())\n            log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n        long updateSeq = this.updateSeq.incrementAndGet();\n\n        // If this is the oldest node.\n        if (oldest.id().equals(loc.id()) || exchFut.dynamicCacheGroupStarted(grpId)) {\n            if (node2part == null) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                        exchId + \", fullMap=\" + fullMapString() + ']');\n            }\n            else if (!node2part.valid()) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                        node2part + ']');\n            }\n            else if (!node2part.nodeId().equals(loc.id())) {\n                node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                        exchId + \", fullMap=\" + fullMapString() + ']');\n            }\n        }\n\n        consistencyCheck();\n\n        if (log.isDebugEnabled())\n            log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                fullMapString() + ']');\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::beforeExchange(GridDhtPartitionsExchangeFuture,boolean)",
            " 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530 -\n 531 -\n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538 -\n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551 -\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575 -\n 576 -\n 577 -\n 578 -\n 579  \n 580 -\n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut, boolean affReady)\n        throws IgniteCheckedException {\n        DiscoveryEvent discoEvt = exchFut.discoveryEvent();\n\n        ClusterState newState = exchFut.newClusterState();\n\n        treatAllPartAsLoc = (newState != null && newState == ClusterState.ACTIVE)\n            || (ctx.kernalContext().state().active()\n            && discoEvt.type() == EventType.EVT_NODE_JOINED\n            && discoEvt.eventNode().isLocal()\n            && !ctx.kernalContext().clientNode()\n        );\n\n        // Wait for rent outside of checkpoint lock.\n        waitForRent();\n\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        synchronized (ctx.exchange().interruptLock()) {\n            if (Thread.currentThread().isInterrupted())\n                throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n            try {\n                U.writeLock(lock);\n            }\n            catch (IgniteInterruptedCheckedException e) {\n                ctx.database().checkpointReadUnlock();\n\n                throw e;\n            }\n\n            try {\n                GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n                if (stopping)\n                    return;\n\n                assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n                    topVer + \", exchId=\" + exchId + ']';\n\n                if (exchId.isLeft())\n                    removeNode(exchId.nodeId());\n\n                ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                cntrMap.clear();\n\n                // If this is the oldest node.\n                if (oldest != null && (loc.equals(oldest) || exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom()))) {\n                    if (node2part == null) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                                exchId + \", fullMap=\" + fullMapString() + ']');\n                    }\n                    else if (!node2part.valid()) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                                node2part + ']');\n                    }\n                    else if (!node2part.nodeId().equals(loc.id())) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                                exchId + \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n\n                if (affReady)\n                    initPartitions0(exchFut, updateSeq);\n                else {\n                    List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                    createPartitions(aff, updateSeq);\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                        fullMapString() + ']');\n            }\n            finally {\n                lock.writeLock().unlock();\n\n                ctx.database().checkpointReadUnlock();\n            }\n        }\n\n        // Wait for evictions.\n        waitForRent();\n    }",
            " 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542 +\n 543 +\n 544  \n 545  \n 546  \n 547 +\n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559 +\n 560 +\n 561  \n 562 +\n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586 +\n 587 +\n 588 +\n 589 +\n 590 +\n 591 +\n 592 +\n 593  \n 594 +\n 595 +\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut, boolean affReady)\n        throws IgniteCheckedException {\n        DiscoveryEvent discoEvt = exchFut.discoveryEvent();\n\n        ClusterState newState = exchFut.newClusterState();\n\n        treatAllPartAsLoc = (newState != null && newState == ClusterState.ACTIVE)\n            || (ctx.kernalContext().state().active()\n            && discoEvt.type() == EventType.EVT_NODE_JOINED\n            && discoEvt.eventNode().isLocal()\n            && !ctx.kernalContext().clientNode()\n        );\n\n        // Wait for rent outside of checkpoint lock.\n        waitForRent();\n\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        synchronized (ctx.exchange().interruptLock()) {\n            if (Thread.currentThread().isInterrupted())\n                throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n            try {\n                U.writeLock(lock);\n            }\n            catch (IgniteInterruptedCheckedException e) {\n                ctx.database().checkpointReadUnlock();\n\n                throw e;\n            }\n\n            try {\n                if (stopping)\n                    return;\n\n                GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n                assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n                    topVer + \", exchId=\" + exchId + ']';\n\n                if (exchId.isLeft() && exchFut.serverNodeDiscoveryEvent())\n                    removeNode(exchId.nodeId());\n\n                ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                cntrMap.clear();\n\n                boolean grpStarted = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                // If this is the oldest node.\n                if (oldest != null && (loc.equals(oldest) || grpStarted)) {\n                    if (node2part == null) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                                exchId + \", fullMap=\" + fullMapString() + ']');\n                    }\n                    else if (!node2part.valid()) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                                node2part + ']');\n                    }\n                    else if (!node2part.nodeId().equals(loc.id())) {\n                        node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                                exchId + \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n\n                if (grpStarted ||\n                    exchFut.discoveryEvent().type() == EVT_DISCOVERY_CUSTOM_EVT ||\n                    exchFut.serverNodeDiscoveryEvent()) {\n                    if (affReady)\n                        initPartitions0(exchFut, updateSeq);\n                    else {\n                        List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                        createPartitions(aff, updateSeq);\n                    }\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                        fullMapString() + ']');\n            }\n            finally {\n                lock.writeLock().unlock();\n\n                ctx.database().checkpointReadUnlock();\n            }\n        }\n\n        // Wait for evictions.\n        waitForRent();\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::initPartitions0(GridDhtPartitionsExchangeFuture,long)",
            " 377  \n 378  \n 379  \n 380  \n 381  \n 382 -\n 383  \n 384 -\n 385  \n 386 -\n 387  \n 388 -\n 389 -\n 390 -\n 391 -\n 392 -\n 393 -\n 394 -\n 395 -\n 396 -\n 397 -\n 398  \n 399 -\n 400  \n 401 -\n 402  \n 403 -\n 404 -\n 405  \n 406 -\n 407  \n 408 -\n 409 -\n 410  \n 411 -\n 412 -\n 413 -\n 414  \n 415 -\n 416  \n 417 -\n 418 -\n 419  \n 420 -\n 421 -\n 422  \n 423 -\n 424  \n 425  \n 426  \n 427 -\n 428 -\n 429 -\n 430 -\n 431 -\n 432 -\n 433 -\n 434 -\n 435  \n 436 -\n 437  \n 438 -\n 439 -\n 440 -\n 441  \n 442 -\n 443 -\n 444  \n 445 -\n 446  \n 447 -\n 448 -\n 449 -\n 450  \n 451  \n 452 -\n 453 -\n 454 -\n 455 -\n 456 -\n 457  \n 458 -\n 459  \n 460 -\n 461  \n 462  \n 463 -\n 464  \n 465 -\n 466 -\n 467  \n 468  \n 469  ",
            "    /**\n     * @param exchFut Exchange future.\n     * @param updateSeq Update sequence.\n     */\n    private void initPartitions0(GridDhtPartitionsExchangeFuture exchFut, long updateSeq) {\n        ClusterNode loc = ctx.localNode();\n\n        ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n        GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n        assert topVer.equals(exchFut.topologyVersion()) :\n            \"Invalid topology [topVer=\" + topVer +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", futVer=\" + exchFut.topologyVersion() +\n                \", fut=\" + exchFut + ']';\n        assert grp.affinity().lastVersion().equals(exchFut.topologyVersion()) :\n            \"Invalid affinity [topVer=\" + grp.affinity().lastVersion() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", futVer=\" + exchFut.topologyVersion() +\n                \", fut=\" + exchFut + ']';\n\n        List<List<ClusterNode>> aff = grp.affinity().assignments(exchFut.topologyVersion());\n\n        int num = grp.affinity().partitions();\n\n        if (grp.rebalanceEnabled()) {\n            boolean added = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n            boolean first = added || (loc.equals(oldest) && loc.id().equals(exchId.nodeId()) && exchId.isJoined());\n\n            if (first) {\n                assert exchId.isJoined() || added;\n\n                for (int p = 0; p < num; p++) {\n                    if (localNode(p, aff)) {\n                        GridDhtLocalPartition locPart = createPartition(p);\n\n                        boolean owned = locPart.own();\n\n                        assert owned : \"Failed to own partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                            \", part=\" + locPart + ']';\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Owned partition for oldest node: \" + locPart);\n\n                        updateSeq = updateLocal(p, locPart.state(), updateSeq);\n                    }\n                }\n            }\n            else\n                createPartitions(aff, updateSeq);\n        }\n        else {\n            // If preloader is disabled, then we simply clear out\n            // the partitions this node is not responsible for.\n            for (int p = 0; p < num; p++) {\n                GridDhtLocalPartition locPart = localPartition(p, topVer, false, false);\n\n                boolean belongs = localNode(p, aff);\n\n                if (locPart != null) {\n                    if (!belongs) {\n                        GridDhtPartitionState state = locPart.state();\n\n                        if (state.active()) {\n                            locPart.rent(false);\n\n                            updateSeq = updateLocal(p, locPart.state(), updateSeq);\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Evicting partition with rebalancing disabled \" +\n                                    \"(it does not belong to affinity): \" + locPart);\n                        }\n                    }\n                    else\n                        locPart.own();\n                }\n                else if (belongs) {\n                    locPart = createPartition(p);\n\n                    locPart.own();\n\n                    updateLocal(p, locPart.state(), updateSeq);\n                }\n            }\n        }\n\n        if (node2part != null && node2part.valid())\n            checkEvictions(updateSeq, aff);\n\n        updateRebalanceVersion(aff);\n    }",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386 +\n 387  \n 388 +\n 389 +\n 390  \n 391 +\n 392  \n 393 +\n 394  \n 395 +\n 396 +\n 397 +\n 398 +\n 399 +\n 400 +\n 401 +\n 402 +\n 403 +\n 404 +\n 405  \n 406 +\n 407  \n 408 +\n 409 +\n 410  \n 411 +\n 412  \n 413 +\n 414 +\n 415  \n 416 +\n 417 +\n 418 +\n 419  \n 420 +\n 421  \n 422 +\n 423 +\n 424  \n 425 +\n 426 +\n 427  \n 428 +\n 429 +\n 430  \n 431  \n 432 +\n 433 +\n 434  \n 435 +\n 436 +\n 437 +\n 438 +\n 439 +\n 440  \n 441 +\n 442  \n 443 +\n 444 +\n 445 +\n 446  \n 447 +\n 448 +\n 449  \n 450 +\n 451  \n 452 +\n 453 +\n 454 +\n 455 +\n 456  \n 457 +\n 458 +\n 459  \n 460 +\n 461 +\n 462  \n 463 +\n 464  \n 465 +\n 466 +\n 467  \n 468  \n 469  \n 470 +\n 471 +\n 472 +\n 473  \n 474  \n 475  ",
            "    /**\n     * @param exchFut Exchange future.\n     * @param updateSeq Update sequence.\n     */\n    private void initPartitions0(GridDhtPartitionsExchangeFuture exchFut, long updateSeq) {\n        List<List<ClusterNode>> aff = grp.affinity().assignments(exchFut.topologyVersion());\n\n        if (grp.affinityNode()) {\n            ClusterNode loc = ctx.localNode();\n\n            ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n            GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n            assert topVer.equals(exchFut.topologyVersion()) :\n                \"Invalid topology [topVer=\" + topVer +\n                    \", grp=\" + grp.cacheOrGroupName() +\n                    \", futVer=\" + exchFut.topologyVersion() +\n                    \", fut=\" + exchFut + ']';\n            assert grp.affinity().lastVersion().equals(exchFut.topologyVersion()) :\n                \"Invalid affinity [topVer=\" + grp.affinity().lastVersion() +\n                    \", grp=\" + grp.cacheOrGroupName() +\n                    \", futVer=\" + exchFut.topologyVersion() +\n                    \", fut=\" + exchFut + ']';\n\n            int num = grp.affinity().partitions();\n\n            if (grp.rebalanceEnabled()) {\n                boolean added = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                boolean first = added || (loc.equals(oldest) && loc.id().equals(exchId.nodeId()) && exchId.isJoined());\n\n                if (first) {\n                    assert exchId.isJoined() || added;\n\n                    for (int p = 0; p < num; p++) {\n                        if (localNode(p, aff)) {\n                            GridDhtLocalPartition locPart = createPartition(p);\n\n                            boolean owned = locPart.own();\n\n                            assert owned : \"Failed to own partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                                \", part=\" + locPart + ']';\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Owned partition for oldest node: \" + locPart);\n\n                            updateSeq = updateLocal(p, locPart.state(), updateSeq);\n                        }\n                    }\n                }\n                else\n                    createPartitions(aff, updateSeq);\n            }\n            else {\n                // If preloader is disabled, then we simply clear out\n                // the partitions this node is not responsible for.\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition(p, topVer, false, false);\n\n                    boolean belongs = localNode(p, aff);\n\n                    if (locPart != null) {\n                        if (!belongs) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state.active()) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq);\n\n                                if (log.isDebugEnabled())\n                                    log.debug(\"Evicting partition with rebalancing disabled \" +\n                                        \"(it does not belong to affinity): \" + locPart);\n                            }\n                        }\n                        else\n                            locPart.own();\n                    }\n                    else if (belongs) {\n                        locPart = createPartition(p);\n\n                        locPart.own();\n\n                        updateLocal(p, locPart.state(), updateSeq);\n                    }\n                }\n            }\n\n            if (node2part != null && node2part.valid())\n                checkEvictions(updateSeq, aff);\n        }\n\n        updateRebalanceVersion(aff);\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::waitForRent()",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  ",
            "    /**\n     * Waits for renting partitions.\n     *\n     * @return {@code True} if mapping was changed.\n     * @throws IgniteCheckedException If failed.\n     */\n    private boolean waitForRent() throws IgniteCheckedException {\n        final long longOpDumpTimeout =\n            IgniteSystemProperties.getLong(IgniteSystemProperties.IGNITE_LONG_OPERATIONS_DUMP_TIMEOUT, 60_000);\n\n        int dumpCnt = 0;\n\n        GridDhtLocalPartition part;\n\n        for (int i = 0; i < locParts.length(); i++) {\n            part = locParts.get(i);\n\n            if (part == null)\n                continue;\n\n            GridDhtPartitionState state = part.state();\n\n            if (state == RENTING || state == EVICTED) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Waiting for renting partition: \" + part);\n\n                part.tryEvictAsync(false);\n\n                // Wait for partition to empty out.\n                if (longOpDumpTimeout > 0) {\n                    while (true) {\n                        try {\n                            part.rent(true).get(longOpDumpTimeout);\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            if (dumpCnt++ < DUMP_PENDING_OBJECTS_THRESHOLD) {\n                                U.warn(log, \"Failed to wait for partition eviction [\" +\n                                    \"topVer=\" + topVer +\n                                    \", group=\" + grp.cacheOrGroupName() +\n                                    \", part=\" + part.id() +\n                                    \", partState=\" + part.state() +\n                                    \", size=\" + part.internalSize() +\n                                    \", reservations=\" + part.reservations() +\n                                    \", grpReservations=\" + part.groupReserved() +\n                                    \", node=\" + ctx.localNodeId() + \"]\");\n\n                                if (IgniteSystemProperties.getBoolean(IGNITE_THREAD_DUMP_ON_EXCHANGE_TIMEOUT, false))\n                                    U.dumpThreads(log);\n                            }\n                        }\n                    }\n                }\n                else\n                    part.rent(true).get();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Finished waiting for renting partition: \" + part);\n            }\n        }\n\n        // Remove evicted partition.\n        lock.writeLock().lock();\n\n        try {\n            boolean changed = false;\n\n            for (int i = 0; i < locParts.length(); i++) {\n                part = locParts.get(i);\n\n                if (part == null)\n                    continue;\n\n                if (part.state() == EVICTED) {\n                    locParts.set(i, null);\n                    changed = true;\n                }\n            }\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 +\n 213 +\n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "    /**\n     * Waits for renting partitions.\n     *\n     * @return {@code True} if mapping was changed.\n     * @throws IgniteCheckedException If failed.\n     */\n    private boolean waitForRent() throws IgniteCheckedException {\n        if (!grp.affinityNode())\n            return false;\n\n        final long longOpDumpTimeout =\n            IgniteSystemProperties.getLong(IgniteSystemProperties.IGNITE_LONG_OPERATIONS_DUMP_TIMEOUT, 60_000);\n\n        int dumpCnt = 0;\n\n        GridDhtLocalPartition part;\n\n        for (int i = 0; i < locParts.length(); i++) {\n            part = locParts.get(i);\n\n            if (part == null)\n                continue;\n\n            GridDhtPartitionState state = part.state();\n\n            if (state == RENTING || state == EVICTED) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Waiting for renting partition: \" + part);\n\n                part.tryEvictAsync(false);\n\n                // Wait for partition to empty out.\n                if (longOpDumpTimeout > 0) {\n                    while (true) {\n                        try {\n                            part.rent(true).get(longOpDumpTimeout);\n\n                            break;\n                        }\n                        catch (IgniteFutureTimeoutCheckedException ignored) {\n                            if (dumpCnt++ < DUMP_PENDING_OBJECTS_THRESHOLD) {\n                                U.warn(log, \"Failed to wait for partition eviction [\" +\n                                    \"topVer=\" + topVer +\n                                    \", group=\" + grp.cacheOrGroupName() +\n                                    \", part=\" + part.id() +\n                                    \", partState=\" + part.state() +\n                                    \", size=\" + part.internalSize() +\n                                    \", reservations=\" + part.reservations() +\n                                    \", grpReservations=\" + part.groupReserved() +\n                                    \", node=\" + ctx.localNodeId() + \"]\");\n\n                                if (IgniteSystemProperties.getBoolean(IGNITE_THREAD_DUMP_ON_EXCHANGE_TIMEOUT, false))\n                                    U.dumpThreads(log);\n                            }\n                        }\n                    }\n                }\n                else\n                    part.rent(true).get();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Finished waiting for renting partition: \" + part);\n            }\n        }\n\n        // Remove evicted partition.\n        lock.writeLock().lock();\n\n        try {\n            boolean changed = false;\n\n            for (int i = 0; i < locParts.length(); i++) {\n                part = locParts.get(i);\n\n                if (part == null)\n                    continue;\n\n                if (part.state() == EVICTED) {\n                    locParts.set(i, null);\n                    changed = true;\n                }\n            }\n\n            return changed;\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ]
    ],
    "bebe4d872cd687f793596ee1b2067f777e9ce46e": [
        [
            "GridDhtPartitionsExchangeFuture::addMergedJoinExchange(ClusterNode,GridDhtPartitionsSingleMessage)",
            "1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591 -\n1592 -\n1593  \n1594  \n1595  \n1596  \n1597  \n1598 -\n1599 -\n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608 -\n1609 -\n1610  \n1611  \n1612  \n1613  \n1614  \n1615  ",
            "    /**\n     * Records that this exchange if merged with another 'node join' exchange.\n     *\n     * @param node Joined node.\n     * @param msg Joined node message if already received.\n     * @return {@code True} if need to wait for message from joined server node.\n     */\n    private boolean addMergedJoinExchange(ClusterNode node, @Nullable GridDhtPartitionsSingleMessage msg) {\n        assert Thread.holdsLock(mux);\n        assert node != null;\n        assert state == ExchangeLocalState.CRD : state;\n\n        if (msg == null && newCrdFut != null)\n            msg = newCrdFut.joinExchangeMessage(node.id());\n\n        UUID nodeId = node.id();\n\n        boolean wait = false;\n\n        if (CU.clientNode(node)) {\n            if (msg != null)\n                waitAndReplyToNode(nodeId, msg);\n        }\n        else {\n            if (mergedJoinExchMsgs == null)\n                mergedJoinExchMsgs = new LinkedHashMap<>();\n\n            if (msg != null) {\n                assert msg.exchangeId().topologyVersion().equals(new AffinityTopologyVersion(node.order()));\n\n                log.info(\"Merge server join exchange, message received [curFut=\" + initialVersion() +\n                    \", node=\" + nodeId + ']');\n\n                mergedJoinExchMsgs.put(nodeId, msg);\n            }\n            else {\n                if (cctx.discovery().alive(nodeId)) {\n                    log.info(\"Merge server join exchange, wait for message [curFut=\" + initialVersion() +\n                        \", node=\" + nodeId + ']');\n\n                    wait = true;\n\n                    mergedJoinExchMsgs.put(nodeId, null);\n\n                    awaitMergedMsgs++;\n                }\n                else {\n                    log.info(\"Merge server join exchange, awaited node left [curFut=\" + initialVersion() +\n                        \", node=\" + nodeId + ']');\n                }\n            }\n        }\n\n        return wait;\n    }",
            "1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594 +\n1595 +\n1596 +\n1597 +\n1598  \n1599  \n1600  \n1601  \n1602  \n1603 +\n1604 +\n1605 +\n1606 +\n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615 +\n1616 +\n1617 +\n1618 +\n1619  \n1620  \n1621  \n1622  \n1623  \n1624  ",
            "    /**\n     * Records that this exchange if merged with another 'node join' exchange.\n     *\n     * @param node Joined node.\n     * @param msg Joined node message if already received.\n     * @return {@code True} if need to wait for message from joined server node.\n     */\n    private boolean addMergedJoinExchange(ClusterNode node, @Nullable GridDhtPartitionsSingleMessage msg) {\n        assert Thread.holdsLock(mux);\n        assert node != null;\n        assert state == ExchangeLocalState.CRD : state;\n\n        if (msg == null && newCrdFut != null)\n            msg = newCrdFut.joinExchangeMessage(node.id());\n\n        UUID nodeId = node.id();\n\n        boolean wait = false;\n\n        if (CU.clientNode(node)) {\n            if (msg != null)\n                waitAndReplyToNode(nodeId, msg);\n        }\n        else {\n            if (mergedJoinExchMsgs == null)\n                mergedJoinExchMsgs = new LinkedHashMap<>();\n\n            if (msg != null) {\n                assert msg.exchangeId().topologyVersion().equals(new AffinityTopologyVersion(node.order()));\n\n                if (log.isInfoEnabled()) {\n                    log.info(\"Merge server join exchange, message received [curFut=\" + initialVersion() +\n                        \", node=\" + nodeId + ']');\n                }\n\n                mergedJoinExchMsgs.put(nodeId, msg);\n            }\n            else {\n                if (cctx.discovery().alive(nodeId)) {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Merge server join exchange, wait for message [curFut=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n\n                    wait = true;\n\n                    mergedJoinExchMsgs.put(nodeId, null);\n\n                    awaitMergedMsgs++;\n                }\n                else {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Merge server join exchange, awaited node left [curFut=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n                }\n            }\n        }\n\n        return wait;\n    }"
        ],
        [
            "InitNewCoordinatorFuture::init(GridDhtPartitionsExchangeFuture)",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 -\n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  ",
            "    /**\n     * @param exchFut Current future.\n     * @throws IgniteCheckedException If failed.\n     */\n    public void init(GridDhtPartitionsExchangeFuture exchFut) throws IgniteCheckedException {\n        initTopVer = exchFut.initialVersion();\n\n        GridCacheSharedContext cctx = exchFut.sharedContext();\n\n        restoreState = exchangeProtocolVersion(exchFut.context().events().discoveryCache().minimumNodeVersion()) > 1;\n\n        boolean newAff = exchFut.localJoinExchange();\n\n        IgniteInternalFuture<?> fut = cctx.affinity().initCoordinatorCaches(exchFut, newAff);\n\n        if (fut != null)\n            add(fut);\n\n        if (restoreState) {\n            DiscoCache curDiscoCache = cctx.discovery().discoCache();\n\n            DiscoCache discoCache = exchFut.events().discoveryCache();\n\n            List<ClusterNode> nodes = new ArrayList<>();\n\n            synchronized (this) {\n                for (ClusterNode node : discoCache.allNodes()) {\n                    if (!node.isLocal() && cctx.discovery().alive(node)) {\n                        awaited.add(node.id());\n\n                        nodes.add(node);\n                    }\n                }\n\n                if (exchFut.context().mergeExchanges() && !curDiscoCache.version().equals(discoCache.version())) {\n                    for (ClusterNode node : curDiscoCache.allNodes()) {\n                        if (discoCache.node(node.id()) == null) {\n                            if (exchangeProtocolVersion(node.version()) == 1)\n                                break;\n\n                            awaited.add(node.id());\n\n                            nodes.add(node);\n\n                            if (joinedNodes == null)\n                                joinedNodes = new HashMap<>();\n\n                            GridDhtPartitionExchangeId exchId = new GridDhtPartitionExchangeId(node.id(),\n                                EVT_NODE_JOINED,\n                                new AffinityTopologyVersion(node.order()));\n\n                            joinedNodes.put(node.id(), exchId);\n                        }\n                    }\n                }\n\n                if (joinedNodes == null)\n                    joinedNodes = Collections.emptyMap();\n\n                if (!awaited.isEmpty()) {\n                    restoreStateFut = new GridFutureAdapter();\n\n                    add(restoreStateFut);\n                }\n            }\n\n            log.info(\"Try restore exchange result [allNodes=\" + awaited +\n                \", joined=\" + joinedNodes.keySet() +  ']');\n\n            if (!nodes.isEmpty()) {\n                GridDhtPartitionsSingleRequest req = GridDhtPartitionsSingleRequest.restoreStateRequest(exchFut.exchangeId(),\n                    exchFut.exchangeId());\n\n                for (ClusterNode node : nodes) {\n                    try {\n                        GridDhtPartitionsSingleRequest sndReq = req;\n\n                        if (joinedNodes.containsKey(node.id())) {\n                            sndReq = GridDhtPartitionsSingleRequest.restoreStateRequest(\n                                joinedNodes.get(node.id()),\n                                exchFut.exchangeId());\n                        }\n\n                        cctx.io().send(node, sndReq, GridIoPolicy.SYSTEM_POOL);\n                    }\n                    catch (ClusterTopologyCheckedException e) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Failed to send partitions request, node failed: \" + node);\n\n                        onNodeLeft(node.id());\n                    }\n                }\n            }\n        }\n\n        markInitialized();\n    }",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 +\n 151 +\n 152 +\n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "    /**\n     * @param exchFut Current future.\n     * @throws IgniteCheckedException If failed.\n     */\n    public void init(GridDhtPartitionsExchangeFuture exchFut) throws IgniteCheckedException {\n        initTopVer = exchFut.initialVersion();\n\n        GridCacheSharedContext cctx = exchFut.sharedContext();\n\n        restoreState = exchangeProtocolVersion(exchFut.context().events().discoveryCache().minimumNodeVersion()) > 1;\n\n        boolean newAff = exchFut.localJoinExchange();\n\n        IgniteInternalFuture<?> fut = cctx.affinity().initCoordinatorCaches(exchFut, newAff);\n\n        if (fut != null)\n            add(fut);\n\n        if (restoreState) {\n            DiscoCache curDiscoCache = cctx.discovery().discoCache();\n\n            DiscoCache discoCache = exchFut.events().discoveryCache();\n\n            List<ClusterNode> nodes = new ArrayList<>();\n\n            synchronized (this) {\n                for (ClusterNode node : discoCache.allNodes()) {\n                    if (!node.isLocal() && cctx.discovery().alive(node)) {\n                        awaited.add(node.id());\n\n                        nodes.add(node);\n                    }\n                }\n\n                if (exchFut.context().mergeExchanges() && !curDiscoCache.version().equals(discoCache.version())) {\n                    for (ClusterNode node : curDiscoCache.allNodes()) {\n                        if (discoCache.node(node.id()) == null) {\n                            if (exchangeProtocolVersion(node.version()) == 1)\n                                break;\n\n                            awaited.add(node.id());\n\n                            nodes.add(node);\n\n                            if (joinedNodes == null)\n                                joinedNodes = new HashMap<>();\n\n                            GridDhtPartitionExchangeId exchId = new GridDhtPartitionExchangeId(node.id(),\n                                EVT_NODE_JOINED,\n                                new AffinityTopologyVersion(node.order()));\n\n                            joinedNodes.put(node.id(), exchId);\n                        }\n                    }\n                }\n\n                if (joinedNodes == null)\n                    joinedNodes = Collections.emptyMap();\n\n                if (!awaited.isEmpty()) {\n                    restoreStateFut = new GridFutureAdapter();\n\n                    add(restoreStateFut);\n                }\n            }\n\n            if (log.isInfoEnabled()) {\n                log.info(\"Try restore exchange result [allNodes=\" + awaited +\n                    \", joined=\" + joinedNodes.keySet() +  ']');\n            }\n\n            if (!nodes.isEmpty()) {\n                GridDhtPartitionsSingleRequest req = GridDhtPartitionsSingleRequest.restoreStateRequest(exchFut.exchangeId(),\n                    exchFut.exchangeId());\n\n                for (ClusterNode node : nodes) {\n                    try {\n                        GridDhtPartitionsSingleRequest sndReq = req;\n\n                        if (joinedNodes.containsKey(node.id())) {\n                            sndReq = GridDhtPartitionsSingleRequest.restoreStateRequest(\n                                joinedNodes.get(node.id()),\n                                exchFut.exchangeId());\n                        }\n\n                        cctx.io().send(node, sndReq, GridIoPolicy.SYSTEM_POOL);\n                    }\n                    catch (ClusterTopologyCheckedException e) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Failed to send partitions request, node failed: \" + node);\n\n                        onNodeLeft(node.id());\n                    }\n                }\n            }\n        }\n\n        markInitialized();\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onBecomeCoordinator(InitNewCoordinatorFuture)",
            "3110  \n3111  \n3112  \n3113  \n3114  \n3115  \n3116  \n3117  \n3118  \n3119  \n3120  \n3121  \n3122  \n3123  \n3124 -\n3125 -\n3126  \n3127  \n3128  \n3129  \n3130  \n3131  \n3132  \n3133  \n3134  \n3135  \n3136  \n3137  \n3138  \n3139  \n3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  \n3157  \n3158  \n3159  \n3160  \n3161  \n3162  \n3163 -\n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198 -\n3199 -\n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212 -\n3213 -\n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229 -\n3230 -\n3231  \n3232  \n3233  \n3234  \n3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  ",
            "    /**\n     * @param newCrdFut Coordinator initialization future.\n     */\n    private void onBecomeCoordinator(InitNewCoordinatorFuture newCrdFut) {\n        boolean allRcvd = false;\n\n        cctx.exchange().onCoordinatorInitialized();\n\n        if (newCrdFut.restoreState()) {\n            GridDhtPartitionsFullMessage fullMsg = newCrdFut.fullMessage();\n\n            assert msgs.isEmpty() : msgs;\n\n            if (fullMsg != null) {\n                log.info(\"New coordinator restored state [ver=\" + initialVersion() +\n                    \", resVer=\" + fullMsg.resultTopologyVersion() + ']');\n\n                synchronized (mux) {\n                    state = ExchangeLocalState.DONE;\n\n                    finishState = new FinishState(crd.id(), fullMsg.resultTopologyVersion(), fullMsg);\n                }\n\n                fullMsg.exchangeId(exchId);\n\n                processFullMessage(false, null, fullMsg);\n\n                Map<ClusterNode, GridDhtPartitionsSingleMessage> msgs = newCrdFut.messages();\n\n                if (!F.isEmpty(msgs)) {\n                    Map<Integer, CacheGroupAffinityMessage> joinedNodeAff = null;\n\n                    for (Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e : msgs.entrySet()) {\n                        this.msgs.put(e.getKey().id(), e.getValue());\n\n                        GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                        Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n                        if (!F.isEmpty(affReq)) {\n                            joinedNodeAff = CacheGroupAffinityMessage.createAffinityMessages(cctx,\n                                fullMsg.resultTopologyVersion(),\n                                affReq,\n                                joinedNodeAff);\n                        }\n                    }\n\n                    sendAllPartitions(fullMsg, msgs.keySet(), null, joinedNodeAff);\n                }\n\n                return;\n            }\n            else {\n                log.info(\"New coordinator restore state finished [ver=\" + initialVersion() + ']');\n\n                for (Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e : newCrdFut.messages().entrySet()) {\n                    GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                    if (!msg.client()) {\n                        msgs.put(e.getKey().id(), e.getValue());\n\n                        updatePartitionSingleMap(e.getKey().id(), msg);\n                    }\n                }\n            }\n\n            allRcvd = true;\n\n            synchronized (mux) {\n                remaining.clear(); // Do not process messages.\n\n                assert crd != null && crd.isLocal();\n\n                state = ExchangeLocalState.CRD;\n\n                assert mergedJoinExchMsgs == null;\n            }\n        }\n        else {\n            Set<UUID> remaining0 = null;\n\n            synchronized (mux) {\n                assert crd != null && crd.isLocal();\n\n                state = ExchangeLocalState.CRD;\n\n                assert mergedJoinExchMsgs == null;\n\n                log.info(\"New coordinator initialization finished [ver=\" + initialVersion() +\n                    \", remaining=\" + remaining + ']');\n\n                if (!remaining.isEmpty())\n                    remaining0 = new HashSet<>(remaining);\n            }\n\n            if (remaining0 != null) {\n                // It is possible that some nodes finished exchange with previous coordinator.\n                GridDhtPartitionsSingleRequest req = new GridDhtPartitionsSingleRequest(exchId);\n\n                for (UUID nodeId : remaining0) {\n                    try {\n                        if (!pendingSingleMsgs.containsKey(nodeId)) {\n                            log.info(\"New coordinator sends request [ver=\" + initialVersion() +\n                                \", node=\" + nodeId + ']');\n\n                            cctx.io().send(nodeId, req, SYSTEM_POOL);\n                        }\n                    }\n                    catch (ClusterTopologyCheckedException ignored) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Node left during partition exchange [nodeId=\" + nodeId +\n                                \", exchId=\" + exchId + ']');\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to request partitions from node: \" + nodeId, e);\n                    }\n                }\n\n                for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> m : pendingSingleMsgs.entrySet()) {\n                    log.info(\"New coordinator process pending message [ver=\" + initialVersion() +\n                        \", node=\" + m.getKey() + ']');\n\n                    processSingleMessage(m.getKey(), m.getValue());\n                }\n            }\n        }\n\n        if (allRcvd) {\n            awaitSingleMapUpdates();\n\n            onAllReceived(newCrdFut.messages().keySet());\n        }\n    }",
            "3158  \n3159  \n3160  \n3161  \n3162  \n3163  \n3164  \n3165  \n3166  \n3167  \n3168  \n3169  \n3170  \n3171  \n3172 +\n3173 +\n3174 +\n3175 +\n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207 +\n3208 +\n3209 +\n3210 +\n3211 +\n3212 +\n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219 +\n3220 +\n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233  \n3234  \n3235  \n3236  \n3237  \n3238  \n3239  \n3240  \n3241  \n3242  \n3243  \n3244  \n3245  \n3246  \n3247  \n3248  \n3249  \n3250  \n3251  \n3252  \n3253  \n3254  \n3255 +\n3256 +\n3257 +\n3258 +\n3259  \n3260  \n3261  \n3262  \n3263  \n3264  \n3265  \n3266  \n3267  \n3268  \n3269  \n3270  \n3271 +\n3272 +\n3273 +\n3274 +\n3275  \n3276  \n3277  \n3278  \n3279  \n3280  \n3281  \n3282  \n3283  \n3284  \n3285  \n3286  \n3287  \n3288  \n3289  \n3290 +\n3291 +\n3292 +\n3293 +\n3294  \n3295  \n3296  \n3297  \n3298  \n3299  \n3300  \n3301  \n3302  \n3303  \n3304  \n3305  ",
            "    /**\n     * @param newCrdFut Coordinator initialization future.\n     */\n    private void onBecomeCoordinator(InitNewCoordinatorFuture newCrdFut) {\n        boolean allRcvd = false;\n\n        cctx.exchange().onCoordinatorInitialized();\n\n        if (newCrdFut.restoreState()) {\n            GridDhtPartitionsFullMessage fullMsg = newCrdFut.fullMessage();\n\n            assert msgs.isEmpty() : msgs;\n\n            if (fullMsg != null) {\n                if (log.isInfoEnabled()) {\n                    log.info(\"New coordinator restored state [ver=\" + initialVersion() +\n                        \", resVer=\" + fullMsg.resultTopologyVersion() + ']');\n                }\n\n                synchronized (mux) {\n                    state = ExchangeLocalState.DONE;\n\n                    finishState = new FinishState(crd.id(), fullMsg.resultTopologyVersion(), fullMsg);\n                }\n\n                fullMsg.exchangeId(exchId);\n\n                processFullMessage(false, null, fullMsg);\n\n                Map<ClusterNode, GridDhtPartitionsSingleMessage> msgs = newCrdFut.messages();\n\n                if (!F.isEmpty(msgs)) {\n                    Map<Integer, CacheGroupAffinityMessage> joinedNodeAff = null;\n\n                    for (Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e : msgs.entrySet()) {\n                        this.msgs.put(e.getKey().id(), e.getValue());\n\n                        GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                        Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n                        if (!F.isEmpty(affReq)) {\n                            joinedNodeAff = CacheGroupAffinityMessage.createAffinityMessages(cctx,\n                                fullMsg.resultTopologyVersion(),\n                                affReq,\n                                joinedNodeAff);\n                        }\n                    }\n\n                    if (log.isInfoEnabled()) {\n                        log.info(\"New coordinator sends full message [ver=\" + initialVersion() +\n                            \", resVer=\" + fullMsg.resultTopologyVersion() +\n                            \", nodes=\" + F.nodeIds(msgs.keySet()) + ']');\n                    }\n\n                    sendAllPartitions(fullMsg, msgs.keySet(), null, joinedNodeAff);\n                }\n\n                return;\n            }\n            else {\n                if (log.isInfoEnabled())\n                    log.info(\"New coordinator restore state finished [ver=\" + initialVersion() + ']');\n\n                for (Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e : newCrdFut.messages().entrySet()) {\n                    GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                    if (!msg.client()) {\n                        msgs.put(e.getKey().id(), e.getValue());\n\n                        updatePartitionSingleMap(e.getKey().id(), msg);\n                    }\n                }\n            }\n\n            allRcvd = true;\n\n            synchronized (mux) {\n                remaining.clear(); // Do not process messages.\n\n                assert crd != null && crd.isLocal();\n\n                state = ExchangeLocalState.CRD;\n\n                assert mergedJoinExchMsgs == null;\n            }\n        }\n        else {\n            Set<UUID> remaining0 = null;\n\n            synchronized (mux) {\n                assert crd != null && crd.isLocal();\n\n                state = ExchangeLocalState.CRD;\n\n                assert mergedJoinExchMsgs == null;\n\n                if (log.isInfoEnabled()) {\n                    log.info(\"New coordinator initialization finished [ver=\" + initialVersion() +\n                        \", remaining=\" + remaining + ']');\n                }\n\n                if (!remaining.isEmpty())\n                    remaining0 = new HashSet<>(remaining);\n            }\n\n            if (remaining0 != null) {\n                // It is possible that some nodes finished exchange with previous coordinator.\n                GridDhtPartitionsSingleRequest req = new GridDhtPartitionsSingleRequest(exchId);\n\n                for (UUID nodeId : remaining0) {\n                    try {\n                        if (!pendingSingleMsgs.containsKey(nodeId)) {\n                            if (log.isInfoEnabled()) {\n                                log.info(\"New coordinator sends request [ver=\" + initialVersion() +\n                                    \", node=\" + nodeId + ']');\n                            }\n\n                            cctx.io().send(nodeId, req, SYSTEM_POOL);\n                        }\n                    }\n                    catch (ClusterTopologyCheckedException ignored) {\n                        if (log.isDebugEnabled())\n                            log.debug(\"Node left during partition exchange [nodeId=\" + nodeId +\n                                \", exchId=\" + exchId + ']');\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to request partitions from node: \" + nodeId, e);\n                    }\n                }\n\n                for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> m : pendingSingleMsgs.entrySet()) {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"New coordinator process pending message [ver=\" + initialVersion() +\n                            \", node=\" + m.getKey() + ']');\n                    }\n\n                    processSingleMessage(m.getKey(), m.getValue());\n                }\n            }\n        }\n\n        if (allRcvd) {\n            awaitSingleMapUpdates();\n\n            onAllReceived(newCrdFut.messages().keySet());\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processMergedMessage(ClusterNode,GridDhtPartitionsSingleMessage)",
            "1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687 -\n1688 -\n1689 -\n1690 -\n1691 -\n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processMergedMessage(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(node.id(), msg);\n\n            return;\n        }\n\n        boolean done = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            if (state == ExchangeLocalState.DONE) {\n                assert finishState != null;\n\n                finishState0 = finishState;\n            }\n            else {\n                boolean process = mergedJoinExchMsgs != null &&\n                    mergedJoinExchMsgs.containsKey(node.id()) &&\n                    mergedJoinExchMsgs.get(node.id()) == null;\n\n                log.info(\"Merge server join exchange, received message [curFut=\" + initialVersion() +\n                    \", node=\" + node.id() +\n                    \", msgVer=\" + msg.exchangeId().topologyVersion() +\n                    \", process=\" + process +\n                    \", awaited=\" + awaitMergedMsgs + ']');\n\n                if (process) {\n                    mergedJoinExchMsgs.put(node.id(), msg);\n\n                    assert awaitMergedMsgs > 0 : awaitMergedMsgs;\n\n                    awaitMergedMsgs--;\n\n                    done = awaitMergedMsgs == 0;\n                }\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, node.id());\n\n            return;\n        }\n\n        if (done)\n            finishExchangeOnCoordinator(null);\n    }",
            "1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696 +\n1697 +\n1698 +\n1699 +\n1700 +\n1701 +\n1702 +\n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processMergedMessage(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(node.id(), msg);\n\n            return;\n        }\n\n        boolean done = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            if (state == ExchangeLocalState.DONE) {\n                assert finishState != null;\n\n                finishState0 = finishState;\n            }\n            else {\n                boolean process = mergedJoinExchMsgs != null &&\n                    mergedJoinExchMsgs.containsKey(node.id()) &&\n                    mergedJoinExchMsgs.get(node.id()) == null;\n\n                if (log.isInfoEnabled()) {\n                    log.info(\"Merge server join exchange, received message [curFut=\" + initialVersion() +\n                        \", node=\" + node.id() +\n                        \", msgVer=\" + msg.exchangeId().topologyVersion() +\n                        \", process=\" + process +\n                        \", awaited=\" + awaitMergedMsgs + ']');\n                }\n\n                if (process) {\n                    mergedJoinExchMsgs.put(node.id(), msg);\n\n                    assert awaitMergedMsgs > 0 : awaitMergedMsgs;\n\n                    awaitMergedMsgs--;\n\n                    done = awaitMergedMsgs == 0;\n                }\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, node.id());\n\n            return;\n        }\n\n        if (done)\n            finishExchangeOnCoordinator(null);\n    }"
        ],
        [
            "InitNewCoordinatorFuture::onAllReceived()",
            " 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 -\n 252 -\n 253 -\n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 -\n 274 -\n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "    /**\n     *\n     */\n    private void onAllReceived() {\n        if (fullMsg != null) {\n            AffinityTopologyVersion resVer = fullMsg.resultTopologyVersion();\n\n            for (Iterator<Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage>> it = msgs.entrySet().iterator(); it.hasNext();) {\n                Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e = it.next();\n\n                GridDhtPartitionExchangeId msgVer = joinedNodes.get(e.getKey().id());\n\n                if (msgVer != null) {\n                    assert msgVer.topologyVersion().compareTo(initTopVer) > 0 : msgVer;\n\n                    log.info(\"Process joined node message [resVer=\" + resVer +\n                        \", initTopVer=\" + initTopVer +\n                        \", msgVer=\" + msgVer.topologyVersion() + ']');\n\n                    if (msgVer.topologyVersion().compareTo(resVer) > 0)\n                        it.remove();\n                    else\n                        e.getValue().exchangeId(msgVer);\n                }\n            }\n        }\n        else {\n            for (Iterator<Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage>> it = msgs.entrySet().iterator(); it.hasNext();) {\n                Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e = it.next();\n\n                GridDhtPartitionExchangeId msgVer = joinedNodes.get(e.getKey().id());\n\n                if (msgVer != null) {\n                    it.remove();\n\n                    assert msgVer.topologyVersion().compareTo(initTopVer) > 0 : msgVer;\n\n                    log.info(\"Process joined node message [initTopVer=\" + initTopVer +\n                        \", msgVer=\" + msgVer.topologyVersion() + ']');\n\n                    if (joinExchMsgs == null)\n                        joinExchMsgs = new HashMap<>();\n\n                    e.getValue().exchangeId(msgVer);\n\n                    joinExchMsgs.put(e.getKey().id(), e.getValue());\n                }\n            }\n\n        }\n    }",
            " 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 +\n 283 +\n 284 +\n 285 +\n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  ",
            "    /**\n     *\n     */\n    private void onAllReceived() {\n        if (fullMsg != null) {\n            AffinityTopologyVersion resVer = fullMsg.resultTopologyVersion();\n\n            for (Iterator<Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage>> it = msgs.entrySet().iterator(); it.hasNext();) {\n                Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e = it.next();\n\n                GridDhtPartitionExchangeId msgVer = joinedNodes.get(e.getKey().id());\n\n                if (msgVer != null) {\n                    assert msgVer.topologyVersion().compareTo(initTopVer) > 0 : msgVer;\n\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Process joined node message [resVer=\" + resVer +\n                            \", initTopVer=\" + initTopVer +\n                            \", msgVer=\" + msgVer.topologyVersion() + ']');\n                    }\n\n                    if (msgVer.topologyVersion().compareTo(resVer) > 0)\n                        it.remove();\n                    else\n                        e.getValue().exchangeId(msgVer);\n                }\n            }\n        }\n        else {\n            for (Iterator<Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage>> it = msgs.entrySet().iterator(); it.hasNext();) {\n                Map.Entry<ClusterNode, GridDhtPartitionsSingleMessage> e = it.next();\n\n                GridDhtPartitionExchangeId msgVer = joinedNodes.get(e.getKey().id());\n\n                if (msgVer != null) {\n                    it.remove();\n\n                    assert msgVer.topologyVersion().compareTo(initTopVer) > 0 : msgVer;\n\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Process joined node message [initTopVer=\" + initTopVer +\n                            \", msgVer=\" + msgVer.topologyVersion() + ']');\n                    }\n\n                    if (joinExchMsgs == null)\n                        joinExchMsgs = new HashMap<>();\n\n                    e.getValue().exchangeId(msgVer);\n\n                    joinExchMsgs.put(e.getKey().id(), e.getValue());\n                }\n            }\n\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onAllReceived(Collection)",
            "2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173 -\n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  ",
            "    /**\n     * @param sndResNodes Additional nodes to send finish message to.\n     */\n    private void onAllReceived(@Nullable Collection<ClusterNode> sndResNodes) {\n        try {\n            assert crd.isLocal();\n\n            assert partHistSuppliers.isEmpty() : partHistSuppliers;\n\n            if (!exchCtx.mergeExchanges() && !crd.equals(events().discoveryCache().serverNodes().get(0))) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (!grp.isLocal())\n                        grp.topology().beforeExchange(this, !centralizedAff, false);\n                }\n            }\n\n            if (exchCtx.mergeExchanges()) {\n                log.info(\"Coordinator received all messages, try merge [ver=\" + initialVersion() + ']');\n\n                boolean finish = cctx.exchange().mergeExchangesOnCoordinator(this);\n\n                if (!finish)\n                    return;\n            }\n\n            finishExchangeOnCoordinator(sndResNodes);\n        }\n        catch (IgniteCheckedException e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else\n                onDone(e);\n        }\n    }",
            "2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190 +\n2191 +\n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  ",
            "    /**\n     * @param sndResNodes Additional nodes to send finish message to.\n     */\n    private void onAllReceived(@Nullable Collection<ClusterNode> sndResNodes) {\n        try {\n            assert crd.isLocal();\n\n            assert partHistSuppliers.isEmpty() : partHistSuppliers;\n\n            if (!exchCtx.mergeExchanges() && !crd.equals(events().discoveryCache().serverNodes().get(0))) {\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (!grp.isLocal())\n                        grp.topology().beforeExchange(this, !centralizedAff, false);\n                }\n            }\n\n            if (exchCtx.mergeExchanges()) {\n                if (log.isInfoEnabled())\n                    log.info(\"Coordinator received all messages, try merge [ver=\" + initialVersion() + ']');\n\n                boolean finish = cctx.exchange().mergeExchangesOnCoordinator(this);\n\n                if (!finish)\n                    return;\n            }\n\n            finishExchangeOnCoordinator(sndResNodes);\n        }\n        catch (IgniteCheckedException e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else\n                onDone(e);\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onDone(AffinityTopologyVersion,Throwable)",
            "1395  \n1396  \n1397  \n1398  \n1399  \n1400 -\n1401 -\n1402 -\n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  ",
            "    /** {@inheritDoc} */\n    @Override public boolean onDone(@Nullable AffinityTopologyVersion res, @Nullable Throwable err) {\n        if (isDone() || !done.compareAndSet(false, true))\n            return false;\n\n        log.info(\"Finish exchange future [startVer=\" + initialVersion() +\n            \", resVer=\" + res +\n            \", err=\" + err + ']');\n\n        assert res != null || err != null;\n\n        if (err == null &&\n            !cctx.kernalContext().clientNode() &&\n            (serverNodeDiscoveryEvent() || affChangeMsg != null)) {\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                if (!cacheCtx.affinityNode() || cacheCtx.isLocal())\n                    continue;\n\n                cacheCtx.continuousQueries().flushBackupQueue(res);\n            }\n       }\n\n        if (err == null) {\n            if (centralizedAff) {\n                assert !exchCtx.mergeExchanges();\n\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    try {\n                        grp.topology().initPartitionsWhenAffinityReady(res, this);\n                    }\n                    catch (IgniteInterruptedCheckedException e) {\n                        U.error(log, \"Failed to initialize partitions.\", e);\n                    }\n                }\n            }\n\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                GridCacheContext drCacheCtx = cacheCtx.isNear() ? cacheCtx.near().dht().context() : cacheCtx;\n\n                if (drCacheCtx.isDrEnabled()) {\n                    try {\n                        drCacheCtx.dr().onExchange(res, exchId.isLeft());\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to notify DR: \" + e, e);\n                    }\n                }\n            }\n\n            if (serverNodeDiscoveryEvent())\n                detectLostPartitions(res);\n\n            Map<Integer, CacheValidation> m = U.newHashMap(cctx.cache().cacheGroups().size());\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                m.put(grp.groupId(), validateCacheGroup(grp, events().lastEvent().topologyNodes()));\n\n            grpValidRes = m;\n        }\n\n        tryToPerformLocalSnapshotOperation();\n\n        cctx.cache().onExchangeDone(initialVersion(), exchActions, err);\n\n        cctx.exchange().onExchangeDone(res, initialVersion(), err);\n\n        if (exchActions != null && err == null)\n            exchActions.completeRequestFutures(cctx);\n\n        if (stateChangeExchange() && err == null)\n            cctx.kernalContext().state().onStateChangeExchangeDone(exchActions.stateChangeRequest());\n\n        Map<T2<Integer, Integer>, Long> localReserved = partHistSuppliers.getReservations(cctx.localNodeId());\n\n        if (localReserved != null) {\n            for (Map.Entry<T2<Integer, Integer>, Long> e : localReserved.entrySet()) {\n                boolean success = cctx.database().reserveHistoryForPreloading(\n                    e.getKey().get1(), e.getKey().get2(), e.getValue());\n\n                if (!success) {\n                    // TODO: how to handle?\n                    err = new IgniteCheckedException(\"Could not reserve history\");\n                }\n            }\n        }\n\n        cctx.database().releaseHistoryForExchange();\n\n        if (err == null) {\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (!grp.isLocal())\n                    grp.topology().onExchangeDone(this, grp.affinity().readyAffinity(res), false);\n            }\n        }\n\n        if (super.onDone(res, err)) {\n            if (log.isDebugEnabled())\n                log.debug(\"Completed partition exchange [localNode=\" + cctx.localNodeId() + \", exchange= \" + this +\n                    \", durationFromInit=\" + (U.currentTimeMillis() - initTs) + ']');\n\n            initFut.onDone(err == null);\n\n            if (exchCtx != null && exchCtx.events().hasServerLeft()) {\n                ExchangeDiscoveryEvents evts = exchCtx.events();\n\n                for (DiscoveryEvent evt : exchCtx.events().events()) {\n                    if (evts.serverLeftEvent(evt)) {\n                        for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                            grp.affinityFunction().removeNode(evt.eventNode().id());\n                    }\n                }\n            }\n\n            exchActions = null;\n\n            if (firstDiscoEvt instanceof DiscoveryCustomEvent)\n                ((DiscoveryCustomEvent)firstDiscoEvt).customMessage(null);\n\n            if (err == null)\n                cctx.exchange().lastFinishedFuture(this);\n\n            return true;\n        }\n\n        return false;\n    }",
            "1396  \n1397  \n1398  \n1399  \n1400  \n1401 +\n1402 +\n1403 +\n1404 +\n1405 +\n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  ",
            "    /** {@inheritDoc} */\n    @Override public boolean onDone(@Nullable AffinityTopologyVersion res, @Nullable Throwable err) {\n        if (isDone() || !done.compareAndSet(false, true))\n            return false;\n\n        if (log.isInfoEnabled()) {\n            log.info(\"Finish exchange future [startVer=\" + initialVersion() +\n                \", resVer=\" + res +\n                \", err=\" + err + ']');\n        }\n\n        assert res != null || err != null;\n\n        if (err == null &&\n            !cctx.kernalContext().clientNode() &&\n            (serverNodeDiscoveryEvent() || affChangeMsg != null)) {\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                if (!cacheCtx.affinityNode() || cacheCtx.isLocal())\n                    continue;\n\n                cacheCtx.continuousQueries().flushBackupQueue(res);\n            }\n       }\n\n        if (err == null) {\n            if (centralizedAff) {\n                assert !exchCtx.mergeExchanges();\n\n                for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                    if (grp.isLocal())\n                        continue;\n\n                    try {\n                        grp.topology().initPartitionsWhenAffinityReady(res, this);\n                    }\n                    catch (IgniteInterruptedCheckedException e) {\n                        U.error(log, \"Failed to initialize partitions.\", e);\n                    }\n                }\n            }\n\n            for (GridCacheContext cacheCtx : cctx.cacheContexts()) {\n                GridCacheContext drCacheCtx = cacheCtx.isNear() ? cacheCtx.near().dht().context() : cacheCtx;\n\n                if (drCacheCtx.isDrEnabled()) {\n                    try {\n                        drCacheCtx.dr().onExchange(res, exchId.isLeft());\n                    }\n                    catch (IgniteCheckedException e) {\n                        U.error(log, \"Failed to notify DR: \" + e, e);\n                    }\n                }\n            }\n\n            if (serverNodeDiscoveryEvent())\n                detectLostPartitions(res);\n\n            Map<Integer, CacheValidation> m = U.newHashMap(cctx.cache().cacheGroups().size());\n\n            for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                m.put(grp.groupId(), validateCacheGroup(grp, events().lastEvent().topologyNodes()));\n\n            grpValidRes = m;\n        }\n\n        tryToPerformLocalSnapshotOperation();\n\n        cctx.cache().onExchangeDone(initialVersion(), exchActions, err);\n\n        cctx.exchange().onExchangeDone(res, initialVersion(), err);\n\n        if (exchActions != null && err == null)\n            exchActions.completeRequestFutures(cctx);\n\n        if (stateChangeExchange() && err == null)\n            cctx.kernalContext().state().onStateChangeExchangeDone(exchActions.stateChangeRequest());\n\n        Map<T2<Integer, Integer>, Long> localReserved = partHistSuppliers.getReservations(cctx.localNodeId());\n\n        if (localReserved != null) {\n            for (Map.Entry<T2<Integer, Integer>, Long> e : localReserved.entrySet()) {\n                boolean success = cctx.database().reserveHistoryForPreloading(\n                    e.getKey().get1(), e.getKey().get2(), e.getValue());\n\n                if (!success) {\n                    // TODO: how to handle?\n                    err = new IgniteCheckedException(\"Could not reserve history\");\n                }\n            }\n        }\n\n        cctx.database().releaseHistoryForExchange();\n\n        if (err == null) {\n            for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                if (!grp.isLocal())\n                    grp.topology().onExchangeDone(this, grp.affinity().readyAffinity(res), false);\n            }\n        }\n\n        if (super.onDone(res, err)) {\n            if (log.isDebugEnabled())\n                log.debug(\"Completed partition exchange [localNode=\" + cctx.localNodeId() + \", exchange= \" + this +\n                    \", durationFromInit=\" + (U.currentTimeMillis() - initTs) + ']');\n\n            initFut.onDone(err == null);\n\n            if (exchCtx != null && exchCtx.events().hasServerLeft()) {\n                ExchangeDiscoveryEvents evts = exchCtx.events();\n\n                for (DiscoveryEvent evt : exchCtx.events().events()) {\n                    if (evts.serverLeftEvent(evt)) {\n                        for (CacheGroupContext grp : cctx.cache().cacheGroups())\n                            grp.affinityFunction().removeNode(evt.eventNode().id());\n                    }\n                }\n            }\n\n            exchActions = null;\n\n            if (firstDiscoEvt instanceof DiscoveryCustomEvent)\n                ((DiscoveryCustomEvent)firstDiscoEvt).customMessage(null);\n\n            if (err == null)\n                cctx.exchange().lastFinishedFuture(this);\n\n            return true;\n        }\n\n        return false;\n    }"
        ],
        [
            "InitNewCoordinatorFuture::onNodeLeft(UUID)",
            " 292  \n 293  \n 294  \n 295  \n 296 -\n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "    /**\n     * @param nodeId Failed node ID.\n     */\n    public void onNodeLeft(UUID nodeId) {\n        log.info(\"Init new coordinator, node left [node=\" + nodeId + ']');\n\n        boolean done;\n\n        synchronized (this) {\n            done = awaited.remove(nodeId) && awaited.isEmpty();\n        }\n\n        if (done)\n            restoreStateFut.onDone();\n    }",
            " 307  \n 308  \n 309  \n 310  \n 311 +\n 312 +\n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  ",
            "    /**\n     * @param nodeId Failed node ID.\n     */\n    public void onNodeLeft(UUID nodeId) {\n        if (log.isInfoEnabled())\n            log.info(\"Init new coordinator, node left [node=\" + nodeId + ']');\n\n        boolean done;\n\n        synchronized (this) {\n            done = awaited.remove(nodeId) && awaited.isEmpty();\n        }\n\n        if (done)\n            restoreStateFut.onDone();\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::init(boolean)",
            " 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680 -\n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  ",
            "    /**\n     * Starts activity.\n     *\n     * @param newCrd {@code True} if node become coordinator on this exchange.\n     * @throws IgniteInterruptedCheckedException If interrupted.\n     */\n    public void init(boolean newCrd) throws IgniteInterruptedCheckedException {\n        if (isDone())\n            return;\n\n        assert !cctx.kernalContext().isDaemon();\n\n        initTs = U.currentTimeMillis();\n\n        U.await(evtLatch);\n\n        assert firstDiscoEvt != null : this;\n        assert exchId.nodeId().equals(firstDiscoEvt.eventNode().id()) : this;\n\n        try {\n            AffinityTopologyVersion topVer = initialVersion();\n\n            srvNodes = new ArrayList<>(firstEvtDiscoCache.serverNodes());\n\n            remaining.addAll(F.nodeIds(F.view(srvNodes, F.remoteNodes(cctx.localNodeId()))));\n\n            crd = srvNodes.isEmpty() ? null : srvNodes.get(0);\n\n            boolean crdNode = crd != null && crd.isLocal();\n\n            exchCtx = new ExchangeContext(crdNode, this);\n\n            assert state == null : state;\n\n            if (crdNode)\n                state = ExchangeLocalState.CRD;\n            else\n                state = cctx.kernalContext().clientNode() ? ExchangeLocalState.CLIENT : ExchangeLocalState.SRV;\n\n            if (exchLog.isInfoEnabled()) {\n                exchLog.info(\"Started exchange init [topVer=\" + topVer +\n                    \", crd=\" + crdNode +\n                    \", evt=\" + IgniteUtils.gridEventName(firstDiscoEvt.type()) +\n                    \", evtNode=\" + firstDiscoEvt.eventNode().id() +\n                    \", customEvt=\" + (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT ? ((DiscoveryCustomEvent)firstDiscoEvt).customMessage() : null) +\n                    \", allowMerge=\" + exchCtx.mergeExchanges() + ']');\n            }\n\n            ExchangeType exchange;\n\n            if (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                assert !exchCtx.mergeExchanges();\n\n                DiscoveryCustomMessage msg = ((DiscoveryCustomEvent)firstDiscoEvt).customMessage();\n\n                if (msg instanceof ChangeGlobalStateMessage) {\n                    assert exchActions != null && !exchActions.empty();\n\n                    exchange = onClusterStateChangeRequest(crdNode);\n                }\n                else if (msg instanceof DynamicCacheChangeBatch) {\n                    assert exchActions != null && !exchActions.empty();\n\n                    exchange = onCacheChangeRequest(crdNode);\n                }\n                else if (msg instanceof SnapshotDiscoveryMessage) {\n                    exchange = CU.clientNode(firstDiscoEvt.eventNode()) ?\n                        onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                }\n                else {\n                    assert affChangeMsg != null : this;\n\n                    exchange = onAffinityChangeRequest(crdNode);\n                }\n\n                initCoordinatorCaches(newCrd);\n            }\n            else {\n                if (firstDiscoEvt.type() == EVT_NODE_JOINED) {\n                    if (!firstDiscoEvt.eventNode().isLocal()) {\n                        Collection<DynamicCacheDescriptor> receivedCaches = cctx.cache().startReceivedCaches(\n                            firstDiscoEvt.eventNode().id(),\n                            topVer);\n\n                        cctx.affinity().initStartedCaches(crdNode, this, receivedCaches);\n                    }\n                    else\n                        initCachesOnLocalJoin();\n                }\n\n                initCoordinatorCaches(newCrd);\n\n                if (exchCtx.mergeExchanges()) {\n                    if (localJoinExchange()) {\n                        if (cctx.kernalContext().clientNode()) {\n                            onClientNodeEvent(crdNode);\n\n                            exchange = ExchangeType.CLIENT;\n                        }\n                        else {\n                            onServerNodeEvent(crdNode);\n\n                            exchange = ExchangeType.ALL;\n                        }\n                    }\n                    else {\n                        if (CU.clientNode(firstDiscoEvt.eventNode()))\n                            exchange = onClientNodeEvent(crdNode);\n                        else\n                            exchange = cctx.kernalContext().clientNode() ? ExchangeType.CLIENT : ExchangeType.ALL;\n                    }\n\n                    if (exchId.isLeft())\n                        onLeft();\n                }\n                else {\n                    exchange = CU.clientNode(firstDiscoEvt.eventNode()) ? onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                }\n            }\n\n            updateTopologies(crdNode);\n\n            switch (exchange) {\n                case ALL: {\n                    distributedExchange();\n\n                    break;\n                }\n\n                case CLIENT: {\n                    if (!exchCtx.mergeExchanges() && exchCtx.fetchAffinityOnJoin())\n                        initTopologies();\n\n                    clientOnlyExchange();\n\n                    break;\n                }\n\n                case NONE: {\n                    initTopologies();\n\n                    onDone(topVer);\n\n                    break;\n                }\n\n                default:\n                    assert false;\n            }\n\n            if (cctx.localNode().isClient())\n                tryToPerformLocalSnapshotOperation();\n\n            exchLog.info(\"Finished exchange init [topVer=\" + topVer + \", crd=\" + crdNode + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            onDone(e);\n\n            throw e;\n        }\n        catch (IgniteNeedReconnectException e) {\n            onDone(e);\n        }\n        catch (Throwable e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else {\n                U.error(log, \"Failed to reinitialize local partitions (preloading will be stopped): \" + exchId, e);\n\n                onDone(e);\n            }\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n    }",
            " 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680 +\n 681 +\n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  ",
            "    /**\n     * Starts activity.\n     *\n     * @param newCrd {@code True} if node become coordinator on this exchange.\n     * @throws IgniteInterruptedCheckedException If interrupted.\n     */\n    public void init(boolean newCrd) throws IgniteInterruptedCheckedException {\n        if (isDone())\n            return;\n\n        assert !cctx.kernalContext().isDaemon();\n\n        initTs = U.currentTimeMillis();\n\n        U.await(evtLatch);\n\n        assert firstDiscoEvt != null : this;\n        assert exchId.nodeId().equals(firstDiscoEvt.eventNode().id()) : this;\n\n        try {\n            AffinityTopologyVersion topVer = initialVersion();\n\n            srvNodes = new ArrayList<>(firstEvtDiscoCache.serverNodes());\n\n            remaining.addAll(F.nodeIds(F.view(srvNodes, F.remoteNodes(cctx.localNodeId()))));\n\n            crd = srvNodes.isEmpty() ? null : srvNodes.get(0);\n\n            boolean crdNode = crd != null && crd.isLocal();\n\n            exchCtx = new ExchangeContext(crdNode, this);\n\n            assert state == null : state;\n\n            if (crdNode)\n                state = ExchangeLocalState.CRD;\n            else\n                state = cctx.kernalContext().clientNode() ? ExchangeLocalState.CLIENT : ExchangeLocalState.SRV;\n\n            if (exchLog.isInfoEnabled()) {\n                exchLog.info(\"Started exchange init [topVer=\" + topVer +\n                    \", crd=\" + crdNode +\n                    \", evt=\" + IgniteUtils.gridEventName(firstDiscoEvt.type()) +\n                    \", evtNode=\" + firstDiscoEvt.eventNode().id() +\n                    \", customEvt=\" + (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT ? ((DiscoveryCustomEvent)firstDiscoEvt).customMessage() : null) +\n                    \", allowMerge=\" + exchCtx.mergeExchanges() + ']');\n            }\n\n            ExchangeType exchange;\n\n            if (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                assert !exchCtx.mergeExchanges();\n\n                DiscoveryCustomMessage msg = ((DiscoveryCustomEvent)firstDiscoEvt).customMessage();\n\n                if (msg instanceof ChangeGlobalStateMessage) {\n                    assert exchActions != null && !exchActions.empty();\n\n                    exchange = onClusterStateChangeRequest(crdNode);\n                }\n                else if (msg instanceof DynamicCacheChangeBatch) {\n                    assert exchActions != null && !exchActions.empty();\n\n                    exchange = onCacheChangeRequest(crdNode);\n                }\n                else if (msg instanceof SnapshotDiscoveryMessage) {\n                    exchange = CU.clientNode(firstDiscoEvt.eventNode()) ?\n                        onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                }\n                else {\n                    assert affChangeMsg != null : this;\n\n                    exchange = onAffinityChangeRequest(crdNode);\n                }\n\n                initCoordinatorCaches(newCrd);\n            }\n            else {\n                if (firstDiscoEvt.type() == EVT_NODE_JOINED) {\n                    if (!firstDiscoEvt.eventNode().isLocal()) {\n                        Collection<DynamicCacheDescriptor> receivedCaches = cctx.cache().startReceivedCaches(\n                            firstDiscoEvt.eventNode().id(),\n                            topVer);\n\n                        cctx.affinity().initStartedCaches(crdNode, this, receivedCaches);\n                    }\n                    else\n                        initCachesOnLocalJoin();\n                }\n\n                initCoordinatorCaches(newCrd);\n\n                if (exchCtx.mergeExchanges()) {\n                    if (localJoinExchange()) {\n                        if (cctx.kernalContext().clientNode()) {\n                            onClientNodeEvent(crdNode);\n\n                            exchange = ExchangeType.CLIENT;\n                        }\n                        else {\n                            onServerNodeEvent(crdNode);\n\n                            exchange = ExchangeType.ALL;\n                        }\n                    }\n                    else {\n                        if (CU.clientNode(firstDiscoEvt.eventNode()))\n                            exchange = onClientNodeEvent(crdNode);\n                        else\n                            exchange = cctx.kernalContext().clientNode() ? ExchangeType.CLIENT : ExchangeType.ALL;\n                    }\n\n                    if (exchId.isLeft())\n                        onLeft();\n                }\n                else {\n                    exchange = CU.clientNode(firstDiscoEvt.eventNode()) ? onClientNodeEvent(crdNode) :\n                        onServerNodeEvent(crdNode);\n                }\n            }\n\n            updateTopologies(crdNode);\n\n            switch (exchange) {\n                case ALL: {\n                    distributedExchange();\n\n                    break;\n                }\n\n                case CLIENT: {\n                    if (!exchCtx.mergeExchanges() && exchCtx.fetchAffinityOnJoin())\n                        initTopologies();\n\n                    clientOnlyExchange();\n\n                    break;\n                }\n\n                case NONE: {\n                    initTopologies();\n\n                    onDone(topVer);\n\n                    break;\n                }\n\n                default:\n                    assert false;\n            }\n\n            if (cctx.localNode().isClient())\n                tryToPerformLocalSnapshotOperation();\n\n            if (exchLog.isInfoEnabled())\n                exchLog.info(\"Finished exchange init [topVer=\" + topVer + \", crd=\" + crdNode + ']');\n        }\n        catch (IgniteInterruptedCheckedException e) {\n            onDone(e);\n\n            throw e;\n        }\n        catch (IgniteNeedReconnectException e) {\n            onDone(e);\n        }\n        catch (Throwable e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else {\n                U.error(log, \"Failed to reinitialize local partitions (preloading will be stopped): \" + exchId, e);\n\n                onDone(e);\n            }\n\n            if (e instanceof Error)\n                throw (Error)e;\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processSingleMessage(UUID,GridDhtPartitionsSingleMessage)",
            "1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845 -\n1846 -\n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868 -\n1869 -\n1870 -\n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878 -\n1879 -\n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  ",
            "    /**\n     * Note this method performs heavy updatePartitionSingleMap operation, this operation is moved out from the\n     * synchronized block. Only count of such updates {@link #pendingSingleUpdates} is managed under critical section.\n     *\n     * @param nodeId Sender node.\n     * @param msg Partition single message.\n     */\n    private void processSingleMessage(UUID nodeId, GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(nodeId, msg);\n\n            return;\n        }\n\n        boolean allReceived = false; // Received all expected messages.\n        boolean updateSingleMap = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            assert crd != null;\n\n            switch (state) {\n                case DONE: {\n                    log.info(\"Received single message, already done [ver=\" + initialVersion() +\n                        \", node=\" + nodeId + ']');\n\n                    assert finishState != null;\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD: {\n                    assert crd.isLocal() : crd;\n\n                    if (remaining.remove(nodeId)) {\n                        updateSingleMap = true;\n\n                        pendingSingleUpdates++;\n\n                        if (stateChangeExchange() && msg.getError() != null)\n                            changeGlobalStateExceptions.put(nodeId, msg.getError());\n\n                        allReceived = remaining.isEmpty();\n\n                        log.info(\"Coordinator received single message [ver=\" + initialVersion() +\n                            \", node=\" + nodeId +\n                            \", allReceived=\" + allReceived + ']');\n                    }\n\n                    break;\n                }\n\n                case SRV:\n                case BECOME_CRD: {\n                    log.info(\"Non-coordinator received single message [ver=\" + initialVersion() +\n                        \", node=\" + nodeId + \", state=\" + state + ']');\n\n                    pendingSingleMsgs.put(nodeId, msg);\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, nodeId);\n\n            return;\n        }\n\n        if (updateSingleMap) {\n            try {\n                // Do not update partition map, in case cluster transitioning to inactive state.\n                if (!deactivateCluster())\n                    updatePartitionSingleMap(nodeId, msg);\n            }\n            finally {\n                synchronized (mux) {\n                    assert pendingSingleUpdates > 0;\n\n                    pendingSingleUpdates--;\n\n                    if (pendingSingleUpdates == 0)\n                        mux.notifyAll();\n                }\n            }\n        }\n\n        if (allReceived) {\n            if (!awaitSingleMapUpdates())\n                return;\n\n            onAllReceived(null);\n        }\n    }",
            "1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856 +\n1857 +\n1858 +\n1859 +\n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881 +\n1882 +\n1883 +\n1884 +\n1885 +\n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893 +\n1894 +\n1895 +\n1896 +\n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  ",
            "    /**\n     * Note this method performs heavy updatePartitionSingleMap operation, this operation is moved out from the\n     * synchronized block. Only count of such updates {@link #pendingSingleUpdates} is managed under critical section.\n     *\n     * @param nodeId Sender node.\n     * @param msg Partition single message.\n     */\n    private void processSingleMessage(UUID nodeId, GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(nodeId, msg);\n\n            return;\n        }\n\n        boolean allReceived = false; // Received all expected messages.\n        boolean updateSingleMap = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            assert crd != null;\n\n            switch (state) {\n                case DONE: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Received single message, already done [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n\n                    assert finishState != null;\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD: {\n                    assert crd.isLocal() : crd;\n\n                    if (remaining.remove(nodeId)) {\n                        updateSingleMap = true;\n\n                        pendingSingleUpdates++;\n\n                        if (stateChangeExchange() && msg.getError() != null)\n                            changeGlobalStateExceptions.put(nodeId, msg.getError());\n\n                        allReceived = remaining.isEmpty();\n\n                        if (log.isInfoEnabled()) {\n                            log.info(\"Coordinator received single message [ver=\" + initialVersion() +\n                                \", node=\" + nodeId +\n                                \", allReceived=\" + allReceived + ']');\n                        }\n                    }\n\n                    break;\n                }\n\n                case SRV:\n                case BECOME_CRD: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Non-coordinator received single message [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + \", state=\" + state + ']');\n                    }\n\n                    pendingSingleMsgs.put(nodeId, msg);\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, nodeId);\n\n            return;\n        }\n\n        if (updateSingleMap) {\n            try {\n                // Do not update partition map, in case cluster transitioning to inactive state.\n                if (!deactivateCluster())\n                    updatePartitionSingleMap(nodeId, msg);\n            }\n            finally {\n                synchronized (mux) {\n                    assert pendingSingleUpdates > 0;\n\n                    pendingSingleUpdates--;\n\n                    if (pendingSingleUpdates == 0)\n                        mux.notifyAll();\n                }\n            }\n        }\n\n        if (allReceived) {\n            if (!awaitSingleMapUpdates())\n                return;\n\n            onAllReceived(null);\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::finishExchangeOnCoordinator(Collection)",
            "2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198 -\n2199 -\n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351  \n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  \n2386  \n2387  \n2388  \n2389  \n2390  \n2391  \n2392  \n2393  \n2394  \n2395  \n2396  \n2397 -\n2398 -\n2399 -\n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  ",
            "    /**\n     * @param sndResNodes Additional nodes to send finish message to.\n     */\n    private void finishExchangeOnCoordinator(@Nullable Collection<ClusterNode> sndResNodes) {\n        try {\n            AffinityTopologyVersion resTopVer = exchCtx.events().topologyVersion();\n\n            log.info(\"finishExchangeOnCoordinator [topVer=\" + initialVersion() +\n                \", resVer=\" + resTopVer + ']');\n\n            Map<Integer, CacheGroupAffinityMessage> idealAffDiff = null;\n\n            if (exchCtx.mergeExchanges()) {\n                synchronized (mux) {\n                    if (mergedJoinExchMsgs != null) {\n                        for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : mergedJoinExchMsgs.entrySet()) {\n                            msgs.put(e.getKey(), e.getValue());\n\n                            updatePartitionSingleMap(e.getKey(), e.getValue());\n                        }\n                    }\n                }\n\n                assert exchCtx.events().hasServerJoin() || exchCtx.events().hasServerLeft();\n\n                exchCtx.events().processEvents(this);\n\n                if (exchCtx.events().hasServerLeft())\n                    idealAffDiff = cctx.affinity().onServerLeftWithExchangeMergeProtocol(this);\n                else\n                    cctx.affinity().onServerJoinWithExchangeMergeProtocol(this, true);\n\n                for (CacheGroupDescriptor desc : cctx.affinity().cacheGroups().values()) {\n                    if (desc.config().getCacheMode() == CacheMode.LOCAL)\n                        continue;\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(desc.groupId());\n\n                    GridDhtPartitionTopology top = grp != null ? grp.topology() :\n                        cctx.exchange().clientTopology(desc.groupId());\n\n                    top.beforeExchange(this, true, true);\n                }\n            }\n\n            Map<Integer, CacheGroupAffinityMessage> joinedNodeAff = null;\n\n            for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : msgs.entrySet()) {\n                GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                // Apply update counters after all single messages are received.\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    GridDhtPartitionTopology top = grp != null ? grp.topology() :\n                        cctx.exchange().clientTopology(grpId);\n\n                    CachePartitionPartialCountersMap cntrs = msg.partitionUpdateCounters(grpId,\n                        top.partitions());\n\n                    if (cntrs != null)\n                        top.collectUpdateCounters(cntrs);\n                }\n\n                Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n                if (affReq != null) {\n                    joinedNodeAff = CacheGroupAffinityMessage.createAffinityMessages(cctx,\n                        resTopVer,\n                        affReq,\n                        joinedNodeAff);\n                }\n            }\n\n            for (CacheGroupContext grpCtx : cctx.cache().cacheGroups()) {\n                if (!grpCtx.isLocal())\n                    grpCtx.topology().applyUpdateCounters();\n            }\n\n            if (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                assert firstDiscoEvt instanceof DiscoveryCustomEvent;\n\n                if (activateCluster())\n                    assignPartitionsStates();\n\n                if (((DiscoveryCustomEvent)firstDiscoEvt).customMessage() instanceof DynamicCacheChangeBatch) {\n                    if (exchActions != null) {\n                        Set<String> caches = exchActions.cachesToResetLostPartitions();\n\n                        if (!F.isEmpty(caches))\n                            resetLostPartitions(caches);\n                    }\n                }\n            }\n            else {\n                if (exchCtx.events().hasServerJoin())\n                    assignPartitionsStates();\n\n                if (exchCtx.events().hasServerLeft())\n                    detectLostPartitions(resTopVer);\n            }\n\n            updateLastVersion(cctx.versions().last());\n\n            cctx.versions().onExchange(lastVer.get().order());\n\n            IgniteProductVersion minVer = exchCtx.events().discoveryCache().minimumNodeVersion();\n\n            GridDhtPartitionsFullMessage msg = createPartitionsMessage(true,\n                minVer.compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0);\n\n            if (exchCtx.mergeExchanges()) {\n                assert !centralizedAff;\n\n                msg.resultTopologyVersion(resTopVer);\n\n                if (exchCtx.events().hasServerLeft())\n                    msg.idealAffinityDiff(idealAffDiff);\n            }\n\n            msg.prepareMarshal(cctx);\n\n            synchronized (mux) {\n                finishState = new FinishState(crd.id(), resTopVer, msg);\n\n                state = ExchangeLocalState.DONE;\n            }\n\n            if (centralizedAff) {\n                assert !exchCtx.mergeExchanges();\n\n                IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut = cctx.affinity().initAffinityOnNodeLeft(this);\n\n                if (!fut.isDone()) {\n                    fut.listen(new IgniteInClosure<IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>>>() {\n                        @Override public void apply(IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut) {\n                            onAffinityInitialized(fut);\n                        }\n                    });\n                }\n                else\n                    onAffinityInitialized(fut);\n            }\n            else {\n                Set<ClusterNode> nodes;\n\n                Map<UUID, GridDhtPartitionsSingleMessage> mergedJoinExchMsgs0;\n\n                synchronized (mux) {\n                    srvNodes.remove(cctx.localNode());\n\n                    nodes = U.newHashSet(srvNodes.size());\n\n                    nodes.addAll(srvNodes);\n\n                    mergedJoinExchMsgs0 = mergedJoinExchMsgs;\n\n                    if (mergedJoinExchMsgs != null) {\n                        for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : mergedJoinExchMsgs.entrySet()) {\n                            if (e.getValue() != null) {\n                                ClusterNode node = cctx.discovery().node(e.getKey());\n\n                                if (node != null)\n                                    nodes.add(node);\n                            }\n                        }\n                    }\n\n                    if (!F.isEmpty(sndResNodes))\n                        nodes.addAll(sndResNodes);\n                }\n\n                IgniteCheckedException err = null;\n\n                if (stateChangeExchange()) {\n                    StateChangeRequest req = exchActions.stateChangeRequest();\n\n                    assert req != null : exchActions;\n\n                    boolean stateChangeErr = false;\n\n                    if (!F.isEmpty(changeGlobalStateExceptions)) {\n                        stateChangeErr = true;\n\n                        err = new IgniteCheckedException(\"Cluster state change failed.\");\n\n                        cctx.kernalContext().state().onStateChangeError(changeGlobalStateExceptions, req);\n                    }\n\n                    boolean active = !stateChangeErr && req.activate();\n\n                    ChangeGlobalStateFinishMessage stateFinishMsg = new ChangeGlobalStateFinishMessage(\n                        req.requestId(),\n                        active);\n\n                    cctx.discovery().sendCustomEvent(stateFinishMsg);\n                }\n\n                if (!nodes.isEmpty())\n                    sendAllPartitions(msg, nodes, mergedJoinExchMsgs0, joinedNodeAff);\n\n                onDone(exchCtx.events().topologyVersion(), err);\n\n                for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : pendingSingleMsgs.entrySet()) {\n                    log.info(\"Process pending message on coordinator [node=\" + e.getKey() +\n                        \", ver=\" + initialVersion() +\n                        \", resVer=\" + resTopVer + ']');\n\n                    processSingleMessage(e.getKey(), e.getValue());\n                }\n            }\n        }\n        catch (IgniteCheckedException e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else\n                onDone(e);\n        }\n    }",
            "2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216 +\n2217 +\n2218 +\n2219 +\n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  \n2306  \n2307  \n2308  \n2309  \n2310  \n2311  \n2312  \n2313  \n2314  \n2315  \n2316  \n2317  \n2318  \n2319  \n2320  \n2321  \n2322  \n2323  \n2324  \n2325  \n2326  \n2327  \n2328  \n2329  \n2330  \n2331  \n2332  \n2333  \n2334  \n2335  \n2336  \n2337  \n2338  \n2339  \n2340  \n2341  \n2342  \n2343  \n2344  \n2345  \n2346  \n2347  \n2348  \n2349  \n2350  \n2351  \n2352  \n2353  \n2354  \n2355  \n2356  \n2357  \n2358  \n2359  \n2360  \n2361  \n2362  \n2363  \n2364  \n2365  \n2366  \n2367  \n2368  \n2369  \n2370  \n2371  \n2372  \n2373  \n2374  \n2375  \n2376  \n2377  \n2378  \n2379  \n2380  \n2381  \n2382  \n2383  \n2384  \n2385  \n2386  \n2387  \n2388  \n2389  \n2390  \n2391  \n2392  \n2393  \n2394  \n2395  \n2396  \n2397  \n2398  \n2399  \n2400  \n2401  \n2402  \n2403  \n2404  \n2405  \n2406  \n2407  \n2408  \n2409  \n2410  \n2411  \n2412  \n2413  \n2414  \n2415  \n2416  \n2417 +\n2418 +\n2419 +\n2420 +\n2421 +\n2422  \n2423  \n2424  \n2425  \n2426  \n2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  ",
            "    /**\n     * @param sndResNodes Additional nodes to send finish message to.\n     */\n    private void finishExchangeOnCoordinator(@Nullable Collection<ClusterNode> sndResNodes) {\n        try {\n            AffinityTopologyVersion resTopVer = exchCtx.events().topologyVersion();\n\n            if (log.isInfoEnabled()) {\n                log.info(\"finishExchangeOnCoordinator [topVer=\" + initialVersion() +\n                    \", resVer=\" + resTopVer + ']');\n            }\n\n            Map<Integer, CacheGroupAffinityMessage> idealAffDiff = null;\n\n            if (exchCtx.mergeExchanges()) {\n                synchronized (mux) {\n                    if (mergedJoinExchMsgs != null) {\n                        for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : mergedJoinExchMsgs.entrySet()) {\n                            msgs.put(e.getKey(), e.getValue());\n\n                            updatePartitionSingleMap(e.getKey(), e.getValue());\n                        }\n                    }\n                }\n\n                assert exchCtx.events().hasServerJoin() || exchCtx.events().hasServerLeft();\n\n                exchCtx.events().processEvents(this);\n\n                if (exchCtx.events().hasServerLeft())\n                    idealAffDiff = cctx.affinity().onServerLeftWithExchangeMergeProtocol(this);\n                else\n                    cctx.affinity().onServerJoinWithExchangeMergeProtocol(this, true);\n\n                for (CacheGroupDescriptor desc : cctx.affinity().cacheGroups().values()) {\n                    if (desc.config().getCacheMode() == CacheMode.LOCAL)\n                        continue;\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(desc.groupId());\n\n                    GridDhtPartitionTopology top = grp != null ? grp.topology() :\n                        cctx.exchange().clientTopology(desc.groupId());\n\n                    top.beforeExchange(this, true, true);\n                }\n            }\n\n            Map<Integer, CacheGroupAffinityMessage> joinedNodeAff = null;\n\n            for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : msgs.entrySet()) {\n                GridDhtPartitionsSingleMessage msg = e.getValue();\n\n                // Apply update counters after all single messages are received.\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    GridDhtPartitionTopology top = grp != null ? grp.topology() :\n                        cctx.exchange().clientTopology(grpId);\n\n                    CachePartitionPartialCountersMap cntrs = msg.partitionUpdateCounters(grpId,\n                        top.partitions());\n\n                    if (cntrs != null)\n                        top.collectUpdateCounters(cntrs);\n                }\n\n                Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n                if (affReq != null) {\n                    joinedNodeAff = CacheGroupAffinityMessage.createAffinityMessages(cctx,\n                        resTopVer,\n                        affReq,\n                        joinedNodeAff);\n                }\n            }\n\n            for (CacheGroupContext grpCtx : cctx.cache().cacheGroups()) {\n                if (!grpCtx.isLocal())\n                    grpCtx.topology().applyUpdateCounters();\n            }\n\n            if (firstDiscoEvt.type() == EVT_DISCOVERY_CUSTOM_EVT) {\n                assert firstDiscoEvt instanceof DiscoveryCustomEvent;\n\n                if (activateCluster())\n                    assignPartitionsStates();\n\n                if (((DiscoveryCustomEvent)firstDiscoEvt).customMessage() instanceof DynamicCacheChangeBatch) {\n                    if (exchActions != null) {\n                        Set<String> caches = exchActions.cachesToResetLostPartitions();\n\n                        if (!F.isEmpty(caches))\n                            resetLostPartitions(caches);\n                    }\n                }\n            }\n            else {\n                if (exchCtx.events().hasServerJoin())\n                    assignPartitionsStates();\n\n                if (exchCtx.events().hasServerLeft())\n                    detectLostPartitions(resTopVer);\n            }\n\n            updateLastVersion(cctx.versions().last());\n\n            cctx.versions().onExchange(lastVer.get().order());\n\n            IgniteProductVersion minVer = exchCtx.events().discoveryCache().minimumNodeVersion();\n\n            GridDhtPartitionsFullMessage msg = createPartitionsMessage(true,\n                minVer.compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0);\n\n            if (exchCtx.mergeExchanges()) {\n                assert !centralizedAff;\n\n                msg.resultTopologyVersion(resTopVer);\n\n                if (exchCtx.events().hasServerLeft())\n                    msg.idealAffinityDiff(idealAffDiff);\n            }\n\n            msg.prepareMarshal(cctx);\n\n            synchronized (mux) {\n                finishState = new FinishState(crd.id(), resTopVer, msg);\n\n                state = ExchangeLocalState.DONE;\n            }\n\n            if (centralizedAff) {\n                assert !exchCtx.mergeExchanges();\n\n                IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut = cctx.affinity().initAffinityOnNodeLeft(this);\n\n                if (!fut.isDone()) {\n                    fut.listen(new IgniteInClosure<IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>>>() {\n                        @Override public void apply(IgniteInternalFuture<Map<Integer, Map<Integer, List<UUID>>>> fut) {\n                            onAffinityInitialized(fut);\n                        }\n                    });\n                }\n                else\n                    onAffinityInitialized(fut);\n            }\n            else {\n                Set<ClusterNode> nodes;\n\n                Map<UUID, GridDhtPartitionsSingleMessage> mergedJoinExchMsgs0;\n\n                synchronized (mux) {\n                    srvNodes.remove(cctx.localNode());\n\n                    nodes = U.newHashSet(srvNodes.size());\n\n                    nodes.addAll(srvNodes);\n\n                    mergedJoinExchMsgs0 = mergedJoinExchMsgs;\n\n                    if (mergedJoinExchMsgs != null) {\n                        for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : mergedJoinExchMsgs.entrySet()) {\n                            if (e.getValue() != null) {\n                                ClusterNode node = cctx.discovery().node(e.getKey());\n\n                                if (node != null)\n                                    nodes.add(node);\n                            }\n                        }\n                    }\n\n                    if (!F.isEmpty(sndResNodes))\n                        nodes.addAll(sndResNodes);\n                }\n\n                IgniteCheckedException err = null;\n\n                if (stateChangeExchange()) {\n                    StateChangeRequest req = exchActions.stateChangeRequest();\n\n                    assert req != null : exchActions;\n\n                    boolean stateChangeErr = false;\n\n                    if (!F.isEmpty(changeGlobalStateExceptions)) {\n                        stateChangeErr = true;\n\n                        err = new IgniteCheckedException(\"Cluster state change failed.\");\n\n                        cctx.kernalContext().state().onStateChangeError(changeGlobalStateExceptions, req);\n                    }\n\n                    boolean active = !stateChangeErr && req.activate();\n\n                    ChangeGlobalStateFinishMessage stateFinishMsg = new ChangeGlobalStateFinishMessage(\n                        req.requestId(),\n                        active);\n\n                    cctx.discovery().sendCustomEvent(stateFinishMsg);\n                }\n\n                if (!nodes.isEmpty())\n                    sendAllPartitions(msg, nodes, mergedJoinExchMsgs0, joinedNodeAff);\n\n                onDone(exchCtx.events().topologyVersion(), err);\n\n                for (Map.Entry<UUID, GridDhtPartitionsSingleMessage> e : pendingSingleMsgs.entrySet()) {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Process pending message on coordinator [node=\" + e.getKey() +\n                            \", ver=\" + initialVersion() +\n                            \", resVer=\" + resTopVer + ']');\n                    }\n\n                    processSingleMessage(e.getKey(), e.getValue());\n                }\n            }\n        }\n        catch (IgniteCheckedException e) {\n            if (reconnectOnError(e))\n                onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n            else\n                onDone(e);\n        }\n    }"
        ],
        [
            "InitNewCoordinatorFuture::onMessage(ClusterNode,GridDhtPartitionsSingleMessage)",
            " 200  \n 201  \n 202  \n 203  \n 204  \n 205 -\n 206 -\n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  ",
            "    /**\n     * @param node Node.\n     * @param msg Message.\n     */\n    public void onMessage(ClusterNode node, GridDhtPartitionsSingleMessage msg) {\n        log.info(\"Init new coordinator, received response [node=\" + node.id() +\n            \", fullMsg=\" + (msg.finishMessage() != null) +\n            \", affReq=\" + !F.isEmpty(msg.cacheGroupsAffinityRequest()) + ']');\n\n        assert msg.restoreState() : msg;\n\n        boolean done = false;\n\n        synchronized (this) {\n            if (awaited.remove(node.id())) {\n                GridDhtPartitionsFullMessage fullMsg0 = msg.finishMessage();\n\n                if (fullMsg0 != null) {\n                    assert fullMsg == null || fullMsg.resultTopologyVersion().equals(fullMsg0.resultTopologyVersion());\n\n                    fullMsg  = fullMsg0;\n                }\n                else\n                    msgs.put(node, msg);\n\n                done = awaited.isEmpty();\n            }\n\n            if (done)\n                onAllReceived();\n        }\n\n        if (done)\n            restoreStateFut.onDone();\n    }",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210 +\n 211 +\n 212 +\n 213 +\n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "    /**\n     * @param node Node.\n     * @param msg Message.\n     */\n    public void onMessage(ClusterNode node, GridDhtPartitionsSingleMessage msg) {\n        if (log.isInfoEnabled()) {\n            log.info(\"Init new coordinator, received response [node=\" + node.id() +\n                \", fullMsg=\" + (msg.finishMessage() != null) +\n                \", affReq=\" + !F.isEmpty(msg.cacheGroupsAffinityRequest()) + ']');\n        }\n\n        assert msg.restoreState() : msg;\n\n        boolean done = false;\n\n        synchronized (this) {\n            if (awaited.remove(node.id())) {\n                GridDhtPartitionsFullMessage fullMsg0 = msg.finishMessage();\n\n                if (fullMsg0 != null) {\n                    assert fullMsg == null || fullMsg.resultTopologyVersion().equals(fullMsg0.resultTopologyVersion());\n\n                    fullMsg  = fullMsg0;\n                }\n                else\n                    msgs.put(node, msg);\n\n                done = awaited.isEmpty();\n            }\n\n            if (done)\n                onAllReceived();\n        }\n\n        if (done)\n            restoreStateFut.onDone();\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onNodeLeft(ClusterNode)",
            "2930  \n2931  \n2932  \n2933  \n2934  \n2935  \n2936  \n2937  \n2938  \n2939  \n2940  \n2941  \n2942  \n2943  \n2944  \n2945  \n2946  \n2947  \n2948  \n2949  \n2950  \n2951  \n2952  \n2953  \n2954  \n2955  \n2956  \n2957  \n2958  \n2959  \n2960  \n2961  \n2962  \n2963  \n2964  \n2965  \n2966  \n2967  \n2968  \n2969  \n2970  \n2971  \n2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027 -\n3028 -\n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  \n3057  \n3058  \n3059  \n3060  \n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069  \n3070  \n3071  \n3072 -\n3073 -\n3074 -\n3075 -\n3076  \n3077  \n3078  \n3079  \n3080  \n3081  \n3082  \n3083  \n3084 -\n3085 -\n3086 -\n3087 -\n3088  \n3089  \n3090  \n3091  \n3092  \n3093  \n3094  \n3095  \n3096  \n3097  \n3098  \n3099  \n3100  \n3101  \n3102  \n3103  \n3104  \n3105  \n3106  \n3107  \n3108  ",
            "    /**\n     * Node left callback, processed from the same thread as {@link #onAffinityChangeMessage}.\n     *\n     * @param node Left node.\n     */\n    public void onNodeLeft(final ClusterNode node) {\n        if (isDone() || !enterBusy())\n            return;\n\n        cctx.mvcc().removeExplicitNodeLocks(node.id(), initialVersion());\n\n        try {\n            onDiscoveryEvent(new IgniteRunnable() {\n                @Override public void run() {\n                    if (isDone() || !enterBusy())\n                        return;\n\n                    try {\n                        boolean crdChanged = false;\n                        boolean allReceived = false;\n\n                        ClusterNode crd0;\n\n                        events().discoveryCache().updateAlives(node);\n\n                        InitNewCoordinatorFuture newCrdFut0;\n\n                        synchronized (mux) {\n                            newCrdFut0 = newCrdFut;\n                        }\n\n                        if (newCrdFut0 != null)\n                            newCrdFut0.onNodeLeft(node.id());\n\n                        synchronized (mux) {\n                            if (!srvNodes.remove(node))\n                                return;\n\n                            boolean rmvd = remaining.remove(node.id());\n\n                            if (!rmvd) {\n                                if (mergedJoinExchMsgs != null && mergedJoinExchMsgs.containsKey(node.id())) {\n                                    if (mergedJoinExchMsgs.get(node.id()) == null) {\n                                        mergedJoinExchMsgs.remove(node.id());\n\n                                        rmvd = true;\n                                    }\n                                }\n                            }\n\n                            if (node.equals(crd)) {\n                                crdChanged = true;\n\n                                crd = !srvNodes.isEmpty() ? srvNodes.get(0) : null;\n                            }\n\n                            switch (state) {\n                                case DONE:\n                                    return;\n\n                                case CRD:\n                                    allReceived = rmvd && (remaining.isEmpty() && F.isEmpty(mergedJoinExchMsgs));\n\n                                    break;\n\n                                case SRV:\n                                    assert crd != null;\n\n                                    if (crdChanged && crd.isLocal()) {\n                                        state = ExchangeLocalState.BECOME_CRD;\n\n                                        newCrdFut = new InitNewCoordinatorFuture(cctx);\n                                    }\n\n                                    break;\n                            }\n\n                            crd0 = crd;\n\n                            if (crd0 == null) {\n                                finishState = new FinishState(null, initialVersion(), null);\n                            }\n                        }\n\n                        if (crd0 == null) {\n                            onAllServersLeft();\n\n                            onDone(initialVersion());\n\n                            return;\n                        }\n\n                        if (crd0.isLocal()) {\n                            if (stateChangeExchange() && changeGlobalStateE != null)\n                                changeGlobalStateExceptions.put(crd0.id(), changeGlobalStateE);\n\n                            if (crdChanged) {\n                                log.info(\"Coordinator failed, node is new coordinator [ver=\" + initialVersion() +\n                                    \", prev=\" + node.id() + ']');\n\n                                assert newCrdFut != null;\n\n                                cctx.kernalContext().closure().callLocal(new Callable<Void>() {\n                                    @Override public Void call() throws Exception {\n                                        newCrdFut.init(GridDhtPartitionsExchangeFuture.this);\n\n                                        newCrdFut.listen(new CI1<IgniteInternalFuture>() {\n                                            @Override public void apply(IgniteInternalFuture fut) {\n                                                if (isDone())\n                                                    return;\n\n                                                Lock lock = cctx.io().readLock();\n\n                                                if (lock == null)\n                                                    return;\n\n                                                try {\n                                                    onBecomeCoordinator((InitNewCoordinatorFuture) fut);\n                                                }\n                                                finally {\n                                                    lock.unlock();\n                                                }\n                                            }\n                                        });\n\n                                        return null;\n                                    }\n                                }, GridIoPolicy.SYSTEM_POOL);\n\n                                return;\n                            }\n\n                            if (allReceived) {\n                                awaitSingleMapUpdates();\n\n                                onAllReceived(null);\n                            }\n                        }\n                        else {\n                            if (crdChanged) {\n                                for (Map.Entry<ClusterNode, GridDhtPartitionsFullMessage> m : fullMsgs.entrySet()) {\n                                    if (crd0.equals(m.getKey())) {\n                                        log.info(\"Coordinator changed, process pending full message [\" +\n                                            \"ver=\" + initialVersion() +\n                                            \", crd=\" + node.id() +\n                                            \", pendingMsgNode=\" + m.getKey() + ']');\n\n                                        processFullMessage(true, m.getKey(), m.getValue());\n\n                                        if (isDone())\n                                            return;\n                                    }\n                                }\n\n                                log.info(\"Coordinator changed, send partitions to new coordinator [\" +\n                                    \"ver=\" + initialVersion() +\n                                    \", crd=\" + node.id() +\n                                    \", newCrd=\" + crd0.id() + ']');\n\n                                sendPartitions(crd0);\n                            }\n                        }\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (reconnectOnError(e))\n                            onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n                        else\n                            U.error(log, \"Failed to process node left event: \" + e, e);\n                    }\n                    finally {\n                        leaveBusy();\n                    }\n                }\n            });\n        }\n        finally {\n            leaveBusy();\n        }\n    }",
            "2972  \n2973  \n2974  \n2975  \n2976  \n2977  \n2978  \n2979  \n2980  \n2981  \n2982  \n2983  \n2984  \n2985  \n2986  \n2987  \n2988  \n2989  \n2990  \n2991  \n2992  \n2993  \n2994  \n2995  \n2996  \n2997  \n2998  \n2999  \n3000  \n3001  \n3002  \n3003  \n3004  \n3005  \n3006  \n3007  \n3008  \n3009  \n3010  \n3011  \n3012  \n3013  \n3014  \n3015  \n3016  \n3017  \n3018  \n3019  \n3020  \n3021  \n3022  \n3023  \n3024  \n3025  \n3026  \n3027  \n3028  \n3029  \n3030  \n3031  \n3032  \n3033  \n3034  \n3035  \n3036  \n3037  \n3038  \n3039  \n3040  \n3041  \n3042  \n3043  \n3044  \n3045  \n3046  \n3047  \n3048  \n3049  \n3050  \n3051  \n3052  \n3053  \n3054  \n3055  \n3056  \n3057  \n3058  \n3059  \n3060  \n3061  \n3062  \n3063  \n3064  \n3065  \n3066  \n3067  \n3068  \n3069 +\n3070 +\n3071 +\n3072 +\n3073  \n3074  \n3075  \n3076  \n3077  \n3078  \n3079  \n3080  \n3081  \n3082  \n3083  \n3084  \n3085  \n3086  \n3087  \n3088  \n3089  \n3090  \n3091  \n3092  \n3093  \n3094  \n3095  \n3096  \n3097  \n3098  \n3099  \n3100  \n3101  \n3102  \n3103  \n3104  \n3105  \n3106  \n3107  \n3108  \n3109  \n3110  \n3111  \n3112  \n3113  \n3114  \n3115  \n3116 +\n3117 +\n3118 +\n3119 +\n3120 +\n3121 +\n3122  \n3123  \n3124  \n3125  \n3126  \n3127  \n3128  \n3129  \n3130 +\n3131 +\n3132 +\n3133 +\n3134 +\n3135 +\n3136  \n3137  \n3138  \n3139  \n3140  \n3141  \n3142  \n3143  \n3144  \n3145  \n3146  \n3147  \n3148  \n3149  \n3150  \n3151  \n3152  \n3153  \n3154  \n3155  \n3156  ",
            "    /**\n     * Node left callback, processed from the same thread as {@link #onAffinityChangeMessage}.\n     *\n     * @param node Left node.\n     */\n    public void onNodeLeft(final ClusterNode node) {\n        if (isDone() || !enterBusy())\n            return;\n\n        cctx.mvcc().removeExplicitNodeLocks(node.id(), initialVersion());\n\n        try {\n            onDiscoveryEvent(new IgniteRunnable() {\n                @Override public void run() {\n                    if (isDone() || !enterBusy())\n                        return;\n\n                    try {\n                        boolean crdChanged = false;\n                        boolean allReceived = false;\n\n                        ClusterNode crd0;\n\n                        events().discoveryCache().updateAlives(node);\n\n                        InitNewCoordinatorFuture newCrdFut0;\n\n                        synchronized (mux) {\n                            newCrdFut0 = newCrdFut;\n                        }\n\n                        if (newCrdFut0 != null)\n                            newCrdFut0.onNodeLeft(node.id());\n\n                        synchronized (mux) {\n                            if (!srvNodes.remove(node))\n                                return;\n\n                            boolean rmvd = remaining.remove(node.id());\n\n                            if (!rmvd) {\n                                if (mergedJoinExchMsgs != null && mergedJoinExchMsgs.containsKey(node.id())) {\n                                    if (mergedJoinExchMsgs.get(node.id()) == null) {\n                                        mergedJoinExchMsgs.remove(node.id());\n\n                                        rmvd = true;\n                                    }\n                                }\n                            }\n\n                            if (node.equals(crd)) {\n                                crdChanged = true;\n\n                                crd = !srvNodes.isEmpty() ? srvNodes.get(0) : null;\n                            }\n\n                            switch (state) {\n                                case DONE:\n                                    return;\n\n                                case CRD:\n                                    allReceived = rmvd && (remaining.isEmpty() && F.isEmpty(mergedJoinExchMsgs));\n\n                                    break;\n\n                                case SRV:\n                                    assert crd != null;\n\n                                    if (crdChanged && crd.isLocal()) {\n                                        state = ExchangeLocalState.BECOME_CRD;\n\n                                        newCrdFut = new InitNewCoordinatorFuture(cctx);\n                                    }\n\n                                    break;\n                            }\n\n                            crd0 = crd;\n\n                            if (crd0 == null) {\n                                finishState = new FinishState(null, initialVersion(), null);\n                            }\n                        }\n\n                        if (crd0 == null) {\n                            onAllServersLeft();\n\n                            onDone(initialVersion());\n\n                            return;\n                        }\n\n                        if (crd0.isLocal()) {\n                            if (stateChangeExchange() && changeGlobalStateE != null)\n                                changeGlobalStateExceptions.put(crd0.id(), changeGlobalStateE);\n\n                            if (crdChanged) {\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Coordinator failed, node is new coordinator [ver=\" + initialVersion() +\n                                        \", prev=\" + node.id() + ']');\n                                }\n\n                                assert newCrdFut != null;\n\n                                cctx.kernalContext().closure().callLocal(new Callable<Void>() {\n                                    @Override public Void call() throws Exception {\n                                        newCrdFut.init(GridDhtPartitionsExchangeFuture.this);\n\n                                        newCrdFut.listen(new CI1<IgniteInternalFuture>() {\n                                            @Override public void apply(IgniteInternalFuture fut) {\n                                                if (isDone())\n                                                    return;\n\n                                                Lock lock = cctx.io().readLock();\n\n                                                if (lock == null)\n                                                    return;\n\n                                                try {\n                                                    onBecomeCoordinator((InitNewCoordinatorFuture) fut);\n                                                }\n                                                finally {\n                                                    lock.unlock();\n                                                }\n                                            }\n                                        });\n\n                                        return null;\n                                    }\n                                }, GridIoPolicy.SYSTEM_POOL);\n\n                                return;\n                            }\n\n                            if (allReceived) {\n                                awaitSingleMapUpdates();\n\n                                onAllReceived(null);\n                            }\n                        }\n                        else {\n                            if (crdChanged) {\n                                for (Map.Entry<ClusterNode, GridDhtPartitionsFullMessage> m : fullMsgs.entrySet()) {\n                                    if (crd0.equals(m.getKey())) {\n                                        if (log.isInfoEnabled()) {\n                                            log.info(\"Coordinator changed, process pending full message [\" +\n                                                \"ver=\" + initialVersion() +\n                                                \", crd=\" + node.id() +\n                                                \", pendingMsgNode=\" + m.getKey() + ']');\n                                        }\n\n                                        processFullMessage(true, m.getKey(), m.getValue());\n\n                                        if (isDone())\n                                            return;\n                                    }\n                                }\n\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Coordinator changed, send partitions to new coordinator [\" +\n                                        \"ver=\" + initialVersion() +\n                                        \", crd=\" + node.id() +\n                                        \", newCrd=\" + crd0.id() + ']');\n                                }\n\n                                sendPartitions(crd0);\n                            }\n                        }\n                    }\n                    catch (IgniteCheckedException e) {\n                        if (reconnectOnError(e))\n                            onDone(new IgniteNeedReconnectException(cctx.localNode(), e));\n                        else\n                            U.error(log, \"Failed to process node left event: \" + e, e);\n                    }\n                    finally {\n                        leaveBusy();\n                    }\n                }\n            });\n        }\n        finally {\n            leaveBusy();\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processFullMessage(boolean,ClusterNode,GridDhtPartitionsFullMessage)",
            "2626  \n2627  \n2628  \n2629  \n2630  \n2631  \n2632  \n2633  \n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640 -\n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648 -\n2649  \n2650  \n2651  \n2652  \n2653  \n2654 -\n2655  \n2656  \n2657  \n2658  \n2659  \n2660  \n2661  \n2662 -\n2663 -\n2664 -\n2665 -\n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673  \n2674  \n2675 -\n2676 -\n2677  \n2678  \n2679  \n2680  \n2681  \n2682  \n2683  \n2684  \n2685  \n2686  \n2687  \n2688  \n2689  \n2690  \n2691  \n2692  \n2693  \n2694  \n2695 -\n2696 -\n2697  \n2698  \n2699  \n2700  \n2701  \n2702  \n2703  \n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712  \n2713  \n2714  \n2715  \n2716  \n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735  \n2736  \n2737  \n2738  \n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processFullMessage(boolean checkCrd, ClusterNode node, GridDhtPartitionsFullMessage msg) {\n        try {\n            assert exchId.equals(msg.exchangeId()) : msg;\n            assert msg.lastVersion() != null : msg;\n\n            if (checkCrd) {\n                assert node != null;\n\n                synchronized (mux) {\n                    if (crd == null) {\n                        log.info(\"Ignore full message, all server nodes left: \" + msg);\n\n                        return;\n                    }\n\n                    switch (state) {\n                        case CRD:\n                        case BECOME_CRD: {\n                            log.info(\"Ignore full message, node is coordinator: \" + msg);\n\n                            return;\n                        }\n\n                        case DONE: {\n                            log.info(\"Ignore full message, future is done: \" + msg);\n\n                            return;\n                        }\n\n                        case SRV:\n                        case CLIENT: {\n                            if (!crd.equals(node)) {\n                                log.info(\"Received full message from non-coordinator [node=\" + node.id() +\n                                    \", nodeOrder=\" + node.order() +\n                                    \", crd=\" + crd.id() +\n                                    \", crdOrder=\" + crd.order() + ']');\n\n                                if (node.order() > crd.order())\n                                    fullMsgs.put(node, msg);\n\n                                return;\n                            }\n                            else {\n                                AffinityTopologyVersion resVer = msg.resultTopologyVersion() != null ? msg.resultTopologyVersion() : initialVersion();\n\n                                log.info(\"Received full message, will finish exchange [node=\" + node.id() +\n                                    \", resVer=\" + resVer + ']');\n\n                                finishState = new FinishState(crd.id(), resVer, msg);\n\n                                state = ExchangeLocalState.DONE;\n\n                                break;\n                            }\n                        }\n                    }\n                }\n            }\n            else\n                assert node == null : node;\n\n            AffinityTopologyVersion resTopVer = initialVersion();\n\n            if (exchCtx.mergeExchanges()) {\n                if (msg.resultTopologyVersion() != null && !initialVersion().equals(msg.resultTopologyVersion())) {\n                    log.info(\"Received full message, need merge [curFut=\" + initialVersion() +\n                        \", resVer=\" + msg.resultTopologyVersion() + ']');\n\n                    resTopVer = msg.resultTopologyVersion();\n\n                    if (cctx.exchange().mergeExchanges(this, msg)) {\n                        assert cctx.kernalContext().isStopping();\n\n                        return; // Node is stopping, no need to further process exchange.\n                    }\n\n                    assert resTopVer.equals(exchCtx.events().topologyVersion()) :  \"Unexpected result version [\" +\n                        \"msgVer=\" + resTopVer +\n                        \", locVer=\" + exchCtx.events().topologyVersion() + ']';\n                }\n\n                exchCtx.events().processEvents(this);\n\n                if (localJoinExchange())\n                    cctx.affinity().onLocalJoin(this, msg, resTopVer);\n                else {\n                    if (exchCtx.events().hasServerLeft())\n                        cctx.affinity().mergeExchangesOnServerLeft(this, msg);\n                    else\n                        cctx.affinity().onServerJoinWithExchangeMergeProtocol(this, false);\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal() || cacheGroupStopping(grp.groupId()))\n                            continue;\n\n                        grp.topology().beforeExchange(this, true, false);\n                    }\n                }\n            }\n            else if (localJoinExchange() && !exchCtx.fetchAffinityOnJoin())\n                cctx.affinity().onLocalJoin(this, msg, resTopVer);\n\n            updatePartitionFullMap(resTopVer, msg);\n\n            IgniteCheckedException err = null;\n\n            if (stateChangeExchange() && !F.isEmpty(msg.getErrorsMap())) {\n                err = new IgniteCheckedException(\"Cluster state change failed\");\n\n                cctx.kernalContext().state().onStateChangeError(msg.getErrorsMap(), exchActions.stateChangeRequest());\n            }\n\n            onDone(resTopVer, err);\n        }\n        catch (IgniteCheckedException e) {\n            onDone(e);\n        }\n    }",
            "2658  \n2659  \n2660  \n2661  \n2662  \n2663  \n2664  \n2665  \n2666  \n2667  \n2668  \n2669  \n2670  \n2671  \n2672  \n2673 +\n2674 +\n2675  \n2676  \n2677  \n2678  \n2679  \n2680  \n2681  \n2682 +\n2683 +\n2684  \n2685  \n2686  \n2687  \n2688  \n2689 +\n2690 +\n2691  \n2692  \n2693  \n2694  \n2695  \n2696  \n2697  \n2698 +\n2699 +\n2700 +\n2701 +\n2702 +\n2703 +\n2704  \n2705  \n2706  \n2707  \n2708  \n2709  \n2710  \n2711  \n2712  \n2713 +\n2714 +\n2715 +\n2716 +\n2717  \n2718  \n2719  \n2720  \n2721  \n2722  \n2723  \n2724  \n2725  \n2726  \n2727  \n2728  \n2729  \n2730  \n2731  \n2732  \n2733  \n2734  \n2735 +\n2736 +\n2737 +\n2738 +\n2739  \n2740  \n2741  \n2742  \n2743  \n2744  \n2745  \n2746  \n2747  \n2748  \n2749  \n2750  \n2751  \n2752  \n2753  \n2754  \n2755  \n2756  \n2757  \n2758  \n2759  \n2760  \n2761  \n2762  \n2763  \n2764  \n2765  \n2766  \n2767  \n2768  \n2769  \n2770  \n2771  \n2772  \n2773  \n2774  \n2775  \n2776  \n2777  \n2778  \n2779  \n2780  \n2781  \n2782  \n2783  \n2784  \n2785  \n2786  \n2787  \n2788  \n2789  ",
            "    /**\n     * @param checkCrd If {@code true} checks that local node is exchange coordinator.\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processFullMessage(boolean checkCrd, ClusterNode node, GridDhtPartitionsFullMessage msg) {\n        try {\n            assert exchId.equals(msg.exchangeId()) : msg;\n            assert msg.lastVersion() != null : msg;\n\n            if (checkCrd) {\n                assert node != null;\n\n                synchronized (mux) {\n                    if (crd == null) {\n                        if (log.isInfoEnabled())\n                            log.info(\"Ignore full message, all server nodes left: \" + msg);\n\n                        return;\n                    }\n\n                    switch (state) {\n                        case CRD:\n                        case BECOME_CRD: {\n                            if (log.isInfoEnabled())\n                                log.info(\"Ignore full message, node is coordinator: \" + msg);\n\n                            return;\n                        }\n\n                        case DONE: {\n                            if (log.isInfoEnabled())\n                                log.info(\"Ignore full message, future is done: \" + msg);\n\n                            return;\n                        }\n\n                        case SRV:\n                        case CLIENT: {\n                            if (!crd.equals(node)) {\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Received full message from non-coordinator [node=\" + node.id() +\n                                        \", nodeOrder=\" + node.order() +\n                                        \", crd=\" + crd.id() +\n                                        \", crdOrder=\" + crd.order() + ']');\n                                }\n\n                                if (node.order() > crd.order())\n                                    fullMsgs.put(node, msg);\n\n                                return;\n                            }\n                            else {\n                                AffinityTopologyVersion resVer = msg.resultTopologyVersion() != null ? msg.resultTopologyVersion() : initialVersion();\n\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Received full message, will finish exchange [node=\" + node.id() +\n                                        \", resVer=\" + resVer + ']');\n                                }\n\n                                finishState = new FinishState(crd.id(), resVer, msg);\n\n                                state = ExchangeLocalState.DONE;\n\n                                break;\n                            }\n                        }\n                    }\n                }\n            }\n            else\n                assert node == null : node;\n\n            AffinityTopologyVersion resTopVer = initialVersion();\n\n            if (exchCtx.mergeExchanges()) {\n                if (msg.resultTopologyVersion() != null && !initialVersion().equals(msg.resultTopologyVersion())) {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Received full message, need merge [curFut=\" + initialVersion() +\n                            \", resVer=\" + msg.resultTopologyVersion() + ']');\n                    }\n\n                    resTopVer = msg.resultTopologyVersion();\n\n                    if (cctx.exchange().mergeExchanges(this, msg)) {\n                        assert cctx.kernalContext().isStopping();\n\n                        return; // Node is stopping, no need to further process exchange.\n                    }\n\n                    assert resTopVer.equals(exchCtx.events().topologyVersion()) :  \"Unexpected result version [\" +\n                        \"msgVer=\" + resTopVer +\n                        \", locVer=\" + exchCtx.events().topologyVersion() + ']';\n                }\n\n                exchCtx.events().processEvents(this);\n\n                if (localJoinExchange())\n                    cctx.affinity().onLocalJoin(this, msg, resTopVer);\n                else {\n                    if (exchCtx.events().hasServerLeft())\n                        cctx.affinity().mergeExchangesOnServerLeft(this, msg);\n                    else\n                        cctx.affinity().onServerJoinWithExchangeMergeProtocol(this, false);\n\n                    for (CacheGroupContext grp : cctx.cache().cacheGroups()) {\n                        if (grp.isLocal() || cacheGroupStopping(grp.groupId()))\n                            continue;\n\n                        grp.topology().beforeExchange(this, true, false);\n                    }\n                }\n            }\n            else if (localJoinExchange() && !exchCtx.fetchAffinityOnJoin())\n                cctx.affinity().onLocalJoin(this, msg, resTopVer);\n\n            updatePartitionFullMap(resTopVer, msg);\n\n            IgniteCheckedException err = null;\n\n            if (stateChangeExchange() && !F.isEmpty(msg.getErrorsMap())) {\n                err = new IgniteCheckedException(\"Cluster state change failed\");\n\n                cctx.kernalContext().state().onStateChangeError(msg.getErrorsMap(), exchActions.stateChangeRequest());\n            }\n\n            onDone(resTopVer, err);\n        }\n        catch (IgniteCheckedException e) {\n            onDone(e);\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::processSinglePartitionRequest(ClusterNode,GridDhtPartitionsSingleRequest)",
            "2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522 -\n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532 -\n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544 -\n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552 -\n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560 -\n2561 -\n2562  \n2563  \n2564  \n2565  \n2566 -\n2567 -\n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598 -\n2599 -\n2600 -\n2601 -\n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processSinglePartitionRequest(ClusterNode node, GridDhtPartitionsSingleRequest msg) {\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            if (crd == null) {\n                log.info(\"Ignore partitions request, no coordinator [node=\" + node.id() + ']');\n\n                return;\n            }\n\n            switch (state) {\n                case DONE: {\n                    assert finishState != null;\n\n                    if (node.id().equals(finishState.crdId)) {\n                        log.info(\"Ignore partitions request, finished exchange with this coordinator: \" + msg);\n\n                        return;\n                    }\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD:\n                case BECOME_CRD: {\n                    log.info(\"Ignore partitions request, node is coordinator: \" + msg);\n\n                    return;\n                }\n\n                case CLIENT:\n                case SRV: {\n                    if (!cctx.discovery().alive(node)) {\n                        log.info(\"Ignore partitions request, node is not alive [node=\" + node.id() + ']');\n\n                        return;\n                    }\n\n                    if (msg.restoreState()) {\n                        if (!node.equals(crd)) {\n                            if (node.order() > crd.order()) {\n                                log.info(\"Received partitions request, change coordinator [oldCrd=\" + crd.id() +\n                                    \", newCrd=\" + node.id() + ']');\n\n                                crd = node; // Do not allow to process FullMessage from old coordinator.\n                            }\n                            else {\n                                log.info(\"Ignore restore state request, coordinator changed [oldCrd=\" + crd.id() +\n                                    \", newCrd=\" + node.id() + ']');\n\n                                return;\n                            }\n                        }\n                    }\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (msg.restoreState()) {\n            try {\n                assert msg.restoreExchangeId() != null : msg;\n\n                GridDhtPartitionsSingleMessage res = cctx.exchange().createPartitionsSingleMessage(\n                    msg.restoreExchangeId(),\n                    cctx.kernalContext().clientNode(),\n                    true,\n                    node.version().compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0,\n                    exchActions);\n\n                if (localJoinExchange() && finishState0 == null)\n                    res.cacheGroupsAffinityRequest(exchCtx.groupsAffinityRequestOnJoin());\n\n                res.restoreState(true);\n\n                log.info(\"Send restore state response [node=\" + node.id() +\n                    \", exchVer=\" + msg.restoreExchangeId().topologyVersion() +\n                    \", hasState=\" + (finishState0 != null) +\n                    \", affReq=\" + !F.isEmpty(res.cacheGroupsAffinityRequest()) + ']');\n\n                res.finishMessage(finishState0 != null ? finishState0.msg : null);\n\n                cctx.io().send(node, res, SYSTEM_POOL);\n            }\n            catch (ClusterTopologyCheckedException ignored) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Node left during partition exchange [nodeId=\" + node.id() + \", exchId=\" + exchId + ']');\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to send partitions message [node=\" + node + \", msg=\" + msg + ']', e);\n            }\n\n            return;\n        }\n\n        try {\n            sendLocalPartitions(node);\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to send message to coordinator: \" + e);\n        }\n    }",
            "2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544 +\n2545 +\n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555 +\n2556 +\n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568 +\n2569 +\n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577 +\n2578 +\n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586 +\n2587 +\n2588 +\n2589 +\n2590  \n2591  \n2592  \n2593  \n2594 +\n2595 +\n2596 +\n2597 +\n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  \n2618  \n2619  \n2620  \n2621  \n2622  \n2623  \n2624  \n2625  \n2626  \n2627  \n2628 +\n2629 +\n2630 +\n2631 +\n2632 +\n2633 +\n2634  \n2635  \n2636  \n2637  \n2638  \n2639  \n2640  \n2641  \n2642  \n2643  \n2644  \n2645  \n2646  \n2647  \n2648  \n2649  \n2650  \n2651  \n2652  \n2653  \n2654  \n2655  \n2656  ",
            "    /**\n     * @param node Sender node.\n     * @param msg Message.\n     */\n    private void processSinglePartitionRequest(ClusterNode node, GridDhtPartitionsSingleRequest msg) {\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            if (crd == null) {\n                if (log.isInfoEnabled())\n                    log.info(\"Ignore partitions request, no coordinator [node=\" + node.id() + ']');\n\n                return;\n            }\n\n            switch (state) {\n                case DONE: {\n                    assert finishState != null;\n\n                    if (node.id().equals(finishState.crdId)) {\n                        if (log.isInfoEnabled())\n                            log.info(\"Ignore partitions request, finished exchange with this coordinator: \" + msg);\n\n                        return;\n                    }\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD:\n                case BECOME_CRD: {\n                    if (log.isInfoEnabled())\n                        log.info(\"Ignore partitions request, node is coordinator: \" + msg);\n\n                    return;\n                }\n\n                case CLIENT:\n                case SRV: {\n                    if (!cctx.discovery().alive(node)) {\n                        if (log.isInfoEnabled())\n                            log.info(\"Ignore partitions request, node is not alive [node=\" + node.id() + ']');\n\n                        return;\n                    }\n\n                    if (msg.restoreState()) {\n                        if (!node.equals(crd)) {\n                            if (node.order() > crd.order()) {\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Received partitions request, change coordinator [oldCrd=\" + crd.id() +\n                                        \", newCrd=\" + node.id() + ']');\n                                }\n\n                                crd = node; // Do not allow to process FullMessage from old coordinator.\n                            }\n                            else {\n                                if (log.isInfoEnabled()) {\n                                    log.info(\"Ignore restore state request, coordinator changed [oldCrd=\" + crd.id() +\n                                        \", newCrd=\" + node.id() + ']');\n                                }\n\n                                return;\n                            }\n                        }\n                    }\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (msg.restoreState()) {\n            try {\n                assert msg.restoreExchangeId() != null : msg;\n\n                GridDhtPartitionsSingleMessage res = cctx.exchange().createPartitionsSingleMessage(\n                    msg.restoreExchangeId(),\n                    cctx.kernalContext().clientNode(),\n                    true,\n                    node.version().compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0,\n                    exchActions);\n\n                if (localJoinExchange() && finishState0 == null)\n                    res.cacheGroupsAffinityRequest(exchCtx.groupsAffinityRequestOnJoin());\n\n                res.restoreState(true);\n\n                if (log.isInfoEnabled()) {\n                    log.info(\"Send restore state response [node=\" + node.id() +\n                        \", exchVer=\" + msg.restoreExchangeId().topologyVersion() +\n                        \", hasState=\" + (finishState0 != null) +\n                        \", affReq=\" + !F.isEmpty(res.cacheGroupsAffinityRequest()) + ']');\n                }\n\n                res.finishMessage(finishState0 != null ? finishState0.msg : null);\n\n                cctx.io().send(node, res, SYSTEM_POOL);\n            }\n            catch (ClusterTopologyCheckedException ignored) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Node left during partition exchange [nodeId=\" + node.id() + \", exchId=\" + exchId + ']');\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to send partitions message [node=\" + node + \", msg=\" + msg + ']', e);\n            }\n\n            return;\n        }\n\n        try {\n            sendLocalPartitions(node);\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to send message to coordinator: \" + e);\n        }\n    }"
        ]
    ],
    "c7b4201e845a69a829f2e81ba2bcd4c33e9785eb": [
        [
            "GridDhtPartitionTopologyImpl::update(AffinityTopologyVersion,GridDhtPartitionFullMap,CachePartitionFullCountersMap,Set,AffinityTopologyVersion)",
            "1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205 -\n1206 -\n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242 -\n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252 -\n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269 -\n1270  \n1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  \n1288  \n1289  \n1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298 -\n1299 -\n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314 -\n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429 -\n1430 -\n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable CachePartitionFullCountersMap incomeCntrMap,\n        Set<Integer> partsToReload,\n        @Nullable AffinityTopologyVersion msgTopVer) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating full partition map [exchVer=\" + exchangeVer + \", parts=\" + fullMapString() + ']');\n\n        assert partMap != null;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (stopping || !lastTopChangeVer.initialized() ||\n                    // Ignore message not-related to exchange if exchange is in progress.\n                    (exchangeVer == null && !lastTopChangeVer.equals(readyTopVer)))\n                    return false;\n\n                if (incomeCntrMap != null) {\n                    // update local counters in partitions\n                    for (int i = 0; i < locParts.length(); i++) {\n                        GridDhtLocalPartition part = locParts.get(i);\n\n                        if (part == null)\n                            continue;\n\n                        if (part.state() == OWNING || part.state() == MOVING) {\n                            long updCntr = incomeCntrMap.updateCounter(part.id());\n\n                            if (updCntr != 0 && updCntr > part.updateCounter())\n                                part.updateCounter(updCntr);\n                        }\n                    }\n                }\n\n                if (exchangeVer != null) {\n                    // Ignore if exchange already finished or new exchange started.\n                    if (readyTopVer.compareTo(exchangeVer) > 0 || lastTopChangeVer.compareTo(exchangeVer) > 0) {\n                        U.warn(log, \"Stale exchange id for full partition map update (will ignore) [\" +\n                            \"lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exchVer=\" + exchangeVer + ']');\n\n                        return false;\n                    }\n                }\n\n                if (msgTopVer != null && lastTopChangeVer.compareTo(msgTopVer) > 0) {\n                    U.warn(log, \"Stale version for full partition map update message (will ignore) [\" +\n                        \"lastTopChange=\" + lastTopChangeVer +\n                        \", readTopVer=\" + readyTopVer +\n                        \", msgVer=\" + msgTopVer + ']');\n\n                    return false;\n                }\n\n                boolean fullMapUpdated = (node2part == null);\n\n                if (node2part != null) {\n                    for (GridDhtPartitionMap part : node2part.values()) {\n                        GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                        if (shouldOverridePartitionMap(part, newPart)) {\n                            fullMapUpdated = true;\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Overriding partition map in full update map [exchVer=\" + exchangeVer +\n                                    \", curPart=\" + mapString(part) +\n                                    \", newPart=\" + mapString(newPart) + ']');\n                            }\n\n                            if (newPart.nodeId().equals(ctx.localNodeId()))\n                                updateSeq.setIfGreater(newPart.updateSequence());\n                        }\n                        else {\n                            // If for some nodes current partition has a newer map,\n                            // then we keep the newer value.\n                            partMap.put(part.nodeId(), part);\n                        }\n                    }\n\n                    // Check that we have new nodes.\n                    for (GridDhtPartitionMap part : partMap.values()) {\n                        if (fullMapUpdated)\n                            break;\n\n                        fullMapUpdated = !node2part.containsKey(part.nodeId());\n                    }\n\n                    // Remove entry if node left.\n                    for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                        UUID nodeId = it.next();\n\n                        if (!ctx.discovery().alive(nodeId)) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Removing left node from full map update [nodeId=\" + nodeId + \", partMap=\" +\n                                    partMap + ']');\n\n                            it.remove();\n                        }\n                    }\n                }\n                else {\n                    GridDhtPartitionMap locNodeMap = partMap.get(ctx.localNodeId());\n\n                    if (locNodeMap != null)\n                        updateSeq.setIfGreater(locNodeMap.updateSequence());\n                }\n\n                if (!fullMapUpdated) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"No updates for full partition map (will ignore) [lastExch=\" + lastTopChangeVer +\n                            \", exchVer=\" + exchangeVer +\n                            \", curMap=\" + node2part +\n                            \", newMap=\" + partMap + ']');\n                    }\n\n                    return false;\n                }\n\n                if (exchangeVer != null) {\n                    assert exchangeVer.compareTo(readyTopVer) >= 0 && exchangeVer.compareTo(lastTopChangeVer) >= 0;\n\n                    lastTopChangeVer = readyTopVer = exchangeVer;\n                }\n\n                node2part = partMap;\n\n                if (exchangeVer == null && !grp.isReplicated() &&\n                        (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0)) {\n                    AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) {\n                            int p = e0.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e0.getValue() == MOVING || e0.getValue() == OWNING || e0.getValue() == RENTING) &&\n                                !affAssignment.getIds(p).contains(e.getKey())) {\n\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                diffIds.add(e.getKey());\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(e.getKey())) {\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n                    }\n\n                    diffFromAffinityVer = readyTopVer;\n                }\n\n                boolean changed = false;\n\n                GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n                if (nodeMap != null && grp.persistenceEnabled() && readyTopVer.initialized()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                        int p = e.getKey();\n                        GridDhtPartitionState state = e.getValue();\n\n                        if (state == OWNING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            assert locPart != null : grp.cacheOrGroupName();\n\n                            if (locPart.state() == MOVING) {\n                                boolean success = locPart.own();\n\n                                assert success : locPart;\n\n                                changed |= success;\n                            }\n                        }\n                        else if (state == MOVING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            if (locPart == null || locPart.state() == EVICTED)\n                                locPart = createPartition(p);\n\n                            if (locPart.state() == OWNING) {\n                                locPart.moving();\n\n                                changed = true;\n                            }\n                        }\n                        else if (state == RENTING && partsToReload.contains(p)) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            if (locPart == null || locPart.state() == EVICTED) {\n                                createPartition(p);\n\n                                changed = true;\n                            }\n                            else if (locPart.state() == OWNING || locPart.state() == MOVING) {\n                                locPart.reload(true);\n\n                                locPart.rent(false);\n\n                                changed = true;\n                            }\n                            else\n                                locPart.reload(true);\n                        }\n                    }\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchangeVer == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.assignment());\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after full update: \" + fullMapString());\n\n                if (changed)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            "1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225 +\n1226 +\n1227 +\n1228 +\n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264 +\n1265 +\n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275 +\n1276 +\n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  \n1288  \n1289  \n1290  \n1291  \n1292  \n1293 +\n1294 +\n1295 +\n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324 +\n1325 +\n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340 +\n1341 +\n1342 +\n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457 +\n1458 +\n1459 +\n1460 +\n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable CachePartitionFullCountersMap incomeCntrMap,\n        Set<Integer> partsToReload,\n        @Nullable AffinityTopologyVersion msgTopVer) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Updating full partition map [grp=\" + grp.cacheOrGroupName() + \", exchVer=\" + exchangeVer +\n                \", fullMap=\" + fullMapString() + ']');\n        }\n\n        assert partMap != null;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (stopping || !lastTopChangeVer.initialized() ||\n                    // Ignore message not-related to exchange if exchange is in progress.\n                    (exchangeVer == null && !lastTopChangeVer.equals(readyTopVer)))\n                    return false;\n\n                if (incomeCntrMap != null) {\n                    // update local counters in partitions\n                    for (int i = 0; i < locParts.length(); i++) {\n                        GridDhtLocalPartition part = locParts.get(i);\n\n                        if (part == null)\n                            continue;\n\n                        if (part.state() == OWNING || part.state() == MOVING) {\n                            long updCntr = incomeCntrMap.updateCounter(part.id());\n\n                            if (updCntr != 0 && updCntr > part.updateCounter())\n                                part.updateCounter(updCntr);\n                        }\n                    }\n                }\n\n                if (exchangeVer != null) {\n                    // Ignore if exchange already finished or new exchange started.\n                    if (readyTopVer.compareTo(exchangeVer) > 0 || lastTopChangeVer.compareTo(exchangeVer) > 0) {\n                        U.warn(log, \"Stale exchange id for full partition map update (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exchVer=\" + exchangeVer + ']');\n\n                        return false;\n                    }\n                }\n\n                if (msgTopVer != null && lastTopChangeVer.compareTo(msgTopVer) > 0) {\n                    U.warn(log, \"Stale version for full partition map update message (will ignore) [\" +\n                        \"grp=\" + grp.cacheOrGroupName() +\n                        \", lastTopChange=\" + lastTopChangeVer +\n                        \", readTopVer=\" + readyTopVer +\n                        \", msgVer=\" + msgTopVer + ']');\n\n                    return false;\n                }\n\n                boolean fullMapUpdated = (node2part == null);\n\n                if (node2part != null) {\n                    for (GridDhtPartitionMap part : node2part.values()) {\n                        GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                        if (shouldOverridePartitionMap(part, newPart)) {\n                            fullMapUpdated = true;\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Overriding partition map in full update map [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() +\n                                    \", exchVer=\" + exchangeVer +\n                                    \", curPart=\" + mapString(part) +\n                                    \", newPart=\" + mapString(newPart) + ']');\n                            }\n\n                            if (newPart.nodeId().equals(ctx.localNodeId()))\n                                updateSeq.setIfGreater(newPart.updateSequence());\n                        }\n                        else {\n                            // If for some nodes current partition has a newer map,\n                            // then we keep the newer value.\n                            partMap.put(part.nodeId(), part);\n                        }\n                    }\n\n                    // Check that we have new nodes.\n                    for (GridDhtPartitionMap part : partMap.values()) {\n                        if (fullMapUpdated)\n                            break;\n\n                        fullMapUpdated = !node2part.containsKey(part.nodeId());\n                    }\n\n                    // Remove entry if node left.\n                    for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                        UUID nodeId = it.next();\n\n                        if (!ctx.discovery().alive(nodeId)) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Removing left node from full map update [grp=\" + grp.cacheOrGroupName() +\n                                    \", nodeId=\" + nodeId + \", partMap=\" + partMap + ']');\n\n                            it.remove();\n                        }\n                    }\n                }\n                else {\n                    GridDhtPartitionMap locNodeMap = partMap.get(ctx.localNodeId());\n\n                    if (locNodeMap != null)\n                        updateSeq.setIfGreater(locNodeMap.updateSequence());\n                }\n\n                if (!fullMapUpdated) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"No updates for full partition map (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastExch=\" + lastTopChangeVer +\n                            \", exchVer=\" + exchangeVer +\n                            \", curMap=\" + node2part +\n                            \", newMap=\" + partMap + ']');\n                    }\n\n                    return false;\n                }\n\n                if (exchangeVer != null) {\n                    assert exchangeVer.compareTo(readyTopVer) >= 0 && exchangeVer.compareTo(lastTopChangeVer) >= 0;\n\n                    lastTopChangeVer = readyTopVer = exchangeVer;\n                }\n\n                node2part = partMap;\n\n                if (exchangeVer == null && !grp.isReplicated() &&\n                        (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0)) {\n                    AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) {\n                            int p = e0.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e0.getValue() == MOVING || e0.getValue() == OWNING || e0.getValue() == RENTING) &&\n                                !affAssignment.getIds(p).contains(e.getKey())) {\n\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                diffIds.add(e.getKey());\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(e.getKey())) {\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n                    }\n\n                    diffFromAffinityVer = readyTopVer;\n                }\n\n                boolean changed = false;\n\n                GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n                if (nodeMap != null && grp.persistenceEnabled() && readyTopVer.initialized()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                        int p = e.getKey();\n                        GridDhtPartitionState state = e.getValue();\n\n                        if (state == OWNING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            assert locPart != null : grp.cacheOrGroupName();\n\n                            if (locPart.state() == MOVING) {\n                                boolean success = locPart.own();\n\n                                assert success : locPart;\n\n                                changed |= success;\n                            }\n                        }\n                        else if (state == MOVING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            if (locPart == null || locPart.state() == EVICTED)\n                                locPart = createPartition(p);\n\n                            if (locPart.state() == OWNING) {\n                                locPart.moving();\n\n                                changed = true;\n                            }\n                        }\n                        else if (state == RENTING && partsToReload.contains(p)) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            if (locPart == null || locPart.state() == EVICTED) {\n                                createPartition(p);\n\n                                changed = true;\n                            }\n                            else if (locPart.state() == OWNING || locPart.state() == MOVING) {\n                                locPart.reload(true);\n\n                                locPart.rent(false);\n\n                                changed = true;\n                            }\n                            else\n                                locPart.reload(true);\n                        }\n                    }\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchangeVer == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.assignment());\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Partition map after full update [grp=\" + grp.cacheOrGroupName() +\n                        \", map=\" + fullMapString() + ']');\n                }\n\n                if (changed)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::setOwners(int,Set,boolean,boolean)",
            "1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982 -\n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009 -\n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  ",
            "    /** {@inheritDoc} */\n    @Override public Set<UUID> setOwners(int p, Set<UUID> owners, boolean haveHistory, boolean updateSeq) {\n        Set<UUID> result = haveHistory ? Collections.<UUID>emptySet() : new HashSet<UUID>();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                GridDhtLocalPartition locPart = locParts.get(p);\n\n                if (locPart != null) {\n                    if (locPart.state() == OWNING && !owners.contains(ctx.localNodeId())) {\n                        if (haveHistory)\n                            locPart.moving();\n                        else {\n                            locPart.rent(false);\n\n                            locPart.reload(true);\n\n                            result.add(ctx.localNodeId());\n                        }\n\n                        U.warn(log, \"Partition has been scheduled for rebalancing due to outdated update counter \" +\n                            \"[nodeId=\" + ctx.localNodeId() + \", cacheOrGroupName=\" + grp.cacheOrGroupName() +\n                            \", partId=\" + locPart.id() + \", haveHistory=\" + haveHistory + \"]\");\n\n                    }\n                }\n\n                for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) {\n                    GridDhtPartitionMap partMap = e.getValue();\n\n                    if (!partMap.containsKey(p))\n                        continue;\n\n                    if (partMap.get(p) == OWNING && !owners.contains(e.getKey())) {\n                        if (haveHistory)\n                            partMap.put(p, MOVING);\n                        else {\n                            partMap.put(p, RENTING);\n\n                            result.add(e.getKey());\n                        }\n\n                        partMap.updateSequence(partMap.updateSequence() + 1, partMap.topologyVersion());\n\n                        if (partMap.nodeId().equals(ctx.localNodeId()))\n                            this.updateSeq.setIfGreater(partMap.updateSequence());\n\n                        U.warn(log, \"Partition has been scheduled for rebalancing due to outdated update counter \" +\n                            \"[nodeId=\" + e.getKey() + \", cacheOrGroupName=\" + grp.cacheOrGroupName() +\n                            \", partId=\" + p + \", haveHistory=\" + haveHistory + \"]\");\n                    }\n                }\n\n                if (updateSeq)\n                    node2part = new GridDhtPartitionFullMap(node2part, this.updateSeq.incrementAndGet());\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        return result;\n    }",
            "1993  \n1994  \n1995  \n1996  \n1997  \n1998  \n1999  \n2000  \n2001  \n2002  \n2003  \n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018 +\n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045 +\n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  ",
            "    /** {@inheritDoc} */\n    @Override public Set<UUID> setOwners(int p, Set<UUID> owners, boolean haveHistory, boolean updateSeq) {\n        Set<UUID> result = haveHistory ? Collections.<UUID>emptySet() : new HashSet<UUID>();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                GridDhtLocalPartition locPart = locParts.get(p);\n\n                if (locPart != null) {\n                    if (locPart.state() == OWNING && !owners.contains(ctx.localNodeId())) {\n                        if (haveHistory)\n                            locPart.moving();\n                        else {\n                            locPart.rent(false);\n\n                            locPart.reload(true);\n\n                            result.add(ctx.localNodeId());\n                        }\n\n                        U.warn(log, \"Partition has been scheduled for rebalancing due to outdated update counter \" +\n                            \"[nodeId=\" + ctx.localNodeId() + \", grp=\" + grp.cacheOrGroupName() +\n                            \", partId=\" + locPart.id() + \", haveHistory=\" + haveHistory + \"]\");\n\n                    }\n                }\n\n                for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) {\n                    GridDhtPartitionMap partMap = e.getValue();\n\n                    if (!partMap.containsKey(p))\n                        continue;\n\n                    if (partMap.get(p) == OWNING && !owners.contains(e.getKey())) {\n                        if (haveHistory)\n                            partMap.put(p, MOVING);\n                        else {\n                            partMap.put(p, RENTING);\n\n                            result.add(e.getKey());\n                        }\n\n                        partMap.updateSequence(partMap.updateSequence() + 1, partMap.topologyVersion());\n\n                        if (partMap.nodeId().equals(ctx.localNodeId()))\n                            this.updateSeq.setIfGreater(partMap.updateSequence());\n\n                        U.warn(log, \"Partition has been scheduled for rebalancing due to outdated update counter \" +\n                            \"[nodeId=\" + e.getKey() + \", grp=\" + grp.cacheOrGroupName() +\n                            \", partId=\" + p + \", haveHistory=\" + haveHistory + \"]\");\n                    }\n                }\n\n                if (updateSeq)\n                    node2part = new GridDhtPartitionFullMap(node2part, this.updateSeq.incrementAndGet());\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        return result;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::beforeExchange(GridDhtPartitionsExchangeFuture,boolean,boolean)",
            " 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479 -\n 480 -\n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494 -\n 495 -\n 496 -\n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506 -\n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519 -\n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563 -\n 564 -\n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut,\n        boolean affReady,\n        boolean updateMoving)\n        throws IgniteCheckedException {\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            synchronized (ctx.exchange().interruptLock()) {\n                if (Thread.currentThread().isInterrupted())\n                    throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n                U.writeLock(lock);\n\n                try {\n                    if (stopping)\n                        return;\n\n                    assert lastTopChangeVer.equals(exchFut.initialVersion()) : \"Invalid topology version [topVer=\" + lastTopChangeVer +\n                        \", exchId=\" + exchFut.exchangeId() + ']';\n\n                    ExchangeDiscoveryEvents evts = exchFut.context().events();\n\n                    if (affReady) {\n                        assert grp.affinity().lastVersion().equals(evts.topologyVersion()) : \"Invalid affinity version [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", affVer=\" + grp.affinity().lastVersion() +\n                            \", evtsVer=\" + evts.topologyVersion() + ']';\n\n                        lastTopChangeVer = readyTopVer = evts.topologyVersion();\n                    }\n\n                    ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map beforeExchange [exchId=\" + exchFut.exchangeId() +\n                            \", fullMap=\" + fullMapString() + ']');\n                    }\n\n                    long updateSeq = this.updateSeq.incrementAndGet();\n\n                    cntrMap.clear();\n\n                    boolean grpStarted = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                    // If this is the oldest node.\n                    if (oldest != null && (loc.equals(oldest) || grpStarted)) {\n                        if (node2part == null) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                                    exchFut.exchangeId() + \", fullMap=\" + fullMapString() + ']');\n                        }\n                        else if (!node2part.valid()) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Created new full topology map on oldest node [exchId=\" + exchFut.exchangeId() +\n                                    \", fullMap=\" + node2part + ']');\n                            }\n                        }\n                        else if (!node2part.nodeId().equals(loc.id())) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [\" +\n                                    \"exchId=\" + exchFut.exchangeId() + \", fullMap=\" + fullMapString() + ']');\n                            }\n                        }\n                    }\n\n                    if (evts.hasServerLeft()) {\n                        List<DiscoveryEvent> evts0 = evts.events();\n\n                        for (int i = 0; i < evts0.size(); i++) {\n                            DiscoveryEvent evt = evts0.get(i);\n\n                            if (ExchangeDiscoveryEvents.serverLeftEvent(evt))\n                                removeNode(evt.eventNode().id());\n                        }\n                    }\n\n                    if (grp.affinityNode()) {\n                        if (grpStarted ||\n                            exchFut.firstEvent().type() == EVT_DISCOVERY_CUSTOM_EVT ||\n                            exchFut.serverNodeDiscoveryEvent()) {\n                            if (affReady) {\n                                assert grp.affinity().lastVersion().equals(evts.topologyVersion());\n\n                                initPartitions0(evts.topologyVersion(), exchFut, updateSeq);\n                            }\n                            else {\n                                assert !exchFut.context().mergeExchanges();\n\n                                List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                                createPartitions(exchFut.initialVersion(), aff, updateSeq);\n                            }\n                        }\n                    }\n\n                    consistencyCheck();\n\n                    if (updateMoving) {\n                        assert grp.affinity().lastVersion().equals(evts.topologyVersion());\n\n                        createMovingPartitions(grp.affinity().readyAffinity(evts.topologyVersion()));\n                    }\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map after beforeExchange [exchId=\" + exchFut.exchangeId() +\n                            \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n                finally {\n                    lock.writeLock().unlock();\n                }\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            " 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482 +\n 483 +\n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497 +\n 498 +\n 499 +\n 500 +\n 501 +\n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511 +\n 512 +\n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525 +\n 526 +\n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570 +\n 571 +\n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut,\n        boolean affReady,\n        boolean updateMoving)\n        throws IgniteCheckedException {\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            synchronized (ctx.exchange().interruptLock()) {\n                if (Thread.currentThread().isInterrupted())\n                    throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n                U.writeLock(lock);\n\n                try {\n                    if (stopping)\n                        return;\n\n                    assert lastTopChangeVer.equals(exchFut.initialVersion()) : \"Invalid topology version [topVer=\" + lastTopChangeVer +\n                        \", exchId=\" + exchFut.exchangeId() + ']';\n\n                    ExchangeDiscoveryEvents evts = exchFut.context().events();\n\n                    if (affReady) {\n                        assert grp.affinity().lastVersion().equals(evts.topologyVersion()) : \"Invalid affinity version [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", affVer=\" + grp.affinity().lastVersion() +\n                            \", evtsVer=\" + evts.topologyVersion() + ']';\n\n                        lastTopChangeVer = readyTopVer = evts.topologyVersion();\n                    }\n\n                    ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map beforeExchange [grp=\" + grp.cacheOrGroupName() +\n                            \", exchId=\" + exchFut.exchangeId() + \", fullMap=\" + fullMapString() + ']');\n                    }\n\n                    long updateSeq = this.updateSeq.incrementAndGet();\n\n                    cntrMap.clear();\n\n                    boolean grpStarted = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                    // If this is the oldest node.\n                    if (oldest != null && (loc.equals(oldest) || grpStarted)) {\n                        if (node2part == null) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Created brand new full topology map on oldest node [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() + \", exchId=\" + exchFut.exchangeId() +\n                                    \", fullMap=\" + fullMapString() + ']');\n                            }\n                        }\n                        else if (!node2part.valid()) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Created new full topology map on oldest node [\" +\n                                    \"grp=\" +  grp.cacheOrGroupName() + \", exchId=\" + exchFut.exchangeId() +\n                                    \", fullMap=\" + node2part + ']');\n                            }\n                        }\n                        else if (!node2part.nodeId().equals(loc.id())) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() + \", exchId=\" + exchFut.exchangeId() +\n                                    \", fullMap=\" + fullMapString() + ']');\n                            }\n                        }\n                    }\n\n                    if (evts.hasServerLeft()) {\n                        List<DiscoveryEvent> evts0 = evts.events();\n\n                        for (int i = 0; i < evts0.size(); i++) {\n                            DiscoveryEvent evt = evts0.get(i);\n\n                            if (ExchangeDiscoveryEvents.serverLeftEvent(evt))\n                                removeNode(evt.eventNode().id());\n                        }\n                    }\n\n                    if (grp.affinityNode()) {\n                        if (grpStarted ||\n                            exchFut.firstEvent().type() == EVT_DISCOVERY_CUSTOM_EVT ||\n                            exchFut.serverNodeDiscoveryEvent()) {\n                            if (affReady) {\n                                assert grp.affinity().lastVersion().equals(evts.topologyVersion());\n\n                                initPartitions0(evts.topologyVersion(), exchFut, updateSeq);\n                            }\n                            else {\n                                assert !exchFut.context().mergeExchanges();\n\n                                List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                                createPartitions(exchFut.initialVersion(), aff, updateSeq);\n                            }\n                        }\n                    }\n\n                    consistencyCheck();\n\n                    if (updateMoving) {\n                        assert grp.affinity().lastVersion().equals(evts.topologyVersion());\n\n                        createMovingPartitions(grp.affinity().readyAffinity(evts.topologyVersion()));\n                    }\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map after beforeExchange [grp=\" + grp.cacheOrGroupName() + \", \" +\n                            \"exchId=\" + exchFut.exchangeId() + \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n                finally {\n                    lock.writeLock().unlock();\n                }\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::nodes0(int,AffinityAssignment,List)",
            "1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029 -\n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057 -\n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  ",
            "    /**\n     * @param p Partition.\n     * @param affAssignment Assignments.\n     * @param affNodes Node assigned for given partition by affinity.\n     * @return Nodes responsible for given partition (primary is first).\n     */\n    @Nullable private List<ClusterNode> nodes0(int p, AffinityAssignment affAssignment, List<ClusterNode> affNodes) {\n        if (grp.isReplicated())\n            return affNodes;\n\n        AffinityTopologyVersion topVer = affAssignment.topologyVersion();\n\n        lock.readLock().lock();\n\n        try {\n            assert node2part != null && node2part.valid() : \"Invalid node-to-partitions map [topVer1=\" + topVer +\n                \", topVer2=\" + this.readyTopVer +\n                \", node=\" + ctx.igniteInstanceName() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", node2part=\" + node2part + ']';\n\n            List<ClusterNode> nodes = null;\n\n            if (!topVer.equals(diffFromAffinityVer)) {\n                LT.warn(log, \"Requested topology version does not match calculated diff, will require full iteration to\" +\n                    \"calculate mapping [topVer=\" + topVer + \", diffVer=\" + diffFromAffinityVer + \"]\");\n\n                nodes = new ArrayList<>();\n\n                nodes.addAll(affNodes);\n\n                for (Map.Entry<UUID, GridDhtPartitionMap> entry : node2part.entrySet()) {\n                    GridDhtPartitionState state = entry.getValue().get(p);\n\n                    ClusterNode n = ctx.discovery().node(entry.getKey());\n\n                    if (n != null && state != null && (state == MOVING || state == OWNING || state == RENTING)\n                        && !nodes.contains(n) && (topVer.topologyVersion() < 0 || n.order() <= topVer.topologyVersion())) {\n                        nodes.add(n);\n                    }\n\n                }\n\n                return nodes;\n            }\n\n            Collection<UUID> diffIds = diffFromAffinity.get(p);\n\n            if (!F.isEmpty(diffIds)) {\n                HashSet<UUID> affIds = affAssignment.getIds(p);\n\n                for (UUID nodeId : diffIds) {\n                    if (affIds.contains(nodeId)) {\n                        U.warn(log, \"Node from diff \" + nodeId + \" is affinity node. Skipping it.\");\n\n                        continue;\n                    }\n\n                    if (hasState(p, nodeId, OWNING, MOVING, RENTING)) {\n                        ClusterNode n = ctx.discovery().node(nodeId);\n\n                        if (n != null && (topVer.topologyVersion() < 0 || n.order() <= topVer.topologyVersion())) {\n                            if (nodes == null) {\n                                nodes = new ArrayList<>(affNodes.size() + diffIds.size());\n\n                                nodes.addAll(affNodes);\n                            }\n\n                            nodes.add(n);\n                        }\n                    }\n                }\n            }\n\n            return nodes;\n        }\n        finally {\n            lock.readLock().unlock();\n        }\n    }",
            "1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047 +\n1048 +\n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076 +\n1077 +\n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  ",
            "    /**\n     * @param p Partition.\n     * @param affAssignment Assignments.\n     * @param affNodes Node assigned for given partition by affinity.\n     * @return Nodes responsible for given partition (primary is first).\n     */\n    @Nullable private List<ClusterNode> nodes0(int p, AffinityAssignment affAssignment, List<ClusterNode> affNodes) {\n        if (grp.isReplicated())\n            return affNodes;\n\n        AffinityTopologyVersion topVer = affAssignment.topologyVersion();\n\n        lock.readLock().lock();\n\n        try {\n            assert node2part != null && node2part.valid() : \"Invalid node-to-partitions map [topVer1=\" + topVer +\n                \", topVer2=\" + this.readyTopVer +\n                \", node=\" + ctx.igniteInstanceName() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", node2part=\" + node2part + ']';\n\n            List<ClusterNode> nodes = null;\n\n            if (!topVer.equals(diffFromAffinityVer)) {\n                LT.warn(log, \"Requested topology version does not match calculated diff, will require full iteration to\" +\n                    \"calculate mapping [grp=\" + grp.cacheOrGroupName() + \", topVer=\" + topVer +\n                    \", diffVer=\" + diffFromAffinityVer + \"]\");\n\n                nodes = new ArrayList<>();\n\n                nodes.addAll(affNodes);\n\n                for (Map.Entry<UUID, GridDhtPartitionMap> entry : node2part.entrySet()) {\n                    GridDhtPartitionState state = entry.getValue().get(p);\n\n                    ClusterNode n = ctx.discovery().node(entry.getKey());\n\n                    if (n != null && state != null && (state == MOVING || state == OWNING || state == RENTING)\n                        && !nodes.contains(n) && (topVer.topologyVersion() < 0 || n.order() <= topVer.topologyVersion())) {\n                        nodes.add(n);\n                    }\n\n                }\n\n                return nodes;\n            }\n\n            Collection<UUID> diffIds = diffFromAffinity.get(p);\n\n            if (!F.isEmpty(diffIds)) {\n                HashSet<UUID> affIds = affAssignment.getIds(p);\n\n                for (UUID nodeId : diffIds) {\n                    if (affIds.contains(nodeId)) {\n                        U.warn(log, \"Node from diff is affinity node, skipping it [grp=\" + grp.cacheOrGroupName() +\n                            \", node=\" + nodeId + ']');\n\n                        continue;\n                    }\n\n                    if (hasState(p, nodeId, OWNING, MOVING, RENTING)) {\n                        ClusterNode n = ctx.discovery().node(nodeId);\n\n                        if (n != null && (topVer.topologyVersion() < 0 || n.order() <= topVer.topologyVersion())) {\n                            if (nodes == null) {\n                                nodes = new ArrayList<>(affNodes.size() + diffIds.size());\n\n                                nodes.addAll(affNodes);\n                            }\n\n                            nodes.add(n);\n                        }\n                    }\n                }\n            }\n\n            return nodes;\n        }\n        finally {\n            lock.readLock().unlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::updateLocal(int,GridDhtPartitionState,long,AffinityTopologyVersion)",
            "2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  ",
            "    /**\n     * Updates value for single partition.\n     *\n     * @param p Partition.\n     * @param state State.\n     * @param updateSeq Update sequence.\n     * @param affVer Affinity version.\n     * @return Update sequence.\n     */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    private long updateLocal(int p, GridDhtPartitionState state, long updateSeq, AffinityTopologyVersion affVer) {\n        assert lock.isWriteLockedByCurrentThread();\n\n        ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n        assert oldest != null || ctx.kernalContext().clientNode();\n\n        // If this node became the oldest node.\n        if (ctx.localNode().equals(oldest) && node2part != null) {\n            long seq = node2part.updateSequence();\n\n            if (seq != updateSeq) {\n                if (seq > updateSeq) {\n                    long seq0 = this.updateSeq.get();\n\n                    if (seq0 < seq) {\n                        // Update global counter if necessary.\n                        boolean b = this.updateSeq.compareAndSet(seq0, seq + 1);\n\n                        assert b : \"Invalid update sequence [updateSeq=\" + updateSeq +\n                            \", seq=\" + seq +\n                            \", curUpdateSeq=\" + this.updateSeq.get() +\n                            \", node2part=\" + node2part.toFullString() + ']';\n\n                        updateSeq = seq + 1;\n                    }\n                    else\n                        updateSeq = seq;\n                }\n\n                node2part.updateSequence(updateSeq);\n            }\n        }\n\n        if (node2part != null) {\n            UUID locNodeId = ctx.localNodeId();\n\n            GridDhtPartitionMap map = node2part.get(locNodeId);\n\n            if (map == null) {\n                map = new GridDhtPartitionMap(locNodeId,\n                    updateSeq,\n                    affVer,\n                    GridPartitionStateMap.EMPTY,\n                    false);\n\n                node2part.put(locNodeId, map);\n            }\n\n            map.updateSequence(updateSeq, affVer);\n\n            map.put(p, state);\n\n            if (!grp.isReplicated() && (state == MOVING || state == OWNING || state == RENTING)) {\n                AffinityAssignment assignment = grp.affinity().cachedAffinity(diffFromAffinityVer);\n\n                if (!assignment.getIds(p).contains(ctx.localNodeId())) {\n                    Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                    if (diffIds == null)\n                        diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                    diffIds.add(ctx.localNodeId());\n                }\n            }\n        }\n\n        return updateSeq;\n    }",
            "2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176 +\n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  ",
            "    /**\n     * Updates value for single partition.\n     *\n     * @param p Partition.\n     * @param state State.\n     * @param updateSeq Update sequence.\n     * @param affVer Affinity version.\n     * @return Update sequence.\n     */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    private long updateLocal(int p, GridDhtPartitionState state, long updateSeq, AffinityTopologyVersion affVer) {\n        assert lock.isWriteLockedByCurrentThread();\n\n        ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n        assert oldest != null || ctx.kernalContext().clientNode();\n\n        // If this node became the oldest node.\n        if (ctx.localNode().equals(oldest) && node2part != null) {\n            long seq = node2part.updateSequence();\n\n            if (seq != updateSeq) {\n                if (seq > updateSeq) {\n                    long seq0 = this.updateSeq.get();\n\n                    if (seq0 < seq) {\n                        // Update global counter if necessary.\n                        boolean b = this.updateSeq.compareAndSet(seq0, seq + 1);\n\n                        assert b : \"Invalid update sequence [updateSeq=\" + updateSeq +\n                            \", grp=\" + grp.cacheOrGroupName() +\n                            \", seq=\" + seq +\n                            \", curUpdateSeq=\" + this.updateSeq.get() +\n                            \", node2part=\" + node2part.toFullString() + ']';\n\n                        updateSeq = seq + 1;\n                    }\n                    else\n                        updateSeq = seq;\n                }\n\n                node2part.updateSequence(updateSeq);\n            }\n        }\n\n        if (node2part != null) {\n            UUID locNodeId = ctx.localNodeId();\n\n            GridDhtPartitionMap map = node2part.get(locNodeId);\n\n            if (map == null) {\n                map = new GridDhtPartitionMap(locNodeId,\n                    updateSeq,\n                    affVer,\n                    GridPartitionStateMap.EMPTY,\n                    false);\n\n                node2part.put(locNodeId, map);\n            }\n\n            map.updateSequence(updateSeq, affVer);\n\n            map.put(p, state);\n\n            if (!grp.isReplicated() && (state == MOVING || state == OWNING || state == RENTING)) {\n                AffinityAssignment assignment = grp.affinity().cachedAffinity(diffFromAffinityVer);\n\n                if (!assignment.getIds(p).contains(ctx.localNodeId())) {\n                    Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                    if (diffIds == null)\n                        diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                    diffIds.add(ctx.localNodeId());\n                }\n            }\n        }\n\n        return updateSeq;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::checkEvictions(long,AffinityAssignment)",
            "2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063 -\n2064 -\n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091 -\n2092 -\n2093 -\n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  ",
            "    /**\n     * @param updateSeq Update sequence.\n     * @param aff Affinity assignments.\n     * @return Checks if any of the local partitions need to be evicted.\n     */\n    private boolean checkEvictions(long updateSeq, AffinityAssignment aff) {\n        boolean changed = false;\n\n        UUID locId = ctx.localNodeId();\n\n        for (int p = 0; p < locParts.length(); p++) {\n            GridDhtLocalPartition part = locParts.get(p);\n\n            if (part == null)\n                continue;\n\n            GridDhtPartitionState state = part.state();\n\n            if (state.active()) {\n                List<ClusterNode> affNodes = aff.get(p);\n\n                if (!affNodes.contains(ctx.localNode())) {\n                    List<ClusterNode> nodes = nodes(p, aff.topologyVersion(), OWNING, null);\n                    Collection<UUID> nodeIds = F.nodeIds(nodes);\n\n                    // If all affinity nodes are owners, then evict partition from local node.\n                    if (nodeIds.containsAll(F.nodeIds(affNodes))) {\n                        part.reload(false);\n\n                        part.rent(false);\n\n                        updateSeq = updateLocal(part.id(), part.state(), updateSeq, aff.topologyVersion());\n\n                        changed = true;\n\n                        if (log.isDebugEnabled())\n                            log.debug(\"Evicted local partition (all affinity nodes are owners): \" + part);\n                    }\n                    else {\n                        int ownerCnt = nodeIds.size();\n                        int affCnt = affNodes.size();\n\n                        if (ownerCnt > affCnt) {\n                            // Sort by node orders in ascending order.\n                            Collections.sort(nodes, CU.nodeComparator(true));\n\n                            int diff = nodes.size() - affCnt;\n\n                            for (int i = 0; i < diff; i++) {\n                                ClusterNode n = nodes.get(i);\n\n                                if (locId.equals(n.id())) {\n                                    part.reload(false);\n\n                                    part.rent(false);\n\n                                    updateSeq = updateLocal(part.id(),\n                                        part.state(),\n                                        updateSeq,\n                                        aff.topologyVersion());\n\n                                    changed = true;\n\n                                    if (log.isDebugEnabled())\n                                        log.debug(\"Evicted local partition (this node is oldest non-affinity node): \" +\n                                            part);\n\n                                    break;\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        return changed;\n    }",
            "2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099 +\n2100 +\n2101 +\n2102 +\n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129 +\n2130 +\n2131 +\n2132 +\n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  ",
            "    /**\n     * @param updateSeq Update sequence.\n     * @param aff Affinity assignments.\n     * @return Checks if any of the local partitions need to be evicted.\n     */\n    private boolean checkEvictions(long updateSeq, AffinityAssignment aff) {\n        boolean changed = false;\n\n        UUID locId = ctx.localNodeId();\n\n        for (int p = 0; p < locParts.length(); p++) {\n            GridDhtLocalPartition part = locParts.get(p);\n\n            if (part == null)\n                continue;\n\n            GridDhtPartitionState state = part.state();\n\n            if (state.active()) {\n                List<ClusterNode> affNodes = aff.get(p);\n\n                if (!affNodes.contains(ctx.localNode())) {\n                    List<ClusterNode> nodes = nodes(p, aff.topologyVersion(), OWNING, null);\n                    Collection<UUID> nodeIds = F.nodeIds(nodes);\n\n                    // If all affinity nodes are owners, then evict partition from local node.\n                    if (nodeIds.containsAll(F.nodeIds(affNodes))) {\n                        part.reload(false);\n\n                        part.rent(false);\n\n                        updateSeq = updateLocal(part.id(), part.state(), updateSeq, aff.topologyVersion());\n\n                        changed = true;\n\n                        if (log.isDebugEnabled()) {\n                            log.debug(\"Evicted local partition (all affinity nodes are owners) [grp=\" + grp.cacheOrGroupName() +\n                                \", part=\" + part + ']');\n                        }\n                    }\n                    else {\n                        int ownerCnt = nodeIds.size();\n                        int affCnt = affNodes.size();\n\n                        if (ownerCnt > affCnt) {\n                            // Sort by node orders in ascending order.\n                            Collections.sort(nodes, CU.nodeComparator(true));\n\n                            int diff = nodes.size() - affCnt;\n\n                            for (int i = 0; i < diff; i++) {\n                                ClusterNode n = nodes.get(i);\n\n                                if (locId.equals(n.id())) {\n                                    part.reload(false);\n\n                                    part.rent(false);\n\n                                    updateSeq = updateLocal(part.id(),\n                                        part.state(),\n                                        updateSeq,\n                                        aff.topologyVersion());\n\n                                    changed = true;\n\n                                    if (log.isDebugEnabled()) {\n                                        log.debug(\"Evicted local partition (this node is oldest non-affinity node) [\" +\n                                            \"grp=\" + grp.cacheOrGroupName() + \", part=\" + part + ']');\n                                    }\n\n                                    break;\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        return changed;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::updateRebalanceVersion(List)",
            "2427  \n2428  \n2429  \n2430  \n2431  \n2432  \n2433  \n2434  \n2435  \n2436  \n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444  \n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471 -\n2472  \n2473  ",
            "    /**\n     * @param aff Affinity assignments.\n     */\n    private void updateRebalanceVersion(List<List<ClusterNode>> aff) {\n        if (!rebalancedTopVer.equals(readyTopVer)) {\n            if (node2part == null || !node2part.valid())\n                return;\n\n            for (int i = 0; i < grp.affinity().partitions(); i++) {\n                List<ClusterNode> affNodes = aff.get(i);\n\n                // Topology doesn't contain server nodes (just clients).\n                if (affNodes.isEmpty())\n                    continue;\n\n                Set<ClusterNode> owners = U.newHashSet(affNodes.size());\n\n                for (ClusterNode node : affNodes) {\n                    if (hasState(i, node.id(), OWNING))\n                        owners.add(node);\n                }\n\n                if (!grp.isReplicated()) {\n                    Set<UUID> diff = diffFromAffinity.get(i);\n\n                    if (diff != null) {\n                        for (UUID nodeId : diff) {\n                            if (hasState(i, nodeId, OWNING)) {\n                                ClusterNode node = ctx.discovery().node(nodeId);\n\n                                if (node != null)\n                                    owners.add(node);\n                            }\n                        }\n                    }\n                }\n\n                if (affNodes.size() != owners.size() || !owners.containsAll(affNodes))\n                    return;\n            }\n\n            rebalancedTopVer = readyTopVer;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Updated rebalanced version [cache=\" + grp.cacheOrGroupName() + \", ver=\" + rebalancedTopVer + ']');\n        }\n    }",
            "2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511 +\n2512  \n2513  ",
            "    /**\n     * @param aff Affinity assignments.\n     */\n    private void updateRebalanceVersion(List<List<ClusterNode>> aff) {\n        if (!rebalancedTopVer.equals(readyTopVer)) {\n            if (node2part == null || !node2part.valid())\n                return;\n\n            for (int i = 0; i < grp.affinity().partitions(); i++) {\n                List<ClusterNode> affNodes = aff.get(i);\n\n                // Topology doesn't contain server nodes (just clients).\n                if (affNodes.isEmpty())\n                    continue;\n\n                Set<ClusterNode> owners = U.newHashSet(affNodes.size());\n\n                for (ClusterNode node : affNodes) {\n                    if (hasState(i, node.id(), OWNING))\n                        owners.add(node);\n                }\n\n                if (!grp.isReplicated()) {\n                    Set<UUID> diff = diffFromAffinity.get(i);\n\n                    if (diff != null) {\n                        for (UUID nodeId : diff) {\n                            if (hasState(i, nodeId, OWNING)) {\n                                ClusterNode node = ctx.discovery().node(nodeId);\n\n                                if (node != null)\n                                    owners.add(node);\n                            }\n                        }\n                    }\n                }\n\n                if (affNodes.size() != owners.size() || !owners.containsAll(affNodes))\n                    return;\n            }\n\n            rebalancedTopVer = readyTopVer;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Updated rebalanced version [grp=\" + grp.cacheOrGroupName() + \", ver=\" + rebalancedTopVer + ']');\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::applyUpdateCounters()",
            "1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495 -\n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  ",
            "    /** {@inheritDoc} */\n    @Override public void applyUpdateCounters() {\n        long now = U.currentTimeMillis();\n\n        lock.writeLock().lock();\n\n        try {\n            long acquired = U.currentTimeMillis();\n\n            if (acquired - now >= 100) {\n                if (timeLog.isInfoEnabled())\n                    timeLog.info(\"Waited too long to acquire topology write lock \" +\n                        \"[cache=\" + grp.groupId() + \", waitTime=\" + (acquired - now) + ']');\n            }\n\n            if (stopping)\n                return;\n\n            for (int i = 0; i < locParts.length(); i++) {\n                GridDhtLocalPartition part = locParts.get(i);\n\n                if (part == null)\n                    continue;\n\n                long updCntr = cntrMap.updateCounter(part.id());\n\n                if (updCntr > part.updateCounter())\n                    part.updateCounter(updCntr);\n                else if (part.updateCounter() > 0) {\n                    cntrMap.initialUpdateCounter(part.id(), part.initialUpdateCounter());\n                    cntrMap.updateCounter(part.id(), part.updateCounter());\n                }\n            }\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            "1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525 +\n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  ",
            "    /** {@inheritDoc} */\n    @Override public void applyUpdateCounters() {\n        long now = U.currentTimeMillis();\n\n        lock.writeLock().lock();\n\n        try {\n            long acquired = U.currentTimeMillis();\n\n            if (acquired - now >= 100) {\n                if (timeLog.isInfoEnabled())\n                    timeLog.info(\"Waited too long to acquire topology write lock \" +\n                        \"[grp=\" + grp.cacheOrGroupName() + \", waitTime=\" + (acquired - now) + ']');\n            }\n\n            if (stopping)\n                return;\n\n            for (int i = 0; i < locParts.length(); i++) {\n                GridDhtLocalPartition part = locParts.get(i);\n\n                if (part == null)\n                    continue;\n\n                long updCntr = cntrMap.updateCounter(part.id());\n\n                if (updCntr > part.updateCounter())\n                    part.updateCounter(updCntr);\n                else if (part.updateCounter() > 0) {\n                    cntrMap.initialUpdateCounter(part.id(), part.initialUpdateCounter());\n                    cntrMap.updateCounter(part.id(), part.updateCounter());\n                }\n            }\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::collectUpdateCounters(CachePartitionPartialCountersMap)",
            "1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460 -\n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  ",
            "    /** {@inheritDoc} */\n    @Override public void collectUpdateCounters(CachePartitionPartialCountersMap cntrMap) {\n        assert cntrMap != null;\n\n        long now = U.currentTimeMillis();\n\n        lock.writeLock().lock();\n\n        try {\n            long acquired = U.currentTimeMillis();\n\n            if (acquired - now >= 100) {\n                if (timeLog.isInfoEnabled())\n                    timeLog.info(\"Waited too long to acquire topology write lock \" +\n                        \"[cache=\" + grp.groupId() + \", waitTime=\" + (acquired - now) + ']');\n            }\n\n            if (stopping)\n                return;\n\n            for (int i = 0; i < cntrMap.size(); i++) {\n                int pId = cntrMap.partitionAt(i);\n\n                long initialUpdateCntr = cntrMap.initialUpdateCounterAt(i);\n                long updateCntr = cntrMap.updateCounterAt(i);\n\n                if (this.cntrMap.updateCounter(pId) < updateCntr) {\n                    this.cntrMap.initialUpdateCounter(pId, initialUpdateCntr);\n                    this.cntrMap.updateCounter(pId, updateCntr);\n                }\n            }\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }",
            "1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490 +\n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  ",
            "    /** {@inheritDoc} */\n    @Override public void collectUpdateCounters(CachePartitionPartialCountersMap cntrMap) {\n        assert cntrMap != null;\n\n        long now = U.currentTimeMillis();\n\n        lock.writeLock().lock();\n\n        try {\n            long acquired = U.currentTimeMillis();\n\n            if (acquired - now >= 100) {\n                if (timeLog.isInfoEnabled())\n                    timeLog.info(\"Waited too long to acquire topology write lock \" +\n                        \"[grp=\" + grp.cacheOrGroupName() + \", waitTime=\" + (acquired - now) + ']');\n            }\n\n            if (stopping)\n                return;\n\n            for (int i = 0; i < cntrMap.size(); i++) {\n                int pId = cntrMap.partitionAt(i);\n\n                long initialUpdateCntr = cntrMap.initialUpdateCounterAt(i);\n                long updateCntr = cntrMap.updateCounterAt(i);\n\n                if (this.cntrMap.updateCounter(pId) < updateCntr) {\n                    this.cntrMap.initialUpdateCounter(pId, initialUpdateCntr);\n                    this.cntrMap.updateCounter(pId, updateCntr);\n                }\n            }\n        }\n        finally {\n            lock.writeLock().unlock();\n        }\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::initPartitions0(AffinityTopologyVersion,GridDhtPartitionsExchangeFuture,long)",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365 -\n 366 -\n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392 -\n 393 -\n 394 -\n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  ",
            "    /**\n     * @param affVer Affinity version to use.\n     * @param exchFut Exchange future.\n     * @param updateSeq Update sequence.\n     */\n    private void initPartitions0(AffinityTopologyVersion affVer, GridDhtPartitionsExchangeFuture exchFut, long updateSeq) {\n        List<List<ClusterNode>> aff = grp.affinity().readyAssignments(affVer);\n\n        if (grp.affinityNode()) {\n            ClusterNode loc = ctx.localNode();\n\n            ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n            GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n            assert grp.affinity().lastVersion().equals(affVer) :\n                \"Invalid affinity [topVer=\" + grp.affinity().lastVersion() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", affVer=\" + affVer +\n                \", fut=\" + exchFut + ']';\n\n            int num = grp.affinity().partitions();\n\n            if (grp.rebalanceEnabled()) {\n                boolean added = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                boolean first = added || (loc.equals(oldest) && loc.id().equals(exchId.nodeId()) && exchId.isJoined());\n\n                if (first) {\n                    assert exchId.isJoined() || added;\n\n                    for (int p = 0; p < num; p++) {\n                        if (localNode(p, aff)) {\n                            GridDhtLocalPartition locPart = createPartition(p);\n\n                            boolean owned = locPart.own();\n\n                            assert owned : \"Failed to own partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                                \", part=\" + locPart + ']';\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Owned partition for oldest node: \" + locPart);\n\n                            updateSeq = updateLocal(p, locPart.state(), updateSeq, affVer);\n                        }\n                    }\n                }\n                else\n                    createPartitions(affVer, aff, updateSeq);\n            }\n            else {\n                // If preloader is disabled, then we simply clear out\n                // the partitions this node is not responsible for.\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition0(p, affVer, false, true, false);\n\n                    boolean belongs = localNode(p, aff);\n\n                    if (locPart != null) {\n                        if (!belongs) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state.active()) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, affVer);\n\n                                if (log.isDebugEnabled())\n                                    log.debug(\"Evicting partition with rebalancing disabled \" +\n                                        \"(it does not belong to affinity): \" + locPart);\n                            }\n                        }\n                        else\n                            locPart.own();\n                    }\n                    else if (belongs) {\n                        locPart = createPartition(p);\n\n                        locPart.own();\n\n                        updateLocal(p, locPart.state(), updateSeq, affVer);\n                    }\n                }\n            }\n        }\n\n        updateRebalanceVersion(aff);\n    }",
            " 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365 +\n 366 +\n 367 +\n 368 +\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394 +\n 395 +\n 396 +\n 397 +\n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  ",
            "    /**\n     * @param affVer Affinity version to use.\n     * @param exchFut Exchange future.\n     * @param updateSeq Update sequence.\n     */\n    private void initPartitions0(AffinityTopologyVersion affVer, GridDhtPartitionsExchangeFuture exchFut, long updateSeq) {\n        List<List<ClusterNode>> aff = grp.affinity().readyAssignments(affVer);\n\n        if (grp.affinityNode()) {\n            ClusterNode loc = ctx.localNode();\n\n            ClusterNode oldest = discoCache.oldestAliveServerNode();\n\n            GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n            assert grp.affinity().lastVersion().equals(affVer) :\n                \"Invalid affinity [topVer=\" + grp.affinity().lastVersion() +\n                \", grp=\" + grp.cacheOrGroupName() +\n                \", affVer=\" + affVer +\n                \", fut=\" + exchFut + ']';\n\n            int num = grp.affinity().partitions();\n\n            if (grp.rebalanceEnabled()) {\n                boolean added = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                boolean first = added || (loc.equals(oldest) && loc.id().equals(exchId.nodeId()) && exchId.isJoined());\n\n                if (first) {\n                    assert exchId.isJoined() || added;\n\n                    for (int p = 0; p < num; p++) {\n                        if (localNode(p, aff)) {\n                            GridDhtLocalPartition locPart = createPartition(p);\n\n                            boolean owned = locPart.own();\n\n                            assert owned : \"Failed to own partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                                \", part=\" + locPart + ']';\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Owned partition for oldest node [grp=\" + grp.cacheOrGroupName() +\n                                    \", part=\" + locPart + ']');\n                            }\n\n                            updateSeq = updateLocal(p, locPart.state(), updateSeq, affVer);\n                        }\n                    }\n                }\n                else\n                    createPartitions(affVer, aff, updateSeq);\n            }\n            else {\n                // If preloader is disabled, then we simply clear out\n                // the partitions this node is not responsible for.\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition0(p, affVer, false, true, false);\n\n                    boolean belongs = localNode(p, aff);\n\n                    if (locPart != null) {\n                        if (!belongs) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state.active()) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, affVer);\n\n                                if (log.isDebugEnabled()) {\n                                    log.debug(\"Evicting partition with rebalancing disabled (it does not belong to \" +\n                                        \"affinity) [grp=\" + grp.cacheOrGroupName() + \", part=\" + locPart + ']');\n                                }\n                            }\n                        }\n                        else\n                            locPart.own();\n                    }\n                    else if (belongs) {\n                        locPart = createPartition(p);\n\n                        locPart.own();\n\n                        updateLocal(p, locPart.state(), updateSeq, affVer);\n                    }\n                }\n            }\n        }\n\n        updateRebalanceVersion(aff);\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::localPartition0(int,AffinityTopologyVersion,boolean,boolean,boolean)",
            " 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837 -\n 838  \n 839  \n 840  \n 841  \n 842 -\n 843 -\n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850 -\n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860 -\n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  ",
            "    /**\n     * @param p Partition number.\n     * @param topVer Topology version.\n     * @param create Create flag.\n     * @param updateSeq Update sequence.\n     * @return Local partition.\n     */\n    @SuppressWarnings(\"TooBroadScope\")\n    private GridDhtLocalPartition localPartition0(int p,\n        AffinityTopologyVersion topVer,\n        boolean create,\n        boolean showRenting,\n        boolean updateSeq) {\n        GridDhtLocalPartition loc;\n\n        loc = locParts.get(p);\n\n        GridDhtPartitionState state = loc != null ? loc.state() : null;\n\n        if (loc != null && state != EVICTED && (state != RENTING || showRenting))\n            return loc;\n\n        if (!create)\n            return null;\n\n        boolean created = false;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                loc = locParts.get(p);\n\n                state = loc != null ? loc.state() : null;\n\n                boolean belongs = partitionLocalNode(p, topVer);\n\n                if (loc != null && state == EVICTED) {\n                    try {\n                        loc.rent(false).get();\n                    }\n                    catch (IgniteCheckedException ex) {\n                        throw new IgniteException(ex);\n                    }\n\n                    locParts.set(p, loc = null);\n\n                    if (!belongs) {\n                        throw new GridDhtInvalidPartitionException(p, \"Adding entry to evicted partition \" +\n                            \"(often may be caused by inconsistent 'key.hashCode()' implementation) \" +\n                            \"[part=\" + p + \", topVer=\" + topVer + \", this.topVer=\" + this.readyTopVer + ']');\n                    }\n                }\n                else if (loc != null && state == RENTING && !showRenting) {\n                    throw new GridDhtInvalidPartitionException(p, \"Adding entry to partition that is concurrently \" +\n                        \"evicted [part=\" + p + \", shouldBeMoving=\" + loc.reload() + \", belongs=\" + belongs +\n                        \", topVer=\" + topVer + \", curTopVer=\" + this.readyTopVer + \"]\");\n                }\n\n                if (loc == null) {\n                    if (!belongs)\n                        throw new GridDhtInvalidPartitionException(p, \"Creating partition which does not belong to \" +\n                            \"local node (often may be caused by inconsistent 'key.hashCode()' implementation) \" +\n                            \"[part=\" + p + \", topVer=\" + topVer + \", this.topVer=\" + this.readyTopVer + ']');\n\n                    locParts.set(p, loc = new GridDhtLocalPartition(ctx, grp, p));\n\n                    if (updateSeq)\n                        this.updateSeq.incrementAndGet();\n\n                    created = true;\n\n                    if (log.isDebugEnabled())\n                        log.debug(\"Created local partition: \" + loc);\n                }\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        if (created && ctx.pageStore() != null) {\n            try {\n                ctx.pageStore().onPartitionCreated(grp.groupId(), p);\n            }\n            catch (IgniteCheckedException e) {\n                // TODO ignite-db\n                throw new IgniteException(e);\n            }\n        }\n\n        return loc;\n    }",
            " 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853 +\n 854 +\n 855  \n 856  \n 857  \n 858  \n 859 +\n 860 +\n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867 +\n 868 +\n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878 +\n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  ",
            "    /**\n     * @param p Partition number.\n     * @param topVer Topology version.\n     * @param create Create flag.\n     * @param updateSeq Update sequence.\n     * @return Local partition.\n     */\n    @SuppressWarnings(\"TooBroadScope\")\n    private GridDhtLocalPartition localPartition0(int p,\n        AffinityTopologyVersion topVer,\n        boolean create,\n        boolean showRenting,\n        boolean updateSeq) {\n        GridDhtLocalPartition loc;\n\n        loc = locParts.get(p);\n\n        GridDhtPartitionState state = loc != null ? loc.state() : null;\n\n        if (loc != null && state != EVICTED && (state != RENTING || showRenting))\n            return loc;\n\n        if (!create)\n            return null;\n\n        boolean created = false;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                loc = locParts.get(p);\n\n                state = loc != null ? loc.state() : null;\n\n                boolean belongs = partitionLocalNode(p, topVer);\n\n                if (loc != null && state == EVICTED) {\n                    try {\n                        loc.rent(false).get();\n                    }\n                    catch (IgniteCheckedException ex) {\n                        throw new IgniteException(ex);\n                    }\n\n                    locParts.set(p, loc = null);\n\n                    if (!belongs) {\n                        throw new GridDhtInvalidPartitionException(p, \"Adding entry to evicted partition \" +\n                            \"(often may be caused by inconsistent 'key.hashCode()' implementation) \" +\n                            \"[grp=\" + grp.cacheOrGroupName() + \", part=\" + p + \", topVer=\" + topVer +\n                            \", this.topVer=\" + this.readyTopVer + ']');\n                    }\n                }\n                else if (loc != null && state == RENTING && !showRenting) {\n                    throw new GridDhtInvalidPartitionException(p, \"Adding entry to partition that is concurrently \" +\n                        \"evicted [grp=\" + grp.cacheOrGroupName() + \", part=\" + p + \", shouldBeMoving=\" +\n                        loc.reload() + \", belongs=\" + belongs + \", topVer=\" + topVer + \", curTopVer=\" + this.readyTopVer + \"]\");\n                }\n\n                if (loc == null) {\n                    if (!belongs)\n                        throw new GridDhtInvalidPartitionException(p, \"Creating partition which does not belong to \" +\n                            \"local node (often may be caused by inconsistent 'key.hashCode()' implementation) \" +\n                            \"[grp=\" + grp.cacheOrGroupName() + \", part=\" + p + \", topVer=\" + topVer +\n                            \", this.topVer=\" + this.readyTopVer + ']');\n\n                    locParts.set(p, loc = new GridDhtLocalPartition(ctx, grp, p));\n\n                    if (updateSeq)\n                        this.updateSeq.incrementAndGet();\n\n                    created = true;\n\n                    if (log.isDebugEnabled())\n                        log.debug(\"Created local partition [grp=\" + grp.cacheOrGroupName() + \", part=\" + loc + ']');\n                }\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        if (created && ctx.pageStore() != null) {\n            try {\n                ctx.pageStore().onPartitionCreated(grp.groupId(), p);\n            }\n            catch (IgniteCheckedException e) {\n                // TODO ignite-db\n                throw new IgniteException(e);\n            }\n        }\n\n        return loc;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::afterExchange(GridDhtPartitionsExchangeFuture)",
            " 586  \n 587 -\n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595 -\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612 -\n 613 -\n 614 -\n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625 -\n 626 -\n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641 -\n 642 -\n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 -\n 659 -\n 660  \n 661  \n 662 -\n 663 -\n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680 -\n 681 -\n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  ",
            "    /** {@inheritDoc} */\n    @Override public boolean afterExchange(GridDhtPartitionsExchangeFuture exchFut) throws IgniteCheckedException {\n        boolean changed = false;\n\n        int num = grp.affinity().partitions();\n\n        AffinityTopologyVersion topVer = exchFut.context().events().topologyVersion();\n\n        assert grp.affinity().lastVersion().equals(topVer) : \"Affinity is not initialized \" +\n            \"[topVer=\" + topVer +\n            \", affVer=\" + grp.affinity().lastVersion() +\n            \", fut=\" + exchFut + ']';\n\n        ctx.database().checkpointReadLock();\n\n        try {\n\n            lock.writeLock().lock();\n\n            try {\n                if (stopping)\n                    return false;\n\n                assert readyTopVer.initialized() : readyTopVer;\n                assert lastTopChangeVer.equals(readyTopVer);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map before afterExchange [exchId=\" + exchFut.exchangeId() + \", fullMap=\" +\n                        fullMapString() + ']');\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition0(p, topVer, false, false, false);\n\n                    if (partitionLocalNode(p, topVer)) {\n                        // This partition will be created during next topology event,\n                        // which obviously has not happened at this point.\n                        if (locPart == null) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Skipping local partition afterExchange (will not create): \" + p);\n\n                            continue;\n                        }\n\n                        GridDhtPartitionState state = locPart.state();\n\n                        if (state == MOVING) {\n                            if (grp.rebalanceEnabled()) {\n                                Collection<ClusterNode> owners = owners(p);\n\n                                // If there are no other owners, then become an owner.\n                                if (F.isEmpty(owners)) {\n                                    boolean owned = locPart.own();\n\n                                    assert owned : \"Failed to own partition [grp=\" + grp.cacheOrGroupName() + \", locPart=\" +\n                                        locPart + ']';\n\n                                    updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n\n                                    changed = true;\n\n                                    if (grp.eventRecordable(EVT_CACHE_REBALANCE_PART_DATA_LOST)) {\n                                        DiscoveryEvent discoEvt = exchFut.events().lastEvent();\n\n                                        grp.addRebalanceEvent(p,\n                                            EVT_CACHE_REBALANCE_PART_DATA_LOST,\n                                            discoEvt.eventNode(),\n                                            discoEvt.type(),\n                                            discoEvt.timestamp());\n                                    }\n\n                                    if (log.isDebugEnabled())\n                                        log.debug(\"Owned partition: \" + locPart);\n                                }\n                                else if (log.isDebugEnabled())\n                                    log.debug(\"Will not own partition (there are owners to rebalance from) [locPart=\" +\n                                        locPart + \", owners = \" + owners + ']');\n                            }\n                            else\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n                        }\n                    }\n                    else {\n                        if (locPart != null) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state == MOVING) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n\n                                changed = true;\n\n                                if (log.isDebugEnabled())\n                                    log.debug(\"Evicting moving partition (it does not belong to affinity): \" + locPart);\n                            }\n                        }\n                    }\n                }\n\n                AffinityAssignment aff = grp.affinity().readyAffinity(topVer);\n\n                if (node2part != null && node2part.valid())\n                    changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff.assignment());\n\n                consistencyCheck();\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        return changed;\n    }",
            " 593  \n 594 +\n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602 +\n 603 +\n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620 +\n 621 +\n 622 +\n 623 +\n 624 +\n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635 +\n 636 +\n 637 +\n 638 +\n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653 +\n 654 +\n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670 +\n 671 +\n 672 +\n 673 +\n 674  \n 675  \n 676 +\n 677 +\n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694 +\n 695 +\n 696 +\n 697 +\n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  ",
            "    /** {@inheritDoc} */\n    @Override public boolean afterExchange(GridDhtPartitionsExchangeFuture exchFut) {\n        boolean changed = false;\n\n        int num = grp.affinity().partitions();\n\n        AffinityTopologyVersion topVer = exchFut.context().events().topologyVersion();\n\n        assert grp.affinity().lastVersion().equals(topVer) : \"Affinity is not initialized \" +\n            \"[grp=\" + grp.cacheOrGroupName() +\n            \", topVer=\" + topVer +\n            \", affVer=\" + grp.affinity().lastVersion() +\n            \", fut=\" + exchFut + ']';\n\n        ctx.database().checkpointReadLock();\n\n        try {\n\n            lock.writeLock().lock();\n\n            try {\n                if (stopping)\n                    return false;\n\n                assert readyTopVer.initialized() : readyTopVer;\n                assert lastTopChangeVer.equals(readyTopVer);\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Partition map before afterExchange [grp=\" + grp.cacheOrGroupName() +\n                        \", exchId=\" + exchFut.exchangeId() +\n                        \", fullMap=\" + fullMapString() + ']');\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                for (int p = 0; p < num; p++) {\n                    GridDhtLocalPartition locPart = localPartition0(p, topVer, false, false, false);\n\n                    if (partitionLocalNode(p, topVer)) {\n                        // This partition will be created during next topology event,\n                        // which obviously has not happened at this point.\n                        if (locPart == null) {\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Skipping local partition afterExchange (will not create) [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() + \", p=\" + p + ']');\n                            }\n\n                            continue;\n                        }\n\n                        GridDhtPartitionState state = locPart.state();\n\n                        if (state == MOVING) {\n                            if (grp.rebalanceEnabled()) {\n                                Collection<ClusterNode> owners = owners(p);\n\n                                // If there are no other owners, then become an owner.\n                                if (F.isEmpty(owners)) {\n                                    boolean owned = locPart.own();\n\n                                    assert owned : \"Failed to own partition [grp=\" + grp.cacheOrGroupName() +\n                                        \", locPart=\" + locPart + ']';\n\n                                    updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n\n                                    changed = true;\n\n                                    if (grp.eventRecordable(EVT_CACHE_REBALANCE_PART_DATA_LOST)) {\n                                        DiscoveryEvent discoEvt = exchFut.events().lastEvent();\n\n                                        grp.addRebalanceEvent(p,\n                                            EVT_CACHE_REBALANCE_PART_DATA_LOST,\n                                            discoEvt.eventNode(),\n                                            discoEvt.type(),\n                                            discoEvt.timestamp());\n                                    }\n\n                                    if (log.isDebugEnabled()) {\n                                        log.debug(\"Owned partition [grp=\" + grp.cacheOrGroupName() +\n                                            \", part=\" + locPart + ']');\n                                    }\n                                }\n                                else if (log.isDebugEnabled())\n                                    log.debug(\"Will not own partition (there are owners to rebalance from) [grp=\" + grp.cacheOrGroupName() +\n                                        \", locPart=\" + locPart + \", owners = \" + owners + ']');\n                            }\n                            else\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n                        }\n                    }\n                    else {\n                        if (locPart != null) {\n                            GridDhtPartitionState state = locPart.state();\n\n                            if (state == MOVING) {\n                                locPart.rent(false);\n\n                                updateSeq = updateLocal(p, locPart.state(), updateSeq, topVer);\n\n                                changed = true;\n\n                                if (log.isDebugEnabled()) {\n                                    log.debug(\"Evicting moving partition (it does not belong to affinity) [\" +\n                                        \"grp=\" + grp.cacheOrGroupName() + \", part=\" + locPart + ']');\n                                }\n                            }\n                        }\n                    }\n                }\n\n                AffinityAssignment aff = grp.affinity().readyAffinity(topVer);\n\n                if (node2part != null && node2part.valid())\n                    changed |= checkEvictions(updateSeq, aff);\n\n                updateRebalanceVersion(aff.assignment());\n\n                consistencyCheck();\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n\n        return changed;\n    }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(GridDhtPartitionExchangeId,GridDhtPartitionMap,boolean)",
            "1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543 -\n1544 -\n1545  \n1546  \n1547 -\n1548 -\n1549 -\n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566 -\n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585 -\n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662 -\n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts,\n        boolean force\n    ) {\n        if (log.isDebugEnabled())\n            log.debug(\"Updating single partition map [exchId=\" + exchId + \", parts=\" + mapString(parts) + ']');\n\n        if (!ctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received partition update for non-existing node (will ignore) [exchId=\" + exchId +\n                    \", parts=\" + parts + ']');\n\n            return false;\n        }\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (stopping)\n                    return false;\n\n                if (!force) {\n                    if (lastTopChangeVer.initialized() && exchId != null && lastTopChangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                        U.warn(log, \"Stale exchange id for single partition map update (will ignore) [\" +\n                            \"lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exch=\" + exchId.topologyVersion() + ']');\n\n                        return false;\n                    }\n                }\n\n                if (node2part == null)\n                    // Create invalid partition map.\n                    node2part = new GridDhtPartitionFullMap();\n\n                GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n                if (force) {\n                    if (cur != null && cur.topologyVersion().initialized())\n                        parts.updateSequence(cur.updateSequence(), cur.topologyVersion());\n                }\n                else if (isStaleUpdate(cur, parts)) {\n                    U.warn(log, \"Stale update for single partition map update (will ignore) [exchId=\" + exchId +\n                        \", curMap=\" + cur +\n                        \", newMap=\" + parts + ']');\n\n                    return false;\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                node2part.newUpdateSequence(updateSeq);\n\n                boolean changed = false;\n\n                if (cur == null || !cur.equals(parts))\n                    changed = true;\n\n                node2part.put(parts.nodeId(), parts);\n\n                // During exchange diff is calculated after all messages are received and affinity initialized.\n                if (exchId == null && !grp.isReplicated()) {\n                    if (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0) {\n                        AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                        // Add new mappings.\n                        for (Map.Entry<Integer, GridDhtPartitionState> e : parts.entrySet()) {\n                            int p = e.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e.getValue() == MOVING || e.getValue() == OWNING || e.getValue() == RENTING)\n                                && !affAssignment.getIds(p).contains(parts.nodeId())) {\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                if (diffIds.add(parts.nodeId()))\n                                    changed = true;\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(parts.nodeId())) {\n                                    changed = true;\n\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n\n                        // Remove obsolete mappings.\n                        if (cur != null) {\n                            for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                                Set<UUID> ids = diffFromAffinity.get(p);\n\n                                if (ids != null && ids.remove(parts.nodeId())) {\n                                    changed = true;\n\n                                    if (ids.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n\n                        diffFromAffinityVer = readyTopVer;\n                    }\n                }\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchId == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.assignment());\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after single update: \" + fullMapString());\n\n                if (changed && exchId == null)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            "1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573 +\n1574 +\n1575 +\n1576 +\n1577  \n1578  \n1579 +\n1580 +\n1581 +\n1582 +\n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  \n1594  \n1595  \n1596  \n1597  \n1598  \n1599 +\n1600 +\n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  \n1614  \n1615  \n1616  \n1617  \n1618  \n1619 +\n1620 +\n1621 +\n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698 +\n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable GridDhtPartitionExchangeId exchId,\n        GridDhtPartitionMap parts,\n        boolean force\n    ) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Updating single partition map [grp=\" + grp.cacheOrGroupName() + \", exchId=\" + exchId +\n                \", parts=\" + mapString(parts) + ']');\n        }\n\n        if (!ctx.discovery().alive(parts.nodeId())) {\n            if (log.isDebugEnabled()) {\n                log.debug(\"Received partition update for non-existing node (will ignore) [grp=\" + grp.cacheOrGroupName() +\n                    \", exchId=\" + exchId + \", parts=\" + parts + ']');\n            }\n\n            return false;\n        }\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (stopping)\n                    return false;\n\n                if (!force) {\n                    if (lastTopChangeVer.initialized() && exchId != null && lastTopChangeVer.compareTo(exchId.topologyVersion()) > 0) {\n                        U.warn(log, \"Stale exchange id for single partition map update (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exch=\" + exchId.topologyVersion() + ']');\n\n                        return false;\n                    }\n                }\n\n                if (node2part == null)\n                    // Create invalid partition map.\n                    node2part = new GridDhtPartitionFullMap();\n\n                GridDhtPartitionMap cur = node2part.get(parts.nodeId());\n\n                if (force) {\n                    if (cur != null && cur.topologyVersion().initialized())\n                        parts.updateSequence(cur.updateSequence(), cur.topologyVersion());\n                }\n                else if (isStaleUpdate(cur, parts)) {\n                    U.warn(log, \"Stale update for single partition map update (will ignore) [\" +\n                        \"grp=\" + grp.cacheOrGroupName() +\n                        \", exchId=\" + exchId +\n                        \", curMap=\" + cur +\n                        \", newMap=\" + parts + ']');\n\n                    return false;\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                node2part.newUpdateSequence(updateSeq);\n\n                boolean changed = false;\n\n                if (cur == null || !cur.equals(parts))\n                    changed = true;\n\n                node2part.put(parts.nodeId(), parts);\n\n                // During exchange diff is calculated after all messages are received and affinity initialized.\n                if (exchId == null && !grp.isReplicated()) {\n                    if (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0) {\n                        AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                        // Add new mappings.\n                        for (Map.Entry<Integer, GridDhtPartitionState> e : parts.entrySet()) {\n                            int p = e.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e.getValue() == MOVING || e.getValue() == OWNING || e.getValue() == RENTING)\n                                && !affAssignment.getIds(p).contains(parts.nodeId())) {\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                if (diffIds.add(parts.nodeId()))\n                                    changed = true;\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(parts.nodeId())) {\n                                    changed = true;\n\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n\n                        // Remove obsolete mappings.\n                        if (cur != null) {\n                            for (Integer p : F.view(cur.keySet(), F0.notIn(parts.keySet()))) {\n                                Set<UUID> ids = diffFromAffinity.get(p);\n\n                                if (ids != null && ids.remove(parts.nodeId())) {\n                                    changed = true;\n\n                                    if (ids.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n\n                        diffFromAffinityVer = readyTopVer;\n                    }\n                }\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchId == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.assignment());\n                }\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Partition map after single update [grp=\" + grp.cacheOrGroupName() + \", map=\" + fullMapString() + ']');\n\n                if (changed && exchId == null)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ]
    ],
    "bc4209bac205a682729d8ab174348bf188356565": [
        [
            "GridCacheDatabaseSharedManager::onDeActivate(GridKernalContext)",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440 -\n 441  \n 442  \n 443  \n 444  \n 445  \n 446  ",
            "    /** {@inheritDoc} */\n    @Override public void onDeActivate(GridKernalContext kctx) throws IgniteCheckedException {\n        super.onDeActivate(kctx);\n\n        if (log.isDebugEnabled())\n            log.debug(\"DeActivate database manager [id=\" + cctx.localNodeId() +\n                \" topVer=\" + cctx.discovery().topologyVersionEx() + \" ]\");\n\n        onKernalStop0(true);\n\n        /* Must be here, because after deactivate we can invoke activate and file lock must be already configured */\n        stopping = false;\n\n        fileLockHolder = new FileLockHolder(storeMgr.workDir().getPath(), cctx.kernalContext(), log);\n    }",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440 +\n 441  \n 442  \n 443  \n 444  \n 445  \n 446  ",
            "    /** {@inheritDoc} */\n    @Override public void onDeActivate(GridKernalContext kctx) throws IgniteCheckedException {\n        super.onDeActivate(kctx);\n\n        if (log.isDebugEnabled())\n            log.debug(\"DeActivate database manager [id=\" + cctx.localNodeId() +\n                \" topVer=\" + cctx.discovery().topologyVersionEx() + \" ]\");\n\n        onKernalStop0(false);\n\n        /* Must be here, because after deactivate we can invoke activate and file lock must be already configured */\n        stopping = false;\n\n        fileLockHolder = new FileLockHolder(storeMgr.workDir().getPath(), cctx.kernalContext(), log);\n    }"
        ],
        [
            "FilePageStoreManager::onDeActivate(GridKernalContext)",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  ",
            "    /** {@inheritDoc} */\n    @Override public void onDeActivate(GridKernalContext kctx) throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"DeActivate page store manager [id=\" + cctx.localNodeId() +\n                \" topVer=\" + cctx.discovery().topologyVersionEx() + \" ]\");\n\n        stop0(true);\n    }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 +\n 158 +\n 159  ",
            "    /** {@inheritDoc} */\n    @Override public void onDeActivate(GridKernalContext kctx) throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"DeActivate page store manager [id=\" + cctx.localNodeId() +\n                \" topVer=\" + cctx.discovery().topologyVersionEx() + \" ]\");\n\n        stop0(true);\n\n        idxCacheStores.clear();\n    }"
        ],
        [
            "GridCacheDatabaseSharedManager::lock()",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  ",
            "    /** {@inheritDoc} */\n    @Override public void lock() throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"Try to capture file lock [nodeId=\" +\n                cctx.localNodeId() + \" path=\" + fileLockHolder.lockPath() + \"]\");\n\n        fileLockHolder.tryLock(lockWaitTime);\n    }",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480 +\n 481 +\n 482  ",
            "    /** {@inheritDoc} */\n    @Override public void lock() throws IgniteCheckedException {\n        if (log.isDebugEnabled())\n            log.debug(\"Try to capture file lock [nodeId=\" +\n                cctx.localNodeId() + \" path=\" + fileLockHolder.lockPath() + \"]\");\n\n        fileLockHolder.tryLock(lockWaitTime);\n\n        System.out.println(\"Lock: \" + fileLockHolder.lockPath() + \" node \" + cctx.igniteInstanceName());\n    }"
        ]
    ],
    "c20a1f00535f17f6c399874592516c0992ed472d": [
        [
            "CacheBaselineTopologyTest::testPrimaryLeft()",
            " 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testPrimaryLeft() throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ig = grid(0);\n\n        ig.cluster().active(true);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache =\n            ig.createCache(\n                new CacheConfiguration<Integer, Integer>()\n                    .setName(CACHE_NAME)\n                    .setCacheMode(PARTITIONED)\n                    .setBackups(1)\n                    .setPartitionLossPolicy(READ_ONLY_SAFE)\n                    .setReadFromBackup(true)\n                    .setWriteSynchronizationMode(FULL_SYNC)\n                    .setRebalanceDelay(-1)\n            );\n\n        int key = 1;\n\n        List<ClusterNode> affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert affNodes.size() == 2;\n\n        int primaryIdx = -1;\n\n        IgniteEx primary = null;\n        IgniteEx backup = null;\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            if (grid(i).localNode().equals(affNodes.get(0))) {\n                primaryIdx = i;\n                primary = grid(i);\n            }\n            else if (grid(i).localNode().equals(affNodes.get(1)))\n                backup = grid(i);\n        }\n\n        assert primary != null;\n        assert backup != null;\n\n        Integer val1 = 1;\n        Integer val2 = 2;\n\n        cache.put(key, val1);\n\n        assertEquals(val1, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val1, backup.cache(CACHE_NAME).get(key));\n\n        if (ig == primary) {\n            ig = backup;\n\n            cache = ig.cache(CACHE_NAME);\n        }\n\n        primary.close();\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        cache.put(key, val2);\n\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        primary = startGrid(primaryIdx);\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        primary.cache(CACHE_NAME).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        assertEquals(primary.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n    }",
            " 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488 +\n 489 +\n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517 +\n 518 +\n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testPrimaryLeft() throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ig = grid(0);\n\n        ig.cluster().active(true);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache =\n            ig.createCache(\n                new CacheConfiguration<Integer, Integer>()\n                    .setName(CACHE_NAME)\n                    .setCacheMode(PARTITIONED)\n                    .setBackups(1)\n                    .setPartitionLossPolicy(READ_ONLY_SAFE)\n                    .setReadFromBackup(true)\n                    .setWriteSynchronizationMode(FULL_SYNC)\n                    .setRebalanceDelay(-1)\n            );\n\n        int key = 1;\n\n        List<ClusterNode> affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert affNodes.size() == 2;\n\n        int primaryIdx = -1;\n\n        IgniteEx primary = null;\n        IgniteEx backup = null;\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n            if (grid(i).localNode().equals(affNodes.get(0))) {\n                primaryIdx = i;\n                primary = grid(i);\n            }\n            else if (grid(i).localNode().equals(affNodes.get(1)))\n                backup = grid(i);\n        }\n\n        assert primary != null;\n        assert backup != null;\n\n        Integer val1 = 1;\n        Integer val2 = 2;\n\n        cache.put(key, val1);\n\n        assertEquals(val1, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val1, backup.cache(CACHE_NAME).get(key));\n\n        if (ig == primary) {\n            ig = backup;\n\n            cache = ig.cache(CACHE_NAME);\n        }\n\n        primary.close();\n\n        backup.context().cache().context().exchange().affinityReadyFuture(new AffinityTopologyVersion(5, 0)).get();\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        cache.put(key, val2);\n\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        primary = startGrid(primaryIdx);\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        primary.cache(CACHE_NAME).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        assertEquals(primary.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n    }"
        ],
        [
            "CacheBaselineTopologyTest::getConfiguration(String)",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(igniteInstanceName);\n\n        cfg.setConsistentId(igniteInstanceName);\n\n        cfg.setDataStorageConfiguration(\n            new DataStorageConfiguration().setDefaultDataRegionConfiguration(\n                new DataRegionConfiguration()\n                    .setPersistenceEnabled(true)\n                    .setMaxSize(100 * 1024 * 1024)\n                    .setInitialSize(100 * 1024 * 1024)\n            )\n            .setDataRegionConfigurations(\n                new DataRegionConfiguration()\n                .setName(\"memory\")\n                .setPersistenceEnabled(false)\n                .setMaxSize(100 * 1024 * 1024)\n                .setInitialSize(100 * 1024 * 1024)\n            )\n            .setWalMode(WALMode.LOG_ONLY)\n        );\n\n        if (client)\n            cfg.setClientMode(true);\n\n        if (delayRebalance)\n            cfg.setCommunicationSpi(new DelayRebalanceCommunicationSpi());\n\n        return cfg;\n    }",
            " 104  \n 105  \n 106  \n 107  \n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String igniteInstanceName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(igniteInstanceName);\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n        discoSpi.setIpFinder(IP_FINDER);\n\n        cfg.setDiscoverySpi(discoSpi);\n\n        cfg.setConsistentId(igniteInstanceName);\n\n        cfg.setDataStorageConfiguration(\n            new DataStorageConfiguration().setDefaultDataRegionConfiguration(\n                new DataRegionConfiguration()\n                    .setPersistenceEnabled(true)\n                    .setMaxSize(100 * 1024 * 1024)\n                    .setInitialSize(100 * 1024 * 1024)\n            )\n            .setDataRegionConfigurations(\n                new DataRegionConfiguration()\n                .setName(\"memory\")\n                .setPersistenceEnabled(false)\n                .setMaxSize(100 * 1024 * 1024)\n                .setInitialSize(100 * 1024 * 1024)\n            )\n            .setWalMode(WALMode.LOG_ONLY)\n        );\n\n        if (client)\n            cfg.setClientMode(true);\n\n        if (delayRebalance)\n            cfg.setCommunicationSpi(new DelayRebalanceCommunicationSpi());\n\n        return cfg;\n    }"
        ],
        [
            "GridCommonAbstractTest::printPartitionState(String,int)",
            " 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853 -\n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  ",
            "    /**\n     * @param cacheName Cache name.\n     * @param firstParts Count partition for print (will be print first count partition).\n     *\n     * Print partitionState for cache.\n     */\n    protected void printPartitionState(String cacheName, int firstParts) {\n        StringBuilder sb = new StringBuilder();\n\n        sb.append(\"----preload sync futures----\\n\");\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteKernal k = ((IgniteKernal)ig);\n\n            IgniteInternalFuture<?> syncFut = k.internalCache(cacheName)\n                .preloader()\n                .syncFuture();\n\n            sb.append(\"nodeId=\")\n                .append(k.context().localNodeId())\n                .append(\" isDone=\")\n                .append(syncFut.isDone())\n                .append(\"\\n\");\n        }\n\n        sb.append(\"----rebalance futures----\\n\");\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteKernal k = ((IgniteKernal)ig);\n\n            IgniteInternalFuture<?> f = k.internalCache(cacheName)\n                .preloader()\n                .rebalanceFuture();\n\n            try {\n                sb.append(\"nodeId=\").append(k.context().localNodeId())\n                    .append(\" isDone=\").append(f.isDone())\n                    .append(\" res=\").append(f.isDone() ? f.get() : \"N/A\")\n                    .append(\" topVer=\")\n                    .append((U.hasField(f, \"topVer\") ?\n                        U.field(f, \"topVer\") : \"[unknown] may be it is finished future\"))\n                    .append(\"\\n\");\n\n                Map<UUID, T2<Long, Collection<Integer>>> remaining = U.field(f, \"remaining\");\n\n                sb.append(\"remaining:\");\n\n                if (remaining.isEmpty())\n                    sb.append(\"empty\\n\");\n                else\n                    for (Map.Entry<UUID, T2<Long, Collection<Integer>>> e : remaining.entrySet())\n                        sb.append(\"\\nuuid=\").append(e.getKey())\n                            .append(\" startTime=\").append(e.getValue().getKey())\n                            .append(\" parts=\").append(Arrays.toString(e.getValue().getValue().toArray()))\n                            .append(\"\\n\");\n\n            }\n            catch (Throwable e) {\n                log.error(e.getMessage());\n            }\n        }\n\n        sb.append(\"----partition state----\\n\");\n\n        for (Ignite g : G.allGrids()) {\n            IgniteKernal g0 = (IgniteKernal)g;\n\n            sb.append(\"localNodeId=\").append(g0.localNode().id())\n                .append(\" grid=\").append(g0.name())\n                .append(\"\\n\");\n\n            IgniteCacheProxy<?, ?> cache = g0.context().cache().jcache(cacheName);\n\n            GridDhtCacheAdapter<?, ?> dht = dht(cache);\n\n            GridDhtPartitionTopology top = dht.topology();\n\n            int parts = firstParts == 0 ? cache.context()\n                .config()\n                .getAffinity()\n                .partitions() : firstParts;\n\n            for (int p = 0; p < parts; p++) {\n                AffinityTopologyVersion readyVer = dht.context().shared().exchange().readyAffinityVersion();\n\n                Collection<UUID> affNodes = F.nodeIds(dht.context()\n                    .affinity()\n                    .assignment(readyVer)\n                    .idealAssignment()\n                    .get(p));\n\n                GridDhtLocalPartition part = top.localPartition(p, AffinityTopologyVersion.NONE, false);\n\n                sb.append(\"local part=\");\n\n                if (part != null)\n                    sb.append(p).append(\" state=\").append(part.state());\n                else\n                    sb.append(p).append(\" is null\");\n\n                sb.append(\" isAffNode=\")\n                    .append(affNodes.contains(g0.localNode().id()))\n                    .append(\"\\n\");\n\n                for (UUID nodeId : F.nodeIds(g0.context().discovery().allNodes())) {\n                    if (!nodeId.equals(g0.localNode().id()))\n                        sb.append(\" nodeId=\")\n                            .append(nodeId)\n                            .append(\" part=\")\n                            .append(p)\n                            .append(\" state=\")\n                            .append(top.partitionState(nodeId, p))\n                            .append(\" isAffNode=\")\n                            .append(affNodes.contains(nodeId))\n                            .append(\"\\n\");\n                }\n            }\n\n            sb.append(\"\\n\");\n        }\n\n        log.info(\"dump partitions state for <\" + cacheName + \">:\\n\" + sb.toString());\n    }",
            " 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853 +\n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  ",
            "    /**\n     * @param cacheName Cache name.\n     * @param firstParts Count partition for print (will be print first count partition).\n     *\n     * Print partitionState for cache.\n     */\n    protected void printPartitionState(String cacheName, int firstParts) {\n        StringBuilder sb = new StringBuilder();\n\n        sb.append(\"----preload sync futures----\\n\");\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteKernal k = ((IgniteKernal)ig);\n\n            IgniteInternalFuture<?> syncFut = k.internalCache(cacheName)\n                .preloader()\n                .syncFuture();\n\n            sb.append(\"nodeId=\")\n                .append(k.context().localNodeId())\n                .append(\" isDone=\")\n                .append(syncFut.isDone())\n                .append(\"\\n\");\n        }\n\n        sb.append(\"----rebalance futures----\\n\");\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteKernal k = ((IgniteKernal)ig);\n\n            IgniteInternalFuture<?> f = k.internalCache(cacheName)\n                .preloader()\n                .rebalanceFuture();\n\n            try {\n                sb.append(\"nodeId=\").append(k.context().localNodeId())\n                    .append(\" isDone=\").append(f.isDone())\n                    .append(\" res=\").append(f.isDone() ? f.get() : \"N/A\")\n                    .append(\" topVer=\")\n                    .append((U.hasField(f, \"topVer\") ?\n                        String.valueOf(U.field(f, \"topVer\")) : \"[unknown] may be it is finished future\"))\n                    .append(\"\\n\");\n\n                Map<UUID, T2<Long, Collection<Integer>>> remaining = U.field(f, \"remaining\");\n\n                sb.append(\"remaining:\");\n\n                if (remaining.isEmpty())\n                    sb.append(\"empty\\n\");\n                else\n                    for (Map.Entry<UUID, T2<Long, Collection<Integer>>> e : remaining.entrySet())\n                        sb.append(\"\\nuuid=\").append(e.getKey())\n                            .append(\" startTime=\").append(e.getValue().getKey())\n                            .append(\" parts=\").append(Arrays.toString(e.getValue().getValue().toArray()))\n                            .append(\"\\n\");\n\n            }\n            catch (Throwable e) {\n                log.error(e.getMessage());\n            }\n        }\n\n        sb.append(\"----partition state----\\n\");\n\n        for (Ignite g : G.allGrids()) {\n            IgniteKernal g0 = (IgniteKernal)g;\n\n            sb.append(\"localNodeId=\").append(g0.localNode().id())\n                .append(\" grid=\").append(g0.name())\n                .append(\"\\n\");\n\n            IgniteCacheProxy<?, ?> cache = g0.context().cache().jcache(cacheName);\n\n            GridDhtCacheAdapter<?, ?> dht = dht(cache);\n\n            GridDhtPartitionTopology top = dht.topology();\n\n            int parts = firstParts == 0 ? cache.context()\n                .config()\n                .getAffinity()\n                .partitions() : firstParts;\n\n            for (int p = 0; p < parts; p++) {\n                AffinityTopologyVersion readyVer = dht.context().shared().exchange().readyAffinityVersion();\n\n                Collection<UUID> affNodes = F.nodeIds(dht.context()\n                    .affinity()\n                    .assignment(readyVer)\n                    .idealAssignment()\n                    .get(p));\n\n                GridDhtLocalPartition part = top.localPartition(p, AffinityTopologyVersion.NONE, false);\n\n                sb.append(\"local part=\");\n\n                if (part != null)\n                    sb.append(p).append(\" state=\").append(part.state());\n                else\n                    sb.append(p).append(\" is null\");\n\n                sb.append(\" isAffNode=\")\n                    .append(affNodes.contains(g0.localNode().id()))\n                    .append(\"\\n\");\n\n                for (UUID nodeId : F.nodeIds(g0.context().discovery().allNodes())) {\n                    if (!nodeId.equals(g0.localNode().id()))\n                        sb.append(\" nodeId=\")\n                            .append(nodeId)\n                            .append(\" part=\")\n                            .append(p)\n                            .append(\" state=\")\n                            .append(top.partitionState(nodeId, p))\n                            .append(\" isAffNode=\")\n                            .append(affNodes.contains(nodeId))\n                            .append(\"\\n\");\n                }\n            }\n\n            sb.append(\"\\n\");\n        }\n\n        log.info(\"dump partitions state for <\" + cacheName + \">:\\n\" + sb.toString());\n    }"
        ],
        [
            "CacheBaselineTopologyTest::testPrimaryLeftAndClusterRestart()",
            " 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testPrimaryLeftAndClusterRestart() throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ig = grid(0);\n\n        ig.cluster().active(true);\n\n        IgniteCache<Integer, Integer> cache =\n            ig.createCache(\n                new CacheConfiguration<Integer, Integer>()\n                    .setName(CACHE_NAME)\n                    .setWriteSynchronizationMode(FULL_SYNC)\n                    .setCacheMode(PARTITIONED)\n                    .setBackups(1)\n                    .setPartitionLossPolicy(READ_ONLY_SAFE)\n                    .setReadFromBackup(true)\n                    .setRebalanceDelay(-1)\n            );\n\n        int key = 1;\n\n        List<ClusterNode> affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert affNodes.size() == 2;\n\n        int primaryIdx = -1;\n        int backupIdx = -1;\n\n        IgniteEx primary = null;\n        IgniteEx backup = null;\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            if (grid(i).localNode().equals(affNodes.get(0))) {\n                primaryIdx = i;\n                primary = grid(i);\n            }\n            else if (grid(i).localNode().equals(affNodes.get(1))) {\n                backupIdx = i;\n                backup = grid(i);\n            }\n        }\n\n        assert primary != null;\n        assert backup != null;\n\n        Integer val1 = 1;\n        Integer val2 = 2;\n\n        cache.put(key, val1);\n\n        assertEquals(val1, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val1, backup.cache(CACHE_NAME).get(key));\n\n        if (ig == primary) {\n            ig = backup;\n\n            cache = ig.cache(CACHE_NAME);\n        }\n\n        stopGrid(primaryIdx, false);\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        cache.put(key, val2);\n\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        stopAllGrids(false);\n\n        startGrids(NODE_COUNT);\n\n        ig = grid(0);\n        primary = grid(primaryIdx);\n        backup = grid(backupIdx);\n\n        boolean activated = GridTestUtils.waitForCondition(() -> {\n            for (int i = 0; i < NODE_COUNT; i++) {\n                if (!grid(i).cluster().active())\n                    return false;\n            }\n\n            return true;\n        }, 10_000);\n\n        assert activated;\n\n//        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        for (int i = 0; i < NODE_COUNT; i++)\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assertEquals(primary.localNode(), affNodes.get(0));\n        assertEquals(backup.localNode(), affNodes.get(1));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n    }",
            " 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574 +\n 575 +\n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testPrimaryLeftAndClusterRestart() throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ig = grid(0);\n\n        ig.cluster().active(true);\n\n        IgniteCache<Integer, Integer> cache =\n            ig.createCache(\n                new CacheConfiguration<Integer, Integer>()\n                    .setName(CACHE_NAME)\n                    .setWriteSynchronizationMode(FULL_SYNC)\n                    .setCacheMode(PARTITIONED)\n                    .setBackups(1)\n                    .setPartitionLossPolicy(READ_ONLY_SAFE)\n                    .setReadFromBackup(true)\n                    .setRebalanceDelay(-1)\n            );\n\n        int key = 1;\n\n        List<ClusterNode> affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert affNodes.size() == 2;\n\n        int primaryIdx = -1;\n        int backupIdx = -1;\n\n        IgniteEx primary = null;\n        IgniteEx backup = null;\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n            if (grid(i).localNode().equals(affNodes.get(0))) {\n                primaryIdx = i;\n                primary = grid(i);\n            }\n            else if (grid(i).localNode().equals(affNodes.get(1))) {\n                backupIdx = i;\n                backup = grid(i);\n            }\n        }\n\n        assert primary != null;\n        assert backup != null;\n\n        Integer val1 = 1;\n        Integer val2 = 2;\n\n        cache.put(key, val1);\n\n        assertEquals(val1, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val1, backup.cache(CACHE_NAME).get(key));\n\n        if (ig == primary) {\n            ig = backup;\n\n            cache = ig.cache(CACHE_NAME);\n        }\n\n        stopGrid(primaryIdx, false);\n\n        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        cache.put(key, val2);\n\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        stopAllGrids(false);\n\n        startGrids(NODE_COUNT);\n\n        ig = grid(0);\n        primary = grid(primaryIdx);\n        backup = grid(backupIdx);\n\n        boolean activated = GridTestUtils.waitForCondition(() -> {\n            for (int i = 0; i < NODE_COUNT; i++) {\n                if (!grid(i).cluster().active())\n                    return false;\n            }\n\n            return true;\n        }, 10_000);\n\n        assert activated;\n\n//        assertEquals(backup.localNode(), ig.affinity(CACHE_NAME).mapKeyToNode(key));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n\n        for (int i = 0; i < NODE_COUNT; i++)\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        affNodes = (List<ClusterNode>) ig.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assertEquals(primary.localNode(), affNodes.get(0));\n        assertEquals(backup.localNode(), affNodes.get(1));\n\n        assertEquals(val2, primary.cache(CACHE_NAME).get(key));\n        assertEquals(val2, backup.cache(CACHE_NAME).get(key));\n    }"
        ],
        [
            "CacheBaselineTopologyTest::testBaselineTopologyChanges(boolean)",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    private void testBaselineTopologyChanges(boolean fromClient) throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ignite;\n\n        if (fromClient) {\n            client = true;\n\n            ignite = startGrid(NODE_COUNT + 10);\n\n            client = false;\n        }\n        else\n            ignite = grid(0);\n\n        ignite.cluster().active(true);\n\n        awaitPartitionMapExchange();\n\n        Map<ClusterNode, Ignite> nodes = new HashMap<>();\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            Ignite ig = grid(i);\n\n            nodes.put(ig.cluster().localNode(), ig);\n        }\n\n        ignite.createCache(\n            new CacheConfiguration<Integer, Integer>()\n                .setName(CACHE_NAME)\n                .setCacheMode(PARTITIONED)\n                .setBackups(1)\n                .setPartitionLossPolicy(READ_ONLY_SAFE)\n        );\n\n        int key = -1;\n\n        for (int k = 0; k < 100_000; k++) {\n            if (!ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(k).contains(ignite.localNode())) {\n                key = k;\n                break;\n            }\n        }\n\n        assert key >= 0;\n\n        Collection<ClusterNode> initialMapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping.size() == 2 : initialMapping;\n\n        ignite.cluster().setBaselineTopology(baselineNodes(nodes.keySet()));\n\n        Set<String> stoppedNodeNames = new HashSet<>();\n\n        ClusterNode node = initialMapping.iterator().next();\n\n        stoppedNodeNames.add(nodes.get(node).name());\n\n        nodes.get(node).close();\n\n        nodes.remove(node);\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == 1 : mapping;\n        assert initialMapping.containsAll(mapping);\n\n        Set<ClusterNode> blt2 = new HashSet<>(ignite.cluster().nodes());\n\n        ignite.cluster().setBaselineTopology(baselineNodes(blt2));\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> initialMapping2 = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping2.size() == 2 : initialMapping2;\n\n        Ignite newIgnite = startGrid(NODE_COUNT);\n\n        awaitPartitionMapExchange();\n\n        mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == initialMapping2.size() : mapping;\n        assert mapping.containsAll(initialMapping2);\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length == 0;\n\n        Set<ClusterNode> blt3 = new HashSet<>(ignite.cluster().nodes());\n\n        ignite.cluster().setBaselineTopology(baselineNodes(blt3));\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> initialMapping3 = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping3.size() == 2;\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length > 0;\n\n        newIgnite = startGrid(NODE_COUNT + 1);\n\n        awaitPartitionMapExchange();\n\n        mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == initialMapping3.size() : mapping;\n        assert mapping.containsAll(initialMapping3);\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length == 0;\n\n        ignite.cluster().setBaselineTopology(null);\n\n        awaitPartitionMapExchange();\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length > 0;\n    }",
            " 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364 +\n 365 +\n 366 +\n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    private void testBaselineTopologyChanges(boolean fromClient) throws Exception {\n        startGrids(NODE_COUNT);\n\n        IgniteEx ignite;\n\n        if (fromClient) {\n            client = true;\n\n            ignite = startGrid(NODE_COUNT + 10);\n\n            client = false;\n        }\n        else\n            ignite = grid(0);\n\n        ignite.cluster().active(true);\n\n        awaitPartitionMapExchange();\n\n        Map<ClusterNode, Ignite> nodes = new HashMap<>();\n\n        for (int i = 0; i < NODE_COUNT; i++) {\n            Ignite ig = grid(i);\n\n            nodes.put(ig.cluster().localNode(), ig);\n        }\n\n        ignite.createCache(\n            new CacheConfiguration<Integer, Integer>()\n                .setName(CACHE_NAME)\n                .setCacheMode(PARTITIONED)\n                .setBackups(1)\n                .setPartitionLossPolicy(READ_ONLY_SAFE)\n        );\n\n        for (int i = 0; i < NODE_COUNT; i++)\n            grid(i).cache(CACHE_NAME).rebalance().get();\n\n        int key = -1;\n\n        for (int k = 0; k < 100_000; k++) {\n            if (!ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(k).contains(ignite.localNode())) {\n                key = k;\n                break;\n            }\n        }\n\n        assert key >= 0;\n\n        Collection<ClusterNode> initialMapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping.size() == 2 : initialMapping;\n\n        ignite.cluster().setBaselineTopology(baselineNodes(nodes.keySet()));\n\n        Set<String> stoppedNodeNames = new HashSet<>();\n\n        ClusterNode node = initialMapping.iterator().next();\n\n        stoppedNodeNames.add(nodes.get(node).name());\n\n        nodes.get(node).close();\n\n        nodes.remove(node);\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == 1 : mapping;\n        assert initialMapping.containsAll(mapping);\n\n        Set<ClusterNode> blt2 = new HashSet<>(ignite.cluster().nodes());\n\n        ignite.cluster().setBaselineTopology(baselineNodes(blt2));\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> initialMapping2 = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping2.size() == 2 : initialMapping2;\n\n        Ignite newIgnite = startGrid(NODE_COUNT);\n\n        awaitPartitionMapExchange();\n\n        mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == initialMapping2.size() : mapping;\n        assert mapping.containsAll(initialMapping2);\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length == 0;\n\n        Set<ClusterNode> blt3 = new HashSet<>(ignite.cluster().nodes());\n\n        ignite.cluster().setBaselineTopology(baselineNodes(blt3));\n\n        awaitPartitionMapExchange();\n\n        Collection<ClusterNode> initialMapping3 = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert initialMapping3.size() == 2;\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length > 0;\n\n        newIgnite = startGrid(NODE_COUNT + 1);\n\n        awaitPartitionMapExchange();\n\n        mapping = ignite.affinity(CACHE_NAME).mapKeyToPrimaryAndBackups(key);\n\n        assert mapping.size() == initialMapping3.size() : mapping;\n        assert mapping.containsAll(initialMapping3);\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length == 0;\n\n        ignite.cluster().setBaselineTopology(null);\n\n        awaitPartitionMapExchange();\n\n        assert ignite.affinity(CACHE_NAME).primaryPartitions(newIgnite.cluster().localNode()).length > 0;\n    }"
        ]
    ],
    "210ad201a2f6df071e3a2e1c5812e9e28afa1be5": [
        [
            "IgniteStandByClientReconnectTest::testInActiveClientReconnectToInActiveCluster()",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255 -\n 256 -\n 257 -\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 -\n 273 -\n 274 -\n 275  \n 276 -\n 277  \n 278 -\n 279 -\n 280 -\n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testInActiveClientReconnectToInActiveCluster() throws Exception {\n        startNodes(null);\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        reconnectedLatch.await();\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        client.active(true);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkStaticCaches();\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255 +\n 256 +\n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 +\n 273 +\n 274 +\n 275  \n 276 +\n 277  \n 278 +\n 279 +\n 280 +\n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testInActiveClientReconnectToInActiveCluster() throws Exception {\n        startNodes(null);\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        reconnectedLatch.await();\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        client.cluster().active(true);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkStaticCaches();\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }"
        ],
        [
            "IgniteAbstractStandByClientReconnectTest::checkOnlySystemCaches()",
            " 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256 -\n 257 -\n 258 -\n 259  ",
            "    protected void checkOnlySystemCaches() {\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        Assert.assertNull(ig1.cache(ccfg1staticName));\n        Assert.assertNull(ig1.cache(ccfg2staticName));\n        Assert.assertNull(ig1.cache(ccfg3staticName));\n\n        Assert.assertNull(ig1.cache(ccfg1staticWithFilterName));\n        Assert.assertNull(ig1.cache(ccfg2staticWithFilterName));\n\n        Assert.assertNull(ig2.cache(ccfg1staticName));\n        Assert.assertNull(ig2.cache(ccfg2staticName));\n        Assert.assertNull(ig2.cache(ccfg3staticName));\n\n        Assert.assertNull(ig2.cache(ccfg3staticWithFilterName));\n        Assert.assertNull(ig2.cache(ccfg2staticWithFilterName));\n\n        Assert.assertNull(client.cache(ccfg1staticName));\n        Assert.assertNull(client.cache(ccfg2staticName));\n        Assert.assertNull(client.cache(ccfg3staticName));\n\n        Assert.assertNull(client.cache(ccfg3staticWithFilterName));\n        Assert.assertNull(client.cache(ccfg1staticWithFilterName));\n\n        checkDescriptors(ig1,Collections.<String>emptySet());\n        checkDescriptors(ig2,Collections.<String>emptySet());\n        checkDescriptors(client, Collections.<String>emptySet());\n    }",
            " 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308 +\n 309 +\n 310 +\n 311  ",
            "    /**\n     *\n     */\n    protected void checkOnlySystemCaches() {\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        Assert.assertNull(ig1.cache(ccfg1staticName));\n        Assert.assertNull(ig1.cache(ccfg2staticName));\n        Assert.assertNull(ig1.cache(ccfg3staticName));\n\n        Assert.assertNull(ig1.cache(ccfg1staticWithFilterName));\n        Assert.assertNull(ig1.cache(ccfg2staticWithFilterName));\n\n        Assert.assertNull(ig2.cache(ccfg1staticName));\n        Assert.assertNull(ig2.cache(ccfg2staticName));\n        Assert.assertNull(ig2.cache(ccfg3staticName));\n\n        Assert.assertNull(ig2.cache(ccfg3staticWithFilterName));\n        Assert.assertNull(ig2.cache(ccfg2staticWithFilterName));\n\n        Assert.assertNull(client.cache(ccfg1staticName));\n        Assert.assertNull(client.cache(ccfg2staticName));\n        Assert.assertNull(client.cache(ccfg3staticName));\n\n        Assert.assertNull(client.cache(ccfg3staticWithFilterName));\n        Assert.assertNull(client.cache(ccfg1staticWithFilterName));\n\n        checkDescriptors(ig1,Collections.emptySet());\n        checkDescriptors(ig2,Collections.emptySet());\n        checkDescriptors(client, Collections.emptySet());\n    }"
        ],
        [
            "IgniteStandByClientReconnectTest::testActiveClientReconnectToInActiveCluster()",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 -\n 123 -\n 124 -\n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 -\n 147 -\n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 -\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 -\n 172 -\n 173 -\n 174  \n 175 -\n 176  \n 177 -\n 178 -\n 179 -\n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testActiveClientReconnectToInActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        info(\">>>> star grid\");\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        info(\">>>> activate grid\");\n\n        client.active(true);\n\n        checkStaticCaches();\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n        checkDescriptors(client, staticCacheNames);\n\n        info(\">>>> dynamic start [\" + ccfgDynamicName + \", \" + ccfgDynamicWithFilterName + \"]\");\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        info(\">>>> stop \" + node2);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig1.active(false);\n\n        activateLatch.countDown();\n\n        info(\">>>> restart \" + node2);\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        reconnectedLatch.await();\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        client.active(true);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124 +\n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147 +\n 148 +\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 +\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172 +\n 173 +\n 174  \n 175 +\n 176  \n 177 +\n 178 +\n 179 +\n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testActiveClientReconnectToInActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        info(\">>>> star grid\");\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        info(\">>>> activate grid\");\n\n        client.cluster().active(true);\n\n        checkStaticCaches();\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n        checkDescriptors(client, staticCacheNames);\n\n        info(\">>>> dynamic start [\" + ccfgDynamicName + \", \" + ccfgDynamicWithFilterName + \"]\");\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        info(\">>>> stop \" + node2);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig1.cluster().active(false);\n\n        activateLatch.countDown();\n\n        info(\">>>> restart \" + node2);\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        reconnectedLatch.await();\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        client.cluster().active(true);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }"
        ],
        [
            "CacheAffinitySharedManager::onDiscoveryEvent(int,DiscoveryCustomMessage,ClusterNode,AffinityTopologyVersion,DiscoveryDataClusterState)",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 -\n 168 -\n 169 -\n 170  \n 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  ",
            "    /**\n     * Callback invoked from discovery thread when discovery message is received.\n     *\n     * @param type Event type.\n     * @param customMsg Custom message instance.\n     * @param node Event node.\n     * @param topVer Topology version.\n     * @param state Cluster state.\n     */\n    void onDiscoveryEvent(int type,\n        @Nullable DiscoveryCustomMessage customMsg,\n        ClusterNode node,\n        AffinityTopologyVersion topVer,\n        DiscoveryDataClusterState state) {\n        if ((state.transition() || !state.active()) &&\n            !DiscoveryCustomEvent.requiresCentralizedAffinityAssignment(customMsg))\n            return;\n\n        if (type == EVT_NODE_JOINED && node.isLocal())\n            lastAffVer = null;\n\n        if ((!CU.clientNode(node) && (type == EVT_NODE_FAILED || type == EVT_NODE_JOINED || type == EVT_NODE_LEFT)) ||\n            DiscoveryCustomEvent.requiresCentralizedAffinityAssignment(customMsg)) {\n            synchronized (mux) {\n                assert lastAffVer == null || topVer.compareTo(lastAffVer) > 0;\n\n                lastAffVer = topVer;\n            }\n        }\n    }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165 +\n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 +\n 175 +\n 176  \n 177  \n 178  \n 179  \n 180  ",
            "    /**\n     * Callback invoked from discovery thread when discovery message is received.\n     *\n     * @param type Event type.\n     * @param customMsg Custom message instance.\n     * @param node Event node.\n     * @param topVer Topology version.\n     * @param state Cluster state.\n     */\n    void onDiscoveryEvent(int type,\n        @Nullable DiscoveryCustomMessage customMsg,\n        ClusterNode node,\n        AffinityTopologyVersion topVer,\n        DiscoveryDataClusterState state) {\n        if (type == EVT_NODE_JOINED && node.isLocal())\n            lastAffVer = null;\n\n        if ((state.transition() || !state.active()) &&\n            !DiscoveryCustomEvent.requiresCentralizedAffinityAssignment(customMsg))\n            return;\n\n        if ((!CU.clientNode(node) && (type == EVT_NODE_FAILED || type == EVT_NODE_JOINED || type == EVT_NODE_LEFT)) ||\n            DiscoveryCustomEvent.requiresCentralizedAffinityAssignment(customMsg)) {\n            synchronized (mux) {\n                assert lastAffVer == null || topVer.compareTo(lastAffVer) > 0 :\n                    \"lastAffVer=\" + lastAffVer + \", topVer=\" + topVer + \", customMsg=\" + customMsg;\n\n                lastAffVer = topVer;\n            }\n        }\n    }"
        ],
        [
            "IgniteAbstractStandByClientReconnectTest::startNodes(CountDownLatch)",
            " 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182 -\n 183 -\n 184 -\n 185  ",
            "    protected void startNodes(CountDownLatch activateLatch) throws Exception {\n        IgniteConfiguration cfg1 = getConfiguration(node1)\n            .setCacheConfiguration(ccfg1static, ccfg1staticWithFilter);\n\n        IgniteConfiguration cfg2 = getConfiguration(node2)\n            .setCacheConfiguration(ccfg2static, ccfg2staticWithFilter);\n\n        IgniteConfiguration cfg3 = getConfiguration(nodeClient)\n            .setCacheConfiguration(ccfg3static, ccfg3staticWithFilter);\n\n        if (activateLatch != null)\n            cfg3.setDiscoverySpi(\n                new AwaitTcpDiscoverySpi(activateLatch)\n                    .setIpFinder(clientIpFinder)\n            );\n\n        cfg3.setClientMode(true);\n\n        IgniteEx ig1 = startGrid(cfg1);\n        IgniteEx ig2 = startGrid(cfg2);\n        IgniteEx client = startGrid(cfg3);\n    }",
            " 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 +\n 226 +\n 227 +\n 228  ",
            "    /**\n     * @param activateLatch Activate latch. Will be fired when cluster is activated.\n     * @throws Exception If failed.\n     */\n    protected void startNodes(CountDownLatch activateLatch) throws Exception {\n        IgniteConfiguration cfg1 = getConfiguration(node1)\n            .setCacheConfiguration(ccfg1static, ccfg1staticWithFilter);\n\n        IgniteConfiguration cfg2 = getConfiguration(node2)\n            .setCacheConfiguration(ccfg2static, ccfg2staticWithFilter);\n\n        IgniteConfiguration cfg3 = getConfiguration(nodeClient)\n            .setCacheConfiguration(ccfg3static, ccfg3staticWithFilter);\n\n        if (activateLatch != null)\n            cfg3.setDiscoverySpi(\n                new AwaitTcpDiscoverySpi(activateLatch)\n                    .setIpFinder(clientIpFinder)\n            );\n\n        cfg3.setClientMode(true);\n\n        startGrid(cfg1);\n        startGrid(cfg2);\n        startGrid(cfg3);\n    }"
        ],
        [
            "IgniteStandByClientReconnectTest::testActiveClientReconnectToActiveCluster()",
            "  27  \n  28  \n  29  \n  30  \n  31  \n  32  \n  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41 -\n  42 -\n  43 -\n  44  \n  45 -\n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 -\n  62 -\n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86 -\n  87 -\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 -\n  98 -\n  99 -\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testActiveClientReconnectToActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        info(\">>>> star grid\");\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        client.active(true);\n\n        info(\">>>> activate grid\");\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n        checkDescriptors(client, staticCacheNames);\n\n        checkStaticCaches();\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        info(\">>>> dynamic start [\" + ccfgDynamicName + \", \" + ccfgDynamicWithFilterName + \"]\");\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        info(\">>>> stop servers\");\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        info(\">>>> activate new servers\");\n\n        ig1.active(true);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n\n        activateLatch.countDown();\n\n        info(\">>>> reconnect client\");\n\n        reconnectedLatch.await();\n\n        info(\">>>> client reconnected\");\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }",
            "  27  \n  28  \n  29  \n  30  \n  31  \n  32  \n  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41 +\n  42 +\n  43 +\n  44  \n  45 +\n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 +\n  85  \n  86 +\n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 +\n  98 +\n  99 +\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testActiveClientReconnectToActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        info(\">>>> star grid\");\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        client.cluster().active(true);\n\n        info(\">>>> activate grid\");\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n        checkDescriptors(client, staticCacheNames);\n\n        checkStaticCaches();\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        info(\">>>> dynamic start [\" + ccfgDynamicName + \", \" + ccfgDynamicWithFilterName + \"]\");\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        info(\">>>> stop servers\");\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        info(\">>>> activate new servers\");\n\n        ig1.cluster().active(true);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n\n        activateLatch.countDown();\n\n        info(\">>>> reconnect client\");\n\n        reconnectedLatch.await();\n\n        info(\">>>> client reconnected\");\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }"
        ],
        [
            "IgniteStandByClientReconnectTest::testInActiveClientReconnectToActiveCluster()",
            " 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 -\n 201 -\n 202 -\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 -\n 216  \n 217 -\n 218 -\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 -\n 228 -\n 229 -\n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testInActiveClientReconnectToActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.active());\n        assertTrue(!ig2.active());\n        assertTrue(!client.active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        ig1.active(true);\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n\n        activateLatch.countDown();\n\n        reconnectedLatch.await();\n\n        assertTrue(ig1.active());\n        assertTrue(ig2.active());\n        assertTrue(client.active());\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }",
            " 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 +\n 201 +\n 202 +\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 +\n 216  \n 217 +\n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 +\n 228 +\n 229 +\n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testInActiveClientReconnectToActiveCluster() throws Exception {\n        CountDownLatch activateLatch = new CountDownLatch(1);\n\n        startNodes(activateLatch);\n\n        IgniteEx ig1 = grid(node1);\n        IgniteEx ig2 = grid(node2);\n        IgniteEx client = grid(nodeClient);\n\n        assertTrue(!ig1.cluster().active());\n        assertTrue(!ig2.cluster().active());\n        assertTrue(!client.cluster().active());\n\n        final CountDownLatch disconnectedLatch = new CountDownLatch(1);\n        final CountDownLatch reconnectedLatch = new CountDownLatch(1);\n\n        addDisconnectListener(disconnectedLatch, reconnectedLatch);\n\n        stopGrid(node2);\n\n        disconnectedLatch.await();\n\n        ig2 = startGrid(getConfiguration(node2));\n\n        ig1.cluster().active(true);\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n\n        activateLatch.countDown();\n\n        reconnectedLatch.await();\n\n        assertTrue(ig1.cluster().active());\n        assertTrue(ig2.cluster().active());\n        assertTrue(client.cluster().active());\n\n        checkDescriptors(ig1, staticCacheNames);\n        checkDescriptors(ig2, staticCacheNames);\n\n        client.createCache(ccfgDynamic);\n\n        client.createCache(ccfgDynamicWithFilter);\n\n        checkDescriptors(ig1, allCacheNames);\n        checkDescriptors(ig2, allCacheNames);\n        checkDescriptors(client, allCacheNames);\n\n        checkAllCaches();\n    }"
        ]
    ],
    "d8eeea84bef33d33470052fbb86c96d75490c9b8": [
        [
            "GridCacheCommandHandler::handleAsync(GridRestRequest)",
            " 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363 -\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  ",
            "    /** {@inheritDoc} */\n    @Override public IgniteInternalFuture<GridRestResponse> handleAsync(final GridRestRequest req) {\n        assert req instanceof GridRestCacheRequest : \"Invalid command for topology handler: \" + req;\n\n        assert SUPPORTED_COMMANDS.contains(req.command());\n\n        if (log.isDebugEnabled())\n            log.debug(\"Handling cache REST request: \" + req);\n\n        GridRestCacheRequest req0 = (GridRestCacheRequest)req;\n\n        final String cacheName = req0.cacheName();\n\n        final Object key = req0.key();\n\n        final boolean skipStore = parseCacheFlags(req0.cacheFlags());\n\n        try {\n            GridRestCommand cmd = req0.command();\n\n            if (key == null && KEY_REQUIRED_REQUESTS.contains(cmd))\n                throw new IgniteCheckedException(GridRestCommandHandlerAdapter.missingParameter(\"key\"));\n\n            final Long ttl = req0.ttl();\n\n            IgniteInternalFuture<GridRestResponse> fut;\n\n            switch (cmd) {\n                case DESTROY_CACHE: {\n                    // Do not check thread tx here since there can be active system cache txs.\n                    fut = ((IgniteKernal)ctx.grid()).destroyCacheAsync(cacheName, false).chain(\n                        new CX1<IgniteInternalFuture<?>, GridRestResponse>() {\n                            @Override public GridRestResponse applyx(IgniteInternalFuture<?> f)\n                                throws IgniteCheckedException {\n                                return new GridRestResponse(f.get());\n                            }\n                        });\n\n                    break;\n                }\n\n                case GET_OR_CREATE_CACHE: {\n                    // Do not check thread tx here since there can be active system cache txs.\n                    fut = ((IgniteKernal)ctx.grid()).getOrCreateCacheAsync(cacheName, false).chain(\n                        new CX1<IgniteInternalFuture<?>, GridRestResponse>() {\n                            @Override public GridRestResponse applyx(IgniteInternalFuture<?> f)\n                                throws IgniteCheckedException {\n                                return new GridRestResponse(f.get());\n                            }\n                        });\n\n                    break;\n                }\n\n                case CACHE_METADATA: {\n                    fut = ctx.task().execute(MetadataTask.class, null);\n\n                    break;\n                }\n\n                case CACHE_CONTAINS_KEYS: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new ContainsKeysCommand(getKeys(req0)));\n\n                    break;\n                }\n\n                case CACHE_CONTAINS_KEY: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new ContainsKeyCommand(key));\n\n                    break;\n                }\n\n                case CACHE_GET: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetCommand(key));\n\n                    break;\n                }\n\n                case CACHE_GET_AND_PUT: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAndPutCommand(key, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_GET_AND_REPLACE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAndReplaceCommand(key, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_GET_AND_PUT_IF_ABSENT: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAndPutIfAbsentCommand(key, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_PUT_IF_ABSENT: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new PutIfAbsentCommand(key, ttl, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_GET_ALL: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAllCommand(getKeys(req0)));\n\n                    break;\n                }\n\n                case CACHE_PUT: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key, new\n                        PutCommand(key, ttl, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_ADD: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new AddCommand(key, ttl, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_PUT_ALL: {\n                    Map<Object, Object> map = req0.values();\n\n                    if (F.isEmpty(map))\n                        throw new IgniteCheckedException(GridRestCommandHandlerAdapter.missingParameter(\"values\"));\n\n                    for (Map.Entry<Object, Object> e : map.entrySet()) {\n                        if (e.getKey() == null)\n                            throw new IgniteCheckedException(\"Failing putAll operation (null keys are not allowed).\");\n\n                        if (e.getValue() == null)\n                            throw new IgniteCheckedException(\"Failing putAll operation (null values are not allowed).\");\n                    }\n\n                    // HashMap wrapping for correct serialization\n                    map = new HashMap<>(map);\n\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new PutAllCommand(map));\n\n                    break;\n                }\n\n                case CACHE_REMOVE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new RemoveCommand(key));\n\n                    break;\n                }\n\n                case CACHE_REMOVE_VALUE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new RemoveValueCommand(key, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_REPLACE_VALUE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new ReplaceValueCommand(key, getValue(req0), req0.value2()));\n\n                    break;\n                }\n\n                case CACHE_GET_AND_REMOVE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAndRemoveCommand(key));\n\n                    break;\n                }\n\n                case CACHE_REMOVE_ALL: {\n                    Map<Object, Object> map = req0.values();\n\n                    // HashSet wrapping for correct serialization\n                    Set<Object> keys = map == null ? null : new HashSet<>(map.keySet());\n\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new RemoveAllCommand(keys));\n\n                    break;\n                }\n\n                case CACHE_REPLACE: {\n                    final Object val = req0.value();\n\n                    if (val == null)\n                        throw new IgniteCheckedException(GridRestCommandHandlerAdapter.missingParameter(\"val\"));\n\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new ReplaceCommand(key, ttl, val));\n\n                    break;\n                }\n\n                case CACHE_CAS: {\n                    final Object val1 = req0.value();\n                    final Object val2 = req0.value2();\n\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new CasCommand(val2, val1, key));\n\n                    break;\n                }\n\n                case CACHE_APPEND: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new AppendCommand(key, req0));\n\n                    break;\n                }\n\n                case CACHE_PREPEND: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new PrependCommand(key, req0));\n\n                    break;\n                }\n\n                case CACHE_METRICS: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, key, new MetricsCommand());\n\n                    break;\n                }\n\n                case CACHE_SIZE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, key, new SizeCommand());\n\n                    break;\n                }\n\n                default:\n                    throw new IllegalArgumentException(\"Invalid command for cache handler: \" + req);\n            }\n\n            return fut;\n        }\n        catch (IgniteException e) {\n            U.error(log, \"Failed to execute cache command: \" + req, e);\n\n            return new GridFinishedFuture<>(e);\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to execute cache command: \" + req, e);\n\n            return new GridFinishedFuture<>(e);\n        }\n        finally {\n            if (log.isDebugEnabled())\n                log.debug(\"Handled cache REST request: \" + req);\n        }\n    }",
            " 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363 +\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  ",
            "    /** {@inheritDoc} */\n    @Override public IgniteInternalFuture<GridRestResponse> handleAsync(final GridRestRequest req) {\n        assert req instanceof GridRestCacheRequest : \"Invalid command for topology handler: \" + req;\n\n        assert SUPPORTED_COMMANDS.contains(req.command());\n\n        if (log.isDebugEnabled())\n            log.debug(\"Handling cache REST request: \" + req);\n\n        GridRestCacheRequest req0 = (GridRestCacheRequest)req;\n\n        final String cacheName = req0.cacheName() == null ? DFLT_CACHE_NAME: req0.cacheName();\n\n        final Object key = req0.key();\n\n        final boolean skipStore = parseCacheFlags(req0.cacheFlags());\n\n        try {\n            GridRestCommand cmd = req0.command();\n\n            if (key == null && KEY_REQUIRED_REQUESTS.contains(cmd))\n                throw new IgniteCheckedException(GridRestCommandHandlerAdapter.missingParameter(\"key\"));\n\n            final Long ttl = req0.ttl();\n\n            IgniteInternalFuture<GridRestResponse> fut;\n\n            switch (cmd) {\n                case DESTROY_CACHE: {\n                    // Do not check thread tx here since there can be active system cache txs.\n                    fut = ((IgniteKernal)ctx.grid()).destroyCacheAsync(cacheName, false).chain(\n                        new CX1<IgniteInternalFuture<?>, GridRestResponse>() {\n                            @Override public GridRestResponse applyx(IgniteInternalFuture<?> f)\n                                throws IgniteCheckedException {\n                                return new GridRestResponse(f.get());\n                            }\n                        });\n\n                    break;\n                }\n\n                case GET_OR_CREATE_CACHE: {\n                    // Do not check thread tx here since there can be active system cache txs.\n                    fut = ((IgniteKernal)ctx.grid()).getOrCreateCacheAsync(cacheName, false).chain(\n                        new CX1<IgniteInternalFuture<?>, GridRestResponse>() {\n                            @Override public GridRestResponse applyx(IgniteInternalFuture<?> f)\n                                throws IgniteCheckedException {\n                                return new GridRestResponse(f.get());\n                            }\n                        });\n\n                    break;\n                }\n\n                case CACHE_METADATA: {\n                    fut = ctx.task().execute(MetadataTask.class, null);\n\n                    break;\n                }\n\n                case CACHE_CONTAINS_KEYS: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new ContainsKeysCommand(getKeys(req0)));\n\n                    break;\n                }\n\n                case CACHE_CONTAINS_KEY: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new ContainsKeyCommand(key));\n\n                    break;\n                }\n\n                case CACHE_GET: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetCommand(key));\n\n                    break;\n                }\n\n                case CACHE_GET_AND_PUT: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAndPutCommand(key, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_GET_AND_REPLACE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAndReplaceCommand(key, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_GET_AND_PUT_IF_ABSENT: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAndPutIfAbsentCommand(key, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_PUT_IF_ABSENT: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new PutIfAbsentCommand(key, ttl, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_GET_ALL: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAllCommand(getKeys(req0)));\n\n                    break;\n                }\n\n                case CACHE_PUT: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key, new\n                        PutCommand(key, ttl, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_ADD: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new AddCommand(key, ttl, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_PUT_ALL: {\n                    Map<Object, Object> map = req0.values();\n\n                    if (F.isEmpty(map))\n                        throw new IgniteCheckedException(GridRestCommandHandlerAdapter.missingParameter(\"values\"));\n\n                    for (Map.Entry<Object, Object> e : map.entrySet()) {\n                        if (e.getKey() == null)\n                            throw new IgniteCheckedException(\"Failing putAll operation (null keys are not allowed).\");\n\n                        if (e.getValue() == null)\n                            throw new IgniteCheckedException(\"Failing putAll operation (null values are not allowed).\");\n                    }\n\n                    // HashMap wrapping for correct serialization\n                    map = new HashMap<>(map);\n\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new PutAllCommand(map));\n\n                    break;\n                }\n\n                case CACHE_REMOVE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new RemoveCommand(key));\n\n                    break;\n                }\n\n                case CACHE_REMOVE_VALUE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new RemoveValueCommand(key, getValue(req0)));\n\n                    break;\n                }\n\n                case CACHE_REPLACE_VALUE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new ReplaceValueCommand(key, getValue(req0), req0.value2()));\n\n                    break;\n                }\n\n                case CACHE_GET_AND_REMOVE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new GetAndRemoveCommand(key));\n\n                    break;\n                }\n\n                case CACHE_REMOVE_ALL: {\n                    Map<Object, Object> map = req0.values();\n\n                    // HashSet wrapping for correct serialization\n                    Set<Object> keys = map == null ? null : new HashSet<>(map.keySet());\n\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new RemoveAllCommand(keys));\n\n                    break;\n                }\n\n                case CACHE_REPLACE: {\n                    final Object val = req0.value();\n\n                    if (val == null)\n                        throw new IgniteCheckedException(GridRestCommandHandlerAdapter.missingParameter(\"val\"));\n\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new ReplaceCommand(key, ttl, val));\n\n                    break;\n                }\n\n                case CACHE_CAS: {\n                    final Object val1 = req0.value();\n                    final Object val2 = req0.value2();\n\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new CasCommand(val2, val1, key));\n\n                    break;\n                }\n\n                case CACHE_APPEND: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new AppendCommand(key, req0));\n\n                    break;\n                }\n\n                case CACHE_PREPEND: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, skipStore, key,\n                        new PrependCommand(key, req0));\n\n                    break;\n                }\n\n                case CACHE_METRICS: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, key, new MetricsCommand());\n\n                    break;\n                }\n\n                case CACHE_SIZE: {\n                    fut = executeCommand(req.destinationId(), req.clientId(), cacheName, key, new SizeCommand());\n\n                    break;\n                }\n\n                default:\n                    throw new IllegalArgumentException(\"Invalid command for cache handler: \" + req);\n            }\n\n            return fut;\n        }\n        catch (IgniteException e) {\n            U.error(log, \"Failed to execute cache command: \" + req, e);\n\n            return new GridFinishedFuture<>(e);\n        }\n        catch (IgniteCheckedException e) {\n            U.error(log, \"Failed to execute cache command: \" + req, e);\n\n            return new GridFinishedFuture<>(e);\n        }\n        finally {\n            if (log.isDebugEnabled())\n                log.debug(\"Handled cache REST request: \" + req);\n        }\n    }"
        ],
        [
            "QueryCommandHandler::ExecuteQueryCallable::call()",
            " 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318 -\n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  ",
            "        /** {@inheritDoc} */\n        @SuppressWarnings(\"ThrowableResultOfMethodCallIgnored\")\n        @Override public GridRestResponse call() throws Exception {\n            final long qryId = qryIdGen.getAndIncrement();\n\n            try {\n                Query qry;\n\n                switch (req.queryType()) {\n                    case SQL:\n                        qry = new SqlQuery(req.typeName(), req.sqlQuery());\n\n                        ((SqlQuery)qry).setArgs(req.arguments());\n\n                        ((SqlQuery)qry).setDistributedJoins(req.distributedJoins());\n\n                        break;\n\n                    case SQL_FIELDS:\n                        qry = new SqlFieldsQuery(req.sqlQuery());\n\n                        ((SqlFieldsQuery)qry).setArgs(req.arguments());\n\n                        ((SqlFieldsQuery)qry).setDistributedJoins(req.distributedJoins());\n\n                        break;\n\n                    case SCAN:\n                        IgniteBiPredicate pred = null;\n\n                        if (req.className() != null)\n                            pred = instance(IgniteBiPredicate.class, req.className());\n\n                        qry = new ScanQuery(pred);\n\n                        break;\n\n                    default:\n                        throw new IgniteException(\"Incorrect query type [type=\" + req.queryType() + \"]\");\n                }\n\n                IgniteCache<Object, Object> cache = ctx.grid().cache(req.cacheName());\n\n                if (cache == null)\n                    return new GridRestResponse(GridRestResponse.STATUS_FAILED,\n                        \"Failed to find cache with name: \" + req.cacheName());\n\n                final QueryCursor qryCur = cache.query(qry);\n\n                Iterator cur = qryCur.iterator();\n\n                QueryCursorIterator qryCurIt = new QueryCursorIterator(qryCur, cur);\n\n                qryCurIt.lock();\n\n                try {\n                    qryCurs.put(qryId, qryCurIt);\n\n                    CacheQueryResult res = createQueryResult(cur, req, qryId, qryCurs);\n\n                    switch (req.queryType()) {\n                        case SQL:\n                        case SQL_FIELDS:\n                            List<GridQueryFieldMetadata> fieldsMeta = ((QueryCursorImpl)qryCur).fieldsMeta();\n\n                            res.setFieldsMetadata(convertMetadata(fieldsMeta));\n\n                            break;\n                        case SCAN:\n                            CacheQueryFieldsMetaResult keyField = new CacheQueryFieldsMetaResult();\n                            keyField.setFieldName(\"key\");\n\n                            CacheQueryFieldsMetaResult valField = new CacheQueryFieldsMetaResult();\n                            valField.setFieldName(\"value\");\n\n                            res.setFieldsMetadata(U.sealList(keyField, valField));\n\n                            break;\n                    }\n\n                    return new GridRestResponse(res);\n                }\n                finally {\n                    qryCurIt.unlock();\n                }\n            }\n            catch (Exception e) {\n                removeQueryCursor(qryId, qryCurs);\n\n                SQLException sqlErr = X.cause(e, SQLException.class);\n\n                return new GridRestResponse(GridRestResponse.STATUS_FAILED,\n                    sqlErr != null ? sqlErr.getMessage() : e.getMessage());\n            }\n        }",
            " 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318 +\n 319 +\n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  ",
            "        /** {@inheritDoc} */\n        @SuppressWarnings(\"ThrowableResultOfMethodCallIgnored\")\n        @Override public GridRestResponse call() throws Exception {\n            final long qryId = qryIdGen.getAndIncrement();\n\n            try {\n                Query qry;\n\n                switch (req.queryType()) {\n                    case SQL:\n                        qry = new SqlQuery(req.typeName(), req.sqlQuery());\n\n                        ((SqlQuery)qry).setArgs(req.arguments());\n\n                        ((SqlQuery)qry).setDistributedJoins(req.distributedJoins());\n\n                        break;\n\n                    case SQL_FIELDS:\n                        qry = new SqlFieldsQuery(req.sqlQuery());\n\n                        ((SqlFieldsQuery)qry).setArgs(req.arguments());\n\n                        ((SqlFieldsQuery)qry).setDistributedJoins(req.distributedJoins());\n\n                        break;\n\n                    case SCAN:\n                        IgniteBiPredicate pred = null;\n\n                        if (req.className() != null)\n                            pred = instance(IgniteBiPredicate.class, req.className());\n\n                        qry = new ScanQuery(pred);\n\n                        break;\n\n                    default:\n                        throw new IgniteException(\"Incorrect query type [type=\" + req.queryType() + \"]\");\n                }\n\n                IgniteCache<Object, Object> cache = ctx.grid().cache(\n                    req.cacheName() == null ? DFLT_CACHE_NAME : req.cacheName());\n\n                if (cache == null)\n                    return new GridRestResponse(GridRestResponse.STATUS_FAILED,\n                        \"Failed to find cache with name: \" + req.cacheName());\n\n                final QueryCursor qryCur = cache.query(qry);\n\n                Iterator cur = qryCur.iterator();\n\n                QueryCursorIterator qryCurIt = new QueryCursorIterator(qryCur, cur);\n\n                qryCurIt.lock();\n\n                try {\n                    qryCurs.put(qryId, qryCurIt);\n\n                    CacheQueryResult res = createQueryResult(cur, req, qryId, qryCurs);\n\n                    switch (req.queryType()) {\n                        case SQL:\n                        case SQL_FIELDS:\n                            List<GridQueryFieldMetadata> fieldsMeta = ((QueryCursorImpl)qryCur).fieldsMeta();\n\n                            res.setFieldsMetadata(convertMetadata(fieldsMeta));\n\n                            break;\n                        case SCAN:\n                            CacheQueryFieldsMetaResult keyField = new CacheQueryFieldsMetaResult();\n                            keyField.setFieldName(\"key\");\n\n                            CacheQueryFieldsMetaResult valField = new CacheQueryFieldsMetaResult();\n                            valField.setFieldName(\"value\");\n\n                            res.setFieldsMetadata(U.sealList(keyField, valField));\n\n                            break;\n                    }\n\n                    return new GridRestResponse(res);\n                }\n                finally {\n                    qryCurIt.unlock();\n                }\n            }\n            catch (Exception e) {\n                removeQueryCursor(qryId, qryCurs);\n\n                SQLException sqlErr = X.cause(e, SQLException.class);\n\n                return new GridRestResponse(GridRestResponse.STATUS_FAILED,\n                    sqlErr != null ? sqlErr.getMessage() : e.getMessage());\n            }\n        }"
        ]
    ],
    "79278e074e313e9ca23b5ccc97926e1f5d5cc031": [
        [
            "GridDhtPartitionsExchangeFuture::processSingleMessage(UUID,GridDhtPartitionsSingleMessage)",
            "2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  ",
            "    /**\n     * Note this method performs heavy updatePartitionSingleMap operation, this operation is moved out from the\n     * synchronized block. Only count of such updates {@link #pendingSingleUpdates} is managed under critical section.\n     *\n     * @param nodeId Sender node.\n     * @param msg Partition single message.\n     */\n    private void processSingleMessage(UUID nodeId, GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(nodeId, msg);\n\n            return;\n        }\n\n        boolean allReceived = false; // Received all expected messages.\n        boolean updateSingleMap = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            assert crd != null;\n\n            switch (state) {\n                case DONE: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Received single message, already done [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n\n                    assert finishState != null;\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD: {\n                    assert crd.isLocal() : crd;\n\n                    if (remaining.remove(nodeId)) {\n                        updateSingleMap = true;\n\n                        pendingSingleUpdates++;\n\n                        if (stateChangeExchange() && msg.getError() != null)\n                            changeGlobalStateExceptions.put(nodeId, msg.getError());\n\n                        allReceived = remaining.isEmpty();\n\n                        if (log.isInfoEnabled()) {\n                            log.info(\"Coordinator received single message [ver=\" + initialVersion() +\n                                \", node=\" + nodeId +\n                                \", allReceived=\" + allReceived + ']');\n                        }\n                    }\n\n                    break;\n                }\n\n                case SRV:\n                case BECOME_CRD: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Non-coordinator received single message [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + \", state=\" + state + ']');\n                    }\n\n                    pendingSingleMsgs.put(nodeId, msg);\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, nodeId);\n\n            return;\n        }\n\n        if (updateSingleMap) {\n            try {\n                // Do not update partition map, in case cluster transitioning to inactive state.\n                if (!deactivateCluster())\n                    updatePartitionSingleMap(nodeId, msg);\n            }\n            finally {\n                synchronized (mux) {\n                    assert pendingSingleUpdates > 0;\n\n                    pendingSingleUpdates--;\n\n                    if (pendingSingleUpdates == 0)\n                        mux.notifyAll();\n                }\n            }\n        }\n\n        if (allReceived) {\n            if (!awaitSingleMapUpdates())\n                return;\n\n            onAllReceived(null);\n        }\n    }",
            "2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219 +\n2220 +\n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  ",
            "    /**\n     * Note this method performs heavy updatePartitionSingleMap operation, this operation is moved out from the\n     * synchronized block. Only count of such updates {@link #pendingSingleUpdates} is managed under critical section.\n     *\n     * @param nodeId Sender node.\n     * @param msg Partition single message.\n     */\n    private void processSingleMessage(UUID nodeId, GridDhtPartitionsSingleMessage msg) {\n        if (msg.client()) {\n            waitAndReplyToNode(nodeId, msg);\n\n            return;\n        }\n\n        boolean allReceived = false; // Received all expected messages.\n        boolean updateSingleMap = false;\n\n        FinishState finishState0 = null;\n\n        synchronized (mux) {\n            assert crd != null;\n\n            switch (state) {\n                case DONE: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Received single message, already done [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + ']');\n                    }\n\n                    assert finishState != null;\n\n                    finishState0 = finishState;\n\n                    break;\n                }\n\n                case CRD: {\n                    assert crd.isLocal() : crd;\n\n                    if (remaining.remove(nodeId)) {\n                        updateSingleMap = true;\n\n                        pendingSingleUpdates++;\n\n                        if (stateChangeExchange() && msg.getError() != null)\n                            changeGlobalStateExceptions.put(nodeId, msg.getError());\n\n                        allReceived = remaining.isEmpty();\n\n                        if (log.isInfoEnabled()) {\n                            log.info(\"Coordinator received single message [ver=\" + initialVersion() +\n                                \", node=\" + nodeId +\n                                \", allReceived=\" + allReceived + ']');\n                        }\n                    }\n                    else if (log.isDebugEnabled())\n                        log.debug(\"Coordinator received single message it didn't expect to receive: \" + msg);\n\n                    break;\n                }\n\n                case SRV:\n                case BECOME_CRD: {\n                    if (log.isInfoEnabled()) {\n                        log.info(\"Non-coordinator received single message [ver=\" + initialVersion() +\n                            \", node=\" + nodeId + \", state=\" + state + ']');\n                    }\n\n                    pendingSingleMsgs.put(nodeId, msg);\n\n                    break;\n                }\n\n                default:\n                    assert false : state;\n            }\n        }\n\n        if (finishState0 != null) {\n            sendAllPartitionsToNode(finishState0, msg, nodeId);\n\n            return;\n        }\n\n        if (updateSingleMap) {\n            try {\n                // Do not update partition map, in case cluster transitioning to inactive state.\n                if (!deactivateCluster())\n                    updatePartitionSingleMap(nodeId, msg);\n            }\n            finally {\n                synchronized (mux) {\n                    assert pendingSingleUpdates > 0;\n\n                    pendingSingleUpdates--;\n\n                    if (pendingSingleUpdates == 0)\n                        mux.notifyAll();\n                }\n            }\n        }\n\n        if (allReceived) {\n            if (!awaitSingleMapUpdates())\n                return;\n\n            onAllReceived(null);\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::sendAllPartitionsToNode(FinishState,GridDhtPartitionsSingleMessage,UUID)",
            "2851  \n2852  \n2853  \n2854  \n2855  \n2856  \n2857  \n2858  \n2859  \n2860  \n2861  \n2862  \n2863  \n2864  \n2865  \n2866  \n2867  \n2868  \n2869  \n2870  \n2871  \n2872  \n2873  \n2874  \n2875  \n2876  \n2877  \n2878  \n2879  \n2880  \n2881  \n2882  \n2883  \n2884  \n2885  \n2886  \n2887  \n2888  \n2889  \n2890  \n2891  \n2892  \n2893  \n2894  ",
            "    /**\n     * @param finishState State.\n     * @param msg Request.\n     * @param nodeId Node ID.\n     */\n    private void sendAllPartitionsToNode(FinishState finishState, GridDhtPartitionsSingleMessage msg, UUID nodeId) {\n        ClusterNode node = cctx.node(nodeId);\n\n        if (node != null) {\n            GridDhtPartitionsFullMessage fullMsg = finishState.msg.copy();\n\n            Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n            if (affReq != null) {\n                Map<Integer, CacheGroupAffinityMessage> aff = CacheGroupAffinityMessage.createAffinityMessages(\n                    cctx,\n                    finishState.resTopVer,\n                    affReq,\n                    null);\n\n                fullMsg.joinedNodeAffinity(aff);\n            }\n\n            if (!fullMsg.exchangeId().equals(msg.exchangeId())) {\n                fullMsg = fullMsg.copy();\n\n                fullMsg.exchangeId(msg.exchangeId());\n            }\n\n            try {\n                cctx.io().send(node, fullMsg, SYSTEM_POOL);\n            }\n            catch (ClusterTopologyCheckedException e) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Failed to send partitions, node failed: \" + node);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to send partitions [node=\" + node + ']', e);\n            }\n        }\n        else if (log.isDebugEnabled())\n            log.debug(\"Failed to send partitions, node failed: \" + nodeId);\n\n    }",
            "2868  \n2869  \n2870  \n2871  \n2872  \n2873  \n2874  \n2875  \n2876  \n2877  \n2878  \n2879  \n2880  \n2881  \n2882  \n2883  \n2884  \n2885  \n2886  \n2887  \n2888  \n2889  \n2890  \n2891  \n2892  \n2893  \n2894  \n2895  \n2896  \n2897  \n2898  \n2899 +\n2900 +\n2901 +\n2902 +\n2903 +\n2904 +\n2905 +\n2906  \n2907  \n2908  \n2909  \n2910  \n2911  \n2912  \n2913  \n2914  \n2915  \n2916  \n2917  \n2918  ",
            "    /**\n     * @param finishState State.\n     * @param msg Request.\n     * @param nodeId Node ID.\n     */\n    private void sendAllPartitionsToNode(FinishState finishState, GridDhtPartitionsSingleMessage msg, UUID nodeId) {\n        ClusterNode node = cctx.node(nodeId);\n\n        if (node != null) {\n            GridDhtPartitionsFullMessage fullMsg = finishState.msg.copy();\n\n            Collection<Integer> affReq = msg.cacheGroupsAffinityRequest();\n\n            if (affReq != null) {\n                Map<Integer, CacheGroupAffinityMessage> aff = CacheGroupAffinityMessage.createAffinityMessages(\n                    cctx,\n                    finishState.resTopVer,\n                    affReq,\n                    null);\n\n                fullMsg.joinedNodeAffinity(aff);\n            }\n\n            if (!fullMsg.exchangeId().equals(msg.exchangeId())) {\n                fullMsg = fullMsg.copy();\n\n                fullMsg.exchangeId(msg.exchangeId());\n            }\n\n            try {\n                cctx.io().send(node, fullMsg, SYSTEM_POOL);\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Full message was sent to node: \" +\n                        node +\n                        \", fullMsg: \" + fullMsg\n                    );\n                }\n            }\n            catch (ClusterTopologyCheckedException e) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Failed to send partitions, node failed: \" + node);\n            }\n            catch (IgniteCheckedException e) {\n                U.error(log, \"Failed to send partitions [node=\" + node + ']', e);\n            }\n        }\n        else if (log.isDebugEnabled())\n            log.debug(\"Failed to send partitions, node failed: \" + nodeId);\n\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::waitAndReplyToNode(UUID,GridDhtPartitionsSingleMessage)",
            "2115  \n2116  \n2117  \n2118  \n2119  \n2120  \n2121  \n2122  \n2123  \n2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136 -\n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  ",
            "    /**\n     * @param nodeId Node ID.\n     * @param msg Client's message.\n     */\n    public void waitAndReplyToNode(final UUID nodeId, final GridDhtPartitionsSingleMessage msg) {\n        listen(new CI1<IgniteInternalFuture<AffinityTopologyVersion>>() {\n            @Override public void apply(IgniteInternalFuture<AffinityTopologyVersion> fut) {\n                if (cctx.kernalContext().isStopping())\n                    return;\n\n                FinishState finishState0;\n\n                synchronized (mux) {\n                    finishState0 = finishState;\n                }\n\n                if (finishState0 == null) {\n                    assert firstDiscoEvt.type() == EVT_NODE_JOINED && CU.clientNode(firstDiscoEvt.eventNode()) : this;\n\n                    ClusterNode node = cctx.node(nodeId);\n\n                    if (node == null)\n                        return;\n\n                    finishState0 = new FinishState(cctx.localNodeId(),\n                        initialVersion(),\n                        createPartitionsMessage(true, node.version().compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0));\n                }\n\n                sendAllPartitionsToNode(finishState0, msg, nodeId);\n            }\n        });\n    }",
            "2118  \n2119  \n2120  \n2121  \n2122  \n2123 +\n2124 +\n2125 +\n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142 +\n2143 +\n2144 +\n2145 +\n2146 +\n2147 +\n2148 +\n2149 +\n2150 +\n2151  \n2152 +\n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  ",
            "    /**\n     * @param nodeId Node ID.\n     * @param msg Client's message.\n     */\n    public void waitAndReplyToNode(final UUID nodeId, final GridDhtPartitionsSingleMessage msg) {\n        if (log.isDebugEnabled())\n            log.debug(\"Single message will be handled on completion of exchange future: \" + this);\n\n        listen(new CI1<IgniteInternalFuture<AffinityTopologyVersion>>() {\n            @Override public void apply(IgniteInternalFuture<AffinityTopologyVersion> fut) {\n                if (cctx.kernalContext().isStopping())\n                    return;\n\n                FinishState finishState0;\n\n                synchronized (mux) {\n                    finishState0 = finishState;\n                }\n\n                if (finishState0 == null) {\n                    assert firstDiscoEvt.type() == EVT_NODE_JOINED && CU.clientNode(firstDiscoEvt.eventNode()) : this;\n\n                    ClusterNode node = cctx.node(nodeId);\n\n                    if (node == null) {\n                        if (log.isDebugEnabled()) {\n                            log.debug(\"No node found for nodeId: \" +\n                                nodeId +\n                                \", handling of single message will be stopped: \" +\n                                msg\n                            );\n                        }\n\n                        return;\n                    }\n\n                    finishState0 = new FinishState(cctx.localNodeId(),\n                        initialVersion(),\n                        createPartitionsMessage(true, node.version().compareToIgnoreTimestamp(PARTIAL_COUNTERS_MAP_SINCE) >= 0));\n                }\n\n                sendAllPartitionsToNode(finishState0, msg, nodeId);\n            }\n        });\n    }"
        ],
        [
            "GridCachePartitionExchangeManager::processSinglePartitionUpdate(ClusterNode,GridDhtPartitionsSingleMessage)",
            "1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555 -\n1556 -\n1557  \n1558  \n1559  \n1560  \n1561  ",
            "    /**\n     * @param node Sender cluster node.\n     * @param msg Message.\n     */\n    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                        msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    if (grp != null &&\n                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                        continue;\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null) {\n                        updated |= top.update(null, entry.getValue(), false);\n\n                        cctx.affinity().checkRebalanceState(top, grpId);\n                    }\n                }\n\n                if (updated)\n                    scheduleResendPartitions();\n            }\n            else\n                exchangeFuture(msg.exchangeId(), null, null, null, null).onReceiveSingleMessage(node, msg);\n        }\n        finally {\n            leaveBusy();\n        }\n    }",
            "1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555 +\n1556 +\n1557 +\n1558 +\n1559 +\n1560 +\n1561 +\n1562 +\n1563  \n1564  \n1565  \n1566  \n1567  ",
            "    /**\n     * @param node Sender cluster node.\n     * @param msg Message.\n     */\n    private void processSinglePartitionUpdate(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        if (!enterBusy())\n            return;\n\n        try {\n            if (msg.exchangeId() == null) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Received local partition update [nodeId=\" + node.id() + \", parts=\" +\n                        msg + ']');\n\n                boolean updated = false;\n\n                for (Map.Entry<Integer, GridDhtPartitionMap> entry : msg.partitions().entrySet()) {\n                    Integer grpId = entry.getKey();\n\n                    CacheGroupContext grp = cctx.cache().cacheGroup(grpId);\n\n                    if (grp != null &&\n                        grp.localStartVersion().compareTo(entry.getValue().topologyVersion()) > 0)\n                        continue;\n\n                    GridDhtPartitionTopology top = null;\n\n                    if (grp == null)\n                        top = clientTops.get(grpId);\n                    else if (!grp.isLocal())\n                        top = grp.topology();\n\n                    if (top != null) {\n                        updated |= top.update(null, entry.getValue(), false);\n\n                        cctx.affinity().checkRebalanceState(top, grpId);\n                    }\n                }\n\n                if (updated)\n                    scheduleResendPartitions();\n            }\n            else {\n                GridDhtPartitionsExchangeFuture exchFut = exchangeFuture(msg.exchangeId(), null, null, null, null);\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Notifying exchange future about single message: \" + exchFut);\n\n                exchFut.onReceiveSingleMessage(node, msg);\n            }\n        }\n        finally {\n            leaveBusy();\n        }\n    }"
        ],
        [
            "GridDhtPartitionsExchangeFuture::onReceiveSingleMessage(ClusterNode,GridDhtPartitionsSingleMessage)",
            "2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  ",
            "    /**\n     * Processing of received single message. Actual processing in future may be delayed if init method was not\n     * completed, see {@link #initDone()}\n     *\n     * @param node Sender node.\n     * @param msg Single partition info.\n     */\n    public void onReceiveSingleMessage(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        assert !node.isDaemon() : node;\n        assert msg != null;\n        assert exchId.equals(msg.exchangeId()) : msg;\n        assert !cctx.kernalContext().clientNode();\n\n        if (msg.restoreState()) {\n            InitNewCoordinatorFuture newCrdFut0;\n\n            synchronized (mux) {\n                assert newCrdFut != null;\n\n                newCrdFut0 = newCrdFut;\n            }\n\n            newCrdFut0.onMessage(node, msg);\n\n            return;\n        }\n\n        if (!msg.client()) {\n            assert msg.lastVersion() != null : msg;\n\n            updateLastVersion(msg.lastVersion());\n        }\n\n        GridDhtPartitionsExchangeFuture mergedWith0 = null;\n\n        synchronized (mux) {\n            if (state == ExchangeLocalState.MERGED) {\n                assert mergedWith != null;\n\n                mergedWith0 = mergedWith;\n            }\n            else {\n                assert state != ExchangeLocalState.CLIENT;\n\n                if (exchangeId().isJoined() && node.id().equals(exchId.nodeId()))\n                    pendingJoinMsg = msg;\n            }\n        }\n\n        if (mergedWith0 != null) {\n            mergedWith0.processMergedMessage(node, msg);\n\n            return;\n        }\n\n        initFut.listen(new CI1<IgniteInternalFuture<Boolean>>() {\n            @Override public void apply(IgniteInternalFuture<Boolean> f) {\n                try {\n                    if (!f.get())\n                        return;\n                }\n                catch (IgniteCheckedException e) {\n                    U.error(log, \"Failed to initialize exchange future: \" + this, e);\n\n                    return;\n                }\n\n                processSingleMessage(node.id(), msg);\n            }\n        });\n    }",
            "2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095 +\n2096 +\n2097 +\n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  \n2113  \n2114  \n2115  \n2116  ",
            "    /**\n     * Processing of received single message. Actual processing in future may be delayed if init method was not\n     * completed, see {@link #initDone()}\n     *\n     * @param node Sender node.\n     * @param msg Single partition info.\n     */\n    public void onReceiveSingleMessage(final ClusterNode node, final GridDhtPartitionsSingleMessage msg) {\n        assert !node.isDaemon() : node;\n        assert msg != null;\n        assert exchId.equals(msg.exchangeId()) : msg;\n        assert !cctx.kernalContext().clientNode();\n\n        if (msg.restoreState()) {\n            InitNewCoordinatorFuture newCrdFut0;\n\n            synchronized (mux) {\n                assert newCrdFut != null;\n\n                newCrdFut0 = newCrdFut;\n            }\n\n            newCrdFut0.onMessage(node, msg);\n\n            return;\n        }\n\n        if (!msg.client()) {\n            assert msg.lastVersion() != null : msg;\n\n            updateLastVersion(msg.lastVersion());\n        }\n\n        GridDhtPartitionsExchangeFuture mergedWith0 = null;\n\n        synchronized (mux) {\n            if (state == ExchangeLocalState.MERGED) {\n                assert mergedWith != null;\n\n                mergedWith0 = mergedWith;\n            }\n            else {\n                assert state != ExchangeLocalState.CLIENT;\n\n                if (exchangeId().isJoined() && node.id().equals(exchId.nodeId()))\n                    pendingJoinMsg = msg;\n            }\n        }\n\n        if (mergedWith0 != null) {\n            mergedWith0.processMergedMessage(node, msg);\n\n            if (log.isDebugEnabled())\n                log.debug(\"Merged message processed, message handling finished: \" + msg);\n\n            return;\n        }\n\n        initFut.listen(new CI1<IgniteInternalFuture<Boolean>>() {\n            @Override public void apply(IgniteInternalFuture<Boolean> f) {\n                try {\n                    if (!f.get())\n                        return;\n                }\n                catch (IgniteCheckedException e) {\n                    U.error(log, \"Failed to initialize exchange future: \" + this, e);\n\n                    return;\n                }\n\n                processSingleMessage(node.id(), msg);\n            }\n        });\n    }"
        ]
    ],
    "3a33095ade910794f8385e43a651b0d922293c3b": [
        [
            "IgniteKernal::checkPhysicalRam()",
            "1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411 -\n1412 -\n1413 -\n1414  \n1415  \n1416  ",
            "    /**\n     * Checks whether physical RAM is not exceeded.\n     */\n    @SuppressWarnings(\"ConstantConditions\")\n    private void checkPhysicalRam() {\n        long ram = ctx.discovery().localNode().attribute(ATTR_PHY_RAM);\n\n        if (ram != -1) {\n            String macs = ctx.discovery().localNode().attribute(ATTR_MACS);\n\n            long totalHeap = 0;\n            long totalOffheap = 0;\n\n            for (ClusterNode node : ctx.discovery().allNodes()) {\n                if (macs.equals(node.attribute(ATTR_MACS))) {\n                    long heap = node.metrics().getHeapMemoryMaximum();\n                    Long offheap = node.<Long>attribute(ATTR_OFFHEAP_SIZE);\n\n                    if (heap != -1)\n                        totalHeap += heap;\n\n                    if (offheap != null)\n                        totalOffheap += offheap;\n                }\n            }\n\n            long total = totalHeap + totalOffheap;\n\n            if (total < 0)\n                total = Long.MAX_VALUE;\n\n            // 4GB or 20% of available memory is expected to be used by OS and user applications\n            long safeToUse = ram - Math.max(4L << 30, (long)(ram * 0.2));\n\n            if (total > safeToUse) {\n                U.quietAndWarn(log, \"Attempting to start more nodes than physical RAM available on the host \" +\n                    \"what can lead to significant slowdown (please decrease JVM heap size, memory policy size or \" +\n                    \"checkpoint buffer size) [required=\" + (total >> 20) + \"MB, available=\" + (ram >> 20) + \"MB]\");\n            }\n        }\n    }",
            "1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411 +\n1412 +\n1413 +\n1414 +\n1415  \n1416  \n1417  ",
            "    /**\n     * Checks whether physical RAM is not exceeded.\n     */\n    @SuppressWarnings(\"ConstantConditions\")\n    private void checkPhysicalRam() {\n        long ram = ctx.discovery().localNode().attribute(ATTR_PHY_RAM);\n\n        if (ram != -1) {\n            String macs = ctx.discovery().localNode().attribute(ATTR_MACS);\n\n            long totalHeap = 0;\n            long totalOffheap = 0;\n\n            for (ClusterNode node : ctx.discovery().allNodes()) {\n                if (macs.equals(node.attribute(ATTR_MACS))) {\n                    long heap = node.metrics().getHeapMemoryMaximum();\n                    Long offheap = node.<Long>attribute(ATTR_OFFHEAP_SIZE);\n\n                    if (heap != -1)\n                        totalHeap += heap;\n\n                    if (offheap != null)\n                        totalOffheap += offheap;\n                }\n            }\n\n            long total = totalHeap + totalOffheap;\n\n            if (total < 0)\n                total = Long.MAX_VALUE;\n\n            // 4GB or 20% of available memory is expected to be used by OS and user applications\n            long safeToUse = ram - Math.max(4L << 30, (long)(ram * 0.2));\n\n            if (total > safeToUse) {\n                U.quietAndWarn(log, \"Nodes started on local machine require more than 80% of physical RAM what can \" +\n                    \"lead to significant slowdown due to swapping (please decrease JVM heap size, memory policy \" +\n                    \"size or checkpoint buffer size) [required=\" + (total >> 20) + \"MB, available=\" +\n                    (ram >> 20) + \"MB]\");\n            }\n        }\n    }"
        ]
    ],
    "9656929a4c713afe4d03495677e1bfae94a8e343": [
        [
            "IgniteKernal::checkPhysicalRam()",
            "1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438 -\n1439  \n1440  \n1441  \n1442  \n1443  \n1444  ",
            "    /**\n     * Checks whether physical RAM is not exceeded.\n     */\n    @SuppressWarnings(\"ConstantConditions\")\n    private void checkPhysicalRam() {\n        long ram = ctx.discovery().localNode().attribute(ATTR_PHY_RAM);\n\n        if (ram != -1) {\n            String macs = ctx.discovery().localNode().attribute(ATTR_MACS);\n\n            long totalHeap = 0;\n            long totalOffheap = 0;\n\n            for (ClusterNode node : ctx.discovery().allNodes()) {\n                if (macs.equals(node.attribute(ATTR_MACS))) {\n                    long heap = node.metrics().getHeapMemoryMaximum();\n                    Long offheap = node.<Long>attribute(ATTR_OFFHEAP_SIZE);\n\n                    if (heap != -1)\n                        totalHeap += heap;\n\n                    if (offheap != null)\n                        totalOffheap += offheap;\n                }\n            }\n\n            long total = totalHeap + totalOffheap;\n\n            if (total < 0)\n                total = Long.MAX_VALUE;\n\n            // 4GB or 20% of available memory is expected to be used by OS and user applications\n            long safeToUse = ram - Math.max(4L << 30, (long)(ram * 0.2));\n\n            if (total > safeToUse) {\n                U.quietAndWarn(log, \"Nodes started on local machine require more than 80% of physical RAM what can \" +\n                    \"lead to significant slowdown due to swapping (please decrease JVM heap size, data region \" +\n                    \"size or checkpoint buffer size) [required=\" + (total >> 20) + \"MB, available=\" +\n                    (ram >> 20) + \"MB]\");\n            }\n        }\n    }",
            "1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438 +\n1439  \n1440  \n1441  \n1442  \n1443  \n1444  ",
            "    /**\n     * Checks whether physical RAM is not exceeded.\n     */\n    @SuppressWarnings(\"ConstantConditions\")\n    private void checkPhysicalRam() {\n        long ram = ctx.discovery().localNode().attribute(ATTR_PHY_RAM);\n\n        if (ram != -1) {\n            String macs = ctx.discovery().localNode().attribute(ATTR_MACS);\n\n            long totalHeap = 0;\n            long totalOffheap = 0;\n\n            for (ClusterNode node : ctx.discovery().allNodes()) {\n                if (macs.equals(node.attribute(ATTR_MACS))) {\n                    long heap = node.metrics().getHeapMemoryMaximum();\n                    Long offheap = node.<Long>attribute(ATTR_OFFHEAP_SIZE);\n\n                    if (heap != -1)\n                        totalHeap += heap;\n\n                    if (offheap != null)\n                        totalOffheap += offheap;\n                }\n            }\n\n            long total = totalHeap + totalOffheap;\n\n            if (total < 0)\n                total = Long.MAX_VALUE;\n\n            // 4GB or 20% of available memory is expected to be used by OS and user applications\n            long safeToUse = ram - Math.max(4L << 30, (long)(ram * 0.2));\n\n            if (total > safeToUse) {\n                U.quietAndWarn(log, \"Nodes started on local machine require more than 20% of physical RAM what can \" +\n                    \"lead to significant slowdown due to swapping (please decrease JVM heap size, data region \" +\n                    \"size or checkpoint buffer size) [required=\" + (total >> 20) + \"MB, available=\" +\n                    (ram >> 20) + \"MB]\");\n            }\n        }\n    }"
        ]
    ],
    "f8e29b73253c5f78435f9e7c173f81694e7d3dbc": [
        [
            "IgniteCachePartitionLossPolicySelfTest::checkLostPartition(boolean,boolean,TopologyChanger)",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 -\n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  ",
            "    /**\n     * @param canWrite {@code True} if writes are allowed.\n     * @param safe {@code True} if lost partition should trigger exception.\n     * @param topChanger topology changer.\n     * @throws Exception if failed.\n     */\n    private void checkLostPartition(boolean canWrite, boolean safe, TopologyChanger topChanger) throws Exception {\n        assert partLossPlc != null;\n\n        int part = topChanger.changeTopology();\n\n        // Wait for all grids (servers and client) have same topology version\n        // to make sure that all nodes received map with lost partition.\n        GridTestUtils.waitForCondition(() -> {\n            AffinityTopologyVersion last = null;\n            for (Ignite ig : G.allGrids()) {\n                AffinityTopologyVersion ver = ((IgniteEx) ig).context().cache().context().exchange().readyAffinityVersion();\n\n                if (last != null && !last.equals(ver))\n                    return false;\n\n                last = ver;\n            }\n\n            return true;\n        }, 10000);\n\n        for (Ignite ig : G.allGrids()) {\n            info(\"Checking node: \" + ig.cluster().localNode().id());\n\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            verifyCacheOps(canWrite, safe, part, ig);\n\n            // Check we can read and write to lost partition in recovery mode.\n            IgniteCache<Integer, Integer> recoverCache = cache.withPartitionRecover();\n\n            for (int lostPart : recoverCache.lostPartitions()) {\n                recoverCache.get(lostPart);\n                recoverCache.put(lostPart, lostPart);\n            }\n\n            // Check that writing in recover mode does not clear partition state.\n            verifyCacheOps(canWrite, safe, part, ig);\n        }\n\n        // Check that partition state does not change after we start a new node.\n        IgniteEx grd = startGrid(3);\n\n        info(\"Newly started node: \" + grd.cluster().localNode().id());\n\n        for (Ignite ig : G.allGrids())\n            verifyCacheOps(canWrite, safe, part, ig);\n\n        ignite(4).resetLostPartitions(Collections.singletonList(CACHE_NAME));\n\n        awaitPartitionMapExchange(true, true, null);\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            assertTrue(cache.lostPartitions().isEmpty());\n\n            int parts = ig.affinity(CACHE_NAME).partitions();\n\n            for (int i = 0; i < parts; i++) {\n                cache.get(i);\n\n                cache.put(i, i);\n            }\n        }\n    }",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262 +\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  ",
            "    /**\n     * @param canWrite {@code True} if writes are allowed.\n     * @param safe {@code True} if lost partition should trigger exception.\n     * @param topChanger topology changer.\n     * @throws Exception if failed.\n     */\n    private void checkLostPartition(boolean canWrite, boolean safe, TopologyChanger topChanger) throws Exception {\n        assert partLossPlc != null;\n\n        int part = topChanger.changeTopology().get(0);\n\n        // Wait for all grids (servers and client) have same topology version\n        // to make sure that all nodes received map with lost partition.\n        GridTestUtils.waitForCondition(() -> {\n            AffinityTopologyVersion last = null;\n            for (Ignite ig : G.allGrids()) {\n                AffinityTopologyVersion ver = ((IgniteEx) ig).context().cache().context().exchange().readyAffinityVersion();\n\n                if (last != null && !last.equals(ver))\n                    return false;\n\n                last = ver;\n            }\n\n            return true;\n        }, 10000);\n\n        for (Ignite ig : G.allGrids()) {\n            info(\"Checking node: \" + ig.cluster().localNode().id());\n\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            verifyCacheOps(canWrite, safe, part, ig);\n\n            // Check we can read and write to lost partition in recovery mode.\n            IgniteCache<Integer, Integer> recoverCache = cache.withPartitionRecover();\n\n            for (int lostPart : recoverCache.lostPartitions()) {\n                recoverCache.get(lostPart);\n                recoverCache.put(lostPart, lostPart);\n            }\n\n            // Check that writing in recover mode does not clear partition state.\n            verifyCacheOps(canWrite, safe, part, ig);\n        }\n\n        // Check that partition state does not change after we start a new node.\n        IgniteEx grd = startGrid(3);\n\n        info(\"Newly started node: \" + grd.cluster().localNode().id());\n\n        for (Ignite ig : G.allGrids())\n            verifyCacheOps(canWrite, safe, part, ig);\n\n        ignite(4).resetLostPartitions(Collections.singletonList(CACHE_NAME));\n\n        awaitPartitionMapExchange(true, true, null);\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            assertTrue(cache.lostPartitions().isEmpty());\n\n            int parts = ig.affinity(CACHE_NAME).partitions();\n\n            for (int i = 0; i < parts; i++) {\n                cache.get(i);\n\n                cache.put(i, i);\n            }\n        }\n    }"
        ],
        [
            "IgniteCachePartitionLossPolicySelfTest::TopologyChanger::changeTopology()",
            " 439  \n 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462 -\n 463  \n 464 -\n 465  \n 466  \n 467 -\n 468  \n 469  \n 470 -\n 471 -\n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479 -\n 480 -\n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507 -\n 508 -\n 509  \n 510 -\n 511 -\n 512  \n 513 -\n 514  ",
            "        /**\n         * @return Lost partition ID.\n         * @throws Exception If failed.\n         */\n        protected int changeTopology() throws Exception {\n            startGrids(4);\n\n            Affinity<Object> aff = ignite(0).affinity(CACHE_NAME);\n\n            for (int i = 0; i < aff.partitions(); i++)\n                ignite(0).cache(CACHE_NAME).put(i, i);\n\n            client = true;\n\n            startGrid(4);\n\n            client = false;\n\n            for (int i = 0; i < 5; i++)\n                info(\">>> Node [idx=\" + i + \", nodeId=\" + ignite(i).cluster().localNode().id() + ']');\n\n            awaitPartitionMapExchange();\n\n            final Integer part = noPrimaryOrBackupPartition(aliveNodes);\n\n            if (part == null)\n                throw new IllegalStateException(\"No partition on nodes: \" + killNodes);\n\n            final List<Semaphore> partLost = new ArrayList<>();\n\n            for (int i : aliveNodes) {\n                final Semaphore sem = new Semaphore(0);\n                partLost.add(sem);\n\n                grid(i).events().localListen(new P1<Event>() {\n                    @Override public boolean apply(Event evt) {\n                        assert evt.type() == EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST;\n\n                        CacheRebalancingEvent cacheEvt = (CacheRebalancingEvent)evt;\n\n                        if (cacheEvt.partition() == part && F.eq(CACHE_NAME, cacheEvt.cacheName()))\n                            sem.release();\n\n                        return true;\n                    }\n                }, EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST);\n\n            }\n\n            if (delayExchange)\n                delayPartExchange.set(true);\n\n            ExecutorService executor = Executors.newFixedThreadPool(killNodes.size());\n\n            for (Integer node : killNodes) {\n                executor.submit(new Runnable() {\n                    @Override public void run() {\n                        grid(node).close();\n                    }\n                });\n\n                Thread.sleep(stopDelay);\n            }\n\n            executor.shutdown();\n\n            delayPartExchange.set(false);\n\n            for (Semaphore sem : partLost)\n                assertTrue(\"Failed to wait for partition LOST event\", sem.tryAcquire(1, 10L, TimeUnit.SECONDS));\n\n            for (Semaphore sem : partLost)\n                assertFalse(\"Partition LOST event raised twice\", sem.tryAcquire(1, 1L, TimeUnit.SECONDS));\n\n            return part;\n        }",
            " 441  \n 442  \n 443  \n 444  \n 445 +\n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464 +\n 465  \n 466 +\n 467  \n 468  \n 469 +\n 470  \n 471  \n 472 +\n 473 +\n 474 +\n 475 +\n 476 +\n 477 +\n 478 +\n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486 +\n 487 +\n 488 +\n 489 +\n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516 +\n 517 +\n 518 +\n 519 +\n 520 +\n 521 +\n 522  \n 523 +\n 524 +\n 525 +\n 526 +\n 527  \n 528 +\n 529  ",
            "        /**\n         * @return Lost partition ID.\n         * @throws Exception If failed.\n         */\n        protected List<Integer> changeTopology() throws Exception {\n            startGrids(4);\n\n            Affinity<Object> aff = ignite(0).affinity(CACHE_NAME);\n\n            for (int i = 0; i < aff.partitions(); i++)\n                ignite(0).cache(CACHE_NAME).put(i, i);\n\n            client = true;\n\n            startGrid(4);\n\n            client = false;\n\n            for (int i = 0; i < 5; i++)\n                info(\">>> Node [idx=\" + i + \", nodeId=\" + ignite(i).cluster().localNode().id() + ']');\n\n            awaitPartitionMapExchange();\n\n            final List<Integer> parts = noPrimaryOrBackupPartition(aliveNodes);\n\n            if (parts.size() == 0)\n                throw new IllegalStateException(\"No partition on nodes: \" + killNodes);\n\n            final List<Map<Integer, Semaphore>> lostMap = new ArrayList<>();\n\n            for (int i : aliveNodes) {\n                HashMap<Integer, Semaphore> semaphoreMap = new HashMap<>();\n\n                for (Integer part : parts)\n                    semaphoreMap.put(part, new Semaphore(0));\n\n                lostMap.add(semaphoreMap);\n\n\n                grid(i).events().localListen(new P1<Event>() {\n                    @Override public boolean apply(Event evt) {\n                        assert evt.type() == EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST;\n\n                        CacheRebalancingEvent cacheEvt = (CacheRebalancingEvent)evt;\n\n                        if (F.eq(CACHE_NAME, cacheEvt.cacheName())) {\n                            if (semaphoreMap.containsKey(cacheEvt.partition()))\n                                semaphoreMap.get(cacheEvt.partition()).release();\n                        }\n\n                        return true;\n                    }\n                }, EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST);\n\n            }\n\n            if (delayExchange)\n                delayPartExchange.set(true);\n\n            ExecutorService executor = Executors.newFixedThreadPool(killNodes.size());\n\n            for (Integer node : killNodes) {\n                executor.submit(new Runnable() {\n                    @Override public void run() {\n                        grid(node).close();\n                    }\n                });\n\n                Thread.sleep(stopDelay);\n            }\n\n            executor.shutdown();\n\n            delayPartExchange.set(false);\n\n            Thread.sleep(5_000L);\n\n            for (Map<Integer, Semaphore> map : lostMap) {\n                for (Map.Entry<Integer, Semaphore> entry : map.entrySet())\n                    assertTrue(\"Failed to wait for partition LOST event for partition:\" + entry.getKey(), entry.getValue().tryAcquire(1));\n            }\n\n            for (Map<Integer, Semaphore> map : lostMap) {\n                for (Map.Entry<Integer, Semaphore> entry : map.entrySet())\n                    assertFalse(\"Partition LOST event raised twice for partition:\" + entry.getKey(), entry.getValue().tryAcquire(1));\n            }\n\n            return parts;\n        }"
        ],
        [
            "GridDhtPartitionTopologyImpl::update(AffinityTopologyVersion,GridDhtPartitionFullMap,CachePartitionFullCountersMap,Set,Map,AffinityTopologyVersion)",
            "1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432 -\n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable CachePartitionFullCountersMap incomeCntrMap,\n        Set<Integer> partsToReload,\n        @Nullable Map<Integer, Long> partSizes,\n        @Nullable AffinityTopologyVersion msgTopVer) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Updating full partition map [grp=\" + grp.cacheOrGroupName() + \", exchVer=\" + exchangeVer +\n                \", fullMap=\" + fullMapString() + ']');\n        }\n\n        assert partMap != null;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (log.isTraceEnabled() && exchangeVer != null) {\n                    log.trace(\"Partition states before full update [grp=\" + grp.cacheOrGroupName()\n                        + \", exchVer=\" + exchangeVer + \", states=\" + dumpPartitionStates() + ']');\n                }\n\n                if (stopping || !lastTopChangeVer.initialized() ||\n                    // Ignore message not-related to exchange if exchange is in progress.\n                    (exchangeVer == null && !lastTopChangeVer.equals(readyTopVer)))\n                    return false;\n\n                if (incomeCntrMap != null) {\n                    // update local counters in partitions\n                    for (int i = 0; i < locParts.length(); i++) {\n                        cntrMap.updateCounter(i, incomeCntrMap.updateCounter(i));\n\n                        GridDhtLocalPartition part = locParts.get(i);\n\n                        if (part == null)\n                            continue;\n\n                        if (part.state() == OWNING || part.state() == MOVING) {\n                            long updCntr = incomeCntrMap.updateCounter(part.id());\n\n                            if (updCntr != 0 && updCntr > part.updateCounter())\n                                part.updateCounter(updCntr);\n                        }\n                    }\n                }\n\n                if (exchangeVer != null) {\n                    // Ignore if exchange already finished or new exchange started.\n                    if (readyTopVer.compareTo(exchangeVer) > 0 || lastTopChangeVer.compareTo(exchangeVer) > 0) {\n                        U.warn(log, \"Stale exchange id for full partition map update (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exchVer=\" + exchangeVer + ']');\n\n                        return false;\n                    }\n                }\n\n                if (msgTopVer != null && lastTopChangeVer.compareTo(msgTopVer) > 0) {\n                    U.warn(log, \"Stale version for full partition map update message (will ignore) [\" +\n                        \"grp=\" + grp.cacheOrGroupName() +\n                        \", lastTopChange=\" + lastTopChangeVer +\n                        \", readTopVer=\" + readyTopVer +\n                        \", msgVer=\" + msgTopVer + ']');\n\n                    return false;\n                }\n\n                boolean fullMapUpdated = (node2part == null);\n\n                if (node2part != null) {\n                    for (GridDhtPartitionMap part : node2part.values()) {\n                        GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                        if (shouldOverridePartitionMap(part, newPart)) {\n                            fullMapUpdated = true;\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Overriding partition map in full update map [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() +\n                                    \", exchVer=\" + exchangeVer +\n                                    \", curPart=\" + mapString(part) +\n                                    \", newPart=\" + mapString(newPart) + ']');\n                            }\n\n                            if (newPart.nodeId().equals(ctx.localNodeId()))\n                                updateSeq.setIfGreater(newPart.updateSequence());\n                        }\n                        else {\n                            // If for some nodes current partition has a newer map,\n                            // then we keep the newer value.\n                            partMap.put(part.nodeId(), part);\n                        }\n                    }\n\n                    // Check that we have new nodes.\n                    for (GridDhtPartitionMap part : partMap.values()) {\n                        if (fullMapUpdated)\n                            break;\n\n                        fullMapUpdated = !node2part.containsKey(part.nodeId());\n                    }\n\n                    // Remove entry if node left.\n                    for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                        UUID nodeId = it.next();\n\n                        if (!ctx.discovery().alive(nodeId)) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Removing left node from full map update [grp=\" + grp.cacheOrGroupName() +\n                                    \", nodeId=\" + nodeId + \", partMap=\" + partMap + ']');\n\n                            leftNode2Part.put(nodeId, partMap.get(nodeId));\n\n                            it.remove();\n                        }\n                    }\n                }\n                else {\n                    GridDhtPartitionMap locNodeMap = partMap.get(ctx.localNodeId());\n\n                    if (locNodeMap != null)\n                        updateSeq.setIfGreater(locNodeMap.updateSequence());\n                }\n\n                if (!fullMapUpdated) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"No updates for full partition map (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastExch=\" + lastTopChangeVer +\n                            \", exchVer=\" + exchangeVer +\n                            \", curMap=\" + node2part +\n                            \", newMap=\" + partMap + ']');\n                    }\n\n                    return false;\n                }\n\n                if (exchangeVer != null) {\n                    assert exchangeVer.compareTo(readyTopVer) >= 0 && exchangeVer.compareTo(lastTopChangeVer) >= 0;\n\n                    lastTopChangeVer = readyTopVer = exchangeVer;\n                }\n\n                node2part = partMap;\n\n                if (exchangeVer == null && !grp.isReplicated() &&\n                        (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0)) {\n                    AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) {\n                            int p = e0.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e0.getValue() == MOVING || e0.getValue() == OWNING || e0.getValue() == RENTING) &&\n                                !affAssignment.getIds(p).contains(e.getKey())) {\n\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                diffIds.add(e.getKey());\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(e.getKey())) {\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n                    }\n\n                    diffFromAffinityVer = readyTopVer;\n                }\n\n                boolean changed = false;\n\n                GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n                // Only in real exchange occurred.\n                if (exchangeVer != null &&\n                    nodeMap != null &&\n                    grp.persistenceEnabled() &&\n                    readyTopVer.initialized()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                        int p = e.getKey();\n                        GridDhtPartitionState state = e.getValue();\n\n                        if (state == OWNING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            assert locPart != null : grp.cacheOrGroupName();\n\n                            if (locPart.state() == MOVING) {\n                                boolean success = locPart.own();\n\n                                assert success : locPart;\n\n                                changed |= success;\n                            }\n                        }\n                        else if (state == MOVING) {\n                            boolean haveHistory = !partsToReload.contains(p);\n\n                            rebalancePartition(p, haveHistory);\n\n                            changed = true;\n                        }\n                    }\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchangeVer == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.topologyVersion(), aff.assignment());\n                }\n\n                if (partSizes != null)\n                    this.globalPartSizes = partSizes;\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Partition map after full update [grp=\" + grp.cacheOrGroupName() +\n                        \", map=\" + fullMapString() + ']');\n                }\n\n                if (log.isTraceEnabled() && exchangeVer != null) {\n                    log.trace(\"Partition states after full update [grp=\" + grp.cacheOrGroupName()\n                        + \", exchVer=\" + exchangeVer + \", states=\" + dumpPartitionStates() + ']');\n                }\n\n                if (changed)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            } finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            "1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  \n1338  \n1339  \n1340  \n1341  \n1342  \n1343  \n1344  \n1345  \n1346  \n1347  \n1348  \n1349  \n1350  \n1351  \n1352  \n1353  \n1354  \n1355  \n1356  \n1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432 +\n1433 +\n1434 +\n1435 +\n1436 +\n1437 +\n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  ",
            "    /** {@inheritDoc} */\n    @SuppressWarnings({\"MismatchedQueryAndUpdateOfCollection\"})\n    @Override public boolean update(\n        @Nullable AffinityTopologyVersion exchangeVer,\n        GridDhtPartitionFullMap partMap,\n        @Nullable CachePartitionFullCountersMap incomeCntrMap,\n        Set<Integer> partsToReload,\n        @Nullable Map<Integer, Long> partSizes,\n        @Nullable AffinityTopologyVersion msgTopVer) {\n        if (log.isDebugEnabled()) {\n            log.debug(\"Updating full partition map [grp=\" + grp.cacheOrGroupName() + \", exchVer=\" + exchangeVer +\n                \", fullMap=\" + fullMapString() + ']');\n        }\n\n        assert partMap != null;\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (log.isTraceEnabled() && exchangeVer != null) {\n                    log.trace(\"Partition states before full update [grp=\" + grp.cacheOrGroupName()\n                        + \", exchVer=\" + exchangeVer + \", states=\" + dumpPartitionStates() + ']');\n                }\n\n                if (stopping || !lastTopChangeVer.initialized() ||\n                    // Ignore message not-related to exchange if exchange is in progress.\n                    (exchangeVer == null && !lastTopChangeVer.equals(readyTopVer)))\n                    return false;\n\n                if (incomeCntrMap != null) {\n                    // update local counters in partitions\n                    for (int i = 0; i < locParts.length(); i++) {\n                        cntrMap.updateCounter(i, incomeCntrMap.updateCounter(i));\n\n                        GridDhtLocalPartition part = locParts.get(i);\n\n                        if (part == null)\n                            continue;\n\n                        if (part.state() == OWNING || part.state() == MOVING) {\n                            long updCntr = incomeCntrMap.updateCounter(part.id());\n\n                            if (updCntr != 0 && updCntr > part.updateCounter())\n                                part.updateCounter(updCntr);\n                        }\n                    }\n                }\n\n                if (exchangeVer != null) {\n                    // Ignore if exchange already finished or new exchange started.\n                    if (readyTopVer.compareTo(exchangeVer) > 0 || lastTopChangeVer.compareTo(exchangeVer) > 0) {\n                        U.warn(log, \"Stale exchange id for full partition map update (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastTopChange=\" + lastTopChangeVer +\n                            \", readTopVer=\" + readyTopVer +\n                            \", exchVer=\" + exchangeVer + ']');\n\n                        return false;\n                    }\n                }\n\n                if (msgTopVer != null && lastTopChangeVer.compareTo(msgTopVer) > 0) {\n                    U.warn(log, \"Stale version for full partition map update message (will ignore) [\" +\n                        \"grp=\" + grp.cacheOrGroupName() +\n                        \", lastTopChange=\" + lastTopChangeVer +\n                        \", readTopVer=\" + readyTopVer +\n                        \", msgVer=\" + msgTopVer + ']');\n\n                    return false;\n                }\n\n                boolean fullMapUpdated = (node2part == null);\n\n                if (node2part != null) {\n                    for (GridDhtPartitionMap part : node2part.values()) {\n                        GridDhtPartitionMap newPart = partMap.get(part.nodeId());\n\n                        if (shouldOverridePartitionMap(part, newPart)) {\n                            fullMapUpdated = true;\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Overriding partition map in full update map [\" +\n                                    \"grp=\" + grp.cacheOrGroupName() +\n                                    \", exchVer=\" + exchangeVer +\n                                    \", curPart=\" + mapString(part) +\n                                    \", newPart=\" + mapString(newPart) + ']');\n                            }\n\n                            if (newPart.nodeId().equals(ctx.localNodeId()))\n                                updateSeq.setIfGreater(newPart.updateSequence());\n                        }\n                        else {\n                            // If for some nodes current partition has a newer map,\n                            // then we keep the newer value.\n                            partMap.put(part.nodeId(), part);\n                        }\n                    }\n\n                    // Check that we have new nodes.\n                    for (GridDhtPartitionMap part : partMap.values()) {\n                        if (fullMapUpdated)\n                            break;\n\n                        fullMapUpdated = !node2part.containsKey(part.nodeId());\n                    }\n\n                    // Remove entry if node left.\n                    for (Iterator<UUID> it = partMap.keySet().iterator(); it.hasNext(); ) {\n                        UUID nodeId = it.next();\n\n                        if (!ctx.discovery().alive(nodeId)) {\n                            if (log.isDebugEnabled())\n                                log.debug(\"Removing left node from full map update [grp=\" + grp.cacheOrGroupName() +\n                                    \", nodeId=\" + nodeId + \", partMap=\" + partMap + ']');\n\n                            if (node2part.containsKey(nodeId)) {\n                                GridDhtPartitionMap map = partMap.get(nodeId);\n\n                                if (map != null)\n                                    leftNode2Part.put(nodeId, map);\n                            }\n\n                            it.remove();\n                        }\n                    }\n                }\n                else {\n                    GridDhtPartitionMap locNodeMap = partMap.get(ctx.localNodeId());\n\n                    if (locNodeMap != null)\n                        updateSeq.setIfGreater(locNodeMap.updateSequence());\n                }\n\n                if (!fullMapUpdated) {\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"No updates for full partition map (will ignore) [\" +\n                            \"grp=\" + grp.cacheOrGroupName() +\n                            \", lastExch=\" + lastTopChangeVer +\n                            \", exchVer=\" + exchangeVer +\n                            \", curMap=\" + node2part +\n                            \", newMap=\" + partMap + ']');\n                    }\n\n                    return false;\n                }\n\n                if (exchangeVer != null) {\n                    assert exchangeVer.compareTo(readyTopVer) >= 0 && exchangeVer.compareTo(lastTopChangeVer) >= 0;\n\n                    lastTopChangeVer = readyTopVer = exchangeVer;\n                }\n\n                node2part = partMap;\n\n                if (exchangeVer == null && !grp.isReplicated() &&\n                        (readyTopVer.initialized() && readyTopVer.compareTo(diffFromAffinityVer) >= 0)) {\n                    AffinityAssignment affAssignment = grp.affinity().readyAffinity(readyTopVer);\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> e : partMap.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> e0 : e.getValue().entrySet()) {\n                            int p = e0.getKey();\n\n                            Set<UUID> diffIds = diffFromAffinity.get(p);\n\n                            if ((e0.getValue() == MOVING || e0.getValue() == OWNING || e0.getValue() == RENTING) &&\n                                !affAssignment.getIds(p).contains(e.getKey())) {\n\n                                if (diffIds == null)\n                                    diffFromAffinity.put(p, diffIds = U.newHashSet(3));\n\n                                diffIds.add(e.getKey());\n                            }\n                            else {\n                                if (diffIds != null && diffIds.remove(e.getKey())) {\n                                    if (diffIds.isEmpty())\n                                        diffFromAffinity.remove(p);\n                                }\n                            }\n                        }\n                    }\n\n                    diffFromAffinityVer = readyTopVer;\n                }\n\n                boolean changed = false;\n\n                GridDhtPartitionMap nodeMap = partMap.get(ctx.localNodeId());\n\n                // Only in real exchange occurred.\n                if (exchangeVer != null &&\n                    nodeMap != null &&\n                    grp.persistenceEnabled() &&\n                    readyTopVer.initialized()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : nodeMap.entrySet()) {\n                        int p = e.getKey();\n                        GridDhtPartitionState state = e.getValue();\n\n                        if (state == OWNING) {\n                            GridDhtLocalPartition locPart = locParts.get(p);\n\n                            assert locPart != null : grp.cacheOrGroupName();\n\n                            if (locPart.state() == MOVING) {\n                                boolean success = locPart.own();\n\n                                assert success : locPart;\n\n                                changed |= success;\n                            }\n                        }\n                        else if (state == MOVING) {\n                            boolean haveHistory = !partsToReload.contains(p);\n\n                            rebalancePartition(p, haveHistory);\n\n                            changed = true;\n                        }\n                    }\n                }\n\n                long updateSeq = this.updateSeq.incrementAndGet();\n\n                if (readyTopVer.initialized() && readyTopVer.equals(lastTopChangeVer)) {\n                    AffinityAssignment aff = grp.affinity().readyAffinity(readyTopVer);\n\n                    if (exchangeVer == null)\n                        changed |= checkEvictions(updateSeq, aff);\n\n                    updateRebalanceVersion(aff.topologyVersion(), aff.assignment());\n                }\n\n                if (partSizes != null)\n                    this.globalPartSizes = partSizes;\n\n                consistencyCheck();\n\n                if (log.isDebugEnabled()) {\n                    log.debug(\"Partition map after full update [grp=\" + grp.cacheOrGroupName() +\n                        \", map=\" + fullMapString() + ']');\n                }\n\n                if (log.isTraceEnabled() && exchangeVer != null) {\n                    log.trace(\"Partition states after full update [grp=\" + grp.cacheOrGroupName()\n                        + \", exchVer=\" + exchangeVer + \", states=\" + dumpPartitionStates() + ']');\n                }\n\n                if (changed)\n                    ctx.exchange().scheduleResendPartitions();\n\n                return changed;\n            } finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ],
        [
            "IgniteCachePartitionLossPolicySelfTest::noPrimaryOrBackupPartition(List)",
            " 383  \n 384  \n 385  \n 386  \n 387 -\n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404 -\n 405 -\n 406  \n 407  \n 408 -\n 409  ",
            "    /**\n     * @param nodes List of nodes to find partition.\n     * @return Partition id that isn't primary or backup for specified nodes.\n     */\n    protected Integer noPrimaryOrBackupPartition(List<Integer> nodes) {\n        Affinity<Object> aff = ignite(4).affinity(CACHE_NAME);\n\n        Integer part;\n\n        for (int i = 0; i < aff.partitions(); i++) {\n            part = i;\n\n            for (Integer id : nodes) {\n                if (aff.isPrimaryOrBackup(grid(id).cluster().localNode(), i)) {\n                    part = null;\n\n                    break;\n                }\n            }\n\n            if (part != null)\n                return part;\n\n        }\n\n        return null;\n    }",
            " 384  \n 385  \n 386  \n 387  \n 388 +\n 389  \n 390  \n 391 +\n 392 +\n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 +\n 408  \n 409  \n 410 +\n 411  ",
            "    /**\n     * @param nodes List of nodes to find partition.\n     * @return List of partitions that aren't primary or backup for specified nodes.\n     */\n    protected List<Integer> noPrimaryOrBackupPartition(List<Integer> nodes) {\n        Affinity<Object> aff = ignite(4).affinity(CACHE_NAME);\n\n        List<Integer> parts = new ArrayList<>();\n\n        Integer part;\n\n        for (int i = 0; i < aff.partitions(); i++) {\n            part = i;\n\n            for (Integer id : nodes) {\n                if (aff.isPrimaryOrBackup(grid(id).cluster().localNode(), i)) {\n                    part = null;\n\n                    break;\n                }\n            }\n\n            if (part != null)\n                parts.add(i);\n        }\n\n        return parts;\n    }"
        ]
    ],
    "a14a594d2168c7e5bda56bc0cdc2492c5ffc1f82": [
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::getConfiguration(String)",
            "  52  \n  53  \n  54  \n  55  \n  56 -\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration ccfg1 = cacheConfiguration(\"cache\");\n        ccfg1.setBackups(1);\n        ccfg1.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        CacheConfiguration ccfg2 = cacheConfiguration(\"indexed\");\n        ccfg2.setBackups(1);\n        ccfg2.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        QueryEntity qryEntity = new QueryEntity(Integer.class.getName(), TestValue.class.getName());\n\n        LinkedHashMap<String, String> fields = new LinkedHashMap<>();\n\n        fields.put(\"v1\", Integer.class.getName());\n        fields.put(\"v2\", Integer.class.getName());\n\n        qryEntity.setFields(fields);\n\n        QueryIndex qryIdx = new QueryIndex(\"v1\", true);\n\n        qryEntity.setIndexes(Collections.singleton(qryIdx));\n\n        ccfg2.setQueryEntities(Collections.singleton(qryEntity));\n\n        cfg.setCacheConfiguration(ccfg1, ccfg2);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setConcurrencyLevel(Runtime.getRuntime().availableProcessors() * 4);\n        dbCfg.setPageSize(1024);\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setSize(100 * 1024 * 1024);\n        memPlcCfg.setSwapFilePath(\"db\");\n\n        dbCfg.setMemoryPolicies(memPlcCfg);\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        return cfg;\n    }",
            "  56  \n  57  \n  58  \n  59  \n  60 +\n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "    /** {@inheritDoc} */\n    @Override protected IgniteConfiguration getConfiguration(String gridName) throws Exception {\n        IgniteConfiguration cfg = super.getConfiguration(gridName);\n\n        CacheConfiguration ccfg1 = cacheConfiguration(cacheName);\n        ccfg1.setBackups(1);\n        ccfg1.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        CacheConfiguration ccfg2 = cacheConfiguration(\"indexed\");\n        ccfg2.setBackups(1);\n        ccfg2.setWriteSynchronizationMode(CacheWriteSynchronizationMode.FULL_SYNC);\n\n        QueryEntity qryEntity = new QueryEntity(Integer.class.getName(), TestValue.class.getName());\n\n        LinkedHashMap<String, String> fields = new LinkedHashMap<>();\n\n        fields.put(\"v1\", Integer.class.getName());\n        fields.put(\"v2\", Integer.class.getName());\n\n        qryEntity.setFields(fields);\n\n        QueryIndex qryIdx = new QueryIndex(\"v1\", true);\n\n        qryEntity.setIndexes(Collections.singleton(qryIdx));\n\n        ccfg2.setQueryEntities(Collections.singleton(qryEntity));\n\n        cfg.setCacheConfiguration(ccfg1, ccfg2);\n\n        MemoryConfiguration dbCfg = new MemoryConfiguration();\n\n        dbCfg.setConcurrencyLevel(Runtime.getRuntime().availableProcessors() * 4);\n        dbCfg.setPageSize(1024);\n\n        MemoryPolicyConfiguration memPlcCfg = new MemoryPolicyConfiguration();\n\n        memPlcCfg.setName(\"dfltMemPlc\");\n        memPlcCfg.setSize(100 * 1024 * 1024);\n        memPlcCfg.setSwapFilePath(\"db\");\n\n        dbCfg.setMemoryPolicies(memPlcCfg);\n        dbCfg.setDefaultMemoryPolicyName(\"dfltMemPlc\");\n\n        cfg.setMemoryConfiguration(dbCfg);\n\n        cfg.setPersistenceConfiguration(new PersistenceConfiguration());\n\n        TcpDiscoverySpi discoSpi = new TcpDiscoverySpi();\n\n        discoSpi.setIpFinder(IP_FINDER);\n\n        return cfg;\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testPartitionLossAndRecover()",
            " 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384 -\n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398 -\n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 -\n 408 -\n 409 -\n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "    /**\n     * Test that partitions are marked as lost when all owners leave cluster, but recover after nodes rejoin.\n     *\n     * @throws Exception If fails.\n     */\n    public void testPartitionLossAndRecover() throws Exception {\n        Ignite ignite1 = G.start(getConfiguration(\"test1\"));\n        Ignite ignite2 = G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(\"cache\");\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        ignite1.active(false);\n\n        ignite3.close();\n        ignite4.close();\n\n        ignite1.active(true);\n\n        awaitPartitionMapExchange();\n\n        assert !ignite1.cache(\"cache\").lostPartitions().isEmpty();\n\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        ignite1.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(\"cache\");\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(\"cache\");\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(\"cache\");\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }",
            " 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388 +\n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402 +\n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411 +\n 412 +\n 413 +\n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  ",
            "    /**\n     * Test that partitions are marked as lost when all owners leave cluster, but recover after nodes rejoin.\n     *\n     * @throws Exception If fails.\n     */\n    public void testPartitionLossAndRecover() throws Exception {\n        Ignite ignite1 = G.start(getConfiguration(\"test1\"));\n        Ignite ignite2 = G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(cacheName);\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        ignite1.active(false);\n\n        ignite3.close();\n        ignite4.close();\n\n        ignite1.active(true);\n\n        awaitPartitionMapExchange();\n\n        assert !ignite1.cache(cacheName).lostPartitions().isEmpty();\n\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        ignite1.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(cacheName);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(cacheName);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(cacheName);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testRebalancingOnRestart()",
            " 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 -\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 -\n 186  \n 187  \n 188  \n 189  ",
            "    /**\n     * Test that outdated partitions on restarted nodes are correctly replaced with newer versions.\n     *\n     * @throws Exception If fails.\n     */\n    public void testRebalancingOnRestart() throws Exception {\n        Ignite ignite0 = startGrid(0);\n\n        startGrid(1);\n\n        IgniteEx ignite2 = startGrid(2);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite0.cache(null);\n\n        for (int i = 0; i < 5000; i++)\n            cache1.put(i, i);\n\n        ignite2.close();\n\n        awaitPartitionMapExchange();\n\n        ignite0.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        assert cache1.lostPartitions().isEmpty();\n\n        for (int i = 0; i < 5000; i++)\n            cache1.put(i, i * 2);\n\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n\n        info(\">>> Done puts...\");\n\n        ignite2 = startGrid(2);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache3 = ignite2.cache(null);\n\n        for (int i = 0; i < 100; i++)\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache3.get(i));\n    }",
            " 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190  \n 191  \n 192  \n 193  ",
            "    /**\n     * Test that outdated partitions on restarted nodes are correctly replaced with newer versions.\n     *\n     * @throws Exception If fails.\n     */\n    public void testRebalancingOnRestart() throws Exception {\n        Ignite ignite0 = startGrid(0);\n\n        startGrid(1);\n\n        IgniteEx ignite2 = startGrid(2);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite0.cache(cacheName);\n\n        for (int i = 0; i < 5000; i++)\n            cache1.put(i, i);\n\n        ignite2.close();\n\n        awaitPartitionMapExchange();\n\n        ignite0.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        assert cache1.lostPartitions().isEmpty();\n\n        for (int i = 0; i < 5000; i++)\n            cache1.put(i, i * 2);\n\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n\n        info(\">>> Done puts...\");\n\n        ignite2 = startGrid(2);\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache3 = ignite2.cache(cacheName);\n\n        for (int i = 0; i < 100; i++)\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache3.get(i));\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testNoRebalancingOnRestartDeactivated()",
            " 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 -\n 274  \n 275  \n 276  \n 277  \n 278  \n 279 -\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293 -\n 294  \n 295  \n 296  \n 297  \n 298  \n 299 -\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316 -\n 317 -\n 318 -\n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  ",
            "    /**\n     * Test that up-to-date partitions aren't rebalanced after cluster restarts gracefully.\n     *\n     * @throws Exception If fails.\n     */\n    public void testNoRebalancingOnRestartDeactivated() throws Exception {\n        fail();\n        IgniteEx ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        IgniteEx ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(null);\n\n        final Collection<Integer> parts = new HashSet<>();\n\n        for (int i = 0; i < 100; i++) {\n            cache1.put(i, i);\n            parts.add(ignite1.affinity(null).partition(i));\n        }\n\n        ignite1.active(false);\n\n        ignite1.close();\n        ignite2.close();\n        ignite3.close();\n        ignite4.close();\n\n        final AtomicInteger evtCnt = new AtomicInteger();\n\n        ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n\n        cache1 = ignite1.cache(null);\n\n        ignite1.active(false);\n\n        ignite1.events().remoteListen(new IgniteBiPredicate<UUID, CacheRebalancingEvent>() {\n            @Override public boolean apply(UUID uuid, CacheRebalancingEvent evt) {\n                if (evt.cacheName() == null && parts.contains(evt.partition()))\n                    evtCnt.incrementAndGet();\n\n                return true;\n            }\n        }, null, EventType.EVT_CACHE_REBALANCE_PART_LOADED);\n\n        ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        ignite1.active(true);\n\n        awaitPartitionMapExchange();\n\n        assert evtCnt.get() == 0 : evtCnt.get();\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(null);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(null);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(null);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }",
            " 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277 +\n 278  \n 279  \n 280  \n 281  \n 282  \n 283 +\n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297 +\n 298  \n 299  \n 300  \n 301  \n 302  \n 303 +\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 +\n 321 +\n 322 +\n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  ",
            "    /**\n     * Test that up-to-date partitions aren't rebalanced after cluster restarts gracefully.\n     *\n     * @throws Exception If fails.\n     */\n    public void testNoRebalancingOnRestartDeactivated() throws Exception {\n        fail();\n        IgniteEx ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        IgniteEx ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(cacheName);\n\n        final Collection<Integer> parts = new HashSet<>();\n\n        for (int i = 0; i < 100; i++) {\n            cache1.put(i, i);\n            parts.add(ignite1.affinity(cacheName).partition(i));\n        }\n\n        ignite1.active(false);\n\n        ignite1.close();\n        ignite2.close();\n        ignite3.close();\n        ignite4.close();\n\n        final AtomicInteger evtCnt = new AtomicInteger();\n\n        ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n\n        cache1 = ignite1.cache(cacheName);\n\n        ignite1.active(false);\n\n        ignite1.events().remoteListen(new IgniteBiPredicate<UUID, CacheRebalancingEvent>() {\n            @Override public boolean apply(UUID uuid, CacheRebalancingEvent evt) {\n                if (Objects.equals(evt.cacheName(), cacheName) && parts.contains(evt.partition()))\n                    evtCnt.incrementAndGet();\n\n                return true;\n            }\n        }, null, EventType.EVT_CACHE_REBALANCE_PART_LOADED);\n\n        ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        ignite1.active(true);\n\n        awaitPartitionMapExchange();\n\n        assert evtCnt.get() == 0 : evtCnt.get();\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(cacheName);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(cacheName);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(cacheName);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testRebalancingOnRestartAfterCheckpoint()",
            " 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204 -\n 205 -\n 206 -\n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245 -\n 246 -\n 247  \n 248  \n 249  \n 250 -\n 251 -\n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "    /**\n     * Test that outdated partitions on restarted nodes are correctly replaced with newer versions.\n     *\n     * @throws Exception If fails.\n     */\n    public void testRebalancingOnRestartAfterCheckpoint() throws Exception {\n        IgniteEx ignite0 = startGrid(0);\n\n        IgniteEx ignite1 = startGrid(1);\n\n        IgniteEx ignite2 = startGrid(2);\n        IgniteEx ignite3 = startGrid(3);\n\n        ignite0.cache(null).rebalance().get();\n        ignite1.cache(null).rebalance().get();\n        ignite2.cache(null).rebalance().get();\n        ignite3.cache(null).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite0.cache(null);\n\n        for (int i = 0; i < 1000; i++)\n            cache1.put(i, i);\n\n        ignite0.context().cache().context().database().waitForCheckpoint(\"test\");\n        ignite1.context().cache().context().database().waitForCheckpoint(\"test\");\n\n        info(\"++++++++++ After checkpoint\");\n\n        ignite2.close();\n        ignite3.close();\n\n        awaitPartitionMapExchange();\n\n        ignite0.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        assert cache1.lostPartitions().isEmpty();\n\n        for (int i = 0; i < 1000; i++)\n            cache1.put(i, i * 2);\n\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n\n        info(\">>> Done puts...\");\n\n        ignite2 = startGrid(2);\n        ignite3 = startGrid(3);\n\n        ignite2.cache(null).rebalance().get();\n        ignite3.cache(null).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(null);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(null);\n\n        for (int i = 0; i < 100; i++) {\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache2.get(i));\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache3.get(i));\n        }\n    }",
            " 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 +\n 209 +\n 210 +\n 211 +\n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249 +\n 250 +\n 251  \n 252  \n 253  \n 254 +\n 255 +\n 256  \n 257  \n 258  \n 259  \n 260  \n 261  ",
            "    /**\n     * Test that outdated partitions on restarted nodes are correctly replaced with newer versions.\n     *\n     * @throws Exception If fails.\n     */\n    public void testRebalancingOnRestartAfterCheckpoint() throws Exception {\n        IgniteEx ignite0 = startGrid(0);\n\n        IgniteEx ignite1 = startGrid(1);\n\n        IgniteEx ignite2 = startGrid(2);\n        IgniteEx ignite3 = startGrid(3);\n\n        ignite0.cache(cacheName).rebalance().get();\n        ignite1.cache(cacheName).rebalance().get();\n        ignite2.cache(cacheName).rebalance().get();\n        ignite3.cache(cacheName).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite0.cache(null);\n\n        for (int i = 0; i < 1000; i++)\n            cache1.put(i, i);\n\n        ignite0.context().cache().context().database().waitForCheckpoint(\"test\");\n        ignite1.context().cache().context().database().waitForCheckpoint(\"test\");\n\n        info(\"++++++++++ After checkpoint\");\n\n        ignite2.close();\n        ignite3.close();\n\n        awaitPartitionMapExchange();\n\n        ignite0.resetLostPartitions(Collections.singletonList(cache1.getName()));\n\n        assert cache1.lostPartitions().isEmpty();\n\n        for (int i = 0; i < 1000; i++)\n            cache1.put(i, i * 2);\n\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n        info(\">>>>>>>>>>>>>>>>>\");\n\n        info(\">>> Done puts...\");\n\n        ignite2 = startGrid(2);\n        ignite3 = startGrid(3);\n\n        ignite2.cache(cacheName).rebalance().get();\n        ignite3.cache(cacheName).rebalance().get();\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(cacheName);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(cacheName);\n\n        for (int i = 0; i < 100; i++) {\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache2.get(i));\n            assertEquals(String.valueOf(i), (Integer)(i * 2), cache3.get(i));\n        }\n    }"
        ],
        [
            "IgnitePersistentStoreCacheRebalancingAbstractTest::testDataCorrectnessAfterRestart()",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 -\n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358 -\n 359 -\n 360 -\n 361 -\n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  ",
            "    /**\n     * Test that all data is correctly restored after non-graceful restart.\n     *\n     * @throws Exception If fails.\n     */\n    public void testDataCorrectnessAfterRestart() throws Exception {\n        IgniteEx ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        IgniteEx ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(null);\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        ignite1.close();\n        ignite2.close();\n        ignite3.close();\n        ignite4.close();\n\n        ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        cache1 = ignite1.cache(null);\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(null);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(null);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(null);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }",
            " 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345 +\n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 +\n 363 +\n 364 +\n 365 +\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  ",
            "    /**\n     * Test that all data is correctly restored after non-graceful restart.\n     *\n     * @throws Exception If fails.\n     */\n    public void testDataCorrectnessAfterRestart() throws Exception {\n        IgniteEx ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        IgniteEx ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        IgniteEx ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        IgniteEx ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        IgniteCache<Integer, Integer> cache1 = ignite1.cache(cacheName);\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        ignite1.close();\n        ignite2.close();\n        ignite3.close();\n        ignite4.close();\n\n        ignite1 = (IgniteEx)G.start(getConfiguration(\"test1\"));\n        ignite2 = (IgniteEx)G.start(getConfiguration(\"test2\"));\n        ignite3 = (IgniteEx)G.start(getConfiguration(\"test3\"));\n        ignite4 = (IgniteEx)G.start(getConfiguration(\"test4\"));\n\n        awaitPartitionMapExchange();\n\n        cache1 = ignite1.cache(cacheName);\n        IgniteCache<Integer, Integer> cache2 = ignite2.cache(cacheName);\n        IgniteCache<Integer, Integer> cache3 = ignite3.cache(cacheName);\n        IgniteCache<Integer, Integer> cache4 = ignite4.cache(cacheName);\n\n        for (int i = 0; i < 100; i++) {\n            assert cache1.get(i).equals(i);\n            assert cache2.get(i).equals(i);\n            assert cache3.get(i).equals(i);\n            assert cache4.get(i).equals(i);\n        }\n    }"
        ]
    ],
    "594df607cfee2d94be76bc841bdb21711ca5f778": [
        [
            "ValidateIndexesClosure::processIndex(GridCacheContext,Index)",
            " 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512 -\n 513 -\n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  ",
            "    /**\n     * @param ctx Context.\n     * @param idx Index.\n     */\n    private Map<String, ValidateIndexesPartitionResult> processIndex(GridCacheContext ctx, Index idx) {\n        Object consId = ignite.context().discovery().localNode().consistentId();\n\n        ValidateIndexesPartitionResult idxValidationRes = new ValidateIndexesPartitionResult(\n            -1, -1, true, consId, idx.getName());\n\n        boolean enoughIssues = false;\n\n        Cursor cursor = null;\n\n        try {\n            cursor = idx.find((Session)null, null, null);\n\n            if (cursor == null)\n                throw new IgniteCheckedException(\"Can't iterate through index: \" + idx);\n        }\n        catch (Throwable t) {\n            IndexValidationIssue is = new IndexValidationIssue(null, ctx.name(), idx.getName(), t);\n\n            log.error(\"Find in index failed: \" + is.toString());\n\n            enoughIssues = true;\n        }\n\n        final boolean skipConditions = checkFirst > 0 || checkThrough > 0;\n        final boolean bothSkipConditions = checkFirst > 0 && checkThrough > 0;\n\n        long current = 0;\n        long processedNumber = 0;\n\n        while (!enoughIssues) {\n            KeyCacheObject h2key = null;\n\n            try {\n                if (!cursor.next())\n                    break;\n\n                GridH2Row h2Row = (GridH2Row)cursor.get();\n\n                if (skipConditions) {\n                    if (bothSkipConditions) {\n                        if (processedNumber > checkFirst)\n                            break;\n                        else if (current++ % checkThrough > 0)\n                            continue;\n                        else\n                            processedNumber++;\n                    }\n                    else {\n                        if (checkFirst > 0) {\n                            if (current++ > checkFirst)\n                                break;\n                        }\n                        else {\n                            if (current++ % checkThrough > 0)\n                                continue;\n                        }\n                    }\n                }\n\n                h2key = h2Row.key();\n\n                CacheDataRow cacheDataStoreRow = ctx.group().offheap().read(ctx, h2key);\n\n                if (cacheDataStoreRow == null)\n                    throw new IgniteCheckedException(\"Key is present in SQL index, but can't be found in CacheDataTree.\");\n            }\n            catch (Throwable t) {\n                Object o = CacheObjectUtils.unwrapBinaryIfNeeded(\n                    ctx.cacheObjectContext(), h2key, true, true);\n\n                IndexValidationIssue is = new IndexValidationIssue(\n                    String.valueOf(o), ctx.name(), idx.getName(), t);\n\n                log.error(\"Failed to lookup key: \" + is.toString());\n\n                enoughIssues |= idxValidationRes.reportIssue(is);\n            }\n        }\n\n        String uniqueIdxName = \"[cache=\" + ctx.name() + \", idx=\" + idx.getName() + \"]\";\n\n        processedIndexes.incrementAndGet();\n\n        printProgressIfNeeded();\n\n        return Collections.singletonMap(uniqueIdxName, idxValidationRes);\n    }",
            " 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508 +\n 509 +\n 510  \n 511  \n 512  \n 513  \n 514 +\n 515 +\n 516 +\n 517 +\n 518 +\n 519 +\n 520 +\n 521 +\n 522 +\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553 +\n 554 +\n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  ",
            "    /**\n     * @param ctx Context.\n     * @param idx Index.\n     */\n    private Map<String, ValidateIndexesPartitionResult> processIndex(GridCacheContext ctx, Index idx) {\n        Object consId = ignite.context().discovery().localNode().consistentId();\n\n        ValidateIndexesPartitionResult idxValidationRes = new ValidateIndexesPartitionResult(\n            -1, -1, true, consId, idx.getName());\n\n        boolean enoughIssues = false;\n\n        Cursor cursor = null;\n\n        try {\n            cursor = idx.find((Session)null, null, null);\n\n            if (cursor == null)\n                throw new IgniteCheckedException(\"Can't iterate through index: \" + idx);\n        }\n        catch (Throwable t) {\n            IndexValidationIssue is = new IndexValidationIssue(null, ctx.name(), idx.getName(), t);\n\n            log.error(\"Find in index failed: \" + is.toString());\n\n            enoughIssues = true;\n        }\n\n        final boolean skipConditions = checkFirst > 0 || checkThrough > 0;\n        final boolean bothSkipConditions = checkFirst > 0 && checkThrough > 0;\n\n        long current = 0;\n        long processedNumber = 0;\n\n        KeyCacheObject previousKey = null;\n\n        while (!enoughIssues) {\n            KeyCacheObject h2key = null;\n\n            try {\n                try {\n                    if (!cursor.next())\n                        break;\n                }\n                catch (IllegalStateException e) {\n                    throw new IgniteCheckedException(\"Key is present in SQL index, but is missing in corresponding \" +\n                        \"data page. Previous successfully read key: \" +\n                        CacheObjectUtils.unwrapBinaryIfNeeded(ctx.cacheObjectContext(), previousKey, true, true), e);\n                }\n\n                GridH2Row h2Row = (GridH2Row)cursor.get();\n\n                if (skipConditions) {\n                    if (bothSkipConditions) {\n                        if (processedNumber > checkFirst)\n                            break;\n                        else if (current++ % checkThrough > 0)\n                            continue;\n                        else\n                            processedNumber++;\n                    }\n                    else {\n                        if (checkFirst > 0) {\n                            if (current++ > checkFirst)\n                                break;\n                        }\n                        else {\n                            if (current++ % checkThrough > 0)\n                                continue;\n                        }\n                    }\n                }\n\n                h2key = h2Row.key();\n\n                CacheDataRow cacheDataStoreRow = ctx.group().offheap().read(ctx, h2key);\n\n                if (cacheDataStoreRow == null)\n                    throw new IgniteCheckedException(\"Key is present in SQL index, but can't be found in CacheDataTree.\");\n\n                previousKey = h2key;\n            }\n            catch (Throwable t) {\n                Object o = CacheObjectUtils.unwrapBinaryIfNeeded(\n                    ctx.cacheObjectContext(), h2key, true, true);\n\n                IndexValidationIssue is = new IndexValidationIssue(\n                    String.valueOf(o), ctx.name(), idx.getName(), t);\n\n                log.error(\"Failed to lookup key: \" + is.toString());\n\n                enoughIssues |= idxValidationRes.reportIssue(is);\n            }\n        }\n\n        String uniqueIdxName = \"[cache=\" + ctx.name() + \", idx=\" + idx.getName() + \"]\";\n\n        processedIndexes.incrementAndGet();\n\n        printProgressIfNeeded();\n\n        return Collections.singletonMap(uniqueIdxName, idxValidationRes);\n    }"
        ],
        [
            "GridCommandHandlerIndexingTest::testBrokenCacheDataTreeShouldFailValidation()",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  ",
            "    /**\n     * Tests that missing rows in CacheDataTree are detected.\n     */\n    public void testBrokenCacheDataTreeShouldFailValidation() throws Exception {\n        Ignite ignite = startGrids(2);\n\n        ignite.cluster().active(true);\n\n        Ignite client = startGrid(\"client\");\n\n        String cacheName = \"persons-cache-vi\";\n\n        IgniteCache<Integer, Person> personCache = createPersonCache(client, cacheName);\n\n        ThreadLocalRandom rand = ThreadLocalRandom.current();\n\n        for (int i = 0; i < 10_000; i++)\n            personCache.put(i, new Person(rand.nextInt(), String.valueOf(rand.nextLong())));\n\n        breakCacheDataTree(ignite, cacheName, 1);\n\n        injectTestSystemOut();\n\n        assertEquals(EXIT_CODE_OK,\n            execute(\n                \"--cache\",\n                \"validate_indexes\",\n                cacheName,\n                \"checkFirst\", \"10000\",\n                \"checkThrough\", \"10\"));\n\n        assertTrue(testOut.toString().contains(\"validate_indexes has finished with errors\"));\n    }",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114 +\n 115 +\n 116  ",
            "    /**\n     * Tests that missing rows in CacheDataTree are detected.\n     */\n    public void testBrokenCacheDataTreeShouldFailValidation() throws Exception {\n        Ignite ignite = startGrids(2);\n\n        ignite.cluster().active(true);\n\n        Ignite client = startGrid(\"client\");\n\n        String cacheName = \"persons-cache-vi\";\n\n        IgniteCache<Integer, Person> personCache = createPersonCache(client, cacheName);\n\n        ThreadLocalRandom rand = ThreadLocalRandom.current();\n\n        for (int i = 0; i < 10_000; i++)\n            personCache.put(i, new Person(rand.nextInt(), String.valueOf(rand.nextLong())));\n\n        breakCacheDataTree(ignite, cacheName, 1);\n\n        injectTestSystemOut();\n\n        assertEquals(EXIT_CODE_OK,\n            execute(\n                \"--cache\",\n                \"validate_indexes\",\n                cacheName,\n                \"checkFirst\", \"10000\",\n                \"checkThrough\", \"10\"));\n\n        assertTrue(testOut.toString().contains(\"validate_indexes has finished with errors\"));\n\n        assertTrue(testOut.toString().contains(\n            \"Key is present in SQL index, but is missing in corresponding data page.\"));\n    }"
        ]
    ],
    "654ccf016951b87a0d24e491cce5605522b47b67": [
        [
            "PartitionsEvictManager::PartitionEvictionTask::run()",
            " 411  \n 412  \n 413 -\n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            if (groupEvictionContext.shouldStop())\n                return;\n\n            try {\n                boolean success = part.tryClear(groupEvictionContext);\n\n                if (success) {\n                    if (part.state() == GridDhtPartitionState.EVICTED && part.markForDestroy())\n                        part.destroy();\n                }\n                else // Re-offer partition if clear was unsuccessful due to partition reservation.\n                    evictionQueue.offer(this);\n\n                // Complete eviction future before schedule new to prevent deadlock with\n                // simultaneous eviction stopping and scheduling new eviction.\n                finishFut.onDone();\n            }\n            catch (Throwable ex) {\n                finishFut.onDone(ex);\n\n                if (cctx.kernalContext().isStopping()) {\n                    LT.warn(log, ex, \"Partition eviction failed (current node is stopping).\",\n                        false,\n                        true);\n                }\n                else{\n                    LT.error(log, ex, \"Partition eviction failed, this can cause grid hang.\");\n                }\n            }\n        }",
            " 411  \n 412  \n 413 +\n 414 +\n 415 +\n 416  \n 417 +\n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            if (groupEvictionContext.shouldStop()) {\n                finishFut.onDone();\n\n                return;\n            }\n\n            try {\n                boolean success = part.tryClear(groupEvictionContext);\n\n                if (success) {\n                    if (part.state() == GridDhtPartitionState.EVICTED && part.markForDestroy())\n                        part.destroy();\n                }\n                else // Re-offer partition if clear was unsuccessful due to partition reservation.\n                    evictionQueue.offer(this);\n\n                // Complete eviction future before schedule new to prevent deadlock with\n                // simultaneous eviction stopping and scheduling new eviction.\n                finishFut.onDone();\n            }\n            catch (Throwable ex) {\n                finishFut.onDone(ex);\n\n                if (cctx.kernalContext().isStopping()) {\n                    LT.warn(log, ex, \"Partition eviction failed (current node is stopping).\",\n                        false,\n                        true);\n                }\n                else{\n                    LT.error(log, ex, \"Partition eviction failed, this can cause grid hang.\");\n                }\n            }\n        }"
        ]
    ],
    "1039c8a7921858c7d577816ef1994de969d18e74": [
        [
            "GridDhtAtomicCache::updateAllAsyncInternal0(ClusterNode,GridNearAtomicAbstractUpdateRequest,UpdateReplyClosure)",
            "1717  \n1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725 -\n1726 -\n1727 -\n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  ",
            "    /**\n     * Executes local update after preloader fetched values.\n     *\n     * @param node Node.\n     * @param req Update request.\n     * @param completionCb Completion callback.\n     */\n    private void updateAllAsyncInternal0(\n        ClusterNode node,\n        GridNearAtomicAbstractUpdateRequest req,\n        UpdateReplyClosure completionCb\n    ) {\n        GridNearAtomicUpdateResponse res = new GridNearAtomicUpdateResponse(ctx.cacheId(),\n            node.id(),\n            req.futureId(),\n            req.partition(),\n            false,\n            ctx.deploymentEnabled());\n\n        assert !req.returnValue() || (req.operation() == TRANSFORM || req.size() == 1);\n\n        GridDhtAtomicAbstractUpdateFuture dhtFut = null;\n\n        IgniteCacheExpiryPolicy expiry = null;\n\n        ctx.shared().database().checkpointReadLock();\n\n        try {\n            ctx.shared().database().ensureFreeSpace(ctx.dataRegion());\n\n            // If batch store update is enabled, we need to lock all entries.\n            // First, need to acquire locks on cache entries, then check filter.\n            List<GridDhtCacheEntry> locked = lockEntries(req, req.topologyVersion());;\n\n            Collection<IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion>> deleted = null;\n\n            DhtAtomicUpdateResult  updDhtRes = new DhtAtomicUpdateResult();\n\n            try {\n                while (true) {\n                    try {\n                        GridDhtPartitionTopology top = topology();\n\n                        top.readLock();\n\n                        try {\n                            if (top.stopping()) {\n                                res.addFailedKeys(req.keys(), new CacheStoppedException(name()));\n\n                                completionCb.apply(req, res);\n\n                                return;\n                            }\n\n                            boolean remap = false;\n\n                            // Do not check topology version if topology was locked on near node by\n                            // external transaction or explicit lock.\n                            if (!req.topologyLocked()) {\n                                // Can not wait for topology future since it will break\n                                // GridNearAtomicCheckUpdateRequest processing.\n                                remap = !top.topologyVersionFuture().isDone() ||\n                                    needRemap(req.topologyVersion(), top.readyTopologyVersion(), req.keys());\n                            }\n\n                            if (!remap) {\n                                boolean validateCache = needCacheValidation(node);\n\n                                if (validateCache) {\n                                    GridDhtTopologyFuture topFut = top.topologyVersionFuture();\n\n                                    assert topFut.isDone() : topFut;\n\n                                    Throwable err = topFut.validateCache(ctx, req.recovery(), false, null, null);\n\n                                    if (err != null) {\n                                        IgniteCheckedException e = new IgniteCheckedException(err);\n\n                                        res.error(e);\n\n                                        completionCb.apply(req, res);\n\n                                        return;\n                                    }\n                                }\n\n                                update(node, locked, req, res, updDhtRes);\n\n                                dhtFut = updDhtRes.dhtFuture();\n                                deleted = updDhtRes.deleted();\n                                expiry = updDhtRes.expiryPolicy();\n                            }\n                            else\n                                // Should remap all keys.\n                                res.remapTopologyVersion(top.lastTopologyChangeVersion());\n                        }\n                        finally {\n                            top.readUnlock();\n                        }\n\n                        // This call will convert entry processor invocation results to cache object instances.\n                        // Must be done outside topology read lock to avoid deadlocks.\n                        if (res.returnValue() != null)\n                            res.returnValue().marshalResult(ctx);\n\n                        break;\n                    }\n                    catch (UnregisteredClassException ex) {\n                        IgniteCacheObjectProcessor cacheObjProc = ctx.cacheObjects();\n\n                        assert cacheObjProc instanceof CacheObjectBinaryProcessorImpl;\n\n                        ((CacheObjectBinaryProcessorImpl)cacheObjProc)\n                            .binaryContext().descriptorForClass(ex.cls(), false, false);\n                    }\n                    catch (UnregisteredBinaryTypeException ex) {\n                        IgniteCacheObjectProcessor cacheObjProc = ctx.cacheObjects();\n\n                        assert cacheObjProc instanceof CacheObjectBinaryProcessorImpl;\n\n                        ((CacheObjectBinaryProcessorImpl)cacheObjProc)\n                            .binaryContext().updateMetadata(ex.typeId(), ex.binaryMetadata(), false);\n                    }\n                }\n            }\n            catch (GridCacheEntryRemovedException e) {\n                assert false : \"Entry should not become obsolete while holding lock.\";\n\n                e.printStackTrace();\n            }\n            finally {\n                if (locked != null)\n                    unlockEntries(locked, req.topologyVersion());\n\n                // Enqueue if necessary after locks release.\n                if (deleted != null) {\n                    assert !deleted.isEmpty();\n                    assert ctx.deferredDelete() : this;\n\n                    for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted)\n                        ctx.onDeferredDelete(e.get1(), e.get2());\n                }\n\n                // TODO handle failure: probably drop the node from topology\n                // TODO fire events only after successful fsync\n                if (ctx.shared().wal() != null)\n                    ctx.shared().wal().flush(null, false);\n            }\n        }\n        catch (GridDhtInvalidPartitionException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Caught invalid partition exception for cache entry (will remap update request): \" + req);\n\n            res.remapTopologyVersion(ctx.topology().lastTopologyChangeVersion());\n        }\n        catch (Throwable e) {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            U.error(log, \"Unexpected exception during cache update\", e);\n\n            res.addFailedKeys(req.keys(), e);\n\n            completionCb.apply(req, res);\n\n            if (e instanceof Error)\n                throw (Error)e;\n\n            return;\n        }\n        finally {\n            ctx.shared().database().checkpointReadUnlock();\n        }\n\n        if (res.remapTopologyVersion() != null) {\n            assert dhtFut == null;\n\n            completionCb.apply(req, res);\n        }\n        else {\n            if (dhtFut != null)\n                dhtFut.map(node, res.returnValue(), res, completionCb);\n        }\n\n        if (req.writeSynchronizationMode() != FULL_ASYNC)\n            req.cleanup(!node.isLocal());\n\n        sendTtlUpdateRequest(expiry);\n    }",
            "1718  \n1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726 +\n1727 +\n1728 +\n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789 +\n1790 +\n1791 +\n1792 +\n1793 +\n1794 +\n1795 +\n1796 +\n1797 +\n1798 +\n1799 +\n1800 +\n1801 +\n1802 +\n1803 +\n1804 +\n1805 +\n1806 +\n1807 +\n1808 +\n1809 +\n1810 +\n1811 +\n1812 +\n1813 +\n1814 +\n1815 +\n1816 +\n1817 +\n1818 +\n1819 +\n1820 +\n1821 +\n1822 +\n1823 +\n1824 +\n1825 +\n1826 +\n1827 +\n1828 +\n1829 +\n1830 +\n1831 +\n1832 +\n1833 +\n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  ",
            "    /**\n     * Executes local update after preloader fetched values.\n     *\n     * @param node Node.\n     * @param req Update request.\n     * @param completionCb Completion callback.\n     */\n    private void updateAllAsyncInternal0(\n        final ClusterNode node,\n        final GridNearAtomicAbstractUpdateRequest req,\n        final UpdateReplyClosure completionCb\n    ) {\n        GridNearAtomicUpdateResponse res = new GridNearAtomicUpdateResponse(ctx.cacheId(),\n            node.id(),\n            req.futureId(),\n            req.partition(),\n            false,\n            ctx.deploymentEnabled());\n\n        assert !req.returnValue() || (req.operation() == TRANSFORM || req.size() == 1);\n\n        GridDhtAtomicAbstractUpdateFuture dhtFut = null;\n\n        IgniteCacheExpiryPolicy expiry = null;\n\n        ctx.shared().database().checkpointReadLock();\n\n        try {\n            ctx.shared().database().ensureFreeSpace(ctx.dataRegion());\n\n            // If batch store update is enabled, we need to lock all entries.\n            // First, need to acquire locks on cache entries, then check filter.\n            List<GridDhtCacheEntry> locked = lockEntries(req, req.topologyVersion());;\n\n            Collection<IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion>> deleted = null;\n\n            DhtAtomicUpdateResult  updDhtRes = new DhtAtomicUpdateResult();\n\n            try {\n                while (true) {\n                    try {\n                        GridDhtPartitionTopology top = topology();\n\n                        top.readLock();\n\n                        try {\n                            if (top.stopping()) {\n                                res.addFailedKeys(req.keys(), new CacheStoppedException(name()));\n\n                                completionCb.apply(req, res);\n\n                                return;\n                            }\n\n                            boolean remap = false;\n\n                            // Do not check topology version if topology was locked on near node by\n                            // external transaction or explicit lock.\n                            if (!req.topologyLocked()) {\n                                // Can not wait for topology future since it will break\n                                // GridNearAtomicCheckUpdateRequest processing.\n                                remap = !top.topologyVersionFuture().isDone() ||\n                                    needRemap(req.topologyVersion(), top.readyTopologyVersion(), req.keys());\n                            }\n\n                            if (!remap) {\n                                boolean validateCache = needCacheValidation(node);\n\n                                if (validateCache) {\n                                    GridDhtTopologyFuture topFut = top.topologyVersionFuture();\n\n                                    // Cache validation should use topology version from the update request\n                                    // in case of the topology version was locked on near node.\n                                    if (req.topologyLocked()) {\n                                        // affinityReadyFuture() can return GridFinishedFuture under some circumstances\n                                        // and therefore it cannot be used for validation.\n                                        IgniteInternalFuture<AffinityTopologyVersion> affFut =\n                                            ctx.shared().exchange().affinityReadyFuture(req.topologyVersion());\n\n                                        if (affFut.isDone()) {\n                                            List<GridDhtPartitionsExchangeFuture> futs =\n                                                ctx.shared().exchange().exchangeFutures();\n\n                                            boolean found = false;\n\n                                            for (int i = 0; i < futs.size(); ++i) {\n                                                GridDhtPartitionsExchangeFuture fut = futs.get(i);\n\n                                                // We have to check fut.exchangeDone() here -\n                                                // otherwise attempt to get topVer will throw error.\n                                                // We won't skip needed future as per affinity ready future is done.\n                                                if (fut.exchangeDone() &&\n                                                    fut.topologyVersion().equals(req.topologyVersion())) {\n                                                    topFut = fut;\n\n                                                    found = true;\n\n                                                    break;\n                                                }\n                                            }\n\n                                            assert found: \"The requested topology future cannot be found [topVer=\"\n                                                + req.topologyVersion() + ']';\n                                        }\n                                        else {\n                                            affFut.listen(f -> updateAllAsyncInternal0(node, req, completionCb));\n\n                                            return;\n                                        }\n\n                                        assert req.topologyVersion().equals(topFut.topologyVersion()) :\n                                            \"The requested topology version cannot be found [\" +\n                                                \"reqTopFut=\" + req.topologyVersion()\n                                                + \", topFut=\" + topFut + ']';\n                                    }\n\n                                    assert topFut.isDone() : topFut;\n\n                                    Throwable err = topFut.validateCache(ctx, req.recovery(), false, null, null);\n\n                                    if (err != null) {\n                                        IgniteCheckedException e = new IgniteCheckedException(err);\n\n                                        res.error(e);\n\n                                        completionCb.apply(req, res);\n\n                                        return;\n                                    }\n                                }\n\n                                update(node, locked, req, res, updDhtRes);\n\n                                dhtFut = updDhtRes.dhtFuture();\n                                deleted = updDhtRes.deleted();\n                                expiry = updDhtRes.expiryPolicy();\n                            }\n                            else\n                                // Should remap all keys.\n                                res.remapTopologyVersion(top.lastTopologyChangeVersion());\n                        }\n                        finally {\n                            top.readUnlock();\n                        }\n\n                        // This call will convert entry processor invocation results to cache object instances.\n                        // Must be done outside topology read lock to avoid deadlocks.\n                        if (res.returnValue() != null)\n                            res.returnValue().marshalResult(ctx);\n\n                        break;\n                    }\n                    catch (UnregisteredClassException ex) {\n                        IgniteCacheObjectProcessor cacheObjProc = ctx.cacheObjects();\n\n                        assert cacheObjProc instanceof CacheObjectBinaryProcessorImpl;\n\n                        ((CacheObjectBinaryProcessorImpl)cacheObjProc)\n                            .binaryContext().descriptorForClass(ex.cls(), false, false);\n                    }\n                    catch (UnregisteredBinaryTypeException ex) {\n                        IgniteCacheObjectProcessor cacheObjProc = ctx.cacheObjects();\n\n                        assert cacheObjProc instanceof CacheObjectBinaryProcessorImpl;\n\n                        ((CacheObjectBinaryProcessorImpl)cacheObjProc)\n                            .binaryContext().updateMetadata(ex.typeId(), ex.binaryMetadata(), false);\n                    }\n                }\n            }\n            catch (GridCacheEntryRemovedException e) {\n                assert false : \"Entry should not become obsolete while holding lock.\";\n\n                e.printStackTrace();\n            }\n            finally {\n                if (locked != null)\n                    unlockEntries(locked, req.topologyVersion());\n\n                // Enqueue if necessary after locks release.\n                if (deleted != null) {\n                    assert !deleted.isEmpty();\n                    assert ctx.deferredDelete() : this;\n\n                    for (IgniteBiTuple<GridDhtCacheEntry, GridCacheVersion> e : deleted)\n                        ctx.onDeferredDelete(e.get1(), e.get2());\n                }\n\n                // TODO handle failure: probably drop the node from topology\n                // TODO fire events only after successful fsync\n                if (ctx.shared().wal() != null)\n                    ctx.shared().wal().flush(null, false);\n            }\n        }\n        catch (GridDhtInvalidPartitionException ignore) {\n            if (log.isDebugEnabled())\n                log.debug(\"Caught invalid partition exception for cache entry (will remap update request): \" + req);\n\n            res.remapTopologyVersion(ctx.topology().lastTopologyChangeVersion());\n        }\n        catch (Throwable e) {\n            // At least RuntimeException can be thrown by the code above when GridCacheContext is cleaned and there is\n            // an attempt to use cleaned resources.\n            U.error(log, \"Unexpected exception during cache update\", e);\n\n            res.addFailedKeys(req.keys(), e);\n\n            completionCb.apply(req, res);\n\n            if (e instanceof Error)\n                throw (Error)e;\n\n            return;\n        }\n        finally {\n            ctx.shared().database().checkpointReadUnlock();\n        }\n\n        if (res.remapTopologyVersion() != null) {\n            assert dhtFut == null;\n\n            completionCb.apply(req, res);\n        }\n        else {\n            if (dhtFut != null)\n                dhtFut.map(node, res.returnValue(), res, completionCb);\n        }\n\n        if (req.writeSynchronizationMode() != FULL_ASYNC)\n            req.cleanup(!node.isLocal());\n\n        sendTtlUpdateRequest(expiry);\n    }"
        ],
        [
            "GridDhtColocatedLockFuture::map()",
            " 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780 -\n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  ",
            "    /**\n     * Basically, future mapping consists from two parts. First, we must determine the topology version this future\n     * will map on. Locking is performed within a user transaction, we must continue to map keys on the same\n     * topology version as it started. If topology version is undefined, we get current topology future and wait\n     * until it completes so the topology is ready to use.\n     * <p/>\n     * During the second part we map keys to primary nodes using topology snapshot we obtained during the first\n     * part. Note that if primary node leaves grid, the future will fail and transaction will be rolled back.\n     */\n    void map() {\n        if (isDone()) // Possible due to async rollback.\n            return;\n\n        if (timeout > 0) {\n            timeoutObj = new LockTimeoutObject();\n\n            cctx.time().addTimeoutObject(timeoutObj);\n        }\n\n        // Obtain the topology version to use.\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx != null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(Thread.currentThread().getId(), tx);\n\n        if (topVer != null && tx != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null && tx != null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = fut.validateCache(cctx, recovery, read, null, keys);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            // Continue mapping on the same topology version as it was before.\n            synchronized (this) {\n                if (this.topVer == null)\n                    this.topVer = topVer;\n            }\n\n            map(keys, false, true);\n\n            markInitialized();\n\n            return;\n        }\n\n        // Must get topology snapshot and map on that version.\n        mapOnTopology(false, null);\n    }",
            " 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780 +\n 781 +\n 782 +\n 783 +\n 784 +\n 785 +\n 786 +\n 787 +\n 788 +\n 789 +\n 790 +\n 791 +\n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  ",
            "    /**\n     * Basically, future mapping consists from two parts. First, we must determine the topology version this future\n     * will map on. Locking is performed within a user transaction, we must continue to map keys on the same\n     * topology version as it started. If topology version is undefined, we get current topology future and wait\n     * until it completes so the topology is ready to use.\n     * <p/>\n     * During the second part we map keys to primary nodes using topology snapshot we obtained during the first\n     * part. Note that if primary node leaves grid, the future will fail and transaction will be rolled back.\n     */\n    void map() {\n        if (isDone()) // Possible due to async rollback.\n            return;\n\n        if (timeout > 0) {\n            timeoutObj = new LockTimeoutObject();\n\n            cctx.time().addTimeoutObject(timeoutObj);\n        }\n\n        // Obtain the topology version to use.\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx != null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(Thread.currentThread().getId(), tx);\n\n        if (topVer != null && tx != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null && tx != null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = null;\n\n                    // Before cache validation, make sure that this topology future is already completed.\n                    try {\n                        fut.get();\n                    }\n                    catch (IgniteCheckedException e) {\n                        err = fut.error();\n                    }\n\n                    if (err == null)\n                        err = fut.validateCache(cctx, recovery, read, null, keys);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            // Continue mapping on the same topology version as it was before.\n            synchronized (this) {\n                if (this.topVer == null)\n                    this.topVer = topVer;\n            }\n\n            map(keys, false, true);\n\n            markInitialized();\n\n            return;\n        }\n\n        // Must get topology snapshot and map on that version.\n        mapOnTopology(false, null);\n    }"
        ],
        [
            "GridNearTxAbstractEnlistFuture::init()",
            " 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226 -\n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  ",
            "    /**\n     *\n     */\n    public void init() {\n        if (timeout < 0) {\n            // Time is out.\n            onDone(timeoutException());\n\n            return;\n        }\n        else if (timeout > 0)\n            timeoutObj = new LockTimeoutObject();\n\n        while (true) {\n            IgniteInternalFuture<?> fut = tx.lockFuture();\n\n            if (fut == GridDhtTxLocalAdapter.ROLLBACK_FUT) {\n                onDone(tx.timedOut() ? tx.timeoutException() : tx.rollbackException());\n\n                return;\n            }\n            else if (fut != null) {\n                // Wait for previous future.\n                assert fut instanceof GridNearTxAbstractEnlistFuture\n                    || fut instanceof GridDhtTxAbstractEnlistFuture\n                    || fut instanceof CompoundLockFuture\n                    || fut instanceof GridNearTxSelectForUpdateFuture : fut;\n\n                // Terminate this future if parent future is terminated by rollback.\n                if (!fut.isDone()) {\n                    fut.listen(new IgniteInClosure<IgniteInternalFuture>() {\n                        @Override public void apply(IgniteInternalFuture fut) {\n                            if (fut.error() != null)\n                                onDone(fut.error());\n                        }\n                    });\n                }\n                else if (fut.error() != null)\n                    onDone(fut.error());\n\n                break;\n            }\n            else if (tx.updateLockFuture(null, this))\n                break;\n        }\n\n        boolean added = cctx.mvcc().addFuture(this);\n\n        assert added : this;\n\n        if (isDone()) {\n            cctx.mvcc().removeFuture(futId);\n\n            return;\n        }\n\n        try {\n            tx.addActiveCache(cctx, false);\n        }\n        catch (IgniteCheckedException e) {\n            onDone(e);\n\n            return;\n        }\n\n        if (timeoutObj != null)\n            cctx.time().addTimeoutObject(timeoutObj);\n\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = fut.validateCache(cctx, false, false, null, null);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            if (this.topVer == null)\n                this.topVer = topVer;\n\n            map(true);\n\n            return;\n        }\n\n        mapOnTopology();\n    }",
            " 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226 +\n 227 +\n 228 +\n 229 +\n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  ",
            "    /**\n     *\n     */\n    public void init() {\n        if (timeout < 0) {\n            // Time is out.\n            onDone(timeoutException());\n\n            return;\n        }\n        else if (timeout > 0)\n            timeoutObj = new LockTimeoutObject();\n\n        while (true) {\n            IgniteInternalFuture<?> fut = tx.lockFuture();\n\n            if (fut == GridDhtTxLocalAdapter.ROLLBACK_FUT) {\n                onDone(tx.timedOut() ? tx.timeoutException() : tx.rollbackException());\n\n                return;\n            }\n            else if (fut != null) {\n                // Wait for previous future.\n                assert fut instanceof GridNearTxAbstractEnlistFuture\n                    || fut instanceof GridDhtTxAbstractEnlistFuture\n                    || fut instanceof CompoundLockFuture\n                    || fut instanceof GridNearTxSelectForUpdateFuture : fut;\n\n                // Terminate this future if parent future is terminated by rollback.\n                if (!fut.isDone()) {\n                    fut.listen(new IgniteInClosure<IgniteInternalFuture>() {\n                        @Override public void apply(IgniteInternalFuture fut) {\n                            if (fut.error() != null)\n                                onDone(fut.error());\n                        }\n                    });\n                }\n                else if (fut.error() != null)\n                    onDone(fut.error());\n\n                break;\n            }\n            else if (tx.updateLockFuture(null, this))\n                break;\n        }\n\n        boolean added = cctx.mvcc().addFuture(this);\n\n        assert added : this;\n\n        if (isDone()) {\n            cctx.mvcc().removeFuture(futId);\n\n            return;\n        }\n\n        try {\n            tx.addActiveCache(cctx, false);\n        }\n        catch (IgniteCheckedException e) {\n            onDone(e);\n\n            return;\n        }\n\n        if (timeoutObj != null)\n            cctx.time().addTimeoutObject(timeoutObj);\n\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = null;\n\n                    // Before cache validation, make sure that this topology future is already completed.\n                    try {\n                        fut.get();\n                    }\n                    catch (IgniteCheckedException e) {\n                        err = fut.error();\n                    }\n\n                    if (err == null)\n                        err = fut.validateCache(cctx, false, false, null, null);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            if (this.topVer == null)\n                this.topVer = topVer;\n\n            map(true);\n\n            return;\n        }\n\n        mapOnTopology();\n    }"
        ],
        [
            "GridNearLockFuture::map()",
            " 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848 -\n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  ",
            "    /**\n     * Basically, future mapping consists from two parts. First, we must determine the topology version this future\n     * will map on. Locking is performed within a user transaction, we must continue to map keys on the same\n     * topology version as it started. If topology version is undefined, we get current topology future and wait\n     * until it completes so the topology is ready to use.\n     * <p/>\n     * During the second part we map keys to primary nodes using topology snapshot we obtained during the first\n     * part. Note that if primary node leaves grid, the future will fail and transaction will be rolled back.\n     */\n    void map() {\n        if (isDone()) // Possible due to async rollback.\n            return;\n\n        if (timeout > 0) {\n            timeoutObj = new LockTimeoutObject();\n\n            cctx.time().addTimeoutObject(timeoutObj);\n        }\n\n        boolean added = cctx.mvcc().addFuture(this);\n\n        assert added : this;\n\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx != null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null && tx != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null && tx != null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)){\n                    Throwable err = fut.validateCache(cctx, recovery, read, null, keys);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            // Continue mapping on the same topology version as it was before.\n            if (this.topVer == null)\n                this.topVer = topVer;\n\n            map(keys, false, true);\n\n            markInitialized();\n\n            return;\n        }\n\n        // Must get topology snapshot and map on that version.\n        mapOnTopology(false);\n    }",
            " 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848 +\n 849 +\n 850 +\n 851 +\n 852 +\n 853 +\n 854 +\n 855 +\n 856 +\n 857 +\n 858 +\n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  ",
            "    /**\n     * Basically, future mapping consists from two parts. First, we must determine the topology version this future\n     * will map on. Locking is performed within a user transaction, we must continue to map keys on the same\n     * topology version as it started. If topology version is undefined, we get current topology future and wait\n     * until it completes so the topology is ready to use.\n     * <p/>\n     * During the second part we map keys to primary nodes using topology snapshot we obtained during the first\n     * part. Note that if primary node leaves grid, the future will fail and transaction will be rolled back.\n     */\n    void map() {\n        if (isDone()) // Possible due to async rollback.\n            return;\n\n        if (timeout > 0) {\n            timeoutObj = new LockTimeoutObject();\n\n            cctx.time().addTimeoutObject(timeoutObj);\n        }\n\n        boolean added = cctx.mvcc().addFuture(this);\n\n        assert added : this;\n\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx != null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null && tx != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null && tx != null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)){\n                    Throwable err = null;\n\n                    // Before cache validation, make sure that this topology future is already completed.\n                    try {\n                        fut.get();\n                    }\n                    catch (IgniteCheckedException e) {\n                        err = fut.error();\n                    }\n\n                    err = (err == null)? fut.validateCache(cctx, recovery, read, null, keys): err;\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            // Continue mapping on the same topology version as it was before.\n            if (this.topVer == null)\n                this.topVer = topVer;\n\n            map(keys, false, true);\n\n            markInitialized();\n\n            return;\n        }\n\n        // Must get topology snapshot and map on that version.\n        mapOnTopology(false);\n    }"
        ],
        [
            "TxTopologyVersionFuture::init()",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "    /** */\n    private void init() {\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = fut.validateCache(cctx, false, false, null, null);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            onDone(topVer);\n\n            topLocked = true;\n\n            return;\n        }\n\n        acquireTopologyVersion();\n    }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 +\n  74 +\n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81 +\n  82 +\n  83 +\n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "    /** */\n    private void init() {\n        // Obtain the topology version to use.\n        long threadId = Thread.currentThread().getId();\n\n        AffinityTopologyVersion topVer = cctx.mvcc().lastExplicitLockTopologyVersion(threadId);\n\n        // If there is another system transaction in progress, use it's topology version to prevent deadlock.\n        if (topVer == null && tx.system())\n            topVer = cctx.tm().lockedTopologyVersion(threadId, tx);\n\n        if (topVer != null)\n            tx.topologyVersion(topVer);\n\n        if (topVer == null)\n            topVer = tx.topologyVersionSnapshot();\n\n        if (topVer != null) {\n            for (GridDhtTopologyFuture fut : cctx.shared().exchange().exchangeFutures()) {\n                if (fut.exchangeDone() && fut.topologyVersion().equals(topVer)) {\n                    Throwable err = null;\n\n                    // Before cache validation, make sure that this topology future is already completed.\n                    try {\n                        fut.get();\n                    }\n                    catch (IgniteCheckedException e) {\n                        err = fut.error();\n                    }\n\n                    if (err == null)\n                        err = fut.validateCache(cctx, false, false, null, null);\n\n                    if (err != null) {\n                        onDone(err);\n\n                        return;\n                    }\n\n                    break;\n                }\n            }\n\n            onDone(topVer);\n\n            topLocked = true;\n\n            return;\n        }\n\n        acquireTopologyVersion();\n    }"
        ]
    ],
    "eeebfca0bddbf31b10a86a6725e2c27933fdb0ae": [
        [
            "GridToStringBuilder::arrayToString(Class,Object)",
            "1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050 -\n1051  \n1052  \n1053  \n1054 -\n1055 -\n1056 -\n1057 -\n1058 -\n1059  \n1060  \n1061 -\n1062 -\n1063 -\n1064 -\n1065 -\n1066 -\n1067 -\n1068 -\n1069 -\n1070 -\n1071 -\n1072 -\n1073 -\n1074 -\n1075 -\n1076 -\n1077 -\n1078 -\n1079 -\n1080 -\n1081  \n1082  \n1083  \n1084 -\n1085  \n1086  \n1087  \n1088  \n1089  \n1090  ",
            "    /**\n     * @param arrType Type of the array.\n     * @param arr Array object.\n     * @return String representation of an array.\n     */\n    @SuppressWarnings({\"ConstantConditions\", \"unchecked\"})\n    public static <T> String arrayToString(Class arrType, @Nullable Object arr) {\n        if (arr == null)\n            return \"null\";\n\n        T[] array = (T[])arr;\n\n        if (array.length > COLLECTION_LIMIT)\n            arr = Arrays.copyOf(array, COLLECTION_LIMIT);\n\n        String res;\n\n        if (arrType.equals(byte[].class))\n            res = Arrays.toString((byte[])arr);\n        else if (arrType.equals(boolean[].class))\n            res = Arrays.toString((boolean[])arr);\n        else if (arrType.equals(short[].class))\n            res = Arrays.toString((short[])arr);\n        else if (arrType.equals(int[].class))\n            res = Arrays.toString((int[])arr);\n        else if (arrType.equals(long[].class))\n            res = Arrays.toString((long[])arr);\n        else if (arrType.equals(float[].class))\n            res = Arrays.toString((float[])arr);\n        else if (arrType.equals(double[].class))\n            res = Arrays.toString((double[])arr);\n        else if (arrType.equals(char[].class))\n            res = Arrays.toString((char[])arr);\n        else\n            res = Arrays.toString((Object[])arr);\n\n        if (array.length > COLLECTION_LIMIT) {\n            StringBuilder resSB = new StringBuilder(res);\n\n            resSB.deleteCharAt(resSB.length() - 1);\n            resSB.append(\"... and \").append(array.length - COLLECTION_LIMIT).append(\" more]\");\n\n            res = resSB.toString();\n        }\n\n        return res;\n    }",
            "1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050 +\n1051  \n1052  \n1053  \n1054  \n1055 +\n1056  \n1057 +\n1058 +\n1059 +\n1060 +\n1061 +\n1062 +\n1063 +\n1064 +\n1065 +\n1066 +\n1067 +\n1068 +\n1069 +\n1070 +\n1071 +\n1072 +\n1073 +\n1074 +\n1075 +\n1076 +\n1077 +\n1078 +\n1079 +\n1080 +\n1081 +\n1082 +\n1083 +\n1084 +\n1085 +\n1086 +\n1087 +\n1088 +\n1089 +\n1090 +\n1091 +\n1092 +\n1093 +\n1094 +\n1095 +\n1096 +\n1097 +\n1098 +\n1099 +\n1100 +\n1101 +\n1102 +\n1103 +\n1104 +\n1105 +\n1106 +\n1107 +\n1108 +\n1109 +\n1110 +\n1111 +\n1112 +\n1113 +\n1114 +\n1115 +\n1116 +\n1117 +\n1118 +\n1119 +\n1120 +\n1121 +\n1122 +\n1123 +\n1124 +\n1125 +\n1126 +\n1127 +\n1128 +\n1129 +\n1130  \n1131  \n1132  \n1133 +\n1134  \n1135  \n1136  \n1137  \n1138  \n1139  ",
            "    /**\n     * @param arrType Type of the array.\n     * @param arr Array object.\n     * @return String representation of an array.\n     */\n    @SuppressWarnings({\"ConstantConditions\", \"unchecked\"})\n    public static <T> String arrayToString(Class arrType, Object arr) {\n        if (arr == null)\n            return \"null\";\n\n        String res;\n        int more = 0;\n\n        if (arrType.equals(byte[].class)) {\n            byte[] byteArr = (byte[])arr;\n            if (byteArr.length > COLLECTION_LIMIT) {\n                more = byteArr.length - COLLECTION_LIMIT;\n                byteArr = Arrays.copyOf(byteArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(byteArr);\n        }\n        else if (arrType.equals(boolean[].class)) {\n            boolean[] boolArr = (boolean[])arr;\n            if (boolArr.length > COLLECTION_LIMIT) {\n                more = boolArr.length - COLLECTION_LIMIT;\n                boolArr = Arrays.copyOf(boolArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(boolArr);\n        }\n        else if (arrType.equals(short[].class)) {\n            short[] shortArr = (short[])arr;\n            if (shortArr.length > COLLECTION_LIMIT) {\n                more = shortArr.length - COLLECTION_LIMIT;\n                shortArr = Arrays.copyOf(shortArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(shortArr);\n        }\n        else if (arrType.equals(int[].class)) {\n            int[] intArr = (int[])arr;\n            if (intArr.length > COLLECTION_LIMIT) {\n                more = intArr.length - COLLECTION_LIMIT;\n                intArr = Arrays.copyOf(intArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(intArr);\n        }\n        else if (arrType.equals(long[].class)) {\n            long[] longArr = (long[])arr;\n            if (longArr.length > COLLECTION_LIMIT) {\n                more = longArr.length - COLLECTION_LIMIT;\n                longArr = Arrays.copyOf(longArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(longArr);\n        }\n        else if (arrType.equals(float[].class)) {\n            float[] floatArr = (float[])arr;\n            if (floatArr.length > COLLECTION_LIMIT) {\n                more = floatArr.length - COLLECTION_LIMIT;\n                floatArr = Arrays.copyOf(floatArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(floatArr);\n        }\n        else if (arrType.equals(double[].class)) {\n            double[] doubleArr = (double[])arr;\n            if (doubleArr.length > COLLECTION_LIMIT) {\n                more = doubleArr.length - COLLECTION_LIMIT;\n                doubleArr = Arrays.copyOf(doubleArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(doubleArr);\n        }\n        else if (arrType.equals(char[].class)) {\n            char[] charArr = (char[])arr;\n            if (charArr.length > COLLECTION_LIMIT) {\n                more = charArr.length - COLLECTION_LIMIT;\n                charArr = Arrays.copyOf(charArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(charArr);\n        }\n        else {\n            Object[] objArr = (Object[])arr;\n            if (objArr.length > COLLECTION_LIMIT) {\n                more = objArr.length - COLLECTION_LIMIT;\n                objArr = Arrays.copyOf(objArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(objArr);\n        }\n        if (more > 0) {\n            StringBuilder resSB = new StringBuilder(res);\n\n            resSB.deleteCharAt(resSB.length() - 1);\n            resSB.append(\"... and \").append(more).append(\" more]\");\n\n            res = resSB.toString();\n        }\n\n        return res;\n    }"
        ],
        [
            "GridToStringBuilderSelfTest::testToStringCollectionLimits()",
            " 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testToStringCollectionLimits() throws Exception {\n        int limit = IgniteSystemProperties.getInteger(IGNITE_TO_STRING_COLLECTION_LIMIT, 100);\n\n        Object vals[] = new Object[] {Byte.MIN_VALUE, Boolean.TRUE, Short.MIN_VALUE, Integer.MIN_VALUE, Long.MIN_VALUE,\n            Float.MIN_VALUE, Double.MIN_VALUE, Character.MIN_VALUE, new TestClass1()};\n        for (Object val : vals)\n            testArr(val, limit);\n\n        Map<String, String> strMap = new TreeMap<>();\n        List<String> strList = new ArrayList<>(limit+1);\n\n        TestClass1 testClass = new TestClass1();\n        testClass.strMap = strMap;\n        testClass.strListIncl = strList;\n\n        for (int i = 0; i < limit; i++) {\n            strMap.put(\"k\" + i, \"v\");\n            strList.add(\"e\");\n        }\n        String testClassStr = GridToStringBuilder.toString(TestClass1.class, testClass);\n\n        strMap.put(\"kz\", \"v\"); // important to add last element in TreeMap here\n        strList.add(\"e\");\n\n        String testClassStrOf = GridToStringBuilder.toString(TestClass1.class, testClass);\n\n        String testClassStrOfR = testClassStrOf.replaceAll(\"... and 1 more\",\"\");\n\n        assertTrue(\"Collection limit error in Map or List, normal: <\" + testClassStr + \">, overflowed: <\"\n            +\"testClassStrOf\", testClassStr.length() == testClassStrOfR.length());\n\n    }",
            " 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172 +\n 173 +\n 174 +\n 175 +\n 176 +\n 177 +\n 178 +\n 179 +\n 180 +\n 181 +\n 182 +\n 183 +\n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193 +\n 194 +\n 195 +\n 196 +\n 197 +\n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214 +\n 215 +\n 216 +\n 217 +\n 218 +\n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testToStringCollectionLimits() throws Exception {\n        int limit = IgniteSystemProperties.getInteger(IGNITE_TO_STRING_COLLECTION_LIMIT, 100);\n\n        Object vals[] = new Object[] {Byte.MIN_VALUE, Boolean.TRUE, Short.MIN_VALUE, Integer.MIN_VALUE, Long.MIN_VALUE,\n            Float.MIN_VALUE, Double.MIN_VALUE, Character.MIN_VALUE, new TestClass1()};\n        for (Object val : vals)\n            testArr(val, limit);\n\n        byte[] byteArr = new byte[1];\n        byteArr[0] = 1;\n        assertEquals(Arrays.toString(byteArr), GridToStringBuilder.arrayToString(byteArr.getClass(), byteArr));\n        byteArr = Arrays.copyOf(byteArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(byteArr.getClass(), byteArr).contains(\"... and 1 more\"));\n\n        boolean[] boolArr = new boolean[1];\n        boolArr[0] = true;\n        assertEquals(Arrays.toString(boolArr), GridToStringBuilder.arrayToString(boolArr.getClass(), boolArr));\n        boolArr = Arrays.copyOf(boolArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(boolArr.getClass(), boolArr).contains(\"... and 1 more\"));\n\n        short[] shortArr = new short[1];\n        shortArr[0] = 100;\n        assertEquals(Arrays.toString(shortArr), GridToStringBuilder.arrayToString(shortArr.getClass(), shortArr));\n        shortArr = Arrays.copyOf(shortArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(shortArr.getClass(), shortArr).contains(\"... and 1 more\"));\n\n        int[] intArr = new int[1];\n        intArr[0] = 10000;\n        assertEquals(Arrays.toString(intArr), GridToStringBuilder.arrayToString(intArr.getClass(), intArr));\n        intArr = Arrays.copyOf(intArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(intArr.getClass(), intArr).contains(\"... and 1 more\"));\n\n        long[] longArr = new long[1];\n        longArr[0] = 10000000;\n        assertEquals(Arrays.toString(longArr), GridToStringBuilder.arrayToString(longArr.getClass(), longArr));\n        longArr = Arrays.copyOf(longArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(longArr.getClass(), longArr).contains(\"... and 1 more\"));\n\n        float[] floatArr = new float[1];\n        floatArr[0] = 1.f;\n        assertEquals(Arrays.toString(floatArr), GridToStringBuilder.arrayToString(floatArr.getClass(), floatArr));\n        floatArr = Arrays.copyOf(floatArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(floatArr.getClass(), floatArr).contains(\"... and 1 more\"));\n\n        double[] doubleArr = new double[1];\n        doubleArr[0] = 1.;\n        assertEquals(Arrays.toString(doubleArr), GridToStringBuilder.arrayToString(doubleArr.getClass(), doubleArr));\n        doubleArr = Arrays.copyOf(doubleArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(doubleArr.getClass(), doubleArr).contains(\"... and 1 more\"));\n\n        char[] charArr = new char[1];\n        charArr[0] = 'a';\n        assertEquals(Arrays.toString(charArr), GridToStringBuilder.arrayToString(charArr.getClass(), charArr));\n        charArr = Arrays.copyOf(charArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(charArr.getClass(), charArr).contains(\"... and 1 more\"));\n\n        Map<String, String> strMap = new TreeMap<>();\n        List<String> strList = new ArrayList<>(limit+1);\n\n        TestClass1 testClass = new TestClass1();\n        testClass.strMap = strMap;\n        testClass.strListIncl = strList;\n\n        for (int i = 0; i < limit; i++) {\n            strMap.put(\"k\" + i, \"v\");\n            strList.add(\"e\");\n        }\n        String testClassStr = GridToStringBuilder.toString(TestClass1.class, testClass);\n\n        strMap.put(\"kz\", \"v\"); // important to add last element in TreeMap here\n        strList.add(\"e\");\n\n        String testClassStrOf = GridToStringBuilder.toString(TestClass1.class, testClass);\n\n        String testClassStrOfR = testClassStrOf.replaceAll(\"... and 1 more\",\"\");\n\n        assertTrue(\"Collection limit error in Map or List, normal: <\" + testClassStr + \">, overflowed: <\"\n            +\"testClassStrOf\", testClassStr.length() == testClassStrOfR.length());\n\n    }"
        ]
    ],
    "91d77a79c4805e501d8637fc1263866d5e914fba": [
        [
            "GridDhtPartitionTopologyImpl::detectLostPartitions(AffinityTopologyVersion,DiscoveryEvent)",
            "2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  ",
            "    /** {@inheritDoc} */\n    @Override public boolean detectLostPartitions(AffinityTopologyVersion resTopVer, DiscoveryEvent discoEvt) {\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (node2part == null)\n                    return false;\n\n                int parts = grp.affinity().partitions();\n\n                Set<Integer> lost = new HashSet<>(parts);\n\n                for (int p = 0; p < parts; p++)\n                    lost.add(p);\n\n                for (GridDhtPartitionMap partMap : node2part.values()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : partMap.entrySet()) {\n                        if (e.getValue() == OWNING) {\n                            lost.remove(e.getKey());\n\n                            if (lost.isEmpty())\n                                break;\n                        }\n                    }\n                }\n\n                boolean changed = false;\n\n                if (!F.isEmpty(lost)) {\n                    PartitionLossPolicy plc = grp.config().getPartitionLossPolicy();\n\n                    assert plc != null;\n\n                    Set<Integer> recentlyLost = new HashSet<>();\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> leftEntry : leftNode2Part.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> entry : leftEntry.getValue().entrySet()) {\n                            if (entry.getValue() == OWNING)\n                                recentlyLost.add(entry.getKey());\n                        }\n                    }\n\n                    // Update partition state on all nodes.\n                    for (Integer part : lost) {\n                        long updSeq = updateSeq.incrementAndGet();\n\n                        GridDhtLocalPartition locPart = localPartition(part, resTopVer, false, true);\n\n                        if (locPart != null) {\n                            if (locPart.state() == LOST)\n                                continue;\n\n                            boolean marked = plc == PartitionLossPolicy.IGNORE ? locPart.own() : locPart.markLost();\n\n                            if (marked)\n                                updateLocal(locPart.id(), locPart.state(), updSeq, resTopVer);\n\n                            changed |= marked;\n                        }\n                        // Update map for remote node.\n                        else if (plc != PartitionLossPolicy.IGNORE) {\n                            for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) {\n                                if (e.getKey().equals(ctx.localNodeId()))\n                                    continue;\n\n                                if (e.getValue().get(part) != EVICTED)\n                                    e.getValue().put(part, LOST);\n                            }\n                        }\n\n                        if (recentlyLost.contains(part) && grp.eventRecordable(EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST)) {\n                            grp.addRebalanceEvent(part,\n                                EVT_CACHE_REBALANCE_PART_DATA_LOST,\n                                discoEvt.eventNode(),\n                                discoEvt.type(),\n                                discoEvt.timestamp());\n                        }\n                    }\n\n                    if (plc != PartitionLossPolicy.IGNORE)\n                        grp.needsRecovery(true);\n                }\n\n                leftNode2Part.clear();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            "2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  \n2021  \n2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054 +\n2055 +\n2056 +\n2057 +\n2058 +\n2059 +\n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110  \n2111  \n2112  ",
            "    /** {@inheritDoc} */\n    @Override public boolean detectLostPartitions(AffinityTopologyVersion resTopVer, DiscoveryEvent discoEvt) {\n        ctx.database().checkpointReadLock();\n\n        try {\n            lock.writeLock().lock();\n\n            try {\n                if (node2part == null)\n                    return false;\n\n                int parts = grp.affinity().partitions();\n\n                Set<Integer> lost = new HashSet<>(parts);\n\n                for (int p = 0; p < parts; p++)\n                    lost.add(p);\n\n                for (GridDhtPartitionMap partMap : node2part.values()) {\n                    for (Map.Entry<Integer, GridDhtPartitionState> e : partMap.entrySet()) {\n                        if (e.getValue() == OWNING) {\n                            lost.remove(e.getKey());\n\n                            if (lost.isEmpty())\n                                break;\n                        }\n                    }\n                }\n\n                boolean changed = false;\n\n                if (!F.isEmpty(lost)) {\n                    PartitionLossPolicy plc = grp.config().getPartitionLossPolicy();\n\n                    assert plc != null;\n\n                    Set<Integer> recentlyLost = new HashSet<>();\n\n                    for (Map.Entry<UUID, GridDhtPartitionMap> leftEntry : leftNode2Part.entrySet()) {\n                        for (Map.Entry<Integer, GridDhtPartitionState> entry : leftEntry.getValue().entrySet()) {\n                            if (entry.getValue() == OWNING)\n                                recentlyLost.add(entry.getKey());\n                        }\n                    }\n\n                    if (!recentlyLost.isEmpty()) {\n                        U.warn(log, \"Detected lost partitions [grp=\" + grp.cacheOrGroupName()\n                            + \", parts=\" + S.compact(recentlyLost)\n                            + \", plc=\" + plc + \"]\");\n                    }\n\n                    // Update partition state on all nodes.\n                    for (Integer part : lost) {\n                        long updSeq = updateSeq.incrementAndGet();\n\n                        GridDhtLocalPartition locPart = localPartition(part, resTopVer, false, true);\n\n                        if (locPart != null) {\n                            if (locPart.state() == LOST)\n                                continue;\n\n                            boolean marked = plc == PartitionLossPolicy.IGNORE ? locPart.own() : locPart.markLost();\n\n                            if (marked)\n                                updateLocal(locPart.id(), locPart.state(), updSeq, resTopVer);\n\n                            changed |= marked;\n                        }\n                        // Update map for remote node.\n                        else if (plc != PartitionLossPolicy.IGNORE) {\n                            for (Map.Entry<UUID, GridDhtPartitionMap> e : node2part.entrySet()) {\n                                if (e.getKey().equals(ctx.localNodeId()))\n                                    continue;\n\n                                if (e.getValue().get(part) != EVICTED)\n                                    e.getValue().put(part, LOST);\n                            }\n                        }\n\n                        if (recentlyLost.contains(part) && grp.eventRecordable(EventType.EVT_CACHE_REBALANCE_PART_DATA_LOST)) {\n                            grp.addRebalanceEvent(part,\n                                EVT_CACHE_REBALANCE_PART_DATA_LOST,\n                                discoEvt.eventNode(),\n                                discoEvt.type(),\n                                discoEvt.timestamp());\n                        }\n                    }\n\n                    if (plc != PartitionLossPolicy.IGNORE)\n                        grp.needsRecovery(true);\n                }\n\n                leftNode2Part.clear();\n\n                return changed;\n            }\n            finally {\n                lock.writeLock().unlock();\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ]
    ],
    "4924ed430f0adfd9385414d7e341393a00577d1a": [
        [
            "TxRollbackAsyncTest::testRollbackOnTopologyLockPessimistic()",
            " 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903 -\n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  ",
            "    /**\n     *\n     */\n    public void testRollbackOnTopologyLockPessimistic() throws Exception {\n        final Ignite client = startClient();\n\n        Ignite crd = grid(0);\n\n        List<Integer> keys = primaryKeys(grid(1).cache(CACHE_NAME), 1);\n\n        assertTrue(crd.cluster().localNode().order() == 1);\n\n        CountDownLatch txLatch = new CountDownLatch(1);\n        CountDownLatch tx2Latch = new CountDownLatch(1);\n        CountDownLatch commitLatch = new CountDownLatch(1);\n\n        // Start tx holding topology.\n        IgniteInternalFuture txFut = runAsync(new Runnable() {\n            @Override public void run() {\n                List<Integer> keys = primaryKeys(grid(0).cache(CACHE_NAME), 1);\n\n                try (Transaction tx = client.transactions().txStart()) {\n                    client.cache(CACHE_NAME).put(keys.get(0), 0);\n\n                    txLatch.countDown();\n\n                    U.awaitQuiet(commitLatch);\n\n                    tx.commit();\n\n                    fail();\n                }\n                catch (Exception e) {\n                    // Expected.\n                }\n            }\n        });\n\n        U.awaitQuiet(txLatch);\n\n        crd.events().localListen(new IgnitePredicate<Event>() {\n            @Override public boolean apply(Event evt) {\n                runAsync(new Runnable() {\n                    @Override public void run() {\n                        try(Transaction tx = crd.transactions().withLabel(\"testLbl\").txStart()) {\n                            // Wait for node start.\n                            waitForCondition(new GridAbsPredicate() {\n                                @Override public boolean apply() {\n                                    return crd.cluster().topologyVersion() != GRID_CNT +\n                                        /** client node */ 1  + /** stop server node */ 1 + /** start server node */ 1;\n                                }\n                            }, 10_000);\n\n                            tx2Latch.countDown();\n\n                            crd.cache(CACHE_NAME).put(keys.get(0), 0);\n\n                            tx.commit();\n\n                            fail();\n                        }\n                        catch (Exception e) {\n                            // Expected.\n                        }\n                    }\n                });\n\n                return false;\n            }\n        }, EventType.EVT_NODE_FAILED, EventType.EVT_NODE_LEFT);\n\n        IgniteInternalFuture restartFut = runAsync(new Runnable() {\n            @Override public void run() {\n                stopGrid(2);\n\n                try {\n                    startGrid(2);\n                }\n                catch (Exception e) {\n                    fail();\n                }\n            }\n        });\n\n        U.awaitQuiet(tx2Latch);\n\n        // Rollback tx using kill task.\n        VisorTxTaskArg arg =\n            new VisorTxTaskArg(VisorTxOperation.KILL, null, null, null, null, null, null, null, null, null);\n\n        Map<ClusterNode, VisorTxTaskResult> res = client.compute(client.cluster().forPredicate(F.alwaysTrue())).\n            execute(new VisorTxTask(), new VisorTaskArgument<>(client.cluster().localNode().id(), arg, false));\n\n        int expCnt = 0;\n\n        for (Map.Entry<ClusterNode, VisorTxTaskResult> entry : res.entrySet()) {\n            if (entry.getValue().getInfos().isEmpty())\n                continue;\n\n            for (VisorTxInfo info : entry.getValue().getInfos()) {\n                log.info(info.toUserString());\n\n                expCnt++;\n            }\n        }\n\n        assertEquals(\"Expecting 2 transactions\", 2, expCnt);\n\n        commitLatch.countDown();\n\n        txFut.get();\n        restartFut.get();\n\n        checkFutures();\n    }",
            " 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904 +\n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935 +\n 936 +\n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  ",
            "    /**\n     *\n     */\n    public void testRollbackOnTopologyLockPessimistic() throws Exception {\n        final Ignite client = startClient();\n\n        Ignite crd = grid(0);\n\n        List<Integer> keys = primaryKeys(grid(1).cache(CACHE_NAME), 1);\n\n        assertTrue(crd.cluster().localNode().order() == 1);\n\n        CountDownLatch txLatch = new CountDownLatch(1);\n        CountDownLatch tx2Latch = new CountDownLatch(1);\n        CountDownLatch commitLatch = new CountDownLatch(1);\n\n        // Start tx holding topology.\n        IgniteInternalFuture txFut = runAsync(new Runnable() {\n            @Override public void run() {\n                List<Integer> keys = primaryKeys(grid(0).cache(CACHE_NAME), 1);\n\n                try (Transaction tx = client.transactions().txStart()) {\n                    client.cache(CACHE_NAME).put(keys.get(0), 0);\n\n                    txLatch.countDown();\n\n                    assertTrue(U.await(commitLatch, 10, TimeUnit.SECONDS));\n\n                    tx.commit();\n\n                    fail();\n                }\n                catch (Exception e) {\n                    // Expected.\n                }\n            }\n        });\n\n        U.awaitQuiet(txLatch);\n\n        crd.events().localListen(new IgnitePredicate<Event>() {\n            @Override public boolean apply(Event evt) {\n                runAsync(new Runnable() {\n                    @Override public void run() {\n                        try(Transaction tx = crd.transactions().withLabel(\"testLbl\").txStart()) {\n                            // Wait for node start.\n                            waitForCondition(new GridAbsPredicate() {\n                                @Override public boolean apply() {\n                                    return crd.cluster().topologyVersion() != GRID_CNT +\n                                        /** client node */ 1  + /** stop server node */ 1 + /** start server node */ 1;\n                                }\n                            }, 10_000);\n\n                            tx2Latch.countDown();\n\n                            crd.cache(CACHE_NAME).put(keys.get(0), 0);\n\n                            assertTrue(U.await(commitLatch, 10, TimeUnit.SECONDS));\n\n                            tx.commit();\n\n                            fail();\n                        }\n                        catch (Exception e) {\n                            // Expected.\n                        }\n                    }\n                });\n\n                return false;\n            }\n        }, EventType.EVT_NODE_FAILED, EventType.EVT_NODE_LEFT);\n\n        IgniteInternalFuture restartFut = runAsync(new Runnable() {\n            @Override public void run() {\n                stopGrid(2);\n\n                try {\n                    startGrid(2);\n                }\n                catch (Exception e) {\n                    fail();\n                }\n            }\n        });\n\n        U.awaitQuiet(tx2Latch);\n\n        // Rollback tx using kill task.\n        VisorTxTaskArg arg =\n            new VisorTxTaskArg(VisorTxOperation.KILL, null, null, null, null, null, null, null, null, null);\n\n        Map<ClusterNode, VisorTxTaskResult> res = client.compute(client.cluster().forPredicate(F.alwaysTrue())).\n            execute(new VisorTxTask(), new VisorTaskArgument<>(client.cluster().localNode().id(), arg, false));\n\n        int expCnt = 0;\n\n        for (Map.Entry<ClusterNode, VisorTxTaskResult> entry : res.entrySet()) {\n            if (entry.getValue().getInfos().isEmpty())\n                continue;\n\n            for (VisorTxInfo info : entry.getValue().getInfos()) {\n                log.info(info.toUserString());\n\n                expCnt++;\n            }\n        }\n\n        assertEquals(\"Expecting 2 transactions\", 2, expCnt);\n\n        commitLatch.countDown();\n\n        txFut.get();\n        restartFut.get();\n\n        checkFutures();\n    }"
        ]
    ],
    "34ca49abfdd40cb8a00172ed9d433c454a8febf7": [
        [
            "GridCacheProcessor::setTxOwnerDumpRequestsAllowed(boolean)",
            "5777  \n5778  \n5779  \n5780  \n5781  \n5782  \n5783  \n5784  \n5785 -\n5786  \n5787  \n5788  ",
            "    /**\n     * Sets if dump requests from local node to near node are allowed, when long running transaction\n     * is found. If allowed, the compute request to near node will be made to get thread dump of transaction\n     * owner thread. Also broadcasts this setting on other server nodes in cluster.\n     *\n     * @param allowed whether allowed\n     */\n    public void setTxOwnerDumpRequestsAllowed(boolean allowed) {\n        IgniteCompute compute = ctx.grid().compute(ctx.grid().cluster().forServers());\n\n        compute.broadcast(new TxOwnerDumpRequestAllowedSettingClosure(allowed));\n    }",
            "5779  \n5780  \n5781  \n5782  \n5783  \n5784  \n5785  \n5786  \n5787 +\n5788 +\n5789 +\n5790 +\n5791 +\n5792 +\n5793  \n5794  \n5795  ",
            "    /**\n     * Sets if dump requests from local node to near node are allowed, when long running transaction\n     * is found. If allowed, the compute request to near node will be made to get thread dump of transaction\n     * owner thread. Also broadcasts this setting on other server nodes in cluster.\n     *\n     * @param allowed whether allowed\n     */\n    public void setTxOwnerDumpRequestsAllowed(boolean allowed) {\n        ClusterGroup grp = ctx.grid()\n            .cluster()\n            .forServers()\n            .forPredicate(node -> IgniteFeatures.nodeSupports(node, TRANSACTION_OWNER_THREAD_DUMP_PROVIDING));\n\n        IgniteCompute compute = ctx.grid().compute(grp);\n\n        compute.broadcast(new TxOwnerDumpRequestAllowedSettingClosure(allowed));\n    }"
        ],
        [
            "GridCachePartitionExchangeManager::dumpLongRunningTransaction(IgniteInternalTx)",
            "2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102 -\n2103  \n2104 -\n2105 -\n2106 -\n2107 -\n2108 -\n2109 -\n2110  \n2111 -\n2112 -\n2113 -\n2114 -\n2115 -\n2116 -\n2117 -\n2118 -\n2119 -\n2120  \n2121 -\n2122 -\n2123 -\n2124 -\n2125 -\n2126 -\n2127 -\n2128 -\n2129 -\n2130  \n2131 -\n2132 -\n2133  \n2134 -\n2135 -\n2136  \n2137  \n2138  \n2139  ",
            "    /**\n     * Dumps long running transaction. If transaction is active and is not near, sends compute request\n     * to near node to get the stack trace of transaction owner thread.\n     *\n     * @param tx Transaction.\n     */\n    private void dumpLongRunningTransaction(IgniteInternalTx tx) {\n        if (cctx.tm().txOwnerDumpRequestsAllowed() && tx.local() && tx.state() == TransactionState.ACTIVE) {\n            Collection<UUID> masterNodeIds = tx.masterNodeIds();\n\n            if (masterNodeIds.size() == 1) {\n                UUID nearNodeId = masterNodeIds.iterator().next();\n\n                long txOwnerThreadId = tx.threadId();\n\n                Ignite ignite = cctx.kernalContext().grid();\n\n                IgniteCompute compute = ignite.compute(ignite.cluster().forNodeId(nearNodeId));\n\n                try {\n                    compute\n                        .callAsync(new FetchActiveTxOwnerTraceClosure(txOwnerThreadId))\n                        .listen(new IgniteInClosure<IgniteFuture<String>>() {\n                            @Override public void apply(IgniteFuture<String> strIgniteFut) {\n                                String traceDump = null;\n\n                                try {\n                                    traceDump = strIgniteFut.get();\n                                }\n                                catch (Exception e) {\n                                    U.warn(\n                                        diagnosticLog,\n                                        \"Could not get thread dump from transaction owner near node: \" + e.getMessage()\n                                    );\n                                }\n\n                                if (traceDump != null) {\n                                    U.warn(\n                                        diagnosticLog,\n                                        String.format(\n                                            \"Dumping the near node thread that started transaction [xidVer=%s]\\n%s\",\n                                            tx.xidVersion().toString(),\n                                            traceDump\n                                        )\n                                    );\n                                }\n                            }\n                        });\n                }\n                catch (Exception e) {\n                    U.warn(diagnosticLog, \"Could not send dump request to transaction owner near node: \" + e.getMessage());\n                }\n            }\n        }\n    }",
            "2088  \n2089  \n2090  \n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105 +\n2106  \n2107 +\n2108 +\n2109  \n2110 +\n2111 +\n2112 +\n2113 +\n2114 +\n2115 +\n2116  \n2117 +\n2118 +\n2119 +\n2120 +\n2121 +\n2122 +\n2123 +\n2124 +\n2125 +\n2126 +\n2127 +\n2128 +\n2129 +\n2130 +\n2131 +\n2132 +\n2133 +\n2134 +\n2135 +\n2136 +\n2137 +\n2138  \n2139 +\n2140 +\n2141 +\n2142 +\n2143 +\n2144  \n2145 +\n2146 +\n2147 +\n2148 +",
            "    /**\n     * Dumps long running transaction. If transaction is active and is not near, sends compute request\n     * to near node to get the stack trace of transaction owner thread.\n     *\n     * @param tx Transaction.\n     */\n    private void dumpLongRunningTransaction(IgniteInternalTx tx) {\n        if (cctx.tm().txOwnerDumpRequestsAllowed() && tx.local() && tx.state() == TransactionState.ACTIVE) {\n            Collection<UUID> masterNodeIds = tx.masterNodeIds();\n\n            if (masterNodeIds.size() == 1) {\n                UUID nearNodeId = masterNodeIds.iterator().next();\n\n                long txOwnerThreadId = tx.threadId();\n\n                Ignite ignite = cctx.kernalContext().grid();\n\n                ClusterGroup nearNode = ignite.cluster().forNodeId(nearNodeId);\n\n                if (allNodesSupports(nearNode.nodes(), TRANSACTION_OWNER_THREAD_DUMP_PROVIDING)) {\n                    IgniteCompute compute = ignite.compute(ignite.cluster().forNodeId(nearNodeId));\n\n                    try {\n                        compute\n                            .callAsync(new FetchActiveTxOwnerTraceClosure(txOwnerThreadId))\n                            .listen(new IgniteInClosure<IgniteFuture<String>>() {\n                                @Override public void apply(IgniteFuture<String> strIgniteFut) {\n                                    String traceDump = null;\n\n                                    try {\n                                        traceDump = strIgniteFut.get();\n                                    }\n                                    catch (Exception e) {\n                                        U.error(\n                                            diagnosticLog,\n                                            \"Could not get thread dump from transaction owner near node: \",\n                                            e\n                                        );\n                                    }\n\n                                    if (traceDump != null) {\n                                        U.warn(\n                                            diagnosticLog,\n                                            String.format(\n                                                \"Dumping the near node thread that started transaction [xidVer=%s]\\n%s\",\n                                                tx.xidVersion().toString(),\n                                                traceDump\n                                            )\n                                        );\n                                    }\n                                }\n                            });\n                    }\n                    catch (Exception e) {\n                        U.error(diagnosticLog, \"Could not send dump request to transaction owner near node: \", e);\n                    }\n                }\n                else {\n                    U.warn(\n                        diagnosticLog,\n                        \"Could not send dump request to transaction owner near node: node does not support this feature.\""
        ]
    ],
    "36532308292dcd8aa8139c4c2642f7f7b4640961": [
        [
            "IgniteTxManager::NodeFailureTimeoutObject::onTimeout0()",
            "2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544 -\n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  ",
            "        /**\n         *\n         */\n        private void onTimeout0() {\n            try {\n                cctx.kernalContext().gateway().readLock();\n            }\n            catch (IllegalStateException | IgniteClientDisconnectedException e) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Failed to acquire kernal gateway [err=\" + e + ']');\n\n                return;\n            }\n\n            UUID evtNodeId = node.id();\n\n            try {\n                if (log.isDebugEnabled())\n                    log.debug(\"Processing node failed event [locNodeId=\" + cctx.localNodeId() +\n                        \", failedNodeId=\" + evtNodeId + ']');\n\n                // Null means that recovery voting is not needed.\n                GridCompoundFuture<IgniteInternalTx, Void> allTxFinFut = node.isClient() && mvccCrd != null\n                    ? new GridCompoundFuture<>() : null;\n\n                for (final IgniteInternalTx tx : activeTransactions()) {\n                    if ((tx.near() && !tx.local()) || (tx.storeWriteThrough() && tx.masterNodeIds().contains(evtNodeId))) {\n                        // Invalidate transactions.\n                        salvageTx(tx, RECOVERY_FINISH);\n                    }\n                    else {\n                        // Check prepare only if originating node ID failed. Otherwise parent node will finish this tx.\n                        if (tx.originatingNodeId().equals(evtNodeId)) {\n                            if (tx.state() == PREPARED)\n                                commitIfPrepared(tx, Collections.singleton(evtNodeId));\n                            else {\n                                IgniteInternalFuture<?> prepFut = tx.currentPrepareFuture();\n\n                                if (prepFut != null) {\n                                    prepFut.listen(fut -> {\n                                        if (tx.state() == PREPARED)\n                                            commitIfPrepared(tx, Collections.singleton(evtNodeId));\n                                            // If we could not mark tx as rollback, it means that transaction is being committed.\n                                        else if (tx.setRollbackOnly())\n                                            tx.rollbackAsync();\n                                    });\n                                }\n                                // If we could not mark tx as rollback, it means that transaction is being committed.\n                                else if (tx.setRollbackOnly())\n                                    tx.rollbackAsync();\n                            }\n                        }\n\n                        // Await only mvcc transactions initiated by failed client node.\n                        if (allTxFinFut != null && tx.eventNodeId().equals(evtNodeId)\n                            && tx.mvccSnapshot() != null)\n                            allTxFinFut.add(tx.finishFuture());\n                    }\n                }\n\n                if (allTxFinFut == null)\n                    return;\n\n                allTxFinFut.markInitialized();\n\n                // Send vote to mvcc coordinator when all recovering transactions have finished.\n                allTxFinFut.listen(fut -> {\n                    // If mvcc coordinator issued snapshot for recovering transaction has failed during recovery,\n                    // then there is no need to send messages to new coordinator.\n                    try {\n                        cctx.kernalContext().io().sendToGridTopic(\n                            mvccCrd.nodeId(),\n                            TOPIC_CACHE_COORDINATOR,\n                            new MvccRecoveryFinishedMessage(evtNodeId),\n                            SYSTEM_POOL);\n                    }\n                    catch (ClusterTopologyCheckedException e) {\n                        if (log.isInfoEnabled())\n                            log.info(\"Mvcc coordinator issued snapshots for recovering transactions \" +\n                                \"has left the cluster (will ignore) [locNodeId=\" + cctx.localNodeId() +\n                                    \", failedNodeId=\" + evtNodeId +\n                                    \", mvccCrdNodeId=\" + mvccCrd.nodeId() + ']');\n                    }\n                    catch (IgniteCheckedException e) {\n                        log.warning(\"Failed to notify mvcc coordinator that all recovering transactions were \" +\n                            \"finished [locNodeId=\" + cctx.localNodeId() +\n                            \", failedNodeId=\" + evtNodeId +\n                            \", mvccCrdNodeId=\" + mvccCrd.nodeId() + ']', e);\n                    }\n                });\n            }\n            finally {\n                cctx.kernalContext().gateway().readUnlock();\n            }\n        }",
            "2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544 +\n2545 +\n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555  \n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  \n2582  \n2583  \n2584  \n2585  \n2586  \n2587  \n2588  \n2589  \n2590  \n2591  \n2592  \n2593  \n2594  \n2595  \n2596  \n2597  \n2598  \n2599  \n2600  \n2601  \n2602  \n2603  \n2604  \n2605  \n2606  \n2607  \n2608  \n2609  \n2610  \n2611  \n2612  \n2613  \n2614  \n2615  \n2616  \n2617  ",
            "        /**\n         *\n         */\n        private void onTimeout0() {\n            try {\n                cctx.kernalContext().gateway().readLock();\n            }\n            catch (IllegalStateException | IgniteClientDisconnectedException e) {\n                if (log.isDebugEnabled())\n                    log.debug(\"Failed to acquire kernal gateway [err=\" + e + ']');\n\n                return;\n            }\n\n            UUID evtNodeId = node.id();\n\n            try {\n                if (log.isDebugEnabled())\n                    log.debug(\"Processing node failed event [locNodeId=\" + cctx.localNodeId() +\n                        \", failedNodeId=\" + evtNodeId + ']');\n\n                // Null means that recovery voting is not needed.\n                GridCompoundFuture<IgniteInternalTx, Void> allTxFinFut =\n                    node.isClient() && mvccCrd != null && mvccCrd.nodeId() != null\n                    ? new GridCompoundFuture<>() : null;\n\n                for (final IgniteInternalTx tx : activeTransactions()) {\n                    if ((tx.near() && !tx.local()) || (tx.storeWriteThrough() && tx.masterNodeIds().contains(evtNodeId))) {\n                        // Invalidate transactions.\n                        salvageTx(tx, RECOVERY_FINISH);\n                    }\n                    else {\n                        // Check prepare only if originating node ID failed. Otherwise parent node will finish this tx.\n                        if (tx.originatingNodeId().equals(evtNodeId)) {\n                            if (tx.state() == PREPARED)\n                                commitIfPrepared(tx, Collections.singleton(evtNodeId));\n                            else {\n                                IgniteInternalFuture<?> prepFut = tx.currentPrepareFuture();\n\n                                if (prepFut != null) {\n                                    prepFut.listen(fut -> {\n                                        if (tx.state() == PREPARED)\n                                            commitIfPrepared(tx, Collections.singleton(evtNodeId));\n                                            // If we could not mark tx as rollback, it means that transaction is being committed.\n                                        else if (tx.setRollbackOnly())\n                                            tx.rollbackAsync();\n                                    });\n                                }\n                                // If we could not mark tx as rollback, it means that transaction is being committed.\n                                else if (tx.setRollbackOnly())\n                                    tx.rollbackAsync();\n                            }\n                        }\n\n                        // Await only mvcc transactions initiated by failed client node.\n                        if (allTxFinFut != null && tx.eventNodeId().equals(evtNodeId)\n                            && tx.mvccSnapshot() != null)\n                            allTxFinFut.add(tx.finishFuture());\n                    }\n                }\n\n                if (allTxFinFut == null)\n                    return;\n\n                allTxFinFut.markInitialized();\n\n                // Send vote to mvcc coordinator when all recovering transactions have finished.\n                allTxFinFut.listen(fut -> {\n                    // If mvcc coordinator issued snapshot for recovering transaction has failed during recovery,\n                    // then there is no need to send messages to new coordinator.\n                    try {\n                        cctx.kernalContext().io().sendToGridTopic(\n                            mvccCrd.nodeId(),\n                            TOPIC_CACHE_COORDINATOR,\n                            new MvccRecoveryFinishedMessage(evtNodeId),\n                            SYSTEM_POOL);\n                    }\n                    catch (ClusterTopologyCheckedException e) {\n                        if (log.isInfoEnabled())\n                            log.info(\"Mvcc coordinator issued snapshots for recovering transactions \" +\n                                \"has left the cluster (will ignore) [locNodeId=\" + cctx.localNodeId() +\n                                    \", failedNodeId=\" + evtNodeId +\n                                    \", mvccCrdNodeId=\" + mvccCrd.nodeId() + ']');\n                    }\n                    catch (IgniteCheckedException e) {\n                        log.warning(\"Failed to notify mvcc coordinator that all recovering transactions were \" +\n                            \"finished [locNodeId=\" + cctx.localNodeId() +\n                            \", failedNodeId=\" + evtNodeId +\n                            \", mvccCrdNodeId=\" + mvccCrd.nodeId() + ']', e);\n                    }\n                });\n            }\n            finally {\n                cctx.kernalContext().gateway().readUnlock();\n            }\n        }"
        ],
        [
            "MvccProcessorImpl::onDiscovery(DiscoveryEvent,DiscoCache)",
            " 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455 -\n 456 -\n 457  \n 458  \n 459  \n 460  \n 461  ",
            "    /**\n     * Discovery listener. Note: initial join event is handled by {@link MvccProcessorImpl#onLocalJoin}\n     * method.\n     *\n     * @param evt Discovery event.\n     */\n    private void onDiscovery(DiscoveryEvent evt, DiscoCache discoCache) {\n        assert evt.type() == EVT_NODE_FAILED\n            || evt.type() == EVT_NODE_LEFT\n            || evt.type() == EVT_NODE_JOINED;\n\n        UUID nodeId = evt.eventNode().id();\n        AffinityTopologyVersion topVer = discoCache.version();\n        List<ClusterNode> nodes = discoCache.allNodes();\n\n        checkMvccSupported(nodes);\n\n        MvccCoordinator curCrd0 = curCrd;\n\n        if (evt.type() == EVT_NODE_JOINED) {\n            if (curCrd0.disconnected()) // Handle join event only if coordinator has not been elected yet.\n                onCoordinatorChanged(topVer, nodes, false);\n        }\n        else if (Objects.equals(nodeId, curCrd0.nodeId())) {\n            // 1. Notify all listeners waiting for a snapshot.\n            onCoordinatorFailed(nodeId);\n\n            // 2. Process coordinator change.\n            onCoordinatorChanged(topVer, nodes, true);\n        }\n        // Process node left event on the current mvcc coordinator.\n        else if (curCrd0.local()) {\n            // 1. Notify active queries.\n            activeQueries.onNodeFailed(nodeId);\n\n            // 2. Notify previous queries.\n            prevCrdQueries.onNodeFailed(nodeId);\n\n            // 3. Recover transactions started by the failed node.\n            recoveryBallotBoxes.forEach((nearNodeId, ballotBox) -> {\n                // Put synthetic vote from another failed node\n                ballotBox.vote(nodeId);\n\n                tryFinishRecoveryVoting(nearNodeId, ballotBox);\n            });\n\n            if (evt.eventNode().isClient()) {\n                RecoveryBallotBox ballotBox = recoveryBallotBoxes\n                    .computeIfAbsent(nodeId, uuid -> new RecoveryBallotBox());\n\n                ballotBox\n                    .voters(evt.topologyNodes().stream().map(ClusterNode::id).collect(Collectors.toList()));\n\n                tryFinishRecoveryVoting(nodeId, ballotBox);\n            }\n        }\n    }",
            " 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455 +\n 456 +\n 457 +\n 458 +\n 459 +\n 460  \n 461  \n 462  \n 463  \n 464  ",
            "    /**\n     * Discovery listener. Note: initial join event is handled by {@link MvccProcessorImpl#onLocalJoin}\n     * method.\n     *\n     * @param evt Discovery event.\n     */\n    private void onDiscovery(DiscoveryEvent evt, DiscoCache discoCache) {\n        assert evt.type() == EVT_NODE_FAILED\n            || evt.type() == EVT_NODE_LEFT\n            || evt.type() == EVT_NODE_JOINED;\n\n        UUID nodeId = evt.eventNode().id();\n        AffinityTopologyVersion topVer = discoCache.version();\n        List<ClusterNode> nodes = discoCache.allNodes();\n\n        checkMvccSupported(nodes);\n\n        MvccCoordinator curCrd0 = curCrd;\n\n        if (evt.type() == EVT_NODE_JOINED) {\n            if (curCrd0.disconnected()) // Handle join event only if coordinator has not been elected yet.\n                onCoordinatorChanged(topVer, nodes, false);\n        }\n        else if (Objects.equals(nodeId, curCrd0.nodeId())) {\n            // 1. Notify all listeners waiting for a snapshot.\n            onCoordinatorFailed(nodeId);\n\n            // 2. Process coordinator change.\n            onCoordinatorChanged(topVer, nodes, true);\n        }\n        // Process node left event on the current mvcc coordinator.\n        else if (curCrd0.local()) {\n            // 1. Notify active queries.\n            activeQueries.onNodeFailed(nodeId);\n\n            // 2. Notify previous queries.\n            prevCrdQueries.onNodeFailed(nodeId);\n\n            // 3. Recover transactions started by the failed node.\n            recoveryBallotBoxes.forEach((nearNodeId, ballotBox) -> {\n                // Put synthetic vote from another failed node\n                ballotBox.vote(nodeId);\n\n                tryFinishRecoveryVoting(nearNodeId, ballotBox);\n            });\n\n            if (evt.eventNode().isClient()) {\n                RecoveryBallotBox ballotBox = recoveryBallotBoxes\n                    .computeIfAbsent(nodeId, uuid -> new RecoveryBallotBox());\n\n                ballotBox.voters(evt.topologyNodes().stream()\n                    // Nodes not supporting MVCC will never send votes to us. So, filter them away.\n                    .filter(this::supportsMvcc)\n                    .map(ClusterNode::id)\n                    .collect(Collectors.toList()));\n\n                tryFinishRecoveryVoting(nodeId, ballotBox);\n            }\n        }\n    }"
        ]
    ],
    "e95c7b1d394606255afeb7d1b9d280c0b5ffd470": [
        [
            "IgnitionEx::IgniteNamedInstance::initializeConfiguration(IgniteConfiguration)",
            "2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142  \n2143 -\n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  ",
            "        /**\n         * @param cfg Ignite configuration copy to.\n         * @return New ignite configuration.\n         * @throws IgniteCheckedException If failed.\n         */\n        private IgniteConfiguration initializeConfiguration(IgniteConfiguration cfg)\n            throws IgniteCheckedException {\n            IgniteConfiguration myCfg = new IgniteConfiguration(cfg);\n\n            String ggHome = cfg.getIgniteHome();\n\n            // Set Ignite home.\n            if (ggHome == null)\n                ggHome = U.getIgniteHome();\n            else\n                // If user provided IGNITE_HOME - set it as a system property.\n                U.setIgniteHome(ggHome);\n\n            // Correctly resolve work directory and set it back to configuration.\n            String workDir = U.workDirectory(cfg.getWorkDirectory(), ggHome);\n\n            myCfg.setWorkDirectory(workDir);\n\n            // Ensure invariant.\n            // It's a bit dirty - but this is a result of late refactoring\n            // and I don't want to reshuffle a lot of code.\n            assert F.eq(name, cfg.getIgniteInstanceName());\n\n            UUID nodeId = cfg.getNodeId() != null ? cfg.getNodeId() : UUID.randomUUID();\n\n            myCfg.setNodeId(nodeId);\n\n            String predefineConsistentId = IgniteSystemProperties.getString(IGNITE_OVERRIDE_CONSISTENT_ID);\n\n            if (!F.isEmpty(predefineConsistentId))\n                myCfg.setConsistentId(predefineConsistentId);\n\n            IgniteLogger cfgLog = initLogger(cfg.getGridLogger(), nodeId, workDir);\n\n            assert cfgLog != null;\n\n            cfgLog = new GridLoggerProxy(cfgLog, null, name, U.id8(nodeId));\n\n            // Initialize factory's log.\n            log = cfgLog.getLogger(G.class);\n\n            myCfg.setGridLogger(cfgLog);\n\n            // Check Ignite home folder (after log is available).\n            if (ggHome != null) {\n                File ggHomeFile = new File(ggHome);\n\n                if (!ggHomeFile.exists() || !ggHomeFile.isDirectory())\n                    throw new IgniteCheckedException(\"Invalid Ignite installation home folder: \" + ggHome);\n            }\n\n            myCfg.setIgniteHome(ggHome);\n\n            // Validate segmentation configuration.\n            SegmentationPolicy segPlc = cfg.getSegmentationPolicy();\n\n            // 1. Warn on potential configuration problem: grid is not configured to wait\n            // for correct segment after segmentation happens.\n            if (!F.isEmpty(cfg.getSegmentationResolvers()) && segPlc == RESTART_JVM && !cfg.isWaitForSegmentOnStart()) {\n                U.warn(log, \"Found potential configuration problem (forgot to enable waiting for segment\" +\n                    \"on start?) [segPlc=\" + segPlc + \", wait=false]\");\n            }\n\n            if (CU.isPersistenceEnabled(cfg) && myCfg.getConsistentId() == null)\n                U.warn(log, \"Consistent ID is not set, it is recommended to set consistent ID for production \" +\n                    \"clusters (use IgniteConfiguration.setConsistentId property)\");\n\n            myCfg.setTransactionConfiguration(myCfg.getTransactionConfiguration() != null ?\n                new TransactionConfiguration(myCfg.getTransactionConfiguration()) : null);\n\n            myCfg.setConnectorConfiguration(myCfg.getConnectorConfiguration() != null ?\n                new ConnectorConfiguration(myCfg.getConnectorConfiguration()) : null);\n\n            // Local host.\n            String locHost = IgniteSystemProperties.getString(IGNITE_LOCAL_HOST);\n\n            myCfg.setLocalHost(F.isEmpty(locHost) ? myCfg.getLocalHost() : locHost);\n\n            // Override daemon flag if it was set on the factory.\n            if (daemon.get())\n                myCfg.setDaemon(true);\n\n            if (myCfg.isClientMode() == null) {\n                Boolean threadClient = clientMode.get();\n\n                if (threadClient == null)\n                    myCfg.setClientMode(IgniteSystemProperties.getBoolean(IGNITE_CACHE_CLIENT, false));\n                else\n                    myCfg.setClientMode(threadClient);\n            }\n\n            // Check for deployment mode override.\n            String depModeName = IgniteSystemProperties.getString(IGNITE_DEP_MODE_OVERRIDE);\n\n            if (!F.isEmpty(depModeName)) {\n                if (!F.isEmpty(myCfg.getCacheConfiguration())) {\n                    U.quietAndInfo(log, \"Skipping deployment mode override for caches (custom closure \" +\n                        \"execution may not work for console Visor)\");\n                }\n                else {\n                    try {\n                        DeploymentMode depMode = DeploymentMode.valueOf(depModeName);\n\n                        if (myCfg.getDeploymentMode() != depMode)\n                            myCfg.setDeploymentMode(depMode);\n                    }\n                    catch (IllegalArgumentException e) {\n                        throw new IgniteCheckedException(\"Failed to override deployment mode using system property \" +\n                            \"(are there any misspellings?)\" +\n                            \"[name=\" + IGNITE_DEP_MODE_OVERRIDE + \", value=\" + depModeName + ']', e);\n                    }\n                }\n            }\n\n            if (myCfg.getUserAttributes() == null)\n                myCfg.setUserAttributes(Collections.<String, Object>emptyMap());\n\n            if (myCfg.getMBeanServer() == null && !U.IGNITE_MBEANS_DISABLED)\n                myCfg.setMBeanServer(ManagementFactory.getPlatformMBeanServer());\n\n            Marshaller marsh = myCfg.getMarshaller();\n\n            if (marsh == null) {\n                if (!BinaryMarshaller.available()) {\n                    U.warn(log, \"Standard BinaryMarshaller can't be used on this JVM. \" +\n                        \"Switch to HotSpot JVM or reach out Apache Ignite community for recommendations.\");\n\n                    marsh = new JdkMarshaller();\n                }\n                else\n                    marsh = new BinaryMarshaller();\n            }\n\n            MarshallerUtils.setNodeName(marsh, cfg.getIgniteInstanceName());\n\n            myCfg.setMarshaller(marsh);\n\n            if (myCfg.getPeerClassLoadingLocalClassPathExclude() == null)\n                myCfg.setPeerClassLoadingLocalClassPathExclude(EMPTY_STR_ARR);\n\n            FileSystemConfiguration[] igfsCfgs = myCfg.getFileSystemConfiguration();\n\n            if (igfsCfgs != null) {\n                FileSystemConfiguration[] clone = igfsCfgs.clone();\n\n                for (int i = 0; i < igfsCfgs.length; i++)\n                    clone[i] = new FileSystemConfiguration(igfsCfgs[i]);\n\n                myCfg.setFileSystemConfiguration(clone);\n            }\n\n            initializeDefaultSpi(myCfg);\n\n            GridDiscoveryManager.initCommunicationErrorResolveConfiguration(myCfg);\n\n            initializeDefaultCacheConfiguration(myCfg);\n\n            ExecutorConfiguration[] execCfgs = myCfg.getExecutorConfiguration();\n\n            if (execCfgs != null) {\n                ExecutorConfiguration[] clone = execCfgs.clone();\n\n                for (int i = 0; i < execCfgs.length; i++)\n                    clone[i] = new ExecutorConfiguration(execCfgs[i]);\n\n                myCfg.setExecutorConfiguration(clone);\n            }\n\n            initializeDataStorageConfiguration(myCfg);\n\n            return myCfg;\n        }",
            "2124  \n2125  \n2126  \n2127  \n2128  \n2129  \n2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142 +\n2143 +\n2144  \n2145 +\n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  \n2161  \n2162  \n2163  \n2164  \n2165  \n2166  \n2167  \n2168  \n2169  \n2170  \n2171  \n2172  \n2173  \n2174 +\n2175 +\n2176 +\n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186  \n2187  \n2188  \n2189  \n2190  \n2191  \n2192  \n2193  \n2194  \n2195  \n2196  \n2197  \n2198  \n2199  \n2200  \n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  \n2245  \n2246  \n2247  \n2248  \n2249  \n2250  \n2251  \n2252  \n2253  \n2254  \n2255  \n2256  \n2257  \n2258  \n2259  \n2260  \n2261  \n2262  \n2263  \n2264  \n2265  \n2266  \n2267  \n2268  \n2269  \n2270  \n2271  \n2272  \n2273  \n2274  \n2275  \n2276  \n2277  \n2278  \n2279  \n2280  \n2281  \n2282  \n2283  \n2284  \n2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296  \n2297  \n2298  \n2299  \n2300  \n2301  \n2302  \n2303  \n2304  \n2305  ",
            "        /**\n         * @param cfg Ignite configuration copy to.\n         * @return New ignite configuration.\n         * @throws IgniteCheckedException If failed.\n         */\n        private IgniteConfiguration initializeConfiguration(IgniteConfiguration cfg)\n            throws IgniteCheckedException {\n            IgniteConfiguration myCfg = new IgniteConfiguration(cfg);\n\n            String ggHome = cfg.getIgniteHome();\n\n            // Set Ignite home.\n            if (ggHome == null)\n                ggHome = U.getIgniteHome();\n            else\n                // If user provided IGNITE_HOME - set it as a system property.\n                U.setIgniteHome(ggHome);\n\n            String userProvidedWorkDir = cfg.getWorkDirectory();\n\n            // Correctly resolve work directory and set it back to configuration.\n            String workDir = U.workDirectory(userProvidedWorkDir, ggHome);\n\n            myCfg.setWorkDirectory(workDir);\n\n            // Ensure invariant.\n            // It's a bit dirty - but this is a result of late refactoring\n            // and I don't want to reshuffle a lot of code.\n            assert F.eq(name, cfg.getIgniteInstanceName());\n\n            UUID nodeId = cfg.getNodeId() != null ? cfg.getNodeId() : UUID.randomUUID();\n\n            myCfg.setNodeId(nodeId);\n\n            String predefineConsistentId = IgniteSystemProperties.getString(IGNITE_OVERRIDE_CONSISTENT_ID);\n\n            if (!F.isEmpty(predefineConsistentId))\n                myCfg.setConsistentId(predefineConsistentId);\n\n            IgniteLogger cfgLog = initLogger(cfg.getGridLogger(), nodeId, workDir);\n\n            assert cfgLog != null;\n\n            cfgLog = new GridLoggerProxy(cfgLog, null, name, U.id8(nodeId));\n\n            // Initialize factory's log.\n            log = cfgLog.getLogger(G.class);\n\n            myCfg.setGridLogger(cfgLog);\n\n            if(F.isEmpty(userProvidedWorkDir) && F.isEmpty(U.IGNITE_WORK_DIR))\n                log.warning(\"Ignite work directory is not provided, automatically resolved to: \" + workDir);\n\n            // Check Ignite home folder (after log is available).\n            if (ggHome != null) {\n                File ggHomeFile = new File(ggHome);\n\n                if (!ggHomeFile.exists() || !ggHomeFile.isDirectory())\n                    throw new IgniteCheckedException(\"Invalid Ignite installation home folder: \" + ggHome);\n            }\n\n            myCfg.setIgniteHome(ggHome);\n\n            // Validate segmentation configuration.\n            SegmentationPolicy segPlc = cfg.getSegmentationPolicy();\n\n            // 1. Warn on potential configuration problem: grid is not configured to wait\n            // for correct segment after segmentation happens.\n            if (!F.isEmpty(cfg.getSegmentationResolvers()) && segPlc == RESTART_JVM && !cfg.isWaitForSegmentOnStart()) {\n                U.warn(log, \"Found potential configuration problem (forgot to enable waiting for segment\" +\n                    \"on start?) [segPlc=\" + segPlc + \", wait=false]\");\n            }\n\n            if (CU.isPersistenceEnabled(cfg) && myCfg.getConsistentId() == null)\n                U.warn(log, \"Consistent ID is not set, it is recommended to set consistent ID for production \" +\n                    \"clusters (use IgniteConfiguration.setConsistentId property)\");\n\n            myCfg.setTransactionConfiguration(myCfg.getTransactionConfiguration() != null ?\n                new TransactionConfiguration(myCfg.getTransactionConfiguration()) : null);\n\n            myCfg.setConnectorConfiguration(myCfg.getConnectorConfiguration() != null ?\n                new ConnectorConfiguration(myCfg.getConnectorConfiguration()) : null);\n\n            // Local host.\n            String locHost = IgniteSystemProperties.getString(IGNITE_LOCAL_HOST);\n\n            myCfg.setLocalHost(F.isEmpty(locHost) ? myCfg.getLocalHost() : locHost);\n\n            // Override daemon flag if it was set on the factory.\n            if (daemon.get())\n                myCfg.setDaemon(true);\n\n            if (myCfg.isClientMode() == null) {\n                Boolean threadClient = clientMode.get();\n\n                if (threadClient == null)\n                    myCfg.setClientMode(IgniteSystemProperties.getBoolean(IGNITE_CACHE_CLIENT, false));\n                else\n                    myCfg.setClientMode(threadClient);\n            }\n\n            // Check for deployment mode override.\n            String depModeName = IgniteSystemProperties.getString(IGNITE_DEP_MODE_OVERRIDE);\n\n            if (!F.isEmpty(depModeName)) {\n                if (!F.isEmpty(myCfg.getCacheConfiguration())) {\n                    U.quietAndInfo(log, \"Skipping deployment mode override for caches (custom closure \" +\n                        \"execution may not work for console Visor)\");\n                }\n                else {\n                    try {\n                        DeploymentMode depMode = DeploymentMode.valueOf(depModeName);\n\n                        if (myCfg.getDeploymentMode() != depMode)\n                            myCfg.setDeploymentMode(depMode);\n                    }\n                    catch (IllegalArgumentException e) {\n                        throw new IgniteCheckedException(\"Failed to override deployment mode using system property \" +\n                            \"(are there any misspellings?)\" +\n                            \"[name=\" + IGNITE_DEP_MODE_OVERRIDE + \", value=\" + depModeName + ']', e);\n                    }\n                }\n            }\n\n            if (myCfg.getUserAttributes() == null)\n                myCfg.setUserAttributes(Collections.<String, Object>emptyMap());\n\n            if (myCfg.getMBeanServer() == null && !U.IGNITE_MBEANS_DISABLED)\n                myCfg.setMBeanServer(ManagementFactory.getPlatformMBeanServer());\n\n            Marshaller marsh = myCfg.getMarshaller();\n\n            if (marsh == null) {\n                if (!BinaryMarshaller.available()) {\n                    U.warn(log, \"Standard BinaryMarshaller can't be used on this JVM. \" +\n                        \"Switch to HotSpot JVM or reach out Apache Ignite community for recommendations.\");\n\n                    marsh = new JdkMarshaller();\n                }\n                else\n                    marsh = new BinaryMarshaller();\n            }\n\n            MarshallerUtils.setNodeName(marsh, cfg.getIgniteInstanceName());\n\n            myCfg.setMarshaller(marsh);\n\n            if (myCfg.getPeerClassLoadingLocalClassPathExclude() == null)\n                myCfg.setPeerClassLoadingLocalClassPathExclude(EMPTY_STR_ARR);\n\n            FileSystemConfiguration[] igfsCfgs = myCfg.getFileSystemConfiguration();\n\n            if (igfsCfgs != null) {\n                FileSystemConfiguration[] clone = igfsCfgs.clone();\n\n                for (int i = 0; i < igfsCfgs.length; i++)\n                    clone[i] = new FileSystemConfiguration(igfsCfgs[i]);\n\n                myCfg.setFileSystemConfiguration(clone);\n            }\n\n            initializeDefaultSpi(myCfg);\n\n            GridDiscoveryManager.initCommunicationErrorResolveConfiguration(myCfg);\n\n            initializeDefaultCacheConfiguration(myCfg);\n\n            ExecutorConfiguration[] execCfgs = myCfg.getExecutorConfiguration();\n\n            if (execCfgs != null) {\n                ExecutorConfiguration[] clone = execCfgs.clone();\n\n                for (int i = 0; i < execCfgs.length; i++)\n                    clone[i] = new ExecutorConfiguration(execCfgs[i]);\n\n                myCfg.setExecutorConfiguration(clone);\n            }\n\n            initializeDataStorageConfiguration(myCfg);\n\n            return myCfg;\n        }"
        ],
        [
            "IgniteUtils::workDirectory(String,String)",
            "9265  \n9266  \n9267  \n9268  \n9269  \n9270  \n9271  \n9272  \n9273  \n9274  \n9275  \n9276  \n9277  \n9278  \n9279  \n9280  \n9281  \n9282  \n9283 -\n9284  \n9285 -\n9286  \n9287 -\n9288 -\n9289 -\n9290  \n9291 -\n9292  \n9293  \n9294  \n9295  \n9296  \n9297  \n9298  \n9299  \n9300  \n9301  \n9302  \n9303  \n9304  \n9305  \n9306  \n9307  ",
            "    /**\n     * Get work directory for the given user-provided work directory and Ignite home.\n     *\n     * @param userWorkDir Ignite work folder provided by user.\n     * @param userIgniteHome Ignite home folder provided by user.\n     */\n    public static String workDirectory(@Nullable String userWorkDir, @Nullable String userIgniteHome)\n        throws IgniteCheckedException {\n        if (userIgniteHome == null)\n            userIgniteHome = getIgniteHome();\n\n        File workDir;\n\n        if (!F.isEmpty(userWorkDir))\n            workDir = new File(userWorkDir);\n        else if (!F.isEmpty(IGNITE_WORK_DIR))\n            workDir = new File(IGNITE_WORK_DIR);\n        else if (!F.isEmpty(userIgniteHome))\n            workDir = new File(userIgniteHome, \"work\");\n        else {\n            String tmpDirPath = System.getProperty(\"java.io.tmpdir\");\n\n            if (tmpDirPath == null)\n                throw new IgniteCheckedException(\"Failed to create work directory in OS temp \" +\n                    \"(property 'java.io.tmpdir' is null).\");\n\n            workDir = new File(tmpDirPath, \"ignite\" + File.separator + \"work\");\n        }\n\n        if (!workDir.isAbsolute())\n            throw new IgniteCheckedException(\"Work directory path must be absolute: \" + workDir);\n\n        if (!mkdirs(workDir))\n            throw new IgniteCheckedException(\"Work directory does not exist and cannot be created: \" + workDir);\n\n        if (!workDir.canRead())\n            throw new IgniteCheckedException(\"Cannot read from work directory: \" + workDir);\n\n        if (!workDir.canWrite())\n            throw new IgniteCheckedException(\"Cannot write to work directory: \" + workDir);\n\n        return workDir.getAbsolutePath();\n    }",
            "9268  \n9269  \n9270  \n9271  \n9272  \n9273  \n9274  \n9275  \n9276  \n9277  \n9278  \n9279  \n9280  \n9281  \n9282  \n9283  \n9284  \n9285  \n9286 +\n9287  \n9288 +\n9289  \n9290 +\n9291 +\n9292 +\n9293 +\n9294 +\n9295 +\n9296  \n9297 +\n9298  \n9299  \n9300  \n9301  \n9302  \n9303  \n9304  \n9305  \n9306  \n9307  \n9308  \n9309  \n9310  \n9311  \n9312  \n9313  ",
            "    /**\n     * Get work directory for the given user-provided work directory and Ignite home.\n     *\n     * @param userWorkDir Ignite work folder provided by user.\n     * @param userIgniteHome Ignite home folder provided by user.\n     */\n    public static String workDirectory(@Nullable String userWorkDir, @Nullable String userIgniteHome)\n        throws IgniteCheckedException {\n        if (userIgniteHome == null)\n            userIgniteHome = getIgniteHome();\n\n        File workDir;\n\n        if (!F.isEmpty(userWorkDir))\n            workDir = new File(userWorkDir);\n        else if (!F.isEmpty(IGNITE_WORK_DIR))\n            workDir = new File(IGNITE_WORK_DIR);\n        else if (!F.isEmpty(userIgniteHome))\n            workDir = new File(userIgniteHome, DEFAULT_WORK_DIR);\n        else {\n            String userDir = System.getProperty(\"user.dir\");\n\n            if (F.isEmpty(userDir))\n                throw new IgniteCheckedException(\n                    \"Failed to resolve Ignite work directory. Either IgniteConfiguration.setWorkDirectory or \" +\n                        \"one of the system properties (\" + IGNITE_HOME + \", \" +\n                        IgniteSystemProperties.IGNITE_WORK_DIR + \") must be explicitly set.\"\n                );\n\n            workDir = new File(userDir, DEFAULT_WORK_DIR);\n        }\n\n        if (!workDir.isAbsolute())\n            throw new IgniteCheckedException(\"Work directory path must be absolute: \" + workDir);\n\n        if (!mkdirs(workDir))\n            throw new IgniteCheckedException(\"Work directory does not exist and cannot be created: \" + workDir);\n\n        if (!workDir.canRead())\n            throw new IgniteCheckedException(\"Cannot read from work directory: \" + workDir);\n\n        if (!workDir.canWrite())\n            throw new IgniteCheckedException(\"Cannot write to work directory: \" + workDir);\n\n        return workDir.getAbsolutePath();\n    }"
        ]
    ]
}