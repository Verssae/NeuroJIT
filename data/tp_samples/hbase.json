{
    "2f5767376f42c0416e025df412e3d5944a1b2a67": [
        [
            "SecureBulkLoadUtil::getBaseStagingDir(Configuration)",
            "  39  \n  40 -\n  41  ",
            "  public static Path getBaseStagingDir(Configuration conf) {\n    return new Path(conf.get(BULKLOAD_STAGING_DIR));\n  }",
            "  40  \n  41 +\n  42 +\n  43 +\n  44 +\n  45  ",
            "  public static Path getBaseStagingDir(Configuration conf) {\n    String hbaseTmpFsDir =\n        conf.get(HConstants.TEMPORARY_FS_DIRECTORY_KEY,\n          HConstants.DEFAULT_TEMPORARY_HDFS_DIRECTORY);\n    return new Path(conf.get(BULKLOAD_STAGING_DIR, hbaseTmpFsDir));\n  }"
        ],
        [
            "TestHFileOutputFormat2::testJobConfiguration()",
            " 349  \n 350  \n 351  \n 352 -\n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  ",
            "  @Ignore(\"Goes zombie too frequently; needs work. See HBASE-14563\") @Test\n  public void testJobConfiguration() throws Exception {\n    Configuration conf = new Configuration(this.util.getConfiguration());\n    conf.set(\"hbase.fs.tmp.dir\", util.getDataTestDir(\"testJobConfiguration\").toString());\n    Job job = new Job(conf);\n    job.setWorkingDirectory(util.getDataTestDir(\"testJobConfiguration\"));\n    Table table = Mockito.mock(Table.class);\n    RegionLocator regionLocator = Mockito.mock(RegionLocator.class);\n    setupMockStartKeys(regionLocator);\n    setupMockTableName(regionLocator);\n    HFileOutputFormat2.configureIncrementalLoad(job, table.getTableDescriptor(), regionLocator);\n    assertEquals(job.getNumReduceTasks(), 4);\n  }",
            " 349  \n 350  \n 351  \n 352 +\n 353 +\n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  ",
            "  @Ignore(\"Goes zombie too frequently; needs work. See HBASE-14563\") @Test\n  public void testJobConfiguration() throws Exception {\n    Configuration conf = new Configuration(this.util.getConfiguration());\n    conf.set(HConstants.TEMPORARY_FS_DIRECTORY_KEY, util.getDataTestDir(\"testJobConfiguration\")\n        .toString());\n    Job job = new Job(conf);\n    job.setWorkingDirectory(util.getDataTestDir(\"testJobConfiguration\"));\n    Table table = Mockito.mock(Table.class);\n    RegionLocator regionLocator = Mockito.mock(RegionLocator.class);\n    setupMockStartKeys(regionLocator);\n    setupMockTableName(regionLocator);\n    HFileOutputFormat2.configureIncrementalLoad(job, table.getTableDescriptor(), regionLocator);\n    assertEquals(job.getNumReduceTasks(), 4);\n  }"
        ],
        [
            "HFileOutputFormat2::configurePartitioner(Job,List)",
            " 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639 -\n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  ",
            "  /**\n   * Configure <code>job</code> with a TotalOrderPartitioner, partitioning against\n   * <code>splitPoints</code>. Cleans up the partitions file after job exists.\n   */\n  static void configurePartitioner(Job job, List<ImmutableBytesWritable> splitPoints)\n      throws IOException {\n    Configuration conf = job.getConfiguration();\n    // create the partitions file\n    FileSystem fs = FileSystem.get(conf);\n    Path partitionsPath = new Path(conf.get(\"hbase.fs.tmp.dir\"), \"partitions_\" + UUID.randomUUID());\n    fs.makeQualified(partitionsPath);\n    writePartitions(conf, partitionsPath, splitPoints);\n    fs.deleteOnExit(partitionsPath);\n\n    // configure job to use it\n    job.setPartitionerClass(TotalOrderPartitioner.class);\n    TotalOrderPartitioner.setPartitionFile(conf, partitionsPath);\n  }",
            " 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639 +\n 640 +\n 641 +\n 642 +\n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  ",
            "  /**\n   * Configure <code>job</code> with a TotalOrderPartitioner, partitioning against\n   * <code>splitPoints</code>. Cleans up the partitions file after job exists.\n   */\n  static void configurePartitioner(Job job, List<ImmutableBytesWritable> splitPoints)\n      throws IOException {\n    Configuration conf = job.getConfiguration();\n    // create the partitions file\n    FileSystem fs = FileSystem.get(conf);\n    String hbaseTmpFsDir =\n        conf.get(HConstants.TEMPORARY_FS_DIRECTORY_KEY,\n          HConstants.DEFAULT_TEMPORARY_HDFS_DIRECTORY);\n    Path partitionsPath = new Path(hbaseTmpFsDir, \"partitions_\" + UUID.randomUUID());\n    fs.makeQualified(partitionsPath);\n    writePartitions(conf, partitionsPath, splitPoints);\n    fs.deleteOnExit(partitionsPath);\n\n    // configure job to use it\n    job.setPartitionerClass(TotalOrderPartitioner.class);\n    TotalOrderPartitioner.setPartitionFile(conf, partitionsPath);\n  }"
        ]
    ],
    "47063031866e321655e63fd6c59128409c31b3b1": [
        [
            "KeyValue::toStringMap()",
            "1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187 -\n1188  \n1189  \n1190  \n1191  \n1192  ",
            "  /**\n   * Produces a string map for this key/value pair. Useful for programmatic use\n   * and manipulation of the data stored in an WALKey, for example, printing\n   * as JSON. Values are left out due to their tendency to be large. If needed,\n   * they can be added manually.\n   *\n   * @return the Map&lt;String,?&gt; containing data from this key\n   */\n  public Map<String, Object> toStringMap() {\n    Map<String, Object> stringMap = new HashMap<String, Object>();\n    stringMap.put(\"row\", Bytes.toStringBinary(getRowArray(), getRowOffset(), getRowLength()));\n    stringMap.put(\"family\",\n      Bytes.toStringBinary(getFamilyArray(), getFamilyOffset(), getFamilyLength()));\n    stringMap.put(\"qualifier\",\n      Bytes.toStringBinary(getQualifierArray(), getQualifierOffset(), getQualifierLength()));\n    stringMap.put(\"timestamp\", getTimestamp());\n    stringMap.put(\"vlen\", getValueLength());\n    List<Tag> tags = getTags();\n    if (tags != null) {\n      List<String> tagsString = new ArrayList<String>();\n      for (Tag t : tags) {\n        tagsString.add((t.getType()) + \":\" + TagUtil.getValueAsString(t));\n      }\n      stringMap.put(\"tag\", tagsString);\n    }\n    return stringMap;\n  }",
            "1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187 +\n1188  \n1189  \n1190  \n1191  \n1192  ",
            "  /**\n   * Produces a string map for this key/value pair. Useful for programmatic use\n   * and manipulation of the data stored in an WALKey, for example, printing\n   * as JSON. Values are left out due to their tendency to be large. If needed,\n   * they can be added manually.\n   *\n   * @return the Map&lt;String,?&gt; containing data from this key\n   */\n  public Map<String, Object> toStringMap() {\n    Map<String, Object> stringMap = new HashMap<String, Object>();\n    stringMap.put(\"row\", Bytes.toStringBinary(getRowArray(), getRowOffset(), getRowLength()));\n    stringMap.put(\"family\",\n      Bytes.toStringBinary(getFamilyArray(), getFamilyOffset(), getFamilyLength()));\n    stringMap.put(\"qualifier\",\n      Bytes.toStringBinary(getQualifierArray(), getQualifierOffset(), getQualifierLength()));\n    stringMap.put(\"timestamp\", getTimestamp());\n    stringMap.put(\"vlen\", getValueLength());\n    List<Tag> tags = getTags();\n    if (tags != null) {\n      List<String> tagsString = new ArrayList<String>();\n      for (Tag t : tags) {\n        tagsString.add(t.toString());\n      }\n      stringMap.put(\"tag\", tagsString);\n    }\n    return stringMap;\n  }"
        ],
        [
            "HFilePrettyPrinter::scanKeysValues(Path,KeyValueStatsCollector,HFileScanner,byte)",
            " 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373 -\n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "  private void scanKeysValues(Path file, KeyValueStatsCollector fileStats,\n      HFileScanner scanner,  byte[] row) throws IOException {\n    Cell pCell = null;\n    FileSystem fs = FileSystem.get(getConf());\n    Set<String> foundMobFiles = new LinkedHashSet<String>(FOUND_MOB_FILES_CACHE_CAPACITY);\n    Set<String> missingMobFiles = new LinkedHashSet<String>(MISSING_MOB_FILES_CACHE_CAPACITY);\n    do {\n      Cell cell = scanner.getCell();\n      if (row != null && row.length != 0) {\n        int result = CellComparator.COMPARATOR.compareRows(cell, row, 0, row.length);\n        if (result > 0) {\n          break;\n        } else if (result < 0) {\n          continue;\n        }\n      }\n      // collect stats\n      if (printStats) {\n        fileStats.collect(cell);\n      }\n      // dump key value\n      if (printKey) {\n        System.out.print(\"K: \" + cell);\n        if (printValue) {\n          System.out.print(\" V: \"\n              + Bytes.toStringBinary(cell.getValueArray(), cell.getValueOffset(),\n                  cell.getValueLength()));\n          int i = 0;\n          List<Tag> tags = TagUtil.asList(cell.getTagsArray(), cell.getTagsOffset(),\n              cell.getTagsLength());\n          for (Tag tag : tags) {\n            System.out.print(String.format(\" T[%d]: %s\", i++, TagUtil.getValueAsString(tag)));\n          }\n        }\n        System.out.println();\n      }\n      // check if rows are in order\n      if (checkRow && pCell != null) {\n        if (CellComparator.COMPARATOR.compareRows(pCell, cell) > 0) {\n          System.err.println(\"WARNING, previous row is greater then\"\n              + \" current row\\n\\tfilename -> \" + file + \"\\n\\tprevious -> \"\n              + CellUtil.getCellKeyAsString(pCell) + \"\\n\\tcurrent  -> \"\n              + CellUtil.getCellKeyAsString(cell));\n        }\n      }\n      // check if families are consistent\n      if (checkFamily) {\n        String fam = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(),\n            cell.getFamilyLength());\n        if (!file.toString().contains(fam)) {\n          System.err.println(\"WARNING, filename does not match kv family,\"\n              + \"\\n\\tfilename -> \" + file + \"\\n\\tkeyvalue -> \"\n              + CellUtil.getCellKeyAsString(cell));\n        }\n        if (pCell != null && CellComparator.compareFamilies(pCell, cell) != 0) {\n          System.err.println(\"WARNING, previous kv has different family\"\n              + \" compared to current key\\n\\tfilename -> \" + file\n              + \"\\n\\tprevious -> \" + CellUtil.getCellKeyAsString(pCell)\n              + \"\\n\\tcurrent  -> \" + CellUtil.getCellKeyAsString(cell));\n        }\n      }\n      // check if mob files are missing.\n      if (checkMobIntegrity && MobUtils.isMobReferenceCell(cell)) {\n        Tag tnTag = MobUtils.getTableNameTag(cell);\n        if (tnTag == null) {\n          System.err.println(\"ERROR, wrong tag format in mob reference cell \"\n            + CellUtil.getCellKeyAsString(cell));\n        } else if (!MobUtils.hasValidMobRefCellValue(cell)) {\n          System.err.println(\"ERROR, wrong value format in mob reference cell \"\n            + CellUtil.getCellKeyAsString(cell));\n        } else {\n          TableName tn = TableName.valueOf(TagUtil.cloneValue(tnTag));\n          String mobFileName = MobUtils.getMobFileName(cell);\n          boolean exist = mobFileExists(fs, tn, mobFileName,\n            Bytes.toString(CellUtil.cloneFamily(cell)), foundMobFiles, missingMobFiles);\n          if (!exist) {\n            // report error\n            System.err.println(\"ERROR, the mob file [\" + mobFileName\n              + \"] is missing referenced by cell \" + CellUtil.getCellKeyAsString(cell));\n          }\n        }\n      }\n      pCell = cell;\n      ++count;\n    } while (scanner.next());\n  }",
            " 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373 +\n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  ",
            "  private void scanKeysValues(Path file, KeyValueStatsCollector fileStats,\n      HFileScanner scanner,  byte[] row) throws IOException {\n    Cell pCell = null;\n    FileSystem fs = FileSystem.get(getConf());\n    Set<String> foundMobFiles = new LinkedHashSet<String>(FOUND_MOB_FILES_CACHE_CAPACITY);\n    Set<String> missingMobFiles = new LinkedHashSet<String>(MISSING_MOB_FILES_CACHE_CAPACITY);\n    do {\n      Cell cell = scanner.getCell();\n      if (row != null && row.length != 0) {\n        int result = CellComparator.COMPARATOR.compareRows(cell, row, 0, row.length);\n        if (result > 0) {\n          break;\n        } else if (result < 0) {\n          continue;\n        }\n      }\n      // collect stats\n      if (printStats) {\n        fileStats.collect(cell);\n      }\n      // dump key value\n      if (printKey) {\n        System.out.print(\"K: \" + cell);\n        if (printValue) {\n          System.out.print(\" V: \"\n              + Bytes.toStringBinary(cell.getValueArray(), cell.getValueOffset(),\n                  cell.getValueLength()));\n          int i = 0;\n          List<Tag> tags = TagUtil.asList(cell.getTagsArray(), cell.getTagsOffset(),\n              cell.getTagsLength());\n          for (Tag tag : tags) {\n            System.out.print(String.format(\" T[%d]: %s\", i++, tag.toString()));\n          }\n        }\n        System.out.println();\n      }\n      // check if rows are in order\n      if (checkRow && pCell != null) {\n        if (CellComparator.COMPARATOR.compareRows(pCell, cell) > 0) {\n          System.err.println(\"WARNING, previous row is greater then\"\n              + \" current row\\n\\tfilename -> \" + file + \"\\n\\tprevious -> \"\n              + CellUtil.getCellKeyAsString(pCell) + \"\\n\\tcurrent  -> \"\n              + CellUtil.getCellKeyAsString(cell));\n        }\n      }\n      // check if families are consistent\n      if (checkFamily) {\n        String fam = Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(),\n            cell.getFamilyLength());\n        if (!file.toString().contains(fam)) {\n          System.err.println(\"WARNING, filename does not match kv family,\"\n              + \"\\n\\tfilename -> \" + file + \"\\n\\tkeyvalue -> \"\n              + CellUtil.getCellKeyAsString(cell));\n        }\n        if (pCell != null && CellComparator.compareFamilies(pCell, cell) != 0) {\n          System.err.println(\"WARNING, previous kv has different family\"\n              + \" compared to current key\\n\\tfilename -> \" + file\n              + \"\\n\\tprevious -> \" + CellUtil.getCellKeyAsString(pCell)\n              + \"\\n\\tcurrent  -> \" + CellUtil.getCellKeyAsString(cell));\n        }\n      }\n      // check if mob files are missing.\n      if (checkMobIntegrity && MobUtils.isMobReferenceCell(cell)) {\n        Tag tnTag = MobUtils.getTableNameTag(cell);\n        if (tnTag == null) {\n          System.err.println(\"ERROR, wrong tag format in mob reference cell \"\n            + CellUtil.getCellKeyAsString(cell));\n        } else if (!MobUtils.hasValidMobRefCellValue(cell)) {\n          System.err.println(\"ERROR, wrong value format in mob reference cell \"\n            + CellUtil.getCellKeyAsString(cell));\n        } else {\n          TableName tn = TableName.valueOf(TagUtil.cloneValue(tnTag));\n          String mobFileName = MobUtils.getMobFileName(cell);\n          boolean exist = mobFileExists(fs, tn, mobFileName,\n            Bytes.toString(CellUtil.cloneFamily(cell)), foundMobFiles, missingMobFiles);\n          if (!exist) {\n            // report error\n            System.err.println(\"ERROR, the mob file [\" + mobFileName\n              + \"] is missing referenced by cell \" + CellUtil.getCellKeyAsString(cell));\n          }\n        }\n      }\n      pCell = cell;\n      ++count;\n    } while (scanner.next());\n  }"
        ]
    ],
    "f4470af95db383a47d1bd431a32cd8a6949dd7c0": [
        [
            "ZKUtil::getServerStats(String,int)",
            "1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929 -\n1930  \n1931 -\n1932 -\n1933 -\n1934 -\n1935 -\n1936 -\n1937 -\n1938 -\n1939 -\n1940 -\n1941 -\n1942 -\n1943 -\n1944 -\n1945 -\n1946  \n1947  \n1948 -\n1949 -\n1950  ",
            "  /**\n   * Gets the statistics from the given server.\n   *\n   * @param server  The server to get the statistics from.\n   * @param timeout  The socket timeout to use.\n   * @return The array of response strings.\n   * @throws IOException When the socket communication fails.\n   */\n  public static String[] getServerStats(String server, int timeout)\n  throws IOException {\n    String[] sp = server.split(\":\");\n    if (sp == null || sp.length == 0) {\n      return null;\n    }\n\n    String host = sp[0];\n    int port = sp.length > 1 ? Integer.parseInt(sp[1])\n        : HConstants.DEFAULT_ZOOKEPER_CLIENT_PORT;\n\n    Socket socket = new Socket();\n    InetSocketAddress sockAddr = new InetSocketAddress(host, port);\n    socket.connect(sockAddr, timeout);\n\n    socket.setSoTimeout(timeout);\n    PrintWriter out = new PrintWriter(socket.getOutputStream(), true);\n    BufferedReader in = new BufferedReader(new InputStreamReader(\n      socket.getInputStream()));\n    out.println(\"stat\");\n    out.flush();\n    ArrayList<String> res = new ArrayList<String>();\n    while (true) {\n      String line = in.readLine();\n      if (line != null) {\n        res.add(line);\n      } else {\n        break;\n      }\n    }\n    socket.close();\n    return res.toArray(new String[res.size()]);\n  }",
            "1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930 +\n1931 +\n1932 +\n1933 +\n1934 +\n1935 +\n1936 +\n1937 +\n1938 +\n1939 +\n1940 +\n1941 +\n1942 +\n1943 +\n1944 +\n1945 +\n1946 +\n1947  \n1948 +\n1949  \n1950  ",
            "  /**\n   * Gets the statistics from the given server.\n   *\n   * @param server  The server to get the statistics from.\n   * @param timeout  The socket timeout to use.\n   * @return The array of response strings.\n   * @throws IOException When the socket communication fails.\n   */\n  public static String[] getServerStats(String server, int timeout)\n  throws IOException {\n    String[] sp = server.split(\":\");\n    if (sp == null || sp.length == 0) {\n      return null;\n    }\n\n    String host = sp[0];\n    int port = sp.length > 1 ? Integer.parseInt(sp[1])\n        : HConstants.DEFAULT_ZOOKEPER_CLIENT_PORT;\n\n    InetSocketAddress sockAddr = new InetSocketAddress(host, port);\n    try (Socket socket = new Socket()) {\n      socket.connect(sockAddr, timeout);\n\n      socket.setSoTimeout(timeout);\n      PrintWriter out = new PrintWriter(socket.getOutputStream(), true);\n      BufferedReader in = new BufferedReader(new InputStreamReader(\n        socket.getInputStream()));\n      out.println(\"stat\");\n      out.flush();\n      ArrayList<String> res = new ArrayList<String>();\n      while (true) {\n        String line = in.readLine();\n        if (line != null) {\n          res.add(line);\n        } else {\n          break;\n        }\n      }\n      return res.toArray(new String[res.size()]);\n    }\n  }"
        ],
        [
            "LogLevel::process(String)",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76 -\n  77 -\n  78 -\n  79 -\n  80 -\n  81  \n  82 -\n  83  \n  84  \n  85  \n  86  ",
            "  private static void process(String urlstring) {\n    try {\n      URL url = new URL(urlstring);\n      System.out.println(\"Connecting to \" + url);\n      URLConnection connection = url.openConnection();\n      connection.connect();\n\n      BufferedReader in = new BufferedReader(new InputStreamReader(\n          connection.getInputStream()));\n      for(String line; (line = in.readLine()) != null; )\n        if (line.startsWith(MARKER)) {\n          System.out.println(TAG.matcher(line).replaceAll(\"\"));\n        }\n      in.close();\n    } catch (IOException ioe) {\n      System.err.println(\"\" + ioe);\n    }\n  }",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80 +\n  81  \n  82 +\n  83  \n  84  \n  85  \n  86  ",
            "  private static void process(String urlstring) {\n    try {\n      URL url = new URL(urlstring);\n      System.out.println(\"Connecting to \" + url);\n      URLConnection connection = url.openConnection();\n      connection.connect();\n      try (BufferedReader in = new BufferedReader(new InputStreamReader(\n               connection.getInputStream()))) {\n        for(String line; (line = in.readLine()) != null; ) {\n          if (line.startsWith(MARKER)) {\n            System.out.println(TAG.matcher(line).replaceAll(\"\"));\n          }\n        }\n      }\n    } catch (IOException ioe) {\n      System.err.println(\"\" + ioe);\n    }\n  }"
        ],
        [
            "CoprocessorClassLoader::init(Path,String,Configuration)",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 -\n 181 -\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "  private void init(Path path, String pathPrefix,\n      Configuration conf) throws IOException {\n    // Copy the jar to the local filesystem\n    String parentDirStr =\n      conf.get(LOCAL_DIR_KEY, DEFAULT_LOCAL_DIR) + TMP_JARS_DIR;\n    synchronized (parentDirLockSet) {\n      if (!parentDirLockSet.contains(parentDirStr)) {\n        Path parentDir = new Path(parentDirStr);\n        FileSystem fs = FileSystem.getLocal(conf);\n        fs.delete(parentDir, true); // it's ok if the dir doesn't exist now\n        parentDirLockSet.add(parentDirStr);\n        if (!fs.mkdirs(parentDir) && !fs.getFileStatus(parentDir).isDirectory()) {\n          throw new RuntimeException(\"Failed to create local dir \" + parentDirStr\n            + \", CoprocessorClassLoader failed to init\");\n        }\n      }\n    }\n\n    FileSystem fs = path.getFileSystem(conf);\n    File dst = new File(parentDirStr, \".\" + pathPrefix + \".\"\n      + path.getName() + \".\" + System.currentTimeMillis() + \".jar\");\n    fs.copyToLocalFile(path, new Path(dst.toString()));\n    dst.deleteOnExit();\n\n    addURL(dst.getCanonicalFile().toURI().toURL());\n\n    JarFile jarFile = new JarFile(dst.toString());\n    try {\n      Enumeration<JarEntry> entries = jarFile.entries();\n      while (entries.hasMoreElements()) {\n        JarEntry entry = entries.nextElement();\n        Matcher m = libJarPattern.matcher(entry.getName());\n        if (m.matches()) {\n          File file = new File(parentDirStr, \".\" + pathPrefix + \".\"\n            + path.getName() + \".\" + System.currentTimeMillis() + \".\" + m.group(1));\n          IOUtils.copyBytes(jarFile.getInputStream(entry),\n            new FileOutputStream(file), conf, true);\n          file.deleteOnExit();\n          addURL(file.toURI().toURL());\n        }\n      }\n    } finally {\n      jarFile.close();\n    }\n  }",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 +\n 181 +\n 182 +\n 183 +\n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  ",
            "  private void init(Path path, String pathPrefix,\n      Configuration conf) throws IOException {\n    // Copy the jar to the local filesystem\n    String parentDirStr =\n      conf.get(LOCAL_DIR_KEY, DEFAULT_LOCAL_DIR) + TMP_JARS_DIR;\n    synchronized (parentDirLockSet) {\n      if (!parentDirLockSet.contains(parentDirStr)) {\n        Path parentDir = new Path(parentDirStr);\n        FileSystem fs = FileSystem.getLocal(conf);\n        fs.delete(parentDir, true); // it's ok if the dir doesn't exist now\n        parentDirLockSet.add(parentDirStr);\n        if (!fs.mkdirs(parentDir) && !fs.getFileStatus(parentDir).isDirectory()) {\n          throw new RuntimeException(\"Failed to create local dir \" + parentDirStr\n            + \", CoprocessorClassLoader failed to init\");\n        }\n      }\n    }\n\n    FileSystem fs = path.getFileSystem(conf);\n    File dst = new File(parentDirStr, \".\" + pathPrefix + \".\"\n      + path.getName() + \".\" + System.currentTimeMillis() + \".jar\");\n    fs.copyToLocalFile(path, new Path(dst.toString()));\n    dst.deleteOnExit();\n\n    addURL(dst.getCanonicalFile().toURI().toURL());\n\n    JarFile jarFile = new JarFile(dst.toString());\n    try {\n      Enumeration<JarEntry> entries = jarFile.entries();\n      while (entries.hasMoreElements()) {\n        JarEntry entry = entries.nextElement();\n        Matcher m = libJarPattern.matcher(entry.getName());\n        if (m.matches()) {\n          File file = new File(parentDirStr, \".\" + pathPrefix + \".\"\n            + path.getName() + \".\" + System.currentTimeMillis() + \".\" + m.group(1));\n          try (FileOutputStream outStream = new FileOutputStream(file)) {\n            IOUtils.copyBytes(jarFile.getInputStream(entry),\n              outStream, conf, true);\n          }\n          file.deleteOnExit();\n          addURL(file.toURI().toURL());\n        }\n      }\n    } finally {\n      jarFile.close();\n    }\n  }"
        ],
        [
            "JarFinder::createJar(File,File)",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 -\n 128 -\n 129  ",
            "  private static void createJar(File dir, File jarFile) throws IOException {\n    Preconditions.checkNotNull(dir, \"dir\");\n    Preconditions.checkNotNull(jarFile, \"jarFile\");\n    File jarDir = jarFile.getParentFile();\n    if (!jarDir.exists()) {\n      if (!jarDir.mkdirs()) {\n        throw new IOException(MessageFormat.format(\"could not create dir [{0}]\",\n                                                   jarDir));\n      }\n    }\n    JarOutputStream zos = new JarOutputStream(new FileOutputStream(jarFile));\n    jarDir(dir, \"\", zos);\n  }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128 +\n 129 +\n 130  ",
            "  private static void createJar(File dir, File jarFile) throws IOException {\n    Preconditions.checkNotNull(dir, \"dir\");\n    Preconditions.checkNotNull(jarFile, \"jarFile\");\n    File jarDir = jarFile.getParentFile();\n    if (!jarDir.exists()) {\n      if (!jarDir.mkdirs()) {\n        throw new IOException(MessageFormat.format(\"could not create dir [{0}]\",\n                                                   jarDir));\n      }\n    }\n    try (JarOutputStream zos = new JarOutputStream(new FileOutputStream(jarFile))) {\n      jarDir(dir, \"\", zos);\n    }\n  }"
        ]
    ],
    "cd2588001cf31ad2fb2020f9e021c9b1be1b76fc": [
        [
            "SimpleRegionNormalizer::computePlanForTable(TableName)",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 -\n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  /**\n   * Computes next most \"urgent\" normalization action on the table.\n   * Action may be either a split, or a merge, or no action.\n   *\n   * @param table table to normalize\n   * @return normalization plan to execute\n   */\n  @Override\n  public List<NormalizationPlan> computePlanForTable(TableName table) throws HBaseIOException {\n    if (table == null || table.isSystemTable()) {\n      LOG.debug(\"Normalization of system table \" + table + \" isn't allowed\");\n      return null;\n    }\n\n    List<NormalizationPlan> plans = new ArrayList<NormalizationPlan>();\n    List<HRegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().\n      getRegionsOfTable(table);\n\n    //TODO: should we make min number of regions a config param?\n    if (tableRegions == null || tableRegions.size() < MIN_REGION_COUNT) {\n      int nrRegions = tableRegions == null ? 0 : tableRegions.size();\n      LOG.debug(\"Table \" + table + \" has \" + nrRegions + \" regions, required min number\"\n        + \" of regions for normalizer to run is \" + MIN_REGION_COUNT + \", not running normalizer\");\n      return null;\n    }\n\n    LOG.debug(\"Computing normalization plan for table: \" + table +\n      \", number of regions: \" + tableRegions.size());\n\n    long totalSizeMb = 0;\n\n    for (int i = 0; i < tableRegions.size(); i++) {\n      HRegionInfo hri = tableRegions.get(i);\n      long regionSize = getRegionSize(hri);\n      if (regionSize > 0) {\n        totalSizeMb += regionSize;\n      }\n    }\n\n    double avgRegionSize = totalSizeMb / (double) tableRegions.size();\n\n    LOG.debug(\"Table \" + table + \", total aggregated regions size: \" + totalSizeMb);\n    LOG.debug(\"Table \" + table + \", average region size: \" + avgRegionSize);\n\n    int candidateIdx = 0;\n    boolean splitEnabled = true, mergeEnabled = true;\n    try {\n      splitEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.SPLIT)).getEnabled();\n    } catch (ServiceException se) {\n      LOG.debug(\"Unable to determine whether split is enabled\", se);\n    }\n    try {\n      mergeEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.MERGE)).getEnabled();\n    } catch (ServiceException se) {\n      LOG.debug(\"Unable to determine whether split is enabled\", se);\n    }\n    while (candidateIdx < tableRegions.size()) {\n      HRegionInfo hri = tableRegions.get(candidateIdx);\n      long regionSize = getRegionSize(hri);\n      // if the region is > 2 times larger than average, we split it, split\n      // is more high priority normalization action than merge.\n      if (regionSize > 2 * avgRegionSize) {\n        if (splitEnabled) {\n          LOG.info(\"Table \" + table + \", large region \" + hri.getRegionNameAsString() + \" has size \"\n              + regionSize + \", more than twice avg size, splitting\");\n          plans.add(new SplitNormalizationPlan(hri, null));\n        }\n      } else {\n        if (candidateIdx == tableRegions.size()-1) {\n          break;\n        }\n        if (mergeEnabled) {\n          HRegionInfo hri2 = tableRegions.get(candidateIdx+1);\n          long regionSize2 = getRegionSize(hri2);\n          if (regionSize + regionSize2 < avgRegionSize) {\n            LOG.info(\"Table \" + table + \", small region size: \" + regionSize\n              + \" plus its neighbor size: \" + regionSize2\n              + \", less than the avg size \" + avgRegionSize + \", merging them\");\n            plans.add(new MergeNormalizationPlan(hri, hri2));\n            candidateIdx++;\n          }\n        }\n      }\n      candidateIdx++;\n    }\n    if (plans.isEmpty()) {\n      LOG.debug(\"No normalization needed, regions look good for table: \" + table);\n      return null;\n    }\n    Collections.sort(plans, planComparator);\n    return plans;\n  }",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 +\n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  /**\n   * Computes next most \"urgent\" normalization action on the table.\n   * Action may be either a split, or a merge, or no action.\n   *\n   * @param table table to normalize\n   * @return normalization plan to execute\n   */\n  @Override\n  public List<NormalizationPlan> computePlanForTable(TableName table) throws HBaseIOException {\n    if (table == null || table.isSystemTable()) {\n      LOG.debug(\"Normalization of system table \" + table + \" isn't allowed\");\n      return null;\n    }\n\n    List<NormalizationPlan> plans = new ArrayList<NormalizationPlan>();\n    List<HRegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().\n      getRegionsOfTable(table);\n\n    //TODO: should we make min number of regions a config param?\n    if (tableRegions == null || tableRegions.size() < MIN_REGION_COUNT) {\n      int nrRegions = tableRegions == null ? 0 : tableRegions.size();\n      LOG.debug(\"Table \" + table + \" has \" + nrRegions + \" regions, required min number\"\n        + \" of regions for normalizer to run is \" + MIN_REGION_COUNT + \", not running normalizer\");\n      return null;\n    }\n\n    LOG.debug(\"Computing normalization plan for table: \" + table +\n      \", number of regions: \" + tableRegions.size());\n\n    long totalSizeMb = 0;\n\n    for (int i = 0; i < tableRegions.size(); i++) {\n      HRegionInfo hri = tableRegions.get(i);\n      long regionSize = getRegionSize(hri);\n      if (regionSize > 0) {\n        totalSizeMb += regionSize;\n      }\n    }\n\n    double avgRegionSize = totalSizeMb / (double) tableRegions.size();\n\n    LOG.debug(\"Table \" + table + \", total aggregated regions size: \" + totalSizeMb);\n    LOG.debug(\"Table \" + table + \", average region size: \" + avgRegionSize);\n\n    int candidateIdx = 0;\n    boolean splitEnabled = true, mergeEnabled = true;\n    try {\n      splitEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.SPLIT)).getEnabled();\n    } catch (ServiceException se) {\n      LOG.debug(\"Unable to determine whether split is enabled\", se);\n    }\n    try {\n      mergeEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.MERGE)).getEnabled();\n    } catch (ServiceException se) {\n      LOG.debug(\"Unable to determine whether split is enabled\", se);\n    }\n    while (candidateIdx < tableRegions.size()) {\n      HRegionInfo hri = tableRegions.get(candidateIdx);\n      long regionSize = getRegionSize(hri);\n      // if the region is > 2 times larger than average, we split it, split\n      // is more high priority normalization action than merge.\n      if (regionSize > 2 * avgRegionSize) {\n        if (splitEnabled) {\n          LOG.info(\"Table \" + table + \", large region \" + hri.getRegionNameAsString() + \" has size \"\n              + regionSize + \", more than twice avg size, splitting\");\n          plans.add(new SplitNormalizationPlan(hri, null));\n        }\n      } else {\n        if (candidateIdx == tableRegions.size()-1) {\n          break;\n        }\n        if (mergeEnabled) {\n          HRegionInfo hri2 = tableRegions.get(candidateIdx+1);\n          long regionSize2 = getRegionSize(hri2);\n          if (regionSize > 0 && regionSize2 > 0 && regionSize + regionSize2 < avgRegionSize) {\n            LOG.info(\"Table \" + table + \", small region size: \" + regionSize\n              + \" plus its neighbor size: \" + regionSize2\n              + \", less than the avg size \" + avgRegionSize + \", merging them\");\n            plans.add(new MergeNormalizationPlan(hri, hri2));\n            candidateIdx++;\n          }\n        }\n      }\n      candidateIdx++;\n    }\n    if (plans.isEmpty()) {\n      LOG.debug(\"No normalization needed, regions look good for table: \" + table);\n      return null;\n    }\n    Collections.sort(plans, planComparator);\n    return plans;\n  }"
        ]
    ],
    "fa50d456a8a05517004fec853258e4a01ca35a23": [
        [
            "SimpleRpcScheduler::SimpleRpcScheduler(Configuration,int,int,int,PriorityFunction,Abortable,int)",
            " 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203 -\n 204  \n 205  \n 206  \n 207  \n 208  \n 209 -\n 210  \n 211  \n 212  \n 213  \n 214 -\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 -\n 223  \n 224  \n 225  \n 226 -\n 227  \n 228  \n 229  \n 230  \n 231 -\n 232  \n 233  \n 234  \n 235  \n 236  \n 237 -\n 238  \n 239  \n 240  \n 241 -\n 242  \n 243  ",
            "  /**\n   * @param conf\n   * @param handlerCount the number of handler threads that will be used to process calls\n   * @param priorityHandlerCount How many threads for priority handling.\n   * @param replicationHandlerCount How many threads for replication handling.\n   * @param highPriorityLevel\n   * @param priority Function to extract request priority.\n   */\n  public SimpleRpcScheduler(\n      Configuration conf,\n      int handlerCount,\n      int priorityHandlerCount,\n      int replicationHandlerCount,\n      PriorityFunction priority,\n      Abortable server,\n      int highPriorityLevel) {\n\n    int maxQueueLength = conf.getInt(RpcScheduler.IPC_SERVER_MAX_CALLQUEUE_LENGTH,\n        handlerCount * RpcServer.DEFAULT_MAX_CALLQUEUE_LENGTH_PER_HANDLER);\n    int maxPriorityQueueLength =\n        conf.getInt(RpcScheduler.IPC_SERVER_PRIORITY_MAX_CALLQUEUE_LENGTH, maxQueueLength);\n\n    this.priority = priority;\n    this.highPriorityLevel = highPriorityLevel;\n    this.abortable = server;\n\n    String callQueueType = conf.get(CALL_QUEUE_TYPE_CONF_KEY,\n      CALL_QUEUE_TYPE_DEADLINE_CONF_VALUE);\n    float callqReadShare = conf.getFloat(CALL_QUEUE_READ_SHARE_CONF_KEY, 0);\n    float callqScanShare = conf.getFloat(CALL_QUEUE_SCAN_SHARE_CONF_KEY, 0);\n\n    int codelTargetDelay = conf.getInt(CALL_QUEUE_CODEL_TARGET_DELAY,\n      CALL_QUEUE_CODEL_DEFAULT_TARGET_DELAY);\n    int codelInterval = conf.getInt(CALL_QUEUE_CODEL_INTERVAL,\n      CALL_QUEUE_CODEL_DEFAULT_INTERVAL);\n    double codelLifoThreshold = conf.getDouble(CALL_QUEUE_CODEL_LIFO_THRESHOLD,\n      CALL_QUEUE_CODEL_DEFAULT_LIFO_THRESHOLD);\n\n    float callQueuesHandlersFactor = conf.getFloat(CALL_QUEUE_HANDLER_FACTOR_CONF_KEY, 0);\n    int numCallQueues = Math.max(1, (int)Math.round(handlerCount * callQueuesHandlersFactor));\n    LOG.info(\"Using \" + callQueueType + \" as user call queue; numCallQueues=\" + numCallQueues +\n        \"; callQReadShare=\" + callqReadShare + \", callQScanShare=\" + callqScanShare);\n    if (numCallQueues > 1 && callqReadShare > 0) {\n      // multiple read/write queues\n      if (isDeadlineQueueType(callQueueType)) {\n        CallPriorityComparator callPriority = new CallPriorityComparator(conf, this.priority);\n        callExecutor = new RWQueueRpcExecutor(\"RWQ.default\", handlerCount, numCallQueues,\n            callqReadShare, callqScanShare, maxQueueLength, conf, abortable,\n            BoundedPriorityBlockingQueue.class, callPriority);\n      } else if (callQueueType.equals(CALL_QUEUE_TYPE_CODEL_CONF_VALUE)) {\n        Object[] callQueueInitArgs = {maxQueueLength, codelTargetDelay, codelInterval,\n          codelLifoThreshold, numGeneralCallsDropped, numLifoModeSwitches};\n        callExecutor = new RWQueueRpcExecutor(\"RWQ.default\", handlerCount,\n          numCallQueues, callqReadShare, callqScanShare,\n          AdaptiveLifoCoDelCallQueue.class, callQueueInitArgs,\n          AdaptiveLifoCoDelCallQueue.class, callQueueInitArgs);\n      } else {\n        callExecutor = new RWQueueRpcExecutor(\"RWQ.default\", handlerCount, numCallQueues,\n          callqReadShare, callqScanShare, maxQueueLength, conf, abortable);\n      }\n    } else {\n      // multiple queues\n      if (isDeadlineQueueType(callQueueType)) {\n        CallPriorityComparator callPriority = new CallPriorityComparator(conf, this.priority);\n        callExecutor =\n          new BalancedQueueRpcExecutor(\"BalancedQ.default\", handlerCount, numCallQueues,\n            conf, abortable, BoundedPriorityBlockingQueue.class, maxQueueLength, callPriority);\n      } else if (callQueueType.equals(CALL_QUEUE_TYPE_CODEL_CONF_VALUE)) {\n        callExecutor =\n          new BalancedQueueRpcExecutor(\"BalancedQ.default\", handlerCount, numCallQueues,\n            conf, abortable, AdaptiveLifoCoDelCallQueue.class, maxQueueLength,\n            codelTargetDelay, codelInterval, codelLifoThreshold,\n            numGeneralCallsDropped, numLifoModeSwitches);\n      } else {\n        callExecutor = new BalancedQueueRpcExecutor(\"BalancedQ.default\", handlerCount,\n            numCallQueues, maxQueueLength, conf, abortable);\n      }\n    }\n    // Create 2 queues to help priorityExecutor be more scalable.\n    this.priorityExecutor = priorityHandlerCount > 0 ?\n      new BalancedQueueRpcExecutor(\"BalancedQ.priority\", priorityHandlerCount, 2,\n          maxPriorityQueueLength):\n      null;\n   this.replicationExecutor =\n     replicationHandlerCount > 0 ? new BalancedQueueRpcExecutor(\"BalancedQ.replication\",\n       replicationHandlerCount, 1, maxQueueLength, conf, abortable) : null;\n  }",
            " 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 +\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 +\n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217  \n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226 +\n 227  \n 228  \n 229  \n 230 +\n 231  \n 232  \n 233  \n 234  \n 235 +\n 236  \n 237  \n 238  \n 239  \n 240  \n 241 +\n 242  \n 243  \n 244  \n 245 +\n 246  \n 247  ",
            "  /**\n   * @param conf\n   * @param handlerCount the number of handler threads that will be used to process calls\n   * @param priorityHandlerCount How many threads for priority handling.\n   * @param replicationHandlerCount How many threads for replication handling.\n   * @param highPriorityLevel\n   * @param priority Function to extract request priority.\n   */\n  public SimpleRpcScheduler(\n      Configuration conf,\n      int handlerCount,\n      int priorityHandlerCount,\n      int replicationHandlerCount,\n      PriorityFunction priority,\n      Abortable server,\n      int highPriorityLevel) {\n\n    int maxQueueLength = conf.getInt(RpcScheduler.IPC_SERVER_MAX_CALLQUEUE_LENGTH,\n        handlerCount * RpcServer.DEFAULT_MAX_CALLQUEUE_LENGTH_PER_HANDLER);\n    int maxPriorityQueueLength =\n        conf.getInt(RpcScheduler.IPC_SERVER_PRIORITY_MAX_CALLQUEUE_LENGTH, maxQueueLength);\n\n    this.priority = priority;\n    this.highPriorityLevel = highPriorityLevel;\n    this.abortable = server;\n\n    String callQueueType = conf.get(CALL_QUEUE_TYPE_CONF_KEY,\n        CALL_QUEUE_TYPE_FIFO_CONF_VALUE);\n    float callqReadShare = conf.getFloat(CALL_QUEUE_READ_SHARE_CONF_KEY, 0);\n    float callqScanShare = conf.getFloat(CALL_QUEUE_SCAN_SHARE_CONF_KEY, 0);\n\n    int codelTargetDelay = conf.getInt(CALL_QUEUE_CODEL_TARGET_DELAY,\n      CALL_QUEUE_CODEL_DEFAULT_TARGET_DELAY);\n    int codelInterval = conf.getInt(CALL_QUEUE_CODEL_INTERVAL,\n      CALL_QUEUE_CODEL_DEFAULT_INTERVAL);\n    double codelLifoThreshold = conf.getDouble(CALL_QUEUE_CODEL_LIFO_THRESHOLD,\n      CALL_QUEUE_CODEL_DEFAULT_LIFO_THRESHOLD);\n\n    float callQueuesHandlersFactor = conf.getFloat(CALL_QUEUE_HANDLER_FACTOR_CONF_KEY, 0);\n    int numCallQueues = Math.max(1, (int)Math.round(handlerCount * callQueuesHandlersFactor));\n    LOG.info(\"Using \" + callQueueType + \" as user call queue; numCallQueues=\" + numCallQueues +\n        \"; callQReadShare=\" + callqReadShare + \", callQScanShare=\" + callqScanShare);\n    if (numCallQueues > 1 && callqReadShare > 0) {\n      // multiple read/write queues\n      if (isDeadlineQueueType(callQueueType)) {\n        CallPriorityComparator callPriority = new CallPriorityComparator(conf, this.priority);\n        callExecutor = new RWQueueRpcExecutor(\"RW.deadline.Q\", handlerCount, numCallQueues,\n            callqReadShare, callqScanShare, maxQueueLength, conf, abortable,\n            BoundedPriorityBlockingQueue.class, callPriority);\n      } else if (callQueueType.equals(CALL_QUEUE_TYPE_CODEL_CONF_VALUE)) {\n        Object[] callQueueInitArgs = {maxQueueLength, codelTargetDelay, codelInterval,\n          codelLifoThreshold, numGeneralCallsDropped, numLifoModeSwitches};\n        callExecutor = new RWQueueRpcExecutor(\"RW.codel.Q\", handlerCount,\n          numCallQueues, callqReadShare, callqScanShare,\n          AdaptiveLifoCoDelCallQueue.class, callQueueInitArgs,\n          AdaptiveLifoCoDelCallQueue.class, callQueueInitArgs);\n      } else {\n        callExecutor = new RWQueueRpcExecutor(\"RW.fifo.Q\", handlerCount, numCallQueues,\n          callqReadShare, callqScanShare, maxQueueLength, conf, abortable);\n      }\n    } else {\n      // multiple queues\n      if (isDeadlineQueueType(callQueueType)) {\n        CallPriorityComparator callPriority = new CallPriorityComparator(conf, this.priority);\n        callExecutor =\n          new BalancedQueueRpcExecutor(\"B.deadline.Q\", handlerCount, numCallQueues,\n            conf, abortable, BoundedPriorityBlockingQueue.class, maxQueueLength, callPriority);\n      } else if (callQueueType.equals(CALL_QUEUE_TYPE_CODEL_CONF_VALUE)) {\n        callExecutor =\n          new BalancedQueueRpcExecutor(\"B.codel.Q\", handlerCount, numCallQueues,\n            conf, abortable, AdaptiveLifoCoDelCallQueue.class, maxQueueLength,\n            codelTargetDelay, codelInterval, codelLifoThreshold,\n            numGeneralCallsDropped, numLifoModeSwitches);\n      } else {\n        callExecutor = new BalancedQueueRpcExecutor(\"B.fifo.Q\", handlerCount,\n            numCallQueues, maxQueueLength, conf, abortable);\n      }\n    }\n    // Create 2 queues to help priorityExecutor be more scalable.\n    this.priorityExecutor = priorityHandlerCount > 0 ?\n      new BalancedQueueRpcExecutor(\"B.priority.fifo.Q\", priorityHandlerCount, 2,\n          maxPriorityQueueLength):\n      null;\n   this.replicationExecutor =\n     replicationHandlerCount > 0 ? new BalancedQueueRpcExecutor(\"B.replication.fifo.Q\",\n       replicationHandlerCount, 1, maxQueueLength, conf, abortable) : null;\n  }"
        ]
    ],
    "d8902ba0e68ec7bc38a8aa8d212353c380e5d378": [
        [
            "VerifyReplication::printUsage(String)",
            " 433  \n 434  \n 435  \n 436  \n 437  \n 438 -\n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  ",
            "  private static void printUsage(final String errorMsg) {\n    if (errorMsg != null && errorMsg.length() > 0) {\n      System.err.println(\"ERROR: \" + errorMsg);\n    }\n    System.err.println(\"Usage: verifyrep [--starttime=X]\" +\n        \" [--stoptime=Y] [--families=A] [--row-prefixes=B] <peerid> <tablename>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" starttime    beginning of the time range\");\n    System.err.println(\"              without endtime means from starttime to forever\");\n    System.err.println(\" endtime      end of the time range\");\n    System.err.println(\" versions     number of cell versions to verify\");\n    System.err.println(\" families     comma-separated list of families to copy\");\n    System.err.println(\" row-prefixes comma-separated list of row key prefixes to filter on \");\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" peerid       Id of the peer used for verification, must match the one given for replication\");\n    System.err.println(\" tablename    Name of the table to verify\");\n    System.err.println();\n    System.err.println(\"Examples:\");\n    System.err.println(\" To verify the data replicated from TestTable for a 1 hour window with peer #5 \");\n    System.err.println(\" $ bin/hbase \" +\n        \"org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication\" +\n        \" --starttime=1265875194289 --endtime=1265878794289 5 TestTable \");\n  }",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442 +\n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  ",
            "  private static void printUsage(final String errorMsg) {\n    if (errorMsg != null && errorMsg.length() > 0) {\n      System.err.println(\"ERROR: \" + errorMsg);\n    }\n    System.err.println(\"Usage: verifyrep [--starttime=X]\" +\n        \" [--endtime=Y] [--families=A] [--row-prefixes=B] <peerid> <tablename>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" starttime    beginning of the time range\");\n    System.err.println(\"              without endtime means from starttime to forever\");\n    System.err.println(\" endtime      end of the time range\");\n    System.err.println(\" versions     number of cell versions to verify\");\n    System.err.println(\" families     comma-separated list of families to copy\");\n    System.err.println(\" row-prefixes comma-separated list of row key prefixes to filter on \");\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" peerid       Id of the peer used for verification, must match the one given for replication\");\n    System.err.println(\" tablename    Name of the table to verify\");\n    System.err.println();\n    System.err.println(\"Examples:\");\n    System.err.println(\" To verify the data replicated from TestTable for a 1 hour window with peer #5 \");\n    System.err.println(\" $ bin/hbase \" +\n        \"org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication\" +\n        \" --starttime=1265875194289 --endtime=1265878794289 5 TestTable \");\n  }"
        ],
        [
            "VerifyReplication::doCommandLine(String)",
            " 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "  private static boolean doCommandLine(final String[] args) {\n    if (args.length < 2) {\n      printUsage(null);\n      return false;\n    }\n    //in case we've been run before, restore all parameters to their initial states\n    //Otherwise, if our previous run included a parameter not in args this time,\n    //we might hold on to the old value.\n    restoreDefaults();\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n        \n        final String batchArgKey = \"--batch=\";\n        if (cmd.startsWith(batchArgKey)) {\n          batch = Integer.parseInt(cmd.substring(batchArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        final String rowPrefixesKey = \"--row-prefixes=\";\n        if (cmd.startsWith(rowPrefixesKey)){\n          rowPrefixes = cmd.substring(rowPrefixesKey.length());\n          continue;\n        }\n\n        if (i == args.length-2) {\n          peerId = cmd;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        }\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }",
            " 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403 +\n 404 +\n 405 +\n 406 +\n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  ",
            "  private static boolean doCommandLine(final String[] args) {\n    if (args.length < 2) {\n      printUsage(null);\n      return false;\n    }\n    //in case we've been run before, restore all parameters to their initial states\n    //Otherwise, if our previous run included a parameter not in args this time,\n    //we might hold on to the old value.\n    restoreDefaults();\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n        \n        final String batchArgKey = \"--batch=\";\n        if (cmd.startsWith(batchArgKey)) {\n          batch = Integer.parseInt(cmd.substring(batchArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        final String rowPrefixesKey = \"--row-prefixes=\";\n        if (cmd.startsWith(rowPrefixesKey)){\n          rowPrefixes = cmd.substring(rowPrefixesKey.length());\n          continue;\n        }\n\n        if (cmd.startsWith(\"--\")) {\n          printUsage(\"Invalid argument '\" + cmd + \"'\");\n        }\n\n        if (i == args.length-2) {\n          peerId = cmd;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        }\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }"
        ]
    ],
    "7f44dfd85fc1aacd451cb8514fbce6dafd3443ca": [
        [
            "ScannerCallableWithReplicas::call(int)",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 -\n 220  ",
            "  @Override\n  public Result [] call(int timeout) throws IOException {\n    // If the active replica callable was closed somewhere, invoke the RPC to\n    // really close it. In the case of regular scanners, this applies. We make couple\n    // of RPCs to a RegionServer, and when that region is exhausted, we set\n    // the closed flag. Then an RPC is required to actually close the scanner.\n    if (currentScannerCallable != null && currentScannerCallable.closed) {\n      // For closing we target that exact scanner (and not do replica fallback like in\n      // the case of normal reads)\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Closing scanner id=\" + currentScannerCallable.scannerId);\n      }\n      Result[] r = currentScannerCallable.call(timeout);\n      currentScannerCallable = null;\n      return r;\n    }\n    // We need to do the following:\n    //1. When a scan goes out to a certain replica (default or not), we need to\n    //   continue to hit that until there is a failure. So store the last successfully invoked\n    //   replica\n    //2. We should close the \"losing\" scanners (scanners other than the ones we hear back\n    //   from first)\n    //\n    RegionLocations rl = RpcRetryingCallerWithReadReplicas.getRegionLocations(true,\n        RegionReplicaUtil.DEFAULT_REPLICA_ID, cConnection, tableName,\n        currentScannerCallable.getRow());\n\n    // allocate a boundedcompletion pool of some multiple of number of replicas.\n    // We want to accomodate some RPCs for redundant replica scans (but are still in progress)\n    ResultBoundedCompletionService<Pair<Result[], ScannerCallable>> cs =\n        new ResultBoundedCompletionService<Pair<Result[], ScannerCallable>>(\n            RpcRetryingCallerFactory.instantiate(ScannerCallableWithReplicas.this.conf), pool,\n            rl.size() * 5);\n\n    AtomicBoolean done = new AtomicBoolean(false);\n    replicaSwitched.set(false);\n    // submit call for the primary replica.\n    addCallsForCurrentReplica(cs, rl);\n\n    try {\n      // wait for the timeout to see whether the primary responds back\n      Future<Pair<Result[], ScannerCallable>> f = cs.poll(timeBeforeReplicas,\n          TimeUnit.MICROSECONDS); // Yes, microseconds\n      if (f != null) {\n        Pair<Result[], ScannerCallable> r = f.get(timeout, TimeUnit.MILLISECONDS);\n        if (r != null && r.getSecond() != null) {\n          updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n        }\n        return r == null ? null : r.getFirst(); //great we got a response\n      }\n    } catch (ExecutionException e) {\n      RpcRetryingCallerWithReadReplicas.throwEnrichedException(e, retries);\n    } catch (CancellationException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (TimeoutException e) {\n      throw new InterruptedIOException(e.getMessage());\n    }\n\n    // submit call for the all of the secondaries at once\n    // TODO: this may be an overkill for large region replication\n    addCallsForOtherReplicas(cs, rl, 0, rl.size() - 1);\n\n    try {\n      Future<Pair<Result[], ScannerCallable>> f = cs.poll(timeout, TimeUnit.MILLISECONDS);\n      if (f != null) {\n        Pair<Result[], ScannerCallable> r = f.get(timeout, TimeUnit.MILLISECONDS);\n        if (r != null && r.getSecond() != null) {\n          updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n        }\n        return r == null ? null : r.getFirst(); // great we got an answer\n      }\n    } catch (ExecutionException e) {\n      RpcRetryingCallerWithReadReplicas.throwEnrichedException(e, retries);\n    } catch (CancellationException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (TimeoutException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } finally {\n      // We get there because we were interrupted or because one or more of the\n      // calls succeeded or failed. In all case, we stop all our tasks.\n      cs.cancelAll();\n    }\n    return null; // unreachable\n  }",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206 +\n 207 +\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 +\n 223 +\n 224  ",
            "  @Override\n  public Result [] call(int timeout) throws IOException {\n    // If the active replica callable was closed somewhere, invoke the RPC to\n    // really close it. In the case of regular scanners, this applies. We make couple\n    // of RPCs to a RegionServer, and when that region is exhausted, we set\n    // the closed flag. Then an RPC is required to actually close the scanner.\n    if (currentScannerCallable != null && currentScannerCallable.closed) {\n      // For closing we target that exact scanner (and not do replica fallback like in\n      // the case of normal reads)\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Closing scanner id=\" + currentScannerCallable.scannerId);\n      }\n      Result[] r = currentScannerCallable.call(timeout);\n      currentScannerCallable = null;\n      return r;\n    }\n    // We need to do the following:\n    //1. When a scan goes out to a certain replica (default or not), we need to\n    //   continue to hit that until there is a failure. So store the last successfully invoked\n    //   replica\n    //2. We should close the \"losing\" scanners (scanners other than the ones we hear back\n    //   from first)\n    //\n    RegionLocations rl = RpcRetryingCallerWithReadReplicas.getRegionLocations(true,\n        RegionReplicaUtil.DEFAULT_REPLICA_ID, cConnection, tableName,\n        currentScannerCallable.getRow());\n\n    // allocate a boundedcompletion pool of some multiple of number of replicas.\n    // We want to accomodate some RPCs for redundant replica scans (but are still in progress)\n    ResultBoundedCompletionService<Pair<Result[], ScannerCallable>> cs =\n        new ResultBoundedCompletionService<Pair<Result[], ScannerCallable>>(\n            RpcRetryingCallerFactory.instantiate(ScannerCallableWithReplicas.this.conf), pool,\n            rl.size() * 5);\n\n    AtomicBoolean done = new AtomicBoolean(false);\n    replicaSwitched.set(false);\n    // submit call for the primary replica.\n    addCallsForCurrentReplica(cs, rl);\n\n    try {\n      // wait for the timeout to see whether the primary responds back\n      Future<Pair<Result[], ScannerCallable>> f = cs.poll(timeBeforeReplicas,\n          TimeUnit.MICROSECONDS); // Yes, microseconds\n      if (f != null) {\n        Pair<Result[], ScannerCallable> r = f.get(timeout, TimeUnit.MILLISECONDS);\n        if (r != null && r.getSecond() != null) {\n          updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n        }\n        return r == null ? null : r.getFirst(); //great we got a response\n      }\n    } catch (ExecutionException e) {\n      RpcRetryingCallerWithReadReplicas.throwEnrichedException(e, retries);\n    } catch (CancellationException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (TimeoutException e) {\n      throw new InterruptedIOException(e.getMessage());\n    }\n\n    // submit call for the all of the secondaries at once\n    // TODO: this may be an overkill for large region replication\n    addCallsForOtherReplicas(cs, rl, 0, rl.size() - 1);\n\n    try {\n      Future<Pair<Result[], ScannerCallable>> f = cs.poll(timeout, TimeUnit.MILLISECONDS);\n      if (f != null) {\n        Pair<Result[], ScannerCallable> r = f.get(timeout, TimeUnit.MILLISECONDS);\n        if (r != null && r.getSecond() != null) {\n          updateCurrentlyServingReplica(r.getSecond(), r.getFirst(), done, pool);\n        }\n        return r == null ? null : r.getFirst(); // great we got an answer\n      } else {\n        throw new IOException(\"Failed to get result within timeout, timeout=\"\n            + timeout + \"ms\");\n      }\n    } catch (ExecutionException e) {\n      RpcRetryingCallerWithReadReplicas.throwEnrichedException(e, retries);\n    } catch (CancellationException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } catch (TimeoutException e) {\n      throw new InterruptedIOException(e.getMessage());\n    } finally {\n      // We get there because we were interrupted or because one or more of the\n      // calls succeeded or failed. In all case, we stop all our tasks.\n      cs.cancelAll();\n    }\n    LOG.error(\"Imposible? Arrive at an unreachable line...\"); // unreachable\n    throw new IOException(\"Imposible? Arrive at an unreachable line...\");\n  }"
        ],
        [
            "HMasterCommandLine::run(String)",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 -\n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "  public int run(String args[]) throws Exception {\n    Options opt = new Options();\n    opt.addOption(\"localRegionServers\", true,\n      \"RegionServers to start in master process when running standalone\");\n    opt.addOption(\"masters\", true, \"Masters to start in this process\");\n    opt.addOption(\"minRegionServers\", true, \"Minimum RegionServers needed to host user tables\");\n    opt.addOption(\"backup\", false, \"Do not try to become HMaster until the primary fails\");\n\n    CommandLine cmd;\n    try {\n      cmd = new GnuParser().parse(opt, args);\n    } catch (ParseException e) {\n      LOG.error(\"Could not parse: \", e);\n      usage(null);\n      return 1;\n    }\n\n\n    if (cmd.hasOption(\"minRegionServers\")) {\n      String val = cmd.getOptionValue(\"minRegionServers\");\n      getConf().setInt(\"hbase.regions.server.count.min\",\n                  Integer.parseInt(val));\n      LOG.debug(\"minRegionServers set to \" + val);\n    }\n\n    // minRegionServers used to be minServers.  Support it too.\n    if (cmd.hasOption(\"minServers\")) {\n      String val = cmd.getOptionValue(\"minServers\");\n      getConf().setInt(\"hbase.regions.server.count.min\",\n                  Integer.parseInt(val));\n      LOG.debug(\"minServers set to \" + val);\n    }\n\n    // check if we are the backup master - override the conf if so\n    if (cmd.hasOption(\"backup\")) {\n      getConf().setBoolean(HConstants.MASTER_TYPE_BACKUP, true);\n    }\n\n    // How many regionservers to startup in this process (we run regionservers in same process as\n    // master when we are in local/standalone mode. Useful testing)\n    if (cmd.hasOption(\"localRegionServers\")) {\n      String val = cmd.getOptionValue(\"localRegionServers\");\n      getConf().setInt(\"hbase.regionservers\", Integer.parseInt(val));\n      LOG.debug(\"localRegionServers set to \" + val);\n    }\n    // How many masters to startup inside this process; useful testing\n    if (cmd.hasOption(\"masters\")) {\n      String val = cmd.getOptionValue(\"masters\");\n      getConf().setInt(\"hbase.masters\", Integer.parseInt(val));\n      LOG.debug(\"masters set to \" + val);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    List<String> remainingArgs = cmd.getArgList();\n    if (remainingArgs.size() != 1) {\n      usage(null);\n      return 1;\n    }\n\n    String command = remainingArgs.get(0);\n\n    if (\"start\".equals(command)) {\n      return startMaster();\n    } else if (\"stop\".equals(command)) {\n      return stopMaster();\n    } else if (\"clear\".equals(command)) {\n      return (ZNodeClearer.clear(getConf()) ? 0 : 1);\n    } else {\n      usage(\"Invalid command: \" + command);\n      return 1;\n    }\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "  public int run(String args[]) throws Exception {\n    Options opt = new Options();\n    opt.addOption(\"localRegionServers\", true,\n      \"RegionServers to start in master process when running standalone\");\n    opt.addOption(\"masters\", true, \"Masters to start in this process\");\n    opt.addOption(\"minRegionServers\", true, \"Minimum RegionServers needed to host user tables\");\n    opt.addOption(\"backup\", false, \"Do not try to become HMaster until the primary fails\");\n\n    CommandLine cmd;\n    try {\n      cmd = new GnuParser().parse(opt, args);\n    } catch (ParseException e) {\n      LOG.error(\"Could not parse: \", e);\n      usage(null);\n      return 1;\n    }\n\n\n    if (cmd.hasOption(\"minRegionServers\")) {\n      String val = cmd.getOptionValue(\"minRegionServers\");\n      getConf().setInt(\"hbase.regions.server.count.min\",\n                  Integer.parseInt(val));\n      LOG.debug(\"minRegionServers set to \" + val);\n    }\n\n    // minRegionServers used to be minServers.  Support it too.\n    if (cmd.hasOption(\"minServers\")) {\n      String val = cmd.getOptionValue(\"minServers\");\n      getConf().setInt(\"hbase.regions.server.count.min\", Integer.parseInt(val));\n      LOG.debug(\"minServers set to \" + val);\n    }\n\n    // check if we are the backup master - override the conf if so\n    if (cmd.hasOption(\"backup\")) {\n      getConf().setBoolean(HConstants.MASTER_TYPE_BACKUP, true);\n    }\n\n    // How many regionservers to startup in this process (we run regionservers in same process as\n    // master when we are in local/standalone mode. Useful testing)\n    if (cmd.hasOption(\"localRegionServers\")) {\n      String val = cmd.getOptionValue(\"localRegionServers\");\n      getConf().setInt(\"hbase.regionservers\", Integer.parseInt(val));\n      LOG.debug(\"localRegionServers set to \" + val);\n    }\n    // How many masters to startup inside this process; useful testing\n    if (cmd.hasOption(\"masters\")) {\n      String val = cmd.getOptionValue(\"masters\");\n      getConf().setInt(\"hbase.masters\", Integer.parseInt(val));\n      LOG.debug(\"masters set to \" + val);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    List<String> remainingArgs = cmd.getArgList();\n    if (remainingArgs.size() != 1) {\n      usage(null);\n      return 1;\n    }\n\n    String command = remainingArgs.get(0);\n\n    if (\"start\".equals(command)) {\n      return startMaster();\n    } else if (\"stop\".equals(command)) {\n      return stopMaster();\n    } else if (\"clear\".equals(command)) {\n      return (ZNodeClearer.clear(getConf()) ? 0 : 1);\n    } else {\n      usage(\"Invalid command: \" + command);\n      return 1;\n    }\n  }"
        ]
    ],
    "ed6e5d69995432aec4daf1a550edc8395fbc8930": [
        [
            "IPCUtil::wrapException(InetSocketAddress,Exception)",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "  /**\n   * Takes an Exception and the address we were trying to connect to and return an IOException with\n   * the input exception as the cause. The new exception provides the stack trace of the place where\n   * the exception is thrown and some extra diagnostics information. If the exception is\n   * ConnectException or SocketTimeoutException, return a new one of the same type; Otherwise return\n   * an IOException.\n   * @param addr target address\n   * @param exception the relevant exception\n   * @return an exception to throw\n   */\n  static IOException wrapException(InetSocketAddress addr, Exception exception) {\n    if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return (ConnectException) new ConnectException(\n          \"Call to \" + addr + \" failed on connection exception: \" + exception).initCause(exception);\n    } else if (exception instanceof SocketTimeoutException) {\n      return (SocketTimeoutException) new SocketTimeoutException(\n          \"Call to \" + addr + \" failed because \" + exception).initCause(exception);\n    } else if (exception instanceof ConnectionClosingException) {\n      return (ConnectionClosingException) new ConnectionClosingException(\n          \"Call to \" + addr + \" failed on local exception: \" + exception).initCause(exception);\n    } else {\n      return (IOException) new IOException(\n          \"Call to \" + addr + \" failed on local exception: \" + exception).initCause(exception);\n    }\n  }",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172 +\n 173 +\n 174 +\n 175 +\n 176 +\n 177 +\n 178  \n 179  \n 180  \n 181  \n 182  ",
            "  /**\n   * Takes an Exception and the address we were trying to connect to and return an IOException with\n   * the input exception as the cause. The new exception provides the stack trace of the place where\n   * the exception is thrown and some extra diagnostics information. If the exception is\n   * ConnectException or SocketTimeoutException, return a new one of the same type; Otherwise return\n   * an IOException.\n   * @param addr target address\n   * @param exception the relevant exception\n   * @return an exception to throw\n   */\n  static IOException wrapException(InetSocketAddress addr, Exception exception) {\n    if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return (ConnectException) new ConnectException(\n          \"Call to \" + addr + \" failed on connection exception: \" + exception).initCause(exception);\n    } else if (exception instanceof SocketTimeoutException) {\n      return (SocketTimeoutException) new SocketTimeoutException(\n          \"Call to \" + addr + \" failed because \" + exception).initCause(exception);\n    } else if (exception instanceof ConnectionClosingException) {\n      return (ConnectionClosingException) new ConnectionClosingException(\n          \"Call to \" + addr + \" failed on local exception: \" + exception).initCause(exception);\n    } else if (exception instanceof ServerTooBusyException) {\n      // we already have address in the exception message\n      return (IOException) exception;\n    } else if (exception instanceof DoNotRetryIOException) {\n      return (IOException) new DoNotRetryIOException(\n          \"Call to \" + addr + \" failed on local exception: \" + exception).initCause(exception);\n    } else {\n      return (IOException) new IOException(\n          \"Call to \" + addr + \" failed on local exception: \" + exception).initCause(exception);\n    }\n  }"
        ],
        [
            "TestHCM::TestPutThread::run()",
            "1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497 -\n1498  \n1499  \n1500  \n1501  \n1502  ",
            "    @Override\n    public void run() {\n      try {\n        Put p = new Put(ROW);\n        p.addColumn(FAM_NAM, new byte[]{0}, new byte[]{0});\n        table.put(p);\n      } catch (RetriesExhaustedWithDetailsException e) {\n        if (e.exceptions.get(0).getCause() instanceof ServerTooBusyException) {\n          getServerBusyException = 1;\n        }\n      } catch (IOException ignore) {\n      }\n    }",
            "1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497 +\n1498  \n1499  \n1500  \n1501  \n1502  ",
            "    @Override\n    public void run() {\n      try {\n        Put p = new Put(ROW);\n        p.addColumn(FAM_NAM, new byte[]{0}, new byte[]{0});\n        table.put(p);\n      } catch (RetriesExhaustedWithDetailsException e) {\n        if (e.exceptions.get(0) instanceof ServerTooBusyException) {\n          getServerBusyException = 1;\n        }\n      } catch (IOException ignore) {\n      }\n    }"
        ],
        [
            "TestHCM::TestGetThread::run()",
            "1513  \n1514  \n1515  \n1516  \n1517 -\n1518  \n1519 -\n1520 -\n1521 -\n1522 -\n1523  \n1524  \n1525  ",
            "    @Override\n    public void run() {\n      try {\n        Get g = new Get(ROW);\n        g.addColumn(FAM_NAM, new byte[]{0});\n        table.get(g);\n      } catch (RetriesExhaustedException e) {\n        if (e.getCause().getCause() instanceof ServerTooBusyException) {\n          getServerBusyException = 1;\n        }\n      } catch (IOException ignore) {\n      }\n    }",
            "1513  \n1514  \n1515  \n1516  \n1517 +\n1518  \n1519 +\n1520 +\n1521  \n1522  \n1523  ",
            "    @Override\n    public void run() {\n      try {\n        Get g = new Get(ROW);\n        g.addColumn(FAM_NAM, new byte[] { 0 });\n        table.get(g);\n      } catch (ServerTooBusyException e) {\n        getServerBusyException = 1;\n      } catch (IOException ignore) {\n      }\n    }"
        ]
    ],
    "d5aefbd2c78463bf4b3815f38f43dd745035cc93": [
        [
            "TestBlockEvictionFromClient::testBlockEvictionWithParallelScans()",
            " 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  ",
            "  @Test\n  public void testBlockEvictionWithParallelScans() throws Exception {\n    Table table = null;\n    try {\n      latch = new CountDownLatch(1);\n      final TableName tableName = TableName.valueOf(name.getMethodName());\n      // Create a table with block size as 1024\n      table = TEST_UTIL.createTable(tableName, FAMILIES_1, 1, 1024,\n          CustomInnerRegionObserver.class.getName());\n      // get the block cache and region\n      RegionLocator locator = TEST_UTIL.getConnection().getRegionLocator(tableName);\n      String regionName = locator.getAllRegionLocations().get(0).getRegionInfo().getEncodedName();\n      HRegion region = (HRegion) TEST_UTIL.getRSForFirstRegionInTable(tableName)\n          .getRegion(regionName);\n      HStore store = region.getStores().iterator().next();\n      CacheConfig cacheConf = store.getCacheConfig();\n      cacheConf.setCacheDataOnWrite(true);\n      cacheConf.setEvictOnClose(true);\n      BlockCache cache = cacheConf.getBlockCache();\n\n      // insert data. 2 Rows are added\n      Put put = new Put(ROW);\n      put.addColumn(FAMILY, QUALIFIER, data);\n      table.put(put);\n      put = new Put(ROW1);\n      put.addColumn(FAMILY, QUALIFIER, data);\n      table.put(put);\n      assertTrue(Bytes.equals(table.get(new Get(ROW)).value(), data));\n      // data was in memstore so don't expect any changes\n      // flush the data\n      System.out.println(\"Flushing cache in problematic area\");\n      // Should create one Hfile with 2 blocks\n      region.flush(true);\n      // Load cache\n      // Create three sets of scan\n      ScanThread[] scanThreads = initiateScan(table, false);\n      Thread.sleep(100);\n      checkForBlockEviction(cache, false, false);\n      for (ScanThread thread : scanThreads) {\n        thread.join();\n      }\n      // CustomInnerRegionObserver.sleepTime.set(0);\n      Iterator<CachedBlock> iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // read the data and expect same blocks, one new hit, no misses\n      assertTrue(Bytes.equals(table.get(new Get(ROW)).value(), data));\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // Check how this miss is happening\n      // insert a second column, read the row, no new blocks, 3 new hits\n      byte[] QUALIFIER2 = Bytes.add(QUALIFIER, QUALIFIER);\n      byte[] data2 = Bytes.add(data, data);\n      put = new Put(ROW);\n      put.addColumn(FAMILY, QUALIFIER2, data2);\n      table.put(put);\n      Result r = table.get(new Get(ROW));\n      assertTrue(Bytes.equals(r.getValue(FAMILY, QUALIFIER), data));\n      assertTrue(Bytes.equals(r.getValue(FAMILY, QUALIFIER2), data2));\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // flush, one new block\n      System.out.println(\"Flushing cache\");\n      region.flush(true);\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // compact, net minus two blocks, two hits, no misses\n      System.out.println(\"Compacting\");\n      assertEquals(2, store.getStorefilesCount());\n      store.triggerMajorCompaction();\n      region.compact(true);\n      waitForStoreFileCount(store, 1, 10000); // wait 10 seconds max\n      assertEquals(1, store.getStorefilesCount());\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // read the row, this should be a cache miss because we don't cache data\n      // blocks on compaction\n      r = table.get(new Get(ROW));\n      assertTrue(Bytes.equals(r.getValue(FAMILY, QUALIFIER), data));\n      assertTrue(Bytes.equals(r.getValue(FAMILY, QUALIFIER2), data2));\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n    } finally {\n      if (table != null) {\n        table.close();\n      }\n    }\n  }",
            " 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  ",
            "  @Test\n  public void testBlockEvictionWithParallelScans() throws Exception {\n    Table table = null;\n    try {\n      latch = new CountDownLatch(1);\n      final TableName tableName = TableName.valueOf(name.getMethodName());\n      // Create a table with block size as 1024\n      table = TEST_UTIL.createTable(tableName, FAMILIES_1, 1, 1024,\n          CustomInnerRegionObserver.class.getName());\n      // get the block cache and region\n      RegionLocator locator = TEST_UTIL.getConnection().getRegionLocator(tableName);\n      String regionName = locator.getAllRegionLocations().get(0).getRegionInfo().getEncodedName();\n      HRegion region = (HRegion) TEST_UTIL.getRSForFirstRegionInTable(tableName)\n          .getRegion(regionName);\n      HStore store = region.getStores().iterator().next();\n      CacheConfig cacheConf = store.getCacheConfig();\n      cacheConf.setCacheDataOnWrite(true);\n      cacheConf.setEvictOnClose(true);\n      BlockCache cache = cacheConf.getBlockCache();\n\n      // insert data. 2 Rows are added\n      Put put = new Put(ROW);\n      put.addColumn(FAMILY, QUALIFIER, data);\n      table.put(put);\n      put = new Put(ROW1);\n      put.addColumn(FAMILY, QUALIFIER, data);\n      table.put(put);\n      assertTrue(Bytes.equals(table.get(new Get(ROW)).value(), data));\n      // data was in memstore so don't expect any changes\n      // flush the data\n      // Should create one Hfile with 2 blocks\n      region.flush(true);\n      // Load cache\n      // Create three sets of scan\n      ScanThread[] scanThreads = initiateScan(table, false);\n      Thread.sleep(100);\n      checkForBlockEviction(cache, false, false);\n      for (ScanThread thread : scanThreads) {\n        thread.join();\n      }\n      // CustomInnerRegionObserver.sleepTime.set(0);\n      Iterator<CachedBlock> iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // read the data and expect same blocks, one new hit, no misses\n      assertTrue(Bytes.equals(table.get(new Get(ROW)).value(), data));\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // Check how this miss is happening\n      // insert a second column, read the row, no new blocks, 3 new hits\n      byte[] QUALIFIER2 = Bytes.add(QUALIFIER, QUALIFIER);\n      byte[] data2 = Bytes.add(data, data);\n      put = new Put(ROW);\n      put.addColumn(FAMILY, QUALIFIER2, data2);\n      table.put(put);\n      Result r = table.get(new Get(ROW));\n      assertTrue(Bytes.equals(r.getValue(FAMILY, QUALIFIER), data));\n      assertTrue(Bytes.equals(r.getValue(FAMILY, QUALIFIER2), data2));\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // flush, one new block\n      System.out.println(\"Flushing cache\");\n      region.flush(true);\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // compact, net minus two blocks, two hits, no misses\n      System.out.println(\"Compacting\");\n      assertEquals(2, store.getStorefilesCount());\n      store.triggerMajorCompaction();\n      region.compact(true);\n      waitForStoreFileCount(store, 1, 10000); // wait 10 seconds max\n      assertEquals(1, store.getStorefilesCount());\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n      // read the row, this should be a cache miss because we don't cache data\n      // blocks on compaction\n      r = table.get(new Get(ROW));\n      assertTrue(Bytes.equals(r.getValue(FAMILY, QUALIFIER), data));\n      assertTrue(Bytes.equals(r.getValue(FAMILY, QUALIFIER2), data2));\n      iterator = cache.iterator();\n      iterateBlockCache(cache, iterator);\n    } finally {\n      if (table != null) {\n        table.close();\n      }\n    }\n  }"
        ],
        [
            "TestBlockEvictionFromClient::testBlockRefCountAfterSplits()",
            " 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600 -\n 601  \n 602 -\n 603 -\n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  ",
            "  @Test\n  public void testBlockRefCountAfterSplits() throws IOException, InterruptedException {\n    Table table = null;\n    try {\n      final TableName tableName = TableName.valueOf(name.getMethodName());\n      table = TEST_UTIL.createTable(tableName, FAMILIES_1, 1, 1024);\n      // get the block cache and region\n      RegionLocator locator = TEST_UTIL.getConnection().getRegionLocator(tableName);\n      String regionName = locator.getAllRegionLocations().get(0).getRegionInfo().getEncodedName();\n      HRegion region =\n          (HRegion) TEST_UTIL.getRSForFirstRegionInTable(tableName).getRegion(regionName);\n      HStore store = region.getStores().iterator().next();\n      CacheConfig cacheConf = store.getCacheConfig();\n      cacheConf.setEvictOnClose(true);\n      BlockCache cache = cacheConf.getBlockCache();\n\n      Put put = new Put(ROW);\n      put.addColumn(FAMILY, QUALIFIER, data);\n      table.put(put);\n      region.flush(true);\n      put = new Put(ROW1);\n      put.addColumn(FAMILY, QUALIFIER, data);\n      table.put(put);\n      region.flush(true);\n      byte[] QUALIFIER2 = Bytes.add(QUALIFIER, QUALIFIER);\n      put = new Put(ROW2);\n      put.addColumn(FAMILY, QUALIFIER2, data2);\n      table.put(put);\n      put = new Put(ROW3);\n      put.addColumn(FAMILY, QUALIFIER2, data2);\n      table.put(put);\n      region.flush(true);\n      LOG.info(\"About to SPLIT on \" + Bytes.toString(ROW1));\n      TEST_UTIL.getAdmin().split(tableName, ROW1);\n      List<RegionInfo> tableRegions = TEST_UTIL.getAdmin().getRegions(tableName);\n      // Wait for splits\n      while (tableRegions.size() != 2) {\n        tableRegions = TEST_UTIL.getAdmin().getRegions(tableName);\n        Thread.sleep(100);\n        LOG.info(\"Waiting on SPLIT to complete...\");\n      }\n      region.compact(true);\n      Iterator<CachedBlock> iterator = cache.iterator();\n      // Though the split had created the HalfStorefileReader - the firstkey and lastkey scanners\n      // should be closed inorder to return those blocks\n      iterateBlockCache(cache, iterator);\n    } finally {\n      if (table != null) {\n        table.close();\n      }\n    }\n  }",
            " 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601 +\n 602 +\n 603 +\n 604 +\n 605 +\n 606 +\n 607 +\n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  ",
            "  @Test\n  public void testBlockRefCountAfterSplits() throws IOException, InterruptedException {\n    Table table = null;\n    try {\n      final TableName tableName = TableName.valueOf(name.getMethodName());\n      table = TEST_UTIL.createTable(tableName, FAMILIES_1, 1, 1024);\n      // get the block cache and region\n      RegionLocator locator = TEST_UTIL.getConnection().getRegionLocator(tableName);\n      String regionName = locator.getAllRegionLocations().get(0).getRegionInfo().getEncodedName();\n      HRegion region =\n          (HRegion) TEST_UTIL.getRSForFirstRegionInTable(tableName).getRegion(regionName);\n      HStore store = region.getStores().iterator().next();\n      CacheConfig cacheConf = store.getCacheConfig();\n      cacheConf.setEvictOnClose(true);\n      BlockCache cache = cacheConf.getBlockCache();\n\n      Put put = new Put(ROW);\n      put.addColumn(FAMILY, QUALIFIER, data);\n      table.put(put);\n      region.flush(true);\n      put = new Put(ROW1);\n      put.addColumn(FAMILY, QUALIFIER, data);\n      table.put(put);\n      region.flush(true);\n      byte[] QUALIFIER2 = Bytes.add(QUALIFIER, QUALIFIER);\n      put = new Put(ROW2);\n      put.addColumn(FAMILY, QUALIFIER2, data2);\n      table.put(put);\n      put = new Put(ROW3);\n      put.addColumn(FAMILY, QUALIFIER2, data2);\n      table.put(put);\n      region.flush(true);\n      LOG.info(\"About to SPLIT on \" + Bytes.toString(ROW1));\n      TEST_UTIL.getAdmin().split(tableName, ROW1);\n      // Wait for splits\n      Collection<ServerName> regionServers = TEST_UTIL.getAdmin().getRegionServers();\n      Iterator<ServerName> serverItr = regionServers.iterator();\n      serverItr.hasNext();\n      ServerName rs = serverItr.next();\n      List<RegionInfo> onlineRegions = TEST_UTIL.getAdmin().getRegions(rs);\n      while (onlineRegions.size() != 2) {\n        onlineRegions = TEST_UTIL.getAdmin().getRegions(rs);\n        Thread.sleep(100);\n        LOG.info(\"Waiting on SPLIT to complete...\");\n      }\n      region.compact(true);\n      Iterator<CachedBlock> iterator = cache.iterator();\n      // Though the split had created the HalfStorefileReader - the firstkey and lastkey scanners\n      // should be closed inorder to return those blocks\n      iterateBlockCache(cache, iterator);\n    } finally {\n      if (table != null) {\n        table.close();\n      }\n    }\n  }"
        ],
        [
            "BucketCache::cacheBlockWithWait(BlockCacheKey,Cacheable,boolean,boolean)",
            " 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430 -\n 431 -\n 432 -\n 433  \n 434 -\n 435 -\n 436 -\n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "  /**\n   * Cache the block to ramCache\n   * @param cacheKey block's cache key\n   * @param cachedItem block buffer\n   * @param inMemory if block is in-memory\n   * @param wait if true, blocking wait when queue is full\n   */\n  public void cacheBlockWithWait(BlockCacheKey cacheKey, Cacheable cachedItem, boolean inMemory,\n      boolean wait) {\n    if (LOG.isTraceEnabled()) LOG.trace(\"Caching key=\" + cacheKey + \", item=\" + cachedItem);\n    if (!cacheEnabled) {\n      return;\n    }\n\n    if (backingMap.containsKey(cacheKey)) {\n      Cacheable existingBlock = getBlock(cacheKey, false, false, false);\n      if (BlockCacheUtil.compareCacheBlock(cachedItem, existingBlock) != 0) {\n        throw new RuntimeException(\"Cached block contents differ, which should not have happened.\"\n            + \"cacheKey:\" + cacheKey);\n      }\n       String msg = \"Caching an already cached block: \" + cacheKey;\n       msg += \". This is harmless and can happen in rare cases (see HBASE-8547)\";\n       LOG.warn(msg);\n      return;\n    }\n\n    /*\n     * Stuff the entry into the RAM cache so it can get drained to the persistent store\n     */\n    RAMQueueEntry re =\n        new RAMQueueEntry(cacheKey, cachedItem, accessCount.incrementAndGet(), inMemory);\n    if (ramCache.putIfAbsent(cacheKey, re) != null) {\n      return;\n    }\n    int queueNum = (cacheKey.hashCode() & 0x7FFFFFFF) % writerQueues.size();\n    BlockingQueue<RAMQueueEntry> bq = writerQueues.get(queueNum);\n    boolean successfulAddition = false;\n    if (wait) {\n      try {\n        successfulAddition = bq.offer(re, DEFAULT_CACHE_WAIT_TIME, TimeUnit.MILLISECONDS);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n    } else {\n      successfulAddition = bq.offer(re);\n    }\n    if (!successfulAddition) {\n      ramCache.remove(cacheKey);\n      cacheStats.failInsert();\n    } else {\n      this.blockNumber.increment();\n      this.heapSize.add(cachedItem.heapSize());\n      blocksByHFile.add(cacheKey);\n    }\n  }",
            " 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430 +\n 431 +\n 432 +\n 433 +\n 434 +\n 435 +\n 436 +\n 437 +\n 438 +\n 439 +\n 440 +\n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  ",
            "  /**\n   * Cache the block to ramCache\n   * @param cacheKey block's cache key\n   * @param cachedItem block buffer\n   * @param inMemory if block is in-memory\n   * @param wait if true, blocking wait when queue is full\n   */\n  public void cacheBlockWithWait(BlockCacheKey cacheKey, Cacheable cachedItem, boolean inMemory,\n      boolean wait) {\n    if (LOG.isTraceEnabled()) LOG.trace(\"Caching key=\" + cacheKey + \", item=\" + cachedItem);\n    if (!cacheEnabled) {\n      return;\n    }\n\n    if (backingMap.containsKey(cacheKey)) {\n      Cacheable existingBlock = getBlock(cacheKey, false, false, false);\n      try {\n        if (BlockCacheUtil.compareCacheBlock(cachedItem, existingBlock) != 0) {\n          throw new RuntimeException(\"Cached block contents differ, which should not have happened.\"\n              + \"cacheKey:\" + cacheKey);\n        }\n        String msg = \"Caching an already cached block: \" + cacheKey;\n        msg += \". This is harmless and can happen in rare cases (see HBASE-8547)\";\n        LOG.warn(msg);\n      } finally {\n        // return the block since we need to decrement the count\n        returnBlock(cacheKey, existingBlock);\n      }\n      return;\n    }\n\n    /*\n     * Stuff the entry into the RAM cache so it can get drained to the persistent store\n     */\n    RAMQueueEntry re =\n        new RAMQueueEntry(cacheKey, cachedItem, accessCount.incrementAndGet(), inMemory);\n    if (ramCache.putIfAbsent(cacheKey, re) != null) {\n      return;\n    }\n    int queueNum = (cacheKey.hashCode() & 0x7FFFFFFF) % writerQueues.size();\n    BlockingQueue<RAMQueueEntry> bq = writerQueues.get(queueNum);\n    boolean successfulAddition = false;\n    if (wait) {\n      try {\n        successfulAddition = bq.offer(re, DEFAULT_CACHE_WAIT_TIME, TimeUnit.MILLISECONDS);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n      }\n    } else {\n      successfulAddition = bq.offer(re);\n    }\n    if (!successfulAddition) {\n      ramCache.remove(cacheKey);\n      cacheStats.failInsert();\n    } else {\n      this.blockNumber.increment();\n      this.heapSize.add(cachedItem.heapSize());\n      blocksByHFile.add(cacheKey);\n    }\n  }"
        ]
    ],
    "a47afc84cd17e1c01a20760cb3b007ca9afa5d35": [
        [
            "HStore::closeAndArchiveCompactedFiles()",
            "2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464 -\n2465  \n2466  \n2467  \n2468 -\n2469 -\n2470 -\n2471 -\n2472  \n2473  \n2474  \n2475  \n2476 -\n2477  \n2478  \n2479  \n2480  \n2481  \n2482  ",
            "  /**\n   * Closes and archives the compacted files under this store\n   */\n  public synchronized void closeAndArchiveCompactedFiles() throws IOException {\n    // ensure other threads do not attempt to archive the same files on close()\n    archiveLock.lock();\n    try {\n      lock.readLock().lock();\n      Collection<HStoreFile> copyCompactedfiles = null;\n      try {\n        Collection<HStoreFile> compactedfiles =\n            this.getStoreEngine().getStoreFileManager().getCompactedfiles();\n        if (compactedfiles != null && compactedfiles.size() != 0) {\n          // Do a copy under read lock\n          copyCompactedfiles = new ArrayList<>(compactedfiles);\n        } else {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"No compacted files to archive\");\n            return;\n          }\n        }\n      } finally {\n        lock.readLock().unlock();\n      }\n      if (copyCompactedfiles != null && !copyCompactedfiles.isEmpty()) {\n        removeCompactedfiles(copyCompactedfiles);\n      }\n    } finally {\n      archiveLock.unlock();\n    }\n  }",
            "2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468 +\n2469  \n2470  \n2471  \n2472 +\n2473  \n2474  \n2475  \n2476  \n2477 +\n2478  \n2479  \n2480  \n2481  \n2482  \n2483  ",
            "  /**\n   * Closes and archives the compacted files under this store\n   */\n  public synchronized void closeAndArchiveCompactedFiles() throws IOException {\n    // ensure other threads do not attempt to archive the same files on close()\n    archiveLock.lock();\n    try {\n      lock.readLock().lock();\n      Collection<HStoreFile> copyCompactedfiles = null;\n      try {\n        Collection<HStoreFile> compactedfiles =\n            this.getStoreEngine().getStoreFileManager().getCompactedfiles();\n        if (CollectionUtils.isNotEmpty(compactedfiles)) {\n          // Do a copy under read lock\n          copyCompactedfiles = new ArrayList<>(compactedfiles);\n        } else {\n          LOG.trace(\"No compacted files to archive\");\n        }\n      } finally {\n        lock.readLock().unlock();\n      }\n      if (CollectionUtils.isNotEmpty(copyCompactedfiles)) {\n        removeCompactedfiles(copyCompactedfiles);\n      }\n    } finally {\n      archiveLock.unlock();\n    }\n  }"
        ],
        [
            "HStore::recreateScanners(List,boolean,boolean,boolean,ScanQueryMatcher,byte,boolean,byte,boolean,long,boolean)",
            "1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962 -\n1963 -\n1964 -\n1965 -\n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  ",
            "  /**\n   * Recreates the scanners on the current list of active store file scanners\n   * @param currentFileScanners the current set of active store file scanners\n   * @param cacheBlocks cache the blocks or not\n   * @param usePread use pread or not\n   * @param isCompaction is the scanner for compaction\n   * @param matcher the scan query matcher\n   * @param startRow the scan's start row\n   * @param includeStartRow should the scan include the start row\n   * @param stopRow the scan's stop row\n   * @param includeStopRow should the scan include the stop row\n   * @param readPt the read point of the current scane\n   * @param includeMemstoreScanner whether the current scanner should include memstorescanner\n   * @return list of scanners recreated on the current Scanners\n   * @throws IOException\n   */\n  public List<KeyValueScanner> recreateScanners(List<KeyValueScanner> currentFileScanners,\n      boolean cacheBlocks, boolean usePread, boolean isCompaction, ScanQueryMatcher matcher,\n      byte[] startRow, boolean includeStartRow, byte[] stopRow, boolean includeStopRow, long readPt,\n      boolean includeMemstoreScanner) throws IOException {\n    this.lock.readLock().lock();\n    try {\n      Map<String, HStoreFile> name2File =\n          new HashMap<>(getStorefilesCount() + getCompactedFilesCount());\n      for (HStoreFile file : getStorefiles()) {\n        name2File.put(file.getFileInfo().getActiveFileName(), file);\n      }\n      if (getCompactedFiles() != null) {\n        for (HStoreFile file : getCompactedFiles()) {\n          name2File.put(file.getFileInfo().getActiveFileName(), file);\n        }\n      }\n      List<HStoreFile> filesToReopen = new ArrayList<>();\n      for (KeyValueScanner kvs : currentFileScanners) {\n        assert kvs.isFileScanner();\n        if (kvs.peek() == null) {\n          continue;\n        }\n        filesToReopen.add(name2File.get(kvs.getFilePath().getName()));\n      }\n      if (filesToReopen.isEmpty()) {\n        return null;\n      }\n      return getScanners(filesToReopen, cacheBlocks, false, false, matcher, startRow,\n        includeStartRow, stopRow, includeStopRow, readPt, false);\n    } finally {\n      this.lock.readLock().unlock();\n    }\n  }",
            "1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968 +\n1969 +\n1970 +\n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  ",
            "  /**\n   * Recreates the scanners on the current list of active store file scanners\n   * @param currentFileScanners the current set of active store file scanners\n   * @param cacheBlocks cache the blocks or not\n   * @param usePread use pread or not\n   * @param isCompaction is the scanner for compaction\n   * @param matcher the scan query matcher\n   * @param startRow the scan's start row\n   * @param includeStartRow should the scan include the start row\n   * @param stopRow the scan's stop row\n   * @param includeStopRow should the scan include the stop row\n   * @param readPt the read point of the current scane\n   * @param includeMemstoreScanner whether the current scanner should include memstorescanner\n   * @return list of scanners recreated on the current Scanners\n   * @throws IOException\n   */\n  public List<KeyValueScanner> recreateScanners(List<KeyValueScanner> currentFileScanners,\n      boolean cacheBlocks, boolean usePread, boolean isCompaction, ScanQueryMatcher matcher,\n      byte[] startRow, boolean includeStartRow, byte[] stopRow, boolean includeStopRow, long readPt,\n      boolean includeMemstoreScanner) throws IOException {\n    this.lock.readLock().lock();\n    try {\n      Map<String, HStoreFile> name2File =\n          new HashMap<>(getStorefilesCount() + getCompactedFilesCount());\n      for (HStoreFile file : getStorefiles()) {\n        name2File.put(file.getFileInfo().getActiveFileName(), file);\n      }\n      Collection<HStoreFile> compactedFiles = getCompactedFiles();\n      for (HStoreFile file : IterableUtils.emptyIfNull(compactedFiles)) {\n        name2File.put(file.getFileInfo().getActiveFileName(), file);\n      }\n      List<HStoreFile> filesToReopen = new ArrayList<>();\n      for (KeyValueScanner kvs : currentFileScanners) {\n        assert kvs.isFileScanner();\n        if (kvs.peek() == null) {\n          continue;\n        }\n        filesToReopen.add(name2File.get(kvs.getFilePath().getName()));\n      }\n      if (filesToReopen.isEmpty()) {\n        return null;\n      }\n      return getScanners(filesToReopen, cacheBlocks, false, false, matcher, startRow,\n        includeStartRow, stopRow, includeStopRow, readPt, false);\n    } finally {\n      this.lock.readLock().unlock();\n    }\n  }"
        ],
        [
            "HStore::requestCompaction(int,CompactionLifeCycleTracker,User)",
            "1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718 -\n1719 -\n1720 -\n1721  \n1722  \n1723  ",
            "  public Optional<CompactionContext> requestCompaction(int priority,\n      CompactionLifeCycleTracker tracker, User user) throws IOException {\n    // don't even select for compaction if writes are disabled\n    if (!this.areWritesEnabled()) {\n      return Optional.empty();\n    }\n    // Before we do compaction, try to get rid of unneeded files to simplify things.\n    removeUnneededFiles();\n\n    final CompactionContext compaction = storeEngine.createCompaction();\n    CompactionRequestImpl request = null;\n    this.lock.readLock().lock();\n    try {\n      synchronized (filesCompacting) {\n        // First, see if coprocessor would want to override selection.\n        if (this.getCoprocessorHost() != null) {\n          final List<HStoreFile> candidatesForCoproc = compaction.preSelect(this.filesCompacting);\n          boolean override = getCoprocessorHost().preCompactSelection(this,\n              candidatesForCoproc, tracker, user);\n          if (override) {\n            // Coprocessor is overriding normal file selection.\n            compaction.forceSelect(new CompactionRequestImpl(candidatesForCoproc));\n          }\n        }\n\n        // Normal case - coprocessor is not overriding file selection.\n        if (!compaction.hasSelection()) {\n          boolean isUserCompaction = priority == Store.PRIORITY_USER;\n          boolean mayUseOffPeak = offPeakHours.isOffPeakHour() &&\n              offPeakCompactionTracker.compareAndSet(false, true);\n          try {\n            compaction.select(this.filesCompacting, isUserCompaction,\n              mayUseOffPeak, forceMajor && filesCompacting.isEmpty());\n          } catch (IOException e) {\n            if (mayUseOffPeak) {\n              offPeakCompactionTracker.set(false);\n            }\n            throw e;\n          }\n          assert compaction.hasSelection();\n          if (mayUseOffPeak && !compaction.getRequest().isOffPeak()) {\n            // Compaction policy doesn't want to take advantage of off-peak.\n            offPeakCompactionTracker.set(false);\n          }\n        }\n        if (this.getCoprocessorHost() != null) {\n          this.getCoprocessorHost().postCompactSelection(\n              this, ImmutableList.copyOf(compaction.getRequest().getFiles()), tracker,\n              compaction.getRequest(), user);\n        }\n        // Finally, we have the resulting files list. Check if we have any files at all.\n        request = compaction.getRequest();\n        Collection<HStoreFile> selectedFiles = request.getFiles();\n        if (selectedFiles.isEmpty()) {\n          return Optional.empty();\n        }\n\n        addToCompactingFiles(selectedFiles);\n\n        // If we're enqueuing a major, clear the force flag.\n        this.forceMajor = this.forceMajor && !request.isMajor();\n\n        // Set common request properties.\n        // Set priority, either override value supplied by caller or from store.\n        request.setPriority((priority != Store.NO_PRIORITY) ? priority : getCompactPriority());\n        request.setDescription(getRegionInfo().getRegionNameAsString(), getColumnFamilyName());\n        request.setTracker(tracker);\n      }\n    } finally {\n      this.lock.readLock().unlock();\n    }\n\n    LOG.debug(getRegionInfo().getEncodedName() + \" - \" + getColumnFamilyName()\n        + \": Initiating \" + (request.isMajor() ? \"major\" : \"minor\") + \" compaction\"\n        + (request.isAllFiles() ? \" (all files)\" : \"\"));\n    this.region.reportCompactionRequestStart(request.isMajor());\n    return Optional.of(compaction);\n  }",
            "1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  \n1680  \n1681  \n1682  \n1683  \n1684  \n1685  \n1686  \n1687  \n1688  \n1689  \n1690  \n1691  \n1692  \n1693  \n1694  \n1695  \n1696  \n1697  \n1698  \n1699  \n1700  \n1701  \n1702  \n1703  \n1704  \n1705  \n1706  \n1707  \n1708  \n1709  \n1710  \n1711  \n1712  \n1713  \n1714  \n1715  \n1716  \n1717  \n1718  \n1719  \n1720  \n1721 +\n1722 +\n1723 +\n1724 +\n1725 +\n1726  \n1727  \n1728  ",
            "  public Optional<CompactionContext> requestCompaction(int priority,\n      CompactionLifeCycleTracker tracker, User user) throws IOException {\n    // don't even select for compaction if writes are disabled\n    if (!this.areWritesEnabled()) {\n      return Optional.empty();\n    }\n    // Before we do compaction, try to get rid of unneeded files to simplify things.\n    removeUnneededFiles();\n\n    final CompactionContext compaction = storeEngine.createCompaction();\n    CompactionRequestImpl request = null;\n    this.lock.readLock().lock();\n    try {\n      synchronized (filesCompacting) {\n        // First, see if coprocessor would want to override selection.\n        if (this.getCoprocessorHost() != null) {\n          final List<HStoreFile> candidatesForCoproc = compaction.preSelect(this.filesCompacting);\n          boolean override = getCoprocessorHost().preCompactSelection(this,\n              candidatesForCoproc, tracker, user);\n          if (override) {\n            // Coprocessor is overriding normal file selection.\n            compaction.forceSelect(new CompactionRequestImpl(candidatesForCoproc));\n          }\n        }\n\n        // Normal case - coprocessor is not overriding file selection.\n        if (!compaction.hasSelection()) {\n          boolean isUserCompaction = priority == Store.PRIORITY_USER;\n          boolean mayUseOffPeak = offPeakHours.isOffPeakHour() &&\n              offPeakCompactionTracker.compareAndSet(false, true);\n          try {\n            compaction.select(this.filesCompacting, isUserCompaction,\n              mayUseOffPeak, forceMajor && filesCompacting.isEmpty());\n          } catch (IOException e) {\n            if (mayUseOffPeak) {\n              offPeakCompactionTracker.set(false);\n            }\n            throw e;\n          }\n          assert compaction.hasSelection();\n          if (mayUseOffPeak && !compaction.getRequest().isOffPeak()) {\n            // Compaction policy doesn't want to take advantage of off-peak.\n            offPeakCompactionTracker.set(false);\n          }\n        }\n        if (this.getCoprocessorHost() != null) {\n          this.getCoprocessorHost().postCompactSelection(\n              this, ImmutableList.copyOf(compaction.getRequest().getFiles()), tracker,\n              compaction.getRequest(), user);\n        }\n        // Finally, we have the resulting files list. Check if we have any files at all.\n        request = compaction.getRequest();\n        Collection<HStoreFile> selectedFiles = request.getFiles();\n        if (selectedFiles.isEmpty()) {\n          return Optional.empty();\n        }\n\n        addToCompactingFiles(selectedFiles);\n\n        // If we're enqueuing a major, clear the force flag.\n        this.forceMajor = this.forceMajor && !request.isMajor();\n\n        // Set common request properties.\n        // Set priority, either override value supplied by caller or from store.\n        request.setPriority((priority != Store.NO_PRIORITY) ? priority : getCompactPriority());\n        request.setDescription(getRegionInfo().getRegionNameAsString(), getColumnFamilyName());\n        request.setTracker(tracker);\n      }\n    } finally {\n      this.lock.readLock().unlock();\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(getRegionInfo().getEncodedName() + \" - \" + getColumnFamilyName()\n          + \": Initiating \" + (request.isMajor() ? \"major\" : \"minor\") + \" compaction\"\n          + (request.isAllFiles() ? \" (all files)\" : \"\"));\n    }\n    this.region.reportCompactionRequestStart(request.isMajor());\n    return Optional.of(compaction);\n  }"
        ],
        [
            "HStore::replayCompactionMarker(CompactionDescriptor,boolean,boolean)",
            "1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521 -\n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  ",
            "  /**\n   * Call to complete a compaction. Its for the case where we find in the WAL a compaction\n   * that was not finished.  We could find one recovering a WAL after a regionserver crash.\n   * See HBASE-2231.\n   * @param compaction\n   */\n  public void replayCompactionMarker(CompactionDescriptor compaction, boolean pickCompactionFiles,\n      boolean removeFiles) throws IOException {\n    LOG.debug(\"Completing compaction from the WAL marker\");\n    List<String> compactionInputs = compaction.getCompactionInputList();\n    List<String> compactionOutputs = Lists.newArrayList(compaction.getCompactionOutputList());\n\n    // The Compaction Marker is written after the compaction is completed,\n    // and the files moved into the region/family folder.\n    //\n    // If we crash after the entry is written, we may not have removed the\n    // input files, but the output file is present.\n    // (The unremoved input files will be removed by this function)\n    //\n    // If we scan the directory and the file is not present, it can mean that:\n    //   - The file was manually removed by the user\n    //   - The file was removed as consequence of subsequent compaction\n    // so, we can't do anything with the \"compaction output list\" because those\n    // files have already been loaded when opening the region (by virtue of\n    // being in the store's folder) or they may be missing due to a compaction.\n\n    String familyName = this.getColumnFamilyName();\n    List<String> inputFiles = new ArrayList<>(compactionInputs.size());\n    for (String compactionInput : compactionInputs) {\n      Path inputPath = fs.getStoreFilePath(familyName, compactionInput);\n      inputFiles.add(inputPath.getName());\n    }\n\n    //some of the input files might already be deleted\n    List<HStoreFile> inputStoreFiles = new ArrayList<>(compactionInputs.size());\n    for (HStoreFile sf : this.getStorefiles()) {\n      if (inputFiles.contains(sf.getPath().getName())) {\n        inputStoreFiles.add(sf);\n      }\n    }\n\n    // check whether we need to pick up the new files\n    List<HStoreFile> outputStoreFiles = new ArrayList<>(compactionOutputs.size());\n\n    if (pickCompactionFiles) {\n      for (HStoreFile sf : this.getStorefiles()) {\n        compactionOutputs.remove(sf.getPath().getName());\n      }\n      for (String compactionOutput : compactionOutputs) {\n        StoreFileInfo storeFileInfo = fs.getStoreFileInfo(getColumnFamilyName(), compactionOutput);\n        HStoreFile storeFile = createStoreFileAndReader(storeFileInfo);\n        outputStoreFiles.add(storeFile);\n      }\n    }\n\n    if (!inputStoreFiles.isEmpty() || !outputStoreFiles.isEmpty()) {\n      LOG.info(\"Replaying compaction marker, replacing input files: \" +\n          inputStoreFiles + \" with output files : \" + outputStoreFiles);\n      this.replaceStoreFiles(inputStoreFiles, outputStoreFiles);\n      this.completeCompaction(inputStoreFiles);\n    }\n  }",
            "1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524 +\n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  ",
            "  /**\n   * Call to complete a compaction. Its for the case where we find in the WAL a compaction\n   * that was not finished.  We could find one recovering a WAL after a regionserver crash.\n   * See HBASE-2231.\n   * @param compaction\n   */\n  public void replayCompactionMarker(CompactionDescriptor compaction, boolean pickCompactionFiles,\n      boolean removeFiles) throws IOException {\n    LOG.debug(\"Completing compaction from the WAL marker\");\n    List<String> compactionInputs = compaction.getCompactionInputList();\n    List<String> compactionOutputs = Lists.newArrayList(compaction.getCompactionOutputList());\n\n    // The Compaction Marker is written after the compaction is completed,\n    // and the files moved into the region/family folder.\n    //\n    // If we crash after the entry is written, we may not have removed the\n    // input files, but the output file is present.\n    // (The unremoved input files will be removed by this function)\n    //\n    // If we scan the directory and the file is not present, it can mean that:\n    //   - The file was manually removed by the user\n    //   - The file was removed as consequence of subsequent compaction\n    // so, we can't do anything with the \"compaction output list\" because those\n    // files have already been loaded when opening the region (by virtue of\n    // being in the store's folder) or they may be missing due to a compaction.\n\n    String familyName = this.getColumnFamilyName();\n    Set<String> inputFiles = new HashSet<>();\n    for (String compactionInput : compactionInputs) {\n      Path inputPath = fs.getStoreFilePath(familyName, compactionInput);\n      inputFiles.add(inputPath.getName());\n    }\n\n    //some of the input files might already be deleted\n    List<HStoreFile> inputStoreFiles = new ArrayList<>(compactionInputs.size());\n    for (HStoreFile sf : this.getStorefiles()) {\n      if (inputFiles.contains(sf.getPath().getName())) {\n        inputStoreFiles.add(sf);\n      }\n    }\n\n    // check whether we need to pick up the new files\n    List<HStoreFile> outputStoreFiles = new ArrayList<>(compactionOutputs.size());\n\n    if (pickCompactionFiles) {\n      for (HStoreFile sf : this.getStorefiles()) {\n        compactionOutputs.remove(sf.getPath().getName());\n      }\n      for (String compactionOutput : compactionOutputs) {\n        StoreFileInfo storeFileInfo = fs.getStoreFileInfo(getColumnFamilyName(), compactionOutput);\n        HStoreFile storeFile = createStoreFileAndReader(storeFileInfo);\n        outputStoreFiles.add(storeFile);\n      }\n    }\n\n    if (!inputStoreFiles.isEmpty() || !outputStoreFiles.isEmpty()) {\n      LOG.info(\"Replaying compaction marker, replacing input files: \" +\n          inputStoreFiles + \" with output files : \" + outputStoreFiles);\n      this.replaceStoreFiles(inputStoreFiles, outputStoreFiles);\n      this.completeCompaction(inputStoreFiles);\n    }\n  }"
        ],
        [
            "HStore::removeUnneededFiles()",
            "1736  \n1737  \n1738  \n1739 -\n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757 -\n1758  \n1759 -\n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  ",
            "  private void removeUnneededFiles() throws IOException {\n    if (!conf.getBoolean(\"hbase.store.delete.expired.storefile\", true)) return;\n    if (getColumnFamilyDescriptor().getMinVersions() > 0) {\n      LOG.debug(\"Skipping expired store file removal due to min version being \" +\n          getColumnFamilyDescriptor().getMinVersions());\n      return;\n    }\n    this.lock.readLock().lock();\n    Collection<HStoreFile> delSfs = null;\n    try {\n      synchronized (filesCompacting) {\n        long cfTtl = getStoreFileTtl();\n        if (cfTtl != Long.MAX_VALUE) {\n          delSfs = storeEngine.getStoreFileManager().getUnneededFiles(\n              EnvironmentEdgeManager.currentTime() - cfTtl, filesCompacting);\n          addToCompactingFiles(delSfs);\n        }\n      }\n    } finally {\n      this.lock.readLock().unlock();\n    }\n    if (delSfs == null || delSfs.isEmpty()) return;\n\n    Collection<HStoreFile> newFiles = new ArrayList<>(); // No new files.\n    writeCompactionWalRecord(delSfs, newFiles);\n    replaceStoreFiles(delSfs, newFiles);\n    completeCompaction(delSfs);\n    LOG.info(\"Completed removal of \" + delSfs.size() + \" unnecessary (expired) file(s) in \"\n        + this + \" of \" + this.getRegionInfo().getRegionNameAsString()\n        + \"; total size for store is \" + TraditionalBinaryPrefix.long2String(storeSize, \"\", 1));\n  }",
            "1743  \n1744  \n1745  \n1746 +\n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765 +\n1766 +\n1767 +\n1768 +\n1769 +\n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  ",
            "  private void removeUnneededFiles() throws IOException {\n    if (!conf.getBoolean(\"hbase.store.delete.expired.storefile\", true)) return;\n    if (getColumnFamilyDescriptor().getMinVersions() > 0) {\n      LOG.debug(\"Skipping expired store file removal due to min version being {}\",\n          getColumnFamilyDescriptor().getMinVersions());\n      return;\n    }\n    this.lock.readLock().lock();\n    Collection<HStoreFile> delSfs = null;\n    try {\n      synchronized (filesCompacting) {\n        long cfTtl = getStoreFileTtl();\n        if (cfTtl != Long.MAX_VALUE) {\n          delSfs = storeEngine.getStoreFileManager().getUnneededFiles(\n              EnvironmentEdgeManager.currentTime() - cfTtl, filesCompacting);\n          addToCompactingFiles(delSfs);\n        }\n      }\n    } finally {\n      this.lock.readLock().unlock();\n    }\n\n    if (CollectionUtils.isEmpty(delSfs)) {\n      return;\n    }\n\n    Collection<HStoreFile> newFiles = Collections.emptyList(); // No new files.\n    writeCompactionWalRecord(delSfs, newFiles);\n    replaceStoreFiles(delSfs, newFiles);\n    completeCompaction(delSfs);\n    LOG.info(\"Completed removal of \" + delSfs.size() + \" unnecessary (expired) file(s) in \"\n        + this + \" of \" + this.getRegionInfo().getRegionNameAsString()\n        + \"; total size for store is \" + TraditionalBinaryPrefix.long2String(storeSize, \"\", 1));\n  }"
        ],
        [
            "HStore::clearCompactedfiles(List)",
            "2563  \n2564 -\n2565 -\n2566 -\n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  ",
            "  private void clearCompactedfiles(List<HStoreFile> filesToRemove) throws IOException {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Clearing the compacted file \" + filesToRemove + \" from this store\");\n    }\n    try {\n      lock.writeLock().lock();\n      this.getStoreEngine().getStoreFileManager().removeCompactedFiles(filesToRemove);\n    } finally {\n      lock.writeLock().unlock();\n    }\n  }",
            "2558  \n2559 +\n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  ",
            "  private void clearCompactedfiles(List<HStoreFile> filesToRemove) throws IOException {\n    LOG.trace(\"Clearing the compacted file {} from this store\", filesToRemove);\n    try {\n      lock.writeLock().lock();\n      this.getStoreEngine().getStoreFileManager().removeCompactedFiles(filesToRemove);\n    } finally {\n      lock.writeLock().unlock();\n    }\n  }"
        ],
        [
            "HStore::StoreFlusherImpl::commit(MonitoredTask)",
            "2198  \n2199  \n2200 -\n2201  \n2202  \n2203  \n2204  \n2205  \n2206  \n2207  \n2208  \n2209  \n2210 -\n2211  \n2212  \n2213  \n2214  \n2215  \n2216  \n2217  \n2218 -\n2219  \n2220  \n2221  \n2222  \n2223  \n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  ",
            "    @Override\n    public boolean commit(MonitoredTask status) throws IOException {\n      if (this.tempFiles == null || this.tempFiles.isEmpty()) {\n        return false;\n      }\n      List<HStoreFile> storeFiles = new ArrayList<>(this.tempFiles.size());\n      for (Path storeFilePath : tempFiles) {\n        try {\n          HStoreFile sf = HStore.this.commitFile(storeFilePath, cacheFlushSeqNum, status);\n          outputFileSize += sf.getReader().length();\n          storeFiles.add(sf);\n        } catch (IOException ex) {\n          LOG.error(\"Failed to commit store file \" + storeFilePath, ex);\n          // Try to delete the files we have committed before.\n          for (HStoreFile sf : storeFiles) {\n            Path pathToDelete = sf.getPath();\n            try {\n              sf.deleteStoreFile();\n            } catch (IOException deleteEx) {\n              LOG.error(HBaseMarkers.FATAL, \"Failed to delete store file we committed, \"\n                  + \"halting \" + pathToDelete, ex);\n              Runtime.getRuntime().halt(1);\n            }\n          }\n          throw new IOException(\"Failed to commit the flush\", ex);\n        }\n      }\n\n      for (HStoreFile sf : storeFiles) {\n        if (HStore.this.getCoprocessorHost() != null) {\n          HStore.this.getCoprocessorHost().postFlush(HStore.this, sf, tracker);\n        }\n        committedFiles.add(sf.getPath());\n      }\n\n      HStore.this.flushedCellsCount += cacheFlushCount;\n      HStore.this.flushedCellsSize += cacheFlushSize;\n      HStore.this.flushedOutputFileSize += outputFileSize;\n\n      // Add new file to store files.  Clear snapshot too while we have the Store write lock.\n      return HStore.this.updateStorefiles(storeFiles, snapshot.getId());\n    }",
            "2203  \n2204  \n2205 +\n2206  \n2207  \n2208  \n2209  \n2210  \n2211  \n2212  \n2213  \n2214  \n2215 +\n2216  \n2217  \n2218  \n2219  \n2220  \n2221  \n2222  \n2223 +\n2224  \n2225  \n2226  \n2227  \n2228  \n2229  \n2230  \n2231  \n2232  \n2233  \n2234  \n2235  \n2236  \n2237  \n2238  \n2239  \n2240  \n2241  \n2242  \n2243  \n2244  ",
            "    @Override\n    public boolean commit(MonitoredTask status) throws IOException {\n      if (CollectionUtils.isEmpty(this.tempFiles)) {\n        return false;\n      }\n      List<HStoreFile> storeFiles = new ArrayList<>(this.tempFiles.size());\n      for (Path storeFilePath : tempFiles) {\n        try {\n          HStoreFile sf = HStore.this.commitFile(storeFilePath, cacheFlushSeqNum, status);\n          outputFileSize += sf.getReader().length();\n          storeFiles.add(sf);\n        } catch (IOException ex) {\n          LOG.error(\"Failed to commit store file {}\", storeFilePath, ex);\n          // Try to delete the files we have committed before.\n          for (HStoreFile sf : storeFiles) {\n            Path pathToDelete = sf.getPath();\n            try {\n              sf.deleteStoreFile();\n            } catch (IOException deleteEx) {\n              LOG.error(HBaseMarkers.FATAL, \"Failed to delete store file we committed, \"\n                  + \"halting {}\", pathToDelete, ex);\n              Runtime.getRuntime().halt(1);\n            }\n          }\n          throw new IOException(\"Failed to commit the flush\", ex);\n        }\n      }\n\n      for (HStoreFile sf : storeFiles) {\n        if (HStore.this.getCoprocessorHost() != null) {\n          HStore.this.getCoprocessorHost().postFlush(HStore.this, sf, tracker);\n        }\n        committedFiles.add(sf.getPath());\n      }\n\n      HStore.this.flushedCellsCount += cacheFlushCount;\n      HStore.this.flushedCellsSize += cacheFlushSize;\n      HStore.this.flushedOutputFileSize += outputFileSize;\n\n      // Add new file to store files.  Clear snapshot too while we have the Store write lock.\n      return HStore.this.updateStorefiles(storeFiles, snapshot.getId());\n    }"
        ],
        [
            "HStore::bulkLoadHFile(byte,String,Path)",
            " 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831 -\n 832 -\n 833  \n 834  \n 835  ",
            "  public Path bulkLoadHFile(byte[] family, String srcPathStr, Path dstPath) throws IOException {\n    Path srcPath = new Path(srcPathStr);\n    try {\n      fs.commitStoreFile(srcPath, dstPath);\n    } finally {\n      if (this.getCoprocessorHost() != null) {\n        this.getCoprocessorHost().postCommitStoreFile(family, srcPath, dstPath);\n      }\n    }\n\n    LOG.info(\"Loaded HFile \" + srcPath + \" into store '\" + getColumnFamilyName() + \"' as \"\n        + dstPath + \" - updating store file list.\");\n\n    HStoreFile sf = createStoreFileAndReader(dstPath);\n    bulkLoadHFile(sf);\n\n    LOG.info(\"Successfully loaded store file \" + srcPath + \" into store \" + this\n        + \" (new location: \" + dstPath + \")\");\n\n    return dstPath;\n  }",
            " 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834 +\n 835 +\n 836  \n 837  \n 838  ",
            "  public Path bulkLoadHFile(byte[] family, String srcPathStr, Path dstPath) throws IOException {\n    Path srcPath = new Path(srcPathStr);\n    try {\n      fs.commitStoreFile(srcPath, dstPath);\n    } finally {\n      if (this.getCoprocessorHost() != null) {\n        this.getCoprocessorHost().postCommitStoreFile(family, srcPath, dstPath);\n      }\n    }\n\n    LOG.info(\"Loaded HFile \" + srcPath + \" into store '\" + getColumnFamilyName() + \"' as \"\n        + dstPath + \" - updating store file list.\");\n\n    HStoreFile sf = createStoreFileAndReader(dstPath);\n    bulkLoadHFile(sf);\n\n    LOG.info(\"Successfully loaded store file {} into store {} (new location: {})\",\n        srcPath, this, dstPath);\n\n    return dstPath;\n  }"
        ],
        [
            "HStore::flushCache(long,MemStoreSnapshot,MonitoredTask,ThroughputController,FlushLifeCycleTracker)",
            " 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981 -\n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989 -\n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  ",
            "  /**\n   * Write out current snapshot. Presumes {@link #snapshot()} has been called previously.\n   * @param logCacheFlushId flush sequence number\n   * @param snapshot\n   * @param status\n   * @param throughputController\n   * @return The path name of the tmp file to which the store was flushed\n   * @throws IOException if exception occurs during process\n   */\n  protected List<Path> flushCache(final long logCacheFlushId, MemStoreSnapshot snapshot,\n      MonitoredTask status, ThroughputController throughputController,\n      FlushLifeCycleTracker tracker) throws IOException {\n    // If an exception happens flushing, we let it out without clearing\n    // the memstore snapshot.  The old snapshot will be returned when we say\n    // 'snapshot', the next time flush comes around.\n    // Retry after catching exception when flushing, otherwise server will abort\n    // itself\n    StoreFlusher flusher = storeEngine.getStoreFlusher();\n    IOException lastException = null;\n    for (int i = 0; i < flushRetriesNumber; i++) {\n      try {\n        List<Path> pathNames =\n            flusher.flushSnapshot(snapshot, logCacheFlushId, status, throughputController, tracker);\n        Path lastPathName = null;\n        try {\n          for (Path pathName : pathNames) {\n            lastPathName = pathName;\n            validateStoreFile(pathName);\n          }\n          return pathNames;\n        } catch (Exception e) {\n          LOG.warn(\"Failed validating store file \" + lastPathName + \", retrying num=\" + i, e);\n          if (e instanceof IOException) {\n            lastException = (IOException) e;\n          } else {\n            lastException = new IOException(e);\n          }\n        }\n      } catch (IOException e) {\n        LOG.warn(\"Failed flushing store file, retrying num=\" + i, e);\n        lastException = e;\n      }\n      if (lastException != null && i < (flushRetriesNumber - 1)) {\n        try {\n          Thread.sleep(pauseTime);\n        } catch (InterruptedException e) {\n          IOException iie = new InterruptedIOException();\n          iie.initCause(e);\n          throw iie;\n        }\n      }\n    }\n    throw lastException;\n  }",
            " 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984 +\n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992 +\n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  ",
            "  /**\n   * Write out current snapshot. Presumes {@link #snapshot()} has been called previously.\n   * @param logCacheFlushId flush sequence number\n   * @param snapshot\n   * @param status\n   * @param throughputController\n   * @return The path name of the tmp file to which the store was flushed\n   * @throws IOException if exception occurs during process\n   */\n  protected List<Path> flushCache(final long logCacheFlushId, MemStoreSnapshot snapshot,\n      MonitoredTask status, ThroughputController throughputController,\n      FlushLifeCycleTracker tracker) throws IOException {\n    // If an exception happens flushing, we let it out without clearing\n    // the memstore snapshot.  The old snapshot will be returned when we say\n    // 'snapshot', the next time flush comes around.\n    // Retry after catching exception when flushing, otherwise server will abort\n    // itself\n    StoreFlusher flusher = storeEngine.getStoreFlusher();\n    IOException lastException = null;\n    for (int i = 0; i < flushRetriesNumber; i++) {\n      try {\n        List<Path> pathNames =\n            flusher.flushSnapshot(snapshot, logCacheFlushId, status, throughputController, tracker);\n        Path lastPathName = null;\n        try {\n          for (Path pathName : pathNames) {\n            lastPathName = pathName;\n            validateStoreFile(pathName);\n          }\n          return pathNames;\n        } catch (Exception e) {\n          LOG.warn(\"Failed validating store file {}, retrying num={}\", lastPathName, i, e);\n          if (e instanceof IOException) {\n            lastException = (IOException) e;\n          } else {\n            lastException = new IOException(e);\n          }\n        }\n      } catch (IOException e) {\n        LOG.warn(\"Failed flushing store file, retrying num={}\", i, e);\n        lastException = e;\n      }\n      if (lastException != null && i < (flushRetriesNumber - 1)) {\n        try {\n          Thread.sleep(pauseTime);\n        } catch (InterruptedException e) {\n          IOException iie = new InterruptedIOException();\n          iie.initCause(e);\n          throw iie;\n        }\n      }\n    }\n    throw lastException;\n  }"
        ],
        [
            "HStore::getStoreFileAgeStream()",
            "2000  \n2001  \n2002  \n2003 -\n2004  \n2005  \n2006  \n2007  \n2008  \n2009  \n2010  ",
            "  private LongStream getStoreFileAgeStream() {\n    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n      if (sf.getReader() == null) {\n        LOG.warn(\"StoreFile \" + sf + \" has a null Reader\");\n        return false;\n      } else {\n        return true;\n      }\n    }).filter(HStoreFile::isHFile).mapToLong(sf -> sf.getFileInfo().getCreatedTimestamp())\n        .map(t -> EnvironmentEdgeManager.currentTime() - t);\n  }",
            "2005  \n2006  \n2007  \n2008 +\n2009  \n2010  \n2011  \n2012  \n2013  \n2014  \n2015  ",
            "  private LongStream getStoreFileAgeStream() {\n    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n      if (sf.getReader() == null) {\n        LOG.warn(\"StoreFile {} has a null Reader\", sf);\n        return false;\n      } else {\n        return true;\n      }\n    }).filter(HStoreFile::isHFile).mapToLong(sf -> sf.getFileInfo().getCreatedTimestamp())\n        .map(t -> EnvironmentEdgeManager.currentTime() - t);\n  }"
        ],
        [
            "HStore::openStoreFiles(Collection)",
            " 521  \n 522 -\n 523 -\n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548 -\n 549 -\n 550 -\n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568 -\n 569  \n 570 -\n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  ",
            "  private List<HStoreFile> openStoreFiles(Collection<StoreFileInfo> files) throws IOException {\n    if (files == null || files.isEmpty()) {\n      return new ArrayList<>();\n    }\n    // initialize the thread pool for opening store files in parallel..\n    ThreadPoolExecutor storeFileOpenerThreadPool =\n      this.region.getStoreFileOpenAndCloseThreadPool(\"StoreFileOpenerThread-\" +\n          this.getColumnFamilyName());\n    CompletionService<HStoreFile> completionService = new ExecutorCompletionService<>(storeFileOpenerThreadPool);\n\n    int totalValidStoreFile = 0;\n    for (StoreFileInfo storeFileInfo : files) {\n      // open each store file in parallel\n      completionService.submit(() -> this.createStoreFileAndReader(storeFileInfo));\n      totalValidStoreFile++;\n    }\n\n    ArrayList<HStoreFile> results = new ArrayList<>(files.size());\n    IOException ioe = null;\n    try {\n      for (int i = 0; i < totalValidStoreFile; i++) {\n        try {\n          HStoreFile storeFile = completionService.take().get();\n          if (storeFile != null) {\n            long length = storeFile.getReader().length();\n            this.storeSize += length;\n            this.totalUncompressedBytes += storeFile.getReader().getTotalUncompressedBytes();\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"loaded \" + storeFile.toStringDetailed());\n            }\n            results.add(storeFile);\n          }\n        } catch (InterruptedException e) {\n          if (ioe == null) ioe = new InterruptedIOException(e.getMessage());\n        } catch (ExecutionException e) {\n          if (ioe == null) ioe = new IOException(e.getCause());\n        }\n      }\n    } finally {\n      storeFileOpenerThreadPool.shutdownNow();\n    }\n    if (ioe != null) {\n      // close StoreFile readers\n      boolean evictOnClose =\n          cacheConf != null? cacheConf.shouldEvictOnClose(): true;\n      for (HStoreFile file : results) {\n        try {\n          if (file != null) file.closeStoreFile(evictOnClose);\n        } catch (IOException e) {\n          LOG.warn(e.getMessage());\n        }\n      }\n      throw ioe;\n    }\n\n    return results;\n  }",
            " 522  \n 523 +\n 524 +\n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549 +\n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567 +\n 568 +\n 569 +\n 570  \n 571 +\n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  ",
            "  private List<HStoreFile> openStoreFiles(Collection<StoreFileInfo> files) throws IOException {\n    if (CollectionUtils.isEmpty(files)) {\n      return Collections.emptyList();\n    }\n    // initialize the thread pool for opening store files in parallel..\n    ThreadPoolExecutor storeFileOpenerThreadPool =\n      this.region.getStoreFileOpenAndCloseThreadPool(\"StoreFileOpenerThread-\" +\n          this.getColumnFamilyName());\n    CompletionService<HStoreFile> completionService = new ExecutorCompletionService<>(storeFileOpenerThreadPool);\n\n    int totalValidStoreFile = 0;\n    for (StoreFileInfo storeFileInfo : files) {\n      // open each store file in parallel\n      completionService.submit(() -> this.createStoreFileAndReader(storeFileInfo));\n      totalValidStoreFile++;\n    }\n\n    ArrayList<HStoreFile> results = new ArrayList<>(files.size());\n    IOException ioe = null;\n    try {\n      for (int i = 0; i < totalValidStoreFile; i++) {\n        try {\n          HStoreFile storeFile = completionService.take().get();\n          if (storeFile != null) {\n            long length = storeFile.getReader().length();\n            this.storeSize += length;\n            this.totalUncompressedBytes += storeFile.getReader().getTotalUncompressedBytes();\n            LOG.debug(\"loaded {}\", storeFile);\n            results.add(storeFile);\n          }\n        } catch (InterruptedException e) {\n          if (ioe == null) ioe = new InterruptedIOException(e.getMessage());\n        } catch (ExecutionException e) {\n          if (ioe == null) ioe = new IOException(e.getCause());\n        }\n      }\n    } finally {\n      storeFileOpenerThreadPool.shutdownNow();\n    }\n    if (ioe != null) {\n      // close StoreFile readers\n      boolean evictOnClose =\n          cacheConf != null? cacheConf.shouldEvictOnClose(): true;\n      for (HStoreFile file : results) {\n        try {\n          if (file != null) {\n            file.closeStoreFile(evictOnClose);\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Could not close store file\", e);\n        }\n      }\n      throw ioe;\n    }\n\n    return results;\n  }"
        ],
        [
            "HStore::close()",
            " 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883 -\n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927 -\n 928  \n 929  \n 930  \n 931  \n 932  \n 933  ",
            "  /**\n   * Close all the readers We don't need to worry about subsequent requests because the Region holds\n   * a write lock that will prevent any more reads or writes.\n   * @return the {@link StoreFile StoreFiles} that were previously being used.\n   * @throws IOException on failure\n   */\n  public ImmutableCollection<HStoreFile> close() throws IOException {\n    this.archiveLock.lock();\n    this.lock.writeLock().lock();\n    try {\n      // Clear so metrics doesn't find them.\n      ImmutableCollection<HStoreFile> result = storeEngine.getStoreFileManager().clearFiles();\n      Collection<HStoreFile> compactedfiles =\n          storeEngine.getStoreFileManager().clearCompactedFiles();\n      // clear the compacted files\n      if (compactedfiles != null && !compactedfiles.isEmpty()) {\n        removeCompactedfiles(compactedfiles);\n      }\n      if (!result.isEmpty()) {\n        // initialize the thread pool for closing store files in parallel.\n        ThreadPoolExecutor storeFileCloserThreadPool = this.region\n            .getStoreFileOpenAndCloseThreadPool(\"StoreFileCloserThread-\"\n                + this.getColumnFamilyName());\n\n        // close each store file in parallel\n        CompletionService<Void> completionService =\n          new ExecutorCompletionService<>(storeFileCloserThreadPool);\n        for (HStoreFile f : result) {\n          completionService.submit(new Callable<Void>() {\n            @Override\n            public Void call() throws IOException {\n              boolean evictOnClose =\n                  cacheConf != null? cacheConf.shouldEvictOnClose(): true;\n              f.closeStoreFile(evictOnClose);\n              return null;\n            }\n          });\n        }\n\n        IOException ioe = null;\n        try {\n          for (int i = 0; i < result.size(); i++) {\n            try {\n              Future<Void> future = completionService.take();\n              future.get();\n            } catch (InterruptedException e) {\n              if (ioe == null) {\n                ioe = new InterruptedIOException();\n                ioe.initCause(e);\n              }\n            } catch (ExecutionException e) {\n              if (ioe == null) ioe = new IOException(e.getCause());\n            }\n          }\n        } finally {\n          storeFileCloserThreadPool.shutdownNow();\n        }\n        if (ioe != null) throw ioe;\n      }\n      LOG.info(\"Closed \" + this);\n      return result;\n    } finally {\n      this.lock.writeLock().unlock();\n      this.archiveLock.unlock();\n    }\n  }",
            " 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886 +\n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930 +\n 931  \n 932  \n 933  \n 934  \n 935  \n 936  ",
            "  /**\n   * Close all the readers We don't need to worry about subsequent requests because the Region holds\n   * a write lock that will prevent any more reads or writes.\n   * @return the {@link StoreFile StoreFiles} that were previously being used.\n   * @throws IOException on failure\n   */\n  public ImmutableCollection<HStoreFile> close() throws IOException {\n    this.archiveLock.lock();\n    this.lock.writeLock().lock();\n    try {\n      // Clear so metrics doesn't find them.\n      ImmutableCollection<HStoreFile> result = storeEngine.getStoreFileManager().clearFiles();\n      Collection<HStoreFile> compactedfiles =\n          storeEngine.getStoreFileManager().clearCompactedFiles();\n      // clear the compacted files\n      if (CollectionUtils.isNotEmpty(compactedfiles)) {\n        removeCompactedfiles(compactedfiles);\n      }\n      if (!result.isEmpty()) {\n        // initialize the thread pool for closing store files in parallel.\n        ThreadPoolExecutor storeFileCloserThreadPool = this.region\n            .getStoreFileOpenAndCloseThreadPool(\"StoreFileCloserThread-\"\n                + this.getColumnFamilyName());\n\n        // close each store file in parallel\n        CompletionService<Void> completionService =\n          new ExecutorCompletionService<>(storeFileCloserThreadPool);\n        for (HStoreFile f : result) {\n          completionService.submit(new Callable<Void>() {\n            @Override\n            public Void call() throws IOException {\n              boolean evictOnClose =\n                  cacheConf != null? cacheConf.shouldEvictOnClose(): true;\n              f.closeStoreFile(evictOnClose);\n              return null;\n            }\n          });\n        }\n\n        IOException ioe = null;\n        try {\n          for (int i = 0; i < result.size(); i++) {\n            try {\n              Future<Void> future = completionService.take();\n              future.get();\n            } catch (InterruptedException e) {\n              if (ioe == null) {\n                ioe = new InterruptedIOException();\n                ioe.initCause(e);\n              }\n            } catch (ExecutionException e) {\n              if (ioe == null) ioe = new IOException(e.getCause());\n            }\n          }\n        } finally {\n          storeFileCloserThreadPool.shutdownNow();\n        }\n        if (ioe != null) throw ioe;\n      }\n      LOG.info(\"Closed {}\", this);\n      return result;\n    } finally {\n      this.lock.writeLock().unlock();\n      this.archiveLock.unlock();\n    }\n  }"
        ],
        [
            "HStore::getSplitPoint()",
            "1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872 -\n1873 -\n1874 -\n1875  \n1876  \n1877  \n1878  \n1879 -\n1880  \n1881  \n1882  \n1883  \n1884  ",
            "  /**\n   * Determines if Store should be split.\n   */\n  public Optional<byte[]> getSplitPoint() {\n    this.lock.readLock().lock();\n    try {\n      // Should already be enforced by the split policy!\n      assert !this.getRegionInfo().isMetaRegion();\n      // Not split-able if we find a reference store file present in the store.\n      if (hasReferences()) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Not splittable; has references: \" + this);\n        }\n        return Optional.empty();\n      }\n      return this.storeEngine.getStoreFileManager().getSplitPoint();\n    } catch(IOException e) {\n      LOG.warn(\"Failed getting store size for \" + this, e);\n    } finally {\n      this.lock.readLock().unlock();\n    }\n    return Optional.empty();\n  }",
            "1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880 +\n1881  \n1882  \n1883  \n1884  \n1885 +\n1886  \n1887  \n1888  \n1889  \n1890  ",
            "  /**\n   * Determines if Store should be split.\n   */\n  public Optional<byte[]> getSplitPoint() {\n    this.lock.readLock().lock();\n    try {\n      // Should already be enforced by the split policy!\n      assert !this.getRegionInfo().isMetaRegion();\n      // Not split-able if we find a reference store file present in the store.\n      if (hasReferences()) {\n        LOG.trace(\"Not splittable; has references: {}\", this);\n        return Optional.empty();\n      }\n      return this.storeEngine.getStoreFileManager().getSplitPoint();\n    } catch(IOException e) {\n      LOG.warn(\"Failed getting store size for {}\", this, e);\n    } finally {\n      this.lock.readLock().unlock();\n    }\n    return Optional.empty();\n  }"
        ],
        [
            "HStore::getStoreFileFieldSize(ToLongFunction)",
            "2067  \n2068  \n2069  \n2070 -\n2071  \n2072  \n2073  \n2074  \n2075  \n2076  ",
            "  private long getStoreFileFieldSize(ToLongFunction<StoreFileReader> f) {\n    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n      if (sf.getReader() == null) {\n        LOG.warn(\"StoreFile \" + sf + \" has a null Reader\");\n        return false;\n      } else {\n        return true;\n      }\n    }).map(HStoreFile::getReader).mapToLong(f).sum();\n  }",
            "2072  \n2073  \n2074  \n2075 +\n2076  \n2077  \n2078  \n2079  \n2080  \n2081  ",
            "  private long getStoreFileFieldSize(ToLongFunction<StoreFileReader> f) {\n    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n      if (sf.getReader() == null) {\n        LOG.warn(\"StoreFile {} has a null Reader\", sf);\n        return false;\n      } else {\n        return true;\n      }\n    }).map(HStoreFile::getReader).mapToLong(f).sum();\n  }"
        ],
        [
            "HStore::addToCompactingFiles(Collection)",
            "1725  \n1726  \n1727 -\n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  ",
            "  /** Adds the files to compacting files. filesCompacting must be locked. */\n  private void addToCompactingFiles(Collection<HStoreFile> filesToAdd) {\n    if (filesToAdd == null) return;\n    // Check that we do not try to compact the same StoreFile twice.\n    if (!Collections.disjoint(filesCompacting, filesToAdd)) {\n      Preconditions.checkArgument(false, \"%s overlaps with %s\", filesToAdd, filesCompacting);\n    }\n    filesCompacting.addAll(filesToAdd);\n    Collections.sort(filesCompacting, storeEngine.getStoreFileManager().getStoreFileComparator());\n  }",
            "1730  \n1731  \n1732 +\n1733 +\n1734 +\n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  ",
            "  /** Adds the files to compacting files. filesCompacting must be locked. */\n  private void addToCompactingFiles(Collection<HStoreFile> filesToAdd) {\n    if (CollectionUtils.isEmpty(filesToAdd)) {\n      return;\n    }\n    // Check that we do not try to compact the same StoreFile twice.\n    if (!Collections.disjoint(filesCompacting, filesToAdd)) {\n      Preconditions.checkArgument(false, \"%s overlaps with %s\", filesToAdd, filesCompacting);\n    }\n    filesCompacting.addAll(filesToAdd);\n    Collections.sort(filesCompacting, storeEngine.getStoreFileManager().getStoreFileComparator());\n  }"
        ],
        [
            "HStore::HStore(HRegion,ColumnFamilyDescriptor,Configuration)",
            " 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 -\n 260 -\n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 -\n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305 -\n 306 -\n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 -\n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  ",
            "  /**\n   * Constructor\n   * @param region\n   * @param family HColumnDescriptor for this column\n   * @param confParam configuration object\n   * failed.  Can be null.\n   * @throws IOException\n   */\n  protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n      final Configuration confParam) throws IOException {\n\n    this.fs = region.getRegionFileSystem();\n\n    // Assemble the store's home directory and Ensure it exists.\n    fs.createStoreDir(family.getNameAsString());\n    this.region = region;\n    this.family = family;\n    // 'conf' renamed to 'confParam' b/c we use this.conf in the constructor\n    // CompoundConfiguration will look for keys in reverse order of addition, so we'd\n    // add global config first, then table and cf overrides, then cf metadata.\n    this.conf = new CompoundConfiguration()\n      .add(confParam)\n      .addBytesMap(region.getTableDescriptor().getValues())\n      .addStringMap(family.getConfiguration())\n      .addBytesMap(family.getValues());\n    this.blocksize = family.getBlocksize();\n\n    // set block storage policy for store directory\n    String policyName = family.getStoragePolicy();\n    if (null == policyName) {\n      policyName = this.conf.get(BLOCK_STORAGE_POLICY_KEY, DEFAULT_BLOCK_STORAGE_POLICY);\n    }\n    this.fs.setStoragePolicy(family.getNameAsString(), policyName.trim());\n\n    this.dataBlockEncoder =\n        new HFileDataBlockEncoderImpl(family.getDataBlockEncoding());\n\n    this.comparator = region.getCellComparator();\n    // used by ScanQueryMatcher\n    long timeToPurgeDeletes =\n        Math.max(conf.getLong(\"hbase.hstore.time.to.purge.deletes\", 0), 0);\n    LOG.trace(\"Time to purge deletes set to \" + timeToPurgeDeletes +\n        \"ms in store \" + this);\n    // Get TTL\n    long ttl = determineTTLFromFamily(family);\n    // Why not just pass a HColumnDescriptor in here altogether?  Even if have\n    // to clone it?\n    scanInfo = new ScanInfo(conf, family, ttl, timeToPurgeDeletes, this.comparator);\n    MemoryCompactionPolicy inMemoryCompaction = null;\n    if (this.getTableName().isSystemTable()) {\n      inMemoryCompaction = MemoryCompactionPolicy\n          .valueOf(conf.get(\"hbase.systemtables.compacting.memstore.type\", \"NONE\"));\n    } else {\n      inMemoryCompaction = family.getInMemoryCompaction();\n    }\n    if (inMemoryCompaction == null) {\n      inMemoryCompaction =\n          MemoryCompactionPolicy.valueOf(conf.get(CompactingMemStore.COMPACTING_MEMSTORE_TYPE_KEY,\n            CompactingMemStore.COMPACTING_MEMSTORE_TYPE_DEFAULT));\n    }\n    String className;\n    switch (inMemoryCompaction) {\n      case NONE:\n        className = DefaultMemStore.class.getName();\n        this.memstore = ReflectionUtils.newInstance(DefaultMemStore.class,\n            new Object[] { conf, this.comparator });\n        break;\n      default:\n        Class<? extends CompactingMemStore> clz = conf.getClass(MEMSTORE_CLASS_NAME,\n          CompactingMemStore.class, CompactingMemStore.class);\n        className = clz.getName();\n        this.memstore = ReflectionUtils.newInstance(clz, new Object[] { conf, this.comparator, this,\n            this.getHRegion().getRegionServicesForStores(), inMemoryCompaction });\n    }\n    LOG.info(\"Memstore class name is \" + className);\n    this.offPeakHours = OffPeakHours.getInstance(conf);\n\n    // Setting up cache configuration for this family\n    createCacheConf(family);\n\n    this.verifyBulkLoads = conf.getBoolean(\"hbase.hstore.bulkload.verify\", false);\n\n    this.blockingFileCount =\n        conf.getInt(BLOCKING_STOREFILES_KEY, DEFAULT_BLOCKING_STOREFILE_COUNT);\n    this.compactionCheckMultiplier = conf.getInt(\n        COMPACTCHECKER_INTERVAL_MULTIPLIER_KEY, DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER);\n    if (this.compactionCheckMultiplier <= 0) {\n      LOG.error(\"Compaction check period multiplier must be positive, setting default: \"\n          + DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER);\n      this.compactionCheckMultiplier = DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER;\n    }\n\n    if (HStore.closeCheckInterval == 0) {\n      HStore.closeCheckInterval = conf.getInt(\n          \"hbase.hstore.close.check.interval\", 10*1000*1000 /* 10 MB */);\n    }\n\n    this.storeEngine = createStoreEngine(this, this.conf, this.comparator);\n    this.storeEngine.getStoreFileManager().loadFiles(loadStoreFiles());\n\n    // Initialize checksum type from name. The names are CRC32, CRC32C, etc.\n    this.checksumType = getChecksumType(conf);\n    // initilize bytes per checksum\n    this.bytesPerChecksum = getBytesPerChecksum(conf);\n    flushRetriesNumber = conf.getInt(\n        \"hbase.hstore.flush.retries.number\", DEFAULT_FLUSH_RETRIES_NUMBER);\n    pauseTime = conf.getInt(HConstants.HBASE_SERVER_PAUSE, HConstants.DEFAULT_HBASE_SERVER_PAUSE);\n    if (flushRetriesNumber <= 0) {\n      throw new IllegalArgumentException(\n          \"hbase.hstore.flush.retries.number must be > 0, not \"\n              + flushRetriesNumber);\n    }\n    cryptoContext = EncryptionUtil.createEncryptionContext(conf, family);\n  }",
            " 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 +\n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293 +\n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 +\n 307 +\n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321 +\n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  ",
            "  /**\n   * Constructor\n   * @param region\n   * @param family HColumnDescriptor for this column\n   * @param confParam configuration object\n   * failed.  Can be null.\n   * @throws IOException\n   */\n  protected HStore(final HRegion region, final ColumnFamilyDescriptor family,\n      final Configuration confParam) throws IOException {\n\n    this.fs = region.getRegionFileSystem();\n\n    // Assemble the store's home directory and Ensure it exists.\n    fs.createStoreDir(family.getNameAsString());\n    this.region = region;\n    this.family = family;\n    // 'conf' renamed to 'confParam' b/c we use this.conf in the constructor\n    // CompoundConfiguration will look for keys in reverse order of addition, so we'd\n    // add global config first, then table and cf overrides, then cf metadata.\n    this.conf = new CompoundConfiguration()\n      .add(confParam)\n      .addBytesMap(region.getTableDescriptor().getValues())\n      .addStringMap(family.getConfiguration())\n      .addBytesMap(family.getValues());\n    this.blocksize = family.getBlocksize();\n\n    // set block storage policy for store directory\n    String policyName = family.getStoragePolicy();\n    if (null == policyName) {\n      policyName = this.conf.get(BLOCK_STORAGE_POLICY_KEY, DEFAULT_BLOCK_STORAGE_POLICY);\n    }\n    this.fs.setStoragePolicy(family.getNameAsString(), policyName.trim());\n\n    this.dataBlockEncoder =\n        new HFileDataBlockEncoderImpl(family.getDataBlockEncoding());\n\n    this.comparator = region.getCellComparator();\n    // used by ScanQueryMatcher\n    long timeToPurgeDeletes =\n        Math.max(conf.getLong(\"hbase.hstore.time.to.purge.deletes\", 0), 0);\n    LOG.trace(\"Time to purge deletes set to {}ms in store {}\", timeToPurgeDeletes, this);\n    // Get TTL\n    long ttl = determineTTLFromFamily(family);\n    // Why not just pass a HColumnDescriptor in here altogether?  Even if have\n    // to clone it?\n    scanInfo = new ScanInfo(conf, family, ttl, timeToPurgeDeletes, this.comparator);\n    MemoryCompactionPolicy inMemoryCompaction = null;\n    if (this.getTableName().isSystemTable()) {\n      inMemoryCompaction = MemoryCompactionPolicy\n          .valueOf(conf.get(\"hbase.systemtables.compacting.memstore.type\", \"NONE\"));\n    } else {\n      inMemoryCompaction = family.getInMemoryCompaction();\n    }\n    if (inMemoryCompaction == null) {\n      inMemoryCompaction =\n          MemoryCompactionPolicy.valueOf(conf.get(CompactingMemStore.COMPACTING_MEMSTORE_TYPE_KEY,\n            CompactingMemStore.COMPACTING_MEMSTORE_TYPE_DEFAULT));\n    }\n    String className;\n    switch (inMemoryCompaction) {\n      case NONE:\n        className = DefaultMemStore.class.getName();\n        this.memstore = ReflectionUtils.newInstance(DefaultMemStore.class,\n            new Object[] { conf, this.comparator });\n        break;\n      default:\n        Class<? extends CompactingMemStore> clz = conf.getClass(MEMSTORE_CLASS_NAME,\n          CompactingMemStore.class, CompactingMemStore.class);\n        className = clz.getName();\n        this.memstore = ReflectionUtils.newInstance(clz, new Object[] { conf, this.comparator, this,\n            this.getHRegion().getRegionServicesForStores(), inMemoryCompaction });\n    }\n    LOG.info(\"Memstore class name is {}\", className);\n    this.offPeakHours = OffPeakHours.getInstance(conf);\n\n    // Setting up cache configuration for this family\n    createCacheConf(family);\n\n    this.verifyBulkLoads = conf.getBoolean(\"hbase.hstore.bulkload.verify\", false);\n\n    this.blockingFileCount =\n        conf.getInt(BLOCKING_STOREFILES_KEY, DEFAULT_BLOCKING_STOREFILE_COUNT);\n    this.compactionCheckMultiplier = conf.getInt(\n        COMPACTCHECKER_INTERVAL_MULTIPLIER_KEY, DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER);\n    if (this.compactionCheckMultiplier <= 0) {\n      LOG.error(\"Compaction check period multiplier must be positive, setting default: {}\",\n          DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER);\n      this.compactionCheckMultiplier = DEFAULT_COMPACTCHECKER_INTERVAL_MULTIPLIER;\n    }\n\n    if (HStore.closeCheckInterval == 0) {\n      HStore.closeCheckInterval = conf.getInt(\n          \"hbase.hstore.close.check.interval\", 10*1000*1000 /* 10 MB */);\n    }\n\n    this.storeEngine = createStoreEngine(this, this.conf, this.comparator);\n    this.storeEngine.getStoreFileManager().loadFiles(loadStoreFiles());\n\n    // Initialize checksum type from name. The names are CRC32, CRC32C, etc.\n    this.checksumType = getChecksumType(conf);\n    // Initialize bytes per checksum\n    this.bytesPerChecksum = getBytesPerChecksum(conf);\n    flushRetriesNumber = conf.getInt(\n        \"hbase.hstore.flush.retries.number\", DEFAULT_FLUSH_RETRIES_NUMBER);\n    pauseTime = conf.getInt(HConstants.HBASE_SERVER_PAUSE, HConstants.DEFAULT_HBASE_SERVER_PAUSE);\n    if (flushRetriesNumber <= 0) {\n      throw new IllegalArgumentException(\n          \"hbase.hstore.flush.retries.number must be > 0, not \"\n              + flushRetriesNumber);\n    }\n    cryptoContext = EncryptionUtil.createEncryptionContext(conf, family);\n  }"
        ],
        [
            "HStore::assertBulkLoadHFileOk(Path)",
            " 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748 -\n 749 -\n 750 -\n 751 -\n 752 -\n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762 -\n 763  \n 764  \n 765  \n 766  \n 767  \n 768 -\n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  ",
            "  /**\n   * This throws a WrongRegionException if the HFile does not fit in this region, or an\n   * InvalidHFileException if the HFile is not valid.\n   */\n  public void assertBulkLoadHFileOk(Path srcPath) throws IOException {\n    HFile.Reader reader  = null;\n    try {\n      LOG.info(\"Validating hfile at \" + srcPath + \" for inclusion in \"\n          + \"store \" + this + \" region \" + this.getRegionInfo().getRegionNameAsString());\n      reader = HFile.createReader(srcPath.getFileSystem(conf), srcPath, cacheConf,\n        isPrimaryReplicaStore(), conf);\n      reader.loadFileInfo();\n\n      Optional<byte[]> firstKey = reader.getFirstRowKey();\n      Preconditions.checkState(firstKey.isPresent(), \"First key can not be null\");\n      Optional<Cell> lk = reader.getLastKey();\n      Preconditions.checkState(lk.isPresent(), \"Last key can not be null\");\n      byte[] lastKey =  CellUtil.cloneRow(lk.get());\n\n      LOG.debug(\"HFile bounds: first=\" + Bytes.toStringBinary(firstKey.get()) +\n          \" last=\" + Bytes.toStringBinary(lastKey));\n      LOG.debug(\"Region bounds: first=\" +\n          Bytes.toStringBinary(getRegionInfo().getStartKey()) +\n          \" last=\" + Bytes.toStringBinary(getRegionInfo().getEndKey()));\n\n      if (!this.getRegionInfo().containsRange(firstKey.get(), lastKey)) {\n        throw new WrongRegionException(\n            \"Bulk load file \" + srcPath.toString() + \" does not fit inside region \"\n            + this.getRegionInfo().getRegionNameAsString());\n      }\n\n      if(reader.length() > conf.getLong(HConstants.HREGION_MAX_FILESIZE,\n          HConstants.DEFAULT_MAX_FILE_SIZE)) {\n        LOG.warn(\"Trying to bulk load hfile \" + srcPath.toString() + \" with size: \" +\n            reader.length() + \" bytes can be problematic as it may lead to oversplitting.\");\n      }\n\n      if (verifyBulkLoads) {\n        long verificationStartTime = EnvironmentEdgeManager.currentTime();\n        LOG.info(\"Full verification started for bulk load hfile: \" + srcPath.toString());\n        Cell prevCell = null;\n        HFileScanner scanner = reader.getScanner(false, false, false);\n        scanner.seekTo();\n        do {\n          Cell cell = scanner.getCell();\n          if (prevCell != null) {\n            if (comparator.compareRows(prevCell, cell) > 0) {\n              throw new InvalidHFileException(\"Previous row is greater than\"\n                  + \" current row: path=\" + srcPath + \" previous=\"\n                  + CellUtil.getCellKeyAsString(prevCell) + \" current=\"\n                  + CellUtil.getCellKeyAsString(cell));\n            }\n            if (CellComparator.getInstance().compareFamilies(prevCell, cell) != 0) {\n              throw new InvalidHFileException(\"Previous key had different\"\n                  + \" family compared to current key: path=\" + srcPath\n                  + \" previous=\"\n                  + Bytes.toStringBinary(prevCell.getFamilyArray(), prevCell.getFamilyOffset(),\n                      prevCell.getFamilyLength())\n                  + \" current=\"\n                  + Bytes.toStringBinary(cell.getFamilyArray(), cell.getFamilyOffset(),\n                      cell.getFamilyLength()));\n            }\n          }\n          prevCell = cell;\n        } while (scanner.next());\n      LOG.info(\"Full verification complete for bulk load hfile: \" + srcPath.toString()\n         + \" took \" + (EnvironmentEdgeManager.currentTime() - verificationStartTime)\n         + \" ms\");\n      }\n    } finally {\n      if (reader != null) reader.close();\n    }\n  }",
            " 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749 +\n 750 +\n 751 +\n 752 +\n 753 +\n 754 +\n 755 +\n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765 +\n 766  \n 767  \n 768  \n 769  \n 770  \n 771 +\n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  ",
            "  /**\n   * This throws a WrongRegionException if the HFile does not fit in this region, or an\n   * InvalidHFileException if the HFile is not valid.\n   */\n  public void assertBulkLoadHFileOk(Path srcPath) throws IOException {\n    HFile.Reader reader  = null;\n    try {\n      LOG.info(\"Validating hfile at \" + srcPath + \" for inclusion in \"\n          + \"store \" + this + \" region \" + this.getRegionInfo().getRegionNameAsString());\n      reader = HFile.createReader(srcPath.getFileSystem(conf), srcPath, cacheConf,\n        isPrimaryReplicaStore(), conf);\n      reader.loadFileInfo();\n\n      Optional<byte[]> firstKey = reader.getFirstRowKey();\n      Preconditions.checkState(firstKey.isPresent(), \"First key can not be null\");\n      Optional<Cell> lk = reader.getLastKey();\n      Preconditions.checkState(lk.isPresent(), \"Last key can not be null\");\n      byte[] lastKey =  CellUtil.cloneRow(lk.get());\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"HFile bounds: first=\" + Bytes.toStringBinary(firstKey.get()) +\n            \" last=\" + Bytes.toStringBinary(lastKey));\n        LOG.debug(\"Region bounds: first=\" +\n            Bytes.toStringBinary(getRegionInfo().getStartKey()) +\n            \" last=\" + Bytes.toStringBinary(getRegionInfo().getEndKey()));\n      }\n\n      if (!this.getRegionInfo().containsRange(firstKey.get(), lastKey)) {\n        throw new WrongRegionException(\n            \"Bulk load file \" + srcPath.toString() + \" does not fit inside region \"\n            + this.getRegionInfo().getRegionNameAsString());\n      }\n\n      if(reader.length() > conf.getLong(HConstants.HREGION_MAX_FILESIZE,\n          HConstants.DEFAULT_MAX_FILE_SIZE)) {\n        LOG.warn(\"Trying to bulk load hfile \" + srcPath + \" with size: \" +\n            reader.length() + \" bytes can be problematic as it may lead to oversplitting.\");\n      }\n\n      if (verifyBulkLoads) {\n        long verificationStartTime = EnvironmentEdgeManager.currentTime();\n        LOG.info(\"Full verification started for bulk load hfile: {}\", srcPath);\n        Cell prevCell = null;\n        HFileScanner scanner = reader.getScanner(false, false, false);\n        scanner.seekTo();\n        do {\n          Cell cell = scanner.getCell();\n          if (prevCell != null) {\n            if (comparator.compareRows(prevCell, cell) > 0) {\n              throw new InvalidHFileException(\"Previous row is greater than\"\n                  + \" current row: path=\" + srcPath + \" previous=\"\n                  + CellUtil.getCellKeyAsString(prevCell) + \" current=\"\n                  + CellUtil.getCellKeyAsString(cell));\n            }\n            if (CellComparator.getInstance().compareFamilies(prevCell, cell) != 0) {\n              throw new InvalidHFileException(\"Previous key had different\"\n                  + \" family compared to current key: path=\" + srcPath\n                  + \" previous=\"\n                  + Bytes.toStringBinary(prevCell.getFamilyArray(), prevCell.getFamilyOffset(),\n                      prevCell.getFamilyLength())\n                  + \" current=\"\n                  + Bytes.toStringBinary(cell.getFamilyArray(), cell.getFamilyOffset(),\n                      cell.getFamilyLength()));\n            }\n          }\n          prevCell = cell;\n        } while (scanner.next());\n      LOG.info(\"Full verification complete for bulk load hfile: \" + srcPath.toString()\n         + \" took \" + (EnvironmentEdgeManager.currentTime() - verificationStartTime)\n         + \" ms\");\n      }\n    } finally {\n      if (reader != null) reader.close();\n    }\n  }"
        ],
        [
            "HStore::validateStoreFile(Path)",
            "1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793 -\n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  ",
            "  /**\n   * Validates a store file by opening and closing it. In HFileV2 this should not be an expensive\n   * operation.\n   * @param path the path to the store file\n   */\n  private void validateStoreFile(Path path) throws IOException {\n    HStoreFile storeFile = null;\n    try {\n      storeFile = createStoreFileAndReader(path);\n    } catch (IOException e) {\n      LOG.error(\"Failed to open store file : \" + path + \", keeping it in tmp location\", e);\n      throw e;\n    } finally {\n      if (storeFile != null) {\n        storeFile.closeStoreFile(false);\n      }\n    }\n  }",
            "1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803 +\n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  ",
            "  /**\n   * Validates a store file by opening and closing it. In HFileV2 this should not be an expensive\n   * operation.\n   * @param path the path to the store file\n   */\n  private void validateStoreFile(Path path) throws IOException {\n    HStoreFile storeFile = null;\n    try {\n      storeFile = createStoreFileAndReader(path);\n    } catch (IOException e) {\n      LOG.error(\"Failed to open store file : {}, keeping it in tmp location\", path, e);\n      throw e;\n    } finally {\n      if (storeFile != null) {\n        storeFile.closeStoreFile(false);\n      }\n    }\n  }"
        ],
        [
            "HStore::canSplit()",
            "1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852 -\n1853 -\n1854 -\n1855  \n1856  \n1857  \n1858  \n1859  \n1860  ",
            "  @Override\n  public boolean canSplit() {\n    this.lock.readLock().lock();\n    try {\n      // Not split-able if we find a reference store file present in the store.\n      boolean result = !hasReferences();\n      if (!result) {\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Not splittable; has references: \" + this);\n          }\n      }\n      return result;\n    } finally {\n      this.lock.readLock().unlock();\n    }\n  }",
            "1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862 +\n1863  \n1864  \n1865  \n1866  \n1867  \n1868  ",
            "  @Override\n  public boolean canSplit() {\n    this.lock.readLock().lock();\n    try {\n      // Not split-able if we find a reference store file present in the store.\n      boolean result = !hasReferences();\n      if (!result) {\n        LOG.trace(\"Not splittable; has references: {}\", this);\n      }\n      return result;\n    } finally {\n      this.lock.readLock().unlock();\n    }\n  }"
        ],
        [
            "HStore::removeCompactedfiles(Collection)",
            "2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497 -\n2498 -\n2499 -\n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506 -\n2507 -\n2508 -\n2509  \n2510  \n2511  \n2512  \n2513  \n2514 -\n2515 -\n2516  \n2517  \n2518  \n2519  \n2520  \n2521  \n2522  \n2523  \n2524 -\n2525 -\n2526 -\n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  ",
            "  /**\n   * Archives and removes the compacted files\n   * @param compactedfiles The compacted files in this store that are not active in reads\n   * @throws IOException\n   */\n  private void removeCompactedfiles(Collection<HStoreFile> compactedfiles)\n      throws IOException {\n    final List<HStoreFile> filesToRemove = new ArrayList<>(compactedfiles.size());\n    for (final HStoreFile file : compactedfiles) {\n      synchronized (file) {\n        try {\n          StoreFileReader r = file.getReader();\n          if (r == null) {\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"The file \" + file + \" was closed but still not archived.\");\n            }\n            filesToRemove.add(file);\n            continue;\n          }\n          if (file.isCompactedAway() && !file.isReferencedInReads()) {\n            // Even if deleting fails we need not bother as any new scanners won't be\n            // able to use the compacted file as the status is already compactedAway\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Closing and archiving the file \" + file.getPath());\n            }\n            r.close(true);\n            // Just close and return\n            filesToRemove.add(file);\n          }\n        } catch (Exception e) {\n          LOG.error(\n            \"Exception while trying to close the compacted store file \" + file.getPath().getName());\n        }\n      }\n    }\n    if (this.isPrimaryReplicaStore()) {\n      // Only the primary region is allowed to move the file to archive.\n      // The secondary region does not move the files to archive. Any active reads from\n      // the secondary region will still work because the file as such has active readers on it.\n      if (!filesToRemove.isEmpty()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Moving the files \" + filesToRemove + \" to archive\");\n        }\n        // Only if this is successful it has to be removed\n        try {\n          this.fs.removeStoreFiles(this.getColumnFamilyDescriptor().getNameAsString(), filesToRemove);\n        } catch (FailedArchiveException fae) {\n          // Even if archiving some files failed, we still need to clear out any of the\n          // files which were successfully archived.  Otherwise we will receive a\n          // FileNotFoundException when we attempt to re-archive them in the next go around.\n          Collection<Path> failedFiles = fae.getFailedFiles();\n          Iterator<HStoreFile> iter = filesToRemove.iterator();\n          while (iter.hasNext()) {\n            if (failedFiles.contains(iter.next().getPath())) {\n              iter.remove();\n            }\n          }\n          if (!filesToRemove.isEmpty()) {\n            clearCompactedfiles(filesToRemove);\n          }\n          throw fae;\n        }\n      }\n    }\n    if (!filesToRemove.isEmpty()) {\n      // Clear the compactedfiles from the store file manager\n      clearCompactedfiles(filesToRemove);\n    }\n  }",
            "2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498 +\n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505 +\n2506  \n2507  \n2508  \n2509  \n2510  \n2511 +\n2512 +\n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521 +\n2522  \n2523  \n2524  \n2525  \n2526  \n2527  \n2528  \n2529  \n2530  \n2531  \n2532  \n2533  \n2534  \n2535  \n2536  \n2537  \n2538  \n2539  \n2540  \n2541  \n2542  \n2543  \n2544  \n2545  \n2546  \n2547  ",
            "  /**\n   * Archives and removes the compacted files\n   * @param compactedfiles The compacted files in this store that are not active in reads\n   * @throws IOException\n   */\n  private void removeCompactedfiles(Collection<HStoreFile> compactedfiles)\n      throws IOException {\n    final List<HStoreFile> filesToRemove = new ArrayList<>(compactedfiles.size());\n    for (final HStoreFile file : compactedfiles) {\n      synchronized (file) {\n        try {\n          StoreFileReader r = file.getReader();\n          if (r == null) {\n            LOG.debug(\"The file {} was closed but still not archived\", file);\n            filesToRemove.add(file);\n            continue;\n          }\n          if (file.isCompactedAway() && !file.isReferencedInReads()) {\n            // Even if deleting fails we need not bother as any new scanners won't be\n            // able to use the compacted file as the status is already compactedAway\n            LOG.trace(\"Closing and archiving the file {}\", file);\n            r.close(true);\n            // Just close and return\n            filesToRemove.add(file);\n          }\n        } catch (Exception e) {\n          LOG.error(\"Exception while trying to close the compacted store file {}\",\n            file.getPath(), e);\n        }\n      }\n    }\n    if (this.isPrimaryReplicaStore()) {\n      // Only the primary region is allowed to move the file to archive.\n      // The secondary region does not move the files to archive. Any active reads from\n      // the secondary region will still work because the file as such has active readers on it.\n      if (!filesToRemove.isEmpty()) {\n        LOG.debug(\"Moving the files {} to archive\", filesToRemove);\n        // Only if this is successful it has to be removed\n        try {\n          this.fs.removeStoreFiles(this.getColumnFamilyDescriptor().getNameAsString(), filesToRemove);\n        } catch (FailedArchiveException fae) {\n          // Even if archiving some files failed, we still need to clear out any of the\n          // files which were successfully archived.  Otherwise we will receive a\n          // FileNotFoundException when we attempt to re-archive them in the next go around.\n          Collection<Path> failedFiles = fae.getFailedFiles();\n          Iterator<HStoreFile> iter = filesToRemove.iterator();\n          while (iter.hasNext()) {\n            if (failedFiles.contains(iter.next().getPath())) {\n              iter.remove();\n            }\n          }\n          if (!filesToRemove.isEmpty()) {\n            clearCompactedfiles(filesToRemove);\n          }\n          throw fae;\n        }\n      }\n    }\n    if (!filesToRemove.isEmpty()) {\n      // Clear the compactedfiles from the store file manager\n      clearCompactedfiles(filesToRemove);\n    }\n  }"
        ],
        [
            "HStore::completeCompaction(Collection)",
            "1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824 -\n1825  \n1826  \n1827  \n1828  \n1829  \n1830  ",
            "  /**\n   * <p>It works by processing a compaction that's been written to disk.\n   *\n   * <p>It is usually invoked at the end of a compaction, but might also be\n   * invoked at HStore startup, if the prior execution died midway through.\n   *\n   * <p>Moving the compacted TreeMap into place means:\n   * <pre>\n   * 1) Unload all replaced StoreFile, close and collect list to delete.\n   * 2) Compute new store size\n   * </pre>\n   *\n   * @param compactedFiles list of files that were compacted\n   */\n  @VisibleForTesting\n  protected void completeCompaction(Collection<HStoreFile> compactedFiles)\n    throws IOException {\n    this.storeSize = 0L;\n    this.totalUncompressedBytes = 0L;\n    for (HStoreFile hsf : this.storeEngine.getStoreFileManager().getStorefiles()) {\n      StoreFileReader r = hsf.getReader();\n      if (r == null) {\n        LOG.warn(\"StoreFile \" + hsf + \" has a null Reader\");\n        continue;\n      }\n      this.storeSize += r.length();\n      this.totalUncompressedBytes += r.getTotalUncompressedBytes();\n    }\n  }",
            "1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834 +\n1835  \n1836  \n1837  \n1838  \n1839  \n1840  ",
            "  /**\n   * <p>It works by processing a compaction that's been written to disk.\n   *\n   * <p>It is usually invoked at the end of a compaction, but might also be\n   * invoked at HStore startup, if the prior execution died midway through.\n   *\n   * <p>Moving the compacted TreeMap into place means:\n   * <pre>\n   * 1) Unload all replaced StoreFile, close and collect list to delete.\n   * 2) Compute new store size\n   * </pre>\n   *\n   * @param compactedFiles list of files that were compacted\n   */\n  @VisibleForTesting\n  protected void completeCompaction(Collection<HStoreFile> compactedFiles)\n    throws IOException {\n    this.storeSize = 0L;\n    this.totalUncompressedBytes = 0L;\n    for (HStoreFile hsf : this.storeEngine.getStoreFileManager().getStorefiles()) {\n      StoreFileReader r = hsf.getReader();\n      if (r == null) {\n        LOG.warn(\"StoreFile {} has a null Reader\", hsf);\n        continue;\n      }\n      this.storeSize += r.length();\n      this.totalUncompressedBytes += r.getTotalUncompressedBytes();\n    }\n  }"
        ],
        [
            "HStore::shouldPerformMajorCompaction()",
            "1629  \n1630  \n1631  \n1632  \n1633  \n1634 -\n1635  \n1636  \n1637  \n1638  \n1639  \n1640  ",
            "  @Override\n  public boolean shouldPerformMajorCompaction() throws IOException {\n    for (HStoreFile sf : this.storeEngine.getStoreFileManager().getStorefiles()) {\n      // TODO: what are these reader checks all over the place?\n      if (sf.getReader() == null) {\n        LOG.debug(\"StoreFile \" + sf + \" has null Reader\");\n        return false;\n      }\n    }\n    return storeEngine.getCompactionPolicy().shouldPerformMajorCompaction(\n        this.storeEngine.getStoreFileManager().getStorefiles());\n  }",
            "1632  \n1633  \n1634  \n1635  \n1636  \n1637 +\n1638  \n1639  \n1640  \n1641  \n1642  \n1643  ",
            "  @Override\n  public boolean shouldPerformMajorCompaction() throws IOException {\n    for (HStoreFile sf : this.storeEngine.getStoreFileManager().getStorefiles()) {\n      // TODO: what are these reader checks all over the place?\n      if (sf.getReader() == null) {\n        LOG.debug(\"StoreFile {} has null Reader\", sf);\n        return false;\n      }\n    }\n    return storeEngine.getCompactionPolicy().shouldPerformMajorCompaction(\n        this.storeEngine.getStoreFileManager().getStorefiles());\n  }"
        ],
        [
            "HStore::getStorefilesSize(Predicate)",
            "2056  \n2057  \n2058  \n2059 -\n2060  \n2061  \n2062  \n2063  \n2064  \n2065  ",
            "  private long getStorefilesSize(Predicate<HStoreFile> predicate) {\n    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n      if (sf.getReader() == null) {\n        LOG.warn(\"StoreFile \" + sf + \" has a null Reader\");\n        return false;\n      } else {\n        return true;\n      }\n    }).filter(predicate).mapToLong(sf -> sf.getReader().length()).sum();\n  }",
            "2061  \n2062  \n2063  \n2064 +\n2065  \n2066  \n2067  \n2068  \n2069  \n2070  ",
            "  private long getStorefilesSize(Predicate<HStoreFile> predicate) {\n    return this.storeEngine.getStoreFileManager().getStorefiles().stream().filter(sf -> {\n      if (sf.getReader() == null) {\n        LOG.warn(\"StoreFile {} has a null Reader\", sf);\n        return false;\n      } else {\n        return true;\n      }\n    }).filter(predicate).mapToLong(sf -> sf.getReader().length()).sum();\n  }"
        ],
        [
            "HStore::StoreFlusherImpl::abort()",
            "2285  \n2286  \n2287  \n2288  \n2289  \n2290  \n2291 -\n2292 -\n2293  \n2294 -\n2295  ",
            "    /**\n     * Abort the snapshot preparation. Drops the snapshot if any.\n     * @throws IOException\n     */\n    @Override\n    public void abort() throws IOException {\n      if (snapshot == null) {\n        return;\n      }\n      HStore.this.updateStorefiles(new ArrayList<>(0), snapshot.getId());\n    }",
            "2290  \n2291  \n2292  \n2293  \n2294  \n2295  \n2296 +\n2297 +\n2298  \n2299  ",
            "    /**\n     * Abort the snapshot preparation. Drops the snapshot if any.\n     * @throws IOException\n     */\n    @Override\n    public void abort() throws IOException {\n      if (snapshot != null) {\n        HStore.this.updateStorefiles(Collections.emptyList(), snapshot.getId());\n      }\n    }"
        ]
    ],
    "301062566ac6e32d5bc3c6dbfd819b5e62742e8c": [
        [
            "WALSplitter::LogRecoveredEditsOutputSink::createWAP(byte,Entry,Path)",
            "1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511 -\n1512  \n1513  \n1514  \n1515 -\n1516  \n1517  ",
            "    /**\n     * @return a path with a write for that path. caller should close.\n     */\n    WriterAndPath createWAP(byte[] region, Entry entry, Path rootdir) throws IOException {\n      Path regionedits = getRegionSplitEditsPath(fs, entry, rootdir, fileBeingSplit.getPath().getName());\n      if (regionedits == null) {\n        return null;\n      }\n      if (fs.exists(regionedits)) {\n        LOG.warn(\"Found old edits file. It could be the \"\n            + \"result of a previous failed split attempt. Deleting \" + regionedits + \", length=\"\n            + fs.getFileStatus(regionedits).getLen());\n        if (!fs.delete(regionedits, false)) {\n          LOG.warn(\"Failed delete of old \" + regionedits);\n        }\n      }\n      Writer w = createWriter(regionedits);\n      LOG.debug(\"Creating writer path=\" + regionedits);\n      return new WriterAndPath(regionedits, w, entry.getKey().getSequenceId());\n    }",
            "1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498 +\n1499  \n1500  \n1501  \n1502 +\n1503  \n1504  ",
            "    /**\n     * @return a path with a write for that path. caller should close.\n     */\n    WriterAndPath createWAP(byte[] region, Entry entry, Path rootdir) throws IOException {\n      Path regionedits = getRegionSplitEditsPath(fs, entry, rootdir, fileBeingSplit.getPath().getName());\n      if (regionedits == null) {\n        return null;\n      }\n      if (fs.exists(regionedits)) {\n        LOG.warn(\"Found old edits file. It could be the \"\n            + \"result of a previous failed split attempt. Deleting \" + regionedits + \", length=\"\n            + fs.getFileStatus(regionedits).getLen());\n        if (!fs.delete(regionedits, false)) {\n          LOG.warn(\"Failed delete of old {}\", regionedits);\n        }\n      }\n      Writer w = createWriter(regionedits);\n      LOG.debug(\"Creating writer path={}\", regionedits);\n      return new WriterAndPath(regionedits, w, entry.getKey().getSequenceId());\n    }"
        ],
        [
            "WALSplitter::getReader(FileStatus,boolean,CancelableProgressable)",
            " 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713 -\n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727 -\n 728 -\n 729 -\n 730 -\n 731 -\n 732  \n 733  \n 734  \n 735  \n 736  \n 737 -\n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  ",
            "  /**\n   * Create a new {@link Reader} for reading logs to split.\n   *\n   * @param file\n   * @return A new Reader instance, caller should close\n   * @throws IOException\n   * @throws CorruptedLogFileException\n   */\n  protected Reader getReader(FileStatus file, boolean skipErrors, CancelableProgressable reporter)\n      throws IOException, CorruptedLogFileException {\n    Path path = file.getPath();\n    long length = file.getLen();\n    Reader in;\n\n    // Check for possibly empty file. With appends, currently Hadoop reports a\n    // zero length even if the file has been sync'd. Revisit if HDFS-376 or\n    // HDFS-878 is committed.\n    if (length <= 0) {\n      LOG.warn(\"File \" + path + \" might be still open, length is 0\");\n    }\n\n    try {\n      FSUtils.getInstance(fs, conf).recoverFileLease(fs, path, conf, reporter);\n      try {\n        in = getReader(path, reporter);\n      } catch (EOFException e) {\n        if (length <= 0) {\n          // TODO should we ignore an empty, not-last log file if skip.errors\n          // is false? Either way, the caller should decide what to do. E.g.\n          // ignore if this is the last log in sequence.\n          // TODO is this scenario still possible if the log has been\n          // recovered (i.e. closed)\n          LOG.warn(\"Could not open \" + path + \" for reading. File is empty\", e);\n          return null;\n        } else {\n          // EOFException being ignored\n          return null;\n        }\n      }\n    } catch (IOException e) {\n      if (e instanceof FileNotFoundException) {\n        // A wal file may not exist anymore. Nothing can be recovered so move on\n        LOG.warn(\"File \" + path + \" doesn't exist anymore.\", e);\n        return null;\n      }\n      if (!skipErrors || e instanceof InterruptedIOException) {\n        throw e; // Don't mark the file corrupted if interrupted, or not skipErrors\n      }\n      CorruptedLogFileException t =\n        new CorruptedLogFileException(\"skipErrors=true Could not open wal \" +\n            path + \" ignoring\");\n      t.initCause(e);\n      throw t;\n    }\n    return in;\n  }",
            " 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710 +\n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724 +\n 725  \n 726 +\n 727 +\n 728  \n 729  \n 730  \n 731  \n 732 +\n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  ",
            "  /**\n   * Create a new {@link Reader} for reading logs to split.\n   *\n   * @param file\n   * @return A new Reader instance, caller should close\n   * @throws IOException\n   * @throws CorruptedLogFileException\n   */\n  protected Reader getReader(FileStatus file, boolean skipErrors, CancelableProgressable reporter)\n      throws IOException, CorruptedLogFileException {\n    Path path = file.getPath();\n    long length = file.getLen();\n    Reader in;\n\n    // Check for possibly empty file. With appends, currently Hadoop reports a\n    // zero length even if the file has been sync'd. Revisit if HDFS-376 or\n    // HDFS-878 is committed.\n    if (length <= 0) {\n      LOG.warn(\"File {} might be still open, length is 0\", path);\n    }\n\n    try {\n      FSUtils.getInstance(fs, conf).recoverFileLease(fs, path, conf, reporter);\n      try {\n        in = getReader(path, reporter);\n      } catch (EOFException e) {\n        if (length <= 0) {\n          // TODO should we ignore an empty, not-last log file if skip.errors\n          // is false? Either way, the caller should decide what to do. E.g.\n          // ignore if this is the last log in sequence.\n          // TODO is this scenario still possible if the log has been\n          // recovered (i.e. closed)\n          LOG.warn(\"Could not open {} for reading. File is empty\", path, e);\n        }\n        // EOFException being ignored\n        return null;\n      }\n    } catch (IOException e) {\n      if (e instanceof FileNotFoundException) {\n        // A wal file may not exist anymore. Nothing can be recovered so move on\n        LOG.warn(\"File {} does not exist anymore\", path, e);\n        return null;\n      }\n      if (!skipErrors || e instanceof InterruptedIOException) {\n        throw e; // Don't mark the file corrupted if interrupted, or not skipErrors\n      }\n      CorruptedLogFileException t =\n        new CorruptedLogFileException(\"skipErrors=true Could not open wal \" +\n            path + \" ignoring\");\n      t.initCause(e);\n      throw t;\n    }\n    return in;\n  }"
        ],
        [
            "WALSplitter::LogRecoveredEditsOutputSink::closeWriter(String,WriterAndPath,List)",
            "1378  \n1379  \n1380 -\n1381 -\n1382 -\n1383  \n1384  \n1385  \n1386 -\n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398 -\n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417 -\n1418  \n1419  \n1420 -\n1421  \n1422  \n1423  \n1424  \n1425  ",
            "    Path closeWriter(String encodedRegionName, WriterAndPath wap,\n        List<IOException> thrown) throws IOException{\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"Closing \" + wap.p);\n      }\n      try {\n        wap.w.close();\n      } catch (IOException ioe) {\n        LOG.error(\"Couldn't close log at \" + wap.p, ioe);\n        thrown.add(ioe);\n        return null;\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Closed wap \" + wap.p + \" (wrote \" + wap.editsWritten\n            + \" edits, skipped \" + wap.editsSkipped + \" edits in \"\n            + (wap.nanosSpent / 1000 / 1000) + \"ms\");\n      }\n      if (wap.editsWritten == 0) {\n        // just remove the empty recovered.edits file\n        if (fs.exists(wap.p) && !fs.delete(wap.p, false)) {\n          LOG.warn(\"Failed deleting empty \" + wap.p);\n          throw new IOException(\"Failed deleting empty  \" + wap.p);\n        }\n        return null;\n      }\n\n      Path dst = getCompletedRecoveredEditsFilePath(wap.p,\n          regionMaximumEditLogSeqNum.get(encodedRegionName));\n      try {\n        if (!dst.equals(wap.p) && fs.exists(dst)) {\n          deleteOneWithFewerEntries(wap, dst);\n        }\n        // Skip the unit tests which create a splitter that reads and\n        // writes the data without touching disk.\n        // TestHLogSplit#testThreading is an example.\n        if (fs.exists(wap.p)) {\n          if (!fs.rename(wap.p, dst)) {\n            throw new IOException(\"Failed renaming \" + wap.p + \" to \" + dst);\n          }\n          LOG.info(\"Rename \" + wap.p + \" to \" + dst);\n        }\n      } catch (IOException ioe) {\n        LOG.error(\"Couldn't rename \" + wap.p + \" to \" + dst, ioe);\n        thrown.add(ioe);\n        return null;\n      }\n      return dst;\n    }",
            "1368  \n1369  \n1370 +\n1371  \n1372  \n1373  \n1374 +\n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386 +\n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405 +\n1406  \n1407  \n1408 +\n1409  \n1410  \n1411  \n1412  \n1413  ",
            "    Path closeWriter(String encodedRegionName, WriterAndPath wap,\n        List<IOException> thrown) throws IOException{\n      LOG.trace(\"Closing {}\", wap.p);\n      try {\n        wap.w.close();\n      } catch (IOException ioe) {\n        LOG.error(\"Could not close log at {}\", wap.p, ioe);\n        thrown.add(ioe);\n        return null;\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Closed wap \" + wap.p + \" (wrote \" + wap.editsWritten\n            + \" edits, skipped \" + wap.editsSkipped + \" edits in \"\n            + (wap.nanosSpent / 1000 / 1000) + \"ms\");\n      }\n      if (wap.editsWritten == 0) {\n        // just remove the empty recovered.edits file\n        if (fs.exists(wap.p) && !fs.delete(wap.p, false)) {\n          LOG.warn(\"Failed deleting empty {}\", wap.p);\n          throw new IOException(\"Failed deleting empty  \" + wap.p);\n        }\n        return null;\n      }\n\n      Path dst = getCompletedRecoveredEditsFilePath(wap.p,\n          regionMaximumEditLogSeqNum.get(encodedRegionName));\n      try {\n        if (!dst.equals(wap.p) && fs.exists(dst)) {\n          deleteOneWithFewerEntries(wap, dst);\n        }\n        // Skip the unit tests which create a splitter that reads and\n        // writes the data without touching disk.\n        // TestHLogSplit#testThreading is an example.\n        if (fs.exists(wap.p)) {\n          if (!fs.rename(wap.p, dst)) {\n            throw new IOException(\"Failed renaming \" + wap.p + \" to \" + dst);\n          }\n          LOG.info(\"Rename {} to {}\", wap.p, dst);\n        }\n      } catch (IOException ioe) {\n        LOG.error(\"Could not rename {} to {}\", wap.p, dst, ioe);\n        thrown.add(ioe);\n        return null;\n      }\n      return dst;\n    }"
        ],
        [
            "WALSplitter::BoundedLogWriterCreationOutputSink::executeCloseTask(CompletionService,List,List)",
            "1655  \n1656  \n1657  \n1658  \n1659  \n1660 -\n1661  \n1662  \n1663  \n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673  \n1674  \n1675  \n1676  \n1677  \n1678  \n1679  ",
            "    @Override\n    boolean executeCloseTask(CompletionService<Void> completionService,\n        List<IOException> thrown, List<Path> paths)\n        throws InterruptedException, ExecutionException {\n      for (final Map.Entry<byte[], RegionEntryBuffer> buffer : entryBuffers.buffers.entrySet()) {\n        LOG.info(\"Submitting writeThenClose of \" + buffer.getValue().encodedRegionName);\n        completionService.submit(new Callable<Void>() {\n          public Void call() throws Exception {\n            Path dst = writeThenClose(buffer.getValue());\n            paths.add(dst);\n            return null;\n          }\n        });\n      }\n      boolean progress_failed = false;\n      for (int i = 0, n = entryBuffers.buffers.size(); i < n; i++) {\n        Future<Void> future = completionService.take();\n        future.get();\n        if (!progress_failed && reporter != null && !reporter.progress()) {\n          progress_failed = true;\n        }\n      }\n\n      return progress_failed;\n    }",
            "1640  \n1641  \n1642  \n1643  \n1644  \n1645 +\n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661  \n1662  \n1663  \n1664  ",
            "    @Override\n    boolean executeCloseTask(CompletionService<Void> completionService,\n        List<IOException> thrown, List<Path> paths)\n        throws InterruptedException, ExecutionException {\n      for (final Map.Entry<byte[], RegionEntryBuffer> buffer : entryBuffers.buffers.entrySet()) {\n        LOG.info(\"Submitting writeThenClose of {}\", buffer.getValue().encodedRegionName);\n        completionService.submit(new Callable<Void>() {\n          public Void call() throws Exception {\n            Path dst = writeThenClose(buffer.getValue());\n            paths.add(dst);\n            return null;\n          }\n        });\n      }\n      boolean progress_failed = false;\n      for (int i = 0, n = entryBuffers.buffers.size(); i < n; i++) {\n        Future<Void> future = completionService.take();\n        future.get();\n        if (!progress_failed && reporter != null && !reporter.progress()) {\n          progress_failed = true;\n        }\n      }\n\n      return progress_failed;\n    }"
        ],
        [
            "WALSplitter::RegionEntryBuffer::RegionEntryBuffer(TableName,byte)",
            " 980  \n 981  \n 982  \n 983 -\n 984  ",
            "    RegionEntryBuffer(TableName tableName, byte[] region) {\n      this.tableName = tableName;\n      this.encodedRegionName = region;\n      this.entryBuffer = new LinkedList<>();\n    }",
            " 973  \n 974  \n 975  \n 976 +\n 977  ",
            "    RegionEntryBuffer(TableName tableName, byte[] region) {\n      this.tableName = tableName;\n      this.encodedRegionName = region;\n      this.entryBuffer = new ArrayList<>();\n    }"
        ],
        [
            "WALSplitter::LogRecoveredEditsOutputSink::filterCellByStore(Entry)",
            "1519  \n1520  \n1521  \n1522 -\n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  ",
            "    void filterCellByStore(Entry logEntry) {\n      Map<byte[], Long> maxSeqIdInStores =\n          regionMaxSeqIdInStores.get(Bytes.toString(logEntry.getKey().getEncodedRegionName()));\n      if (maxSeqIdInStores == null || maxSeqIdInStores.isEmpty()) {\n        return;\n      }\n      // Create the array list for the cells that aren't filtered.\n      // We make the assumption that most cells will be kept.\n      ArrayList<Cell> keptCells = new ArrayList<>(logEntry.getEdit().getCells().size());\n      for (Cell cell : logEntry.getEdit().getCells()) {\n        if (CellUtil.matchingFamily(cell, WALEdit.METAFAMILY)) {\n          keptCells.add(cell);\n        } else {\n          byte[] family = CellUtil.cloneFamily(cell);\n          Long maxSeqId = maxSeqIdInStores.get(family);\n          // Do not skip cell even if maxSeqId is null. Maybe we are in a rolling upgrade,\n          // or the master was crashed before and we can not get the information.\n          if (maxSeqId == null || maxSeqId.longValue() < logEntry.getKey().getSequenceId()) {\n            keptCells.add(cell);\n          }\n        }\n      }\n\n      // Anything in the keptCells array list is still live.\n      // So rather than removing the cells from the array list\n      // which would be an O(n^2) operation, we just replace the list\n      logEntry.getEdit().setCells(keptCells);\n    }",
            "1506  \n1507  \n1508  \n1509 +\n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  ",
            "    void filterCellByStore(Entry logEntry) {\n      Map<byte[], Long> maxSeqIdInStores =\n          regionMaxSeqIdInStores.get(Bytes.toString(logEntry.getKey().getEncodedRegionName()));\n      if (MapUtils.isEmpty(maxSeqIdInStores)) {\n        return;\n      }\n      // Create the array list for the cells that aren't filtered.\n      // We make the assumption that most cells will be kept.\n      ArrayList<Cell> keptCells = new ArrayList<>(logEntry.getEdit().getCells().size());\n      for (Cell cell : logEntry.getEdit().getCells()) {\n        if (CellUtil.matchingFamily(cell, WALEdit.METAFAMILY)) {\n          keptCells.add(cell);\n        } else {\n          byte[] family = CellUtil.cloneFamily(cell);\n          Long maxSeqId = maxSeqIdInStores.get(family);\n          // Do not skip cell even if maxSeqId is null. Maybe we are in a rolling upgrade,\n          // or the master was crashed before and we can not get the information.\n          if (maxSeqId == null || maxSeqId.longValue() < logEntry.getKey().getSequenceId()) {\n            keptCells.add(cell);\n          }\n        }\n      }\n\n      // Anything in the keptCells array list is still live.\n      // So rather than removing the cells from the array list\n      // which would be an O(n^2) operation, we just replace the list\n      logEntry.getEdit().setCells(keptCells);\n    }"
        ],
        [
            "WALSplitter::moveAsideBadEditsFile(FileSystem,Path)",
            " 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608 -\n 609  \n 610  \n 611  ",
            "  /**\n   * Move aside a bad edits file.\n   *\n   * @param fs\n   * @param edits\n   *          Edits file to move aside.\n   * @return The name of the moved aside file.\n   * @throws IOException\n   */\n  public static Path moveAsideBadEditsFile(final FileSystem fs, final Path edits)\n      throws IOException {\n    Path moveAsideName = new Path(edits.getParent(), edits.getName() + \".\"\n        + System.currentTimeMillis());\n    if (!fs.rename(edits, moveAsideName)) {\n      LOG.warn(\"Rename failed from \" + edits + \" to \" + moveAsideName);\n    }\n    return moveAsideName;\n  }",
            " 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608 +\n 609  \n 610  \n 611  ",
            "  /**\n   * Move aside a bad edits file.\n   *\n   * @param fs\n   * @param edits\n   *          Edits file to move aside.\n   * @return The name of the moved aside file.\n   * @throws IOException\n   */\n  public static Path moveAsideBadEditsFile(final FileSystem fs, final Path edits)\n      throws IOException {\n    Path moveAsideName = new Path(edits.getParent(), edits.getName() + \".\"\n        + System.currentTimeMillis());\n    if (!fs.rename(edits, moveAsideName)) {\n      LOG.warn(\"Rename failed from {} to {}\", edits, moveAsideName);\n    }\n    return moveAsideName;\n  }"
        ],
        [
            "WALSplitter::getSplitEditFilesSorted(FileSystem,Path)",
            " 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557 -\n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 -\n 581  \n 582  \n 583  \n 584  \n 585 -\n 586 -\n 587 -\n 588 -\n 589 -\n 590  \n 591  \n 592  ",
            "  /**\n   * Returns sorted set of edit files made by splitter, excluding files\n   * with '.temp' suffix.\n   *\n   * @param fs\n   * @param regiondir\n   * @return Files in passed <code>regiondir</code> as a sorted set.\n   * @throws IOException\n   */\n  public static NavigableSet<Path> getSplitEditFilesSorted(final FileSystem fs,\n      final Path regiondir) throws IOException {\n    NavigableSet<Path> filesSorted = new TreeSet<>();\n    Path editsdir = getRegionDirRecoveredEditsDir(regiondir);\n    if (!fs.exists(editsdir))\n      return filesSorted;\n    FileStatus[] files = FSUtils.listStatus(fs, editsdir, new PathFilter() {\n      @Override\n      public boolean accept(Path p) {\n        boolean result = false;\n        try {\n          // Return files and only files that match the editfile names pattern.\n          // There can be other files in this directory other than edit files.\n          // In particular, on error, we'll move aside the bad edit file giving\n          // it a timestamp suffix. See moveAsideBadEditsFile.\n          Matcher m = EDITFILES_NAME_PATTERN.matcher(p.getName());\n          result = fs.isFile(p) && m.matches();\n          // Skip the file whose name ends with RECOVERED_LOG_TMPFILE_SUFFIX,\n          // because it means splitwal thread is writting this file.\n          if (p.getName().endsWith(RECOVERED_LOG_TMPFILE_SUFFIX)) {\n            result = false;\n          }\n          // Skip SeqId Files\n          if (isSequenceIdFile(p)) {\n            result = false;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Failed isFile check on \" + p);\n        }\n        return result;\n      }\n    });\n    if (files == null) {\n      return filesSorted;\n    }\n    for (FileStatus status : files) {\n      filesSorted.add(status.getPath());\n    }\n    return filesSorted;\n  }",
            " 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559 +\n 560  \n 561 +\n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583 +\n 584  \n 585  \n 586  \n 587  \n 588 +\n 589 +\n 590  \n 591  \n 592  ",
            "  /**\n   * Returns sorted set of edit files made by splitter, excluding files\n   * with '.temp' suffix.\n   *\n   * @param fs\n   * @param regiondir\n   * @return Files in passed <code>regiondir</code> as a sorted set.\n   * @throws IOException\n   */\n  public static NavigableSet<Path> getSplitEditFilesSorted(final FileSystem fs,\n      final Path regiondir) throws IOException {\n    NavigableSet<Path> filesSorted = new TreeSet<>();\n    Path editsdir = getRegionDirRecoveredEditsDir(regiondir);\n    if (!fs.exists(editsdir)) {\n      return filesSorted;\n    }\n    FileStatus[] files = FSUtils.listStatus(fs, editsdir, new PathFilter() {\n      @Override\n      public boolean accept(Path p) {\n        boolean result = false;\n        try {\n          // Return files and only files that match the editfile names pattern.\n          // There can be other files in this directory other than edit files.\n          // In particular, on error, we'll move aside the bad edit file giving\n          // it a timestamp suffix. See moveAsideBadEditsFile.\n          Matcher m = EDITFILES_NAME_PATTERN.matcher(p.getName());\n          result = fs.isFile(p) && m.matches();\n          // Skip the file whose name ends with RECOVERED_LOG_TMPFILE_SUFFIX,\n          // because it means splitwal thread is writting this file.\n          if (p.getName().endsWith(RECOVERED_LOG_TMPFILE_SUFFIX)) {\n            result = false;\n          }\n          // Skip SeqId Files\n          if (isSequenceIdFile(p)) {\n            result = false;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Failed isFile check on {}\", p, e);\n        }\n        return result;\n      }\n    });\n    if (ArrayUtils.isNotEmpty(files)) {\n      Arrays.asList(files).forEach(status -> filesSorted.add(status.getPath()));\n    }\n    return filesSorted;\n  }"
        ],
        [
            "WALSplitter::LogRecoveredEditsOutputSink::appendBuffer(RegionEntryBuffer,boolean)",
            "1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570 -\n1571 -\n1572 -\n1573 -\n1574 -\n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593 -\n1594  \n1595  \n1596  \n1597  ",
            "    WriterAndPath appendBuffer(RegionEntryBuffer buffer, boolean reusable) throws IOException{\n      List<Entry> entries = buffer.entryBuffer;\n      if (entries.isEmpty()) {\n        LOG.warn(\"got an empty buffer, skipping\");\n        return null;\n      }\n\n      WriterAndPath wap = null;\n\n      long startTime = System.nanoTime();\n      try {\n        int editsCount = 0;\n\n        for (Entry logEntry : entries) {\n          if (wap == null) {\n            wap = getWriterAndPath(logEntry, reusable);\n            if (wap == null) {\n              if (LOG.isTraceEnabled()) {\n                // This log spews the full edit. Can be massive in the log. Enable only debugging\n                // WAL lost edit issues.\n                LOG.trace(\"getWriterAndPath decided we don't need to write edits for \" + logEntry);\n              }\n              return null;\n            }\n          }\n          filterCellByStore(logEntry);\n          if (!logEntry.getEdit().isEmpty()) {\n            wap.w.append(logEntry);\n            this.updateRegionMaximumEditLogSeqNum(logEntry);\n            editsCount++;\n          } else {\n            wap.incrementSkippedEdits(1);\n          }\n        }\n        // Pass along summary statistics\n        wap.incrementEdits(editsCount);\n        wap.incrementNanoTime(System.nanoTime() - startTime);\n      } catch (IOException e) {\n          e = e instanceof RemoteException ?\n                  ((RemoteException)e).unwrapRemoteException() : e;\n        LOG.error(HBaseMarkers.FATAL, \" Got while writing log entry to log\", e);\n        throw e;\n      }\n      return wap;\n    }",
            "1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557 +\n1558 +\n1559 +\n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578 +\n1579  \n1580  \n1581  \n1582  ",
            "    WriterAndPath appendBuffer(RegionEntryBuffer buffer, boolean reusable) throws IOException{\n      List<Entry> entries = buffer.entryBuffer;\n      if (entries.isEmpty()) {\n        LOG.warn(\"got an empty buffer, skipping\");\n        return null;\n      }\n\n      WriterAndPath wap = null;\n\n      long startTime = System.nanoTime();\n      try {\n        int editsCount = 0;\n\n        for (Entry logEntry : entries) {\n          if (wap == null) {\n            wap = getWriterAndPath(logEntry, reusable);\n            if (wap == null) {\n              // This log spews the full edit. Can be massive in the log. Enable only debugging\n              // WAL lost edit issues.\n              LOG.trace(\"getWriterAndPath decided we don't need to write edits for {}\", logEntry);\n              return null;\n            }\n          }\n          filterCellByStore(logEntry);\n          if (!logEntry.getEdit().isEmpty()) {\n            wap.w.append(logEntry);\n            this.updateRegionMaximumEditLogSeqNum(logEntry);\n            editsCount++;\n          } else {\n            wap.incrementSkippedEdits(1);\n          }\n        }\n        // Pass along summary statistics\n        wap.incrementEdits(editsCount);\n        wap.incrementNanoTime(System.nanoTime() - startTime);\n      } catch (IOException e) {\n          e = e instanceof RemoteException ?\n                  ((RemoteException)e).unwrapRemoteException() : e;\n        LOG.error(HBaseMarkers.FATAL, \"Got while writing log entry to log\", e);\n        throw e;\n      }\n      return wap;\n    }"
        ],
        [
            "WALSplitter::LogRecoveredEditsOutputSink::keepRegionEvent(Entry)",
            "1599  \n1600  \n1601  \n1602 -\n1603 -\n1604  \n1605  \n1606  \n1607  \n1608  ",
            "    @Override\n    public boolean keepRegionEvent(Entry entry) {\n      ArrayList<Cell> cells = entry.getEdit().getCells();\n      for (int i = 0; i < cells.size(); i++) {\n        if (WALEdit.isCompactionMarker(cells.get(i))) {\n          return true;\n        }\n      }\n      return false;\n    }",
            "1584  \n1585  \n1586  \n1587 +\n1588 +\n1589  \n1590  \n1591  \n1592  \n1593  ",
            "    @Override\n    public boolean keepRegionEvent(Entry entry) {\n      ArrayList<Cell> cells = entry.getEdit().getCells();\n      for (Cell cell : cells) {\n        if (WALEdit.isCompactionMarker(cell)) {\n          return true;\n        }\n      }\n      return false;\n    }"
        ],
        [
            "WALSplitter::OutputSink::finishWriting(boolean)",
            "1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193 -\n1194  \n1195  ",
            "    /**\n     * Wait for writer threads to dump all info to the sink\n     * @return true when there is no error\n     * @throws IOException\n     */\n    protected boolean finishWriting(boolean interrupt) throws IOException {\n      LOG.debug(\"Waiting for split writer threads to finish\");\n      boolean progress_failed = false;\n      for (WriterThread t : writerThreads) {\n        t.finish();\n      }\n      if (interrupt) {\n        for (WriterThread t : writerThreads) {\n          t.interrupt(); // interrupt the writer threads. We are stopping now.\n        }\n      }\n\n      for (WriterThread t : writerThreads) {\n        if (!progress_failed && reporter != null && !reporter.progress()) {\n          progress_failed = true;\n        }\n        try {\n          t.join();\n        } catch (InterruptedException ie) {\n          IOException iie = new InterruptedIOException();\n          iie.initCause(ie);\n          throw iie;\n        }\n      }\n      controller.checkForErrors();\n      LOG.info(this.writerThreads.size() + \" split writers finished; closing...\");\n      return (!progress_failed);\n    }",
            "1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176  \n1177  \n1178  \n1179  \n1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186 +\n1187  \n1188  ",
            "    /**\n     * Wait for writer threads to dump all info to the sink\n     * @return true when there is no error\n     * @throws IOException\n     */\n    protected boolean finishWriting(boolean interrupt) throws IOException {\n      LOG.debug(\"Waiting for split writer threads to finish\");\n      boolean progress_failed = false;\n      for (WriterThread t : writerThreads) {\n        t.finish();\n      }\n      if (interrupt) {\n        for (WriterThread t : writerThreads) {\n          t.interrupt(); // interrupt the writer threads. We are stopping now.\n        }\n      }\n\n      for (WriterThread t : writerThreads) {\n        if (!progress_failed && reporter != null && !reporter.progress()) {\n          progress_failed = true;\n        }\n        try {\n          t.join();\n        } catch (InterruptedException ie) {\n          IOException iie = new InterruptedIOException();\n          iie.initCause(ie);\n          throw iie;\n        }\n      }\n      controller.checkForErrors();\n      LOG.info(\"{} split writers finished; closing.\", this.writerThreads.size());\n      return (!progress_failed);\n    }"
        ],
        [
            "WALSplitter::LogRecoveredEditsOutputSink::closeLogWriters(List)",
            "1427  \n1428  \n1429  \n1430  \n1431 -\n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456 -\n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  ",
            "    private List<IOException> closeLogWriters(List<IOException> thrown) throws IOException {\n      if (writersClosed) {\n        return thrown;\n      }\n\n      if (thrown == null) {\n        thrown = Lists.newArrayList();\n      }\n      try {\n        for (WriterThread t : writerThreads) {\n          while (t.isAlive()) {\n            t.shouldStop = true;\n            t.interrupt();\n            try {\n              t.join(10);\n            } catch (InterruptedException e) {\n              IOException iie = new InterruptedIOException();\n              iie.initCause(e);\n              throw iie;\n            }\n          }\n        }\n      } finally {\n        WriterAndPath wap = null;\n        for (SinkWriter tmpWAP : writers.values()) {\n          try {\n            wap = (WriterAndPath) tmpWAP;\n            wap.w.close();\n          } catch (IOException ioe) {\n            LOG.error(\"Couldn't close log at \" + wap.p, ioe);\n            thrown.add(ioe);\n            continue;\n          }\n          LOG.info(\n              \"Closed log \" + wap.p + \" (wrote \" + wap.editsWritten + \" edits in \" + (wap.nanosSpent\n                  / 1000 / 1000) + \"ms)\");\n        }\n        writersClosed = true;\n      }\n\n      return thrown;\n    }",
            "1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443 +\n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  ",
            "    private List<IOException> closeLogWriters(List<IOException> thrown) throws IOException {\n      if (writersClosed) {\n        return thrown;\n      }\n      if (thrown == null) {\n        thrown = Lists.newArrayList();\n      }\n      try {\n        for (WriterThread t : writerThreads) {\n          while (t.isAlive()) {\n            t.shouldStop = true;\n            t.interrupt();\n            try {\n              t.join(10);\n            } catch (InterruptedException e) {\n              IOException iie = new InterruptedIOException();\n              iie.initCause(e);\n              throw iie;\n            }\n          }\n        }\n      } finally {\n        WriterAndPath wap = null;\n        for (SinkWriter tmpWAP : writers.values()) {\n          try {\n            wap = (WriterAndPath) tmpWAP;\n            wap.w.close();\n          } catch (IOException ioe) {\n            LOG.error(\"Couldn't close log at {}\", wap.p, ioe);\n            thrown.add(ioe);\n            continue;\n          }\n          LOG.info(\n              \"Closed log \" + wap.p + \" (wrote \" + wap.editsWritten + \" edits in \" + (wap.nanosSpent\n                  / 1000 / 1000) + \"ms)\");\n        }\n        writersClosed = true;\n      }\n\n      return thrown;\n    }"
        ],
        [
            "WALSplitter::archiveLogs(List,List,Path,FileSystem,Configuration)",
            " 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420 -\n 421 -\n 422  \n 423  \n 424 -\n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434 -\n 435  \n 436 -\n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 -\n 446  \n 447 -\n 448  \n 449  \n 450  \n 451  ",
            "  /**\n   * Moves processed logs to a oldLogDir after successful processing Moves\n   * corrupted logs (any log that couldn't be successfully parsed to corruptDir\n   * (.corrupt) for later investigation\n   *\n   * @param corruptedLogs\n   * @param processedLogs\n   * @param oldLogDir\n   * @param fs\n   * @param conf\n   * @throws IOException\n   */\n  private static void archiveLogs(\n      final List<Path> corruptedLogs,\n      final List<Path> processedLogs, final Path oldLogDir,\n      final FileSystem fs, final Configuration conf) throws IOException {\n    final Path corruptDir = new Path(FSUtils.getWALRootDir(conf), HConstants.CORRUPT_DIR_NAME);\n    if (conf.get(\"hbase.regionserver.hlog.splitlog.corrupt.dir\") != null) {\n      LOG.warn(\"hbase.regionserver.hlog.splitlog.corrupt.dir is deprecated. Default to \"\n          + corruptDir);\n    }\n    if (!fs.mkdirs(corruptDir)) {\n      LOG.info(\"Unable to mkdir \" + corruptDir);\n    }\n    fs.mkdirs(oldLogDir);\n\n    // this method can get restarted or called multiple times for archiving\n    // the same log files.\n    for (Path corrupted : corruptedLogs) {\n      Path p = new Path(corruptDir, corrupted.getName());\n      if (fs.exists(corrupted)) {\n        if (!fs.rename(corrupted, p)) {\n          LOG.warn(\"Unable to move corrupted log \" + corrupted + \" to \" + p);\n        } else {\n          LOG.warn(\"Moved corrupted log \" + corrupted + \" to \" + p);\n        }\n      }\n    }\n\n    for (Path p : processedLogs) {\n      Path newPath = AbstractFSWAL.getWALArchivePath(oldLogDir, p);\n      if (fs.exists(p)) {\n        if (!FSUtils.renameAndSetModifyTime(fs, p, newPath)) {\n          LOG.warn(\"Unable to move  \" + p + \" to \" + newPath);\n        } else {\n          LOG.info(\"Archived processed log \" + p + \" to \" + newPath);\n        }\n      }\n    }\n  }",
            " 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422 +\n 423 +\n 424  \n 425  \n 426 +\n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436 +\n 437  \n 438 +\n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447 +\n 448  \n 449 +\n 450  \n 451  \n 452  \n 453  ",
            "  /**\n   * Moves processed logs to a oldLogDir after successful processing Moves\n   * corrupted logs (any log that couldn't be successfully parsed to corruptDir\n   * (.corrupt) for later investigation\n   *\n   * @param corruptedLogs\n   * @param processedLogs\n   * @param oldLogDir\n   * @param fs\n   * @param conf\n   * @throws IOException\n   */\n  private static void archiveLogs(\n      final List<Path> corruptedLogs,\n      final List<Path> processedLogs, final Path oldLogDir,\n      final FileSystem fs, final Configuration conf) throws IOException {\n    final Path corruptDir = new Path(FSUtils.getWALRootDir(conf), HConstants.CORRUPT_DIR_NAME);\n    if (conf.get(\"hbase.regionserver.hlog.splitlog.corrupt.dir\") != null) {\n      LOG.warn(\"hbase.regionserver.hlog.splitlog.corrupt.dir is deprecated. Default to {}\",\n          corruptDir);\n    }\n    if (!fs.mkdirs(corruptDir)) {\n      LOG.info(\"Unable to mkdir {}\", corruptDir);\n    }\n    fs.mkdirs(oldLogDir);\n\n    // this method can get restarted or called multiple times for archiving\n    // the same log files.\n    for (Path corrupted : corruptedLogs) {\n      Path p = new Path(corruptDir, corrupted.getName());\n      if (fs.exists(corrupted)) {\n        if (!fs.rename(corrupted, p)) {\n          LOG.warn(\"Unable to move corrupted log {} to {}\", corrupted, p);\n        } else {\n          LOG.warn(\"Moved corrupted log {} to {}\", corrupted, p);\n        }\n      }\n    }\n\n    for (Path p : processedLogs) {\n      Path newPath = AbstractFSWAL.getWALArchivePath(oldLogDir, p);\n      if (fs.exists(p)) {\n        if (!FSUtils.renameAndSetModifyTime(fs, p, newPath)) {\n          LOG.warn(\"Unable to move {} to {}\", p, newPath);\n        } else {\n          LOG.info(\"Archived processed log {} to {}\", p, newPath);\n        }\n      }\n    }\n  }"
        ],
        [
            "WALSplitter::getNextLogLine(Reader,Path,boolean)",
            " 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758 -\n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766 -\n 767 -\n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  ",
            "  static private Entry getNextLogLine(Reader in, Path path, boolean skipErrors)\n  throws CorruptedLogFileException, IOException {\n    try {\n      return in.next();\n    } catch (EOFException eof) {\n      // truncated files are expected if a RS crashes (see HBASE-2643)\n      LOG.info(\"EOF from wal \" + path + \".  continuing\");\n      return null;\n    } catch (IOException e) {\n      // If the IOE resulted from bad file format,\n      // then this problem is idempotent and retrying won't help\n      if (e.getCause() != null &&\n          (e.getCause() instanceof ParseException ||\n           e.getCause() instanceof org.apache.hadoop.fs.ChecksumException)) {\n        LOG.warn(\"Parse exception \" + e.getCause().toString() + \" from wal \"\n           + path + \".  continuing\");\n        return null;\n      }\n      if (!skipErrors) {\n        throw e;\n      }\n      CorruptedLogFileException t =\n        new CorruptedLogFileException(\"skipErrors=true Ignoring exception\" +\n            \" while parsing wal \" + path + \". Marking as corrupted\");\n      t.initCause(e);\n      throw t;\n    }\n  }",
            " 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753 +\n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761 +\n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  ",
            "  static private Entry getNextLogLine(Reader in, Path path, boolean skipErrors)\n  throws CorruptedLogFileException, IOException {\n    try {\n      return in.next();\n    } catch (EOFException eof) {\n      // truncated files are expected if a RS crashes (see HBASE-2643)\n      LOG.info(\"EOF from wal {}. Continuing.\", path);\n      return null;\n    } catch (IOException e) {\n      // If the IOE resulted from bad file format,\n      // then this problem is idempotent and retrying won't help\n      if (e.getCause() != null &&\n          (e.getCause() instanceof ParseException ||\n           e.getCause() instanceof org.apache.hadoop.fs.ChecksumException)) {\n        LOG.warn(\"Parse exception from wal {}. Continuing\", path, e);\n        return null;\n      }\n      if (!skipErrors) {\n        throw e;\n      }\n      CorruptedLogFileException t =\n        new CorruptedLogFileException(\"skipErrors=true Ignoring exception\" +\n            \" while parsing wal \" + path + \". Marking as corrupted\");\n      t.initCause(e);\n      throw t;\n    }\n  }"
        ],
        [
            "WALSplitter::getRegionSplitEditsPath(FileSystem,Entry,Path,String)",
            " 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477 -\n 478 -\n 479 -\n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489 -\n 490  \n 491 -\n 492  \n 493 -\n 494  \n 495  \n 496  \n 497  \n 498 -\n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  ",
            "  /**\n   * Path to a file under RECOVERED_EDITS_DIR directory of the region found in\n   * <code>logEntry</code> named for the sequenceid in the passed\n   * <code>logEntry</code>: e.g. /hbase/some_table/2323432434/recovered.edits/2332.\n   * This method also ensures existence of RECOVERED_EDITS_DIR under the region\n   * creating it if necessary.\n   * @param fs\n   * @param logEntry\n   * @param rootDir HBase root dir.\n   * @param fileNameBeingSplit the file being split currently. Used to generate tmp file name.\n   * @return Path to file into which to dump split log edits.\n   * @throws IOException\n   */\n  @SuppressWarnings(\"deprecation\")\n  @VisibleForTesting\n  static Path getRegionSplitEditsPath(final FileSystem fs,\n      final Entry logEntry, final Path rootDir, String fileNameBeingSplit)\n  throws IOException {\n    Path tableDir = FSUtils.getTableDir(rootDir, logEntry.getKey().getTablename());\n    String encodedRegionName = Bytes.toString(logEntry.getKey().getEncodedRegionName());\n    Path regiondir = HRegion.getRegionDir(tableDir, encodedRegionName);\n    Path dir = getRegionDirRecoveredEditsDir(regiondir);\n\n    if (!fs.exists(regiondir)) {\n      LOG.info(\"This region's directory doesn't exist: \"\n          + regiondir.toString() + \". It is very likely that it was\" +\n          \" already split so it's safe to discard those edits.\");\n      return null;\n    }\n    if (fs.exists(dir) && fs.isFile(dir)) {\n      Path tmp = new Path(\"/tmp\");\n      if (!fs.exists(tmp)) {\n        fs.mkdirs(tmp);\n      }\n      tmp = new Path(tmp,\n        HConstants.RECOVERED_EDITS_DIR + \"_\" + encodedRegionName);\n      LOG.warn(\"Found existing old file: \" + dir + \". It could be some \"\n        + \"leftover of an old installation. It should be a folder instead. \"\n        + \"So moving it to \" + tmp);\n      if (!fs.rename(dir, tmp)) {\n        LOG.warn(\"Failed to sideline old file \" + dir);\n      }\n    }\n\n    if (!fs.exists(dir) && !fs.mkdirs(dir)) {\n      LOG.warn(\"mkdir failed on \" + dir);\n    }\n    // Append fileBeingSplit to prevent name conflict since we may have duplicate wal entries now.\n    // Append file name ends with RECOVERED_LOG_TMPFILE_SUFFIX to ensure\n    // region's replayRecoveredEdits will not delete it\n    String fileName = formatRecoveredEditsFileName(logEntry.getKey().getSequenceId());\n    fileName = getTmpRecoveredEditsFileName(fileName + \"-\" + fileNameBeingSplit);\n    return new Path(dir, fileName);\n  }",
            " 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479 +\n 480 +\n 481 +\n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491 +\n 492  \n 493 +\n 494  \n 495 +\n 496  \n 497  \n 498  \n 499  \n 500 +\n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  ",
            "  /**\n   * Path to a file under RECOVERED_EDITS_DIR directory of the region found in\n   * <code>logEntry</code> named for the sequenceid in the passed\n   * <code>logEntry</code>: e.g. /hbase/some_table/2323432434/recovered.edits/2332.\n   * This method also ensures existence of RECOVERED_EDITS_DIR under the region\n   * creating it if necessary.\n   * @param fs\n   * @param logEntry\n   * @param rootDir HBase root dir.\n   * @param fileNameBeingSplit the file being split currently. Used to generate tmp file name.\n   * @return Path to file into which to dump split log edits.\n   * @throws IOException\n   */\n  @SuppressWarnings(\"deprecation\")\n  @VisibleForTesting\n  static Path getRegionSplitEditsPath(final FileSystem fs,\n      final Entry logEntry, final Path rootDir, String fileNameBeingSplit)\n  throws IOException {\n    Path tableDir = FSUtils.getTableDir(rootDir, logEntry.getKey().getTablename());\n    String encodedRegionName = Bytes.toString(logEntry.getKey().getEncodedRegionName());\n    Path regiondir = HRegion.getRegionDir(tableDir, encodedRegionName);\n    Path dir = getRegionDirRecoveredEditsDir(regiondir);\n\n    if (!fs.exists(regiondir)) {\n      LOG.info(\"This region's directory does not exist: {}.\"\n          + \"It is very likely that it was already split so it is \"\n          + \"safe to discard those edits.\", regiondir);\n      return null;\n    }\n    if (fs.exists(dir) && fs.isFile(dir)) {\n      Path tmp = new Path(\"/tmp\");\n      if (!fs.exists(tmp)) {\n        fs.mkdirs(tmp);\n      }\n      tmp = new Path(tmp,\n        HConstants.RECOVERED_EDITS_DIR + \"_\" + encodedRegionName);\n      LOG.warn(\"Found existing old file: {}. It could be some \"\n        + \"leftover of an old installation. It should be a folder instead. \"\n        + \"So moving it to {}\", dir, tmp);\n      if (!fs.rename(dir, tmp)) {\n        LOG.warn(\"Failed to sideline old file {}\", dir);\n      }\n    }\n\n    if (!fs.exists(dir) && !fs.mkdirs(dir)) {\n      LOG.warn(\"mkdir failed on {}\", dir);\n    }\n    // Append fileBeingSplit to prevent name conflict since we may have duplicate wal entries now.\n    // Append file name ends with RECOVERED_LOG_TMPFILE_SUFFIX to ensure\n    // region's replayRecoveredEdits will not delete it\n    String fileName = formatRecoveredEditsFileName(logEntry.getKey().getSequenceId());\n    fileName = getTmpRecoveredEditsFileName(fileName + \"-\" + fileNameBeingSplit);\n    return new Path(dir, fileName);\n  }"
        ],
        [
            "WALSplitter::EntryBuffers::appendEntry(Entry)",
            " 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896 -\n 897 -\n 898  \n 899  \n 900  \n 901  \n 902  \n 903  ",
            "    /**\n     * Append a log entry into the corresponding region buffer.\n     * Blocks if the total heap usage has crossed the specified threshold.\n     *\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void appendEntry(Entry entry) throws InterruptedException, IOException {\n      WALKey key = entry.getKey();\n\n      RegionEntryBuffer buffer;\n      long incrHeap;\n      synchronized (this) {\n        buffer = buffers.get(key.getEncodedRegionName());\n        if (buffer == null) {\n          buffer = new RegionEntryBuffer(key.getTablename(), key.getEncodedRegionName());\n          buffers.put(key.getEncodedRegionName(), buffer);\n        }\n        incrHeap= buffer.appendEntry(entry);\n      }\n\n      // If we crossed the chunk threshold, wait for more space to be available\n      synchronized (controller.dataAvailable) {\n        totalBuffered += incrHeap;\n        while (totalBuffered > maxHeapUsage && controller.thrown.get() == null) {\n          LOG.debug(\"Used \" + totalBuffered +\n              \" bytes of buffered edits, waiting for IO threads...\");\n          controller.dataAvailable.wait(2000);\n        }\n        controller.dataAvailable.notifyAll();\n      }\n      controller.checkForErrors();\n    }",
            " 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890 +\n 891  \n 892  \n 893  \n 894  \n 895  \n 896  ",
            "    /**\n     * Append a log entry into the corresponding region buffer.\n     * Blocks if the total heap usage has crossed the specified threshold.\n     *\n     * @throws InterruptedException\n     * @throws IOException\n     */\n    public void appendEntry(Entry entry) throws InterruptedException, IOException {\n      WALKey key = entry.getKey();\n\n      RegionEntryBuffer buffer;\n      long incrHeap;\n      synchronized (this) {\n        buffer = buffers.get(key.getEncodedRegionName());\n        if (buffer == null) {\n          buffer = new RegionEntryBuffer(key.getTablename(), key.getEncodedRegionName());\n          buffers.put(key.getEncodedRegionName(), buffer);\n        }\n        incrHeap= buffer.appendEntry(entry);\n      }\n\n      // If we crossed the chunk threshold, wait for more space to be available\n      synchronized (controller.dataAvailable) {\n        totalBuffered += incrHeap;\n        while (totalBuffered > maxHeapUsage && controller.thrown.get() == null) {\n          LOG.debug(\"Used {} bytes of buffered edits, waiting for IO threads\", totalBuffered);\n          controller.dataAvailable.wait(2000);\n        }\n        controller.dataAvailable.notifyAll();\n      }\n      controller.checkForErrors();\n    }"
        ],
        [
            "WALSplitter::LogRecoveredEditsOutputSink::finishWritingAndClose()",
            "1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260 -\n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  ",
            "    /**\n     * @return null if failed to report progress\n     * @throws IOException\n     */\n    @Override\n    public List<Path> finishWritingAndClose() throws IOException {\n      boolean isSuccessful = false;\n      List<Path> result = null;\n      try {\n        isSuccessful = finishWriting(false);\n      } finally {\n        result = close();\n        List<IOException> thrown = closeLogWriters(null);\n        if (thrown != null && !thrown.isEmpty()) {\n          throw MultipleIOException.createIOException(thrown);\n        }\n      }\n      if (isSuccessful) {\n        splits = result;\n      }\n      return splits;\n    }",
            "1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253 +\n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  ",
            "    /**\n     * @return null if failed to report progress\n     * @throws IOException\n     */\n    @Override\n    public List<Path> finishWritingAndClose() throws IOException {\n      boolean isSuccessful = false;\n      List<Path> result = null;\n      try {\n        isSuccessful = finishWriting(false);\n      } finally {\n        result = close();\n        List<IOException> thrown = closeLogWriters(null);\n        if (CollectionUtils.isNotEmpty(thrown)) {\n          throw MultipleIOException.createIOException(thrown);\n        }\n      }\n      if (isSuccessful) {\n        splits = result;\n      }\n      return splits;\n    }"
        ],
        [
            "WALSplitter::getMutationsFromWALEntry(WALEntry,CellScanner,Pair,Durability)",
            "1822  \n1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838 -\n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849 -\n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  ",
            "  /**\n   * This function is used to construct mutations from a WALEntry. It also\n   * reconstructs WALKey &amp; WALEdit from the passed in WALEntry\n   * @param entry\n   * @param cells\n   * @param logEntry pair of WALKey and WALEdit instance stores WALKey and WALEdit instances\n   *          extracted from the passed in WALEntry.\n   * @return list of Pair&lt;MutationType, Mutation&gt; to be replayed\n   * @throws IOException\n   */\n  public static List<MutationReplay> getMutationsFromWALEntry(WALEntry entry, CellScanner cells,\n      Pair<WALKey, WALEdit> logEntry, Durability durability)\n          throws IOException {\n\n    if (entry == null) {\n      // return an empty array\n      return new ArrayList<>();\n    }\n\n    long replaySeqId = (entry.getKey().hasOrigSequenceNumber()) ?\n      entry.getKey().getOrigSequenceNumber() : entry.getKey().getLogSequenceNumber();\n    int count = entry.getAssociatedCellCount();\n    List<MutationReplay> mutations = new ArrayList<>();\n    Cell previousCell = null;\n    Mutation m = null;\n    WALKeyImpl key = null;\n    WALEdit val = null;\n    if (logEntry != null) val = new WALEdit();\n\n    for (int i = 0; i < count; i++) {\n      // Throw index out of bounds if our cell count is off\n      if (!cells.advance()) {\n        throw new ArrayIndexOutOfBoundsException(\"Expected=\" + count + \", index=\" + i);\n      }\n      Cell cell = cells.current();\n      if (val != null) val.add(cell);\n\n      boolean isNewRowOrType =\n          previousCell == null || previousCell.getTypeByte() != cell.getTypeByte()\n              || !CellUtil.matchingRows(previousCell, cell);\n      if (isNewRowOrType) {\n        // Create new mutation\n        if (CellUtil.isDelete(cell)) {\n          m = new Delete(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength());\n          // Deletes don't have nonces.\n          mutations.add(new MutationReplay(\n              MutationType.DELETE, m, HConstants.NO_NONCE, HConstants.NO_NONCE));\n        } else {\n          m = new Put(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength());\n          // Puts might come from increment or append, thus we need nonces.\n          long nonceGroup = entry.getKey().hasNonceGroup()\n              ? entry.getKey().getNonceGroup() : HConstants.NO_NONCE;\n          long nonce = entry.getKey().hasNonce() ? entry.getKey().getNonce() : HConstants.NO_NONCE;\n          mutations.add(new MutationReplay(MutationType.PUT, m, nonceGroup, nonce));\n        }\n      }\n      if (CellUtil.isDelete(cell)) {\n        ((Delete) m).add(cell);\n      } else {\n        ((Put) m).add(cell);\n      }\n      m.setDurability(durability);\n      previousCell = cell;\n    }\n\n    // reconstruct WALKey\n    if (logEntry != null) {\n      org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos.WALKey walKeyProto =\n          entry.getKey();\n      List<UUID> clusterIds = new ArrayList<>(walKeyProto.getClusterIdsCount());\n      for (HBaseProtos.UUID uuid : entry.getKey().getClusterIdsList()) {\n        clusterIds.add(new UUID(uuid.getMostSigBits(), uuid.getLeastSigBits()));\n      }\n      key = new WALKeyImpl(walKeyProto.getEncodedRegionName().toByteArray(), TableName.valueOf(\n              walKeyProto.getTableName().toByteArray()), replaySeqId, walKeyProto.getWriteTime(),\n              clusterIds, walKeyProto.getNonceGroup(), walKeyProto.getNonce(), null);\n      logEntry.setFirst(key);\n      logEntry.setSecond(val);\n    }\n\n    return mutations;\n  }",
            "1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823 +\n1824  \n1825  \n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834 +\n1835 +\n1836 +\n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  \n1851  \n1852  \n1853  \n1854  \n1855  \n1856  \n1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  ",
            "  /**\n   * This function is used to construct mutations from a WALEntry. It also\n   * reconstructs WALKey &amp; WALEdit from the passed in WALEntry\n   * @param entry\n   * @param cells\n   * @param logEntry pair of WALKey and WALEdit instance stores WALKey and WALEdit instances\n   *          extracted from the passed in WALEntry.\n   * @return list of Pair&lt;MutationType, Mutation&gt; to be replayed\n   * @throws IOException\n   */\n  public static List<MutationReplay> getMutationsFromWALEntry(WALEntry entry, CellScanner cells,\n      Pair<WALKey, WALEdit> logEntry, Durability durability)\n          throws IOException {\n\n    if (entry == null) {\n      // return an empty array\n      return Collections.emptyList();\n    }\n\n    long replaySeqId = (entry.getKey().hasOrigSequenceNumber()) ?\n      entry.getKey().getOrigSequenceNumber() : entry.getKey().getLogSequenceNumber();\n    int count = entry.getAssociatedCellCount();\n    List<MutationReplay> mutations = new ArrayList<>();\n    Cell previousCell = null;\n    Mutation m = null;\n    WALKeyImpl key = null;\n    WALEdit val = null;\n    if (logEntry != null) {\n      val = new WALEdit();\n    }\n\n    for (int i = 0; i < count; i++) {\n      // Throw index out of bounds if our cell count is off\n      if (!cells.advance()) {\n        throw new ArrayIndexOutOfBoundsException(\"Expected=\" + count + \", index=\" + i);\n      }\n      Cell cell = cells.current();\n      if (val != null) val.add(cell);\n\n      boolean isNewRowOrType =\n          previousCell == null || previousCell.getTypeByte() != cell.getTypeByte()\n              || !CellUtil.matchingRows(previousCell, cell);\n      if (isNewRowOrType) {\n        // Create new mutation\n        if (CellUtil.isDelete(cell)) {\n          m = new Delete(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength());\n          // Deletes don't have nonces.\n          mutations.add(new MutationReplay(\n              MutationType.DELETE, m, HConstants.NO_NONCE, HConstants.NO_NONCE));\n        } else {\n          m = new Put(cell.getRowArray(), cell.getRowOffset(), cell.getRowLength());\n          // Puts might come from increment or append, thus we need nonces.\n          long nonceGroup = entry.getKey().hasNonceGroup()\n              ? entry.getKey().getNonceGroup() : HConstants.NO_NONCE;\n          long nonce = entry.getKey().hasNonce() ? entry.getKey().getNonce() : HConstants.NO_NONCE;\n          mutations.add(new MutationReplay(MutationType.PUT, m, nonceGroup, nonce));\n        }\n      }\n      if (CellUtil.isDelete(cell)) {\n        ((Delete) m).add(cell);\n      } else {\n        ((Put) m).add(cell);\n      }\n      m.setDurability(durability);\n      previousCell = cell;\n    }\n\n    // reconstruct WALKey\n    if (logEntry != null) {\n      org.apache.hadoop.hbase.shaded.protobuf.generated.WALProtos.WALKey walKeyProto =\n          entry.getKey();\n      List<UUID> clusterIds = new ArrayList<>(walKeyProto.getClusterIdsCount());\n      for (HBaseProtos.UUID uuid : entry.getKey().getClusterIdsList()) {\n        clusterIds.add(new UUID(uuid.getMostSigBits(), uuid.getLeastSigBits()));\n      }\n      key = new WALKeyImpl(walKeyProto.getEncodedRegionName().toByteArray(), TableName.valueOf(\n              walKeyProto.getTableName().toByteArray()), replaySeqId, walKeyProto.getWriteTime(),\n              clusterIds, walKeyProto.getNonceGroup(), walKeyProto.getNonce(), null);\n      logEntry.setFirst(key);\n      logEntry.setSecond(val);\n    }\n\n    return mutations;\n  }"
        ],
        [
            "WALSplitter::split(Path,Path,Path,FileSystem,Configuration,WALFactory)",
            " 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206 -\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "  @VisibleForTesting\n  public static List<Path> split(Path rootDir, Path logDir, Path oldLogDir,\n      FileSystem fs, Configuration conf, final WALFactory factory) throws IOException {\n    final FileStatus[] logfiles = SplitLogManager.getFileList(conf,\n        Collections.singletonList(logDir), null);\n    List<Path> splits = new ArrayList<>();\n    if (logfiles != null && logfiles.length > 0) {\n      for (FileStatus logfile: logfiles) {\n        WALSplitter s = new WALSplitter(factory, conf, rootDir, fs, null, null);\n        if (s.splitLogFile(logfile, null)) {\n          finishSplitLogFile(rootDir, oldLogDir, logfile.getPath(), conf);\n          if (s.outputSink.splits != null) {\n            splits.addAll(s.outputSink.splits);\n          }\n        }\n      }\n    }\n    if (!fs.delete(logDir, true)) {\n      throw new IOException(\"Unable to delete src dir: \" + logDir);\n    }\n    return splits;\n  }",
            " 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 +\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  ",
            "  @VisibleForTesting\n  public static List<Path> split(Path rootDir, Path logDir, Path oldLogDir,\n      FileSystem fs, Configuration conf, final WALFactory factory) throws IOException {\n    final FileStatus[] logfiles = SplitLogManager.getFileList(conf,\n        Collections.singletonList(logDir), null);\n    List<Path> splits = new ArrayList<>();\n    if (ArrayUtils.isNotEmpty(logfiles)) {\n      for (FileStatus logfile: logfiles) {\n        WALSplitter s = new WALSplitter(factory, conf, rootDir, fs, null, null);\n        if (s.splitLogFile(logfile, null)) {\n          finishSplitLogFile(rootDir, oldLogDir, logfile.getPath(), conf);\n          if (s.outputSink.splits != null) {\n            splits.addAll(s.outputSink.splits);\n          }\n        }\n      }\n    }\n    if (!fs.delete(logDir, true)) {\n      throw new IOException(\"Unable to delete src dir: \" + logDir);\n    }\n    return splits;\n  }"
        ],
        [
            "WALSplitter::WriterThread::doRun()",
            "1043  \n1044 -\n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  ",
            "    private void doRun() throws IOException {\n      if (LOG.isTraceEnabled()) LOG.trace(\"Writer thread starting\");\n      while (true) {\n        RegionEntryBuffer buffer = entryBuffers.getChunkToWrite();\n        if (buffer == null) {\n          // No data currently available, wait on some more to show up\n          synchronized (controller.dataAvailable) {\n            if (shouldStop && !this.outputSink.flush()) {\n              return;\n            }\n            try {\n              controller.dataAvailable.wait(500);\n            } catch (InterruptedException ie) {\n              if (!shouldStop) {\n                throw new RuntimeException(ie);\n              }\n            }\n          }\n          continue;\n        }\n\n        assert buffer != null;\n        try {\n          writeBuffer(buffer);\n        } finally {\n          entryBuffers.doneWriting(buffer);\n        }\n      }\n    }",
            "1036  \n1037 +\n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  ",
            "    private void doRun() throws IOException {\n      LOG.trace(\"Writer thread starting\");\n      while (true) {\n        RegionEntryBuffer buffer = entryBuffers.getChunkToWrite();\n        if (buffer == null) {\n          // No data currently available, wait on some more to show up\n          synchronized (controller.dataAvailable) {\n            if (shouldStop && !this.outputSink.flush()) {\n              return;\n            }\n            try {\n              controller.dataAvailable.wait(500);\n            } catch (InterruptedException ie) {\n              if (!shouldStop) {\n                throw new RuntimeException(ie);\n              }\n            }\n          }\n          continue;\n        }\n\n        assert buffer != null;\n        try {\n          writeBuffer(buffer);\n        } finally {\n          entryBuffers.doneWriting(buffer);\n        }\n      }\n    }"
        ],
        [
            "WALSplitter::splitLogFile(FileStatus,CancelableProgressable)",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248 -\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256 -\n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320 -\n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333 -\n 334  \n 335  \n 336  \n 337  \n 338  \n 339 -\n 340 -\n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "  /**\n   * log splitting implementation, splits one log file.\n   * @param logfile should be an actual log file.\n   */\n  @VisibleForTesting\n  boolean splitLogFile(FileStatus logfile, CancelableProgressable reporter) throws IOException {\n    Preconditions.checkState(status == null);\n    Preconditions.checkArgument(logfile.isFile(),\n        \"passed in file status is for something other than a regular file.\");\n    boolean isCorrupted = false;\n    boolean skipErrors = conf.getBoolean(\"hbase.hlog.split.skip.errors\",\n      SPLIT_SKIP_ERRORS_DEFAULT);\n    int interval = conf.getInt(\"hbase.splitlog.report.interval.loglines\", 1024);\n    Path logPath = logfile.getPath();\n    boolean outputSinkStarted = false;\n    boolean progress_failed = false;\n    int editsCount = 0;\n    int editsSkipped = 0;\n\n    status = TaskMonitor.get().createStatus(\n          \"Splitting log file \" + logfile.getPath() + \"into a temporary staging area.\");\n    Reader logFileReader = null;\n    this.fileBeingSplit = logfile;\n    try {\n      long logLength = logfile.getLen();\n      LOG.info(\"Splitting WAL=\" + logPath + \", length=\" + logLength);\n      status.setStatus(\"Opening log file\");\n      if (reporter != null && !reporter.progress()) {\n        progress_failed = true;\n        return false;\n      }\n      logFileReader = getReader(logfile, skipErrors, reporter);\n      if (logFileReader == null) {\n        LOG.warn(\"Nothing to split in WAL=\" + logPath);\n        return true;\n      }\n      int numOpenedFilesBeforeReporting = conf.getInt(\"hbase.splitlog.report.openedfiles\", 3);\n      int numOpenedFilesLastCheck = 0;\n      outputSink.setReporter(reporter);\n      outputSink.startWriterThreads();\n      outputSinkStarted = true;\n      Entry entry;\n      Long lastFlushedSequenceId = -1L;\n      while ((entry = getNextLogLine(logFileReader, logPath, skipErrors)) != null) {\n        byte[] region = entry.getKey().getEncodedRegionName();\n        String encodedRegionNameAsStr = Bytes.toString(region);\n        lastFlushedSequenceId = lastFlushedSequenceIds.get(encodedRegionNameAsStr);\n        if (lastFlushedSequenceId == null) {\n          if (sequenceIdChecker != null) {\n            RegionStoreSequenceIds ids = sequenceIdChecker.getLastSequenceId(region);\n            Map<byte[], Long> maxSeqIdInStores = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n            for (StoreSequenceId storeSeqId : ids.getStoreSequenceIdList()) {\n              maxSeqIdInStores.put(storeSeqId.getFamilyName().toByteArray(),\n                storeSeqId.getSequenceId());\n            }\n            regionMaxSeqIdInStores.put(encodedRegionNameAsStr, maxSeqIdInStores);\n            lastFlushedSequenceId = ids.getLastFlushedSequenceId();\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DLS Last flushed sequenceid for \" + encodedRegionNameAsStr + \": \" +\n                  TextFormat.shortDebugString(ids));\n            }\n          }\n          if (lastFlushedSequenceId == null) {\n            lastFlushedSequenceId = -1L;\n          }\n          lastFlushedSequenceIds.put(encodedRegionNameAsStr, lastFlushedSequenceId);\n        }\n        if (lastFlushedSequenceId >= entry.getKey().getSequenceId()) {\n          editsSkipped++;\n          continue;\n        }\n        // Don't send Compaction/Close/Open region events to recovered edit type sinks.\n        if (entry.getEdit().isMetaEdit() && !outputSink.keepRegionEvent(entry)) {\n          editsSkipped++;\n          continue;\n        }\n        entryBuffers.appendEntry(entry);\n        editsCount++;\n        int moreWritersFromLastCheck = this.getNumOpenWriters() - numOpenedFilesLastCheck;\n        // If sufficient edits have passed, check if we should report progress.\n        if (editsCount % interval == 0\n            || moreWritersFromLastCheck > numOpenedFilesBeforeReporting) {\n          numOpenedFilesLastCheck = this.getNumOpenWriters();\n          String countsStr = (editsCount - (editsSkipped + outputSink.getSkippedEdits()))\n              + \" edits, skipped \" + editsSkipped + \" edits.\";\n          status.setStatus(\"Split \" + countsStr);\n          if (reporter != null && !reporter.progress()) {\n            progress_failed = true;\n            return false;\n          }\n        }\n      }\n    } catch (InterruptedException ie) {\n      IOException iie = new InterruptedIOException();\n      iie.initCause(ie);\n      throw iie;\n    } catch (CorruptedLogFileException e) {\n      LOG.warn(\"Could not parse, corrupted WAL=\" + logPath, e);\n      if (splitLogWorkerCoordination != null) {\n        // Some tests pass in a csm of null.\n        splitLogWorkerCoordination.markCorrupted(rootDir, logfile.getPath().getName(), fs);\n      } else {\n        // for tests only\n        ZKSplitLog.markCorrupted(rootDir, logfile.getPath().getName(), fs);\n      }\n      isCorrupted = true;\n    } catch (IOException e) {\n      e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;\n      throw e;\n    } finally {\n      LOG.debug(\"Finishing writing output logs and closing down.\");\n      try {\n        if (null != logFileReader) {\n          logFileReader.close();\n        }\n      } catch (IOException exception) {\n        LOG.warn(\"Could not close WAL reader: \" + exception.getMessage());\n        LOG.debug(\"exception details\", exception);\n      }\n      try {\n        if (outputSinkStarted) {\n          // Set progress_failed to true as the immediate following statement will reset its value\n          // when finishWritingAndClose() throws exception, progress_failed has the right value\n          progress_failed = true;\n          progress_failed = outputSink.finishWritingAndClose() == null;\n        }\n      } finally {\n        String msg =\n            \"Processed \" + editsCount + \" edits across \" + outputSink.getNumberOfRecoveredRegions()\n                + \" regions; edits skipped=\" + editsSkipped + \"; log file=\" + logPath +\n                \", length=\" + logfile.getLen() + // See if length got updated post lease recovery\n                \", corrupted=\" + isCorrupted + \", progress failed=\" + progress_failed;\n        LOG.info(msg);\n        status.markComplete(msg);\n      }\n    }\n    return !progress_failed;\n  }",
            " 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 +\n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 +\n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336 +\n 337  \n 338  \n 339  \n 340  \n 341  \n 342 +\n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  ",
            "  /**\n   * log splitting implementation, splits one log file.\n   * @param logfile should be an actual log file.\n   */\n  @VisibleForTesting\n  boolean splitLogFile(FileStatus logfile, CancelableProgressable reporter) throws IOException {\n    Preconditions.checkState(status == null);\n    Preconditions.checkArgument(logfile.isFile(),\n        \"passed in file status is for something other than a regular file.\");\n    boolean isCorrupted = false;\n    boolean skipErrors = conf.getBoolean(\"hbase.hlog.split.skip.errors\",\n      SPLIT_SKIP_ERRORS_DEFAULT);\n    int interval = conf.getInt(\"hbase.splitlog.report.interval.loglines\", 1024);\n    Path logPath = logfile.getPath();\n    boolean outputSinkStarted = false;\n    boolean progress_failed = false;\n    int editsCount = 0;\n    int editsSkipped = 0;\n\n    status = TaskMonitor.get().createStatus(\n          \"Splitting log file \" + logfile.getPath() + \"into a temporary staging area.\");\n    Reader logFileReader = null;\n    this.fileBeingSplit = logfile;\n    try {\n      long logLength = logfile.getLen();\n      LOG.info(\"Splitting WAL={}, length={}\", logPath, logLength);\n      status.setStatus(\"Opening log file\");\n      if (reporter != null && !reporter.progress()) {\n        progress_failed = true;\n        return false;\n      }\n      logFileReader = getReader(logfile, skipErrors, reporter);\n      if (logFileReader == null) {\n        LOG.warn(\"Nothing to split in WAL={}\", logPath);\n        return true;\n      }\n      int numOpenedFilesBeforeReporting = conf.getInt(\"hbase.splitlog.report.openedfiles\", 3);\n      int numOpenedFilesLastCheck = 0;\n      outputSink.setReporter(reporter);\n      outputSink.startWriterThreads();\n      outputSinkStarted = true;\n      Entry entry;\n      Long lastFlushedSequenceId = -1L;\n      while ((entry = getNextLogLine(logFileReader, logPath, skipErrors)) != null) {\n        byte[] region = entry.getKey().getEncodedRegionName();\n        String encodedRegionNameAsStr = Bytes.toString(region);\n        lastFlushedSequenceId = lastFlushedSequenceIds.get(encodedRegionNameAsStr);\n        if (lastFlushedSequenceId == null) {\n          if (sequenceIdChecker != null) {\n            RegionStoreSequenceIds ids = sequenceIdChecker.getLastSequenceId(region);\n            Map<byte[], Long> maxSeqIdInStores = new TreeMap<>(Bytes.BYTES_COMPARATOR);\n            for (StoreSequenceId storeSeqId : ids.getStoreSequenceIdList()) {\n              maxSeqIdInStores.put(storeSeqId.getFamilyName().toByteArray(),\n                storeSeqId.getSequenceId());\n            }\n            regionMaxSeqIdInStores.put(encodedRegionNameAsStr, maxSeqIdInStores);\n            lastFlushedSequenceId = ids.getLastFlushedSequenceId();\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DLS Last flushed sequenceid for \" + encodedRegionNameAsStr + \": \" +\n                  TextFormat.shortDebugString(ids));\n            }\n          }\n          if (lastFlushedSequenceId == null) {\n            lastFlushedSequenceId = -1L;\n          }\n          lastFlushedSequenceIds.put(encodedRegionNameAsStr, lastFlushedSequenceId);\n        }\n        if (lastFlushedSequenceId >= entry.getKey().getSequenceId()) {\n          editsSkipped++;\n          continue;\n        }\n        // Don't send Compaction/Close/Open region events to recovered edit type sinks.\n        if (entry.getEdit().isMetaEdit() && !outputSink.keepRegionEvent(entry)) {\n          editsSkipped++;\n          continue;\n        }\n        entryBuffers.appendEntry(entry);\n        editsCount++;\n        int moreWritersFromLastCheck = this.getNumOpenWriters() - numOpenedFilesLastCheck;\n        // If sufficient edits have passed, check if we should report progress.\n        if (editsCount % interval == 0\n            || moreWritersFromLastCheck > numOpenedFilesBeforeReporting) {\n          numOpenedFilesLastCheck = this.getNumOpenWriters();\n          String countsStr = (editsCount - (editsSkipped + outputSink.getSkippedEdits()))\n              + \" edits, skipped \" + editsSkipped + \" edits.\";\n          status.setStatus(\"Split \" + countsStr);\n          if (reporter != null && !reporter.progress()) {\n            progress_failed = true;\n            return false;\n          }\n        }\n      }\n    } catch (InterruptedException ie) {\n      IOException iie = new InterruptedIOException();\n      iie.initCause(ie);\n      throw iie;\n    } catch (CorruptedLogFileException e) {\n      LOG.warn(\"Could not parse, corrupted WAL={}\", logPath, e);\n      if (splitLogWorkerCoordination != null) {\n        // Some tests pass in a csm of null.\n        splitLogWorkerCoordination.markCorrupted(rootDir, logfile.getPath().getName(), fs);\n      } else {\n        // for tests only\n        ZKSplitLog.markCorrupted(rootDir, logfile.getPath().getName(), fs);\n      }\n      isCorrupted = true;\n    } catch (IOException e) {\n      e = e instanceof RemoteException ? ((RemoteException) e).unwrapRemoteException() : e;\n      throw e;\n    } finally {\n      LOG.debug(\"Finishing writing output logs and closing down\");\n      try {\n        if (null != logFileReader) {\n          logFileReader.close();\n        }\n      } catch (IOException exception) {\n        LOG.warn(\"Could not close WAL reader\", exception);\n      }\n      try {\n        if (outputSinkStarted) {\n          // Set progress_failed to true as the immediate following statement will reset its value\n          // when finishWritingAndClose() throws exception, progress_failed has the right value\n          progress_failed = true;\n          progress_failed = outputSink.finishWritingAndClose() == null;\n        }\n      } finally {\n        String msg =\n            \"Processed \" + editsCount + \" edits across \" + outputSink.getNumberOfRecoveredRegions()\n                + \" regions; edits skipped=\" + editsSkipped + \"; log file=\" + logPath +\n                \", length=\" + logfile.getLen() + // See if length got updated post lease recovery\n                \", corrupted=\" + isCorrupted + \", progress failed=\" + progress_failed;\n        LOG.info(msg);\n        status.markComplete(msg);\n      }\n    }\n    return !progress_failed;\n  }"
        ],
        [
            "WALSplitter::LogRecoveredEditsOutputSink::deleteOneWithFewerEntries(WriterAndPath,Path)",
            "1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  \n1278  \n1279 -\n1280 -\n1281 -\n1282 -\n1283 -\n1284  \n1285  \n1286  \n1287  \n1288  \n1289  \n1290 -\n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297 -\n1298  \n1299  \n1300  \n1301  ",
            "    private void deleteOneWithFewerEntries(WriterAndPath wap, Path dst) throws IOException {\n      long dstMinLogSeqNum = -1L;\n      try (WAL.Reader reader = walFactory.createReader(fs, dst)) {\n        WAL.Entry entry = reader.next();\n        if (entry != null) {\n          dstMinLogSeqNum = entry.getKey().getSequenceId();\n        }\n      } catch (EOFException e) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\n            \"Got EOF when reading first WAL entry from \" + dst + \", an empty or broken WAL file?\",\n            e);\n        }\n      }\n      if (wap.minLogSeqNum < dstMinLogSeqNum) {\n        LOG.warn(\"Found existing old edits file. It could be the result of a previous failed\"\n            + \" split attempt or we have duplicated wal entries. Deleting \" + dst + \", length=\"\n            + fs.getFileStatus(dst).getLen());\n        if (!fs.delete(dst, false)) {\n          LOG.warn(\"Failed deleting of old \" + dst);\n          throw new IOException(\"Failed deleting of old \" + dst);\n        }\n      } else {\n        LOG.warn(\"Found existing old edits file and we have less entries. Deleting \" + wap.p\n            + \", length=\" + fs.getFileStatus(wap.p).getLen());\n        if (!fs.delete(wap.p, false)) {\n          LOG.warn(\"Failed deleting of \" + wap.p);\n          throw new IOException(\"Failed deleting of \" + wap.p);\n        }\n      }\n    }",
            "1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272 +\n1273 +\n1274  \n1275  \n1276  \n1277  \n1278  \n1279  \n1280 +\n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287 +\n1288  \n1289  \n1290  \n1291  ",
            "    private void deleteOneWithFewerEntries(WriterAndPath wap, Path dst) throws IOException {\n      long dstMinLogSeqNum = -1L;\n      try (WAL.Reader reader = walFactory.createReader(fs, dst)) {\n        WAL.Entry entry = reader.next();\n        if (entry != null) {\n          dstMinLogSeqNum = entry.getKey().getSequenceId();\n        }\n      } catch (EOFException e) {\n        LOG.debug(\"Got EOF when reading first WAL entry from {}, an empty or broken WAL file?\",\n            dst, e);\n      }\n      if (wap.minLogSeqNum < dstMinLogSeqNum) {\n        LOG.warn(\"Found existing old edits file. It could be the result of a previous failed\"\n            + \" split attempt or we have duplicated wal entries. Deleting \" + dst + \", length=\"\n            + fs.getFileStatus(dst).getLen());\n        if (!fs.delete(dst, false)) {\n          LOG.warn(\"Failed deleting of old {}\", dst);\n          throw new IOException(\"Failed deleting of old \" + dst);\n        }\n      } else {\n        LOG.warn(\"Found existing old edits file and we have less entries. Deleting \" + wap.p\n            + \", length=\" + fs.getFileStatus(wap.p).getLen());\n        if (!fs.delete(wap.p, false)) {\n          LOG.warn(\"Failed deleting of {}\", wap.p);\n          throw new IOException(\"Failed deleting of \" + wap.p);\n        }\n      }\n    }"
        ],
        [
            "WALSplitter::writeRegionSequenceIdFile(FileSystem,Path,long,long)",
            " 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 -\n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675 -\n 676 -\n 677 -\n 678 -\n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686 -\n 687 -\n 688  \n 689 -\n 690  \n 691  \n 692  \n 693  ",
            "  /**\n   * Create a file with name as region open sequence id\n   * @param fs\n   * @param regiondir\n   * @param newSeqId\n   * @param safetyBumper\n   * @return long new sequence Id value\n   * @throws IOException\n   */\n  public static long writeRegionSequenceIdFile(final FileSystem fs, final Path regiondir,\n      long newSeqId, long safetyBumper) throws IOException {\n    // TODO: Why are we using a method in here as part of our normal region open where\n    // there is no splitting involved? Fix. St.Ack 01/20/2017.\n    Path editsdir = WALSplitter.getRegionDirRecoveredEditsDir(regiondir);\n    long maxSeqId = 0;\n    FileStatus[] files = null;\n    if (fs.exists(editsdir)) {\n      files = FSUtils.listStatus(fs, editsdir, new PathFilter() {\n        @Override\n        public boolean accept(Path p) {\n          return isSequenceIdFile(p);\n        }\n      });\n      if (files != null) {\n        for (FileStatus status : files) {\n          String fileName = status.getPath().getName();\n          try {\n            long tmpSeqId =\n                Long.parseLong(fileName.substring(0, fileName.length()\n                - SEQUENCE_ID_FILE_SUFFIX_LENGTH));\n            maxSeqId = Math.max(tmpSeqId, maxSeqId);\n          } catch (NumberFormatException ex) {\n            LOG.warn(\"Invalid SeqId File Name=\" + fileName);\n          }\n        }\n      }\n    }\n    if (maxSeqId > newSeqId) {\n      newSeqId = maxSeqId;\n    }\n    newSeqId += safetyBumper; // bump up SeqId\n\n    // write a new seqId file\n    Path newSeqIdFile = new Path(editsdir, newSeqId + SEQUENCE_ID_FILE_SUFFIX);\n    if (newSeqId != maxSeqId) {\n      try {\n        if (!fs.createNewFile(newSeqIdFile) && !fs.exists(newSeqIdFile)) {\n          throw new IOException(\"Failed to create SeqId file:\" + newSeqIdFile);\n        }\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Wrote file=\" + newSeqIdFile + \", newSeqId=\" + newSeqId\n              + \", maxSeqId=\" + maxSeqId);\n        }\n      } catch (FileAlreadyExistsException ignored) {\n        // latest hdfs throws this exception. it's all right if newSeqIdFile already exists\n      }\n    }\n    // remove old ones\n    if (files != null) {\n      for (FileStatus status : files) {\n        if (newSeqIdFile.equals(status.getPath())) {\n          continue;\n        }\n        fs.delete(status.getPath(), false);\n      }\n    }\n    return newSeqId;\n  }",
            " 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 +\n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675 +\n 676 +\n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684 +\n 685 +\n 686  \n 687  \n 688  \n 689  \n 690  ",
            "  /**\n   * Create a file with name as region open sequence id\n   * @param fs\n   * @param regiondir\n   * @param newSeqId\n   * @param safetyBumper\n   * @return long new sequence Id value\n   * @throws IOException\n   */\n  public static long writeRegionSequenceIdFile(final FileSystem fs, final Path regiondir,\n      long newSeqId, long safetyBumper) throws IOException {\n    // TODO: Why are we using a method in here as part of our normal region open where\n    // there is no splitting involved? Fix. St.Ack 01/20/2017.\n    Path editsdir = WALSplitter.getRegionDirRecoveredEditsDir(regiondir);\n    long maxSeqId = 0;\n    FileStatus[] files = null;\n    if (fs.exists(editsdir)) {\n      files = FSUtils.listStatus(fs, editsdir, new PathFilter() {\n        @Override\n        public boolean accept(Path p) {\n          return isSequenceIdFile(p);\n        }\n      });\n      if (files != null) {\n        for (FileStatus status : files) {\n          String fileName = status.getPath().getName();\n          try {\n            long tmpSeqId =\n                Long.parseLong(fileName.substring(0, fileName.length()\n                - SEQUENCE_ID_FILE_SUFFIX_LENGTH));\n            maxSeqId = Math.max(tmpSeqId, maxSeqId);\n          } catch (NumberFormatException ex) {\n            LOG.warn(\"Invalid SeqId File Name={}\", fileName);\n          }\n        }\n      }\n    }\n    if (maxSeqId > newSeqId) {\n      newSeqId = maxSeqId;\n    }\n    newSeqId += safetyBumper; // bump up SeqId\n\n    // write a new seqId file\n    Path newSeqIdFile = new Path(editsdir, newSeqId + SEQUENCE_ID_FILE_SUFFIX);\n    if (newSeqId != maxSeqId) {\n      try {\n        if (!fs.createNewFile(newSeqIdFile) && !fs.exists(newSeqIdFile)) {\n          throw new IOException(\"Failed to create SeqId file:\" + newSeqIdFile);\n        }\n        LOG.debug(\"Wrote file={}, newSeqId={}, maxSeqId={}\", newSeqIdFile,\n            newSeqId, maxSeqId);\n      } catch (FileAlreadyExistsException ignored) {\n        // latest hdfs throws this exception. it's all right if newSeqIdFile already exists\n      }\n    }\n    // remove old ones\n    if (files != null) {\n      for (FileStatus status : files) {\n        if (!newSeqIdFile.equals(status.getPath())) {\n          fs.delete(status.getPath(), false);\n        }\n      }\n    }\n    return newSeqId;\n  }"
        ]
    ],
    "ac6b998afe033cbb6a307d249c8e18bb97d54c9f": [
        [
            "IntegrationTestRSGroup::beforeMethod()",
            "  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  ",
            "  @Before\n  public void beforeMethod() throws Exception {\n    if(!initialized) {\n      LOG.info(\"Setting up IntegrationTestRSGroup\");\n      LOG.info(\"Initializing cluster with \" + NUM_SLAVES_BASE + \" servers\");\n      TEST_UTIL = new IntegrationTestingUtility();\n      ((IntegrationTestingUtility)TEST_UTIL).initializeCluster(NUM_SLAVES_BASE);\n      //set shared configs\n      admin = TEST_UTIL.getAdmin();\n      cluster = TEST_UTIL.getHBaseClusterInterface();\n      rsGroupAdmin = new VerifyingRSGroupAdminClient(new RSGroupAdminClient(TEST_UTIL.getConnection()),\n          TEST_UTIL.getConfiguration());\n      LOG.info(\"Done initializing cluster\");\n      initialized = true;\n      //cluster may not be clean\n      //cleanup when initializing\n      afterMethod();\n    }\n  }",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48 +\n  49 +\n  50 +\n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "  @Before\n  public void beforeMethod() throws Exception {\n    if(!initialized) {\n      LOG.info(\"Setting up IntegrationTestRSGroup\");\n      LOG.info(\"Initializing cluster with \" + NUM_SLAVES_BASE + \" servers\");\n      TEST_UTIL = new IntegrationTestingUtility();\n      TEST_UTIL.getConfiguration().set(HConstants.HBASE_MASTER_LOADBALANCER_CLASS,\n        RSGroupBasedLoadBalancer.class.getName());\n      TEST_UTIL.getConfiguration().set(CoprocessorHost.MASTER_COPROCESSOR_CONF_KEY,\n        RSGroupAdminEndpoint.class.getName());\n      ((IntegrationTestingUtility)TEST_UTIL).initializeCluster(NUM_SLAVES_BASE);\n      //set shared configs\n      admin = TEST_UTIL.getAdmin();\n      cluster = TEST_UTIL.getHBaseClusterInterface();\n      rsGroupAdmin = new VerifyingRSGroupAdminClient(new RSGroupAdminClient(TEST_UTIL.getConnection()),\n          TEST_UTIL.getConfiguration());\n      LOG.info(\"Done initializing cluster\");\n      initialized = true;\n      //cluster may not be clean\n      //cleanup when initializing\n      afterMethod();\n    }\n  }"
        ]
    ],
    "7fe4aa6fe44ce5c43642606af87c4cc4c328fbaa": [
        [
            "ActiveMasterManager::stop()",
            " 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280 -\n 281  \n 282  ",
            "  public void stop() {\n    try {\n      synchronized (clusterHasActiveMaster) {\n        // Master is already stopped, wake up the manager\n        // thread so that it can shutdown soon.\n        clusterHasActiveMaster.notifyAll();\n      }\n      // If our address is in ZK, delete it on our way out\n      ServerName activeMaster = null;\n      try {\n        activeMaster = MasterAddressTracker.getMasterAddress(this.watcher);\n      } catch (IOException e) {\n        LOG.warn(\"Failed get of master address: \" + e.toString());\n      }\n      if (activeMaster != null &&  activeMaster.equals(this.sn)) {\n        ZKUtil.deleteNode(watcher, watcher.znodePaths.masterAddressZNode);\n        // We may have failed to delete the znode at the previous step, but\n        //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n        ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n      }\n    } catch (KeeperException e) {\n      LOG.error(this.watcher.prefix(\"Error deleting our own master address node\"), e);\n    }\n  }",
            " 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280 +\n 281 +\n 282  \n 283  ",
            "  public void stop() {\n    try {\n      synchronized (clusterHasActiveMaster) {\n        // Master is already stopped, wake up the manager\n        // thread so that it can shutdown soon.\n        clusterHasActiveMaster.notifyAll();\n      }\n      // If our address is in ZK, delete it on our way out\n      ServerName activeMaster = null;\n      try {\n        activeMaster = MasterAddressTracker.getMasterAddress(this.watcher);\n      } catch (IOException e) {\n        LOG.warn(\"Failed get of master address: \" + e.toString());\n      }\n      if (activeMaster != null &&  activeMaster.equals(this.sn)) {\n        ZKUtil.deleteNode(watcher, watcher.znodePaths.masterAddressZNode);\n        // We may have failed to delete the znode at the previous step, but\n        //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n        ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n      }\n    } catch (KeeperException e) {\n      LOG.debug(this.watcher.prefix(\"Failed delete of our master address node; \" +\n          e.getMessage()));\n    }\n  }"
        ],
        [
            "ProcedureExecutor::start(int,boolean)",
            " 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513 -\n 514  \n 515  \n 516 -\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  ",
            "  /**\n   * Start the procedure executor.\n   * It calls ProcedureStore.recoverLease() and ProcedureStore.load() to\n   * recover the lease, and ensure a single executor, and start the procedure\n   * replay to resume and recover the previous pending and in-progress perocedures.\n   *\n   * @param numThreads number of threads available for procedure execution.\n   * @param abortOnCorruption true if you want to abort your service in case\n   *          a corrupted procedure is found on replay. otherwise false.\n   */\n  public void start(int numThreads, boolean abortOnCorruption) throws IOException {\n    if (running.getAndSet(true)) {\n      LOG.warn(\"Already running\");\n      return;\n    }\n\n    // We have numThreads executor + one timer thread used for timing out\n    // procedures and triggering periodic procedures.\n    this.corePoolSize = numThreads;\n    LOG.info(\"Starting ProcedureExecutor Worker threads (ProcExecWrkr)=\" + corePoolSize);\n\n    // Create the Thread Group for the executors\n    threadGroup = new ThreadGroup(\"ProcExecThrdGrp\");\n\n    // Create the timeout executor\n    timeoutExecutor = new TimeoutExecutorThread(threadGroup);\n\n    // Create the workers\n    workerId.set(0);\n    workerThreads = new CopyOnWriteArrayList<>();\n    for (int i = 0; i < corePoolSize; ++i) {\n      workerThreads.add(new WorkerThread(threadGroup));\n    }\n\n    long st, et;\n\n    // Acquire the store lease.\n    st = EnvironmentEdgeManager.currentTime();\n    store.recoverLease();\n    et = EnvironmentEdgeManager.currentTime();\n    LOG.info(String.format(\"Recover store (%s) lease: %s\",\n      store.getClass().getSimpleName(), StringUtils.humanTimeDiff(et - st)));\n\n    // start the procedure scheduler\n    scheduler.start();\n\n    // TODO: Split in two steps.\n    // TODO: Handle corrupted procedures (currently just a warn)\n    // The first one will make sure that we have the latest id,\n    // so we can start the threads and accept new procedures.\n    // The second step will do the actual load of old procedures.\n    st = EnvironmentEdgeManager.currentTime();\n    load(abortOnCorruption);\n    et = EnvironmentEdgeManager.currentTime();\n    LOG.info(String.format(\"Load store (%s): %s\",\n      store.getClass().getSimpleName(), StringUtils.humanTimeDiff(et - st)));\n\n    // Start the executors. Here we must have the lastProcId set.\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Start workers \" + workerThreads.size());\n    }\n    timeoutExecutor.start();\n    for (WorkerThread worker: workerThreads) {\n      worker.start();\n    }\n\n    // Internal chores\n    timeoutExecutor.add(new WorkerMonitor());\n\n    // Add completed cleaner chore\n    addChore(new CompletedProcedureCleaner(conf, store, completed, nonceKeysToProcIdsMap));\n  }",
            " 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513 +\n 514  \n 515  \n 516 +\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  ",
            "  /**\n   * Start the procedure executor.\n   * It calls ProcedureStore.recoverLease() and ProcedureStore.load() to\n   * recover the lease, and ensure a single executor, and start the procedure\n   * replay to resume and recover the previous pending and in-progress perocedures.\n   *\n   * @param numThreads number of threads available for procedure execution.\n   * @param abortOnCorruption true if you want to abort your service in case\n   *          a corrupted procedure is found on replay. otherwise false.\n   */\n  public void start(int numThreads, boolean abortOnCorruption) throws IOException {\n    if (running.getAndSet(true)) {\n      LOG.warn(\"Already running\");\n      return;\n    }\n\n    // We have numThreads executor + one timer thread used for timing out\n    // procedures and triggering periodic procedures.\n    this.corePoolSize = numThreads;\n    LOG.info(\"Starting ProcedureExecutor Worker threads (ProcedureExecutorWorker)=\" + corePoolSize);\n\n    // Create the Thread Group for the executors\n    threadGroup = new ThreadGroup(\"ProcedureExecutorWorkerGroup\");\n\n    // Create the timeout executor\n    timeoutExecutor = new TimeoutExecutorThread(threadGroup);\n\n    // Create the workers\n    workerId.set(0);\n    workerThreads = new CopyOnWriteArrayList<>();\n    for (int i = 0; i < corePoolSize; ++i) {\n      workerThreads.add(new WorkerThread(threadGroup));\n    }\n\n    long st, et;\n\n    // Acquire the store lease.\n    st = EnvironmentEdgeManager.currentTime();\n    store.recoverLease();\n    et = EnvironmentEdgeManager.currentTime();\n    LOG.info(String.format(\"Recover store (%s) lease: %s\",\n      store.getClass().getSimpleName(), StringUtils.humanTimeDiff(et - st)));\n\n    // start the procedure scheduler\n    scheduler.start();\n\n    // TODO: Split in two steps.\n    // TODO: Handle corrupted procedures (currently just a warn)\n    // The first one will make sure that we have the latest id,\n    // so we can start the threads and accept new procedures.\n    // The second step will do the actual load of old procedures.\n    st = EnvironmentEdgeManager.currentTime();\n    load(abortOnCorruption);\n    et = EnvironmentEdgeManager.currentTime();\n    LOG.info(String.format(\"Load store (%s): %s\",\n      store.getClass().getSimpleName(), StringUtils.humanTimeDiff(et - st)));\n\n    // Start the executors. Here we must have the lastProcId set.\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Start workers \" + workerThreads.size());\n    }\n    timeoutExecutor.start();\n    for (WorkerThread worker: workerThreads) {\n      worker.start();\n    }\n\n    // Internal chores\n    timeoutExecutor.add(new WorkerMonitor());\n\n    // Add completed cleaner chore\n    addChore(new CompletedProcedureCleaner(conf, store, completed, nonceKeysToProcIdsMap));\n  }"
        ],
        [
            "ProcedureExecutor::WorkerThread::WorkerThread(ThreadGroup)",
            "1711  \n1712 -\n1713  \n1714  ",
            "    public WorkerThread(final ThreadGroup group) {\n      super(group, \"ProcExecWrkr-\" + workerId.incrementAndGet());\n      setDaemon(true);\n    }",
            "1711  \n1712 +\n1713  \n1714  ",
            "    public WorkerThread(final ThreadGroup group) {\n      super(group, \"ProcedureExecutorWorker-\" + workerId.incrementAndGet());\n      setDaemon(true);\n    }"
        ],
        [
            "ProcedureExecutor::StoppableThread::awaitTermination()",
            "1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909 -\n1910  \n1911  \n1912  \n1913  \n1914  \n1915  ",
            "    public void awaitTermination() {\n      try {\n        final long startTime = EnvironmentEdgeManager.currentTime();\n        for (int i = 0; isAlive(); ++i) {\n          sendStopSignal();\n          join(250);\n          if (i > 0 && (i % 8) == 0) {\n            LOG.warn(\"Waiting termination of thread \" + getName() + \", \" +\n              StringUtils.humanTimeDiff(EnvironmentEdgeManager.currentTime() - startTime));\n          }\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(getName() + \" join wait got interrupted\", e);\n      }\n    }",
            "1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907 +\n1908  \n1909  \n1910 +\n1911 +\n1912 +\n1913  \n1914  \n1915  \n1916  \n1917  \n1918  ",
            "    public void awaitTermination() {\n      try {\n        final long startTime = EnvironmentEdgeManager.currentTime();\n        for (int i = 0; isAlive(); ++i) {\n          sendStopSignal();\n          join(250);\n          // Log every two seconds; send interrupt too.\n          if (i > 0 && (i % 8) == 0) {\n            LOG.warn(\"Waiting termination of thread \" + getName() + \", \" +\n              StringUtils.humanTimeDiff(EnvironmentEdgeManager.currentTime() - startTime) +\n            \"; sending interrupt\");\n            interrupt();\n          }\n        }\n      } catch (InterruptedException e) {\n        LOG.warn(getName() + \" join wait got interrupted\", e);\n      }\n    }"
        ],
        [
            "ProcedureExecutor::WorkerThread::run()",
            "1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755 -\n1756  \n1757  \n1758  ",
            "    @Override\n    public void run() {\n      long lastUpdate = EnvironmentEdgeManager.currentTime();\n      try {\n        while (isRunning() && keepAlive(lastUpdate)) {\n          this.activeProcedure = scheduler.poll(keepAliveTime, TimeUnit.MILLISECONDS);\n          if (this.activeProcedure == null) continue;\n          int activeCount = activeExecutorCount.incrementAndGet();\n          int runningCount = store.setRunningProcedureCount(activeCount);\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Execute pid=\" + this.activeProcedure.getProcId() +\n                \" runningCount=\" + runningCount + \", activeCount=\" + activeCount);\n          }\n          executionStartTime.set(EnvironmentEdgeManager.currentTime());\n          try {\n            executeProcedure(this.activeProcedure);\n          } catch (AssertionError e) {\n            LOG.info(\"ASSERT pid=\" + this.activeProcedure.getProcId(), e);\n            throw e;\n          } finally {\n            activeCount = activeExecutorCount.decrementAndGet();\n            runningCount = store.setRunningProcedureCount(activeCount);\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Halt pid=\" + this.activeProcedure.getProcId() +\n                  \" runningCount=\" + runningCount + \", activeCount=\" + activeCount);\n            }\n            this.activeProcedure = null;\n            lastUpdate = EnvironmentEdgeManager.currentTime();\n            executionStartTime.set(Long.MAX_VALUE);\n          }\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Worker terminating UNNATURALLY \" + this.activeProcedure, t);\n      } finally {\n        LOG.debug(\"Worker terminated.\");\n      }\n      workerThreads.remove(this);\n    }",
            "1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730  \n1731  \n1732  \n1733  \n1734  \n1735  \n1736  \n1737  \n1738  \n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755 +\n1756  \n1757  \n1758  ",
            "    @Override\n    public void run() {\n      long lastUpdate = EnvironmentEdgeManager.currentTime();\n      try {\n        while (isRunning() && keepAlive(lastUpdate)) {\n          this.activeProcedure = scheduler.poll(keepAliveTime, TimeUnit.MILLISECONDS);\n          if (this.activeProcedure == null) continue;\n          int activeCount = activeExecutorCount.incrementAndGet();\n          int runningCount = store.setRunningProcedureCount(activeCount);\n          if (LOG.isTraceEnabled()) {\n            LOG.trace(\"Execute pid=\" + this.activeProcedure.getProcId() +\n                \" runningCount=\" + runningCount + \", activeCount=\" + activeCount);\n          }\n          executionStartTime.set(EnvironmentEdgeManager.currentTime());\n          try {\n            executeProcedure(this.activeProcedure);\n          } catch (AssertionError e) {\n            LOG.info(\"ASSERT pid=\" + this.activeProcedure.getProcId(), e);\n            throw e;\n          } finally {\n            activeCount = activeExecutorCount.decrementAndGet();\n            runningCount = store.setRunningProcedureCount(activeCount);\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Halt pid=\" + this.activeProcedure.getProcId() +\n                  \" runningCount=\" + runningCount + \", activeCount=\" + activeCount);\n            }\n            this.activeProcedure = null;\n            lastUpdate = EnvironmentEdgeManager.currentTime();\n            executionStartTime.set(Long.MAX_VALUE);\n          }\n        }\n      } catch (Throwable t) {\n        LOG.warn(\"Worker terminating UNNATURALLY \" + this.activeProcedure, t);\n      } finally {\n        LOG.trace(\"Worker terminated.\");\n      }\n      workerThreads.remove(this);\n    }"
        ],
        [
            "HMaster::stopProcedureExecutor()",
            "1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  ",
            "  private void stopProcedureExecutor() {\n    if (procedureExecutor != null) {\n      configurationManager.deregisterObserver(procedureExecutor.getEnvironment());\n      procedureExecutor.getEnvironment().getRemoteDispatcher().stop();\n      procedureExecutor.stop();\n      procedureExecutor = null;\n    }\n\n    if (procedureStore != null) {\n      procedureStore.stop(isAborted());\n      procedureStore = null;\n    }\n  }",
            "1235  \n1236  \n1237  \n1238  \n1239  \n1240 +\n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  ",
            "  private void stopProcedureExecutor() {\n    if (procedureExecutor != null) {\n      configurationManager.deregisterObserver(procedureExecutor.getEnvironment());\n      procedureExecutor.getEnvironment().getRemoteDispatcher().stop();\n      procedureExecutor.stop();\n      procedureExecutor.join();\n      procedureExecutor = null;\n    }\n\n    if (procedureStore != null) {\n      procedureStore.stop(isAborted());\n      procedureStore = null;\n    }\n  }"
        ],
        [
            "ProcedureExecutor::join()",
            " 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595 -\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  ",
            "  @VisibleForTesting\n  public void join() {\n    assert !isRunning() : \"expected not running\";\n\n    // stop the timeout executor\n    timeoutExecutor.awaitTermination();\n    timeoutExecutor = null;\n\n    // stop the worker threads\n    for (WorkerThread worker: workerThreads) {\n      worker.awaitTermination();\n    }\n    workerThreads = null;\n\n    // Destroy the Thread Group for the executors\n    try {\n      threadGroup.destroy();\n    } catch (IllegalThreadStateException e) {\n      LOG.error(\"Thread group \" + threadGroup + \" contains running threads\");\n      threadGroup.list();\n    } finally {\n      threadGroup = null;\n    }\n\n    // reset the in-memory state for testing\n    completed.clear();\n    rollbackStack.clear();\n    procedures.clear();\n    nonceKeysToProcIdsMap.clear();\n    scheduler.clear();\n    lastProcId.set(-1);\n  }",
            " 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595 +\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  ",
            "  @VisibleForTesting\n  public void join() {\n    assert !isRunning() : \"expected not running\";\n\n    // stop the timeout executor\n    timeoutExecutor.awaitTermination();\n    timeoutExecutor = null;\n\n    // stop the worker threads\n    for (WorkerThread worker: workerThreads) {\n      worker.awaitTermination();\n    }\n    workerThreads = null;\n\n    // Destroy the Thread Group for the executors\n    try {\n      threadGroup.destroy();\n    } catch (IllegalThreadStateException e) {\n      LOG.error(\"ThreadGroup \" + threadGroup + \" contains running threads; \" + e.getMessage());\n      threadGroup.list();\n    } finally {\n      threadGroup = null;\n    }\n\n    // reset the in-memory state for testing\n    completed.clear();\n    rollbackStack.clear();\n    procedures.clear();\n    nonceKeysToProcIdsMap.clear();\n    scheduler.clear();\n    lastProcId.set(-1);\n  }"
        ]
    ],
    "f5dbdf0dab731a986d9aea2ad3dfdb400f1ba46c": [
        [
            "HBaseTestingUtility::HBaseTestingUtility(Configuration)",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  ",
            "  /**\n   * <p>Create an HBaseTestingUtility using a given configuration.\n   *\n   * <p>Initially, all tmp files are written to a local test data directory.\n   * Once {@link #startMiniDFSCluster} is called, either directly or via\n   * {@link #startMiniCluster()}, tmp data will be written to the DFS directory instead.\n   *\n   * <p>Previously, there was a distinction between the type of utility returned by\n   * {@link #createLocalHTU()} and this constructor; this is no longer the case. All\n   * HBaseTestingUtility objects will behave as local until a DFS cluster is started,\n   * at which point they will switch to using mini DFS for storage.\n   *\n   * @param conf The configuration to use for further operations\n   */\n  public HBaseTestingUtility(@Nullable Configuration conf) {\n    super(conf);\n\n    // a hbase checksum verification failure will cause unit tests to fail\n    ChecksumUtil.generateExceptionForChecksumFailureForTest(true);\n\n    // if conf is provided, prevent contention for ports if other hbase thread(s) are running\n    if (conf != null) {\n      if (conf.getInt(HConstants.MASTER_INFO_PORT, HConstants.DEFAULT_MASTER_INFOPORT)\n              == HConstants.DEFAULT_MASTER_INFOPORT) {\n        conf.setInt(HConstants.MASTER_INFO_PORT, -1);\n        LOG.debug(\"Config property {} changed to -1\", HConstants.MASTER_INFO_PORT);\n      }\n      if (conf.getInt(HConstants.REGIONSERVER_PORT, HConstants.DEFAULT_REGIONSERVER_PORT)\n              == HConstants.DEFAULT_REGIONSERVER_PORT) {\n        conf.setInt(HConstants.REGIONSERVER_PORT, -1);\n        LOG.debug(\"Config property {} changed to -1\", HConstants.REGIONSERVER_PORT);\n      }\n    }\n\n    // Every cluster is a local cluster until we start DFS\n    // Note that conf could be null, but this.conf will not be\n    String dataTestDir = getDataTestDir().toString();\n    this.conf.set(\"fs.defaultFS\",\"file:///\");\n    this.conf.set(HConstants.HBASE_DIR, \"file://\" + dataTestDir);\n    LOG.debug(\"Setting {} to {}\", HConstants.HBASE_DIR, dataTestDir);\n    this.conf.setBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE,false);\n  }",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349 +\n 350 +\n 351 +\n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  ",
            "  /**\n   * <p>Create an HBaseTestingUtility using a given configuration.\n   *\n   * <p>Initially, all tmp files are written to a local test data directory.\n   * Once {@link #startMiniDFSCluster} is called, either directly or via\n   * {@link #startMiniCluster()}, tmp data will be written to the DFS directory instead.\n   *\n   * <p>Previously, there was a distinction between the type of utility returned by\n   * {@link #createLocalHTU()} and this constructor; this is no longer the case. All\n   * HBaseTestingUtility objects will behave as local until a DFS cluster is started,\n   * at which point they will switch to using mini DFS for storage.\n   *\n   * @param conf The configuration to use for further operations\n   */\n  public HBaseTestingUtility(@Nullable Configuration conf) {\n    super(conf);\n\n    // a hbase checksum verification failure will cause unit tests to fail\n    ChecksumUtil.generateExceptionForChecksumFailureForTest(true);\n\n    // if conf is provided, prevent contention for ports if other hbase thread(s) are running\n    if (conf != null) {\n      if (conf.getInt(HConstants.MASTER_INFO_PORT, HConstants.DEFAULT_MASTER_INFOPORT)\n              == HConstants.DEFAULT_MASTER_INFOPORT) {\n        conf.setInt(HConstants.MASTER_INFO_PORT, -1);\n        LOG.debug(\"Config property {} changed to -1\", HConstants.MASTER_INFO_PORT);\n      }\n      if (conf.getInt(HConstants.REGIONSERVER_PORT, HConstants.DEFAULT_REGIONSERVER_PORT)\n              == HConstants.DEFAULT_REGIONSERVER_PORT) {\n        conf.setInt(HConstants.REGIONSERVER_PORT, -1);\n        LOG.debug(\"Config property {} changed to -1\", HConstants.REGIONSERVER_PORT);\n      }\n    }\n\n    // Save this for when setting default file:// breaks things\n    this.conf.set(\"original.defaultFS\", this.conf.get(\"fs.defaultFS\"));\n\n    // Every cluster is a local cluster until we start DFS\n    // Note that conf could be null, but this.conf will not be\n    String dataTestDir = getDataTestDir().toString();\n    this.conf.set(\"fs.defaultFS\",\"file:///\");\n    this.conf.set(HConstants.HBASE_DIR, \"file://\" + dataTestDir);\n    LOG.debug(\"Setting {} to {}\", HConstants.HBASE_DIR, dataTestDir);\n    this.conf.setBoolean(CommonFSUtils.UNSAFE_STREAM_CAPABILITY_ENFORCE,false);\n  }"
        ],
        [
            "IntegrationTestingUtility::createDistributedHBaseCluster()",
            " 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "  public void createDistributedHBaseCluster() throws IOException {\n    Configuration conf = getConfiguration();\n    Class<? extends ClusterManager> clusterManagerClass = conf.getClass(HBASE_CLUSTER_MANAGER_CLASS,\n      DEFAULT_HBASE_CLUSTER_MANAGER_CLASS, ClusterManager.class);\n    ClusterManager clusterManager = ReflectionUtils.newInstance(\n      clusterManagerClass, conf);\n    setHBaseCluster(new DistributedHBaseCluster(conf, clusterManager));\n    getAdmin();\n  }",
            " 133  \n 134  \n 135 +\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  ",
            "  public void createDistributedHBaseCluster() throws IOException {\n    Configuration conf = getConfiguration();\n    conf.set(\"fs.defaultFS\", conf.get(\"original.defaultFS\"));\n    Class<? extends ClusterManager> clusterManagerClass = conf.getClass(HBASE_CLUSTER_MANAGER_CLASS,\n      DEFAULT_HBASE_CLUSTER_MANAGER_CLASS, ClusterManager.class);\n    ClusterManager clusterManager = ReflectionUtils.newInstance(\n      clusterManagerClass, conf);\n    setHBaseCluster(new DistributedHBaseCluster(conf, clusterManager));\n    getAdmin();\n  }"
        ]
    ],
    "26b69dc45bf3c953905d5ed279e3f691ea802157": [
        [
            "MetaTableLocator::getCachedConnection(ClusterConnection,ServerName)",
            " 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376 -\n 377 -\n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "  /**\n   * @param sn ServerName to get a connection against.\n   * @return The AdminProtocol we got when we connected to <code>sn</code>\n   * May have come from cache, may not be good, may have been setup by this\n   * invocation, or may be null.\n   * @throws IOException\n   */\n  private static AdminService.BlockingInterface getCachedConnection(ClusterConnection connection,\n    ServerName sn)\n  throws IOException {\n    if (sn == null) {\n      return null;\n    }\n    AdminService.BlockingInterface service = null;\n    try {\n      service = connection.getAdmin(sn);\n    } catch (RetriesExhaustedException e) {\n      if (e.getCause() != null && e.getCause() instanceof ConnectException) {\n        // Catch this; presume it means the cached connection has gone bad.\n      } else {\n        throw e;\n      }\n    } catch (SocketTimeoutException e) {\n      LOG.debug(\"Timed out connecting to \" + sn);\n    } catch (NoRouteToHostException e) {\n      LOG.debug(\"Connecting to \" + sn, e);\n    } catch (SocketException e) {\n      LOG.debug(\"Exception connecting to \" + sn);\n    } catch (UnknownHostException e) {\n      LOG.debug(\"Unknown host exception connecting to  \" + sn);\n    } catch (FailedServerException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Server \" + sn + \" is in failed server list.\");\n      }\n    } catch (IOException ioe) {\n      Throwable cause = ioe.getCause();\n      if (ioe instanceof ConnectException) {\n        // Catch. Connect refused.\n      } else if (cause != null && cause instanceof EOFException) {\n        // Catch. Other end disconnected us.\n      } else if (cause != null && cause.getMessage() != null &&\n        cause.getMessage().toLowerCase(Locale.ROOT).contains(\"connection reset\")) {\n        // Catch. Connection reset.\n      } else {\n        throw ioe;\n      }\n\n    }\n    return service;\n  }",
            " 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383 +\n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  ",
            "  /**\n   * @param sn ServerName to get a connection against.\n   * @return The AdminProtocol we got when we connected to <code>sn</code>\n   *         May have come from cache, may not be good, may have been setup by this invocation, or\n   *         may be null.\n   * @throws IOException if the number of retries for getting the connection is exceeded\n   */\n  private static AdminService.BlockingInterface getCachedConnection(ClusterConnection connection,\n      ServerName sn) throws IOException {\n    if (sn == null) {\n      return null;\n    }\n    AdminService.BlockingInterface service = null;\n    try {\n      service = connection.getAdmin(sn);\n    } catch (RetriesExhaustedException e) {\n      if (e.getCause() != null && e.getCause() instanceof ConnectException) {\n        // Catch this; presume it means the cached connection has gone bad.\n      } else {\n        throw e;\n      }\n    } catch (SocketTimeoutException e) {\n      LOG.debug(\"Timed out connecting to \" + sn);\n    } catch (NoRouteToHostException e) {\n      LOG.debug(\"Connecting to \" + sn, e);\n    } catch (SocketException e) {\n      LOG.debug(\"Exception connecting to \" + sn);\n    } catch (UnknownHostException e) {\n      LOG.debug(\"Unknown host exception connecting to  \" + sn);\n    } catch (FailedServerException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Server \" + sn + \" is in failed server list.\");\n      }\n    } catch (IOException ioe) {\n      Throwable cause = ioe.getCause();\n      if (ioe instanceof ConnectException) {\n        // Catch. Connect refused.\n      } else if (cause != null && cause instanceof EOFException) {\n        // Catch. Other end disconnected us.\n      } else if (cause != null && cause.getMessage() != null &&\n        cause.getMessage().toLowerCase(Locale.ROOT).contains(\"connection reset\")) {\n        // Catch. Connection reset.\n      } else {\n        throw ioe;\n      }\n\n    }\n    return service;\n  }"
        ],
        [
            "MetaTableLocator::verifyMetaRegionLocation(ClusterConnection,ZKWatcher,long,int)",
            " 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278 -\n 279 -\n 280 -\n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "  /**\n   * Verify <code>hbase:meta</code> is deployed and accessible.\n   * @param connection\n   * @param zkw\n   * @param timeout How long to wait on zk for meta address (passed through to\n   * @param replicaId\n   * @return True if the <code>hbase:meta</code> location is healthy.\n   * @throws InterruptedException\n   * @throws IOException\n   */\n  public boolean verifyMetaRegionLocation(ClusterConnection connection,\n                                          ZKWatcher zkw, final long timeout, int replicaId)\n  throws InterruptedException, IOException {\n    AdminProtos.AdminService.BlockingInterface service = null;\n    try {\n      service = getMetaServerConnection(connection, zkw, timeout, replicaId);\n    } catch (NotAllMetaRegionsOnlineException e) {\n      // Pass\n    } catch (ServerNotRunningYetException e) {\n      // Pass -- remote server is not up so can't be carrying root\n    } catch (UnknownHostException e) {\n      // Pass -- server name doesn't resolve so it can't be assigned anything.\n    } catch (RegionServerStoppedException e) {\n      // Pass -- server name sends us to a server that is dying or already dead.\n    }\n    return (service != null) && verifyRegionLocation(connection, service,\n            getMetaRegionLocation(zkw, replicaId), RegionReplicaUtil.getRegionInfoForReplica(\n                RegionInfoBuilder.FIRST_META_REGIONINFO, replicaId).getRegionName());\n  }",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289 +\n 290 +\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "  /**\n   * Verify <code>hbase:meta</code> is deployed and accessible.\n   *\n   * @param connection the connection to use\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param timeout How long to wait on zk for meta address (passed through to\n   * @param replicaId the ID of the replica\n   * @return True if the <code>hbase:meta</code> location is healthy.\n   * @throws InterruptedException if waiting for the socket operation fails\n   * @throws IOException if the number of retries for getting the connection is exceeded\n   */\n  public boolean verifyMetaRegionLocation(ClusterConnection connection, ZKWatcher zkw,\n      final long timeout, int replicaId) throws InterruptedException, IOException {\n    AdminProtos.AdminService.BlockingInterface service = null;\n    try {\n      service = getMetaServerConnection(connection, zkw, timeout, replicaId);\n    } catch (NotAllMetaRegionsOnlineException e) {\n      // Pass\n    } catch (ServerNotRunningYetException e) {\n      // Pass -- remote server is not up so can't be carrying root\n    } catch (UnknownHostException e) {\n      // Pass -- server name doesn't resolve so it can't be assigned anything.\n    } catch (RegionServerStoppedException e) {\n      // Pass -- server name sends us to a server that is dying or already dead.\n    }\n    return (service != null) && verifyRegionLocation(connection, service,\n            getMetaRegionLocation(zkw, replicaId), RegionReplicaUtil.getRegionInfoForReplica(\n                RegionInfoBuilder.FIRST_META_REGIONINFO, replicaId).getRegionName());\n  }"
        ],
        [
            "MetaTableLocator::blockUntilAvailable(ZKWatcher,long)",
            " 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588 -\n 589 -\n 590 -\n 591  \n 592  ",
            "  /**\n   * Wait until the meta region is available and is not in transition.\n   * @param zkw zookeeper connection to use\n   * @param timeout maximum time to wait, in millis\n   * @return ServerName or null if we timed out.\n   * @throws InterruptedException\n   */\n  public ServerName blockUntilAvailable(final ZKWatcher zkw,\n      final long timeout)\n  throws InterruptedException {\n    return blockUntilAvailable(zkw, RegionInfo.DEFAULT_REPLICA_ID, timeout);\n  }",
            " 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599 +\n 600 +\n 601  \n 602  ",
            "  /**\n   * Wait until the meta region is available and is not in transition.\n   * @param zkw zookeeper connection to use\n   * @param timeout maximum time to wait, in millis\n   * @return ServerName or null if we timed out.\n   * @throws InterruptedException if waiting for the socket operation fails\n   */\n  public ServerName blockUntilAvailable(final ZKWatcher zkw, final long timeout)\n          throws InterruptedException {\n    return blockUntilAvailable(zkw, RegionInfo.DEFAULT_REPLICA_ID, timeout);\n  }"
        ],
        [
            "ZKWatcher::getMetaReplicaNodes()",
            " 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375 -\n 376  \n 377  \n 378  \n 379  ",
            "  /**\n   * Get the znodes corresponding to the meta replicas from ZK\n   * @return list of znodes\n   * @throws KeeperException\n   */\n  public List<String> getMetaReplicaNodes() throws KeeperException {\n    List<String> childrenOfBaseNode = ZKUtil.listChildrenNoWatch(this, znodePaths.baseZNode);\n    List<String> metaReplicaNodes = new ArrayList<>(2);\n    if (childrenOfBaseNode != null) {\n      String pattern = conf.get(\"zookeeper.znode.metaserver\",\"meta-region-server\");\n      for (String child : childrenOfBaseNode) {\n        if (child.startsWith(pattern)) metaReplicaNodes.add(child);\n      }\n    }\n    return metaReplicaNodes;\n  }",
            " 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373 +\n 374 +\n 375 +\n 376  \n 377  \n 378  \n 379  ",
            "  /**\n   * Get the znodes corresponding to the meta replicas from ZK\n   * @return list of znodes\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public List<String> getMetaReplicaNodes() throws KeeperException {\n    List<String> childrenOfBaseNode = ZKUtil.listChildrenNoWatch(this, znodePaths.baseZNode);\n    List<String> metaReplicaNodes = new ArrayList<>(2);\n    if (childrenOfBaseNode != null) {\n      String pattern = conf.get(\"zookeeper.znode.metaserver\",\"meta-region-server\");\n      for (String child : childrenOfBaseNode) {\n        if (child.startsWith(pattern)) {\n          metaReplicaNodes.add(child);\n        }\n      }\n    }\n    return metaReplicaNodes;\n  }"
        ],
        [
            "RecoverableZooKeeper::exists(String,boolean)",
            " 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242 -\n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  ",
            "  /**\n   * exists is an idempotent operation. Retry before throwing exception\n   * @return A Stat instance\n   */\n  public Stat exists(String path, boolean watch) throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.exists\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().exists(path, watch);\n          this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return nodeStat;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"exists\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"exists\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249 +\n 250 +\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  ",
            "  /**\n   * exists is an idempotent operation. Retry before throwing exception\n   * @return A Stat instance\n   */\n  public Stat exists(String path, boolean watch) throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.exists\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().exists(path, watch);\n          this.metrics.registerReadOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return nodeStat;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"exists\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"exists\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::ZKUtilOp::DeleteNodeFailSilent::equals(Object)",
            "1546  \n1547  \n1548 -\n1549 -\n1550  \n1551  \n1552  ",
            "      @Override\n      public boolean equals(Object o) {\n        if (this == o) return true;\n        if (!(o instanceof DeleteNodeFailSilent)) return false;\n\n        return super.equals(o);\n      }",
            "1559  \n1560  \n1561 +\n1562 +\n1563 +\n1564 +\n1565 +\n1566 +\n1567  \n1568  \n1569  ",
            "      @Override\n      public boolean equals(Object o) {\n        if (this == o) {\n          return true;\n        }\n        if (!(o instanceof DeleteNodeFailSilent)) {\n          return false;\n        }\n\n        return super.equals(o);\n      }"
        ],
        [
            "RecoverableZooKeeper::findPreviousSequentialNode(String)",
            " 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744 -\n 745  \n 746  \n 747  \n 748  \n 749  \n 750 -\n 751  \n 752  \n 753  \n 754  \n 755  \n 756  ",
            "  private String findPreviousSequentialNode(String path)\n    throws KeeperException, InterruptedException {\n    int lastSlashIdx = path.lastIndexOf('/');\n    assert(lastSlashIdx != -1);\n    String parent = path.substring(0, lastSlashIdx);\n    String nodePrefix = path.substring(lastSlashIdx+1);\n    long startTime = EnvironmentEdgeManager.currentTime();\n    List<String> nodes = checkZk().getChildren(parent, false);\n    this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n    List<String> matching = filterByPrefix(nodes, nodePrefix);\n    for (String node : matching) {\n      String nodePath = parent + \"/\" + node;\n      startTime = EnvironmentEdgeManager.currentTime();\n      Stat stat = checkZk().exists(nodePath, false);\n      this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n      if (stat != null) {\n        return nodePath;\n      }\n    }\n    return null;\n  }",
            " 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765 +\n 766 +\n 767  \n 768  \n 769  \n 770  \n 771  \n 772 +\n 773 +\n 774  \n 775  \n 776  \n 777  \n 778  \n 779  ",
            "  private String findPreviousSequentialNode(String path)\n    throws KeeperException, InterruptedException {\n    int lastSlashIdx = path.lastIndexOf('/');\n    assert(lastSlashIdx != -1);\n    String parent = path.substring(0, lastSlashIdx);\n    String nodePrefix = path.substring(lastSlashIdx+1);\n    long startTime = EnvironmentEdgeManager.currentTime();\n    List<String> nodes = checkZk().getChildren(parent, false);\n    this.metrics.registerReadOperationLatency(\n            Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n    List<String> matching = filterByPrefix(nodes, nodePrefix);\n    for (String node : matching) {\n      String nodePath = parent + \"/\" + node;\n      startTime = EnvironmentEdgeManager.currentTime();\n      Stat stat = checkZk().exists(nodePath, false);\n      this.metrics.registerReadOperationLatency(\n              Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n      if (stat != null) {\n        return nodePath;\n      }\n    }\n    return null;\n  }"
        ],
        [
            "ZKUtil::updateExistingNodeData(ZKWatcher,String,byte,int)",
            " 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759 -\n 760 -\n 761 -\n 762  \n 763  \n 764  \n 765  \n 766  \n 767  ",
            "  /**\n   * Update the data of an existing node with the expected version to have the\n   * specified data.\n   *\n   * Throws an exception if there is a version mismatch or some other problem.\n   *\n   * Sets no watches under any conditions.\n   *\n   * @param zkw zk reference\n   * @param znode\n   * @param data\n   * @param expectedVersion\n   * @throws KeeperException if unexpected zookeeper exception\n   * @throws KeeperException.BadVersionException if version mismatch\n   * @deprecated Unused\n   */\n  @Deprecated\n  public static void updateExistingNodeData(ZKWatcher zkw, String znode,\n                                            byte [] data, int expectedVersion)\n  throws KeeperException {\n    try {\n      zkw.getRecoverableZooKeeper().setData(znode, data, expectedVersion);\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n  }",
            " 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769 +\n 770 +\n 771  \n 772  \n 773  \n 774  \n 775  \n 776  ",
            "  /**\n   * Update the data of an existing node with the expected version to have the\n   * specified data.\n   *\n   * Throws an exception if there is a version mismatch or some other problem.\n   *\n   * Sets no watches under any conditions.\n   *\n   * @param zkw zk reference\n   * @param znode the path to the ZNode\n   * @param data the data to store in ZooKeeper\n   * @param expectedVersion the expected version\n   * @throws KeeperException if unexpected zookeeper exception\n   * @throws KeeperException.BadVersionException if version mismatch\n   * @deprecated Unused\n   */\n  @Deprecated\n  public static void updateExistingNodeData(ZKWatcher zkw, String znode, byte[] data,\n      int expectedVersion) throws KeeperException {\n    try {\n      zkw.getRecoverableZooKeeper().setData(znode, data, expectedVersion);\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n  }"
        ],
        [
            "ZKUtil::ZKUtilOp::CreateAndFailSilent::equals(Object)",
            "1521  \n1522  \n1523 -\n1524 -\n1525  \n1526  \n1527  \n1528  ",
            "      @Override\n      public boolean equals(Object o) {\n        if (this == o) return true;\n        if (!(o instanceof CreateAndFailSilent)) return false;\n\n        CreateAndFailSilent op = (CreateAndFailSilent) o;\n        return getPath().equals(op.getPath()) && Arrays.equals(data, op.data);\n      }",
            "1530  \n1531  \n1532 +\n1533 +\n1534 +\n1535 +\n1536 +\n1537 +\n1538  \n1539  \n1540  \n1541  ",
            "      @Override\n      public boolean equals(Object o) {\n        if (this == o) {\n          return true;\n        }\n        if (!(o instanceof CreateAndFailSilent)) {\n          return false;\n        }\n\n        CreateAndFailSilent op = (CreateAndFailSilent) o;\n        return getPath().equals(op.getPath()) && Arrays.equals(data, op.data);\n      }"
        ],
        [
            "RecoverableZooKeeper::close()",
            " 762  \n 763 -\n 764  ",
            "  public synchronized void close() throws InterruptedException {\n    if (zk != null) zk.close();\n  }",
            " 785  \n 786 +\n 787 +\n 788 +\n 789  ",
            "  public synchronized void close() throws InterruptedException {\n    if (zk != null) {\n      zk.close();\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::exists(String,Watcher)",
            " 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 -\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "  /**\n   * exists is an idempotent operation. Retry before throwing exception\n   * @return A Stat instance\n   */\n  public Stat exists(String path, Watcher watcher) throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.exists\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().exists(path, watcher);\n          this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime, 1));\n          return nodeStat;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"exists\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"exists\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214 +\n 215 +\n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  ",
            "  /**\n   * exists is an idempotent operation. Retry before throwing exception\n   * @return A Stat instance\n   */\n  public Stat exists(String path, Watcher watcher) throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.exists\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().exists(path, watcher);\n          this.metrics.registerReadOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime, 1));\n          return nodeStat;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"exists\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"exists\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::RecoverableZooKeeper(String,int,Watcher,int,int,int,String)",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 -\n 117  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"DE_MIGHT_IGNORE\",\n      justification=\"None. Its always been this way.\")\n  public RecoverableZooKeeper(String quorumServers, int sessionTimeout,\n      Watcher watcher, int maxRetries, int retryIntervalMillis, int maxSleepTime, String identifier)\n  throws IOException {\n    // TODO: Add support for zk 'chroot'; we don't add it to the quorumServers String as we should.\n    this.retryCounterFactory =\n      new RetryCounterFactory(maxRetries+1, retryIntervalMillis, maxSleepTime);\n\n    if (identifier == null || identifier.length() == 0) {\n      // the identifier = processID@hostName\n      identifier = ManagementFactory.getRuntimeMXBean().getName();\n    }\n    LOG.info(\"Process identifier=\" + identifier +\n      \" connecting to ZooKeeper ensemble=\" + quorumServers);\n    this.identifier = identifier;\n    this.id = Bytes.toBytes(identifier);\n\n    this.watcher = watcher;\n    this.sessionTimeout = sessionTimeout;\n    this.quorumServers = quorumServers;\n    this.metrics = new ZKMetrics();\n    try {checkZk();} catch (Exception x) {/* ignore */}\n  }",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121 +\n 122  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"DE_MIGHT_IGNORE\",\n      justification=\"None. Its always been this way.\")\n  public RecoverableZooKeeper(String quorumServers, int sessionTimeout,\n      Watcher watcher, int maxRetries, int retryIntervalMillis, int maxSleepTime, String identifier)\n  throws IOException {\n    // TODO: Add support for zk 'chroot'; we don't add it to the quorumServers String as we should.\n    this.retryCounterFactory =\n      new RetryCounterFactory(maxRetries+1, retryIntervalMillis, maxSleepTime);\n\n    if (identifier == null || identifier.length() == 0) {\n      // the identifier = processID@hostName\n      identifier = ManagementFactory.getRuntimeMXBean().getName();\n    }\n    LOG.info(\"Process identifier=\" + identifier +\n      \" connecting to ZooKeeper ensemble=\" + quorumServers);\n    this.identifier = identifier;\n    this.id = Bytes.toBytes(identifier);\n\n    this.watcher = watcher;\n    this.sessionTimeout = sessionTimeout;\n    this.quorumServers = quorumServers;\n    this.metrics = new ZKMetrics();\n\n    try {\n      checkZk();\n    } catch (Exception x) {\n      /* ignore */\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::getChildren(String,boolean)",
            " 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 -\n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  ",
            "  /**\n   * getChildren is an idempotent operation. Retry before throwing exception\n   * @return List of children znodes\n   */\n  public List<String> getChildren(String path, boolean watch)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getChildren\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<String> children = checkZk().getChildren(path, watch);\n          this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return children;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332 +\n 333 +\n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  ",
            "  /**\n   * getChildren is an idempotent operation. Retry before throwing exception\n   * @return List of children znodes\n   */\n  public List<String> getChildren(String path, boolean watch)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getChildren\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<String> children = checkZk().getChildren(path, watch);\n          this.metrics.registerReadOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return children;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::sync(String,AsyncCallback,Object)",
            " 778  \n 779  \n 780  \n 781 -\n 782  ",
            "  public void sync(String path, AsyncCallback.VoidCallback cb, Object ctx) throws KeeperException {\n    long startTime = EnvironmentEdgeManager.currentTime();\n    checkZk().sync(path, cb, null);\n    this.metrics.registerSyncOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n  }",
            " 803  \n 804  \n 805  \n 806 +\n 807 +\n 808  ",
            "  public void sync(String path, AsyncCallback.VoidCallback cb, Object ctx) throws KeeperException {\n    long startTime = EnvironmentEdgeManager.currentTime();\n    checkZk().sync(path, cb, null);\n    this.metrics.registerSyncOperationLatency(\n            Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n  }"
        ],
        [
            "RegionNormalizerTracker::RegionNormalizerTracker(ZKWatcher,Abortable)",
            "  38 -\n  39 -\n  40  \n  41  ",
            "  public RegionNormalizerTracker(ZKWatcher watcher,\n                             Abortable abortable) {\n    super(watcher, watcher.znodePaths.regionNormalizerZNode, abortable);\n  }",
            "  39 +\n  40  \n  41  ",
            "  public RegionNormalizerTracker(ZKWatcher watcher, Abortable abortable) {\n    super(watcher, watcher.znodePaths.regionNormalizerZNode, abortable);\n  }"
        ],
        [
            "MiniZooKeeperCluster::killOneBackupZooKeeperServer()",
            " 365  \n 366  \n 367  \n 368  \n 369  \n 370 -\n 371 -\n 372 -\n 373 -\n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  ",
            "  /**\n   * Kill one back up ZK servers\n   * @throws IOException\n   * @throws InterruptedException\n   */\n  public void killOneBackupZooKeeperServer() throws IOException,\n                                        InterruptedException {\n    if (!started || activeZKServerIndex < 0 ||\n        standaloneServerFactoryList.size() <= 1) {\n      return ;\n    }\n\n    int backupZKServerIndex = activeZKServerIndex+1;\n    // Shutdown the current active one\n    NIOServerCnxnFactory standaloneServerFactory =\n      standaloneServerFactoryList.get(backupZKServerIndex);\n    int clientPort = clientPortList.get(backupZKServerIndex);\n\n    standaloneServerFactory.shutdown();\n    if (!waitForServerDown(clientPort, connectionTimeout)) {\n      throw new IOException(\"Waiting for shutdown of standalone server\");\n    }\n\n    zooKeeperServers.get(backupZKServerIndex).getZKDatabase().close();\n\n    // remove this backup zk server\n    standaloneServerFactoryList.remove(backupZKServerIndex);\n    clientPortList.remove(backupZKServerIndex);\n    zooKeeperServers.remove(backupZKServerIndex);\n    LOG.info(\"Kill one backup ZK servers in the cluster \" +\n        \"on client port: \" + clientPort);\n  }",
            " 368  \n 369  \n 370  \n 371  \n 372  \n 373 +\n 374 +\n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  ",
            "  /**\n   * Kill one back up ZK servers.\n   *\n   * @throws IOException if waiting for the shutdown of a server fails\n   */\n  public void killOneBackupZooKeeperServer() throws IOException, InterruptedException {\n    if (!started || activeZKServerIndex < 0 || standaloneServerFactoryList.size() <= 1) {\n      return ;\n    }\n\n    int backupZKServerIndex = activeZKServerIndex+1;\n    // Shutdown the current active one\n    NIOServerCnxnFactory standaloneServerFactory =\n      standaloneServerFactoryList.get(backupZKServerIndex);\n    int clientPort = clientPortList.get(backupZKServerIndex);\n\n    standaloneServerFactory.shutdown();\n    if (!waitForServerDown(clientPort, connectionTimeout)) {\n      throw new IOException(\"Waiting for shutdown of standalone server\");\n    }\n\n    zooKeeperServers.get(backupZKServerIndex).getZKDatabase().close();\n\n    // remove this backup zk server\n    standaloneServerFactoryList.remove(backupZKServerIndex);\n    clientPortList.remove(backupZKServerIndex);\n    zooKeeperServers.remove(backupZKServerIndex);\n    LOG.info(\"Kill one backup ZK servers in the cluster \" +\n        \"on client port: \" + clientPort);\n  }"
        ],
        [
            "MetaTableLocator::setMetaLocation(ZKWatcher,ServerName,int,RegionState)",
            " 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441 -\n 442 -\n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "  /**\n   * Sets the location of <code>hbase:meta</code> in ZooKeeper to the\n   * specified server address.\n   * @param zookeeper\n   * @param serverName\n   * @param replicaId\n   * @param state\n   * @throws KeeperException\n   */\n  public static void setMetaLocation(ZKWatcher zookeeper,\n      ServerName serverName, int replicaId, RegionState.State state) throws KeeperException {\n    if (serverName == null) {\n      LOG.warn(\"Tried to set null ServerName in hbase:meta; skipping -- ServerName required\");\n      return;\n    }\n    LOG.info(\"Setting hbase:meta (replicaId=\" + replicaId + \") location in ZooKeeper as \" +\n      serverName);\n    // Make the MetaRegionServer pb and then get its bytes and save this as\n    // the znode content.\n    MetaRegionServer pbrsr = MetaRegionServer.newBuilder()\n      .setServer(ProtobufUtil.toServerName(serverName))\n      .setRpcVersion(HConstants.RPC_CURRENT_VERSION)\n      .setState(state.convert()).build();\n    byte[] data = ProtobufUtil.prependPBMagic(pbrsr.toByteArray());\n    try {\n      ZKUtil.setData(zookeeper,\n          zookeeper.znodePaths.getZNodeForReplica(replicaId), data);\n    } catch(KeeperException.NoNodeException nne) {\n      if (replicaId == RegionInfo.DEFAULT_REPLICA_ID) {\n        LOG.debug(\"META region location doesn't exist, create it\");\n      } else {\n        LOG.debug(\"META region location doesn't exist for replicaId=\" + replicaId +\n            \", create it\");\n      }\n      ZKUtil.createAndWatch(zookeeper, zookeeper.znodePaths.getZNodeForReplica(replicaId), data);\n    }\n  }",
            " 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447 +\n 448 +\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  ",
            "  /**\n   * Sets the location of <code>hbase:meta</code> in ZooKeeper to the specified server address.\n   * @param zookeeper reference to the {@link ZKWatcher} which also contains configuration and\n   *                  operation\n   * @param serverName the name of the server\n   * @param replicaId the ID of the replica\n   * @param state the state of the region\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static void setMetaLocation(ZKWatcher zookeeper, ServerName serverName, int replicaId,\n      RegionState.State state) throws KeeperException {\n    if (serverName == null) {\n      LOG.warn(\"Tried to set null ServerName in hbase:meta; skipping -- ServerName required\");\n      return;\n    }\n    LOG.info(\"Setting hbase:meta (replicaId=\" + replicaId + \") location in ZooKeeper as \" +\n      serverName);\n    // Make the MetaRegionServer pb and then get its bytes and save this as\n    // the znode content.\n    MetaRegionServer pbrsr = MetaRegionServer.newBuilder()\n      .setServer(ProtobufUtil.toServerName(serverName))\n      .setRpcVersion(HConstants.RPC_CURRENT_VERSION)\n      .setState(state.convert()).build();\n    byte[] data = ProtobufUtil.prependPBMagic(pbrsr.toByteArray());\n    try {\n      ZKUtil.setData(zookeeper,\n          zookeeper.znodePaths.getZNodeForReplica(replicaId), data);\n    } catch(KeeperException.NoNodeException nne) {\n      if (replicaId == RegionInfo.DEFAULT_REPLICA_ID) {\n        LOG.debug(\"META region location doesn't exist, create it\");\n      } else {\n        LOG.debug(\"META region location doesn't exist for replicaId=\" + replicaId +\n            \", create it\");\n      }\n      ZKUtil.createAndWatch(zookeeper, zookeeper.znodePaths.getZNodeForReplica(replicaId), data);\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::setData(String,byte,int)",
            " 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433 -\n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453 -\n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  ",
            "  /**\n   * setData is NOT an idempotent operation. Retry may cause BadVersion Exception\n   * Adding an identifier field into the data to check whether\n   * badversion is caused by the result of previous correctly setData\n   * @return Stat instance\n   */\n  public Stat setData(String path, byte[] data, int version)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.setData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      byte[] newData = ZKMetadata.appendMetaData(id, data);\n      boolean isRetry = false;\n      long startTime;\n      while (true) {\n        try {\n          startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().setData(path, newData, version);\n          this.metrics.registerWriteOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return nodeStat;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"setData\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"setData\");\n              break;\n            case BADVERSION:\n              if (isRetry) {\n                // try to verify whether the previous setData success or not\n                try{\n                  Stat stat = new Stat();\n                  startTime = EnvironmentEdgeManager.currentTime();\n                  byte[] revData = checkZk().getData(path, false, stat);\n                  this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n                  if(Bytes.compareTo(revData, newData) == 0) {\n                    // the bad version is caused by previous successful setData\n                    return stat;\n                  }\n                } catch(KeeperException keeperException){\n                  this.metrics.registerFailedZKCall();\n                  // the ZK is not reliable at this moment. just throwing exception\n                  throw keeperException;\n                }\n              }\n            // throw other exceptions and verified bad version exceptions\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n        isRetry = true;\n      }\n    }\n  }",
            " 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 +\n 446 +\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466 +\n 467 +\n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  ",
            "  /**\n   * setData is NOT an idempotent operation. Retry may cause BadVersion Exception\n   * Adding an identifier field into the data to check whether\n   * badversion is caused by the result of previous correctly setData\n   * @return Stat instance\n   */\n  public Stat setData(String path, byte[] data, int version)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.setData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      byte[] newData = ZKMetadata.appendMetaData(id, data);\n      boolean isRetry = false;\n      long startTime;\n      while (true) {\n        try {\n          startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().setData(path, newData, version);\n          this.metrics.registerWriteOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return nodeStat;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"setData\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"setData\");\n              break;\n            case BADVERSION:\n              if (isRetry) {\n                // try to verify whether the previous setData success or not\n                try{\n                  Stat stat = new Stat();\n                  startTime = EnvironmentEdgeManager.currentTime();\n                  byte[] revData = checkZk().getData(path, false, stat);\n                  this.metrics.registerReadOperationLatency(\n                          Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n                  if(Bytes.compareTo(revData, newData) == 0) {\n                    // the bad version is caused by previous successful setData\n                    return stat;\n                  }\n                } catch(KeeperException keeperException){\n                  this.metrics.registerFailedZKCall();\n                  // the ZK is not reliable at this moment. just throwing exception\n                  throw keeperException;\n                }\n              }\n            // throw other exceptions and verified bad version exceptions\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n        isRetry = true;\n      }\n    }\n  }"
        ],
        [
            "ZKWatcher::abort(String,Throwable)",
            " 624  \n 625  \n 626 -\n 627 -\n 628  ",
            "  @Override\n  public void abort(String why, Throwable e) {\n    if (this.abortable != null) this.abortable.abort(why, e);\n    else this.aborted = true;\n  }",
            " 623  \n 624  \n 625 +\n 626 +\n 627 +\n 628 +\n 629 +\n 630  ",
            "  @Override\n  public void abort(String why, Throwable e) {\n    if (this.abortable != null) {\n      this.abortable.abort(why, e);\n    } else {\n      this.aborted = true;\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::delete(String,int)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  ",
            "  /**\n   * delete is an idempotent operation. Retry before throwing exception.\n   * This function will not throw NoNodeException if the path does not\n   * exist.\n   */\n  public void delete(String path, int version) throws InterruptedException, KeeperException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.delete\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      boolean isRetry = false; // False for first attempt, true for all retries.\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          checkZk().delete(path, version);\n          this.metrics.registerWriteOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case NONODE:\n              if (isRetry) {\n                LOG.debug(\"Node \" + path + \" already deleted. Assuming a \" +\n                    \"previous attempt succeeded.\");\n                return;\n              }\n              LOG.debug(\"Node \" + path + \" already deleted, retry=\" + isRetry);\n              throw e;\n\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"delete\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"delete\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n        isRetry = true;\n      }\n    }\n  }",
            " 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169 +\n 170 +\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  ",
            "  /**\n   * delete is an idempotent operation. Retry before throwing exception.\n   * This function will not throw NoNodeException if the path does not\n   * exist.\n   */\n  public void delete(String path, int version) throws InterruptedException, KeeperException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.delete\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      boolean isRetry = false; // False for first attempt, true for all retries.\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          checkZk().delete(path, version);\n          this.metrics.registerWriteOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case NONODE:\n              if (isRetry) {\n                LOG.debug(\"Node \" + path + \" already deleted. Assuming a \" +\n                    \"previous attempt succeeded.\");\n                return;\n              }\n              LOG.debug(\"Node \" + path + \" already deleted, retry=\" + isRetry);\n              throw e;\n\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"delete\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"delete\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n        isRetry = true;\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::createSetData(ZKWatcher,String,byte)",
            " 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813 -\n 814 -\n 815 -\n 816  \n 817  \n 818  \n 819  \n 820  \n 821  ",
            "  /**\n   * Set data into node creating node if it doesn't yet exist.\n   * Does not set watch.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data to set for node\n   * @throws KeeperException\n   */\n  public static void createSetData(final ZKWatcher zkw, final String znode,\n                                   final byte [] data)\n  throws KeeperException {\n    if (checkExists(zkw, znode) == -1) {\n      ZKUtil.createWithParents(zkw, znode, data);\n    } else {\n      ZKUtil.setData(zkw, znode, data);\n    }\n  }",
            " 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822 +\n 823 +\n 824  \n 825  \n 826  \n 827  \n 828  \n 829  ",
            "  /**\n   * Set data into node creating node if it doesn't yet exist.\n   * Does not set watch.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data to set for node\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static void createSetData(final ZKWatcher zkw, final String znode, final byte [] data)\n          throws KeeperException {\n    if (checkExists(zkw, znode) == -1) {\n      ZKUtil.createWithParents(zkw, znode, data);\n    } else {\n      ZKUtil.setData(zkw, znode, data);\n    }\n  }"
        ],
        [
            "ZKUtil::createNodeIfNotExistsNoWatch(ZKWatcher,String,byte,CreateMode)",
            "1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028 -\n1029 -\n1030 -\n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  ",
            "  /**\n   * Creates the specified znode with the specified data but does not watch it.\n   *\n   * Returns the znode of the newly created node\n   *\n   * If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data of node\n   * @param createMode specifying whether the node to be created is ephemeral and/or sequential\n   * @return true name of the newly created znode or null\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static String createNodeIfNotExistsNoWatch(ZKWatcher zkw, String znode,\n                                                    byte[] data, CreateMode createMode) throws KeeperException {\n\n    String createdZNode = null;\n    try {\n      createdZNode = zkw.getRecoverableZooKeeper().create(znode, data,\n          createACL(zkw, znode), createMode);\n    } catch (KeeperException.NodeExistsException nee) {\n      return znode;\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return null;\n    }\n    return createdZNode;\n  }",
            "1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036 +\n1037 +\n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  ",
            "  /**\n   * Creates the specified znode with the specified data but does not watch it.\n   *\n   * Returns the znode of the newly created node\n   *\n   * If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data of node\n   * @param createMode specifying whether the node to be created is ephemeral and/or sequential\n   * @return true name of the newly created znode or null\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static String createNodeIfNotExistsNoWatch(ZKWatcher zkw, String znode, byte[] data,\n      CreateMode createMode) throws KeeperException {\n    String createdZNode = null;\n    try {\n      createdZNode = zkw.getRecoverableZooKeeper().create(znode, data,\n          createACL(zkw, znode), createMode);\n    } catch (KeeperException.NodeExistsException nee) {\n      return znode;\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return null;\n    }\n    return createdZNode;\n  }"
        ],
        [
            "RecoverableZooKeeper::getChildren(String,Watcher)",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288 -\n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  ",
            "  /**\n   * getChildren is an idempotent operation. Retry before throwing exception\n   * @return List of children znodes\n   */\n  public List<String> getChildren(String path, Watcher watcher)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getChildren\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<String> children = checkZk().getChildren(path, watcher);\n          this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return children;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296 +\n 297 +\n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  ",
            "  /**\n   * getChildren is an idempotent operation. Retry before throwing exception\n   * @return List of children znodes\n   */\n  public List<String> getChildren(String path, Watcher watcher)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getChildren\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<String> children = checkZk().getChildren(path, watcher);\n          this.metrics.registerReadOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return children;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::prepareZKMulti(Iterable)",
            " 673  \n 674  \n 675  \n 676  \n 677 -\n 678 -\n 679 -\n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692 -\n 693 -\n 694  \n 695  \n 696  \n 697  \n 698  \n 699  ",
            "  /**\n   * Convert Iterable of {@link org.apache.zookeeper.Op} we got into the ZooKeeper.Op\n   * instances to actually pass to multi (need to do this in order to appendMetaData).\n   */\n  private Iterable<Op> prepareZKMulti(Iterable<Op> ops)\n  throws UnsupportedOperationException {\n    if(ops == null) return null;\n\n    List<Op> preparedOps = new LinkedList<>();\n    for (Op op : ops) {\n      if (op.getType() == ZooDefs.OpCode.create) {\n        CreateRequest create = (CreateRequest)op.toRequestRecord();\n        preparedOps.add(Op.create(create.getPath(), ZKMetadata.appendMetaData(id, create.getData()),\n          create.getAcl(), create.getFlags()));\n      } else if (op.getType() == ZooDefs.OpCode.delete) {\n        // no need to appendMetaData for delete\n        preparedOps.add(op);\n      } else if (op.getType() == ZooDefs.OpCode.setData) {\n        SetDataRequest setData = (SetDataRequest)op.toRequestRecord();\n        preparedOps.add(Op.setData(setData.getPath(), ZKMetadata.appendMetaData(id, setData.getData()),\n          setData.getVersion()));\n      } else {\n        throw new UnsupportedOperationException(\"Unexpected ZKOp type: \" + op.getClass().getName());\n      }\n    }\n    return preparedOps;\n  }",
            " 692  \n 693  \n 694  \n 695  \n 696 +\n 697 +\n 698 +\n 699 +\n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712 +\n 713 +\n 714  \n 715  \n 716  \n 717  \n 718  \n 719  ",
            "  /**\n   * Convert Iterable of {@link org.apache.zookeeper.Op} we got into the ZooKeeper.Op\n   * instances to actually pass to multi (need to do this in order to appendMetaData).\n   */\n  private Iterable<Op> prepareZKMulti(Iterable<Op> ops) throws UnsupportedOperationException {\n    if(ops == null) {\n      return null;\n    }\n\n    List<Op> preparedOps = new LinkedList<>();\n    for (Op op : ops) {\n      if (op.getType() == ZooDefs.OpCode.create) {\n        CreateRequest create = (CreateRequest)op.toRequestRecord();\n        preparedOps.add(Op.create(create.getPath(), ZKMetadata.appendMetaData(id, create.getData()),\n          create.getAcl(), create.getFlags()));\n      } else if (op.getType() == ZooDefs.OpCode.delete) {\n        // no need to appendMetaData for delete\n        preparedOps.add(op);\n      } else if (op.getType() == ZooDefs.OpCode.setData) {\n        SetDataRequest setData = (SetDataRequest)op.toRequestRecord();\n        preparedOps.add(Op.setData(setData.getPath(),\n                ZKMetadata.appendMetaData(id, setData.getData()), setData.getVersion()));\n      } else {\n        throw new UnsupportedOperationException(\"Unexpected ZKOp type: \" + op.getClass().getName());\n      }\n    }\n    return preparedOps;\n  }"
        ],
        [
            "ZKUtil::getReplicationZnodesDump(ZKWatcher,StringBuilder)",
            "1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758 -\n1759  \n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  ",
            "  /**\n   * Appends replication znodes to the passed StringBuilder.\n   * @param zkw\n   * @param sb\n   * @throws KeeperException\n   */\n  private static void getReplicationZnodesDump(ZKWatcher zkw, StringBuilder sb)\n      throws KeeperException {\n    String replicationZnode = zkw.znodePaths.replicationZNode;\n    if (ZKUtil.checkExists(zkw, replicationZnode) == -1) return;\n    // do a ls -r on this znode\n    sb.append(\"\\n\").append(replicationZnode).append(\": \");\n    List<String> children = ZKUtil.listChildrenNoWatch(zkw, replicationZnode);\n    for (String child : children) {\n      String znode = ZNodePaths.joinZNode(replicationZnode, child);\n      if (znode.equals(zkw.znodePaths.peersZNode)) {\n        appendPeersZnodes(zkw, znode, sb);\n      } else if (znode.equals(zkw.znodePaths.queuesZNode)) {\n        appendRSZnodes(zkw, znode, sb);\n      } else if (znode.equals(zkw.znodePaths.hfileRefsZNode)) {\n        appendHFileRefsZnodes(zkw, znode, sb);\n      }\n    }\n  }",
            "1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783 +\n1784 +\n1785 +\n1786 +\n1787 +\n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  ",
            "  /**\n   * Appends replication znodes to the passed StringBuilder.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param sb the {@link StringBuilder} to append to\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  private static void getReplicationZnodesDump(ZKWatcher zkw, StringBuilder sb)\n      throws KeeperException {\n    String replicationZnode = zkw.znodePaths.replicationZNode;\n\n    if (ZKUtil.checkExists(zkw, replicationZnode) == -1) {\n      return;\n    }\n\n    // do a ls -r on this znode\n    sb.append(\"\\n\").append(replicationZnode).append(\": \");\n    List<String> children = ZKUtil.listChildrenNoWatch(zkw, replicationZnode);\n    for (String child : children) {\n      String znode = ZNodePaths.joinZNode(replicationZnode, child);\n      if (znode.equals(zkw.znodePaths.peersZNode)) {\n        appendPeersZnodes(zkw, znode, sb);\n      } else if (znode.equals(zkw.znodePaths.queuesZNode)) {\n        appendRSZnodes(zkw, znode, sb);\n      } else if (znode.equals(zkw.znodePaths.hfileRefsZNode)) {\n        appendHFileRefsZnodes(zkw, znode, sb);\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::nodeHasChildren(ZKWatcher,String)",
            " 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547 -\n 548 -\n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  ",
            "  /**\n   * Checks if the specified znode has any children.  Sets no watches.\n   *\n   * Returns true if the node exists and has children.  Returns false if the\n   * node does not exist or if the node does not have any children.\n   *\n   * Used during master initialization to determine if the master is a\n   * failed-over-to master or the first master during initial cluster startup.\n   * If the directory for regionserver ephemeral nodes is empty then this is\n   * a cluster startup, if not then it is not cluster startup.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to check for children of\n   * @return true if node has children, false if not or node does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean nodeHasChildren(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    try {\n      return !zkw.getRecoverableZooKeeper().getChildren(znode, null).isEmpty();\n    } catch(KeeperException.NoNodeException ke) {\n      LOG.debug(zkw.prefix(\"Unable to list children of znode \" + znode + \" \" +\n      \"because node does not exist (not an error)\"));\n      return false;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode), e);\n      zkw.keeperException(e);\n      return false;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode), e);\n      zkw.interruptedException(e);\n      return false;\n    }\n  }",
            " 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557 +\n 558 +\n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  ",
            "  /**\n   * Checks if the specified znode has any children.  Sets no watches.\n   *\n   * Returns true if the node exists and has children.  Returns false if the\n   * node does not exist or if the node does not have any children.\n   *\n   * Used during master initialization to determine if the master is a\n   * failed-over-to master or the first master during initial cluster startup.\n   * If the directory for regionserver ephemeral nodes is empty then this is\n   * a cluster startup, if not then it is not cluster startup.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to check for children of\n   * @return true if node has children, false if not or node does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean nodeHasChildren(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    try {\n      return !zkw.getRecoverableZooKeeper().getChildren(znode, null).isEmpty();\n    } catch(KeeperException.NoNodeException ke) {\n      LOG.debug(zkw.prefix(\"Unable to list children of znode \" + znode +\n              \" because node does not exist (not an error)\"));\n      return false;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode), e);\n      zkw.keeperException(e);\n      return false;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode), e);\n      zkw.interruptedException(e);\n      return false;\n    }\n  }"
        ],
        [
            "LoadBalancerTracker::setBalancerOn(boolean)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 -\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "  /**\n   * Set the balancer on/off\n   * @param balancerOn\n   * @throws KeeperException\n   */\n  public void setBalancerOn(boolean balancerOn) throws KeeperException {\n  byte [] upData = toByteArray(balancerOn);\n    try {\n      ZKUtil.setData(watcher, watcher.znodePaths.balancerZNode, upData);\n    } catch(KeeperException.NoNodeException nne) {\n      ZKUtil.createAndWatch(watcher, watcher.znodePaths.balancerZNode, upData);\n    }\n    super.nodeDataChanged(watcher.znodePaths.balancerZNode);\n  }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67 +\n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  ",
            "  /**\n   * Set the balancer on/off.\n   *\n   * @param balancerOn true if the balancher should be on, false otherwise\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public void setBalancerOn(boolean balancerOn) throws KeeperException {\n    byte [] upData = toByteArray(balancerOn);\n\n    try {\n      ZKUtil.setData(watcher, watcher.znodePaths.balancerZNode, upData);\n    } catch(KeeperException.NoNodeException nne) {\n      ZKUtil.createAndWatch(watcher, watcher.znodePaths.balancerZNode, upData);\n    }\n    super.nodeDataChanged(watcher.znodePaths.balancerZNode);\n  }"
        ],
        [
            "MetaTableLocator::blockUntilAvailable(ZKWatcher,int,long)",
            " 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602 -\n 603 -\n 604 -\n 605 -\n 606 -\n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  ",
            "  /**\n   * Wait until the meta region is available and is not in transition.\n   * @param zkw\n   * @param replicaId\n   * @param timeout\n   * @return ServerName or null if we timed out.\n   * @throws InterruptedException\n   */\n  public ServerName blockUntilAvailable(final ZKWatcher zkw, int replicaId,\n                                        final long timeout)\n  throws InterruptedException {\n    if (timeout < 0) throw new IllegalArgumentException();\n    if (zkw == null) throw new IllegalArgumentException();\n    long startTime = System.currentTimeMillis();\n    ServerName sn = null;\n    while (true) {\n      sn = getMetaRegionLocation(zkw, replicaId);\n      if (sn != null || (System.currentTimeMillis() - startTime)\n          > timeout - HConstants.SOCKET_RETRY_WAIT_MS) {\n        break;\n      }\n      Thread.sleep(HConstants.SOCKET_RETRY_WAIT_MS);\n    }\n    return sn;\n  }",
            " 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613 +\n 614 +\n 615 +\n 616 +\n 617 +\n 618 +\n 619 +\n 620 +\n 621 +\n 622 +\n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  ",
            "  /**\n   * Wait until the meta region is available and is not in transition.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and constants\n   * @param replicaId the ID of the replica\n   * @param timeout maximum time to wait in millis\n   * @return ServerName or null if we timed out.\n   * @throws InterruptedException if waiting for the socket operation fails\n   */\n  public ServerName blockUntilAvailable(final ZKWatcher zkw, int replicaId, final long timeout)\n          throws InterruptedException {\n    if (timeout < 0) {\n      throw new IllegalArgumentException();\n    }\n\n    if (zkw == null) {\n      throw new IllegalArgumentException();\n    }\n\n    long startTime = System.currentTimeMillis();\n    ServerName sn = null;\n    while (true) {\n      sn = getMetaRegionLocation(zkw, replicaId);\n      if (sn != null || (System.currentTimeMillis() - startTime)\n          > timeout - HConstants.SOCKET_RETRY_WAIT_MS) {\n        break;\n      }\n      Thread.sleep(HConstants.SOCKET_RETRY_WAIT_MS);\n    }\n    return sn;\n  }"
        ],
        [
            "ZKNodeTracker::nodeCreated(String)",
            " 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  ",
            "  @Override\n  public synchronized void nodeCreated(String path) {\n    if (!path.equals(node)) return;\n    try {\n      byte [] data = ZKUtil.getDataAndWatch(watcher, node);\n      if (data != null) {\n        this.data = data;\n        notifyAll();\n      } else {\n        nodeDeleted(path);\n      }\n    } catch(KeeperException e) {\n      abortable.abort(\"Unexpected exception handling nodeCreated event\", e);\n    }\n  }",
            " 192  \n 193  \n 194 +\n 195 +\n 196 +\n 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  ",
            "  @Override\n  public synchronized void nodeCreated(String path) {\n    if (!path.equals(node)) {\n      return;\n    }\n\n    try {\n      byte [] data = ZKUtil.getDataAndWatch(watcher, node);\n      if (data != null) {\n        this.data = data;\n        notifyAll();\n      } else {\n        nodeDeleted(path);\n      }\n    } catch(KeeperException e) {\n      abortable.abort(\"Unexpected exception handling nodeCreated event\", e);\n    }\n  }"
        ],
        [
            "MetaTableLocator::getMetaServerConnection(ClusterConnection,ZKWatcher,long,int)",
            " 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363 -\n 364 -\n 365  \n 366  ",
            "  /**\n   * Gets a connection to the server hosting meta, as reported by ZooKeeper,\n   * waiting up to the specified timeout for availability.\n   * <p>WARNING: Does not retry.  Use an {@link org.apache.hadoop.hbase.client.HTable} instead.\n   * @param connection\n   * @param zkw\n   * @param timeout How long to wait on meta location\n   * @param replicaId\n   * @return connection to server hosting meta\n   * @throws InterruptedException\n   * @throws NotAllMetaRegionsOnlineException if timed out waiting\n   * @throws IOException\n   */\n  private AdminService.BlockingInterface getMetaServerConnection(ClusterConnection connection,\n                                                                 ZKWatcher zkw, long timeout, int replicaId)\n  throws InterruptedException, NotAllMetaRegionsOnlineException, IOException {\n    return getCachedConnection(connection, waitMetaRegionLocation(zkw, replicaId, timeout));\n  }",
            " 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 +\n 372  \n 373  ",
            "  /**\n   * Gets a connection to the server hosting meta, as reported by ZooKeeper, waiting up to the\n   * specified timeout for availability.\n   *\n   * <p>WARNING: Does not retry.  Use an {@link org.apache.hadoop.hbase.client.HTable} instead.\n   *\n   * @param connection the connection to use\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param timeout How long to wait on meta location\n   * @param replicaId the ID of the replica\n   * @return connection to server hosting meta\n   * @throws InterruptedException if waiting for the socket operation fails\n   * @throws IOException if the number of retries for getting the connection is exceeded\n   */\n  private AdminService.BlockingInterface getMetaServerConnection(ClusterConnection connection,\n      ZKWatcher zkw, long timeout, int replicaId) throws InterruptedException, IOException {\n    return getCachedConnection(connection, waitMetaRegionLocation(zkw, replicaId, timeout));\n  }"
        ],
        [
            "RecoverableZooKeeper::getData(String,Watcher,Stat)",
            " 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358 -\n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  ",
            "  /**\n   * getData is an idempotent operation. Retry before throwing exception\n   * @return Data\n   */\n  public byte[] getData(String path, Watcher watcher, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          byte[] revData = checkZk().getData(path, watcher, stat);\n          this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return ZKMetadata.removeMetaData(revData);\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 +\n 369 +\n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  ",
            "  /**\n   * getData is an idempotent operation. Retry before throwing exception\n   * @return Data\n   */\n  public byte[] getData(String path, Watcher watcher, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          byte[] revData = checkZk().getData(path, watcher, stat);\n          this.metrics.registerReadOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return ZKMetadata.removeMetaData(revData);\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "MetaTableLocator::verifyMetaRegionLocation(ClusterConnection,ZKWatcher,long)",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262 -\n 263 -\n 264 -\n 265  \n 266  ",
            "  /**\n   * Verify <code>hbase:meta</code> is deployed and accessible.\n   * @param hConnection\n   * @param zkw\n   * @param timeout How long to wait on zk for meta address (passed through to\n   * the internal call to {@link #getMetaServerConnection}.\n   * @return True if the <code>hbase:meta</code> location is healthy.\n   * @throws java.io.IOException\n   * @throws InterruptedException\n   */\n  public boolean verifyMetaRegionLocation(ClusterConnection hConnection,\n                                          ZKWatcher zkw, final long timeout)\n  throws InterruptedException, IOException {\n    return verifyMetaRegionLocation(hConnection, zkw, timeout, RegionInfo.DEFAULT_REPLICA_ID);\n  }",
            " 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 +\n 274 +\n 275  \n 276  ",
            "  /**\n   * Verify <code>hbase:meta</code> is deployed and accessible.\n   *\n   * @param hConnection the connection to use\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param timeout How long to wait on zk for meta address (passed through to\n   *                the internal call to {@link #getMetaServerConnection}.\n   * @return True if the <code>hbase:meta</code> location is healthy.\n   * @throws IOException if the number of retries for getting the connection is exceeded\n   * @throws InterruptedException if waiting for the socket operation fails\n   */\n  public boolean verifyMetaRegionLocation(ClusterConnection hConnection, ZKWatcher zkw,\n      final long timeout) throws InterruptedException, IOException {\n    return verifyMetaRegionLocation(hConnection, zkw, timeout, RegionInfo.DEFAULT_REPLICA_ID);\n  }"
        ],
        [
            "RecoverableZooKeeper::createNonSequential(String,byte,List,CreateMode)",
            " 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590 -\n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602 -\n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "  private String createNonSequential(String path, byte[] data, List<ACL> acl,\n      CreateMode createMode) throws KeeperException, InterruptedException {\n    RetryCounter retryCounter = retryCounterFactory.create();\n    boolean isRetry = false; // False for first attempt, true for all retries.\n    long startTime;\n    while (true) {\n      try {\n        startTime = EnvironmentEdgeManager.currentTime();\n        String nodePath = checkZk().create(path, data, acl, createMode);\n        this.metrics.registerWriteOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n        return nodePath;\n      } catch (KeeperException e) {\n        this.metrics.registerFailedZKCall();\n        switch (e.code()) {\n          case NODEEXISTS:\n            if (isRetry) {\n              // If the connection was lost, there is still a possibility that\n              // we have successfully created the node at our previous attempt,\n              // so we read the node and compare.\n              startTime = EnvironmentEdgeManager.currentTime();\n              byte[] currentData = checkZk().getData(path, false, null);\n              this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n              if (currentData != null &&\n                  Bytes.compareTo(currentData, data) == 0) {\n                // We successfully created a non-sequential node\n                return path;\n              }\n              LOG.error(\"Node \" + path + \" already exists with \" +\n                  Bytes.toStringBinary(currentData) + \", could not write \" +\n                  Bytes.toStringBinary(data));\n              throw e;\n            }\n            LOG.debug(\"Node \" + path + \" already exists\");\n            throw e;\n\n          case CONNECTIONLOSS:\n            this.metrics.registerConnectionLossException();\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n          case OPERATIONTIMEOUT:\n            this.metrics.registerOperationTimeoutException();\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n\n          default:\n            throw e;\n        }\n      }\n      retryCounter.sleepUntilNextRetry();\n      isRetry = true;\n    }\n  }",
            " 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606 +\n 607 +\n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619 +\n 620 +\n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  ",
            "  private String createNonSequential(String path, byte[] data, List<ACL> acl,\n      CreateMode createMode) throws KeeperException, InterruptedException {\n    RetryCounter retryCounter = retryCounterFactory.create();\n    boolean isRetry = false; // False for first attempt, true for all retries.\n    long startTime;\n    while (true) {\n      try {\n        startTime = EnvironmentEdgeManager.currentTime();\n        String nodePath = checkZk().create(path, data, acl, createMode);\n        this.metrics.registerWriteOperationLatency(\n                Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n        return nodePath;\n      } catch (KeeperException e) {\n        this.metrics.registerFailedZKCall();\n        switch (e.code()) {\n          case NODEEXISTS:\n            if (isRetry) {\n              // If the connection was lost, there is still a possibility that\n              // we have successfully created the node at our previous attempt,\n              // so we read the node and compare.\n              startTime = EnvironmentEdgeManager.currentTime();\n              byte[] currentData = checkZk().getData(path, false, null);\n              this.metrics.registerReadOperationLatency(\n                      Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n              if (currentData != null &&\n                  Bytes.compareTo(currentData, data) == 0) {\n                // We successfully created a non-sequential node\n                return path;\n              }\n              LOG.error(\"Node \" + path + \" already exists with \" +\n                  Bytes.toStringBinary(currentData) + \", could not write \" +\n                  Bytes.toStringBinary(data));\n              throw e;\n            }\n            LOG.debug(\"Node \" + path + \" already exists\");\n            throw e;\n\n          case CONNECTIONLOSS:\n            this.metrics.registerConnectionLossException();\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n          case OPERATIONTIMEOUT:\n            this.metrics.registerOperationTimeoutException();\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n\n          default:\n            throw e;\n        }\n      }\n      retryCounter.sleepUntilNextRetry();\n      isRetry = true;\n    }\n  }"
        ],
        [
            "MetaTableLocator::getListOfRegionInfos(List)",
            " 136 -\n 137 -\n 138 -\n 139  \n 140  \n 141  \n 142  \n 143  \n 144  ",
            "  private List<RegionInfo> getListOfRegionInfos(\n      final List<Pair<RegionInfo, ServerName>> pairs) {\n    if (pairs == null || pairs.isEmpty()) return Collections.EMPTY_LIST;\n    List<RegionInfo> result = new ArrayList<>(pairs.size());\n    for (Pair<RegionInfo, ServerName> pair: pairs) {\n      result.add(pair.getFirst());\n    }\n    return result;\n  }",
            " 140 +\n 141 +\n 142 +\n 143 +\n 144 +\n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "  private List<RegionInfo> getListOfRegionInfos(final List<Pair<RegionInfo, ServerName>> pairs) {\n    if (pairs == null || pairs.isEmpty()) {\n      return Collections.EMPTY_LIST;\n    }\n\n    List<RegionInfo> result = new ArrayList<>(pairs.size());\n    for (Pair<RegionInfo, ServerName> pair: pairs) {\n      result.add(pair.getFirst());\n    }\n    return result;\n  }"
        ],
        [
            "ZKWatcher::keeperException(KeeperException)",
            " 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577 -\n 578 -\n 579  \n 580  \n 581  ",
            "  /**\n   * Handles KeeperExceptions in client calls.\n   * <p>\n   * This may be temporary but for now this gives one place to deal with these.\n   * <p>\n   * TODO: Currently this method rethrows the exception to let the caller handle\n   * <p>\n   * @param ke\n   * @throws KeeperException\n   */\n  public void keeperException(KeeperException ke)\n  throws KeeperException {\n    LOG.error(prefix(\"Received unexpected KeeperException, re-throwing exception\"), ke);\n    throw ke;\n  }",
            " 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577 +\n 578  \n 579  \n 580  ",
            "  /**\n   * Handles KeeperExceptions in client calls.\n   * <p>\n   * This may be temporary but for now this gives one place to deal with these.\n   * <p>\n   * TODO: Currently this method rethrows the exception to let the caller handle\n   * <p>\n   * @param ke the exception to rethrow\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public void keeperException(KeeperException ke) throws KeeperException {\n    LOG.error(prefix(\"Received unexpected KeeperException, re-throwing exception\"), ke);\n    throw ke;\n  }"
        ],
        [
            "RecoverableZooKeeper::createSequential(String,byte,List,CreateMode)",
            " 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652 -\n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  ",
            "  private String createSequential(String path, byte[] data,\n      List<ACL> acl, CreateMode createMode)\n  throws KeeperException, InterruptedException {\n    RetryCounter retryCounter = retryCounterFactory.create();\n    boolean first = true;\n    String newPath = path+this.identifier;\n    while (true) {\n      try {\n        if (!first) {\n          // Check if we succeeded on a previous attempt\n          String previousResult = findPreviousSequentialNode(newPath);\n          if (previousResult != null) {\n            return previousResult;\n          }\n        }\n        first = false;\n        long startTime = EnvironmentEdgeManager.currentTime();\n        String nodePath = checkZk().create(newPath, data, acl, createMode);\n        this.metrics.registerWriteOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n        return nodePath;\n      } catch (KeeperException e) {\n        this.metrics.registerFailedZKCall();\n        switch (e.code()) {\n          case CONNECTIONLOSS:\n            this.metrics.registerConnectionLossException();\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n          case OPERATIONTIMEOUT:\n            this.metrics.registerOperationTimeoutException();\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n\n          default:\n            throw e;\n        }\n      }\n      retryCounter.sleepUntilNextRetry();\n    }\n  }",
            " 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670 +\n 671 +\n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  ",
            "  private String createSequential(String path, byte[] data,\n      List<ACL> acl, CreateMode createMode)\n  throws KeeperException, InterruptedException {\n    RetryCounter retryCounter = retryCounterFactory.create();\n    boolean first = true;\n    String newPath = path+this.identifier;\n    while (true) {\n      try {\n        if (!first) {\n          // Check if we succeeded on a previous attempt\n          String previousResult = findPreviousSequentialNode(newPath);\n          if (previousResult != null) {\n            return previousResult;\n          }\n        }\n        first = false;\n        long startTime = EnvironmentEdgeManager.currentTime();\n        String nodePath = checkZk().create(newPath, data, acl, createMode);\n        this.metrics.registerWriteOperationLatency(\n                Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n        return nodePath;\n      } catch (KeeperException e) {\n        this.metrics.registerFailedZKCall();\n        switch (e.code()) {\n          case CONNECTIONLOSS:\n            this.metrics.registerConnectionLossException();\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n          case OPERATIONTIMEOUT:\n            this.metrics.registerOperationTimeoutException();\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n\n          default:\n            throw e;\n        }\n      }\n      retryCounter.sleepUntilNextRetry();\n    }\n  }"
        ],
        [
            "ZKUtil::ZKUtilOp::SetData::equals(Object)",
            "1575  \n1576  \n1577 -\n1578 -\n1579  \n1580  \n1581  \n1582  ",
            "      @Override\n      public boolean equals(Object o) {\n        if (this == o) return true;\n        if (!(o instanceof SetData)) return false;\n\n        SetData op = (SetData) o;\n        return getPath().equals(op.getPath()) && Arrays.equals(data, op.data);\n      }",
            "1592  \n1593  \n1594 +\n1595 +\n1596 +\n1597 +\n1598 +\n1599 +\n1600  \n1601  \n1602  \n1603  ",
            "      @Override\n      public boolean equals(Object o) {\n        if (this == o) {\n          return true;\n        }\n        if (!(o instanceof SetData)) {\n          return false;\n        }\n\n        SetData op = (SetData) o;\n        return getPath().equals(op.getPath()) && Arrays.equals(data, op.data);\n      }"
        ],
        [
            "ZKUtil::toZooKeeperOp(ZKWatcher,ZKUtilOp)",
            "1592  \n1593  \n1594  \n1595 -\n1596 -\n1597 -\n1598  \n1599  \n1600  \n1601  \n1602  \n1603  \n1604  \n1605  \n1606  \n1607  \n1608  \n1609  \n1610  \n1611  \n1612  \n1613  ",
            "  /**\n   * Convert from ZKUtilOp to ZKOp\n   */\n  private static Op toZooKeeperOp(ZKWatcher zkw, ZKUtilOp op)\n  throws UnsupportedOperationException {\n    if(op == null) return null;\n\n    if (op instanceof CreateAndFailSilent) {\n      CreateAndFailSilent cafs = (CreateAndFailSilent)op;\n      return Op.create(cafs.getPath(), cafs.getData(), createACL(zkw, cafs.getPath()),\n        CreateMode.PERSISTENT);\n    } else if (op instanceof DeleteNodeFailSilent) {\n      DeleteNodeFailSilent dnfs = (DeleteNodeFailSilent)op;\n      return Op.delete(dnfs.getPath(), -1);\n    } else if (op instanceof SetData) {\n      SetData sd = (SetData)op;\n      return Op.setData(sd.getPath(), sd.getData(), -1);\n    } else {\n      throw new UnsupportedOperationException(\"Unexpected ZKUtilOp type: \"\n        + op.getClass().getName());\n    }\n  }",
            "1613  \n1614  \n1615  \n1616 +\n1617 +\n1618 +\n1619 +\n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  ",
            "  /**\n   * Convert from ZKUtilOp to ZKOp\n   */\n  private static Op toZooKeeperOp(ZKWatcher zkw, ZKUtilOp op) throws UnsupportedOperationException {\n    if(op == null) {\n      return null;\n    }\n\n    if (op instanceof CreateAndFailSilent) {\n      CreateAndFailSilent cafs = (CreateAndFailSilent)op;\n      return Op.create(cafs.getPath(), cafs.getData(), createACL(zkw, cafs.getPath()),\n        CreateMode.PERSISTENT);\n    } else if (op instanceof DeleteNodeFailSilent) {\n      DeleteNodeFailSilent dnfs = (DeleteNodeFailSilent)op;\n      return Op.delete(dnfs.getPath(), -1);\n    } else if (op instanceof SetData) {\n      SetData sd = (SetData)op;\n      return Op.setData(sd.getPath(), sd.getData(), -1);\n    } else {\n      throw new UnsupportedOperationException(\"Unexpected ZKUtilOp type: \"\n        + op.getClass().getName());\n    }\n  }"
        ],
        [
            "MetaTableLocator::waitMetaRegionLocation(ZKWatcher)",
            " 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237 -\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "  /**\n   * Waits indefinitely for availability of <code>hbase:meta</code>.  Used during\n   * cluster startup.  Does not verify meta, just that something has been\n   * set up in zk.\n   * @see #waitMetaRegionLocation(ZKWatcher, long)\n   * @throws InterruptedException if interrupted while waiting\n   */\n  public void waitMetaRegionLocation(ZKWatcher zkw) throws InterruptedException {\n    long startTime = System.currentTimeMillis();\n    while (!stopped) {\n      try {\n        if (waitMetaRegionLocation(zkw, 100) != null) break;\n        long sleepTime = System.currentTimeMillis() - startTime;\n        // +1 in case sleepTime=0\n        if ((sleepTime + 1) % 10000 == 0) {\n          LOG.warn(\"Have been waiting for meta to be assigned for \" + sleepTime + \"ms\");\n        }\n      } catch (NotAllMetaRegionsOnlineException e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"hbase:meta still not available, sleeping and retrying.\" +\n            \" Reason: \" + e.getMessage());\n        }\n      }\n    }\n  }",
            " 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244 +\n 245 +\n 246 +\n 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  ",
            "  /**\n   * Waits indefinitely for availability of <code>hbase:meta</code>.  Used during\n   * cluster startup.  Does not verify meta, just that something has been\n   * set up in zk.\n   * @see #waitMetaRegionLocation(ZKWatcher, long)\n   * @throws InterruptedException if interrupted while waiting\n   */\n  public void waitMetaRegionLocation(ZKWatcher zkw) throws InterruptedException {\n    long startTime = System.currentTimeMillis();\n    while (!stopped) {\n      try {\n        if (waitMetaRegionLocation(zkw, 100) != null) {\n          break;\n        }\n\n        long sleepTime = System.currentTimeMillis() - startTime;\n        // +1 in case sleepTime=0\n        if ((sleepTime + 1) % 10000 == 0) {\n          LOG.warn(\"Have been waiting for meta to be assigned for \" + sleepTime + \"ms\");\n        }\n      } catch (NotAllMetaRegionsOnlineException e) {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"hbase:meta still not available, sleeping and retrying.\" +\n            \" Reason: \" + e.getMessage());\n        }\n      }\n    }\n  }"
        ],
        [
            "ZKMainServer::hasServer(String)",
            "  78  \n  79  \n  80  \n  81  \n  82 -\n  83  \n  84  ",
            "  /**\n   * @param args\n   * @return True if argument strings have a '-server' in them.\n   */\n  private static boolean hasServer(final String args[]) {\n    return args.length > 0 && args[0].equals(SERVER_ARG);\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82 +\n  83  \n  84  ",
            "  /**\n   * @param args the arguments to check\n   * @return True if argument strings have a '-server' in them.\n   */\n  private static boolean hasServer(final String[] args) {\n    return args.length > 0 && args[0].equals(SERVER_ARG);\n  }"
        ],
        [
            "RecoverableZooKeeper::getData(String,boolean,Stat)",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393 -\n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  ",
            "  /**\n   * getData is an idempotent operation. Retry before throwing exception\n   * @return Data\n   */\n  public byte[] getData(String path, boolean watch, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          byte[] revData = checkZk().getData(path, watch, stat);\n          this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return ZKMetadata.removeMetaData(revData);\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404 +\n 405 +\n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  ",
            "  /**\n   * getData is an idempotent operation. Retry before throwing exception\n   * @return Data\n   */\n  public byte[] getData(String path, boolean watch, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          byte[] revData = checkZk().getData(path, watch, stat);\n          this.metrics.registerReadOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return ZKMetadata.removeMetaData(revData);\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::JaasConfiguration::getAppConfigurationEntry(String)",
            " 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298 -\n 299  \n 300  ",
            "    @Override\n    public AppConfigurationEntry[] getAppConfigurationEntry(String appName) {\n      if (loginContextName.equals(appName)) {\n        if (!useTicketCache) {\n          KEYTAB_KERBEROS_OPTIONS.put(\"keyTab\", keytabFile);\n          KEYTAB_KERBEROS_OPTIONS.put(\"useKeyTab\", \"true\");\n        }\n        KEYTAB_KERBEROS_OPTIONS.put(\"principal\", principal);\n        KEYTAB_KERBEROS_OPTIONS.put(\"useTicketCache\", useTicketCache ? \"true\" : \"false\");\n        return KEYTAB_KERBEROS_CONF;\n      }\n      if (baseConfig != null) return baseConfig.getAppConfigurationEntry(appName);\n      return(null);\n    }",
            " 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 +\n 305 +\n 306 +\n 307 +\n 308 +\n 309  \n 310  ",
            "    @Override\n    public AppConfigurationEntry[] getAppConfigurationEntry(String appName) {\n      if (loginContextName.equals(appName)) {\n        if (!useTicketCache) {\n          KEYTAB_KERBEROS_OPTIONS.put(\"keyTab\", keytabFile);\n          KEYTAB_KERBEROS_OPTIONS.put(\"useKeyTab\", \"true\");\n        }\n        KEYTAB_KERBEROS_OPTIONS.put(\"principal\", principal);\n        KEYTAB_KERBEROS_OPTIONS.put(\"useTicketCache\", useTicketCache ? \"true\" : \"false\");\n        return KEYTAB_KERBEROS_CONF;\n      }\n\n      if (baseConfig != null) {\n        return baseConfig.getAppConfigurationEntry(appName);\n      }\n\n      return(null);\n    }"
        ],
        [
            "ZKMainServer::hasCommandLineArguments(String)",
            "  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92 -\n  93  \n  94  \n  95  \n  96  ",
            "  /**\n   * @param args\n   * @return True if command-line arguments were passed.\n   */\n  private static boolean hasCommandLineArguments(final String args[]) {\n    if (hasServer(args)) {\n      if (args.length < 2) throw new IllegalStateException(\"-server param but no value\");\n      return args.length > 2;\n    }\n    return args.length > 0;\n  }",
            "  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92 +\n  93 +\n  94 +\n  95 +\n  96  \n  97  \n  98 +\n  99  \n 100  ",
            "  /**\n   * @param args the arguments to check for command-line arguments\n   * @return True if command-line arguments were passed.\n   */\n  private static boolean hasCommandLineArguments(final String[] args) {\n    if (hasServer(args)) {\n      if (args.length < 2) {\n        throw new IllegalStateException(\"-server param but no value\");\n      }\n\n      return args.length > 2;\n    }\n\n    return args.length > 0;\n  }"
        ],
        [
            "ZKUtil::multiOrSequential(ZKWatcher,List,boolean)",
            "1615  \n1616  \n1617  \n1618  \n1619  \n1620  \n1621  \n1622  \n1623  \n1624  \n1625  \n1626  \n1627  \n1628  \n1629  \n1630  \n1631  \n1632  \n1633  \n1634  \n1635  \n1636  \n1637  \n1638  \n1639 -\n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649 -\n1650 -\n1651 -\n1652 -\n1653 -\n1654 -\n1655 -\n1656 -\n1657 -\n1658 -\n1659 -\n1660 -\n1661 -\n1662 -\n1663 -\n1664  \n1665  \n1666  \n1667  \n1668  ",
            "  /**\n   * Use ZooKeeper's multi-update functionality.\n   *\n   * If all of the following are true:\n   * - runSequentialOnMultiFailure is true\n   * - on calling multi, we get a ZooKeeper exception that can be handled by a sequential call(*)\n   * Then:\n   * - we retry the operations one-by-one (sequentially)\n   *\n   * Note *: an example is receiving a NodeExistsException from a \"create\" call.  Without multi,\n   * a user could call \"createAndFailSilent\" to ensure that a node exists if they don't care who\n   * actually created the node (i.e. the NodeExistsException from ZooKeeper is caught).\n   * This will cause all operations in the multi to fail, however, because\n   * the NodeExistsException that zk.create throws will fail the multi transaction.\n   * In this case, if the previous conditions hold, the commands are run sequentially, which should\n   * result in the correct final state, but means that the operations will not run atomically.\n   *\n   * @throws KeeperException\n   */\n  public static void multiOrSequential(ZKWatcher zkw, List<ZKUtilOp> ops,\n                                       boolean runSequentialOnMultiFailure) throws KeeperException {\n    if (zkw.getConfiguration().get(\"hbase.zookeeper.useMulti\") != null) {\n      LOG.warn(\"hbase.zookeeper.useMulti is deprecated. Default to true always.\");\n    }\n    if (ops == null) return;\n\n    List<Op> zkOps = new LinkedList<>();\n    for (ZKUtilOp op : ops) {\n      zkOps.add(toZooKeeperOp(zkw, op));\n    }\n    try {\n      zkw.getRecoverableZooKeeper().multi(zkOps);\n    } catch (KeeperException ke) {\n      switch (ke.code()) {\n      case NODEEXISTS:\n      case NONODE:\n      case BADVERSION:\n      case NOAUTH:\n        // if we get an exception that could be solved by running sequentially\n        // (and the client asked us to), then break out and run sequentially\n        if (runSequentialOnMultiFailure) {\n          LOG.info(\"On call to ZK.multi, received exception: \" + ke.toString() + \".\"\n              + \"  Attempting to run operations sequentially because\"\n              + \" runSequentialOnMultiFailure is: \" + runSequentialOnMultiFailure + \".\");\n          processSequentially(zkw, ops);\n          break;\n        }\n      default:\n        throw ke;\n      }\n    } catch (InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n  }",
            "1637  \n1638  \n1639  \n1640  \n1641  \n1642  \n1643  \n1644  \n1645  \n1646  \n1647  \n1648  \n1649  \n1650  \n1651  \n1652  \n1653  \n1654  \n1655  \n1656  \n1657  \n1658  \n1659  \n1660  \n1661 +\n1662 +\n1663 +\n1664  \n1665  \n1666  \n1667  \n1668  \n1669  \n1670  \n1671  \n1672  \n1673 +\n1674 +\n1675 +\n1676 +\n1677 +\n1678 +\n1679 +\n1680 +\n1681 +\n1682 +\n1683 +\n1684 +\n1685 +\n1686 +\n1687 +\n1688  \n1689  \n1690  \n1691  \n1692  ",
            "  /**\n   * Use ZooKeeper's multi-update functionality.\n   *\n   * If all of the following are true:\n   * - runSequentialOnMultiFailure is true\n   * - on calling multi, we get a ZooKeeper exception that can be handled by a sequential call(*)\n   * Then:\n   * - we retry the operations one-by-one (sequentially)\n   *\n   * Note *: an example is receiving a NodeExistsException from a \"create\" call.  Without multi,\n   * a user could call \"createAndFailSilent\" to ensure that a node exists if they don't care who\n   * actually created the node (i.e. the NodeExistsException from ZooKeeper is caught).\n   * This will cause all operations in the multi to fail, however, because\n   * the NodeExistsException that zk.create throws will fail the multi transaction.\n   * In this case, if the previous conditions hold, the commands are run sequentially, which should\n   * result in the correct final state, but means that the operations will not run atomically.\n   *\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static void multiOrSequential(ZKWatcher zkw, List<ZKUtilOp> ops,\n                                       boolean runSequentialOnMultiFailure) throws KeeperException {\n    if (zkw.getConfiguration().get(\"hbase.zookeeper.useMulti\") != null) {\n      LOG.warn(\"hbase.zookeeper.useMulti is deprecated. Default to true always.\");\n    }\n    if (ops == null) {\n      return;\n    }\n\n    List<Op> zkOps = new LinkedList<>();\n    for (ZKUtilOp op : ops) {\n      zkOps.add(toZooKeeperOp(zkw, op));\n    }\n    try {\n      zkw.getRecoverableZooKeeper().multi(zkOps);\n    } catch (KeeperException ke) {\n      switch (ke.code()) {\n        case NODEEXISTS:\n        case NONODE:\n        case BADVERSION:\n        case NOAUTH:\n          // if we get an exception that could be solved by running sequentially\n          // (and the client asked us to), then break out and run sequentially\n          if (runSequentialOnMultiFailure) {\n            LOG.info(\"On call to ZK.multi, received exception: \" + ke.toString() + \".\"\n                + \"  Attempting to run operations sequentially because\"\n                + \" runSequentialOnMultiFailure is: \" + runSequentialOnMultiFailure + \".\");\n            processSequentially(zkw, ops);\n            break;\n          }\n        default:\n          throw ke;\n      }\n    } catch (InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::getAcl(String,Stat)",
            " 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487 -\n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  ",
            "  /**\n   * getAcl is an idempotent operation. Retry before throwing exception\n   * @return list of ACLs\n   */\n  public List<ACL> getAcl(String path, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getAcl\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<ACL> nodeACL = checkZk().getACL(path, stat);\n          this.metrics.registerReadOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return nodeACL;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getAcl\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getAcl\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501 +\n 502 +\n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  ",
            "  /**\n   * getAcl is an idempotent operation. Retry before throwing exception\n   * @return list of ACLs\n   */\n  public List<ACL> getAcl(String path, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getAcl\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<ACL> nodeACL = checkZk().getACL(path, stat);\n          this.metrics.registerReadOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return nodeACL;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"getAcl\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"getAcl\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "ZKMainServer::main(String)",
            "  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "  /**\n   * Run the tool.\n   * @param args Command line arguments. First arg is path to zookeepers file.\n   */\n  public static void main(String args[]) throws Exception {\n    String [] newArgs = args;\n    if (!hasServer(args)) {\n      // Add the zk ensemble from configuration if none passed on command-line.\n      Configuration conf = HBaseConfiguration.create();\n      String hostport = new ZKMainServer().parse(conf);\n      if (hostport != null && hostport.length() > 0) {\n        newArgs = new String[args.length + 2];\n        System.arraycopy(args, 0, newArgs, 2, args.length);\n        newArgs[0] = \"-server\";\n        newArgs[1] = hostport;\n      }\n    }\n    // If command-line arguments, run our hack so they are executed.\n    // ZOOKEEPER-1897 was committed to zookeeper-3.4.6 but elsewhere in this class we say\n    // 3.4.6 breaks command-processing; TODO.\n    if (hasCommandLineArguments(args)) {\n      HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain zkm =\n        new HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(newArgs);\n      zkm.runCmdLine();\n    } else {\n      ZooKeeperMain.main(newArgs);\n    }\n  }",
            " 102  \n 103  \n 104  \n 105  \n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "  /**\n   * Run the tool.\n   * @param args Command line arguments. First arg is path to zookeepers file.\n   */\n  public static void main(String[] args) throws Exception {\n    String [] newArgs = args;\n    if (!hasServer(args)) {\n      // Add the zk ensemble from configuration if none passed on command-line.\n      Configuration conf = HBaseConfiguration.create();\n      String hostport = new ZKMainServer().parse(conf);\n      if (hostport != null && hostport.length() > 0) {\n        newArgs = new String[args.length + 2];\n        System.arraycopy(args, 0, newArgs, 2, args.length);\n        newArgs[0] = \"-server\";\n        newArgs[1] = hostport;\n      }\n    }\n    // If command-line arguments, run our hack so they are executed.\n    // ZOOKEEPER-1897 was committed to zookeeper-3.4.6 but elsewhere in this class we say\n    // 3.4.6 breaks command-processing; TODO.\n    if (hasCommandLineArguments(args)) {\n      HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain zkm =\n        new HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(newArgs);\n      zkm.runCmdLine();\n    } else {\n      ZooKeeperMain.main(newArgs);\n    }\n  }"
        ],
        [
            "MasterAddressTracker::getMasterInfoPort(ZKWatcher)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177 -\n 178 -\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "  /**\n   * Get master info port.\n   * Use this instead of {@link #getMasterInfoPort()} if you do not have an\n   * instance of this tracker in your context.\n   * @param zkw ZKWatcher to use\n   * @return master info port in the the master address znode or null if no\n   * znode present.\n   * // TODO can't return null for 'int' return type. non-static verison returns 0\n   * @throws KeeperException\n   * @throws IOException\n   */\n  public static int getMasterInfoPort(final ZKWatcher zkw) throws KeeperException,\n      IOException {\n    byte[] data;\n    try {\n      data = ZKUtil.getData(zkw, zkw.znodePaths.masterAddressZNode);\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException();\n    }\n    // TODO javadoc claims we return null in this case. :/\n    if (data == null) {\n      throw new IOException(\"Can't get master address from ZooKeeper; znode data == null\");\n    }\n    try {\n      return parse(data).getInfoPort();\n    } catch (DeserializationException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    }\n  }",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "  /**\n   * Get master info port.\n   * Use this instead of {@link #getMasterInfoPort()} if you do not have an\n   * instance of this tracker in your context.\n   * @param zkw ZKWatcher to use\n   * @return master info port in the the master address znode or null if no\n   *         znode present.\n   *         // TODO can't return null for 'int' return type. non-static verison returns 0\n   * @throws KeeperException if a ZooKeeper operation fails\n   * @throws IOException if the address of the ZooKeeper master cannot be retrieved\n   */\n  public static int getMasterInfoPort(final ZKWatcher zkw) throws KeeperException, IOException {\n    byte[] data;\n    try {\n      data = ZKUtil.getData(zkw, zkw.znodePaths.masterAddressZNode);\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException();\n    }\n    // TODO javadoc claims we return null in this case. :/\n    if (data == null) {\n      throw new IOException(\"Can't get master address from ZooKeeper; znode data == null\");\n    }\n    try {\n      return parse(data).getInfoPort();\n    } catch (DeserializationException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    }\n  }"
        ],
        [
            "MiniZooKeeperCluster::startup(File,int)",
            " 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198 -\n 199 -\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229 -\n 230 -\n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237 -\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "  /**\n   * @param baseDir\n   * @param numZooKeeperServers\n   * @return ClientPort server bound to, -1 if there was a\n   *         binding problem and we couldn't pick another port.\n   * @throws IOException\n   * @throws InterruptedException\n   */\n  public int startup(File baseDir, int numZooKeeperServers) throws IOException,\n      InterruptedException {\n    if (numZooKeeperServers <= 0)\n      return -1;\n\n    setupTestEnv();\n    shutdown();\n\n    int tentativePort = -1; // the seed port\n    int currentClientPort;\n\n    // running all the ZK servers\n    for (int i = 0; i < numZooKeeperServers; i++) {\n      File dir = new File(baseDir, \"zookeeper_\"+i).getAbsoluteFile();\n      createDir(dir);\n      int tickTimeToUse;\n      if (this.tickTime > 0) {\n        tickTimeToUse = this.tickTime;\n      } else {\n        tickTimeToUse = TICK_TIME;\n      }\n\n      // Set up client port - if we have already had a list of valid ports, use it.\n      if (hasValidClientPortInList(i)) {\n        currentClientPort = clientPortList.get(i);\n      } else {\n        tentativePort = selectClientPort(tentativePort); // update the seed\n        currentClientPort = tentativePort;\n      }\n\n      ZooKeeperServer server = new ZooKeeperServer(dir, dir, tickTimeToUse);\n      // Setting {min,max}SessionTimeout defaults to be the same as in Zookeeper\n      server.setMinSessionTimeout(configuration.getInt(\"hbase.zookeeper.property.minSessionTimeout\", -1));\n      server.setMaxSessionTimeout(configuration.getInt(\"hbase.zookeeper.property.maxSessionTimeout\", -1));\n      NIOServerCnxnFactory standaloneServerFactory;\n      while (true) {\n        try {\n          standaloneServerFactory = new NIOServerCnxnFactory();\n          standaloneServerFactory.configure(\n            new InetSocketAddress(currentClientPort),\n            configuration.getInt(HConstants.ZOOKEEPER_MAX_CLIENT_CNXNS, HConstants.DEFAULT_ZOOKEPER_MAX_CLIENT_CNXNS));\n        } catch (BindException e) {\n          LOG.debug(\"Failed binding ZK Server to client port: \" +\n              currentClientPort, e);\n          // We're told to use some port but it's occupied, fail\n          if (hasValidClientPortInList(i)) {\n            return -1;\n          }\n          // This port is already in use, try to use another.\n          tentativePort = selectClientPort(tentativePort);\n          currentClientPort = tentativePort;\n          continue;\n        }\n        break;\n      }\n\n      // Start up this ZK server\n      standaloneServerFactory.startup(server);\n      // Runs a 'stat' against the servers.\n      if (!waitForServerUp(currentClientPort, connectionTimeout)) {\n        throw new IOException(\"Waiting for startup of standalone server\");\n      }\n\n      // We have selected a port as a client port.  Update clientPortList if necessary.\n      if (clientPortList.size() <= i) { // it is not in the list, add the port\n        clientPortList.add(currentClientPort);\n      }\n      else if (clientPortList.get(i) <= 0) { // the list has invalid port, update with valid port\n        clientPortList.remove(i);\n        clientPortList.add(i, currentClientPort);\n      }\n\n      standaloneServerFactoryList.add(standaloneServerFactory);\n      zooKeeperServers.add(server);\n    }\n\n    // set the first one to be active ZK; Others are backups\n    activeZKServerIndex = 0;\n    started = true;\n    int clientPort = clientPortList.get(activeZKServerIndex);\n    LOG.info(\"Started MiniZooKeeperCluster and ran successful 'stat' \" +\n        \"on client port=\" + clientPort);\n    return clientPort;\n  }",
            " 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198 +\n 199 +\n 200  \n 201 +\n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231 +\n 232 +\n 233 +\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 +\n 241 +\n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  ",
            "  /**\n   * @param baseDir the base directory to use\n   * @param numZooKeeperServers the number of ZooKeeper servers\n   * @return ClientPort server bound to, -1 if there was a binding problem and we couldn't pick\n   *         another port.\n   * @throws IOException if an operation fails during the startup\n   * @throws InterruptedException if the startup fails\n   */\n  public int startup(File baseDir, int numZooKeeperServers) throws IOException,\n          InterruptedException {\n    if (numZooKeeperServers <= 0) {\n      return -1;\n    }\n\n    setupTestEnv();\n    shutdown();\n\n    int tentativePort = -1; // the seed port\n    int currentClientPort;\n\n    // running all the ZK servers\n    for (int i = 0; i < numZooKeeperServers; i++) {\n      File dir = new File(baseDir, \"zookeeper_\"+i).getAbsoluteFile();\n      createDir(dir);\n      int tickTimeToUse;\n      if (this.tickTime > 0) {\n        tickTimeToUse = this.tickTime;\n      } else {\n        tickTimeToUse = TICK_TIME;\n      }\n\n      // Set up client port - if we have already had a list of valid ports, use it.\n      if (hasValidClientPortInList(i)) {\n        currentClientPort = clientPortList.get(i);\n      } else {\n        tentativePort = selectClientPort(tentativePort); // update the seed\n        currentClientPort = tentativePort;\n      }\n\n      ZooKeeperServer server = new ZooKeeperServer(dir, dir, tickTimeToUse);\n      // Setting {min,max}SessionTimeout defaults to be the same as in Zookeeper\n      server.setMinSessionTimeout(configuration.getInt(\n              \"hbase.zookeeper.property.minSessionTimeout\", -1));\n      server.setMaxSessionTimeout(configuration.getInt(\n              \"hbase.zookeeper.property.maxSessionTimeout\", -1));\n      NIOServerCnxnFactory standaloneServerFactory;\n      while (true) {\n        try {\n          standaloneServerFactory = new NIOServerCnxnFactory();\n          standaloneServerFactory.configure(\n            new InetSocketAddress(currentClientPort),\n            configuration.getInt(HConstants.ZOOKEEPER_MAX_CLIENT_CNXNS,\n                    HConstants.DEFAULT_ZOOKEPER_MAX_CLIENT_CNXNS));\n        } catch (BindException e) {\n          LOG.debug(\"Failed binding ZK Server to client port: \" +\n              currentClientPort, e);\n          // We're told to use some port but it's occupied, fail\n          if (hasValidClientPortInList(i)) {\n            return -1;\n          }\n          // This port is already in use, try to use another.\n          tentativePort = selectClientPort(tentativePort);\n          currentClientPort = tentativePort;\n          continue;\n        }\n        break;\n      }\n\n      // Start up this ZK server\n      standaloneServerFactory.startup(server);\n      // Runs a 'stat' against the servers.\n      if (!waitForServerUp(currentClientPort, connectionTimeout)) {\n        throw new IOException(\"Waiting for startup of standalone server\");\n      }\n\n      // We have selected a port as a client port.  Update clientPortList if necessary.\n      if (clientPortList.size() <= i) { // it is not in the list, add the port\n        clientPortList.add(currentClientPort);\n      }\n      else if (clientPortList.get(i) <= 0) { // the list has invalid port, update with valid port\n        clientPortList.remove(i);\n        clientPortList.add(i, currentClientPort);\n      }\n\n      standaloneServerFactoryList.add(standaloneServerFactory);\n      zooKeeperServers.add(server);\n    }\n\n    // set the first one to be active ZK; Others are backups\n    activeZKServerIndex = 0;\n    started = true;\n    int clientPort = clientPortList.get(activeZKServerIndex);\n    LOG.info(\"Started MiniZooKeeperCluster and ran successful 'stat' \" +\n        \"on client port=\" + clientPort);\n    return clientPort;\n  }"
        ],
        [
            "ZKUtil::logRetrievedMsg(ZKWatcher,String,byte,boolean)",
            "1930  \n1931  \n1932 -\n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  ",
            "  private static void logRetrievedMsg(final ZKWatcher zkw,\n      final String znode, final byte [] data, final boolean watcherSet) {\n    if (!LOG.isTraceEnabled()) return;\n    LOG.trace(zkw.prefix(\"Retrieved \" + ((data == null)? 0: data.length) +\n      \" byte(s) of data from znode \" + znode +\n      (watcherSet? \" and set watcher; \": \"; data=\") +\n      (data == null? \"null\": data.length == 0? \"empty\": (\n          znode.startsWith(zkw.znodePaths.metaZNodePrefix)?\n            getServerNameOrEmptyString(data):\n          znode.startsWith(zkw.znodePaths.backupMasterAddressesZNode)?\n            getServerNameOrEmptyString(data):\n          StringUtils.abbreviate(Bytes.toStringBinary(data), 32)))));\n  }",
            "1962  \n1963  \n1964 +\n1965 +\n1966 +\n1967 +\n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  ",
            "  private static void logRetrievedMsg(final ZKWatcher zkw,\n      final String znode, final byte [] data, final boolean watcherSet) {\n    if (!LOG.isTraceEnabled()) {\n      return;\n    }\n\n    LOG.trace(zkw.prefix(\"Retrieved \" + ((data == null)? 0: data.length) +\n      \" byte(s) of data from znode \" + znode +\n      (watcherSet? \" and set watcher; \": \"; data=\") +\n      (data == null? \"null\": data.length == 0? \"empty\": (\n          znode.startsWith(zkw.znodePaths.metaZNodePrefix)?\n            getServerNameOrEmptyString(data):\n          znode.startsWith(zkw.znodePaths.backupMasterAddressesZNode)?\n            getServerNameOrEmptyString(data):\n          StringUtils.abbreviate(Bytes.toStringBinary(data), 32)))));\n  }"
        ],
        [
            "MetaTableLocator::getMetaRegionState(ZKWatcher,int)",
            " 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494 -\n 495 -\n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  ",
            "  /**\n   * Load the meta region state from the meta server ZNode.\n   * @param zkw\n   * @param replicaId\n   * @return regionstate\n   * @throws KeeperException\n   */\n  public static RegionState getMetaRegionState(ZKWatcher zkw, int replicaId)\n      throws KeeperException {\n    RegionState.State state = RegionState.State.OPEN;\n    ServerName serverName = null;\n    try {\n      byte[] data = ZKUtil.getData(zkw, zkw.znodePaths.getZNodeForReplica(replicaId));\n      if (data != null && data.length > 0 && ProtobufUtil.isPBMagicPrefix(data)) {\n        try {\n          int prefixLen = ProtobufUtil.lengthOfPBMagic();\n          ZooKeeperProtos.MetaRegionServer rl =\n            ZooKeeperProtos.MetaRegionServer.PARSER.parseFrom\n              (data, prefixLen, data.length - prefixLen);\n          if (rl.hasState()) {\n            state = RegionState.State.convert(rl.getState());\n          }\n          HBaseProtos.ServerName sn = rl.getServer();\n          serverName = ServerName.valueOf(\n            sn.getHostName(), sn.getPort(), sn.getStartCode());\n        } catch (InvalidProtocolBufferException e) {\n          throw new DeserializationException(\"Unable to parse meta region location\");\n        }\n      } else {\n        // old style of meta region location?\n        serverName = ProtobufUtil.parseServerNameFrom(data);\n      }\n    } catch (DeserializationException e) {\n      throw ZKUtil.convert(e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    if (serverName == null) {\n      state = RegionState.State.OFFLINE;\n    }\n    return new RegionState(\n        RegionReplicaUtil.getRegionInfoForReplica(\n            RegionInfoBuilder.FIRST_META_REGIONINFO, replicaId),\n        state, serverName);\n  }",
            " 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492 +\n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501 +\n 502 +\n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  ",
            "  /**\n   * Load the meta region state from the meta server ZNode.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param replicaId the ID of the replica\n   * @return regionstate\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static RegionState getMetaRegionState(ZKWatcher zkw, int replicaId)\n          throws KeeperException {\n    RegionState.State state = RegionState.State.OPEN;\n    ServerName serverName = null;\n    try {\n      byte[] data = ZKUtil.getData(zkw, zkw.znodePaths.getZNodeForReplica(replicaId));\n      if (data != null && data.length > 0 && ProtobufUtil.isPBMagicPrefix(data)) {\n        try {\n          int prefixLen = ProtobufUtil.lengthOfPBMagic();\n          ZooKeeperProtos.MetaRegionServer rl =\n            ZooKeeperProtos.MetaRegionServer.PARSER.parseFrom(data, prefixLen,\n                    data.length - prefixLen);\n          if (rl.hasState()) {\n            state = RegionState.State.convert(rl.getState());\n          }\n          HBaseProtos.ServerName sn = rl.getServer();\n          serverName = ServerName.valueOf(\n            sn.getHostName(), sn.getPort(), sn.getStartCode());\n        } catch (InvalidProtocolBufferException e) {\n          throw new DeserializationException(\"Unable to parse meta region location\");\n        }\n      } else {\n        // old style of meta region location?\n        serverName = ProtobufUtil.parseServerNameFrom(data);\n      }\n    } catch (DeserializationException e) {\n      throw ZKUtil.convert(e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    if (serverName == null) {\n      state = RegionState.State.OFFLINE;\n    }\n    return new RegionState(\n        RegionReplicaUtil.getRegionInfoForReplica(\n            RegionInfoBuilder.FIRST_META_REGIONINFO, replicaId),\n        state, serverName);\n  }"
        ],
        [
            "RecoverableZooKeeper::setAcl(String,List,int)",
            " 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522 -\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  ",
            "  /**\n   * setAcl is an idempotent operation. Retry before throwing exception\n   * @return list of ACLs\n   */\n  public Stat setAcl(String path, List<ACL> acls, int version)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.setAcl\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().setACL(path, acls, version);\n          this.metrics.registerWriteOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return nodeStat;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"setAcl\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"setAcl\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537 +\n 538 +\n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  ",
            "  /**\n   * setAcl is an idempotent operation. Retry before throwing exception\n   * @return list of ACLs\n   */\n  public Stat setAcl(String path, List<ACL> acls, int version)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.setAcl\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().setACL(path, acls, version);\n          this.metrics.registerWriteOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return nodeStat;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"setAcl\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"setAcl\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "ZKServerTool::main(String)",
            "  55  \n  56  \n  57  \n  58  \n  59 -\n  60  \n  61  \n  62  \n  63  \n  64  ",
            "  /**\n   * Run the tool.\n   * @param args Command line arguments.\n   */\n  public static void main(String args[]) {\n    for(ServerName server: readZKNodes(HBaseConfiguration.create())) {\n      // bin/zookeeper.sh relies on the \"ZK host\" string for grepping which is case sensitive.\n      System.out.println(\"ZK host: \" + server.getHostname());\n    }\n  }",
            "  58  \n  59  \n  60  \n  61  \n  62 +\n  63  \n  64  \n  65  \n  66  \n  67  ",
            "  /**\n   * Run the tool.\n   * @param args Command line arguments.\n   */\n  public static void main(String[] args) {\n    for(ServerName server: readZKNodes(HBaseConfiguration.create())) {\n      // bin/zookeeper.sh relies on the \"ZK host\" string for grepping which is case sensitive.\n      System.out.println(\"ZK host: \" + server.getHostname());\n    }\n  }"
        ],
        [
            "ZKUtil::login(Configuration,String,String,String,String,String)",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 -\n 202  \n 203  \n 204  \n 205  \n 206 -\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "  /**\n   * Log in the current process using the given configuration keys for the\n   * credential file and login principal.\n   *\n   * <p><strong>This is only applicable when running on secure hbase</strong>\n   * On regular HBase (without security features), this will safely be ignored.\n   * </p>\n   *\n   * @param conf The configuration data to use\n   * @param keytabFileKey Property key used to configure the path to the credential file\n   * @param userNameKey Property key used to configure the login principal\n   * @param hostname Current hostname to use in any credentials\n   * @param loginContextProperty property name to expose the entry name\n   * @param loginContextName jaas entry name\n   * @throws IOException underlying exception from SecurityUtil.login() call\n   */\n  private static void login(Configuration conf, String keytabFileKey,\n      String userNameKey, String hostname,\n      String loginContextProperty, String loginContextName)\n      throws IOException {\n    if (!isSecureZooKeeper(conf))\n      return;\n\n    // User has specified a jaas.conf, keep this one as the good one.\n    // HBASE_OPTS=\"-Djava.security.auth.login.config=jaas.conf\"\n    if (System.getProperty(\"java.security.auth.login.config\") != null)\n      return;\n\n    // No keytab specified, no auth\n    String keytabFilename = conf.get(keytabFileKey);\n    if (keytabFilename == null) {\n      LOG.warn(\"no keytab specified for: \" + keytabFileKey);\n      return;\n    }\n\n    String principalConfig = conf.get(userNameKey, System.getProperty(\"user.name\"));\n    String principalName = SecurityUtil.getServerPrincipal(principalConfig, hostname);\n\n    // Initialize the \"jaas.conf\" for keyTab/principal,\n    // If keyTab is not specified use the Ticket Cache.\n    // and set the zookeeper login context name.\n    JaasConfiguration jaasConf = new JaasConfiguration(loginContextName,\n        principalName, keytabFilename);\n    javax.security.auth.login.Configuration.setConfiguration(jaasConf);\n    System.setProperty(loginContextProperty, loginContextName);\n  }",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207 +\n 208  \n 209  \n 210  \n 211 +\n 212  \n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  ",
            "  /**\n   * Log in the current process using the given configuration keys for the\n   * credential file and login principal.\n   *\n   * <p><strong>This is only applicable when running on secure hbase</strong>\n   * On regular HBase (without security features), this will safely be ignored.\n   * </p>\n   *\n   * @param conf The configuration data to use\n   * @param keytabFileKey Property key used to configure the path to the credential file\n   * @param userNameKey Property key used to configure the login principal\n   * @param hostname Current hostname to use in any credentials\n   * @param loginContextProperty property name to expose the entry name\n   * @param loginContextName jaas entry name\n   * @throws IOException underlying exception from SecurityUtil.login() call\n   */\n  private static void login(Configuration conf, String keytabFileKey,\n      String userNameKey, String hostname,\n      String loginContextProperty, String loginContextName)\n      throws IOException {\n    if (!isSecureZooKeeper(conf)) {\n      return;\n    }\n\n    // User has specified a jaas.conf, keep this one as the good one.\n    // HBASE_OPTS=\"-Djava.security.auth.login.config=jaas.conf\"\n    if (System.getProperty(\"java.security.auth.login.config\") != null) {\n      return;\n    }\n\n    // No keytab specified, no auth\n    String keytabFilename = conf.get(keytabFileKey);\n    if (keytabFilename == null) {\n      LOG.warn(\"no keytab specified for: \" + keytabFileKey);\n      return;\n    }\n\n    String principalConfig = conf.get(userNameKey, System.getProperty(\"user.name\"));\n    String principalName = SecurityUtil.getServerPrincipal(principalConfig, hostname);\n\n    // Initialize the \"jaas.conf\" for keyTab/principal,\n    // If keyTab is not specified use the Ticket Cache.\n    // and set the zookeeper login context name.\n    JaasConfiguration jaasConf = new JaasConfiguration(loginContextName,\n        principalName, keytabFilename);\n    javax.security.auth.login.Configuration.setConfiguration(jaasConf);\n    System.setProperty(loginContextProperty, loginContextName);\n  }"
        ],
        [
            "ZKUtil::logZKTree(ZKWatcher,String)",
            "2006  \n2007  \n2008  \n2009  \n2010  \n2011 -\n2012  \n2013  \n2014  \n2015  \n2016  \n2017  \n2018  \n2019  \n2020  ",
            "  /**\n   * Recursively print the current state of ZK (non-transactional)\n   * @param root name of the root directory in zk to print\n   */\n  public static void logZKTree(ZKWatcher zkw, String root) {\n    if (!LOG.isDebugEnabled()) return;\n    LOG.debug(\"Current zk system:\");\n    String prefix = \"|-\";\n    LOG.debug(prefix + root);\n    try {\n      logZKTree(zkw, root, prefix);\n    } catch (KeeperException e) {\n      throw new RuntimeException(e);\n    }\n  }",
            "2041  \n2042  \n2043  \n2044  \n2045  \n2046 +\n2047 +\n2048 +\n2049 +\n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  ",
            "  /**\n   * Recursively print the current state of ZK (non-transactional)\n   * @param root name of the root directory in zk to print\n   */\n  public static void logZKTree(ZKWatcher zkw, String root) {\n    if (!LOG.isDebugEnabled()) {\n      return;\n    }\n\n    LOG.debug(\"Current zk system:\");\n    String prefix = \"|-\";\n    LOG.debug(prefix + root);\n    try {\n      logZKTree(zkw, root, prefix);\n    } catch (KeeperException e) {\n      throw new RuntimeException(e);\n    }\n  }"
        ],
        [
            "MiniZooKeeperCluster::killCurrentActiveZooKeeperServer()",
            " 324  \n 325  \n 326  \n 327  \n 328  \n 329 -\n 330 -\n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  ",
            "  /**@return clientPort return clientPort if there is another ZK backup can run\n   *         when killing the current active; return -1, if there is no backups.\n   * @throws IOException\n   * @throws InterruptedException\n   */\n  public int killCurrentActiveZooKeeperServer() throws IOException,\n                                        InterruptedException {\n    if (!started || activeZKServerIndex < 0) {\n      return -1;\n    }\n\n    // Shutdown the current active one\n    NIOServerCnxnFactory standaloneServerFactory =\n      standaloneServerFactoryList.get(activeZKServerIndex);\n    int clientPort = clientPortList.get(activeZKServerIndex);\n\n    standaloneServerFactory.shutdown();\n    if (!waitForServerDown(clientPort, connectionTimeout)) {\n      throw new IOException(\"Waiting for shutdown of standalone server\");\n    }\n\n    zooKeeperServers.get(activeZKServerIndex).getZKDatabase().close();\n\n    // remove the current active zk server\n    standaloneServerFactoryList.remove(activeZKServerIndex);\n    clientPortList.remove(activeZKServerIndex);\n    zooKeeperServers.remove(activeZKServerIndex);\n    LOG.info(\"Kill the current active ZK servers in the cluster \" +\n        \"on client port: \" + clientPort);\n\n    if (standaloneServerFactoryList.isEmpty()) {\n      // there is no backup servers;\n      return -1;\n    }\n    clientPort = clientPortList.get(activeZKServerIndex);\n    LOG.info(\"Activate a backup zk server in the cluster \" +\n        \"on client port: \" + clientPort);\n    // return the next back zk server's port\n    return clientPort;\n  }",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333 +\n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  ",
            "  /**\n   * @return clientPort return clientPort if there is another ZK backup can run\n   *         when killing the current active; return -1, if there is no backups.\n   * @throws IOException if waiting for the shutdown of a server fails\n   */\n  public int killCurrentActiveZooKeeperServer() throws IOException, InterruptedException {\n    if (!started || activeZKServerIndex < 0) {\n      return -1;\n    }\n\n    // Shutdown the current active one\n    NIOServerCnxnFactory standaloneServerFactory =\n      standaloneServerFactoryList.get(activeZKServerIndex);\n    int clientPort = clientPortList.get(activeZKServerIndex);\n\n    standaloneServerFactory.shutdown();\n    if (!waitForServerDown(clientPort, connectionTimeout)) {\n      throw new IOException(\"Waiting for shutdown of standalone server\");\n    }\n\n    zooKeeperServers.get(activeZKServerIndex).getZKDatabase().close();\n\n    // remove the current active zk server\n    standaloneServerFactoryList.remove(activeZKServerIndex);\n    clientPortList.remove(activeZKServerIndex);\n    zooKeeperServers.remove(activeZKServerIndex);\n    LOG.info(\"Kill the current active ZK servers in the cluster \" +\n        \"on client port: \" + clientPort);\n\n    if (standaloneServerFactoryList.isEmpty()) {\n      // there is no backup servers;\n      return -1;\n    }\n    clientPort = clientPortList.get(activeZKServerIndex);\n    LOG.info(\"Activate a backup zk server in the cluster \" +\n        \"on client port: \" + clientPort);\n    // return the next back zk server's port\n    return clientPort;\n  }"
        ],
        [
            "ZKUtil::createAndWatch(ZKWatcher,String,byte)",
            "1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072 -\n1073  \n1074  \n1075  \n1076  \n1077  ",
            "  /**\n   * Creates the specified node with the specified data and watches it.\n   *\n   * <p>Throws an exception if the node already exists.\n   *\n   * <p>The node created is persistent and open access.\n   *\n   * <p>Returns the version number of the created node if successful.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to create\n   * @param data data of node to create\n   * @return version of node created\n   * @throws KeeperException if unexpected zookeeper exception\n   * @throws KeeperException.NodeExistsException if node already exists\n   */\n  public static int createAndWatch(ZKWatcher zkw,\n      String znode, byte [] data)\n  throws KeeperException, KeeperException.NodeExistsException {\n    try {\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.PERSISTENT);\n      Stat stat = zkw.getRecoverableZooKeeper().exists(znode, zkw);\n      if (stat == null){\n        // Likely a race condition. Someone deleted the znode.\n        throw KeeperException.create(KeeperException.Code.SYSTEMERROR,\n            \"ZK.exists returned null (i.e.: znode does not exist) for znode=\" + znode);\n      }\n     return stat.getVersion();\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return -1;\n    }\n  }",
            "1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079 +\n1080 +\n1081  \n1082  \n1083  \n1084  \n1085  ",
            "  /**\n   * Creates the specified node with the specified data and watches it.\n   *\n   * <p>Throws an exception if the node already exists.\n   *\n   * <p>The node created is persistent and open access.\n   *\n   * <p>Returns the version number of the created node if successful.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to create\n   * @param data data of node to create\n   * @return version of node created\n   * @throws KeeperException if unexpected zookeeper exception\n   * @throws KeeperException.NodeExistsException if node already exists\n   */\n  public static int createAndWatch(ZKWatcher zkw,\n      String znode, byte [] data)\n  throws KeeperException, KeeperException.NodeExistsException {\n    try {\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.PERSISTENT);\n      Stat stat = zkw.getRecoverableZooKeeper().exists(znode, zkw);\n      if (stat == null){\n        // Likely a race condition. Someone deleted the znode.\n        throw KeeperException.create(KeeperException.Code.SYSTEMERROR,\n            \"ZK.exists returned null (i.e.: znode does not exist) for znode=\" + znode);\n      }\n\n      return stat.getVersion();\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return -1;\n    }\n  }"
        ],
        [
            "HQuorumPeer::runZKServer(QuorumPeerConfig)",
            "  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "  private static void runZKServer(QuorumPeerConfig zkConfig) throws UnknownHostException, IOException {\n    if (zkConfig.isDistributed()) {\n      QuorumPeerMain qp = new QuorumPeerMain();\n      qp.runFromConfig(zkConfig);\n    } else {\n      ZooKeeperServerMain zk = new ZooKeeperServerMain();\n      ServerConfig serverConfig = new ServerConfig();\n      serverConfig.readFrom(zkConfig);\n      zk.runFromConfig(serverConfig);\n    }\n  }",
            "  85 +\n  86 +\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "  private static void runZKServer(QuorumPeerConfig zkConfig)\n          throws UnknownHostException, IOException {\n    if (zkConfig.isDistributed()) {\n      QuorumPeerMain qp = new QuorumPeerMain();\n      qp.runFromConfig(zkConfig);\n    } else {\n      ZooKeeperServerMain zk = new ZooKeeperServerMain();\n      ServerConfig serverConfig = new ServerConfig();\n      serverConfig.readFrom(zkConfig);\n      zk.runFromConfig(serverConfig);\n    }\n  }"
        ],
        [
            "MetaTableLocator::blockUntilAvailable(ZKWatcher,long,Configuration)",
            " 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556 -\n 557 -\n 558 -\n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565 -\n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  ",
            "  /**\n   * Wait until the primary meta region is available. Get the secondary\n   * locations as well but don't block for those.\n   * @param zkw\n   * @param timeout\n   * @param conf\n   * @return ServerName or null if we timed out.\n   * @throws InterruptedException\n   */\n  public List<ServerName> blockUntilAvailable(final ZKWatcher zkw,\n      final long timeout, Configuration conf)\n          throws InterruptedException {\n    int numReplicasConfigured = 1;\n\n    List<ServerName> servers = new ArrayList<>();\n    // Make the blocking call first so that we do the wait to know\n    // the znodes are all in place or timeout.\n    ServerName server = blockUntilAvailable(zkw, timeout);\n    if (server == null) return null;\n    servers.add(server);\n\n    try {\n      List<String> metaReplicaNodes = zkw.getMetaReplicaNodes();\n      numReplicasConfigured = metaReplicaNodes.size();\n    } catch (KeeperException e) {\n      LOG.warn(\"Got ZK exception \" + e);\n    }\n    for (int replicaId = 1; replicaId < numReplicasConfigured; replicaId++) {\n      // return all replica locations for the meta\n      servers.add(getMetaRegionLocation(zkw, replicaId));\n    }\n    return servers;\n  }",
            " 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564 +\n 565 +\n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572 +\n 573 +\n 574 +\n 575 +\n 576 +\n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  ",
            "  /**\n   * Wait until the primary meta region is available. Get the secondary locations as well but don't\n   * block for those.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param timeout maximum time to wait in millis\n   * @param conf the {@link Configuration} to use\n   * @return ServerName or null if we timed out.\n   * @throws InterruptedException if waiting for the socket operation fails\n   */\n  public List<ServerName> blockUntilAvailable(final ZKWatcher zkw, final long timeout,\n      Configuration conf) throws InterruptedException {\n    int numReplicasConfigured = 1;\n\n    List<ServerName> servers = new ArrayList<>();\n    // Make the blocking call first so that we do the wait to know\n    // the znodes are all in place or timeout.\n    ServerName server = blockUntilAvailable(zkw, timeout);\n\n    if (server == null) {\n      return null;\n    }\n\n    servers.add(server);\n\n    try {\n      List<String> metaReplicaNodes = zkw.getMetaReplicaNodes();\n      numReplicasConfigured = metaReplicaNodes.size();\n    } catch (KeeperException e) {\n      LOG.warn(\"Got ZK exception \" + e);\n    }\n    for (int replicaId = 1; replicaId < numReplicasConfigured; replicaId++) {\n      // return all replica locations for the meta\n      servers.add(getMetaRegionLocation(zkw, replicaId));\n    }\n    return servers;\n  }"
        ],
        [
            "ZKUtil::appendPeerState(ZKWatcher,String,StringBuilder)",
            "1861 -\n1862 -\n1863  \n1864  \n1865  \n1866  \n1867 -\n1868  \n1869  \n1870  \n1871  \n1872  \n1873  \n1874  \n1875  \n1876  \n1877  \n1878  \n1879  \n1880  \n1881  \n1882  \n1883  \n1884  ",
            "  private static void appendPeerState(ZKWatcher zkw, String znodeToProcess,\n                                      StringBuilder sb) throws KeeperException, InvalidProtocolBufferException {\n    String peerState = zkw.getConfiguration().get(\"zookeeper.znode.replication.peers.state\",\n      \"peer-state\");\n    int pblen = ProtobufUtil.lengthOfPBMagic();\n    for (String child : ZKUtil.listChildrenNoWatch(zkw, znodeToProcess)) {\n      if (!child.equals(peerState)) continue;\n      String peerStateZnode = ZNodePaths.joinZNode(znodeToProcess, child);\n      sb.append(\"\\n\").append(peerStateZnode).append(\": \");\n      byte[] peerStateData;\n      try {\n        peerStateData = ZKUtil.getData(zkw, peerStateZnode);\n        ReplicationProtos.ReplicationState.Builder builder =\n            ReplicationProtos.ReplicationState.newBuilder();\n        ProtobufUtil.mergeFrom(builder, peerStateData, pblen, peerStateData.length - pblen);\n        sb.append(builder.getState().name());\n      } catch (IOException ipbe) {\n        LOG.warn(\"Got Exception while parsing peer: \" + znodeToProcess, ipbe);\n      } catch (InterruptedException e) {\n        zkw.interruptedException(e);\n        return;\n      }\n    }\n  }",
            "1890 +\n1891 +\n1892  \n1893  \n1894  \n1895  \n1896 +\n1897 +\n1898 +\n1899 +\n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  ",
            "  private static void appendPeerState(ZKWatcher zkw, String znodeToProcess, StringBuilder sb)\n          throws KeeperException, InvalidProtocolBufferException {\n    String peerState = zkw.getConfiguration().get(\"zookeeper.znode.replication.peers.state\",\n      \"peer-state\");\n    int pblen = ProtobufUtil.lengthOfPBMagic();\n    for (String child : ZKUtil.listChildrenNoWatch(zkw, znodeToProcess)) {\n      if (!child.equals(peerState)) {\n        continue;\n      }\n\n      String peerStateZnode = ZNodePaths.joinZNode(znodeToProcess, child);\n      sb.append(\"\\n\").append(peerStateZnode).append(\": \");\n      byte[] peerStateData;\n      try {\n        peerStateData = ZKUtil.getData(zkw, peerStateZnode);\n        ReplicationProtos.ReplicationState.Builder builder =\n            ReplicationProtos.ReplicationState.newBuilder();\n        ProtobufUtil.mergeFrom(builder, peerStateData, pblen, peerStateData.length - pblen);\n        sb.append(builder.getState().name());\n      } catch (IOException ipbe) {\n        LOG.warn(\"Got Exception while parsing peer: \" + znodeToProcess, ipbe);\n      } catch (InterruptedException e) {\n        zkw.interruptedException(e);\n        return;\n      }\n    }\n  }"
        ],
        [
            "MetaTableLocator::verifyRegionLocation(ClusterConnection,AdminService,ServerName,byte)",
            " 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315 -\n 316 -\n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  ",
            "   * @param address The servername that goes with the <code>metaServer</code>\n   * Interface.  Used logging.\n   * @param regionName The regionname we are interested in.\n   * @return True if we were able to verify the region located at other side of\n   * the Interface.\n   * @throws IOException\n   */\n  // TODO: We should be able to get the ServerName from the AdminProtocol\n  // rather than have to pass it in.  Its made awkward by the fact that the\n  // HRI is likely a proxy against remote server so the getServerName needs\n  // to be fixed to go to a local method or to a cache before we can do this.\n  private boolean verifyRegionLocation(final ClusterConnection connection,\n      AdminService.BlockingInterface hostingServer, final ServerName address,\n      final byte [] regionName)\n  throws IOException {\n    if (hostingServer == null) {\n      LOG.info(\"Passed hostingServer is null\");\n      return false;\n    }\n    Throwable t;\n    HBaseRpcController controller = connection.getRpcControllerFactory().newController();\n    try {\n      // Try and get regioninfo from the hosting server.\n      return ProtobufUtil.getRegionInfo(controller, hostingServer, regionName) != null;\n    } catch (ConnectException e) {\n      t = e;\n    } catch (RetriesExhaustedException e) {\n      t = e;\n    } catch (RemoteException e) {\n      IOException ioe = e.unwrapRemoteException();\n      t = ioe;\n    } catch (IOException e) {\n      Throwable cause = e.getCause();\n      if (cause != null && cause instanceof EOFException) {\n        t = cause;\n      } else if (cause != null && cause.getMessage() != null\n          && cause.getMessage().contains(\"Connection reset\")) {\n        t = cause;\n      } else {\n        t = e;\n      }\n    }\n    LOG.info(\"Failed verification of \" + Bytes.toStringBinary(regionName) +\n      \" at address=\" + address + \", exception=\" + t.getMessage());\n    return false;\n  }",
            " 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 +\n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  ",
            "   * @param address The servername that goes with the <code>metaServer</code> interface.\n   *                Used logging.\n   * @param regionName The regionname we are interested in.\n   * @return True if we were able to verify the region located at other side of the interface.\n   */\n  // TODO: We should be able to get the ServerName from the AdminProtocol\n  // rather than have to pass it in.  Its made awkward by the fact that the\n  // HRI is likely a proxy against remote server so the getServerName needs\n  // to be fixed to go to a local method or to a cache before we can do this.\n  private boolean verifyRegionLocation(final ClusterConnection connection,\n      AdminService.BlockingInterface hostingServer, final ServerName address,\n      final byte [] regionName) {\n    if (hostingServer == null) {\n      LOG.info(\"Passed hostingServer is null\");\n      return false;\n    }\n    Throwable t;\n    HBaseRpcController controller = connection.getRpcControllerFactory().newController();\n    try {\n      // Try and get regioninfo from the hosting server.\n      return ProtobufUtil.getRegionInfo(controller, hostingServer, regionName) != null;\n    } catch (ConnectException e) {\n      t = e;\n    } catch (RetriesExhaustedException e) {\n      t = e;\n    } catch (RemoteException e) {\n      IOException ioe = e.unwrapRemoteException();\n      t = ioe;\n    } catch (IOException e) {\n      Throwable cause = e.getCause();\n      if (cause != null && cause instanceof EOFException) {\n        t = cause;\n      } else if (cause != null && cause.getMessage() != null\n          && cause.getMessage().contains(\"Connection reset\")) {\n        t = cause;\n      } else {\n        t = e;\n      }\n    }\n    LOG.info(\"Failed verification of \" + Bytes.toStringBinary(regionName) +\n      \" at address=\" + address + \", exception=\" + t.getMessage());\n    return false;\n  }"
        ],
        [
            "MasterAddressTracker::getMasterAddress(ZKWatcher)",
            " 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 -\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "  /**\n   * Get master address.\n   * Use this instead of {@link #getMasterAddress()} if you do not have an\n   * instance of this tracker in your context.\n   * @param zkw ZKWatcher to use\n   * @return ServerName stored in the the master address znode or null if no\n   * znode present.\n   * @throws KeeperException\n   * @throws IOException\n   */\n  public static ServerName getMasterAddress(final ZKWatcher zkw)\n  throws KeeperException, IOException {\n    byte [] data;\n    try {\n      data = ZKUtil.getData(zkw, zkw.znodePaths.masterAddressZNode);\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException();\n    }\n    // TODO javadoc claims we return null in this case. :/\n    if (data == null){\n      throw new IOException(\"Can't get master address from ZooKeeper; znode data == null\");\n    }\n    try {\n      return ProtobufUtil.parseServerNameFrom(data);\n    } catch (DeserializationException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    }\n  }",
            " 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "  /**\n   * Get master address.\n   * Use this instead of {@link #getMasterAddress()} if you do not have an\n   * instance of this tracker in your context.\n   * @param zkw ZKWatcher to use\n   * @return ServerName stored in the the master address znode or null if no\n   *         znode present.\n   * @throws KeeperException if a ZooKeeper operation fails\n   * @throws IOException if the address of the ZooKeeper master cannot be retrieved\n   */\n  public static ServerName getMasterAddress(final ZKWatcher zkw)\n          throws KeeperException, IOException {\n    byte [] data;\n    try {\n      data = ZKUtil.getData(zkw, zkw.znodePaths.masterAddressZNode);\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException();\n    }\n    // TODO javadoc claims we return null in this case. :/\n    if (data == null){\n      throw new IOException(\"Can't get master address from ZooKeeper; znode data == null\");\n    }\n    try {\n      return ProtobufUtil.parseServerNameFrom(data);\n    } catch (DeserializationException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::multi(Iterable)",
            " 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713 -\n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  ",
            "  /**\n   * Run multiple operations in a transactional manner. Retry before throwing exception\n   */\n  public List<OpResult> multi(Iterable<Op> ops)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.multi\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      Iterable<Op> multiOps = prepareZKMulti(ops);\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<OpResult> opResults = checkZk().multi(multiOps);\n          this.metrics.registerWriteOperationLatency(Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return opResults;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"multi\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"multi\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733 +\n 734 +\n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  ",
            "  /**\n   * Run multiple operations in a transactional manner. Retry before throwing exception\n   */\n  public List<OpResult> multi(Iterable<Op> ops)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.multi\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      Iterable<Op> multiOps = prepareZKMulti(ops);\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<OpResult> opResults = checkZk().multi(multiOps);\n          this.metrics.registerWriteOperationLatency(\n                  Math.min(EnvironmentEdgeManager.currentTime() - startTime,  1));\n          return opResults;\n        } catch (KeeperException e) {\n          this.metrics.registerFailedZKCall();\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              this.metrics.registerConnectionLossException();\n              retryOrThrow(retryCounter, e, \"multi\");\n              break;\n            case OPERATIONTIMEOUT:\n              this.metrics.registerOperationTimeoutException();\n              retryOrThrow(retryCounter, e, \"multi\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::logZKTree(ZKWatcher,String,String)",
            "2022  \n2023  \n2024  \n2025  \n2026  \n2027  \n2028  \n2029  \n2030 -\n2031  \n2032  \n2033  \n2034  \n2035  \n2036  ",
            "  /**\n   * Helper method to print the current state of the ZK tree.\n   * @see #logZKTree(ZKWatcher, String)\n   * @throws KeeperException if an unexpected exception occurs\n   */\n  protected static void logZKTree(ZKWatcher zkw, String root, String prefix)\n      throws KeeperException {\n    List<String> children = ZKUtil.listChildrenNoWatch(zkw, root);\n    if (children == null) return;\n    for (String child : children) {\n      LOG.debug(prefix + child);\n      String node = ZNodePaths.joinZNode(root.equals(\"/\") ? \"\" : root, child);\n      logZKTree(zkw, node, prefix + \"---\");\n    }\n  }",
            "2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068 +\n2069 +\n2070 +\n2071 +\n2072 +\n2073  \n2074  \n2075  \n2076  \n2077  \n2078  ",
            "  /**\n   * Helper method to print the current state of the ZK tree.\n   * @see #logZKTree(ZKWatcher, String)\n   * @throws KeeperException if an unexpected exception occurs\n   */\n  protected static void logZKTree(ZKWatcher zkw, String root, String prefix)\n      throws KeeperException {\n    List<String> children = ZKUtil.listChildrenNoWatch(zkw, root);\n\n    if (children == null) {\n      return;\n    }\n\n    for (String child : children) {\n      LOG.debug(prefix + child);\n      String node = ZNodePaths.joinZNode(root.equals(\"/\") ? \"\" : root, child);\n      logZKTree(zkw, node, prefix + \"---\");\n    }\n  }"
        ],
        [
            "HQuorumPeer::writeMyID(Properties)",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 -\n 143 -\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "  static void writeMyID(Properties properties) throws IOException {\n    long myId = -1;\n\n    Configuration conf = HBaseConfiguration.create();\n    String myAddress = Strings.domainNamePointerToHostName(DNS.getDefaultHost(\n        conf.get(\"hbase.zookeeper.dns.interface\",\"default\"),\n        conf.get(\"hbase.zookeeper.dns.nameserver\",\"default\")));\n\n    List<String> ips = new ArrayList<>();\n\n    // Add what could be the best (configured) match\n    ips.add(myAddress.contains(\".\") ?\n        myAddress :\n        StringUtils.simpleHostname(myAddress));\n\n    // For all nics get all hostnames and IPs\n    Enumeration<?> nics = NetworkInterface.getNetworkInterfaces();\n    while(nics.hasMoreElements()) {\n      Enumeration<?> rawAdrs =\n          ((NetworkInterface)nics.nextElement()).getInetAddresses();\n      while(rawAdrs.hasMoreElements()) {\n        InetAddress inet = (InetAddress) rawAdrs.nextElement();\n        ips.add(StringUtils.simpleHostname(inet.getHostName()));\n        ips.add(inet.getHostAddress());\n      }\n    }\n\n    for (Entry<Object, Object> entry : properties.entrySet()) {\n      String key = entry.getKey().toString().trim();\n      String value = entry.getValue().toString().trim();\n      if (key.startsWith(\"server.\")) {\n        int dot = key.indexOf('.');\n        long id = Long.parseLong(key.substring(dot + 1));\n        String[] parts = value.split(\":\");\n        String address = parts[0];\n        if (addressIsLocalHost(address) || ips.contains(address)) {\n          myId = id;\n          break;\n        }\n      }\n    }\n\n    // Set the max session timeout from the provided client-side timeout\n    properties.setProperty(\"maxSessionTimeout\",\n      conf.get(HConstants.ZK_SESSION_TIMEOUT, Integer.toString(HConstants.DEFAULT_ZK_SESSION_TIMEOUT)));\n\n    if (myId == -1) {\n      throw new IOException(\"Could not find my address: \" + myAddress +\n                            \" in list of ZooKeeper quorum servers\");\n    }\n\n    String dataDirStr = properties.get(\"dataDir\").toString().trim();\n    File dataDir = new File(dataDirStr);\n    if (!dataDir.isDirectory()) {\n      if (!dataDir.mkdirs()) {\n        throw new IOException(\"Unable to create data dir \" + dataDir);\n      }\n    }\n\n    File myIdFile = new File(dataDir, \"myid\");\n    PrintWriter w = new PrintWriter(myIdFile, StandardCharsets.UTF_8.name());\n    w.println(myId);\n    w.close();\n  }",
            " 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146 +\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "  static void writeMyID(Properties properties) throws IOException {\n    long myId = -1;\n\n    Configuration conf = HBaseConfiguration.create();\n    String myAddress = Strings.domainNamePointerToHostName(DNS.getDefaultHost(\n        conf.get(\"hbase.zookeeper.dns.interface\",\"default\"),\n        conf.get(\"hbase.zookeeper.dns.nameserver\",\"default\")));\n\n    List<String> ips = new ArrayList<>();\n\n    // Add what could be the best (configured) match\n    ips.add(myAddress.contains(\".\") ?\n        myAddress :\n        StringUtils.simpleHostname(myAddress));\n\n    // For all nics get all hostnames and IPs\n    Enumeration<?> nics = NetworkInterface.getNetworkInterfaces();\n    while(nics.hasMoreElements()) {\n      Enumeration<?> rawAdrs =\n          ((NetworkInterface)nics.nextElement()).getInetAddresses();\n      while(rawAdrs.hasMoreElements()) {\n        InetAddress inet = (InetAddress) rawAdrs.nextElement();\n        ips.add(StringUtils.simpleHostname(inet.getHostName()));\n        ips.add(inet.getHostAddress());\n      }\n    }\n\n    for (Entry<Object, Object> entry : properties.entrySet()) {\n      String key = entry.getKey().toString().trim();\n      String value = entry.getValue().toString().trim();\n      if (key.startsWith(\"server.\")) {\n        int dot = key.indexOf('.');\n        long id = Long.parseLong(key.substring(dot + 1));\n        String[] parts = value.split(\":\");\n        String address = parts[0];\n        if (addressIsLocalHost(address) || ips.contains(address)) {\n          myId = id;\n          break;\n        }\n      }\n    }\n\n    // Set the max session timeout from the provided client-side timeout\n    properties.setProperty(\"maxSessionTimeout\", conf.get(HConstants.ZK_SESSION_TIMEOUT,\n            Integer.toString(HConstants.DEFAULT_ZK_SESSION_TIMEOUT)));\n\n    if (myId == -1) {\n      throw new IOException(\"Could not find my address: \" + myAddress +\n                            \" in list of ZooKeeper quorum servers\");\n    }\n\n    String dataDirStr = properties.get(\"dataDir\").toString().trim();\n    File dataDir = new File(dataDirStr);\n    if (!dataDir.isDirectory()) {\n      if (!dataDir.mkdirs()) {\n        throw new IOException(\"Unable to create data dir \" + dataDir);\n      }\n    }\n\n    File myIdFile = new File(dataDir, \"myid\");\n    PrintWriter w = new PrintWriter(myIdFile, StandardCharsets.UTF_8.name());\n    w.println(myId);\n    w.close();\n  }"
        ],
        [
            "ZKNodeTracker::blockUntilAvailable(long,boolean)",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 -\n 141 -\n 142 -\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "  /**\n   * Gets the data of the node, blocking until the node is available or the\n   * specified timeout has elapsed.\n   *\n   * @param timeout maximum time to wait for the node data to be available,\n   * n milliseconds.  Pass 0 for no timeout.\n   * @return data of the node\n   * @throws InterruptedException if the waiting thread is interrupted\n   */\n  public synchronized byte [] blockUntilAvailable(long timeout, boolean refresh)\n  throws InterruptedException {\n    if (timeout < 0) throw new IllegalArgumentException();\n    boolean notimeout = timeout == 0;\n    long startTime = System.currentTimeMillis();\n    long remaining = timeout;\n    if (refresh) {\n      try {\n        // This does not create a watch if the node does not exists\n        this.data = ZKUtil.getDataAndWatch(watcher, node);\n      } catch(KeeperException e) {\n        // We use to abort here, but in some cases the abort is ignored (\n        //  (empty Abortable), so it's better to log...\n        LOG.warn(\"Unexpected exception handling blockUntilAvailable\", e);\n        abortable.abort(\"Unexpected exception handling blockUntilAvailable\", e);\n      }\n    }\n    boolean nodeExistsChecked = (!refresh ||data!=null);\n    while (!this.stopped && (notimeout || remaining > 0) && this.data == null) {\n      if (!nodeExistsChecked) {\n        try {\n          nodeExistsChecked = (ZKUtil.checkExists(watcher, node) != -1);\n        } catch (KeeperException e) {\n          LOG.warn(\n            \"Got exception while trying to check existence in  ZooKeeper\" +\n            \" of the node: \"+node+\", retrying if timeout not reached\",e );\n        }\n\n        // It did not exists, and now it does.\n        if (nodeExistsChecked){\n          LOG.debug(\"Node \" + node + \" now exists, resetting a watcher\");\n          try {\n            // This does not create a watch if the node does not exists\n            this.data = ZKUtil.getDataAndWatch(watcher, node);\n          } catch (KeeperException e) {\n            LOG.warn(\"Unexpected exception handling blockUntilAvailable\", e);\n            abortable.abort(\"Unexpected exception handling blockUntilAvailable\", e);\n          }\n        }\n      }\n      // We expect a notification; but we wait with a\n      //  a timeout to lower the impact of a race condition if any\n      wait(100);\n      remaining = timeout - (System.currentTimeMillis() - startTime);\n    }\n    return this.data;\n  }",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 +\n 120 +\n 121 +\n 122 +\n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144 +\n 145 +\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  ",
            "  /**\n   * Gets the data of the node, blocking until the node is available or the\n   * specified timeout has elapsed.\n   *\n   * @param timeout maximum time to wait for the node data to be available, n milliseconds. Pass 0\n   *                for no timeout.\n   * @return data of the node\n   * @throws InterruptedException if the waiting thread is interrupted\n   */\n  public synchronized byte [] blockUntilAvailable(long timeout, boolean refresh)\n          throws InterruptedException {\n    if (timeout < 0) {\n      throw new IllegalArgumentException();\n    }\n\n    boolean notimeout = timeout == 0;\n    long startTime = System.currentTimeMillis();\n    long remaining = timeout;\n    if (refresh) {\n      try {\n        // This does not create a watch if the node does not exists\n        this.data = ZKUtil.getDataAndWatch(watcher, node);\n      } catch(KeeperException e) {\n        // We use to abort here, but in some cases the abort is ignored (\n        //  (empty Abortable), so it's better to log...\n        LOG.warn(\"Unexpected exception handling blockUntilAvailable\", e);\n        abortable.abort(\"Unexpected exception handling blockUntilAvailable\", e);\n      }\n    }\n    boolean nodeExistsChecked = (!refresh ||data!=null);\n    while (!this.stopped && (notimeout || remaining > 0) && this.data == null) {\n      if (!nodeExistsChecked) {\n        try {\n          nodeExistsChecked = (ZKUtil.checkExists(watcher, node) != -1);\n        } catch (KeeperException e) {\n          LOG.warn(\"Got exception while trying to check existence in  ZooKeeper\" +\n            \" of the node: \" + node + \", retrying if timeout not reached\", e);\n        }\n\n        // It did not exists, and now it does.\n        if (nodeExistsChecked){\n          LOG.debug(\"Node \" + node + \" now exists, resetting a watcher\");\n          try {\n            // This does not create a watch if the node does not exists\n            this.data = ZKUtil.getDataAndWatch(watcher, node);\n          } catch (KeeperException e) {\n            LOG.warn(\"Unexpected exception handling blockUntilAvailable\", e);\n            abortable.abort(\"Unexpected exception handling blockUntilAvailable\", e);\n          }\n        }\n      }\n      // We expect a notification; but we wait with a\n      //  a timeout to lower the impact of a race condition if any\n      wait(100);\n      remaining = timeout - (System.currentTimeMillis() - startTime);\n    }\n    return this.data;\n  }"
        ]
    ],
    "2f1b3eab675ac327a6f61b724d5f0bce01ec6e68": [
        [
            "MasterProcedureScheduler::waitRegions(Procedure,TableName,RegionInfo)",
            " 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883 -\n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  ",
            "  /**\n   * Suspend the procedure if the specified set of regions are already locked.\n   * @param procedure the procedure trying to acquire the lock on the regions\n   * @param table the table name of the regions we are trying to lock\n   * @param regionInfo the list of regions we are trying to lock\n   * @return true if the procedure has to wait for the regions to be available\n   */\n  public boolean waitRegions(final Procedure procedure, final TableName table,\n      final RegionInfo... regionInfo) {\n    Arrays.sort(regionInfo, RegionInfo.COMPARATOR);\n    schedLock();\n    try {\n      // If there is parent procedure, it would have already taken xlock, so no need to take\n      // shared lock here. Otherwise, take shared lock.\n      if (!procedure.hasParent()\n          && waitTableQueueSharedLock(procedure, table) == null) {\n          return true;\n      }\n\n      // acquire region xlocks or wait\n      boolean hasLock = true;\n      final LockAndQueue[] regionLocks = new LockAndQueue[regionInfo.length];\n      for (int i = 0; i < regionInfo.length; ++i) {\n        LOG.info(procedure + \" \" + table + \" \" + regionInfo[i].getRegionNameAsString());\n        assert table != null;\n        assert regionInfo[i] != null;\n        assert regionInfo[i].getTable() != null;\n        assert regionInfo[i].getTable().equals(table): regionInfo[i] + \" \" + procedure;\n        assert i == 0 || regionInfo[i] != regionInfo[i - 1] : \"duplicate region: \" + regionInfo[i];\n\n        regionLocks[i] = locking.getRegionLock(regionInfo[i].getEncodedName());\n        if (!regionLocks[i].tryExclusiveLock(procedure)) {\n          waitProcedure(regionLocks[i], procedure);\n          hasLock = false;\n          while (i-- > 0) {\n            regionLocks[i].releaseExclusiveLock(procedure);\n          }\n          break;\n        }\n      }\n\n      if (!hasLock && !procedure.hasParent()) {\n        wakeTableSharedLock(procedure, table);\n      }\n      return !hasLock;\n    } finally {\n      schedUnlock();\n    }\n  }",
            " 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883 +\n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  ",
            "  /**\n   * Suspend the procedure if the specified set of regions are already locked.\n   * @param procedure the procedure trying to acquire the lock on the regions\n   * @param table the table name of the regions we are trying to lock\n   * @param regionInfo the list of regions we are trying to lock\n   * @return true if the procedure has to wait for the regions to be available\n   */\n  public boolean waitRegions(final Procedure procedure, final TableName table,\n      final RegionInfo... regionInfo) {\n    Arrays.sort(regionInfo, RegionInfo.COMPARATOR);\n    schedLock();\n    try {\n      // If there is parent procedure, it would have already taken xlock, so no need to take\n      // shared lock here. Otherwise, take shared lock.\n      if (!procedure.hasParent()\n          && waitTableQueueSharedLock(procedure, table) == null) {\n          return true;\n      }\n\n      // acquire region xlocks or wait\n      boolean hasLock = true;\n      final LockAndQueue[] regionLocks = new LockAndQueue[regionInfo.length];\n      for (int i = 0; i < regionInfo.length; ++i) {\n        LOG.info(procedure + \", table=\" + table + \", \" + regionInfo[i].getRegionNameAsString());\n        assert table != null;\n        assert regionInfo[i] != null;\n        assert regionInfo[i].getTable() != null;\n        assert regionInfo[i].getTable().equals(table): regionInfo[i] + \" \" + procedure;\n        assert i == 0 || regionInfo[i] != regionInfo[i - 1] : \"duplicate region: \" + regionInfo[i];\n\n        regionLocks[i] = locking.getRegionLock(regionInfo[i].getEncodedName());\n        if (!regionLocks[i].tryExclusiveLock(procedure)) {\n          waitProcedure(regionLocks[i], procedure);\n          hasLock = false;\n          while (i-- > 0) {\n            regionLocks[i].releaseExclusiveLock(procedure);\n          }\n          break;\n        }\n      }\n\n      if (!hasLock && !procedure.hasParent()) {\n        wakeTableSharedLock(procedure, table);\n      }\n      return !hasLock;\n    } finally {\n      schedUnlock();\n    }\n  }"
        ],
        [
            "ServerCrashProcedure::processMeta(MasterProcedureEnv)",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192 -\n 193 -\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  ",
            "  /**\n   * @param env\n   * @throws IOException\n   */\n  private void processMeta(final MasterProcedureEnv env) throws IOException {\n    if (LOG.isDebugEnabled()) LOG.debug(this + \"; Processing hbase:meta that was on \" +\n        this.serverName);\n\n    // Assign meta if still carrying it. Check again: region may be assigned because of RIT timeout\n    final AssignmentManager am = env.getMasterServices().getAssignmentManager();\n    for (RegionInfo hri: am.getRegionStates().getServerRegionInfoSet(serverName)) {\n      if (!isDefaultMetaRegion(hri)) {\n        continue;\n      }\n      addChildProcedure(new RecoverMetaProcedure(serverName, this.shouldSplitWal));\n    }\n  }",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192 +\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "  /**\n   * @param env\n   * @throws IOException\n   */\n  private void processMeta(final MasterProcedureEnv env) throws IOException {\n    LOG.debug(\"{}; processing hbase:meta\", this);\n\n    // Assign meta if still carrying it. Check again: region may be assigned because of RIT timeout\n    final AssignmentManager am = env.getMasterServices().getAssignmentManager();\n    for (RegionInfo hri: am.getRegionStates().getServerRegionInfoSet(serverName)) {\n      if (!isDefaultMetaRegion(hri)) {\n        continue;\n      }\n      addChildProcedure(new RecoverMetaProcedure(serverName, this.shouldSplitWal));\n    }\n  }"
        ],
        [
            "MoveRegionProcedure::MoveRegionProcedure(MasterProcedureEnv,RegionPlan)",
            "  54  \n  55  \n  56  \n  57  ",
            "  public MoveRegionProcedure(final MasterProcedureEnv env, final RegionPlan plan) {\n    super(env, plan.getRegionInfo());\n    this.plan = plan;\n  }",
            "  54  \n  55  \n  56  \n  57 +\n  58  ",
            "  public MoveRegionProcedure(final MasterProcedureEnv env, final RegionPlan plan) {\n    super(env, plan.getRegionInfo());\n    this.plan = plan;\n    LOG.info(\"REMOVE\", new Throwable(\"REMOVE: Just to see who is calling Move!!!\"));\n  }"
        ]
    ],
    "c1ada0a373561132a3359b48a27975b2e85978da": [
        [
            "LocalHBaseCluster::LocalHBaseCluster(Configuration,int,int,Class,Class)",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 -\n 148 -\n 149 -\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "  /**\n   * Constructor.\n   * @param conf Configuration to use.  Post construction has the master's\n   * address.\n   * @param noMasters Count of masters to start.\n   * @param noRegionServers Count of regionservers to start.\n   * @param masterClass\n   * @param regionServerClass\n   * @throws IOException\n   */\n  @SuppressWarnings(\"unchecked\")\n  public LocalHBaseCluster(final Configuration conf, final int noMasters,\n    final int noRegionServers, final Class<? extends HMaster> masterClass,\n    final Class<? extends HRegionServer> regionServerClass)\n  throws IOException {\n    this.conf = conf;\n\n    // Always have masters and regionservers come up on port '0' so we don't\n    // clash over default ports.\n    conf.set(HConstants.MASTER_PORT, \"0\");\n    if (conf.getInt(HConstants.MASTER_INFO_PORT, 0) != -1) {\n      conf.set(HConstants.MASTER_INFO_PORT, \"0\");\n    }\n    conf.set(HConstants.REGIONSERVER_PORT, \"0\");\n    if (conf.getInt(HConstants.REGIONSERVER_INFO_PORT, 0) != -1) {\n      conf.set(HConstants.REGIONSERVER_INFO_PORT, \"0\");\n    }\n\n    this.masterClass = (Class<? extends HMaster>)\n      conf.getClass(HConstants.MASTER_IMPL, masterClass);\n    // Start the HMasters.\n    for (int i = 0; i < noMasters; i++) {\n      addMaster(new Configuration(conf), i);\n    }\n    // Start the HRegionServers.\n    this.regionServerClass =\n      (Class<? extends HRegionServer>)conf.getClass(HConstants.REGION_SERVER_IMPL,\n       regionServerClass);\n\n    for (int i = 0; i < noRegionServers; i++) {\n      addRegionServer(new Configuration(conf), i);\n    }\n  }",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  ",
            "  /**\n   * Constructor.\n   * @param conf Configuration to use.  Post construction has the master's\n   * address.\n   * @param noMasters Count of masters to start.\n   * @param noRegionServers Count of regionservers to start.\n   * @param masterClass\n   * @param regionServerClass\n   * @throws IOException\n   */\n  @SuppressWarnings(\"unchecked\")\n  public LocalHBaseCluster(final Configuration conf, final int noMasters,\n    final int noRegionServers, final Class<? extends HMaster> masterClass,\n    final Class<? extends HRegionServer> regionServerClass)\n  throws IOException {\n    this.conf = conf;\n\n    // Always have masters and regionservers come up on port '0' so we don't\n    // clash over default ports.\n    conf.set(HConstants.MASTER_PORT, \"0\");\n    conf.set(HConstants.REGIONSERVER_PORT, \"0\");\n    if (conf.getInt(HConstants.REGIONSERVER_INFO_PORT, 0) != -1) {\n      conf.set(HConstants.REGIONSERVER_INFO_PORT, \"0\");\n    }\n\n    this.masterClass = (Class<? extends HMaster>)\n      conf.getClass(HConstants.MASTER_IMPL, masterClass);\n    // Start the HMasters.\n    for (int i = 0; i < noMasters; i++) {\n      addMaster(new Configuration(conf), i);\n    }\n    // Start the HRegionServers.\n    this.regionServerClass =\n      (Class<? extends HRegionServer>)conf.getClass(HConstants.REGION_SERVER_IMPL,\n       regionServerClass);\n\n    for (int i = 0; i < noRegionServers; i++) {\n      addRegionServer(new Configuration(conf), i);\n    }\n  }"
        ],
        [
            "MiniHBaseCluster::MiniHBaseCluster(Configuration,int,int,Class,Class)",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "  public MiniHBaseCluster(Configuration conf, int numMasters, int numRegionServers,\n         Class<? extends HMaster> masterClass,\n         Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> regionserverClass)\n      throws IOException, InterruptedException {\n    super(conf);\n    conf.set(HConstants.MASTER_PORT, \"0\");\n\n    // Hadoop 2\n    CompatibilityFactory.getInstance(MetricsAssertHelper.class).init();\n\n    init(numMasters, numRegionServers, masterClass, regionserverClass);\n    this.initialClusterStatus = getClusterStatus();\n  }",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "  public MiniHBaseCluster(Configuration conf, int numMasters, int numRegionServers,\n         Class<? extends HMaster> masterClass,\n         Class<? extends MiniHBaseCluster.MiniHBaseClusterRegionServer> regionserverClass)\n      throws IOException, InterruptedException {\n    super(conf);\n    conf.set(HConstants.MASTER_PORT, \"0\");\n    if (conf.getInt(HConstants.MASTER_INFO_PORT, 0) != -1) {\n      conf.set(HConstants.MASTER_INFO_PORT, \"0\");\n    }\n\n    // Hadoop 2\n    CompatibilityFactory.getInstance(MetricsAssertHelper.class).init();\n\n    init(numMasters, numRegionServers, masterClass, regionserverClass);\n    this.initialClusterStatus = getClusterStatus();\n  }"
        ]
    ],
    "66781864aaf78e8c8afb0978a7f68b6773d69649": [
        [
            "SnapshotFileCache::refreshCache()",
            " 207  \n 208 -\n 209 -\n 210 -\n 211  \n 212  \n 213 -\n 214 -\n 215 -\n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229 -\n 230 -\n 231 -\n 232 -\n 233 -\n 234 -\n 235 -\n 236 -\n 237 -\n 238 -\n 239 -\n 240 -\n 241 -\n 242 -\n 243  \n 244 -\n 245 -\n 246 -\n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 -\n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "  private synchronized void refreshCache() throws IOException {\n    long lastTimestamp = Long.MAX_VALUE;\n    boolean hasChanges = false;\n\n    // get the status of the snapshots directory and check if it is has changes\n    try {\n      FileStatus dirStatus = fs.getFileStatus(snapshotDir);\n      lastTimestamp = dirStatus.getModificationTime();\n      hasChanges |= (lastTimestamp >= lastModifiedTime);\n    } catch (FileNotFoundException e) {\n      if (this.cache.size() > 0) {\n        LOG.error(\"Snapshot directory: \" + snapshotDir + \" doesn't exist\");\n      }\n      return;\n    }\n\n    // get the status of the snapshots temporary directory and check if it has changes\n    // The top-level directory timestamp is not updated, so we have to check the inner-level.\n    try {\n      Path snapshotTmpDir = new Path(snapshotDir, SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME);\n      FileStatus tempDirStatus = fs.getFileStatus(snapshotTmpDir);\n      lastTimestamp = Math.min(lastTimestamp, tempDirStatus.getModificationTime());\n      hasChanges |= (lastTimestamp >= lastModifiedTime);\n      if (!hasChanges) {\n        FileStatus[] tmpSnapshots = FSUtils.listStatus(fs, snapshotDir);\n        if (tmpSnapshots != null) {\n          for (FileStatus dirStatus: tmpSnapshots) {\n            lastTimestamp = Math.min(lastTimestamp, dirStatus.getModificationTime());\n          }\n          hasChanges |= (lastTimestamp >= lastModifiedTime);\n        }\n      }\n    } catch (FileNotFoundException e) {\n      // Nothing todo, if the tmp dir is empty\n    }\n\n    // if the snapshot directory wasn't modified since we last check, we are done\n    if (!hasChanges) {\n      return;\n    }\n\n    // directory was modified, so we need to reload our cache\n    // there could be a slight race here where we miss the cache, check the directory modification\n    // time, then someone updates the directory, causing us to not scan the directory again.\n    // However, snapshot directories are only created once, so this isn't an issue.\n\n    // 1. update the modified time\n    this.lastModifiedTime = lastTimestamp;\n\n    // 2.clear the cache\n    this.cache.clear();\n    Map<String, SnapshotDirectoryInfo> known = new HashMap<String, SnapshotDirectoryInfo>();\n\n    // 3. check each of the snapshot directories\n    FileStatus[] snapshots = FSUtils.listStatus(fs, snapshotDir);\n    if (snapshots == null) {\n      // remove all the remembered snapshots because we don't have any left\n      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {\n        LOG.debug(\"No snapshots on-disk, cache empty\");\n      }\n      this.snapshots.clear();\n      return;\n    }\n\n    // 3.1 iterate through the on-disk snapshots\n    for (FileStatus snapshot : snapshots) {\n      String name = snapshot.getPath().getName();\n      // its not the tmp dir,\n      if (!name.equals(SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME)) {\n        SnapshotDirectoryInfo files = this.snapshots.remove(name);\n        // 3.1.1 if we don't know about the snapshot or its been modified, we need to update the\n        // files the latter could occur where I create a snapshot, then delete it, and then make a\n        // new snapshot with the same name. We will need to update the cache the information from\n        // that new snapshot, even though it has the same name as the files referenced have\n        // probably changed.\n        if (files == null || files.hasBeenModified(snapshot.getModificationTime())) {\n          // get all files for the snapshot and create a new info\n          Collection<String> storedFiles = fileInspector.filesUnderSnapshot(snapshot.getPath());\n          files = new SnapshotDirectoryInfo(snapshot.getModificationTime(), storedFiles);\n        }\n        // 3.2 add all the files to cache\n        this.cache.addAll(files.getFiles());\n        known.put(name, files);\n      }\n    }\n\n    // 4. set the snapshots we are tracking\n    this.snapshots.clear();\n    this.snapshots.putAll(known);\n  }",
            " 207  \n 208  \n 209 +\n 210  \n 211 +\n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 +\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 +\n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  ",
            "  private synchronized void refreshCache() throws IOException {\n    // get the status of the snapshots directory and check if it is has changes\n    FileStatus dirStatus;\n    try {\n      dirStatus = fs.getFileStatus(snapshotDir);\n    } catch (FileNotFoundException e) {\n      if (this.cache.size() > 0) {\n        LOG.error(\"Snapshot directory: \" + snapshotDir + \" doesn't exist\");\n      }\n      return;\n    }\n\n    // if the snapshot directory wasn't modified since we last check, we are done\n    if (dirStatus.getModificationTime() <= this.lastModifiedTime) return;\n\n    // directory was modified, so we need to reload our cache\n    // there could be a slight race here where we miss the cache, check the directory modification\n    // time, then someone updates the directory, causing us to not scan the directory again.\n    // However, snapshot directories are only created once, so this isn't an issue.\n\n    // 1. update the modified time\n    this.lastModifiedTime = dirStatus.getModificationTime();\n\n    // 2.clear the cache\n    this.cache.clear();\n    Map<String, SnapshotDirectoryInfo> known = new HashMap<String, SnapshotDirectoryInfo>();\n\n    // 3. check each of the snapshot directories\n    FileStatus[] snapshots = FSUtils.listStatus(fs, snapshotDir);\n    if (snapshots == null) {\n      // remove all the remembered snapshots because we don't have any left\n      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {\n        LOG.debug(\"No snapshots on-disk, cache empty\");\n      }\n      this.snapshots.clear();\n      return;\n    }\n\n    // 3.1 iterate through the on-disk snapshots\n    for (FileStatus snapshot : snapshots) {\n      String name = snapshot.getPath().getName();\n      // its not the tmp dir,\n      if (!name.equals(SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME)) {\n        SnapshotDirectoryInfo files = this.snapshots.remove(name);\n        // 3.1.1 if we don't know about the snapshot or its been modified, we need to update the\n        // files the latter could occur where I create a snapshot, then delete it, and then make a\n        // new snapshot with the same name. We will need to update the cache the information from\n        // that new snapshot, even though it has the same name as the files referenced have\n        // probably changed.\n        if (files == null || files.hasBeenModified(snapshot.getModificationTime())) {\n          // get all files for the snapshot and create a new info\n          Collection<String> storedFiles = fileInspector.filesUnderSnapshot(snapshot.getPath());\n          files = new SnapshotDirectoryInfo(snapshot.getModificationTime(), storedFiles);\n        }\n        // 3.2 add all the files to cache\n        this.cache.addAll(files.getFiles());\n        known.put(name, files);\n      }\n    }\n\n    // 4. set the snapshots we are tracking\n    this.snapshots.clear();\n    this.snapshots.putAll(known);\n  }"
        ]
    ],
    "287f95a579ee95a40e0f3a0986a246d29718ee3b": [
        [
            "HRegionFileSystem::setStoragePolicy(String,String)",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199 -\n 200  \n 201  ",
            "  /**\n   * Set the directory of CF to the specified storage policy. <br>\n   * <i>\"LAZY_PERSIST\"</i>, <i>\"ALL_SSD\"</i>, <i>\"ONE_SSD\"</i>, <i>\"HOT\"</i>, <i>\"WARM\"</i>,\n   * <i>\"COLD\"</i> <br>\n   * <br>\n   * See {@link org.apache.hadoop.hdfs.protocol.HdfsConstants} for more details.\n   * @param familyName The name of column family.\n   * @param policyName The name of the storage policy.\n   */\n  public void setStoragePolicy(String familyName, String policyName) {\n    Path storeDir = getStoreDir(familyName);\n    try {\n      ReflectionUtils.invokeMethod(this.fs, \"setStoragePolicy\", storeDir, policyName);\n    } catch (Exception e) {\n      LOG.warn(\"Failed to set storage policy of [\" + storeDir + \"] to [\" + policyName + \"]\", e);\n    }\n  }",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 +\n 201 +\n 202 +\n 203  \n 204  ",
            "  /**\n   * Set the directory of CF to the specified storage policy. <br>\n   * <i>\"LAZY_PERSIST\"</i>, <i>\"ALL_SSD\"</i>, <i>\"ONE_SSD\"</i>, <i>\"HOT\"</i>, <i>\"WARM\"</i>,\n   * <i>\"COLD\"</i> <br>\n   * <br>\n   * See {@link org.apache.hadoop.hdfs.protocol.HdfsConstants} for more details.\n   * @param familyName The name of column family.\n   * @param policyName The name of the storage policy.\n   */\n  public void setStoragePolicy(String familyName, String policyName) {\n    Path storeDir = getStoreDir(familyName);\n    try {\n      ReflectionUtils.invokeMethod(this.fs, \"setStoragePolicy\", storeDir, policyName);\n    } catch (Exception e) {\n      if (!(this.fs instanceof LocalFileSystem)) {\n        LOG.warn(\"Failed to set storage policy of [\" + storeDir + \"] to [\" + policyName + \"]\", e);\n      }\n    }\n  }"
        ]
    ],
    "09a31bd1e9476106d9c4700e850627f5b2822c18": [
        [
            "HttpServer::addDefaultServlets()",
            " 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713 -\n 714  \n 715  \n 716  ",
            "  /**\n   * Add default servlets.\n   */\n  protected void addDefaultServlets() {\n    // set up default servlets\n    addServlet(\"stacks\", \"/stacks\", StackServlet.class);\n    addServlet(\"logLevel\", \"/logLevel\", LogLevel.Servlet.class);\n    addServlet(\"metrics\", \"/metrics\", MetricsServlet.class);\n    addServlet(\"jmx\", \"/jmx\", JMXJsonServlet.class);\n    addServlet(\"conf\", \"/conf\", ConfServlet.class);\n  }",
            " 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712 +\n 713 +\n 714 +\n 715 +\n 716 +\n 717 +\n 718 +\n 719 +\n 720 +\n 721 +\n 722 +\n 723  \n 724  \n 725  ",
            "  /**\n   * Add default servlets.\n   */\n  protected void addDefaultServlets() {\n    // set up default servlets\n    addServlet(\"stacks\", \"/stacks\", StackServlet.class);\n    addServlet(\"logLevel\", \"/logLevel\", LogLevel.Servlet.class);\n\n    // Hadoop3 has moved completely to metrics2, and  dropped support for Metrics v1's\n    // MetricsServlet (see HADOOP-12504).  We'll using reflection to load if against hadoop2.\n    // Remove when we drop support for hbase on hadoop2.x.\n    try {\n      Class clz = Class.forName(\"org.apache.hadoop.metrics.MetricsServlet\");\n      addServlet(\"metrics\", \"/metrics\", clz);\n    } catch (Exception e) {\n      // do nothing\n    }\n\n    addServlet(\"jmx\", \"/jmx\", JMXJsonServlet.class);\n    addServlet(\"conf\", \"/conf\", ConfServlet.class);\n  }"
        ],
        [
            "TestBulkLoad::createHFileForFamilies(byte)",
            " 272  \n 273  \n 274  \n 275  \n 276 -\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  ",
            "  private String createHFileForFamilies(byte[] family) throws IOException {\n    HFile.WriterFactory hFileFactory = HFile.getWriterFactoryNoCache(conf);\n    // TODO We need a way to do this without creating files\n    File hFileLocation = testFolder.newFile();\n    FSDataOutputStream out = new FSDataOutputStream(new FileOutputStream(hFileLocation));\n    try {\n      hFileFactory.withOutputStream(out);\n      hFileFactory.withFileContext(new HFileContext());\n      HFile.Writer writer = hFileFactory.create();\n      try {\n        writer.append(new KeyValue(CellUtil.createCell(randomBytes,\n            family,\n            randomBytes,\n            0l,\n            KeyValue.Type.Put.getCode(),\n            randomBytes)));\n      } finally {\n        writer.close();\n      }\n    } finally {\n      out.close();\n    }\n    return hFileLocation.getAbsoluteFile().getAbsolutePath();\n  }",
            " 272  \n 273  \n 274  \n 275  \n 276 +\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  ",
            "  private String createHFileForFamilies(byte[] family) throws IOException {\n    HFile.WriterFactory hFileFactory = HFile.getWriterFactoryNoCache(conf);\n    // TODO We need a way to do this without creating files\n    File hFileLocation = testFolder.newFile();\n    FSDataOutputStream out = new FSDataOutputStream(new FileOutputStream(hFileLocation), null);\n    try {\n      hFileFactory.withOutputStream(out);\n      hFileFactory.withFileContext(new HFileContext());\n      HFile.Writer writer = hFileFactory.create();\n      try {\n        writer.append(new KeyValue(CellUtil.createCell(randomBytes,\n            family,\n            randomBytes,\n            0l,\n            KeyValue.Type.Put.getCode(),\n            randomBytes)));\n      } finally {\n        writer.close();\n      }\n    } finally {\n      out.close();\n    }\n    return hFileLocation.getAbsoluteFile().getAbsolutePath();\n  }"
        ],
        [
            "HRegionServer::run()",
            " 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034 -\n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  ",
            "  /**\n   * The HRegionServer sticks in this loop until closed.\n   */\n  @Override\n  public void run() {\n    try {\n      // Do pre-registration initializations; zookeeper, lease threads, etc.\n      preRegistrationInitialization();\n    } catch (Throwable e) {\n      abort(\"Fatal exception during initialization\", e);\n    }\n\n    try {\n      if (!isStopped() && !isAborted()) {\n        ShutdownHook.install(conf, fs, this, Thread.currentThread());\n        // Set our ephemeral znode up in zookeeper now we have a name.\n        createMyEphemeralNode();\n        // Initialize the RegionServerCoprocessorHost now that our ephemeral\n        // node was created, in case any coprocessors want to use ZooKeeper\n        this.rsHost = new RegionServerCoprocessorHost(this, this.conf);\n      }\n\n      // Try and register with the Master; tell it we are here.  Break if\n      // server is stopped or the clusterup flag is down or hdfs went wacky.\n      while (keepLooping()) {\n        RegionServerStartupResponse w = reportForDuty();\n        if (w == null) {\n          LOG.warn(\"reportForDuty failed; sleeping and then retrying.\");\n          this.sleeper.sleep();\n        } else {\n          handleReportForDutyResponse(w);\n          break;\n        }\n      }\n\n      if (!isStopped() && isHealthy()){\n        // start the snapshot handler and other procedure handlers,\n        // since the server is ready to run\n        rspmHost.start();\n\n        // Start the Quota Manager\n        rsQuotaManager.start(getRpcServer().getScheduler());\n      }\n\n      // We registered with the Master.  Go into run mode.\n      long lastMsg = System.currentTimeMillis();\n      long oldRequestCount = -1;\n      // The main run loop.\n      while (!isStopped() && isHealthy()) {\n        if (!isClusterUp()) {\n          if (isOnlineRegionsEmpty()) {\n            stop(\"Exiting; cluster shutdown set and not carrying any regions\");\n          } else if (!this.stopping) {\n            this.stopping = true;\n            LOG.info(\"Closing user regions\");\n            closeUserRegions(this.abortRequested);\n          } else if (this.stopping) {\n            boolean allUserRegionsOffline = areAllUserRegionsOffline();\n            if (allUserRegionsOffline) {\n              // Set stopped if no more write requests tp meta tables\n              // since last time we went around the loop.  Any open\n              // meta regions will be closed on our way out.\n              if (oldRequestCount == getWriteRequestCount()) {\n                stop(\"Stopped; only catalog regions remaining online\");\n                break;\n              }\n              oldRequestCount = getWriteRequestCount();\n            } else {\n              // Make sure all regions have been closed -- some regions may\n              // have not got it because we were splitting at the time of\n              // the call to closeUserRegions.\n              closeUserRegions(this.abortRequested);\n            }\n            LOG.debug(\"Waiting on \" + getOnlineRegionsAsPrintableString());\n          }\n        }\n        long now = System.currentTimeMillis();\n        if ((now - lastMsg) >= msgInterval) {\n          tryRegionServerReport(lastMsg, now);\n          lastMsg = System.currentTimeMillis();\n        }\n        if (!isStopped() && !isAborted()) {\n          this.sleeper.sleep();\n        }\n      } // for\n    } catch (Throwable t) {\n      if (!rpcServices.checkOOME(t)) {\n        String prefix = t instanceof YouAreDeadException? \"\": \"Unhandled: \";\n        abort(prefix + t.getMessage(), t);\n      }\n    }\n    // Run shutdown.\n    if (mxBean != null) {\n      MBeanUtil.unregisterMBean(mxBean);\n      mxBean = null;\n    }\n    if (this.leases != null) this.leases.closeAfterLeasesExpire();\n    if (this.splitLogWorker != null) {\n      splitLogWorker.stop();\n    }\n    if (this.infoServer != null) {\n      LOG.info(\"Stopping infoServer\");\n      try {\n        this.infoServer.stop();\n      } catch (Exception e) {\n        LOG.error(\"Failed to stop infoServer\", e);\n      }\n    }\n    // Send cache a shutdown.\n    if (cacheConfig != null && cacheConfig.isBlockCacheEnabled()) {\n      cacheConfig.getBlockCache().shutdown();\n    }\n    mobCacheConfig.getMobFileCache().shutdown();\n\n    if (movedRegionsCleaner != null) {\n      movedRegionsCleaner.stop(\"Region Server stopping\");\n    }\n\n    // Send interrupts to wake up threads if sleeping so they notice shutdown.\n    // TODO: Should we check they are alive? If OOME could have exited already\n    if (this.hMemManager != null) this.hMemManager.stop();\n    if (this.cacheFlusher != null) this.cacheFlusher.interruptIfNecessary();\n    if (this.compactSplitThread != null) this.compactSplitThread.interruptIfNecessary();\n    if (this.compactionChecker != null) this.compactionChecker.cancel(true);\n    if (this.healthCheckChore != null) this.healthCheckChore.cancel(true);\n    if (this.nonceManagerChore != null) this.nonceManagerChore.cancel(true);\n    if (this.storefileRefresher != null) this.storefileRefresher.cancel(true);\n    sendShutdownInterrupt();\n\n    // Stop the quota manager\n    if (rsQuotaManager != null) {\n      rsQuotaManager.stop();\n    }\n\n    // Stop the snapshot and other procedure handlers, forcefully killing all running tasks\n    if (rspmHost != null) {\n      rspmHost.stop(this.abortRequested || this.killed);\n    }\n\n    if (this.killed) {\n      // Just skip out w/o closing regions.  Used when testing.\n    } else if (abortRequested) {\n      if (this.fsOk) {\n        closeUserRegions(abortRequested); // Don't leave any open file handles\n      }\n      LOG.info(\"aborting server \" + this.serverName);\n    } else {\n      closeUserRegions(abortRequested);\n      LOG.info(\"stopping server \" + this.serverName);\n    }\n\n    // so callers waiting for meta without timeout can stop\n    if (this.metaTableLocator != null) this.metaTableLocator.stop();\n    if (this.clusterConnection != null && !clusterConnection.isClosed()) {\n      try {\n        this.clusterConnection.close();\n      } catch (IOException e) {\n        // Although the {@link Closeable} interface throws an {@link\n        // IOException}, in reality, the implementation would never do that.\n        LOG.warn(\"Attempt to close server's short circuit ClusterConnection failed.\", e);\n      }\n    }\n\n    // Closing the compactSplit thread before closing meta regions\n    if (!this.killed && containsMetaTableRegions()) {\n      if (!abortRequested || this.fsOk) {\n        if (this.compactSplitThread != null) {\n          this.compactSplitThread.join();\n          this.compactSplitThread = null;\n        }\n        closeMetaTableRegions(abortRequested);\n      }\n    }\n\n    if (!this.killed && this.fsOk) {\n      waitOnAllRegionsToClose(abortRequested);\n      LOG.info(\"stopping server \" + this.serverName +\n        \"; all regions closed.\");\n    }\n\n    //fsOk flag may be changed when closing regions throws exception.\n    if (this.fsOk) {\n      shutdownWAL(!abortRequested);\n    }\n\n    // Make sure the proxy is down.\n    if (this.rssStub != null) {\n      this.rssStub = null;\n    }\n    if (this.rpcClient != null) {\n      this.rpcClient.close();\n    }\n    if (this.leases != null) {\n      this.leases.close();\n    }\n    if (this.pauseMonitor != null) {\n      this.pauseMonitor.stop();\n    }\n\n    if (!killed) {\n      stopServiceThreads();\n    }\n\n    if (this.rpcServices != null) {\n      this.rpcServices.stop();\n    }\n\n    try {\n      deleteMyEphemeralNode();\n    } catch (KeeperException.NoNodeException nn) {\n    } catch (KeeperException e) {\n      LOG.warn(\"Failed deleting my ephemeral node\", e);\n    }\n    // We may have failed to delete the znode at the previous step, but\n    //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n    ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n\n    if (this.zooKeeper != null) {\n      this.zooKeeper.close();\n    }\n    LOG.info(\"stopping server \" + this.serverName +\n      \"; zookeeper connection closed.\");\n\n    LOG.info(Thread.currentThread().getName() + \" exiting\");\n  }",
            " 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034 +\n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  ",
            "  /**\n   * The HRegionServer sticks in this loop until closed.\n   */\n  @Override\n  public void run() {\n    try {\n      // Do pre-registration initializations; zookeeper, lease threads, etc.\n      preRegistrationInitialization();\n    } catch (Throwable e) {\n      abort(\"Fatal exception during initialization\", e);\n    }\n\n    try {\n      if (!isStopped() && !isAborted()) {\n        ShutdownHook.install(conf, fs, this, Thread.currentThread());\n        // Set our ephemeral znode up in zookeeper now we have a name.\n        createMyEphemeralNode();\n        // Initialize the RegionServerCoprocessorHost now that our ephemeral\n        // node was created, in case any coprocessors want to use ZooKeeper\n        this.rsHost = new RegionServerCoprocessorHost(this, this.conf);\n      }\n\n      // Try and register with the Master; tell it we are here.  Break if\n      // server is stopped or the clusterup flag is down or hdfs went wacky.\n      while (keepLooping()) {\n        RegionServerStartupResponse w = reportForDuty();\n        if (w == null) {\n          LOG.warn(\"reportForDuty failed; sleeping and then retrying.\");\n          this.sleeper.sleep();\n        } else {\n          handleReportForDutyResponse(w);\n          break;\n        }\n      }\n\n      if (!isStopped() && isHealthy()){\n        // start the snapshot handler and other procedure handlers,\n        // since the server is ready to run\n        rspmHost.start();\n\n        // Start the Quota Manager\n        rsQuotaManager.start(getRpcServer().getScheduler());\n      }\n\n      // We registered with the Master.  Go into run mode.\n      long lastMsg = System.currentTimeMillis();\n      long oldRequestCount = -1;\n      // The main run loop.\n      while (!isStopped() && isHealthy()) {\n        if (!isClusterUp()) {\n          if (isOnlineRegionsEmpty()) {\n            stop(\"Exiting; cluster shutdown set and not carrying any regions\");\n          } else if (!this.stopping) {\n            this.stopping = true;\n            LOG.info(\"Closing user regions\");\n            closeUserRegions(this.abortRequested);\n          } else if (this.stopping) {\n            boolean allUserRegionsOffline = areAllUserRegionsOffline();\n            if (allUserRegionsOffline) {\n              // Set stopped if no more write requests tp meta tables\n              // since last time we went around the loop.  Any open\n              // meta regions will be closed on our way out.\n              if (oldRequestCount == getWriteRequestCount()) {\n                stop(\"Stopped; only catalog regions remaining online\");\n                break;\n              }\n              oldRequestCount = getWriteRequestCount();\n            } else {\n              // Make sure all regions have been closed -- some regions may\n              // have not got it because we were splitting at the time of\n              // the call to closeUserRegions.\n              closeUserRegions(this.abortRequested);\n            }\n            LOG.debug(\"Waiting on \" + getOnlineRegionsAsPrintableString());\n          }\n        }\n        long now = System.currentTimeMillis();\n        if ((now - lastMsg) >= msgInterval) {\n          tryRegionServerReport(lastMsg, now);\n          lastMsg = System.currentTimeMillis();\n        }\n        if (!isStopped() && !isAborted()) {\n          this.sleeper.sleep();\n        }\n      } // for\n    } catch (Throwable t) {\n      if (!rpcServices.checkOOME(t)) {\n        String prefix = t instanceof YouAreDeadException? \"\": \"Unhandled: \";\n        abort(prefix + t.getMessage(), t);\n      }\n    }\n    // Run shutdown.\n    if (mxBean != null) {\n      MBeans.unregister(mxBean);\n      mxBean = null;\n    }\n    if (this.leases != null) this.leases.closeAfterLeasesExpire();\n    if (this.splitLogWorker != null) {\n      splitLogWorker.stop();\n    }\n    if (this.infoServer != null) {\n      LOG.info(\"Stopping infoServer\");\n      try {\n        this.infoServer.stop();\n      } catch (Exception e) {\n        LOG.error(\"Failed to stop infoServer\", e);\n      }\n    }\n    // Send cache a shutdown.\n    if (cacheConfig != null && cacheConfig.isBlockCacheEnabled()) {\n      cacheConfig.getBlockCache().shutdown();\n    }\n    mobCacheConfig.getMobFileCache().shutdown();\n\n    if (movedRegionsCleaner != null) {\n      movedRegionsCleaner.stop(\"Region Server stopping\");\n    }\n\n    // Send interrupts to wake up threads if sleeping so they notice shutdown.\n    // TODO: Should we check they are alive? If OOME could have exited already\n    if (this.hMemManager != null) this.hMemManager.stop();\n    if (this.cacheFlusher != null) this.cacheFlusher.interruptIfNecessary();\n    if (this.compactSplitThread != null) this.compactSplitThread.interruptIfNecessary();\n    if (this.compactionChecker != null) this.compactionChecker.cancel(true);\n    if (this.healthCheckChore != null) this.healthCheckChore.cancel(true);\n    if (this.nonceManagerChore != null) this.nonceManagerChore.cancel(true);\n    if (this.storefileRefresher != null) this.storefileRefresher.cancel(true);\n    sendShutdownInterrupt();\n\n    // Stop the quota manager\n    if (rsQuotaManager != null) {\n      rsQuotaManager.stop();\n    }\n\n    // Stop the snapshot and other procedure handlers, forcefully killing all running tasks\n    if (rspmHost != null) {\n      rspmHost.stop(this.abortRequested || this.killed);\n    }\n\n    if (this.killed) {\n      // Just skip out w/o closing regions.  Used when testing.\n    } else if (abortRequested) {\n      if (this.fsOk) {\n        closeUserRegions(abortRequested); // Don't leave any open file handles\n      }\n      LOG.info(\"aborting server \" + this.serverName);\n    } else {\n      closeUserRegions(abortRequested);\n      LOG.info(\"stopping server \" + this.serverName);\n    }\n\n    // so callers waiting for meta without timeout can stop\n    if (this.metaTableLocator != null) this.metaTableLocator.stop();\n    if (this.clusterConnection != null && !clusterConnection.isClosed()) {\n      try {\n        this.clusterConnection.close();\n      } catch (IOException e) {\n        // Although the {@link Closeable} interface throws an {@link\n        // IOException}, in reality, the implementation would never do that.\n        LOG.warn(\"Attempt to close server's short circuit ClusterConnection failed.\", e);\n      }\n    }\n\n    // Closing the compactSplit thread before closing meta regions\n    if (!this.killed && containsMetaTableRegions()) {\n      if (!abortRequested || this.fsOk) {\n        if (this.compactSplitThread != null) {\n          this.compactSplitThread.join();\n          this.compactSplitThread = null;\n        }\n        closeMetaTableRegions(abortRequested);\n      }\n    }\n\n    if (!this.killed && this.fsOk) {\n      waitOnAllRegionsToClose(abortRequested);\n      LOG.info(\"stopping server \" + this.serverName +\n        \"; all regions closed.\");\n    }\n\n    //fsOk flag may be changed when closing regions throws exception.\n    if (this.fsOk) {\n      shutdownWAL(!abortRequested);\n    }\n\n    // Make sure the proxy is down.\n    if (this.rssStub != null) {\n      this.rssStub = null;\n    }\n    if (this.rpcClient != null) {\n      this.rpcClient.close();\n    }\n    if (this.leases != null) {\n      this.leases.close();\n    }\n    if (this.pauseMonitor != null) {\n      this.pauseMonitor.stop();\n    }\n\n    if (!killed) {\n      stopServiceThreads();\n    }\n\n    if (this.rpcServices != null) {\n      this.rpcServices.stop();\n    }\n\n    try {\n      deleteMyEphemeralNode();\n    } catch (KeeperException.NoNodeException nn) {\n    } catch (KeeperException e) {\n      LOG.warn(\"Failed deleting my ephemeral node\", e);\n    }\n    // We may have failed to delete the znode at the previous step, but\n    //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n    ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n\n    if (this.zooKeeper != null) {\n      this.zooKeeper.close();\n    }\n    LOG.info(\"stopping server \" + this.serverName +\n      \"; zookeeper connection closed.\");\n\n    LOG.info(Thread.currentThread().getName() + \" exiting\");\n  }"
        ],
        [
            "IncrementCoalescer::IncrementCoalescer(HBaseHandler)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 -\n 175  ",
            "  @SuppressWarnings(\"deprecation\")\n  public IncrementCoalescer(HBaseHandler hand) {\n    this.handler = hand;\n    LinkedBlockingQueue<Runnable> queue = new LinkedBlockingQueue<Runnable>();\n    pool =\n        new ThreadPoolExecutor(CORE_POOL_SIZE, CORE_POOL_SIZE, 50, TimeUnit.MILLISECONDS, queue,\n            Threads.newDaemonThreadFactory(\"IncrementCoalescer\"));\n\n    MBeanUtil.registerMBean(\"thrift\", \"Thrift\", this);\n  }",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 +\n 175  ",
            "  @SuppressWarnings(\"deprecation\")\n  public IncrementCoalescer(HBaseHandler hand) {\n    this.handler = hand;\n    LinkedBlockingQueue<Runnable> queue = new LinkedBlockingQueue<Runnable>();\n    pool =\n        new ThreadPoolExecutor(CORE_POOL_SIZE, CORE_POOL_SIZE, 50, TimeUnit.MILLISECONDS, queue,\n            Threads.newDaemonThreadFactory(\"IncrementCoalescer\"));\n\n    MBeans.register(\"thrift\", \"Thrift\", this);\n  }"
        ]
    ],
    "3bb8daa60565ec2f7955352e52c2f6379176d8c6": [
        [
            "SplitTableRegionProcedure::SplitTableRegionProcedure(MasterProcedureEnv,RegionInfo,byte)",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 -\n 124 -\n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131  \n 132  \n 133  ",
            "  public SplitTableRegionProcedure(final MasterProcedureEnv env,\n      final RegionInfo regionToSplit, final byte[] splitRow) throws IOException {\n    super(env, regionToSplit);\n    this.bestSplitRow = splitRow;\n    checkSplittable(env, regionToSplit, bestSplitRow);\n    final TableName table = regionToSplit.getTable();\n    final long rid = getDaughterRegionIdTimestamp(regionToSplit);\n    this.daughter_1_RI = RegionInfoBuilder.newBuilder(table)\n        .setStartKey(regionToSplit.getStartKey())\n        .setEndKey(bestSplitRow)\n        .setSplit(false)\n        .setRegionId(rid)\n        .build();\n    this.daughter_2_RI = RegionInfoBuilder.newBuilder(table)\n        .setStartKey(bestSplitRow)\n        .setEndKey(regionToSplit.getEndKey())\n        .setSplit(false)\n        .setRegionId(rid)\n        .build();\n    this.htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n    if(this.htd.getRegionSplitPolicyClassName() != null) {\n      // Since we don't have region reference here, creating the split policy instance without it.\n      // This can be used to invoke methods which don't require Region reference. This instantiation\n      // of a class on Master-side though it only makes sense on the RegionServer-side is\n      // for Phoenix Local Indexing. Refer HBASE-12583 for more information.\n      Class<? extends RegionSplitPolicy> clazz =\n          RegionSplitPolicy.getSplitPolicyClass(this.htd, env.getMasterConfiguration());\n      this.splitPolicy = ReflectionUtils.newInstance(clazz, env.getMasterConfiguration());\n    }\n  }",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129 +\n 130  \n 131  \n 132  ",
            "  public SplitTableRegionProcedure(final MasterProcedureEnv env,\n      final RegionInfo regionToSplit, final byte[] splitRow) throws IOException {\n    super(env, regionToSplit);\n    this.bestSplitRow = splitRow;\n    checkSplittable(env, regionToSplit, bestSplitRow);\n    final TableName table = regionToSplit.getTable();\n    final long rid = getDaughterRegionIdTimestamp(regionToSplit);\n    this.daughter_1_RI = RegionInfoBuilder.newBuilder(table)\n        .setStartKey(regionToSplit.getStartKey())\n        .setEndKey(bestSplitRow)\n        .setSplit(false)\n        .setRegionId(rid)\n        .build();\n    this.daughter_2_RI = RegionInfoBuilder.newBuilder(table)\n        .setStartKey(bestSplitRow)\n        .setEndKey(regionToSplit.getEndKey())\n        .setSplit(false)\n        .setRegionId(rid)\n        .build();\n    TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n    if(htd.getRegionSplitPolicyClassName() != null) {\n      // Since we don't have region reference here, creating the split policy instance without it.\n      // This can be used to invoke methods which don't require Region reference. This instantiation\n      // of a class on Master-side though it only makes sense on the RegionServer-side is\n      // for Phoenix Local Indexing. Refer HBASE-12583 for more information.\n      Class<? extends RegionSplitPolicy> clazz =\n          RegionSplitPolicy.getSplitPolicyClass(htd, env.getMasterConfiguration());\n      this.splitPolicy = ReflectionUtils.newInstance(clazz, env.getMasterConfiguration());\n    }\n  }"
        ],
        [
            "SplitTableRegionProcedure::splitStoreFiles(MasterProcedureEnv,HRegionFileSystem)",
            " 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  ",
            "  /**\n   * Create Split directory\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  private Pair<Integer, Integer> splitStoreFiles(final MasterProcedureEnv env,\n      final HRegionFileSystem regionFs) throws IOException {\n    final MasterFileSystem mfs = env.getMasterServices().getMasterFileSystem();\n    final Configuration conf = env.getMasterConfiguration();\n    // The following code sets up a thread pool executor with as many slots as\n    // there's files to split. It then fires up everything, waits for\n    // completion and finally checks for any exception\n    //\n    // Note: splitStoreFiles creates daughter region dirs under the parent splits dir\n    // Nothing to unroll here if failure -- re-run createSplitsDir will\n    // clean this up.\n    int nbFiles = 0;\n    final Map<String, Collection<StoreFileInfo>> files =\n      new HashMap<String, Collection<StoreFileInfo>>(regionFs.getFamilies().size());\n    for (String family: regionFs.getFamilies()) {\n      Collection<StoreFileInfo> sfis = regionFs.getStoreFiles(family);\n      if (sfis == null) continue;\n      Collection<StoreFileInfo> filteredSfis = null;\n      for (StoreFileInfo sfi: sfis) {\n        // Filter. There is a lag cleaning up compacted reference files. They get cleared\n        // after a delay in case outstanding Scanners still have references. Because of this,\n        // the listing of the Store content may have straggler reference files. Skip these.\n        // It should be safe to skip references at this point because we checked above with\n        // the region if it thinks it is splittable and if we are here, it thinks it is\n        // splitable.\n        if (sfi.isReference()) {\n          LOG.info(\"Skipping split of \" + sfi + \"; presuming ready for archiving.\");\n          continue;\n        }\n        if (filteredSfis == null) {\n          filteredSfis = new ArrayList<StoreFileInfo>(sfis.size());\n          files.put(family, filteredSfis);\n        }\n        filteredSfis.add(sfi);\n        nbFiles++;\n      }\n    }\n    if (nbFiles == 0) {\n      // no file needs to be splitted.\n      return new Pair<Integer, Integer>(0,0);\n    }\n    // Max #threads is the smaller of the number of storefiles or the default max determined above.\n    int maxThreads = Math.min(\n      conf.getInt(HConstants.REGION_SPLIT_THREADS_MAX,\n        conf.getInt(HStore.BLOCKING_STOREFILES_KEY, HStore.DEFAULT_BLOCKING_STOREFILE_COUNT)),\n      nbFiles);\n    LOG.info(\"pid=\" + getProcId() + \" splitting \" + nbFiles + \" storefiles, region=\" +\n      getParentRegion().getShortNameToLog() + \", threads=\" + maxThreads);\n    final ExecutorService threadPool = Executors.newFixedThreadPool(\n      maxThreads, Threads.getNamedThreadFactory(\"StoreFileSplitter-%1$d\"));\n    final List<Future<Pair<Path,Path>>> futures = new ArrayList<Future<Pair<Path,Path>>>(nbFiles);\n\n    // Split each store file.\n    for (Map.Entry<String, Collection<StoreFileInfo>>e: files.entrySet()) {\n      byte [] familyName = Bytes.toBytes(e.getKey());\n      final ColumnFamilyDescriptor hcd = htd.getColumnFamily(familyName);\n      final Collection<StoreFileInfo> storeFiles = e.getValue();\n      if (storeFiles != null && storeFiles.size() > 0) {\n        final CacheConfig cacheConf = new CacheConfig(conf, hcd);\n        for (StoreFileInfo storeFileInfo: storeFiles) {\n          StoreFileSplitter sfs =\n              new StoreFileSplitter(regionFs, familyName, new HStoreFile(mfs.getFileSystem(),\n                  storeFileInfo, conf, cacheConf, hcd.getBloomFilterType(), true));\n          futures.add(threadPool.submit(sfs));\n        }\n      }\n    }\n    // Shutdown the pool\n    threadPool.shutdown();\n\n    // Wait for all the tasks to finish.\n    // When splits ran on the RegionServer, how-long-to-wait-configuration was named\n    // hbase.regionserver.fileSplitTimeout. If set, use its value.\n    long fileSplitTimeout = conf.getLong(\"hbase.master.fileSplitTimeout\",\n        conf.getLong(\"hbase.regionserver.fileSplitTimeout\", 600000));\n    try {\n      boolean stillRunning = !threadPool.awaitTermination(fileSplitTimeout, TimeUnit.MILLISECONDS);\n      if (stillRunning) {\n        threadPool.shutdownNow();\n        // wait for the thread to shutdown completely.\n        while (!threadPool.isTerminated()) {\n          Thread.sleep(50);\n        }\n        throw new IOException(\"Took too long to split the\" +\n            \" files and create the references, aborting split\");\n      }\n    } catch (InterruptedException e) {\n      throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n    }\n\n    int daughterA = 0;\n    int daughterB = 0;\n    // Look for any exception\n    for (Future<Pair<Path, Path>> future : futures) {\n      try {\n        Pair<Path, Path> p = future.get();\n        daughterA += p.getFirst() != null ? 1 : 0;\n        daughterB += p.getSecond() != null ? 1 : 0;\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException) new InterruptedIOException().initCause(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pid=\" + getProcId() + \" split storefiles for region \" +\n        getParentRegion().getShortNameToLog() +\n          \" Daughter A: \" + daughterA + \" storefiles, Daughter B: \" +\n          daughterB + \" storefiles.\");\n    }\n    return new Pair<Integer, Integer>(daughterA, daughterB);\n  }",
            " 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613 +\n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  ",
            "  /**\n   * Create Split directory\n   * @param env MasterProcedureEnv\n   * @throws IOException\n   */\n  private Pair<Integer, Integer> splitStoreFiles(final MasterProcedureEnv env,\n      final HRegionFileSystem regionFs) throws IOException {\n    final MasterFileSystem mfs = env.getMasterServices().getMasterFileSystem();\n    final Configuration conf = env.getMasterConfiguration();\n    // The following code sets up a thread pool executor with as many slots as\n    // there's files to split. It then fires up everything, waits for\n    // completion and finally checks for any exception\n    //\n    // Note: splitStoreFiles creates daughter region dirs under the parent splits dir\n    // Nothing to unroll here if failure -- re-run createSplitsDir will\n    // clean this up.\n    int nbFiles = 0;\n    final Map<String, Collection<StoreFileInfo>> files =\n      new HashMap<String, Collection<StoreFileInfo>>(regionFs.getFamilies().size());\n    for (String family: regionFs.getFamilies()) {\n      Collection<StoreFileInfo> sfis = regionFs.getStoreFiles(family);\n      if (sfis == null) continue;\n      Collection<StoreFileInfo> filteredSfis = null;\n      for (StoreFileInfo sfi: sfis) {\n        // Filter. There is a lag cleaning up compacted reference files. They get cleared\n        // after a delay in case outstanding Scanners still have references. Because of this,\n        // the listing of the Store content may have straggler reference files. Skip these.\n        // It should be safe to skip references at this point because we checked above with\n        // the region if it thinks it is splittable and if we are here, it thinks it is\n        // splitable.\n        if (sfi.isReference()) {\n          LOG.info(\"Skipping split of \" + sfi + \"; presuming ready for archiving.\");\n          continue;\n        }\n        if (filteredSfis == null) {\n          filteredSfis = new ArrayList<StoreFileInfo>(sfis.size());\n          files.put(family, filteredSfis);\n        }\n        filteredSfis.add(sfi);\n        nbFiles++;\n      }\n    }\n    if (nbFiles == 0) {\n      // no file needs to be splitted.\n      return new Pair<Integer, Integer>(0,0);\n    }\n    // Max #threads is the smaller of the number of storefiles or the default max determined above.\n    int maxThreads = Math.min(\n      conf.getInt(HConstants.REGION_SPLIT_THREADS_MAX,\n        conf.getInt(HStore.BLOCKING_STOREFILES_KEY, HStore.DEFAULT_BLOCKING_STOREFILE_COUNT)),\n      nbFiles);\n    LOG.info(\"pid=\" + getProcId() + \" splitting \" + nbFiles + \" storefiles, region=\" +\n      getParentRegion().getShortNameToLog() + \", threads=\" + maxThreads);\n    final ExecutorService threadPool = Executors.newFixedThreadPool(\n      maxThreads, Threads.getNamedThreadFactory(\"StoreFileSplitter-%1$d\"));\n    final List<Future<Pair<Path,Path>>> futures = new ArrayList<Future<Pair<Path,Path>>>(nbFiles);\n\n    TableDescriptor htd = env.getMasterServices().getTableDescriptors().get(getTableName());\n    // Split each store file.\n    for (Map.Entry<String, Collection<StoreFileInfo>>e: files.entrySet()) {\n      byte [] familyName = Bytes.toBytes(e.getKey());\n      final ColumnFamilyDescriptor hcd = htd.getColumnFamily(familyName);\n      final Collection<StoreFileInfo> storeFiles = e.getValue();\n      if (storeFiles != null && storeFiles.size() > 0) {\n        final CacheConfig cacheConf = new CacheConfig(conf, hcd);\n        for (StoreFileInfo storeFileInfo: storeFiles) {\n          StoreFileSplitter sfs =\n              new StoreFileSplitter(regionFs, familyName, new HStoreFile(mfs.getFileSystem(),\n                  storeFileInfo, conf, cacheConf, hcd.getBloomFilterType(), true));\n          futures.add(threadPool.submit(sfs));\n        }\n      }\n    }\n    // Shutdown the pool\n    threadPool.shutdown();\n\n    // Wait for all the tasks to finish.\n    // When splits ran on the RegionServer, how-long-to-wait-configuration was named\n    // hbase.regionserver.fileSplitTimeout. If set, use its value.\n    long fileSplitTimeout = conf.getLong(\"hbase.master.fileSplitTimeout\",\n        conf.getLong(\"hbase.regionserver.fileSplitTimeout\", 600000));\n    try {\n      boolean stillRunning = !threadPool.awaitTermination(fileSplitTimeout, TimeUnit.MILLISECONDS);\n      if (stillRunning) {\n        threadPool.shutdownNow();\n        // wait for the thread to shutdown completely.\n        while (!threadPool.isTerminated()) {\n          Thread.sleep(50);\n        }\n        throw new IOException(\"Took too long to split the\" +\n            \" files and create the references, aborting split\");\n      }\n    } catch (InterruptedException e) {\n      throw (InterruptedIOException)new InterruptedIOException().initCause(e);\n    }\n\n    int daughterA = 0;\n    int daughterB = 0;\n    // Look for any exception\n    for (Future<Pair<Path, Path>> future : futures) {\n      try {\n        Pair<Path, Path> p = future.get();\n        daughterA += p.getFirst() != null ? 1 : 0;\n        daughterB += p.getSecond() != null ? 1 : 0;\n      } catch (InterruptedException e) {\n        throw (InterruptedIOException) new InterruptedIOException().initCause(e);\n      } catch (ExecutionException e) {\n        throw new IOException(e);\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"pid=\" + getProcId() + \" split storefiles for region \" +\n        getParentRegion().getShortNameToLog() +\n          \" Daughter A: \" + daughterA + \" storefiles, Daughter B: \" +\n          daughterB + \" storefiles.\");\n    }\n    return new Pair<Integer, Integer>(daughterA, daughterB);\n  }"
        ]
    ],
    "000a84cb4afce33e6d6601f65955f2de9924cf8a": [
        [
            "BackupLogCleaner::init(Map)",
            "  56  \n  57  \n  58 -\n  59 -\n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "  @Override\n  public void init(Map<String, Object> params) {\n    if (params != null && params.containsKey(HMaster.MASTER)) {\n      MasterServices master = (MasterServices) params.get(HMaster.MASTER);\n      conn = master.getConnection();\n      if (getConf() == null) {\n        super.setConf(conn.getConfiguration());\n      }\n    }\n    if (conn == null) {\n      try {\n        conn = ConnectionFactory.createConnection(getConf());\n      } catch (IOException ioe) {\n        throw new RuntimeException(\"Failed to create connection\", ioe);\n      }\n    }\n  }",
            "  59  \n  60  \n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  ",
            "  @Override\n  public void init(Map<String, Object> params) {\n    MasterServices master = (MasterServices) MapUtils.getObject(params,\n      HMaster.MASTER);\n    if (master != null) {\n      conn = master.getConnection();\n      if (getConf() == null) {\n        super.setConf(conn.getConfiguration());\n      }\n    }\n    if (conn == null) {\n      try {\n        conn = ConnectionFactory.createConnection(getConf());\n      } catch (IOException ioe) {\n        throw new RuntimeException(\"Failed to create connection\", ioe);\n      }\n    }\n  }"
        ],
        [
            "BackupLogCleaner::getDeletableFiles(Iterable)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79 -\n  80 -\n  81 -\n  82 -\n  83  \n  84  \n  85  \n  86 -\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106 -\n 107 -\n 108  \n 109  \n 110 -\n 111 -\n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 -\n 120  \n 121  ",
            "  @Override\n  public Iterable<FileStatus> getDeletableFiles(Iterable<FileStatus> files) {\n    // all members of this class are null if backup is disabled,\n    // so we cannot filter the files\n    if (this.getConf() == null || !BackupManager.isBackupEnabled(getConf())) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Backup is not enabled. Check your \"\n            + BackupRestoreConstants.BACKUP_ENABLE_KEY + \" setting\");\n      }\n      return files;\n    }\n\n    List<FileStatus> list = new ArrayList<>();\n    try (final BackupSystemTable table = new BackupSystemTable(conn)) {\n      // If we do not have recorded backup sessions\n      try {\n        if (!table.hasBackupSessions()) {\n          LOG.trace(\"BackupLogCleaner has no backup sessions\");\n          return files;\n        }\n      } catch (TableNotFoundException tnfe) {\n        LOG.warn(\"backup system table is not available\" + tnfe.getMessage());\n        return files;\n      }\n\n      Map<FileStatus, Boolean> walFilesDeletableMap = table.areWALFilesDeletable(files);\n      for (Map.Entry<FileStatus, Boolean> entry: walFilesDeletableMap.entrySet()) {\n        FileStatus file = entry.getKey();\n        String wal = file.getPath().toString();\n        boolean deletable = entry.getValue();\n        if (deletable) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Found log file in backup system table, deleting: \" + wal);\n          }\n          list.add(file);\n        } else {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Didn't find this log in backup system table, keeping: \" + wal);\n          }\n        }\n      }\n      return list;\n    } catch (IOException e) {\n      LOG.error(\"Failed to get backup system table table, therefore will keep all files\", e);\n      // nothing to delete\n      return new ArrayList<>();\n    }\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83 +\n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 +\n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 +\n 118  \n 119  ",
            "  @Override\n  public Iterable<FileStatus> getDeletableFiles(Iterable<FileStatus> files) {\n    // all members of this class are null if backup is disabled,\n    // so we cannot filter the files\n    if (this.getConf() == null || !BackupManager.isBackupEnabled(getConf())) {\n      LOG.debug(\"Backup is not enabled. Check your {} setting\",\n          BackupRestoreConstants.BACKUP_ENABLE_KEY);\n      return files;\n    }\n\n    try (final BackupSystemTable table = new BackupSystemTable(conn)) {\n      // If we do not have recorded backup sessions\n      try {\n        if (!table.hasBackupSessions()) {\n          LOG.trace(\"BackupLogCleaner has no backup sessions\");\n          return files;\n        }\n      } catch (TableNotFoundException tnfe) {\n        LOG.warn(\"Backup system table is not available: {}\", tnfe.getMessage());\n        return files;\n      }\n\n      List<FileStatus> list = new ArrayList<>();\n      Map<FileStatus, Boolean> walFilesDeletableMap = table.areWALFilesDeletable(files);\n      for (Map.Entry<FileStatus, Boolean> entry: walFilesDeletableMap.entrySet()) {\n        FileStatus file = entry.getKey();\n        String wal = file.getPath().toString();\n        boolean deletable = entry.getValue();\n        if (deletable) {\n          LOG.debug(\"Found log file in backup system table, deleting: {}\", wal);\n          list.add(file);\n        } else {\n          LOG.debug(\"Did not find this log in backup system table, keeping: {}\", wal);\n        }\n      }\n      return list;\n    } catch (IOException e) {\n      LOG.error(\"Failed to get backup system table table, therefore will keep all files\", e);\n      // nothing to delete\n      return Collections.emptyList();\n    }\n  }"
        ],
        [
            "BackupLogCleaner::stop(String)",
            " 133  \n 134  \n 135 -\n 136 -\n 137  \n 138 -\n 139 -\n 140  ",
            "  @Override\n  public void stop(String why) {\n    if (this.stopped) {\n      return;\n    }\n    this.stopped = true;\n    LOG.info(\"Stopping BackupLogCleaner\");\n  }",
            " 131  \n 132  \n 133 +\n 134 +\n 135 +\n 136  \n 137  ",
            "  @Override\n  public void stop(String why) {\n    if (!this.stopped) {\n      this.stopped = true;\n      LOG.info(\"Stopping BackupLogCleaner\");\n    }\n  }"
        ]
    ],
    "97c13338313b6ccf75f3b7988e97f0db5bf5e177": [
        [
            "VerifyReplication::printUsage(String)",
            " 485  \n 486  \n 487  \n 488  \n 489  \n 490 -\n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  ",
            "  private static void printUsage(final String errorMsg) {\n    if (errorMsg != null && errorMsg.length() > 0) {\n      System.err.println(\"ERROR: \" + errorMsg);\n    }\n    System.err.println(\"Usage: verifyrep [--starttime=X]\" +\n        \" [--endtime=Y] [--families=A] [--row-prefixes=B] [--delimiter=] <peerid> <tablename>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" starttime    beginning of the time range\");\n    System.err.println(\"              without endtime means from starttime to forever\");\n    System.err.println(\" endtime      end of the time range\");\n    System.err.println(\" versions     number of cell versions to verify\");\n    System.err.println(\" families     comma-separated list of families to copy\");\n    System.err.println(\" row-prefixes comma-separated list of row key prefixes to filter on \");\n    System.err.println(\" delimiter    the delimiter used in display around rowkey\");\n    System.err.println(\" recomparesleep   milliseconds to sleep before recompare row, \" +\n        \"default value is 0 which disables the recompare.\");\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" peerid       Id of the peer used for verification, must match the one given for replication\");\n    System.err.println(\" tablename    Name of the table to verify\");\n    System.err.println();\n    System.err.println(\"Examples:\");\n    System.err.println(\" To verify the data replicated from TestTable for a 1 hour window with peer #5 \");\n    System.err.println(\" $ bin/hbase \" +\n        \"org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication\" +\n        \" --starttime=1265875194289 --endtime=1265878794289 5 TestTable \");\n  }",
            " 499  \n 500  \n 501  \n 502  \n 503  \n 504 +\n 505 +\n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517 +\n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  ",
            "  private static void printUsage(final String errorMsg) {\n    if (errorMsg != null && errorMsg.length() > 0) {\n      System.err.println(\"ERROR: \" + errorMsg);\n    }\n    System.err.println(\"Usage: verifyrep [--starttime=X]\" +\n        \" [--endtime=Y] [--families=A] [--row-prefixes=B] [--delimiter=] [--recomparesleep=] \" +\n        \"[--verbose] <peerid> <tablename>\");\n    System.err.println();\n    System.err.println(\"Options:\");\n    System.err.println(\" starttime    beginning of the time range\");\n    System.err.println(\"              without endtime means from starttime to forever\");\n    System.err.println(\" endtime      end of the time range\");\n    System.err.println(\" versions     number of cell versions to verify\");\n    System.err.println(\" families     comma-separated list of families to copy\");\n    System.err.println(\" row-prefixes comma-separated list of row key prefixes to filter on \");\n    System.err.println(\" delimiter    the delimiter used in display around rowkey\");\n    System.err.println(\" recomparesleep   milliseconds to sleep before recompare row, \" +\n        \"default value is 0 which disables the recompare.\");\n    System.err.println(\" verbose      logs row keys of good rows\");\n    System.err.println();\n    System.err.println(\"Args:\");\n    System.err.println(\" peerid       Id of the peer used for verification, must match the one given for replication\");\n    System.err.println(\" tablename    Name of the table to verify\");\n    System.err.println();\n    System.err.println(\"Examples:\");\n    System.err.println(\" To verify the data replicated from TestTable for a 1 hour window with peer #5 \");\n    System.err.println(\" $ bin/hbase \" +\n        \"org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication\" +\n        \" --starttime=1265875194289 --endtime=1265878794289 5 TestTable \");\n  }"
        ],
        [
            "VerifyReplication::Verifier::map(ImmutableBytesWritable,Result,Context)",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "    /**\n     * Map method that compares every scanned row with the equivalent from\n     * a distant cluster.\n     * @param row  The current table row key.\n     * @param value  The columns.\n     * @param context  The current context.\n     * @throws IOException When something is broken with the data.\n     */\n    @Override\n    public void map(ImmutableBytesWritable row, final Result value,\n                    Context context)\n        throws IOException {\n      if (replicatedScanner == null) {\n        Configuration conf = context.getConfiguration();\n        sleepMsBeforeReCompare = conf.getInt(NAME +\".sleepMsBeforeReCompare\", 0);\n        final Scan scan = new Scan();\n        scan.setBatch(batch);\n        scan.setCacheBlocks(false);\n        scan.setCaching(conf.getInt(TableInputFormat.SCAN_CACHEDROWS, 1));\n        long startTime = conf.getLong(NAME + \".startTime\", 0);\n        long endTime = conf.getLong(NAME + \".endTime\", Long.MAX_VALUE);\n        String families = conf.get(NAME + \".families\", null);\n        if(families != null) {\n          String[] fams = families.split(\",\");\n          for(String fam : fams) {\n            scan.addFamily(Bytes.toBytes(fam));\n          }\n        }\n        String rowPrefixes = conf.get(NAME + \".rowPrefixes\", null);\n        setRowPrefixFilter(scan, rowPrefixes);\n        scan.setTimeRange(startTime, endTime);\n        int versions = conf.getInt(NAME+\".versions\", -1);\n        LOG.info(\"Setting number of version inside map as: \" + versions);\n        if (versions >= 0) {\n          scan.setMaxVersions(versions);\n        }\n        TableName tableName = TableName.valueOf(conf.get(NAME + \".tableName\"));\n        sourceConnection = ConnectionFactory.createConnection(conf);\n        sourceTable = sourceConnection.getTable(tableName);\n\n        final TableSplit tableSplit = (TableSplit)(context.getInputSplit());\n\n        String zkClusterKey = conf.get(NAME + \".peerQuorumAddress\");\n        Configuration peerConf = HBaseConfiguration.createClusterConf(conf,\n            zkClusterKey, PEER_CONFIG_PREFIX);\n\n        replicatedConnection = ConnectionFactory.createConnection(peerConf);\n        replicatedTable = replicatedConnection.getTable(tableName);\n        scan.setStartRow(value.getRow());\n        scan.setStopRow(tableSplit.getEndRow());\n        replicatedScanner = replicatedTable.getScanner(scan);\n        currentCompareRowInPeerTable = replicatedScanner.next();\n      }\n      while (true) {\n        if (currentCompareRowInPeerTable == null) {\n          // reach the region end of peer table, row only in source table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_SOURCE_TABLE_ROWS, value);\n          break;\n        }\n        int rowCmpRet = Bytes.compareTo(value.getRow(), currentCompareRowInPeerTable.getRow());\n        if (rowCmpRet == 0) {\n          // rowkey is same, need to compare the content of the row\n          try {\n            Result.compareResults(value, currentCompareRowInPeerTable);\n            context.getCounter(Counters.GOODROWS).increment(1);\n          } catch (Exception e) {\n            logFailRowAndIncreaseCounter(context, Counters.CONTENT_DIFFERENT_ROWS, value);\n            LOG.error(\"Exception while comparing row : \" + e);\n          }\n          currentCompareRowInPeerTable = replicatedScanner.next();\n          break;\n        } else if (rowCmpRet < 0) {\n          // row only exists in source table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_SOURCE_TABLE_ROWS, value);\n          break;\n        } else {\n          // row only exists in peer table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_PEER_TABLE_ROWS,\n            currentCompareRowInPeerTable);\n          currentCompareRowInPeerTable = replicatedScanner.next();\n        }\n      }\n    }",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179 +\n 180 +\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  ",
            "    /**\n     * Map method that compares every scanned row with the equivalent from\n     * a distant cluster.\n     * @param row  The current table row key.\n     * @param value  The columns.\n     * @param context  The current context.\n     * @throws IOException When something is broken with the data.\n     */\n    @Override\n    public void map(ImmutableBytesWritable row, final Result value,\n                    Context context)\n        throws IOException {\n      if (replicatedScanner == null) {\n        Configuration conf = context.getConfiguration();\n        sleepMsBeforeReCompare = conf.getInt(NAME +\".sleepMsBeforeReCompare\", 0);\n        verbose = conf.getBoolean(NAME +\".verbose\", false);\n        final Scan scan = new Scan();\n        scan.setBatch(batch);\n        scan.setCacheBlocks(false);\n        scan.setCaching(conf.getInt(TableInputFormat.SCAN_CACHEDROWS, 1));\n        long startTime = conf.getLong(NAME + \".startTime\", 0);\n        long endTime = conf.getLong(NAME + \".endTime\", Long.MAX_VALUE);\n        String families = conf.get(NAME + \".families\", null);\n        if(families != null) {\n          String[] fams = families.split(\",\");\n          for(String fam : fams) {\n            scan.addFamily(Bytes.toBytes(fam));\n          }\n        }\n        String rowPrefixes = conf.get(NAME + \".rowPrefixes\", null);\n        setRowPrefixFilter(scan, rowPrefixes);\n        scan.setTimeRange(startTime, endTime);\n        int versions = conf.getInt(NAME+\".versions\", -1);\n        LOG.info(\"Setting number of version inside map as: \" + versions);\n        if (versions >= 0) {\n          scan.setMaxVersions(versions);\n        }\n        TableName tableName = TableName.valueOf(conf.get(NAME + \".tableName\"));\n        sourceConnection = ConnectionFactory.createConnection(conf);\n        sourceTable = sourceConnection.getTable(tableName);\n\n        final TableSplit tableSplit = (TableSplit)(context.getInputSplit());\n\n        String zkClusterKey = conf.get(NAME + \".peerQuorumAddress\");\n        Configuration peerConf = HBaseConfiguration.createClusterConf(conf,\n            zkClusterKey, PEER_CONFIG_PREFIX);\n\n        replicatedConnection = ConnectionFactory.createConnection(peerConf);\n        replicatedTable = replicatedConnection.getTable(tableName);\n        scan.setStartRow(value.getRow());\n        scan.setStopRow(tableSplit.getEndRow());\n        replicatedScanner = replicatedTable.getScanner(scan);\n        currentCompareRowInPeerTable = replicatedScanner.next();\n      }\n      while (true) {\n        if (currentCompareRowInPeerTable == null) {\n          // reach the region end of peer table, row only in source table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_SOURCE_TABLE_ROWS, value);\n          break;\n        }\n        int rowCmpRet = Bytes.compareTo(value.getRow(), currentCompareRowInPeerTable.getRow());\n        if (rowCmpRet == 0) {\n          // rowkey is same, need to compare the content of the row\n          try {\n            Result.compareResults(value, currentCompareRowInPeerTable);\n            context.getCounter(Counters.GOODROWS).increment(1);\n            if (verbose) {\n              LOG.info(\"Good row key: \" + delimiter + Bytes.toString(value.getRow()) + delimiter);\n            }\n          } catch (Exception e) {\n            logFailRowAndIncreaseCounter(context, Counters.CONTENT_DIFFERENT_ROWS, value);\n            LOG.error(\"Exception while comparing row : \" + e);\n          }\n          currentCompareRowInPeerTable = replicatedScanner.next();\n          break;\n        } else if (rowCmpRet < 0) {\n          // row only exists in source table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_SOURCE_TABLE_ROWS, value);\n          break;\n        } else {\n          // row only exists in peer table\n          logFailRowAndIncreaseCounter(context, Counters.ONLY_IN_PEER_TABLE_ROWS,\n            currentCompareRowInPeerTable);\n          currentCompareRowInPeerTable = replicatedScanner.next();\n        }\n      }\n    }"
        ],
        [
            "VerifyReplication::Verifier::logFailRowAndIncreaseCounter(Context,Counters,Result)",
            " 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  ",
            "    private void logFailRowAndIncreaseCounter(Context context, Counters counter, Result row) {\n      if (sleepMsBeforeReCompare > 0) {\n        Threads.sleep(sleepMsBeforeReCompare);\n        try {\n          Result sourceResult = sourceTable.get(new Get(row.getRow()));\n          Result replicatedResult = replicatedTable.get(new Get(row.getRow()));\n          Result.compareResults(sourceResult, replicatedResult);\n          return;\n        } catch (Exception e) {\n          LOG.error(\"recompare fail after sleep, rowkey=\" + delimiter +\n              Bytes.toString(row.getRow()) + delimiter);\n        }\n      }\n      context.getCounter(counter).increment(1);\n      context.getCounter(Counters.BADROWS).increment(1);\n      LOG.error(counter.toString() + \", rowkey=\" + delimiter + Bytes.toString(row.getRow()) +\n          delimiter);\n    }",
            " 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 +\n 208 +\n 209 +\n 210 +\n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "    private void logFailRowAndIncreaseCounter(Context context, Counters counter, Result row) {\n      if (sleepMsBeforeReCompare > 0) {\n        Threads.sleep(sleepMsBeforeReCompare);\n        try {\n          Result sourceResult = sourceTable.get(new Get(row.getRow()));\n          Result replicatedResult = replicatedTable.get(new Get(row.getRow()));\n          Result.compareResults(sourceResult, replicatedResult);\n          context.getCounter(Counters.GOODROWS).increment(1);\n          if (verbose) {\n            LOG.info(\"Good row key: \" + delimiter + Bytes.toString(row.getRow()) + delimiter);\n          }\n          return;\n        } catch (Exception e) {\n          LOG.error(\"recompare fail after sleep, rowkey=\" + delimiter +\n              Bytes.toString(row.getRow()) + delimiter);\n        }\n      }\n      context.getCounter(counter).increment(1);\n      context.getCounter(Counters.BADROWS).increment(1);\n      LOG.error(counter.toString() + \", rowkey=\" + delimiter + Bytes.toString(row.getRow()) +\n          delimiter);\n    }"
        ],
        [
            "VerifyReplication::doCommandLine(String)",
            " 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454 -\n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  ",
            "  private static boolean doCommandLine(final String[] args) {\n    if (args.length < 2) {\n      printUsage(null);\n      return false;\n    }\n    //in case we've been run before, restore all parameters to their initial states\n    //Otherwise, if our previous run included a parameter not in args this time,\n    //we might hold on to the old value.\n    restoreDefaults();\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n        \n        final String batchArgKey = \"--batch=\";\n        if (cmd.startsWith(batchArgKey)) {\n          batch = Integer.parseInt(cmd.substring(batchArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        final String rowPrefixesKey = \"--row-prefixes=\";\n        if (cmd.startsWith(rowPrefixesKey)){\n          rowPrefixes = cmd.substring(rowPrefixesKey.length());\n          continue;\n        }\n\n        if (cmd.startsWith(\"--\")) {\n          printUsage(\"Invalid argument '\" + cmd + \"'\");\n        }\n\n        final String delimiterArgKey = \"--delimiter=\";\n        if (cmd.startsWith(delimiterArgKey)) {\n          delimiter = cmd.substring(delimiterArgKey.length());\n          continue;\n        }\n\n        final String sleepToReCompareKey = \"--recomparesleep=\";\n        if (cmd.startsWith(sleepToReCompareKey)) {\n          sleepMsBeforeReCompare = Integer.parseInt(cmd.substring(sleepToReCompareKey.length()));\n          continue;\n        }\n        \n        if (i == args.length-2) {\n          peerId = cmd;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        }\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }",
            " 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464 +\n 465 +\n 466 +\n 467 +\n 468 +\n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  ",
            "  private static boolean doCommandLine(final String[] args) {\n    if (args.length < 2) {\n      printUsage(null);\n      return false;\n    }\n    //in case we've been run before, restore all parameters to their initial states\n    //Otherwise, if our previous run included a parameter not in args this time,\n    //we might hold on to the old value.\n    restoreDefaults();\n    try {\n      for (int i = 0; i < args.length; i++) {\n        String cmd = args[i];\n        if (cmd.equals(\"-h\") || cmd.startsWith(\"--h\")) {\n          printUsage(null);\n          return false;\n        }\n\n        final String startTimeArgKey = \"--starttime=\";\n        if (cmd.startsWith(startTimeArgKey)) {\n          startTime = Long.parseLong(cmd.substring(startTimeArgKey.length()));\n          continue;\n        }\n\n        final String endTimeArgKey = \"--endtime=\";\n        if (cmd.startsWith(endTimeArgKey)) {\n          endTime = Long.parseLong(cmd.substring(endTimeArgKey.length()));\n          continue;\n        }\n\n        final String versionsArgKey = \"--versions=\";\n        if (cmd.startsWith(versionsArgKey)) {\n          versions = Integer.parseInt(cmd.substring(versionsArgKey.length()));\n          continue;\n        }\n        \n        final String batchArgKey = \"--batch=\";\n        if (cmd.startsWith(batchArgKey)) {\n          batch = Integer.parseInt(cmd.substring(batchArgKey.length()));\n          continue;\n        }\n\n        final String familiesArgKey = \"--families=\";\n        if (cmd.startsWith(familiesArgKey)) {\n          families = cmd.substring(familiesArgKey.length());\n          continue;\n        }\n\n        final String rowPrefixesKey = \"--row-prefixes=\";\n        if (cmd.startsWith(rowPrefixesKey)){\n          rowPrefixes = cmd.substring(rowPrefixesKey.length());\n          continue;\n        }\n\n        if (cmd.startsWith(\"--\")) {\n          printUsage(\"Invalid argument '\" + cmd + \"'\");\n        }\n\n        final String delimiterArgKey = \"--delimiter=\";\n        if (cmd.startsWith(delimiterArgKey)) {\n          delimiter = cmd.substring(delimiterArgKey.length());\n          continue;\n        }\n\n        final String sleepToReCompareKey = \"--recomparesleep=\";\n        if (cmd.startsWith(sleepToReCompareKey)) {\n          sleepMsBeforeReCompare = Integer.parseInt(cmd.substring(sleepToReCompareKey.length()));\n          continue;\n        }\n        final String verboseKey = \"--verbose\";\n        if (cmd.startsWith(verboseKey)) {\n          verbose = true;\n          continue;\n        }        \n        if (i == args.length-2) {\n          peerId = cmd;\n        }\n\n        if (i == args.length-1) {\n          tableName = cmd;\n        }\n      }\n    } catch (Exception e) {\n      e.printStackTrace();\n      printUsage(\"Can't start because \" + e.getMessage());\n      return false;\n    }\n    return true;\n  }"
        ],
        [
            "VerifyReplication::createSubmittableJob(Configuration,String)",
            " 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  ",
            "  /**\n   * Sets up the actual job.\n   *\n   * @param conf  The current configuration.\n   * @param args  The command line parameters.\n   * @return The newly created job.\n   * @throws java.io.IOException When setting up the job fails.\n   */\n  public static Job createSubmittableJob(Configuration conf, String[] args)\n  throws IOException {\n    if (!doCommandLine(args)) {\n      return null;\n    }\n    conf.set(NAME+\".peerId\", peerId);\n    conf.set(NAME+\".tableName\", tableName);\n    conf.setLong(NAME+\".startTime\", startTime);\n    conf.setLong(NAME+\".endTime\", endTime);\n    conf.setInt(NAME +\".sleepMsBeforeReCompare\", sleepMsBeforeReCompare);\n    if (families != null) {\n      conf.set(NAME+\".families\", families);\n    }\n    if (rowPrefixes != null){\n      conf.set(NAME+\".rowPrefixes\", rowPrefixes);\n    }\n\n    Pair<ReplicationPeerConfig, Configuration> peerConfigPair = getPeerQuorumConfig(conf);\n    ReplicationPeerConfig peerConfig = peerConfigPair.getFirst();\n    String peerQuorumAddress = peerConfig.getClusterKey();\n    LOG.info(\"Peer Quorum Address: \" + peerQuorumAddress + \", Peer Configuration: \" +\n        peerConfig.getConfiguration());\n    conf.set(NAME + \".peerQuorumAddress\", peerQuorumAddress);\n    HBaseConfiguration.setWithPrefix(conf, PEER_CONFIG_PREFIX,\n        peerConfig.getConfiguration().entrySet());\n\n    conf.setInt(NAME + \".versions\", versions);\n    LOG.info(\"Number of version: \" + versions);\n\n    Job job = Job.getInstance(conf, conf.get(JOB_NAME_CONF_KEY, NAME + \"_\" + tableName));\n    job.setJarByClass(VerifyReplication.class);\n\n    Scan scan = new Scan();\n    scan.setTimeRange(startTime, endTime);\n    if (versions >= 0) {\n      scan.setMaxVersions(versions);\n      LOG.info(\"Number of versions set to \" + versions);\n    }\n    if(families != null) {\n      String[] fams = families.split(\",\");\n      for(String fam : fams) {\n        scan.addFamily(Bytes.toBytes(fam));\n      }\n    }\n\n    setRowPrefixFilter(scan, rowPrefixes);\n\n    TableMapReduceUtil.initTableMapperJob(tableName, scan,\n        Verifier.class, null, null, job);\n\n    Configuration peerClusterConf = peerConfigPair.getSecond();\n    // Obtain the auth token from peer cluster\n    TableMapReduceUtil.initCredentialsForCluster(job, peerClusterConf);\n\n    job.setOutputFormatClass(NullOutputFormat.class);\n    job.setNumReduceTasks(0);\n    return job;\n  }",
            " 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 +\n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  ",
            "  /**\n   * Sets up the actual job.\n   *\n   * @param conf  The current configuration.\n   * @param args  The command line parameters.\n   * @return The newly created job.\n   * @throws java.io.IOException When setting up the job fails.\n   */\n  public static Job createSubmittableJob(Configuration conf, String[] args)\n  throws IOException {\n    if (!doCommandLine(args)) {\n      return null;\n    }\n    conf.set(NAME+\".peerId\", peerId);\n    conf.set(NAME+\".tableName\", tableName);\n    conf.setLong(NAME+\".startTime\", startTime);\n    conf.setLong(NAME+\".endTime\", endTime);\n    conf.setInt(NAME +\".sleepMsBeforeReCompare\", sleepMsBeforeReCompare);\n    conf.setBoolean(NAME +\".verbose\", verbose);\n    if (families != null) {\n      conf.set(NAME+\".families\", families);\n    }\n    if (rowPrefixes != null){\n      conf.set(NAME+\".rowPrefixes\", rowPrefixes);\n    }\n\n    Pair<ReplicationPeerConfig, Configuration> peerConfigPair = getPeerQuorumConfig(conf);\n    ReplicationPeerConfig peerConfig = peerConfigPair.getFirst();\n    String peerQuorumAddress = peerConfig.getClusterKey();\n    LOG.info(\"Peer Quorum Address: \" + peerQuorumAddress + \", Peer Configuration: \" +\n        peerConfig.getConfiguration());\n    conf.set(NAME + \".peerQuorumAddress\", peerQuorumAddress);\n    HBaseConfiguration.setWithPrefix(conf, PEER_CONFIG_PREFIX,\n        peerConfig.getConfiguration().entrySet());\n\n    conf.setInt(NAME + \".versions\", versions);\n    LOG.info(\"Number of version: \" + versions);\n\n    Job job = Job.getInstance(conf, conf.get(JOB_NAME_CONF_KEY, NAME + \"_\" + tableName));\n    job.setJarByClass(VerifyReplication.class);\n\n    Scan scan = new Scan();\n    scan.setTimeRange(startTime, endTime);\n    if (versions >= 0) {\n      scan.setMaxVersions(versions);\n      LOG.info(\"Number of versions set to \" + versions);\n    }\n    if(families != null) {\n      String[] fams = families.split(\",\");\n      for(String fam : fams) {\n        scan.addFamily(Bytes.toBytes(fam));\n      }\n    }\n\n    setRowPrefixFilter(scan, rowPrefixes);\n\n    TableMapReduceUtil.initTableMapperJob(tableName, scan,\n        Verifier.class, null, null, job);\n\n    Configuration peerClusterConf = peerConfigPair.getSecond();\n    // Obtain the auth token from peer cluster\n    TableMapReduceUtil.initCredentialsForCluster(job, peerClusterConf);\n\n    job.setOutputFormatClass(NullOutputFormat.class);\n    job.setNumReduceTasks(0);\n    return job;\n  }"
        ]
    ],
    "3c0750de54276cf19a35feb6351c71b8dea360b7": [
        [
            "HBaseInterClusterReplicationEndpoint::replicate(ReplicateContext)",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  ",
            "  /**\n   * Do the shipping logic\n   */\n  @Override\n  public boolean replicate(ReplicateContext replicateContext) {\n    CompletionService<Integer> pool = new ExecutorCompletionService<Integer>(this.exec);\n    List<Entry> entries = replicateContext.getEntries();\n    String walGroupId = replicateContext.getWalGroupId();\n    int sleepMultiplier = 1;\n    int numReplicated = 0;\n\n    if (!peersSelected && this.isRunning()) {\n      connectToPeers();\n      peersSelected = true;\n    }\n\n    int numSinks = replicationSinkMgr.getNumSinks();\n    if (numSinks == 0) {\n      LOG.warn(\"No replication sinks found, returning without replicating. The source should retry\"\n          + \" with the same set of edits.\");\n      return false;\n    }\n\n    // minimum of: configured threads, number of 100-waledit batches,\n    //  and number of current sinks\n    int n = Math.min(Math.min(this.maxThreads, entries.size()/100+1), numSinks);\n\n    List<List<Entry>> entryLists = new ArrayList<List<Entry>>(n);\n    if (n == 1) {\n      entryLists.add(entries);\n    } else {\n      for (int i=0; i<n; i++) {\n        entryLists.add(new ArrayList<Entry>(entries.size()/n+1));\n      }\n      // now group by region\n      for (Entry e : entries) {\n        entryLists.get(Math.abs(Bytes.hashCode(e.getKey().getEncodedRegionName())%n)).add(e);\n      }\n    }\n    while (this.isRunning() && !exec.isShutdown()) {\n      if (!isPeerEnabled()) {\n        if (sleepForRetries(\"Replication is disabled\", sleepMultiplier)) {\n          sleepMultiplier++;\n        }\n        continue;\n      }\n      try {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Replicating \" + entries.size() +\n              \" entries of total size \" + replicateContext.getSize());\n        }\n\n        int futures = 0;\n        for (int i=0; i<entryLists.size(); i++) {\n          if (!entryLists.get(i).isEmpty()) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Submitting \" + entryLists.get(i).size() +\n                  \" entries of total size \" + replicateContext.getSize());\n            }\n            // RuntimeExceptions encountered here bubble up and are handled in ReplicationSource\n            pool.submit(createReplicator(entryLists.get(i), i));\n            futures++;\n          }\n        }\n        IOException iox = null;\n\n        for (int i=0; i<futures; i++) {\n          try {\n            // wait for all futures, remove successful parts\n            // (only the remaining parts will be retried)\n            Future<Integer> f = pool.take();\n            int index = f.get().intValue();\n            int batchSize =  entryLists.get(index).size();\n            entryLists.set(index, Collections.<Entry>emptyList());\n            // Now, we have marked the batch as done replicating, record its size\n            numReplicated += batchSize;\n          } catch (InterruptedException ie) {\n            iox =  new IOException(ie);\n          } catch (ExecutionException ee) {\n            // cause must be an IOException\n            iox = (IOException)ee.getCause();\n          }\n        }\n        if (iox != null) {\n          // if we had any exceptions, try again\n          throw iox;\n        }\n        if (numReplicated != entries.size()) {\n          // Something went wrong here and we don't know what, let's just fail and retry.\n          LOG.warn(\"The number of edits replicated is different from the number received,\"\n              + \" failing for now.\");\n          return false;\n        }\n        // update metrics\n        this.metrics.setAgeOfLastShippedOp(entries.get(entries.size() - 1).getKey().getWriteTime(),\n          walGroupId);\n        return true;\n\n      } catch (IOException ioe) {\n        // Didn't ship anything, but must still age the last time we did\n        this.metrics.refreshAgeOfLastShippedOp(walGroupId);\n        if (ioe instanceof RemoteException) {\n          ioe = ((RemoteException) ioe).unwrapRemoteException();\n          LOG.warn(\"Can't replicate because of an error on the remote cluster: \", ioe);\n          if (ioe instanceof TableNotFoundException) {\n            if (sleepForRetries(\"A table is missing in the peer cluster. \"\n                + \"Replication cannot proceed without losing data.\", sleepMultiplier)) {\n              sleepMultiplier++;\n            }\n          }\n        } else {\n          if (ioe instanceof SocketTimeoutException) {\n            // This exception means we waited for more than 60s and nothing\n            // happened, the cluster is alive and calling it right away\n            // even for a test just makes things worse.\n            sleepForRetries(\"Encountered a SocketTimeoutException. Since the \" +\n              \"call to the remote cluster timed out, which is usually \" +\n              \"caused by a machine failure or a massive slowdown\",\n              this.socketTimeoutMultiplier);\n          } else if (ioe instanceof ConnectException) {\n            LOG.warn(\"Peer is unavailable, rechecking all sinks: \", ioe);\n            replicationSinkMgr.chooseSinks();\n          } else {\n            LOG.warn(\"Can't replicate because of a local or network error: \", ioe);\n          }\n        }\n        if (sleepForRetries(\"Since we are unable to replicate\", sleepMultiplier)) {\n          sleepMultiplier++;\n        }\n      }\n    }\n    return false; // in case we exited before replicating\n  }",
            " 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297 +\n 298 +\n 299 +\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  ",
            "  /**\n   * Do the shipping logic\n   */\n  @Override\n  public boolean replicate(ReplicateContext replicateContext) {\n    CompletionService<Integer> pool = new ExecutorCompletionService<Integer>(this.exec);\n    List<Entry> entries = replicateContext.getEntries();\n    String walGroupId = replicateContext.getWalGroupId();\n    int sleepMultiplier = 1;\n    int numReplicated = 0;\n\n    if (!peersSelected && this.isRunning()) {\n      connectToPeers();\n      peersSelected = true;\n    }\n\n    int numSinks = replicationSinkMgr.getNumSinks();\n    if (numSinks == 0) {\n      LOG.warn(\"No replication sinks found, returning without replicating. The source should retry\"\n          + \" with the same set of edits.\");\n      return false;\n    }\n\n    // minimum of: configured threads, number of 100-waledit batches,\n    //  and number of current sinks\n    int n = Math.min(Math.min(this.maxThreads, entries.size()/100+1), numSinks);\n\n    List<List<Entry>> entryLists = new ArrayList<List<Entry>>(n);\n    if (n == 1) {\n      entryLists.add(entries);\n    } else {\n      for (int i=0; i<n; i++) {\n        entryLists.add(new ArrayList<Entry>(entries.size()/n+1));\n      }\n      // now group by region\n      for (Entry e : entries) {\n        entryLists.get(Math.abs(Bytes.hashCode(e.getKey().getEncodedRegionName())%n)).add(e);\n      }\n    }\n    while (this.isRunning() && !exec.isShutdown()) {\n      if (!isPeerEnabled()) {\n        if (sleepForRetries(\"Replication is disabled\", sleepMultiplier)) {\n          sleepMultiplier++;\n        }\n        continue;\n      }\n      try {\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(\"Replicating \" + entries.size() +\n              \" entries of total size \" + replicateContext.getSize());\n        }\n\n        int futures = 0;\n        for (int i=0; i<entryLists.size(); i++) {\n          if (!entryLists.get(i).isEmpty()) {\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Submitting \" + entryLists.get(i).size() +\n                  \" entries of total size \" + replicateContext.getSize());\n            }\n            // RuntimeExceptions encountered here bubble up and are handled in ReplicationSource\n            pool.submit(createReplicator(entryLists.get(i), i));\n            futures++;\n          }\n        }\n        IOException iox = null;\n\n        for (int i=0; i<futures; i++) {\n          try {\n            // wait for all futures, remove successful parts\n            // (only the remaining parts will be retried)\n            Future<Integer> f = pool.take();\n            int index = f.get().intValue();\n            int batchSize =  entryLists.get(index).size();\n            entryLists.set(index, Collections.<Entry>emptyList());\n            // Now, we have marked the batch as done replicating, record its size\n            numReplicated += batchSize;\n          } catch (InterruptedException ie) {\n            iox =  new IOException(ie);\n          } catch (ExecutionException ee) {\n            // cause must be an IOException\n            iox = (IOException)ee.getCause();\n          }\n        }\n        if (iox != null) {\n          // if we had any exceptions, try again\n          throw iox;\n        }\n        if (numReplicated != entries.size()) {\n          // Something went wrong here and we don't know what, let's just fail and retry.\n          LOG.warn(\"The number of edits replicated is different from the number received,\"\n              + \" failing for now.\");\n          return false;\n        }\n        // update metrics\n        this.metrics.setAgeOfLastShippedOp(entries.get(entries.size() - 1).getKey().getWriteTime(),\n          walGroupId);\n        return true;\n\n      } catch (IOException ioe) {\n        // Didn't ship anything, but must still age the last time we did\n        this.metrics.refreshAgeOfLastShippedOp(walGroupId);\n        if (ioe instanceof RemoteException) {\n          ioe = ((RemoteException) ioe).unwrapRemoteException();\n          LOG.warn(\"Can't replicate because of an error on the remote cluster: \", ioe);\n          if (ioe instanceof TableNotFoundException) {\n            if (sleepForRetries(\"A table is missing in the peer cluster. \"\n                + \"Replication cannot proceed without losing data.\", sleepMultiplier)) {\n              sleepMultiplier++;\n            }\n          } else if (ioe instanceof SaslException) {\n            LOG.warn(\"Peer encountered SaslException, rechecking all sinks: \", ioe);\n            replicationSinkMgr.chooseSinks();\n          }\n        } else {\n          if (ioe instanceof SocketTimeoutException) {\n            // This exception means we waited for more than 60s and nothing\n            // happened, the cluster is alive and calling it right away\n            // even for a test just makes things worse.\n            sleepForRetries(\"Encountered a SocketTimeoutException. Since the \" +\n              \"call to the remote cluster timed out, which is usually \" +\n              \"caused by a machine failure or a massive slowdown\",\n              this.socketTimeoutMultiplier);\n          } else if (ioe instanceof ConnectException) {\n            LOG.warn(\"Peer is unavailable, rechecking all sinks: \", ioe);\n            replicationSinkMgr.chooseSinks();\n          } else {\n            LOG.warn(\"Can't replicate because of a local or network error: \", ioe);\n          }\n        }\n        if (sleepForRetries(\"Since we are unable to replicate\", sleepMultiplier)) {\n          sleepMultiplier++;\n        }\n      }\n    }\n    return false; // in case we exited before replicating\n  }"
        ]
    ],
    "ba02a8664b23c1d4942fad357b0ef02ec88ec8b5": [
        [
            "RegionStateStore::updateUserRegionLocation(RegionInfo,State,ServerName,ServerName,long,long)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 -\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "  private void updateUserRegionLocation(final RegionInfo regionInfo, final State state,\n      final ServerName regionLocation, final ServerName lastHost, final long openSeqNum,\n      final long pid)\n      throws IOException {\n    long time = EnvironmentEdgeManager.currentTime();\n    final int replicaId = regionInfo.getReplicaId();\n    final Put put = new Put(MetaTableAccessor.getMetaKeyForRegion(regionInfo), time);\n    MetaTableAccessor.addRegionInfo(put, regionInfo);\n    final StringBuilder info =\n      new StringBuilder(\"pid=\").append(pid).append(\" updating hbase:meta row=\")\n        .append(regionInfo.getRegionNameAsString()).append(\", regionState=\").append(state);\n    if (openSeqNum >= 0) {\n      Preconditions.checkArgument(state == State.OPEN && regionLocation != null,\n          \"Open region should be on a server\");\n      MetaTableAccessor.addLocation(put, regionLocation, openSeqNum, replicaId);\n      // only update replication barrier for default replica\n      if (regionInfo.getReplicaId() == RegionInfo.DEFAULT_REPLICA_ID &&\n        hasGlobalReplicationScope(regionInfo.getTable())) {\n        MetaTableAccessor.addReplicationBarrier(put, openSeqNum);\n      }\n      info.append(\", openSeqNum=\").append(openSeqNum);\n      info.append(\", regionLocation=\").append(regionLocation);\n    } else if (regionLocation != null && !regionLocation.equals(lastHost)) {\n      // Ideally, if no regionLocation, write null to the hbase:meta but this will confuse clients\n      // currently; they want a server to hit. TODO: Make clients wait if no location.\n      put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY)\n          .setRow(put.getRow())\n          .setFamily(HConstants.CATALOG_FAMILY)\n          .setQualifier(getServerNameColumn(replicaId))\n          .setTimestamp(put.getTimestamp())\n          .setType(Cell.Type.Put)\n          .setValue(Bytes.toBytes(regionLocation.getServerName()))\n          .build());\n      info.append(\", regionLocation=\").append(regionLocation);\n    }\n    put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY)\n        .setRow(put.getRow())\n        .setFamily(HConstants.CATALOG_FAMILY)\n        .setQualifier(getStateColumn(replicaId))\n        .setTimestamp(put.getTimestamp())\n        .setType(Cell.Type.Put)\n        .setValue(Bytes.toBytes(state.name()))\n        .build());\n    LOG.info(info.toString());\n    updateRegionLocation(regionInfo, state, put);\n  }",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  ",
            "  private void updateUserRegionLocation(final RegionInfo regionInfo, final State state,\n      final ServerName regionLocation, final ServerName lastHost, final long openSeqNum,\n      final long pid)\n      throws IOException {\n    long time = EnvironmentEdgeManager.currentTime();\n    final int replicaId = regionInfo.getReplicaId();\n    final Put put = new Put(MetaTableAccessor.getMetaKeyForRegion(regionInfo), time);\n    MetaTableAccessor.addRegionInfo(put, regionInfo);\n    final StringBuilder info =\n      new StringBuilder(\"pid=\").append(pid).append(\" updating hbase:meta row=\")\n        .append(regionInfo.getEncodedName()).append(\", regionState=\").append(state);\n    if (openSeqNum >= 0) {\n      Preconditions.checkArgument(state == State.OPEN && regionLocation != null,\n          \"Open region should be on a server\");\n      MetaTableAccessor.addLocation(put, regionLocation, openSeqNum, replicaId);\n      // only update replication barrier for default replica\n      if (regionInfo.getReplicaId() == RegionInfo.DEFAULT_REPLICA_ID &&\n        hasGlobalReplicationScope(regionInfo.getTable())) {\n        MetaTableAccessor.addReplicationBarrier(put, openSeqNum);\n      }\n      info.append(\", openSeqNum=\").append(openSeqNum);\n      info.append(\", regionLocation=\").append(regionLocation);\n    } else if (regionLocation != null && !regionLocation.equals(lastHost)) {\n      // Ideally, if no regionLocation, write null to the hbase:meta but this will confuse clients\n      // currently; they want a server to hit. TODO: Make clients wait if no location.\n      put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY)\n          .setRow(put.getRow())\n          .setFamily(HConstants.CATALOG_FAMILY)\n          .setQualifier(getServerNameColumn(replicaId))\n          .setTimestamp(put.getTimestamp())\n          .setType(Cell.Type.Put)\n          .setValue(Bytes.toBytes(regionLocation.getServerName()))\n          .build());\n      info.append(\", regionLocation=\").append(regionLocation);\n    }\n    put.add(CellBuilderFactory.create(CellBuilderType.SHALLOW_COPY)\n        .setRow(put.getRow())\n        .setFamily(HConstants.CATALOG_FAMILY)\n        .setQualifier(getStateColumn(replicaId))\n        .setTimestamp(put.getTimestamp())\n        .setType(Cell.Type.Put)\n        .setValue(Bytes.toBytes(state.name()))\n        .build());\n    LOG.info(info.toString());\n    updateRegionLocation(regionInfo, state, put);\n  }"
        ],
        [
            "RegionPlan::toString()",
            " 190  \n 191  \n 192 -\n 193  \n 194  \n 195  ",
            "  @Override\n  public String toString() {\n    return \"hri=\" + this.hri.getRegionNameAsString() + \", source=\" +\n      (this.source == null? \"\": this.source.toString()) +\n      \", destination=\" + (this.dest == null? \"\": this.dest.toString());\n  }",
            " 190  \n 191  \n 192 +\n 193  \n 194  \n 195  ",
            "  @Override\n  public String toString() {\n    return \"hri=\" + this.hri.getEncodedName() + \", source=\" +\n      (this.source == null? \"\": this.source.toString()) +\n      \", destination=\" + (this.dest == null? \"\": this.dest.toString());\n  }"
        ],
        [
            "RegionStateStore::visitMetaEntry(RegionStateVisitor,Result)",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 -\n 122 -\n 123 -\n 124  \n 125  \n 126  ",
            "  private void visitMetaEntry(final RegionStateVisitor visitor, final Result result)\n      throws IOException {\n    final RegionLocations rl = MetaTableAccessor.getRegionLocations(result);\n    if (rl == null) return;\n\n    final HRegionLocation[] locations = rl.getRegionLocations();\n    if (locations == null) return;\n\n    for (int i = 0; i < locations.length; ++i) {\n      final HRegionLocation hrl = locations[i];\n      if (hrl == null) continue;\n\n      final RegionInfo regionInfo = hrl.getRegion();\n      if (regionInfo == null) continue;\n\n      final int replicaId = regionInfo.getReplicaId();\n      final State state = getRegionState(result, replicaId);\n\n      final ServerName lastHost = hrl.getServerName();\n      final ServerName regionLocation = getRegionServer(result, replicaId);\n      final long openSeqNum = -1;\n\n      // TODO: move under trace, now is visible for debugging\n      LOG.info(String.format(\"Load hbase:meta entry region=%s regionState=%s lastHost=%s regionLocation=%s\",\n        regionInfo, state, lastHost, regionLocation));\n\n      visitor.visitRegionState(regionInfo, state, regionLocation, lastHost, openSeqNum);\n    }\n  }",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 +\n 122 +\n 123  \n 124  \n 125  ",
            "  private void visitMetaEntry(final RegionStateVisitor visitor, final Result result)\n      throws IOException {\n    final RegionLocations rl = MetaTableAccessor.getRegionLocations(result);\n    if (rl == null) return;\n\n    final HRegionLocation[] locations = rl.getRegionLocations();\n    if (locations == null) return;\n\n    for (int i = 0; i < locations.length; ++i) {\n      final HRegionLocation hrl = locations[i];\n      if (hrl == null) continue;\n\n      final RegionInfo regionInfo = hrl.getRegion();\n      if (regionInfo == null) continue;\n\n      final int replicaId = regionInfo.getReplicaId();\n      final State state = getRegionState(result, replicaId);\n\n      final ServerName lastHost = hrl.getServerName();\n      final ServerName regionLocation = getRegionServer(result, replicaId);\n      final long openSeqNum = -1;\n\n      // TODO: move under trace, now is visible for debugging\n      LOG.info(\"Load hbase:meta entry region={}, regionState={}, lastHost={}, \" +\n          \"regionLocation={}\", regionInfo.getEncodedName(), state, lastHost, regionLocation);\n      visitor.visitRegionState(regionInfo, state, regionLocation, lastHost, openSeqNum);\n    }\n  }"
        ],
        [
            "AssignmentManager::updateRegionTransition(ServerName,TransitionCode,RegionInfo,long)",
            " 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867 -\n 868  \n 869  ",
            "  private void updateRegionTransition(final ServerName serverName, final TransitionCode state,\n      final RegionInfo regionInfo, final long seqId)\n      throws PleaseHoldException, UnexpectedStateException {\n    checkFailoverCleanupCompleted(regionInfo);\n\n    final RegionStateNode regionNode = regionStates.getRegionStateNode(regionInfo);\n    if (regionNode == null) {\n      // the table/region is gone. maybe a delete, split, merge\n      throw new UnexpectedStateException(String.format(\n        \"Server %s was trying to transition region %s to %s. but the region was removed.\",\n        serverName, regionInfo, state));\n    }\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(String.format(\"Update region transition serverName=%s region=%s regionState=%s\",\n        serverName, regionNode, state));\n    }\n\n    final ServerStateNode serverNode = regionStates.getOrCreateServer(serverName);\n    if (!reportTransition(regionNode, serverNode, state, seqId)) {\n      // Don't log if shutting down cluster; during shutdown.\n      LOG.warn(\"No matchin procedure found for {} to transition to {}\", regionNode, state);\n    }\n  }",
            " 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867 +\n 868  \n 869  ",
            "  private void updateRegionTransition(final ServerName serverName, final TransitionCode state,\n      final RegionInfo regionInfo, final long seqId)\n      throws PleaseHoldException, UnexpectedStateException {\n    checkFailoverCleanupCompleted(regionInfo);\n\n    final RegionStateNode regionNode = regionStates.getRegionStateNode(regionInfo);\n    if (regionNode == null) {\n      // the table/region is gone. maybe a delete, split, merge\n      throw new UnexpectedStateException(String.format(\n        \"Server %s was trying to transition region %s to %s. but the region was removed.\",\n        serverName, regionInfo, state));\n    }\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(String.format(\"Update region transition serverName=%s region=%s regionState=%s\",\n        serverName, regionNode, state));\n    }\n\n    final ServerStateNode serverNode = regionStates.getOrCreateServer(serverName);\n    if (!reportTransition(regionNode, serverNode, state, seqId)) {\n      // Don't log if shutting down cluster; during shutdown.\n      LOG.warn(\"No matching procedure found for {} transition to {}\", regionNode, state);\n    }\n  }"
        ]
    ],
    "1b98a96caa09ee9be27d6bf028200fe6790ac726": [
        [
            "HMaster::recommissionRegionServer(ServerName,List)",
            "3543  \n3544  \n3545  \n3546  \n3547  \n3548  \n3549  \n3550  \n3551  \n3552 -\n3553  \n3554  \n3555  \n3556  \n3557  \n3558  \n3559  \n3560  \n3561  \n3562  \n3563  \n3564  \n3565  \n3566  \n3567  \n3568  \n3569  \n3570  \n3571  \n3572  \n3573  \n3574  \n3575  \n3576  \n3577  \n3578  \n3579  \n3580  \n3581  \n3582  \n3583  \n3584  \n3585  ",
            "  /**\n   * Remove decommission marker (previously called 'draining') from a region server to allow regions\n   * assignments. Load regions onto the server asynchronously if a list of regions is given\n   * @param server Region server to remove decommission marker from.\n   * @throws HBaseIOException\n   */\n  public void recommissionRegionServer(final ServerName server,\n      final List<byte[]> encodedRegionNames) throws HBaseIOException {\n    // Remove the server from decommissioned (draining) server list.\n    String parentZnode = getZooKeeper().znodePaths.drainingZNode;\n    String node = ZNodePaths.joinZNode(parentZnode, server.getServerName());\n    try {\n      ZKUtil.deleteNodeFailSilent(getZooKeeper(), node);\n    } catch (KeeperException ke) {\n      throw new HBaseIOException(\n          this.zooKeeper.prefix(\"Unable to recommission '\" + server.getServerName() + \"'.\"), ke);\n    }\n    this.serverManager.removeServerFromDrainList(server);\n\n    // Load the regions onto the server if we are given a list of regions.\n    if (encodedRegionNames == null || encodedRegionNames.isEmpty()) {\n      return;\n    }\n    if (!this.serverManager.isServerOnline(server)) {\n      return;\n    }\n    for (byte[] encodedRegionName : encodedRegionNames) {\n      RegionState regionState =\n          assignmentManager.getRegionStates().getRegionState(Bytes.toString(encodedRegionName));\n      if (regionState == null) {\n        LOG.warn(\"Unknown region \" + Bytes.toStringBinary(encodedRegionName));\n        continue;\n      }\n      RegionInfo hri = regionState.getRegion();\n      if (server.equals(regionState.getServerName())) {\n        LOG.info(\"Skipping move of region \" + hri.getRegionNameAsString()\n          + \" because region already assigned to the same server \" + server + \".\");\n        continue;\n      }\n      RegionPlan rp = new RegionPlan(hri, regionState.getServerName(), server);\n      this.assignmentManager.moveAsync(rp);\n    }\n  }",
            "3543  \n3544  \n3545  \n3546  \n3547  \n3548  \n3549  \n3550  \n3551  \n3552 +\n3553  \n3554  \n3555  \n3556  \n3557  \n3558  \n3559  \n3560  \n3561  \n3562  \n3563  \n3564  \n3565  \n3566  \n3567  \n3568  \n3569  \n3570  \n3571  \n3572  \n3573  \n3574  \n3575  \n3576  \n3577  \n3578  \n3579  \n3580  \n3581  \n3582  \n3583  \n3584  \n3585  ",
            "  /**\n   * Remove decommission marker (previously called 'draining') from a region server to allow regions\n   * assignments. Load regions onto the server asynchronously if a list of regions is given\n   * @param server Region server to remove decommission marker from.\n   * @throws HBaseIOException\n   */\n  public void recommissionRegionServer(final ServerName server,\n      final List<byte[]> encodedRegionNames) throws HBaseIOException {\n    // Remove the server from decommissioned (draining) server list.\n    String parentZnode = getZooKeeper().getZNodePaths().drainingZNode;\n    String node = ZNodePaths.joinZNode(parentZnode, server.getServerName());\n    try {\n      ZKUtil.deleteNodeFailSilent(getZooKeeper(), node);\n    } catch (KeeperException ke) {\n      throw new HBaseIOException(\n          this.zooKeeper.prefix(\"Unable to recommission '\" + server.getServerName() + \"'.\"), ke);\n    }\n    this.serverManager.removeServerFromDrainList(server);\n\n    // Load the regions onto the server if we are given a list of regions.\n    if (encodedRegionNames == null || encodedRegionNames.isEmpty()) {\n      return;\n    }\n    if (!this.serverManager.isServerOnline(server)) {\n      return;\n    }\n    for (byte[] encodedRegionName : encodedRegionNames) {\n      RegionState regionState =\n          assignmentManager.getRegionStates().getRegionState(Bytes.toString(encodedRegionName));\n      if (regionState == null) {\n        LOG.warn(\"Unknown region \" + Bytes.toStringBinary(encodedRegionName));\n        continue;\n      }\n      RegionInfo hri = regionState.getRegion();\n      if (server.equals(regionState.getServerName())) {\n        LOG.info(\"Skipping move of region \" + hri.getRegionNameAsString()\n          + \" because region already assigned to the same server \" + server + \".\");\n        continue;\n      }\n      RegionPlan rp = new RegionPlan(hri, regionState.getServerName(), server);\n      this.assignmentManager.moveAsync(rp);\n    }\n  }"
        ],
        [
            "MetaLocationSyncer::validate(String)",
            "  37  \n  38  \n  39 -\n  40  ",
            "  @Override\n  boolean validate(String path) {\n    return watcher.znodePaths.isAnyMetaReplicaZNode(path);\n  }",
            "  37  \n  38  \n  39 +\n  40  ",
            "  @Override\n  boolean validate(String path) {\n    return watcher.getZNodePaths().isAnyMetaReplicaZNode(path);\n  }"
        ],
        [
            "ZKDataMigrator::queryForTableStates(ZKWatcher)",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  /**\n   * Method for table states migration.\n   * Used when upgrading from pre-2.0 to 2.0\n   * Reading state from zk, applying them to internal state\n   * and delete.\n   * Used by master to clean migration from zk based states to\n   * table descriptor based states.\n   * @deprecated Since 2.0.0. To be removed in hbase-3.0.0.\n   */\n  @Deprecated\n  public static Map<TableName, TableState.State> queryForTableStates(ZKWatcher zkw)\n      throws KeeperException, InterruptedException {\n    Map<TableName, TableState.State> rv = new HashMap<>();\n    List<String> children = ZKUtil.listChildrenNoWatch(zkw, zkw.znodePaths.tableZNode);\n    if (children == null)\n      return rv;\n    for (String child: children) {\n      TableName tableName = TableName.valueOf(child);\n      ZooKeeperProtos.DeprecatedTableState.State state = getTableState(zkw, tableName);\n      TableState.State newState = TableState.State.ENABLED;\n      if (state != null) {\n        switch (state) {\n        case ENABLED:\n          newState = TableState.State.ENABLED;\n          break;\n        case DISABLED:\n          newState = TableState.State.DISABLED;\n          break;\n        case DISABLING:\n          newState = TableState.State.DISABLING;\n          break;\n        case ENABLING:\n          newState = TableState.State.ENABLING;\n          break;\n        default:\n        }\n      }\n      rv.put(tableName, newState);\n    }\n    return rv;\n  }",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  /**\n   * Method for table states migration.\n   * Used when upgrading from pre-2.0 to 2.0\n   * Reading state from zk, applying them to internal state\n   * and delete.\n   * Used by master to clean migration from zk based states to\n   * table descriptor based states.\n   * @deprecated Since 2.0.0. To be removed in hbase-3.0.0.\n   */\n  @Deprecated\n  public static Map<TableName, TableState.State> queryForTableStates(ZKWatcher zkw)\n      throws KeeperException, InterruptedException {\n    Map<TableName, TableState.State> rv = new HashMap<>();\n    List<String> children = ZKUtil.listChildrenNoWatch(zkw, zkw.getZNodePaths().tableZNode);\n    if (children == null)\n      return rv;\n    for (String child: children) {\n      TableName tableName = TableName.valueOf(child);\n      ZooKeeperProtos.DeprecatedTableState.State state = getTableState(zkw, tableName);\n      TableState.State newState = TableState.State.ENABLED;\n      if (state != null) {\n        switch (state) {\n        case ENABLED:\n          newState = TableState.State.ENABLED;\n          break;\n        case DISABLED:\n          newState = TableState.State.DISABLED;\n          break;\n        case DISABLING:\n          newState = TableState.State.DISABLING;\n          break;\n        case ENABLING:\n          newState = TableState.State.ENABLING;\n          break;\n        default:\n        }\n      }\n      rv.put(tableName, newState);\n    }\n    return rv;\n  }"
        ],
        [
            "ActiveMasterManager::stop()",
            " 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274 -\n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  ",
            "  public void stop() {\n    try {\n      synchronized (clusterHasActiveMaster) {\n        // Master is already stopped, wake up the manager\n        // thread so that it can shutdown soon.\n        clusterHasActiveMaster.notifyAll();\n      }\n      // If our address is in ZK, delete it on our way out\n      ServerName activeMaster = null;\n      try {\n        activeMaster = MasterAddressTracker.getMasterAddress(this.watcher);\n      } catch (IOException e) {\n        LOG.warn(\"Failed get of master address: \" + e.toString());\n      }\n      if (activeMaster != null &&  activeMaster.equals(this.sn)) {\n        ZKUtil.deleteNode(watcher, watcher.znodePaths.masterAddressZNode);\n        // We may have failed to delete the znode at the previous step, but\n        //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n        ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n      }\n    } catch (KeeperException e) {\n      LOG.debug(this.watcher.prefix(\"Failed delete of our master address node; \" +\n          e.getMessage()));\n    }\n  }",
            " 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274 +\n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  ",
            "  public void stop() {\n    try {\n      synchronized (clusterHasActiveMaster) {\n        // Master is already stopped, wake up the manager\n        // thread so that it can shutdown soon.\n        clusterHasActiveMaster.notifyAll();\n      }\n      // If our address is in ZK, delete it on our way out\n      ServerName activeMaster = null;\n      try {\n        activeMaster = MasterAddressTracker.getMasterAddress(this.watcher);\n      } catch (IOException e) {\n        LOG.warn(\"Failed get of master address: \" + e.toString());\n      }\n      if (activeMaster != null &&  activeMaster.equals(this.sn)) {\n        ZKUtil.deleteNode(watcher, watcher.getZNodePaths().masterAddressZNode);\n        // We may have failed to delete the znode at the previous step, but\n        //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n        ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n      }\n    } catch (KeeperException e) {\n      LOG.debug(this.watcher.prefix(\"Failed delete of our master address node; \" +\n          e.getMessage()));\n    }\n  }"
        ],
        [
            "ZKSplitLogManagerCoordination::remainingTasksInCoordination()",
            " 121  \n 122  \n 123  \n 124  \n 125 -\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "  @Override\n  public int remainingTasksInCoordination() {\n    int count = 0;\n    try {\n      List<String> tasks = ZKUtil.listChildrenNoWatch(watcher, watcher.znodePaths.splitLogZNode);\n      if (tasks != null) {\n        int listSize = tasks.size();\n        for (int i = 0; i < listSize; i++) {\n          if (!ZKSplitLog.isRescanNode(tasks.get(i))) {\n            count++;\n          }\n        }\n      }\n    } catch (KeeperException ke) {\n      LOG.warn(\"Failed to check remaining tasks\", ke);\n      count = -1;\n    }\n    return count;\n  }",
            " 121  \n 122  \n 123  \n 124  \n 125 +\n 126 +\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  ",
            "  @Override\n  public int remainingTasksInCoordination() {\n    int count = 0;\n    try {\n      List<String> tasks = ZKUtil.listChildrenNoWatch(watcher,\n              watcher.getZNodePaths().splitLogZNode);\n      if (tasks != null) {\n        int listSize = tasks.size();\n        for (int i = 0; i < listSize; i++) {\n          if (!ZKSplitLog.isRescanNode(tasks.get(i))) {\n            count++;\n          }\n        }\n      }\n    } catch (KeeperException ke) {\n      LOG.warn(\"Failed to check remaining tasks\", ke);\n      count = -1;\n    }\n    return count;\n  }"
        ],
        [
            "ClusterStatusTracker::ClusterStatusTracker(ZKWatcher,Abortable)",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51 -\n  52  ",
            "  /**\n   * Creates a cluster status tracker.\n   *\n   * <p>After construction, use {@link #start} to kick off tracking.\n   *\n   * @param watcher reference to the {@link ZKWatcher} which also contains configuration and\n   *                constants\n   * @param abortable used to abort if a fatal error occurs\n   */\n  public ClusterStatusTracker(ZKWatcher watcher, Abortable abortable) {\n    super(watcher, watcher.znodePaths.clusterStateZNode, abortable);\n  }",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51 +\n  52  ",
            "  /**\n   * Creates a cluster status tracker.\n   *\n   * <p>After construction, use {@link #start} to kick off tracking.\n   *\n   * @param watcher reference to the {@link ZKWatcher} which also contains configuration and\n   *                constants\n   * @param abortable used to abort if a fatal error occurs\n   */\n  public ClusterStatusTracker(ZKWatcher watcher, Abortable abortable) {\n    super(watcher, watcher.getZNodePaths().clusterStateZNode, abortable);\n  }"
        ],
        [
            "RegionNormalizerTracker::RegionNormalizerTracker(ZKWatcher,Abortable)",
            "  40  \n  41 -\n  42  ",
            "  public RegionNormalizerTracker(ZKWatcher watcher, Abortable abortable) {\n    super(watcher, watcher.znodePaths.regionNormalizerZNode, abortable);\n  }",
            "  40  \n  41 +\n  42  ",
            "  public RegionNormalizerTracker(ZKWatcher watcher, Abortable abortable) {\n    super(watcher, watcher.getZNodePaths().regionNormalizerZNode, abortable);\n  }"
        ],
        [
            "ZKUtil::getNumberOfChildren(ZKWatcher,String)",
            " 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585 -\n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  ",
            "  /**\n   * Get the number of children of the specified node.\n   *\n   * If the node does not exist or has no children, returns 0.\n   *\n   * Sets no watches at all.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to count children of\n   * @return number of children of specified node, 0 if none or parent does not\n   *         exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static int getNumberOfChildren(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    try {\n      Stat stat = zkw.getRecoverableZooKeeper().exists(znode, null);\n      return stat == null ? 0 : stat.getNumChildren();\n    } catch(KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to get children of node \" + znode));\n      zkw.keeperException(e);\n    } catch(InterruptedException e) {\n      zkw.interruptedException(e);\n    }\n    return 0;\n  }",
            " 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585 +\n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  ",
            "  /**\n   * Get the number of children of the specified node.\n   *\n   * If the node does not exist or has no children, returns 0.\n   *\n   * Sets no watches at all.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to count children of\n   * @return number of children of specified node, 0 if none or parent does not\n   *         exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static int getNumberOfChildren(ZKWatcher zkw, String znode)\n    throws KeeperException {\n    try {\n      Stat stat = zkw.getRecoverableZooKeeper().exists(znode, null);\n      return stat == null ? 0 : stat.getNumChildren();\n    } catch(KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to get children of node \" + znode));\n      zkw.keeperException(e);\n    } catch(InterruptedException e) {\n      zkw.interruptedException(e);\n    }\n    return 0;\n  }"
        ],
        [
            "TestZKNodeTracker::testNodeTracker()",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "  @Test\n  public void testNodeTracker() throws Exception {\n    Abortable abortable = new StubAbortable();\n    ZKWatcher zk = new ZKWatcher(TEST_UTIL.getConfiguration(),\n        \"testNodeTracker\", abortable);\n    ZKUtil.createAndFailSilent(zk, zk.znodePaths.baseZNode);\n\n    final String node = ZNodePaths.joinZNode(zk.znodePaths.baseZNode,\n      Long.toString(ThreadLocalRandom.current().nextLong()));\n\n    final byte [] dataOne = Bytes.toBytes(\"dataOne\");\n    final byte [] dataTwo = Bytes.toBytes(\"dataTwo\");\n\n    // Start a ZKNT with no node currently available\n    TestTracker localTracker = new TestTracker(zk, node, abortable);\n    localTracker.start();\n    zk.registerListener(localTracker);\n\n    // Make sure we don't have a node\n    assertNull(localTracker.getData(false));\n\n    // Spin up a thread with another ZKNT and have it block\n    WaitToGetDataThread thread = new WaitToGetDataThread(zk, node);\n    thread.start();\n\n    // Verify the thread doesn't have a node\n    assertFalse(thread.hasData);\n\n    // Now, start a new ZKNT with the node already available\n    TestTracker secondTracker = new TestTracker(zk, node, null);\n    secondTracker.start();\n    zk.registerListener(secondTracker);\n\n    // Put up an additional zk listener so we know when zk event is done\n    TestingZKListener zkListener = new TestingZKListener(zk, node);\n    zk.registerListener(zkListener);\n    assertEquals(0, zkListener.createdLock.availablePermits());\n\n    // Create a completely separate zk connection for test triggers and avoid\n    // any weird watcher interactions from the test\n    final ZooKeeper zkconn = ZooKeeperHelper.\n        getConnectedZooKeeper(ZKConfig.getZKQuorumServersString(TEST_UTIL.getConfiguration()),\n            60000);\n\n    // Add the node with data one\n    zkconn.create(node, dataOne, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\n    // Wait for the zk event to be processed\n    zkListener.waitForCreation();\n    thread.join();\n\n    // Both trackers should have the node available with data one\n    assertNotNull(localTracker.getData(false));\n    assertNotNull(localTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(localTracker.getData(false), dataOne));\n    assertTrue(thread.hasData);\n    assertTrue(Bytes.equals(thread.tracker.getData(false), dataOne));\n    LOG.info(\"Successfully got data one\");\n\n    // Make sure it's available and with the expected data\n    assertNotNull(secondTracker.getData(false));\n    assertNotNull(secondTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(secondTracker.getData(false), dataOne));\n    LOG.info(\"Successfully got data one with the second tracker\");\n\n    // Drop the node\n    zkconn.delete(node, -1);\n    zkListener.waitForDeletion();\n\n    // Create a new thread but with the existing thread's tracker to wait\n    TestTracker threadTracker = thread.tracker;\n    thread = new WaitToGetDataThread(zk, node, threadTracker);\n    thread.start();\n\n    // Verify other guys don't have data\n    assertFalse(thread.hasData);\n    assertNull(secondTracker.getData(false));\n    assertNull(localTracker.getData(false));\n    LOG.info(\"Successfully made unavailable\");\n\n    // Create with second data\n    zkconn.create(node, dataTwo, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\n    // Wait for the zk event to be processed\n    zkListener.waitForCreation();\n    thread.join();\n\n    // All trackers should have the node available with data two\n    assertNotNull(localTracker.getData(false));\n    assertNotNull(localTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(localTracker.getData(false), dataTwo));\n    assertNotNull(secondTracker.getData(false));\n    assertNotNull(secondTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(secondTracker.getData(false), dataTwo));\n    assertTrue(thread.hasData);\n    assertTrue(Bytes.equals(thread.tracker.getData(false), dataTwo));\n    LOG.info(\"Successfully got data two on all trackers and threads\");\n\n    // Change the data back to data one\n    zkconn.setData(node, dataOne, -1);\n\n    // Wait for zk event to be processed\n    zkListener.waitForDataChange();\n\n    // All trackers should have the node available with data one\n    assertNotNull(localTracker.getData(false));\n    assertNotNull(localTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(localTracker.getData(false), dataOne));\n    assertNotNull(secondTracker.getData(false));\n    assertNotNull(secondTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(secondTracker.getData(false), dataOne));\n    assertTrue(thread.hasData);\n    assertTrue(Bytes.equals(thread.tracker.getData(false), dataOne));\n    LOG.info(\"Successfully got data one following a data change on all trackers and threads\");\n  }",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104 +\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "  @Test\n  public void testNodeTracker() throws Exception {\n    Abortable abortable = new StubAbortable();\n    ZKWatcher zk = new ZKWatcher(TEST_UTIL.getConfiguration(),\n        \"testNodeTracker\", abortable);\n    ZKUtil.createAndFailSilent(zk, zk.getZNodePaths().baseZNode);\n\n    final String node = ZNodePaths.joinZNode(zk.getZNodePaths().baseZNode,\n      Long.toString(ThreadLocalRandom.current().nextLong()));\n\n    final byte [] dataOne = Bytes.toBytes(\"dataOne\");\n    final byte [] dataTwo = Bytes.toBytes(\"dataTwo\");\n\n    // Start a ZKNT with no node currently available\n    TestTracker localTracker = new TestTracker(zk, node, abortable);\n    localTracker.start();\n    zk.registerListener(localTracker);\n\n    // Make sure we don't have a node\n    assertNull(localTracker.getData(false));\n\n    // Spin up a thread with another ZKNT and have it block\n    WaitToGetDataThread thread = new WaitToGetDataThread(zk, node);\n    thread.start();\n\n    // Verify the thread doesn't have a node\n    assertFalse(thread.hasData);\n\n    // Now, start a new ZKNT with the node already available\n    TestTracker secondTracker = new TestTracker(zk, node, null);\n    secondTracker.start();\n    zk.registerListener(secondTracker);\n\n    // Put up an additional zk listener so we know when zk event is done\n    TestingZKListener zkListener = new TestingZKListener(zk, node);\n    zk.registerListener(zkListener);\n    assertEquals(0, zkListener.createdLock.availablePermits());\n\n    // Create a completely separate zk connection for test triggers and avoid\n    // any weird watcher interactions from the test\n    final ZooKeeper zkconn = ZooKeeperHelper.\n        getConnectedZooKeeper(ZKConfig.getZKQuorumServersString(TEST_UTIL.getConfiguration()),\n            60000);\n\n    // Add the node with data one\n    zkconn.create(node, dataOne, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\n    // Wait for the zk event to be processed\n    zkListener.waitForCreation();\n    thread.join();\n\n    // Both trackers should have the node available with data one\n    assertNotNull(localTracker.getData(false));\n    assertNotNull(localTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(localTracker.getData(false), dataOne));\n    assertTrue(thread.hasData);\n    assertTrue(Bytes.equals(thread.tracker.getData(false), dataOne));\n    LOG.info(\"Successfully got data one\");\n\n    // Make sure it's available and with the expected data\n    assertNotNull(secondTracker.getData(false));\n    assertNotNull(secondTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(secondTracker.getData(false), dataOne));\n    LOG.info(\"Successfully got data one with the second tracker\");\n\n    // Drop the node\n    zkconn.delete(node, -1);\n    zkListener.waitForDeletion();\n\n    // Create a new thread but with the existing thread's tracker to wait\n    TestTracker threadTracker = thread.tracker;\n    thread = new WaitToGetDataThread(zk, node, threadTracker);\n    thread.start();\n\n    // Verify other guys don't have data\n    assertFalse(thread.hasData);\n    assertNull(secondTracker.getData(false));\n    assertNull(localTracker.getData(false));\n    LOG.info(\"Successfully made unavailable\");\n\n    // Create with second data\n    zkconn.create(node, dataTwo, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n\n    // Wait for the zk event to be processed\n    zkListener.waitForCreation();\n    thread.join();\n\n    // All trackers should have the node available with data two\n    assertNotNull(localTracker.getData(false));\n    assertNotNull(localTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(localTracker.getData(false), dataTwo));\n    assertNotNull(secondTracker.getData(false));\n    assertNotNull(secondTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(secondTracker.getData(false), dataTwo));\n    assertTrue(thread.hasData);\n    assertTrue(Bytes.equals(thread.tracker.getData(false), dataTwo));\n    LOG.info(\"Successfully got data two on all trackers and threads\");\n\n    // Change the data back to data one\n    zkconn.setData(node, dataOne, -1);\n\n    // Wait for zk event to be processed\n    zkListener.waitForDataChange();\n\n    // All trackers should have the node available with data one\n    assertNotNull(localTracker.getData(false));\n    assertNotNull(localTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(localTracker.getData(false), dataOne));\n    assertNotNull(secondTracker.getData(false));\n    assertNotNull(secondTracker.blockUntilAvailable());\n    assertTrue(Bytes.equals(secondTracker.getData(false), dataOne));\n    assertTrue(thread.hasData);\n    assertTrue(Bytes.equals(thread.tracker.getData(false), dataOne));\n    LOG.info(\"Successfully got data one following a data change on all trackers and threads\");\n  }"
        ],
        [
            "ZKUtil::getServerStats(String,int)",
            "1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947 -\n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  ",
            "  /**\n   * Gets the statistics from the given server.\n   *\n   * @param server  The server to get the statistics from.\n   * @param timeout  The socket timeout to use.\n   * @return The array of response strings.\n   * @throws IOException When the socket communication fails.\n   */\n  public static String[] getServerStats(String server, int timeout)\n  throws IOException {\n    String[] sp = server.split(\":\");\n    if (sp == null || sp.length == 0) {\n      return null;\n    }\n\n    String host = sp[0];\n    int port = sp.length > 1 ? Integer.parseInt(sp[1])\n        : HConstants.DEFAULT_ZOOKEEPER_CLIENT_PORT;\n\n    InetSocketAddress sockAddr = new InetSocketAddress(host, port);\n    try (Socket socket = new Socket()) {\n      socket.connect(sockAddr, timeout);\n\n      socket.setSoTimeout(timeout);\n      try (PrintWriter out = new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8)), true);\n          BufferedReader in = new BufferedReader(\n              new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8))) {\n        out.println(\"stat\");\n        out.flush();\n        ArrayList<String> res = new ArrayList<>();\n        while (true) {\n          String line = in.readLine();\n          if (line != null) {\n            res.add(line);\n          } else {\n            break;\n          }\n        }\n        return res.toArray(new String[res.size()]);\n      }\n    }\n  }",
            "1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948 +\n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  ",
            "  /**\n   * Gets the statistics from the given server.\n   *\n   * @param server  The server to get the statistics from.\n   * @param timeout  The socket timeout to use.\n   * @return The array of response strings.\n   * @throws IOException When the socket communication fails.\n   */\n  public static String[] getServerStats(String server, int timeout)\n    throws IOException {\n    String[] sp = server.split(\":\");\n    if (sp == null || sp.length == 0) {\n      return null;\n    }\n\n    String host = sp[0];\n    int port = sp.length > 1 ? Integer.parseInt(sp[1])\n        : HConstants.DEFAULT_ZOOKEEPER_CLIENT_PORT;\n\n    InetSocketAddress sockAddr = new InetSocketAddress(host, port);\n    try (Socket socket = new Socket()) {\n      socket.connect(sockAddr, timeout);\n\n      socket.setSoTimeout(timeout);\n      try (PrintWriter out = new PrintWriter(new BufferedWriter(\n          new OutputStreamWriter(socket.getOutputStream(), StandardCharsets.UTF_8)), true);\n          BufferedReader in = new BufferedReader(\n              new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8))) {\n        out.println(\"stat\");\n        out.flush();\n        ArrayList<String> res = new ArrayList<>();\n        while (true) {\n          String line = in.readLine();\n          if (line != null) {\n            res.add(line);\n          } else {\n            break;\n          }\n        }\n        return res.toArray(new String[res.size()]);\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::dump(ZKWatcher)",
            "1734  \n1735  \n1736  \n1737  \n1738 -\n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746 -\n1747  \n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759 -\n1760  \n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  ",
            "  /** @return String dump of everything in ZooKeeper. */\n  public static String dump(ZKWatcher zkw) {\n    StringBuilder sb = new StringBuilder();\n    try {\n      sb.append(\"HBase is rooted at \").append(zkw.znodePaths.baseZNode);\n      sb.append(\"\\nActive master address: \");\n      try {\n        sb.append(MasterAddressTracker.getMasterAddress(zkw));\n      } catch (IOException e) {\n        sb.append(\"<<FAILED LOOKUP: \" + e.getMessage() + \">>\");\n      }\n      sb.append(\"\\nBackup master addresses:\");\n      for (String child : listChildrenNoWatch(zkw, zkw.znodePaths.backupMasterAddressesZNode)) {\n        sb.append(\"\\n \").append(child);\n      }\n      sb.append(\"\\nRegion server holding hbase:meta: \"\n        + new MetaTableLocator().getMetaRegionLocation(zkw));\n      Configuration conf = HBaseConfiguration.create();\n      int numMetaReplicas = conf.getInt(HConstants.META_REPLICAS_NUM,\n               HConstants.DEFAULT_META_REPLICA_NUM);\n      for (int i = 1; i < numMetaReplicas; i++) {\n        sb.append(\"\\nRegion server holding hbase:meta, replicaId \" + i + \" \"\n                    + new MetaTableLocator().getMetaRegionLocation(zkw, i));\n      }\n      sb.append(\"\\nRegion servers:\");\n      for (String child : listChildrenNoWatch(zkw, zkw.znodePaths.rsZNode)) {\n        sb.append(\"\\n \").append(child);\n      }\n      try {\n        getReplicationZnodesDump(zkw, sb);\n      } catch (KeeperException ke) {\n        LOG.warn(\"Couldn't get the replication znode dump\", ke);\n      }\n      sb.append(\"\\nQuorum Server Statistics:\");\n      String[] servers = zkw.getQuorum().split(\",\");\n      for (String server : servers) {\n        sb.append(\"\\n \").append(server);\n        try {\n          String[] stat = getServerStats(server, ZKUtil.zkDumpConnectionTimeOut);\n\n          if (stat == null) {\n            sb.append(\"[Error] invalid quorum server: \" + server);\n            break;\n          }\n\n          for (String s : stat) {\n            sb.append(\"\\n  \").append(s);\n          }\n        } catch (Exception e) {\n          sb.append(\"\\n  ERROR: \").append(e.getMessage());\n        }\n      }\n    } catch (KeeperException ke) {\n      sb.append(\"\\nFATAL ZooKeeper Exception!\\n\");\n      sb.append(\"\\n\" + ke.getMessage());\n    }\n    return sb.toString();\n  }",
            "1734  \n1735  \n1736  \n1737  \n1738 +\n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746 +\n1747 +\n1748  \n1749  \n1750  \n1751  \n1752  \n1753  \n1754  \n1755  \n1756  \n1757  \n1758  \n1759  \n1760 +\n1761  \n1762  \n1763  \n1764  \n1765  \n1766  \n1767  \n1768  \n1769  \n1770  \n1771  \n1772  \n1773  \n1774  \n1775  \n1776  \n1777  \n1778  \n1779  \n1780  \n1781  \n1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  ",
            "  /** @return String dump of everything in ZooKeeper. */\n  public static String dump(ZKWatcher zkw) {\n    StringBuilder sb = new StringBuilder();\n    try {\n      sb.append(\"HBase is rooted at \").append(zkw.getZNodePaths().baseZNode);\n      sb.append(\"\\nActive master address: \");\n      try {\n        sb.append(MasterAddressTracker.getMasterAddress(zkw));\n      } catch (IOException e) {\n        sb.append(\"<<FAILED LOOKUP: \" + e.getMessage() + \">>\");\n      }\n      sb.append(\"\\nBackup master addresses:\");\n      for (String child : listChildrenNoWatch(zkw,\n              zkw.getZNodePaths().backupMasterAddressesZNode)) {\n        sb.append(\"\\n \").append(child);\n      }\n      sb.append(\"\\nRegion server holding hbase:meta: \"\n        + new MetaTableLocator().getMetaRegionLocation(zkw));\n      Configuration conf = HBaseConfiguration.create();\n      int numMetaReplicas = conf.getInt(HConstants.META_REPLICAS_NUM,\n               HConstants.DEFAULT_META_REPLICA_NUM);\n      for (int i = 1; i < numMetaReplicas; i++) {\n        sb.append(\"\\nRegion server holding hbase:meta, replicaId \" + i + \" \"\n                    + new MetaTableLocator().getMetaRegionLocation(zkw, i));\n      }\n      sb.append(\"\\nRegion servers:\");\n      for (String child : listChildrenNoWatch(zkw, zkw.getZNodePaths().rsZNode)) {\n        sb.append(\"\\n \").append(child);\n      }\n      try {\n        getReplicationZnodesDump(zkw, sb);\n      } catch (KeeperException ke) {\n        LOG.warn(\"Couldn't get the replication znode dump\", ke);\n      }\n      sb.append(\"\\nQuorum Server Statistics:\");\n      String[] servers = zkw.getQuorum().split(\",\");\n      for (String server : servers) {\n        sb.append(\"\\n \").append(server);\n        try {\n          String[] stat = getServerStats(server, ZKUtil.zkDumpConnectionTimeOut);\n\n          if (stat == null) {\n            sb.append(\"[Error] invalid quorum server: \" + server);\n            break;\n          }\n\n          for (String s : stat) {\n            sb.append(\"\\n  \").append(s);\n          }\n        } catch (Exception e) {\n          sb.append(\"\\n  ERROR: \").append(e.getMessage());\n        }\n      }\n    } catch (KeeperException ke) {\n      sb.append(\"\\nFATAL ZooKeeper Exception!\\n\");\n      sb.append(\"\\n\" + ke.getMessage());\n    }\n    return sb.toString();\n  }"
        ],
        [
            "ActiveMasterManager::nodeDeleted(String)",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  @Override\n  public void nodeDeleted(String path) {\n\n    // We need to keep track of the cluster's shutdown status while\n    // we wait on the current master. We consider that, if the cluster\n    // was already in a \"shutdown\" state when we started, that this master\n    // is part of a new cluster that was started shortly after the old cluster\n    // shut down, so that state is now irrelevant. This means that the shutdown\n    // state must be set while we wait on the active master in order\n    // to shutdown this master. See HBASE-8519.\n    if(path.equals(watcher.znodePaths.clusterStateZNode) && !master.isStopped()) {\n      clusterShutDown.set(true);\n    }\n\n    handle(path);\n  }",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  \n  99  \n 100  \n 101  ",
            "  @Override\n  public void nodeDeleted(String path) {\n\n    // We need to keep track of the cluster's shutdown status while\n    // we wait on the current master. We consider that, if the cluster\n    // was already in a \"shutdown\" state when we started, that this master\n    // is part of a new cluster that was started shortly after the old cluster\n    // shut down, so that state is now irrelevant. This means that the shutdown\n    // state must be set while we wait on the active master in order\n    // to shutdown this master. See HBASE-8519.\n    if(path.equals(watcher.getZNodePaths().clusterStateZNode) && !master.isStopped()) {\n      clusterShutDown.set(true);\n    }\n\n    handle(path);\n  }"
        ],
        [
            "ZKUtil::watchAndCheckExists(ZKWatcher,String)",
            " 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350 -\n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  ",
            "  /**\n   * Watch the specified znode for delete/create/change events.  The watcher is\n   * set whether or not the node exists.  If the node already exists, the method\n   * returns true.  If the node does not exist, the method returns false.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to watch\n   * @return true if znode exists, false if does not exist or error\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean watchAndCheckExists(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    try {\n      Stat s = zkw.getRecoverableZooKeeper().exists(znode, zkw);\n      boolean exists = s != null ? true : false;\n      if (exists) {\n        LOG.debug(zkw.prefix(\"Set watcher on existing znode=\" + znode));\n      } else {\n        LOG.debug(zkw.prefix(\"Set watcher on znode that does not yet exist, \" + znode));\n      }\n      return exists;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to set watcher on znode \" + znode), e);\n      zkw.keeperException(e);\n      return false;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to set watcher on znode \" + znode), e);\n      zkw.interruptedException(e);\n      return false;\n    }\n  }",
            " 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350 +\n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  ",
            "  /**\n   * Watch the specified znode for delete/create/change events.  The watcher is\n   * set whether or not the node exists.  If the node already exists, the method\n   * returns true.  If the node does not exist, the method returns false.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to watch\n   * @return true if znode exists, false if does not exist or error\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean watchAndCheckExists(ZKWatcher zkw, String znode)\n    throws KeeperException {\n    try {\n      Stat s = zkw.getRecoverableZooKeeper().exists(znode, zkw);\n      boolean exists = s != null ? true : false;\n      if (exists) {\n        LOG.debug(zkw.prefix(\"Set watcher on existing znode=\" + znode));\n      } else {\n        LOG.debug(zkw.prefix(\"Set watcher on znode that does not yet exist, \" + znode));\n      }\n      return exists;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to set watcher on znode \" + znode), e);\n      zkw.keeperException(e);\n      return false;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to set watcher on znode \" + znode), e);\n      zkw.interruptedException(e);\n      return false;\n    }\n  }"
        ],
        [
            "TestMasterNoCluster::tearDown()",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 -\n 124  \n 125  ",
            "  @After\n  public void tearDown()\n  throws KeeperException, ZooKeeperConnectionException, IOException {\n    // Make sure zk is clean before we run the next test.\n    ZKWatcher zkw = new ZKWatcher(TESTUTIL.getConfiguration(),\n        \"@Before\", new Abortable() {\n      @Override\n      public void abort(String why, Throwable e) {\n        throw new RuntimeException(why, e);\n      }\n\n      @Override\n      public boolean isAborted() {\n        return false;\n      }\n    });\n    ZKUtil.deleteNodeRecursively(zkw, zkw.znodePaths.baseZNode);\n    zkw.close();\n  }",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 +\n 124  \n 125  ",
            "  @After\n  public void tearDown()\n  throws KeeperException, ZooKeeperConnectionException, IOException {\n    // Make sure zk is clean before we run the next test.\n    ZKWatcher zkw = new ZKWatcher(TESTUTIL.getConfiguration(),\n        \"@Before\", new Abortable() {\n      @Override\n      public void abort(String why, Throwable e) {\n        throw new RuntimeException(why, e);\n      }\n\n      @Override\n      public boolean isAborted() {\n        return false;\n      }\n    });\n    ZKUtil.deleteNodeRecursively(zkw, zkw.getZNodePaths().baseZNode);\n    zkw.close();\n  }"
        ],
        [
            "MasterAddressTracker::deleteIfEquals(ZKWatcher,String)",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 -\n 269  \n 270  \n 271 -\n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "  /**\n   * delete the master znode if its content is same as the parameter\n   * @param zkw must not be null\n   * @param content must not be null\n   */\n  public static boolean deleteIfEquals(ZKWatcher zkw, final String content) {\n    if (content == null){\n      throw new IllegalArgumentException(\"Content must not be null\");\n    }\n\n    try {\n      Stat stat = new Stat();\n      byte[] data = ZKUtil.getDataNoWatch(zkw, zkw.znodePaths.masterAddressZNode, stat);\n      ServerName sn = ProtobufUtil.parseServerNameFrom(data);\n      if (sn != null && content.equals(sn.toString())) {\n        return (ZKUtil.deleteNode(zkw, zkw.znodePaths.masterAddressZNode, stat.getVersion()));\n      }\n    } catch (KeeperException e) {\n      LOG.warn(\"Can't get or delete the master znode\", e);\n    } catch (DeserializationException e) {\n      LOG.warn(\"Can't get or delete the master znode\", e);\n    }\n\n    return false;\n  }",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 +\n 269  \n 270  \n 271 +\n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "  /**\n   * delete the master znode if its content is same as the parameter\n   * @param zkw must not be null\n   * @param content must not be null\n   */\n  public static boolean deleteIfEquals(ZKWatcher zkw, final String content) {\n    if (content == null){\n      throw new IllegalArgumentException(\"Content must not be null\");\n    }\n\n    try {\n      Stat stat = new Stat();\n      byte[] data = ZKUtil.getDataNoWatch(zkw, zkw.getZNodePaths().masterAddressZNode, stat);\n      ServerName sn = ProtobufUtil.parseServerNameFrom(data);\n      if (sn != null && content.equals(sn.toString())) {\n        return (ZKUtil.deleteNode(zkw, zkw.getZNodePaths().masterAddressZNode, stat.getVersion()));\n      }\n    } catch (KeeperException e) {\n      LOG.warn(\"Can't get or delete the master znode\", e);\n    } catch (DeserializationException e) {\n      LOG.warn(\"Can't get or delete the master znode\", e);\n    }\n\n    return false;\n  }"
        ],
        [
            "ZKUtil::createEphemeralNodeAndWatch(ZKWatcher,String,byte)",
            " 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962 -\n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  ",
            "  /**\n   *\n   * Set the specified znode to be an ephemeral node carrying the specified\n   * data.\n   *\n   * If the node is created successfully, a watcher is also set on the node.\n   *\n   * If the node is not created successfully because it already exists, this\n   * method will also set a watcher on the node.\n   *\n   * If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data of node\n   * @return true if node created, false if not, watch set in both cases\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean createEphemeralNodeAndWatch(ZKWatcher zkw,\n      String znode, byte [] data)\n  throws KeeperException {\n    boolean ret = true;\n    try {\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.EPHEMERAL);\n    } catch (KeeperException.NodeExistsException nee) {\n      ret = false;\n    } catch (InterruptedException e) {\n      LOG.info(\"Interrupted\", e);\n      Thread.currentThread().interrupt();\n    }\n    if(!watchAndCheckExists(zkw, znode)) {\n      // It did exist but now it doesn't, try again\n      return createEphemeralNodeAndWatch(zkw, znode, data);\n    }\n    return ret;\n  }",
            " 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962 +\n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  ",
            "  /**\n   *\n   * Set the specified znode to be an ephemeral node carrying the specified\n   * data.\n   *\n   * If the node is created successfully, a watcher is also set on the node.\n   *\n   * If the node is not created successfully because it already exists, this\n   * method will also set a watcher on the node.\n   *\n   * If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data of node\n   * @return true if node created, false if not, watch set in both cases\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean createEphemeralNodeAndWatch(ZKWatcher zkw,\n      String znode, byte [] data)\n    throws KeeperException {\n    boolean ret = true;\n    try {\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.EPHEMERAL);\n    } catch (KeeperException.NodeExistsException nee) {\n      ret = false;\n    } catch (InterruptedException e) {\n      LOG.info(\"Interrupted\", e);\n      Thread.currentThread().interrupt();\n    }\n    if(!watchAndCheckExists(zkw, znode)) {\n      // It did exist but now it doesn't, try again\n      return createEphemeralNodeAndWatch(zkw, znode, data);\n    }\n    return ret;\n  }"
        ],
        [
            "MasterAddressSyncer::MasterAddressSyncer(ZKWatcher,ZKWatcher,Server)",
            "  36  \n  37  \n  38 -\n  39  ",
            "  public MasterAddressSyncer(ZKWatcher watcher, ZKWatcher clientZkWatcher, Server server) {\n    super(watcher, clientZkWatcher, server);\n    masterAddressZNode = watcher.znodePaths.masterAddressZNode;\n  }",
            "  36  \n  37  \n  38 +\n  39  ",
            "  public MasterAddressSyncer(ZKWatcher watcher, ZKWatcher clientZkWatcher, Server server) {\n    super(watcher, clientZkWatcher, server);\n    masterAddressZNode = watcher.getZNodePaths().masterAddressZNode;\n  }"
        ],
        [
            "TestSplitLogWorker::testAcquireMultiTasksByAvgTasksPerRS()",
            " 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497 -\n 498  \n 499 -\n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  ",
            "  /**\n   * The test checks SplitLogWorker should not spawn more splitters than expected num of tasks per\n   * RS\n   * @throws Exception\n   */\n  @Test\n  public void testAcquireMultiTasksByAvgTasksPerRS() throws Exception {\n    LOG.info(\"testAcquireMultiTasks\");\n    SplitLogCounters.resetCounters();\n    final String TATAS = \"tatas\";\n    final ServerName RS = ServerName.valueOf(\"rs,1,1\");\n    final ServerName RS2 = ServerName.valueOf(\"rs,1,2\");\n    final int maxTasks = 3;\n    Configuration testConf = HBaseConfiguration.create(TEST_UTIL.getConfiguration());\n    testConf.setInt(\"hbase.regionserver.wal.max.splitters\", maxTasks);\n    RegionServerServices mockedRS = getRegionServer(RS);\n\n    // create two RS nodes\n    String rsPath = ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, RS.getServerName());\n    zkw.getRecoverableZooKeeper().create(rsPath, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\n    rsPath = ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, RS2.getServerName());\n    zkw.getRecoverableZooKeeper().create(rsPath, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\n\n    for (int i = 0; i < maxTasks; i++) {\n      zkw.getRecoverableZooKeeper().create(ZKSplitLog.getEncodedNodeName(zkw, TATAS + i),\n        new SplitLogTask.Unassigned(ServerName.valueOf(\"mgr,1,1\")).toByteArray(),\n          Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n    }\n\n    SplitLogWorker slw = new SplitLogWorker(ds, testConf, mockedRS, neverEndingTask);\n    slw.start();\n    try {\n      int acquiredTasks = 0;\n      waitForCounter(SplitLogCounters.tot_wkr_task_acquired, 0, 2, WAIT_TIME);\n      for (int i = 0; i < maxTasks; i++) {\n        byte[] bytes = ZKUtil.getData(zkw, ZKSplitLog.getEncodedNodeName(zkw, TATAS + i));\n        SplitLogTask slt = SplitLogTask.parseFrom(bytes);\n        if (slt.isOwned(RS)) {\n          acquiredTasks++;\n        }\n      }\n      assertEquals(2, acquiredTasks);\n    } finally {\n      stopSplitLogWorker(slw);\n    }\n  }",
            " 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498 +\n 499  \n 500 +\n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  ",
            "  /**\n   * The test checks SplitLogWorker should not spawn more splitters than expected num of tasks per\n   * RS\n   * @throws Exception\n   */\n  @Test\n  public void testAcquireMultiTasksByAvgTasksPerRS() throws Exception {\n    LOG.info(\"testAcquireMultiTasks\");\n    SplitLogCounters.resetCounters();\n    final String TATAS = \"tatas\";\n    final ServerName RS = ServerName.valueOf(\"rs,1,1\");\n    final ServerName RS2 = ServerName.valueOf(\"rs,1,2\");\n    final int maxTasks = 3;\n    Configuration testConf = HBaseConfiguration.create(TEST_UTIL.getConfiguration());\n    testConf.setInt(\"hbase.regionserver.wal.max.splitters\", maxTasks);\n    RegionServerServices mockedRS = getRegionServer(RS);\n\n    // create two RS nodes\n    String rsPath = ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, RS.getServerName());\n    zkw.getRecoverableZooKeeper().create(rsPath, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\n    rsPath = ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, RS2.getServerName());\n    zkw.getRecoverableZooKeeper().create(rsPath, null, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL);\n\n    for (int i = 0; i < maxTasks; i++) {\n      zkw.getRecoverableZooKeeper().create(ZKSplitLog.getEncodedNodeName(zkw, TATAS + i),\n        new SplitLogTask.Unassigned(ServerName.valueOf(\"mgr,1,1\")).toByteArray(),\n          Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);\n    }\n\n    SplitLogWorker slw = new SplitLogWorker(ds, testConf, mockedRS, neverEndingTask);\n    slw.start();\n    try {\n      int acquiredTasks = 0;\n      waitForCounter(SplitLogCounters.tot_wkr_task_acquired, 0, 2, WAIT_TIME);\n      for (int i = 0; i < maxTasks; i++) {\n        byte[] bytes = ZKUtil.getData(zkw, ZKSplitLog.getEncodedNodeName(zkw, TATAS + i));\n        SplitLogTask slt = SplitLogTask.parseFrom(bytes);\n        if (slt.isOwned(RS)) {\n          acquiredTasks++;\n        }\n      }\n      assertEquals(2, acquiredTasks);\n    } finally {\n      stopSplitLogWorker(slw);\n    }\n  }"
        ],
        [
            "RSGroupInfoManagerImpl::getOnlineRS()",
            " 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552 -\n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  ",
            "  private List<ServerName> getOnlineRS() throws IOException {\n    if (masterServices != null) {\n      return masterServices.getServerManager().getOnlineServersList();\n    }\n    LOG.debug(\"Reading online RS from zookeeper\");\n    List<ServerName> servers = new LinkedList<>();\n    try {\n      for (String el: ZKUtil.listChildrenNoWatch(watcher, watcher.znodePaths.rsZNode)) {\n        servers.add(ServerName.parseServerName(el));\n      }\n    } catch (KeeperException e) {\n      throw new IOException(\"Failed to retrieve server list from zookeeper\", e);\n    }\n    return servers;\n  }",
            " 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552 +\n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  ",
            "  private List<ServerName> getOnlineRS() throws IOException {\n    if (masterServices != null) {\n      return masterServices.getServerManager().getOnlineServersList();\n    }\n    LOG.debug(\"Reading online RS from zookeeper\");\n    List<ServerName> servers = new LinkedList<>();\n    try {\n      for (String el: ZKUtil.listChildrenNoWatch(watcher, watcher.getZNodePaths().rsZNode)) {\n        servers.add(ServerName.parseServerName(el));\n      }\n    } catch (KeeperException e) {\n      throw new IOException(\"Failed to retrieve server list from zookeeper\", e);\n    }\n    return servers;\n  }"
        ],
        [
            "ZKUtil::deleteNode(ZKWatcher,String,int)",
            "1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229 -\n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  ",
            "  /**\n   * Delete the specified node with the specified version.  Sets no watches.\n   * Throws all exceptions.\n   */\n  public static boolean deleteNode(ZKWatcher zkw, String node,\n                                   int version)\n  throws KeeperException {\n    try {\n      zkw.getRecoverableZooKeeper().delete(node, version);\n      return true;\n    } catch(KeeperException.BadVersionException bve) {\n      return false;\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n      return false;\n    }\n  }",
            "1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229 +\n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  ",
            "  /**\n   * Delete the specified node with the specified version.  Sets no watches.\n   * Throws all exceptions.\n   */\n  public static boolean deleteNode(ZKWatcher zkw, String node,\n                                   int version)\n    throws KeeperException {\n    try {\n      zkw.getRecoverableZooKeeper().delete(node, version);\n      return true;\n    } catch(KeeperException.BadVersionException bve) {\n      return false;\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n      return false;\n    }\n  }"
        ],
        [
            "MetaTableLocator::deleteMetaLocation(ZKWatcher)",
            " 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535 -\n 536  \n 537  ",
            "  /**\n   * Deletes the location of <code>hbase:meta</code> in ZooKeeper.\n   * @param zookeeper zookeeper reference\n   * @throws KeeperException unexpected zookeeper exception\n   */\n  public void deleteMetaLocation(ZKWatcher zookeeper)\n  throws KeeperException {\n    deleteMetaLocation(zookeeper, RegionInfo.DEFAULT_REPLICA_ID);\n  }",
            " 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536 +\n 537  \n 538  ",
            "  /**\n   * Deletes the location of <code>hbase:meta</code> in ZooKeeper.\n   * @param zookeeper zookeeper reference\n   * @throws KeeperException unexpected zookeeper exception\n   */\n  public void deleteMetaLocation(ZKWatcher zookeeper)\n    throws KeeperException {\n    deleteMetaLocation(zookeeper, RegionInfo.DEFAULT_REPLICA_ID);\n  }"
        ],
        [
            "TestZKUtilNoServer::testInterruptedDuringAction()",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 -\n 116 -\n 117  ",
            "  @Test(expected = KeeperException.SystemErrorException.class)\n  public void testInterruptedDuringAction()\n      throws ZooKeeperConnectionException, IOException, KeeperException, InterruptedException {\n    final RecoverableZooKeeper recoverableZk = Mockito.mock(RecoverableZooKeeper.class);\n    ZKWatcher zkw = new ZKWatcher(HBaseConfiguration.create(), \"unittest\", null) {\n      @Override\n      public RecoverableZooKeeper getRecoverableZooKeeper() {\n        return recoverableZk;\n      }\n    };\n    Mockito.doThrow(new InterruptedException()).when(recoverableZk)\n        .getChildren(zkw.znodePaths.baseZNode, null);\n    ZKUtil.listChildrenNoWatch(zkw, zkw.znodePaths.baseZNode);\n  }",
            " 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 +\n 116 +\n 117  ",
            "  @Test(expected = KeeperException.SystemErrorException.class)\n  public void testInterruptedDuringAction()\n      throws ZooKeeperConnectionException, IOException, KeeperException, InterruptedException {\n    final RecoverableZooKeeper recoverableZk = Mockito.mock(RecoverableZooKeeper.class);\n    ZKWatcher zkw = new ZKWatcher(HBaseConfiguration.create(), \"unittest\", null) {\n      @Override\n      public RecoverableZooKeeper getRecoverableZooKeeper() {\n        return recoverableZk;\n      }\n    };\n    Mockito.doThrow(new InterruptedException()).when(recoverableZk)\n        .getChildren(zkw.getZNodePaths().baseZNode, null);\n    ZKUtil.listChildrenNoWatch(zkw, zkw.getZNodePaths().baseZNode);\n  }"
        ],
        [
            "ZKSplitLog::getEncodedNodeName(ZKWatcher,String)",
            "  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 -\n  51  ",
            "  /**\n   * Gets the full path node name for the log file being split.\n   * This method will url encode the filename.\n   * @param zkw zk reference\n   * @param filename log file name (only the basename)\n   */\n  public static String getEncodedNodeName(ZKWatcher zkw, String filename) {\n    return ZNodePaths.joinZNode(zkw.znodePaths.splitLogZNode, encode(filename));\n  }",
            "  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 +\n  51  ",
            "  /**\n   * Gets the full path node name for the log file being split.\n   * This method will url encode the filename.\n   * @param zkw zk reference\n   * @param filename log file name (only the basename)\n   */\n  public static String getEncodedNodeName(ZKWatcher zkw, String filename) {\n    return ZNodePaths.joinZNode(zkw.getZNodePaths().splitLogZNode, encode(filename));\n  }"
        ],
        [
            "ZKClusterId::readClusterIdZNode(ZKWatcher)",
            "  63  \n  64 -\n  65 -\n  66  \n  67  \n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "  public static String readClusterIdZNode(ZKWatcher watcher)\n  throws KeeperException {\n    if (ZKUtil.checkExists(watcher, watcher.znodePaths.clusterIdZNode) != -1) {\n      byte [] data;\n      try {\n        data = ZKUtil.getData(watcher, watcher.znodePaths.clusterIdZNode);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        return null;\n      }\n      if (data != null) {\n        try {\n          return ClusterId.parseFrom(data).toString();\n        } catch (DeserializationException e) {\n          throw ZKUtil.convert(e);\n        }\n      }\n    }\n    return null;\n  }",
            "  63  \n  64 +\n  65 +\n  66  \n  67  \n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "  public static String readClusterIdZNode(ZKWatcher watcher)\n    throws KeeperException {\n    if (ZKUtil.checkExists(watcher, watcher.getZNodePaths().clusterIdZNode) != -1) {\n      byte [] data;\n      try {\n        data = ZKUtil.getData(watcher, watcher.getZNodePaths().clusterIdZNode);\n      } catch (InterruptedException e) {\n        Thread.currentThread().interrupt();\n        return null;\n      }\n      if (data != null) {\n        try {\n          return ClusterId.parseFrom(data).toString();\n        } catch (DeserializationException e) {\n          throw ZKUtil.convert(e);\n        }\n      }\n    }\n    return null;\n  }"
        ],
        [
            "ZKUtil::connect(Configuration,String,Watcher,String)",
            " 122  \n 123  \n 124 -\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "  public static RecoverableZooKeeper connect(Configuration conf, String ensemble,\n      Watcher watcher, final String identifier)\n  throws IOException {\n    if(ensemble == null) {\n      throw new IOException(\"Unable to determine ZooKeeper ensemble\");\n    }\n    int timeout = conf.getInt(HConstants.ZK_SESSION_TIMEOUT,\n        HConstants.DEFAULT_ZK_SESSION_TIMEOUT);\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(identifier + \" opening connection to ZooKeeper ensemble=\" + ensemble);\n    }\n    int retry = conf.getInt(\"zookeeper.recovery.retry\", 3);\n    int retryIntervalMillis =\n      conf.getInt(\"zookeeper.recovery.retry.intervalmill\", 1000);\n    int maxSleepTime = conf.getInt(\"zookeeper.recovery.retry.maxsleeptime\", 60000);\n    zkDumpConnectionTimeOut = conf.getInt(\"zookeeper.dump.connection.timeout\",\n        1000);\n    return new RecoverableZooKeeper(ensemble, timeout, watcher,\n        retry, retryIntervalMillis, maxSleepTime, identifier);\n  }",
            " 122  \n 123  \n 124 +\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "  public static RecoverableZooKeeper connect(Configuration conf, String ensemble,\n      Watcher watcher, final String identifier)\n    throws IOException {\n    if(ensemble == null) {\n      throw new IOException(\"Unable to determine ZooKeeper ensemble\");\n    }\n    int timeout = conf.getInt(HConstants.ZK_SESSION_TIMEOUT,\n        HConstants.DEFAULT_ZK_SESSION_TIMEOUT);\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(identifier + \" opening connection to ZooKeeper ensemble=\" + ensemble);\n    }\n    int retry = conf.getInt(\"zookeeper.recovery.retry\", 3);\n    int retryIntervalMillis =\n      conf.getInt(\"zookeeper.recovery.retry.intervalmill\", 1000);\n    int maxSleepTime = conf.getInt(\"zookeeper.recovery.retry.maxsleeptime\", 60000);\n    zkDumpConnectionTimeOut = conf.getInt(\"zookeeper.dump.connection.timeout\",\n        1000);\n    return new RecoverableZooKeeper(ensemble, timeout, watcher,\n        retry, retryIntervalMillis, maxSleepTime, identifier);\n  }"
        ],
        [
            "ReplicationPeerConfigUpgrader::getTableCFsNode(String)",
            "  98  \n  99  \n 100 -\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  ",
            "  @VisibleForTesting\n  protected String getTableCFsNode(String peerId) {\n    String replicationZNode = ZNodePaths.joinZNode(zookeeper.znodePaths.baseZNode,\n      conf.get(REPLICATION_ZNODE, REPLICATION_ZNODE_DEFAULT));\n    String peersZNode =\n        ZNodePaths.joinZNode(replicationZNode, conf.get(PEERS_ZNODE, PEERS_ZNODE_DEFAULT));\n    return ZNodePaths.joinZNode(peersZNode,\n      ZNodePaths.joinZNode(peerId, conf.get(TABLE_CFS_ZNODE, TABLE_CFS_ZNODE_DEFAULT)));\n  }",
            "  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  ",
            "  @VisibleForTesting\n  protected String getTableCFsNode(String peerId) {\n    String replicationZNode = ZNodePaths.joinZNode(zookeeper.getZNodePaths().baseZNode,\n      conf.get(REPLICATION_ZNODE, REPLICATION_ZNODE_DEFAULT));\n    String peersZNode =\n        ZNodePaths.joinZNode(replicationZNode, conf.get(PEERS_ZNODE, PEERS_ZNODE_DEFAULT));\n    return ZNodePaths.joinZNode(peersZNode,\n      ZNodePaths.joinZNode(peerId, conf.get(TABLE_CFS_ZNODE, TABLE_CFS_ZNODE_DEFAULT)));\n  }"
        ],
        [
            "ZKPermissionWatcher::writeToZookeeper(byte,byte)",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 -\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  /***\n   * Write a table's access controls to the permissions mirror in zookeeper\n   * @param entry\n   * @param permsData\n   */\n  public void writeToZookeeper(byte[] entry, byte[] permsData) {\n    String entryName = Bytes.toString(entry);\n    String zkNode = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, ACL_NODE);\n    zkNode = ZNodePaths.joinZNode(zkNode, entryName);\n\n    try {\n      ZKUtil.createWithParents(watcher, zkNode);\n      ZKUtil.updateExistingNodeData(watcher, zkNode, permsData, -1);\n    } catch (KeeperException e) {\n      LOG.error(\"Failed updating permissions for entry '\" +\n          entryName + \"'\", e);\n      watcher.abort(\"Failed writing node \"+zkNode+\" to zookeeper\", e);\n    }\n  }",
            " 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 +\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  /***\n   * Write a table's access controls to the permissions mirror in zookeeper\n   * @param entry\n   * @param permsData\n   */\n  public void writeToZookeeper(byte[] entry, byte[] permsData) {\n    String entryName = Bytes.toString(entry);\n    String zkNode = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, ACL_NODE);\n    zkNode = ZNodePaths.joinZNode(zkNode, entryName);\n\n    try {\n      ZKUtil.createWithParents(watcher, zkNode);\n      ZKUtil.updateExistingNodeData(watcher, zkNode, permsData, -1);\n    } catch (KeeperException e) {\n      LOG.error(\"Failed updating permissions for entry '\" +\n          entryName + \"'\", e);\n      watcher.abort(\"Failed writing node \"+zkNode+\" to zookeeper\", e);\n    }\n  }"
        ],
        [
            "ZkSplitLogWorkerCoordination::nodeChildrenChanged(String)",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100 -\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  /**\n   * Override handler from {@link ZKListener}\n   */\n  @Override\n  public void nodeChildrenChanged(String path) {\n    if (path.equals(watcher.znodePaths.splitLogZNode)) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"tasks arrived or departed on \" + path);\n      }\n      synchronized (taskReadySeq) {\n        this.taskReadySeq.incrementAndGet();\n        taskReadySeq.notify();\n      }\n    }\n  }",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "  /**\n   * Override handler from {@link ZKListener}\n   */\n  @Override\n  public void nodeChildrenChanged(String path) {\n    if (path.equals(watcher.getZNodePaths().splitLogZNode)) {\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"tasks arrived or departed on \" + path);\n      }\n      synchronized (taskReadySeq) {\n        this.taskReadySeq.incrementAndGet();\n        taskReadySeq.notify();\n      }\n    }\n  }"
        ],
        [
            "ZKPermissionWatcher::deleteNamespaceACLNode(String)",
            " 294  \n 295  \n 296  \n 297  \n 298 -\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  ",
            "  /***\n   * Delete the acl notify node of namespace\n   */\n  public void deleteNamespaceACLNode(final String namespace) {\n    String zkNode = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, ACL_NODE);\n    zkNode = ZNodePaths.joinZNode(zkNode, AccessControlLists.NAMESPACE_PREFIX + namespace);\n\n    try {\n      ZKUtil.deleteNode(watcher, zkNode);\n    } catch (KeeperException.NoNodeException e) {\n      LOG.warn(\"No acl notify node of namespace '\" + namespace + \"'\");\n    } catch (KeeperException e) {\n      LOG.error(\"Failed deleting acl node of namespace '\" + namespace + \"'\", e);\n      watcher.abort(\"Failed deleting node \" + zkNode, e);\n    }\n  }",
            " 294  \n 295  \n 296  \n 297  \n 298 +\n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  ",
            "  /***\n   * Delete the acl notify node of namespace\n   */\n  public void deleteNamespaceACLNode(final String namespace) {\n    String zkNode = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, ACL_NODE);\n    zkNode = ZNodePaths.joinZNode(zkNode, AccessControlLists.NAMESPACE_PREFIX + namespace);\n\n    try {\n      ZKUtil.deleteNode(watcher, zkNode);\n    } catch (KeeperException.NoNodeException e) {\n      LOG.warn(\"No acl notify node of namespace '\" + namespace + \"'\");\n    } catch (KeeperException e) {\n      LOG.error(\"Failed deleting acl node of namespace '\" + namespace + \"'\", e);\n      watcher.abort(\"Failed deleting node \" + zkNode, e);\n    }\n  }"
        ],
        [
            "ReplicationTrackerZKImpl::getRegisteredRegionServers(boolean)",
            " 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 -\n 186  \n 187 -\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "  /**\n   * Get a list of all the other region servers in this cluster and set a watch\n   * @return a list of server nanes\n   */\n  private List<String> getRegisteredRegionServers(boolean watch) {\n    List<String> result = null;\n    try {\n      if (watch) {\n        result = ZKUtil.listChildrenAndWatchThem(this.zookeeper, this.zookeeper.znodePaths.rsZNode);\n      } else {\n        result = ZKUtil.listChildrenNoWatch(this.zookeeper, this.zookeeper.znodePaths.rsZNode);\n      }\n    } catch (KeeperException e) {\n      this.abortable.abort(\"Get list of registered region servers\", e);\n    }\n    return result;\n  }",
            " 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 +\n 186 +\n 187  \n 188 +\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  ",
            "  /**\n   * Get a list of all the other region servers in this cluster and set a watch\n   * @return a list of server nanes\n   */\n  private List<String> getRegisteredRegionServers(boolean watch) {\n    List<String> result = null;\n    try {\n      if (watch) {\n        result = ZKUtil.listChildrenAndWatchThem(this.zookeeper,\n                this.zookeeper.getZNodePaths().rsZNode);\n      } else {\n        result = ZKUtil.listChildrenNoWatch(this.zookeeper, this.zookeeper.getZNodePaths().rsZNode);\n      }\n    } catch (KeeperException e) {\n      this.abortable.abort(\"Get list of registered region servers\", e);\n    }\n    return result;\n  }"
        ],
        [
            "ZKWatcher::process(WatchedEvent)",
            " 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  ",
            "  /**\n   * Method called from ZooKeeper for events and connection status.\n   * <p>\n   * Valid events are passed along to listeners.  Connection status changes\n   * are dealt with locally.\n   */\n  @Override\n  public void process(WatchedEvent event) {\n    LOG.debug(prefix(\"Received ZooKeeper Event, \" +\n        \"type=\" + event.getType() + \", \" +\n        \"state=\" + event.getState() + \", \" +\n        \"path=\" + event.getPath()));\n\n    switch(event.getType()) {\n\n      // If event type is NONE, this is a connection status change\n      case None: {\n        connectionEvent(event);\n        break;\n      }\n\n      // Otherwise pass along to the listeners\n\n      case NodeCreated: {\n        for(ZKListener listener : listeners) {\n          listener.nodeCreated(event.getPath());\n        }\n        break;\n      }\n\n      case NodeDeleted: {\n        for(ZKListener listener : listeners) {\n          listener.nodeDeleted(event.getPath());\n        }\n        break;\n      }\n\n      case NodeDataChanged: {\n        for(ZKListener listener : listeners) {\n          listener.nodeDataChanged(event.getPath());\n        }\n        break;\n      }\n\n      case NodeChildrenChanged: {\n        for(ZKListener listener : listeners) {\n          listener.nodeChildrenChanged(event.getPath());\n        }\n        break;\n      }\n    }\n  }",
            " 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520 +\n 521 +\n 522  \n 523  ",
            "  /**\n   * Method called from ZooKeeper for events and connection status.\n   * <p>\n   * Valid events are passed along to listeners.  Connection status changes\n   * are dealt with locally.\n   */\n  @Override\n  public void process(WatchedEvent event) {\n    LOG.debug(prefix(\"Received ZooKeeper Event, \" +\n        \"type=\" + event.getType() + \", \" +\n        \"state=\" + event.getState() + \", \" +\n        \"path=\" + event.getPath()));\n\n    switch(event.getType()) {\n\n      // If event type is NONE, this is a connection status change\n      case None: {\n        connectionEvent(event);\n        break;\n      }\n\n      // Otherwise pass along to the listeners\n\n      case NodeCreated: {\n        for(ZKListener listener : listeners) {\n          listener.nodeCreated(event.getPath());\n        }\n        break;\n      }\n\n      case NodeDeleted: {\n        for(ZKListener listener : listeners) {\n          listener.nodeDeleted(event.getPath());\n        }\n        break;\n      }\n\n      case NodeDataChanged: {\n        for(ZKListener listener : listeners) {\n          listener.nodeDataChanged(event.getPath());\n        }\n        break;\n      }\n\n      case NodeChildrenChanged: {\n        for(ZKListener listener : listeners) {\n          listener.nodeChildrenChanged(event.getPath());\n        }\n        break;\n      }\n      default:\n        throw new IllegalStateException(\"Received event is not valid: \" + event.getState());\n    }\n  }"
        ],
        [
            "ZKAclReset::resetAcls(Configuration,boolean)",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73 -\n  74 -\n  75  \n  76  \n  77  \n  78  ",
            "  private static void resetAcls(final Configuration conf, boolean eraseAcls)\n      throws Exception {\n    ZKWatcher zkw = new ZKWatcher(conf, \"ZKAclReset\", null);\n    try {\n      LOG.info((eraseAcls ? \"Erase\" : \"Set\") + \" HBase ACLs for \" +\n                zkw.getQuorum() + \" \" + zkw.znodePaths.baseZNode);\n      resetAcls(zkw, zkw.znodePaths.baseZNode, eraseAcls);\n    } finally {\n      zkw.close();\n    }\n  }",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73 +\n  74 +\n  75  \n  76  \n  77  \n  78  ",
            "  private static void resetAcls(final Configuration conf, boolean eraseAcls)\n      throws Exception {\n    ZKWatcher zkw = new ZKWatcher(conf, \"ZKAclReset\", null);\n    try {\n      LOG.info((eraseAcls ? \"Erase\" : \"Set\") + \" HBase ACLs for \" +\n                zkw.getQuorum() + \" \" + zkw.getZNodePaths().baseZNode);\n      resetAcls(zkw, zkw.getZNodePaths().baseZNode, eraseAcls);\n    } finally {\n      zkw.close();\n    }\n  }"
        ],
        [
            "HMaster::decommissionRegionServers(List,boolean)",
            "3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503 -\n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514  \n3515  \n3516  \n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  \n3524  \n3525  \n3526  \n3527  \n3528  \n3529  \n3530  \n3531  \n3532  ",
            "  /**\n   * Mark region server(s) as decommissioned (previously called 'draining') to prevent additional\n   * regions from getting assigned to them. Also unload the regions on the servers asynchronously.0\n   * @param servers Region servers to decommission.\n   * @throws HBaseIOException\n   */\n  public void decommissionRegionServers(final List<ServerName> servers, final boolean offload)\n      throws HBaseIOException {\n    List<ServerName> serversAdded = new ArrayList<>(servers.size());\n    // Place the decommission marker first.\n    String parentZnode = getZooKeeper().znodePaths.drainingZNode;\n    for (ServerName server : servers) {\n      try {\n        String node = ZNodePaths.joinZNode(parentZnode, server.getServerName());\n        ZKUtil.createAndFailSilent(getZooKeeper(), node);\n      } catch (KeeperException ke) {\n        throw new HBaseIOException(\n            this.zooKeeper.prefix(\"Unable to decommission '\" + server.getServerName() + \"'.\"), ke);\n      }\n      if (this.serverManager.addServerToDrainList(server)) {\n        serversAdded.add(server);\n      }\n    }\n    // Move the regions off the decommissioned servers.\n    if (offload) {\n      final List<ServerName> destServers = this.serverManager.createDestinationServersList();\n      for (ServerName server : serversAdded) {\n        final List<RegionInfo> regionsOnServer =\n            this.assignmentManager.getRegionStates().getServerRegionInfoSet(server);\n        for (RegionInfo hri : regionsOnServer) {\n          ServerName dest = balancer.randomAssignment(hri, destServers);\n          if (dest == null) {\n            throw new HBaseIOException(\"Unable to determine a plan to move \" + hri);\n          }\n          RegionPlan rp = new RegionPlan(hri, server, dest);\n          this.assignmentManager.moveAsync(rp);\n        }\n      }\n    }\n  }",
            "3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503 +\n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514  \n3515  \n3516  \n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  \n3524  \n3525  \n3526  \n3527  \n3528  \n3529  \n3530  \n3531  \n3532  ",
            "  /**\n   * Mark region server(s) as decommissioned (previously called 'draining') to prevent additional\n   * regions from getting assigned to them. Also unload the regions on the servers asynchronously.0\n   * @param servers Region servers to decommission.\n   * @throws HBaseIOException\n   */\n  public void decommissionRegionServers(final List<ServerName> servers, final boolean offload)\n      throws HBaseIOException {\n    List<ServerName> serversAdded = new ArrayList<>(servers.size());\n    // Place the decommission marker first.\n    String parentZnode = getZooKeeper().getZNodePaths().drainingZNode;\n    for (ServerName server : servers) {\n      try {\n        String node = ZNodePaths.joinZNode(parentZnode, server.getServerName());\n        ZKUtil.createAndFailSilent(getZooKeeper(), node);\n      } catch (KeeperException ke) {\n        throw new HBaseIOException(\n            this.zooKeeper.prefix(\"Unable to decommission '\" + server.getServerName() + \"'.\"), ke);\n      }\n      if (this.serverManager.addServerToDrainList(server)) {\n        serversAdded.add(server);\n      }\n    }\n    // Move the regions off the decommissioned servers.\n    if (offload) {\n      final List<ServerName> destServers = this.serverManager.createDestinationServersList();\n      for (ServerName server : serversAdded) {\n        final List<RegionInfo> regionsOnServer =\n            this.assignmentManager.getRegionStates().getServerRegionInfoSet(server);\n        for (RegionInfo hri : regionsOnServer) {\n          ServerName dest = balancer.randomAssignment(hri, destServers);\n          if (dest == null) {\n            throw new HBaseIOException(\"Unable to determine a plan to move \" + hri);\n          }\n          RegionPlan rp = new RegionPlan(hri, server, dest);\n          this.assignmentManager.moveAsync(rp);\n        }\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::listChildrenNoWatch(ZKWatcher,String)",
            " 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496 -\n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  ",
            "  /**\n   * Lists the children of the specified znode without setting any watches.\n   *\n   * Sets no watches at all, this method is best effort.\n   *\n   * Returns an empty list if the node has no children.  Returns null if the\n   * parent node itself does not exist.\n   *\n   * @param zkw zookeeper reference\n   * @param znode node to get children\n   * @return list of data of children of specified znode, empty if no children,\n   *         null if parent does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static List<String> listChildrenNoWatch(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    List<String> children = null;\n    try {\n      // List the children without watching\n      children = zkw.getRecoverableZooKeeper().getChildren(znode, null);\n    } catch(KeeperException.NoNodeException nne) {\n      return null;\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n    return children;\n  }",
            " 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496 +\n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  ",
            "  /**\n   * Lists the children of the specified znode without setting any watches.\n   *\n   * Sets no watches at all, this method is best effort.\n   *\n   * Returns an empty list if the node has no children.  Returns null if the\n   * parent node itself does not exist.\n   *\n   * @param zkw zookeeper reference\n   * @param znode node to get children\n   * @return list of data of children of specified znode, empty if no children,\n   *         null if parent does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static List<String> listChildrenNoWatch(ZKWatcher zkw, String znode)\n    throws KeeperException {\n    List<String> children = null;\n    try {\n      // List the children without watching\n      children = zkw.getRecoverableZooKeeper().getChildren(znode, null);\n    } catch(KeeperException.NoNodeException nne) {\n      return null;\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n    return children;\n  }"
        ],
        [
            "HMaster::getBackupMasters()",
            "2536  \n2537  \n2538  \n2539  \n2540  \n2541 -\n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555 -\n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  ",
            "  private List<ServerName> getBackupMasters() throws InterruptedIOException {\n    // Build Set of backup masters from ZK nodes\n    List<String> backupMasterStrings;\n    try {\n      backupMasterStrings = ZKUtil.listChildrenNoWatch(this.zooKeeper,\n        this.zooKeeper.znodePaths.backupMasterAddressesZNode);\n    } catch (KeeperException e) {\n      LOG.warn(this.zooKeeper.prefix(\"Unable to list backup servers\"), e);\n      backupMasterStrings = null;\n    }\n\n    List<ServerName> backupMasters = Collections.emptyList();\n    if (backupMasterStrings != null && !backupMasterStrings.isEmpty()) {\n      backupMasters = new ArrayList<>(backupMasterStrings.size());\n      for (String s: backupMasterStrings) {\n        try {\n          byte [] bytes;\n          try {\n            bytes = ZKUtil.getData(this.zooKeeper, ZNodePaths.joinZNode(\n                this.zooKeeper.znodePaths.backupMasterAddressesZNode, s));\n          } catch (InterruptedException e) {\n            throw new InterruptedIOException();\n          }\n          if (bytes != null) {\n            ServerName sn;\n            try {\n              sn = ProtobufUtil.parseServerNameFrom(bytes);\n            } catch (DeserializationException e) {\n              LOG.warn(\"Failed parse, skipping registering backup server\", e);\n              continue;\n            }\n            backupMasters.add(sn);\n          }\n        } catch (KeeperException e) {\n          LOG.warn(this.zooKeeper.prefix(\"Unable to get information about \" +\n                   \"backup servers\"), e);\n        }\n      }\n      Collections.sort(backupMasters, new Comparator<ServerName>() {\n        @Override\n        public int compare(ServerName s1, ServerName s2) {\n          return s1.getServerName().compareTo(s2.getServerName());\n        }});\n    }\n    return backupMasters;\n  }",
            "2536  \n2537  \n2538  \n2539  \n2540  \n2541 +\n2542  \n2543  \n2544  \n2545  \n2546  \n2547  \n2548  \n2549  \n2550  \n2551  \n2552  \n2553  \n2554  \n2555 +\n2556  \n2557  \n2558  \n2559  \n2560  \n2561  \n2562  \n2563  \n2564  \n2565  \n2566  \n2567  \n2568  \n2569  \n2570  \n2571  \n2572  \n2573  \n2574  \n2575  \n2576  \n2577  \n2578  \n2579  \n2580  \n2581  ",
            "  private List<ServerName> getBackupMasters() throws InterruptedIOException {\n    // Build Set of backup masters from ZK nodes\n    List<String> backupMasterStrings;\n    try {\n      backupMasterStrings = ZKUtil.listChildrenNoWatch(this.zooKeeper,\n        this.zooKeeper.getZNodePaths().backupMasterAddressesZNode);\n    } catch (KeeperException e) {\n      LOG.warn(this.zooKeeper.prefix(\"Unable to list backup servers\"), e);\n      backupMasterStrings = null;\n    }\n\n    List<ServerName> backupMasters = Collections.emptyList();\n    if (backupMasterStrings != null && !backupMasterStrings.isEmpty()) {\n      backupMasters = new ArrayList<>(backupMasterStrings.size());\n      for (String s: backupMasterStrings) {\n        try {\n          byte [] bytes;\n          try {\n            bytes = ZKUtil.getData(this.zooKeeper, ZNodePaths.joinZNode(\n                this.zooKeeper.getZNodePaths().backupMasterAddressesZNode, s));\n          } catch (InterruptedException e) {\n            throw new InterruptedIOException();\n          }\n          if (bytes != null) {\n            ServerName sn;\n            try {\n              sn = ProtobufUtil.parseServerNameFrom(bytes);\n            } catch (DeserializationException e) {\n              LOG.warn(\"Failed parse, skipping registering backup server\", e);\n              continue;\n            }\n            backupMasters.add(sn);\n          }\n        } catch (KeeperException e) {\n          LOG.warn(this.zooKeeper.prefix(\"Unable to get information about \" +\n                   \"backup servers\"), e);\n        }\n      }\n      Collections.sort(backupMasters, new Comparator<ServerName>() {\n        @Override\n        public int compare(ServerName s1, ServerName s2) {\n          return s1.getServerName().compareTo(s2.getServerName());\n        }});\n    }\n    return backupMasters;\n  }"
        ],
        [
            "ZKMainServer::HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain::HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(String)",
            "  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  ",
            "    public HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(String[] args)\n    throws IOException, InterruptedException {\n      super(args);\n      // Make sure we are connected before we proceed. Can take a while on some systems. If we\n      // run the command without being connected, we get ConnectionLoss KeeperErrorConnection...\n      // Make it 30seconds. We dont' have a config in this context and zk doesn't have\n      // a timeout until after connection. 30000ms is default for zk.\n      ZooKeeperHelper.ensureConnectedZooKeeper(this.zk, 30000);\n    }",
            "  48  \n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  ",
            "    public HACK_UNTIL_ZOOKEEPER_1897_ZooKeeperMain(String[] args)\n      throws IOException, InterruptedException {\n      super(args);\n      // Make sure we are connected before we proceed. Can take a while on some systems. If we\n      // run the command without being connected, we get ConnectionLoss KeeperErrorConnection...\n      // Make it 30seconds. We dont' have a config in this context and zk doesn't have\n      // a timeout until after connection. 30000ms is default for zk.\n      ZooKeeperHelper.ensureConnectedZooKeeper(this.zk, 30000);\n    }"
        ],
        [
            "TestActiveMasterManager::testActiveMasterManagerFromZK()",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 -\n 125 -\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171  \n 172  \n 173  \n 174 -\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195  ",
            "  /**\n   * Unit tests that uses ZooKeeper but does not use the master-side methods\n   * but rather acts directly on ZK.\n   * @throws Exception\n   */\n  @Test\n  public void testActiveMasterManagerFromZK() throws Exception {\n    ZKWatcher zk = new ZKWatcher(TEST_UTIL.getConfiguration(),\n      \"testActiveMasterManagerFromZK\", null, true);\n    try {\n      ZKUtil.deleteNode(zk, zk.znodePaths.masterAddressZNode);\n      ZKUtil.deleteNode(zk, zk.znodePaths.clusterStateZNode);\n    } catch(KeeperException.NoNodeException nne) {}\n\n    // Create the master node with a dummy address\n    ServerName firstMasterAddress =\n        ServerName.valueOf(\"localhost\", 1, System.currentTimeMillis());\n    ServerName secondMasterAddress =\n        ServerName.valueOf(\"localhost\", 2, System.currentTimeMillis());\n\n    // Should not have a master yet\n    DummyMaster ms1 = new DummyMaster(zk,firstMasterAddress);\n    ActiveMasterManager activeMasterManager =\n      ms1.getActiveMasterManager();\n    assertFalse(activeMasterManager.clusterHasActiveMaster.get());\n\n    // First test becoming the active master uninterrupted\n    ClusterStatusTracker clusterStatusTracker =\n      ms1.getClusterStatusTracker();\n    clusterStatusTracker.setClusterUp();\n    activeMasterManager.blockUntilBecomingActiveMaster(100,\n        Mockito.mock(MonitoredTask.class));\n    assertTrue(activeMasterManager.clusterHasActiveMaster.get());\n    assertMaster(zk, firstMasterAddress);\n\n    // New manager will now try to become the active master in another thread\n    WaitToBeMasterThread t = new WaitToBeMasterThread(zk, secondMasterAddress);\n    t.start();\n    // Wait for this guy to figure out there is another active master\n    // Wait for 1 second at most\n    int sleeps = 0;\n    while(!t.manager.clusterHasActiveMaster.get() && sleeps < 100) {\n      Thread.sleep(10);\n      sleeps++;\n    }\n\n    // Both should see that there is an active master\n    assertTrue(activeMasterManager.clusterHasActiveMaster.get());\n    assertTrue(t.manager.clusterHasActiveMaster.get());\n    // But secondary one should not be the active master\n    assertFalse(t.isActiveMaster);\n\n    // Close the first server and delete it's master node\n    ms1.stop(\"stopping first server\");\n\n    // Use a listener to capture when the node is actually deleted\n    NodeDeletionListener listener = new NodeDeletionListener(zk, zk.znodePaths.masterAddressZNode);\n    zk.registerListener(listener);\n\n    LOG.info(\"Deleting master node\");\n    ZKUtil.deleteNode(zk, zk.znodePaths.masterAddressZNode);\n\n    // Wait for the node to be deleted\n    LOG.info(\"Waiting for active master manager to be notified\");\n    listener.waitForDeletion();\n    LOG.info(\"Master node deleted\");\n\n    // Now we expect the secondary manager to have and be the active master\n    // Wait for 1 second at most\n    sleeps = 0;\n    while(!t.isActiveMaster && sleeps < 100) {\n      Thread.sleep(10);\n      sleeps++;\n    }\n    LOG.debug(\"Slept \" + sleeps + \" times\");\n\n    assertTrue(t.manager.clusterHasActiveMaster.get());\n    assertTrue(t.isActiveMaster);\n\n    LOG.info(\"Deleting master node\");\n    ZKUtil.deleteNode(zk, zk.znodePaths.masterAddressZNode);\n  }",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 +\n 171 +\n 172  \n 173  \n 174  \n 175 +\n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195 +\n 196  ",
            "  /**\n   * Unit tests that uses ZooKeeper but does not use the master-side methods\n   * but rather acts directly on ZK.\n   * @throws Exception\n   */\n  @Test\n  public void testActiveMasterManagerFromZK() throws Exception {\n    ZKWatcher zk = new ZKWatcher(TEST_UTIL.getConfiguration(),\n      \"testActiveMasterManagerFromZK\", null, true);\n    try {\n      ZKUtil.deleteNode(zk, zk.getZNodePaths().masterAddressZNode);\n      ZKUtil.deleteNode(zk, zk.getZNodePaths().clusterStateZNode);\n    } catch(KeeperException.NoNodeException nne) {}\n\n    // Create the master node with a dummy address\n    ServerName firstMasterAddress =\n        ServerName.valueOf(\"localhost\", 1, System.currentTimeMillis());\n    ServerName secondMasterAddress =\n        ServerName.valueOf(\"localhost\", 2, System.currentTimeMillis());\n\n    // Should not have a master yet\n    DummyMaster ms1 = new DummyMaster(zk,firstMasterAddress);\n    ActiveMasterManager activeMasterManager =\n      ms1.getActiveMasterManager();\n    assertFalse(activeMasterManager.clusterHasActiveMaster.get());\n\n    // First test becoming the active master uninterrupted\n    ClusterStatusTracker clusterStatusTracker =\n      ms1.getClusterStatusTracker();\n    clusterStatusTracker.setClusterUp();\n    activeMasterManager.blockUntilBecomingActiveMaster(100,\n        Mockito.mock(MonitoredTask.class));\n    assertTrue(activeMasterManager.clusterHasActiveMaster.get());\n    assertMaster(zk, firstMasterAddress);\n\n    // New manager will now try to become the active master in another thread\n    WaitToBeMasterThread t = new WaitToBeMasterThread(zk, secondMasterAddress);\n    t.start();\n    // Wait for this guy to figure out there is another active master\n    // Wait for 1 second at most\n    int sleeps = 0;\n    while(!t.manager.clusterHasActiveMaster.get() && sleeps < 100) {\n      Thread.sleep(10);\n      sleeps++;\n    }\n\n    // Both should see that there is an active master\n    assertTrue(activeMasterManager.clusterHasActiveMaster.get());\n    assertTrue(t.manager.clusterHasActiveMaster.get());\n    // But secondary one should not be the active master\n    assertFalse(t.isActiveMaster);\n\n    // Close the first server and delete it's master node\n    ms1.stop(\"stopping first server\");\n\n    // Use a listener to capture when the node is actually deleted\n    NodeDeletionListener listener = new NodeDeletionListener(zk,\n            zk.getZNodePaths().masterAddressZNode);\n    zk.registerListener(listener);\n\n    LOG.info(\"Deleting master node\");\n    ZKUtil.deleteNode(zk, zk.getZNodePaths().masterAddressZNode);\n\n    // Wait for the node to be deleted\n    LOG.info(\"Waiting for active master manager to be notified\");\n    listener.waitForDeletion();\n    LOG.info(\"Master node deleted\");\n\n    // Now we expect the secondary manager to have and be the active master\n    // Wait for 1 second at most\n    sleeps = 0;\n    while(!t.isActiveMaster && sleeps < 100) {\n      Thread.sleep(10);\n      sleeps++;\n    }\n    LOG.debug(\"Slept \" + sleeps + \" times\");\n\n    assertTrue(t.manager.clusterHasActiveMaster.get());\n    assertTrue(t.isActiveMaster);\n\n    LOG.info(\"Deleting master node\");\n    ZKUtil.deleteNode(zk, zk.getZNodePaths().masterAddressZNode);\n  }"
        ],
        [
            "HRegionServer::getMyEphemeralNodePath()",
            "3509  \n3510 -\n3511  ",
            "  private String getMyEphemeralNodePath() {\n    return ZNodePaths.joinZNode(this.zooKeeper.znodePaths.rsZNode, getServerName().toString());\n  }",
            "3509  \n3510 +\n3511  ",
            "  private String getMyEphemeralNodePath() {\n    return ZNodePaths.joinZNode(this.zooKeeper.getZNodePaths().rsZNode, getServerName().toString());\n  }"
        ],
        [
            "RSGroupInfoManagerImpl::retrieveGroupListFromZookeeper()",
            " 349  \n 350 -\n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  ",
            "  List<RSGroupInfo> retrieveGroupListFromZookeeper() throws IOException {\n    String groupBasePath = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, rsGroupZNode);\n    List<RSGroupInfo> RSGroupInfoList = Lists.newArrayList();\n    //Overwrite any info stored by table, this takes precedence\n    try {\n      if(ZKUtil.checkExists(watcher, groupBasePath) != -1) {\n        List<String> children = ZKUtil.listChildrenAndWatchForNewChildren(watcher, groupBasePath);\n        if (children == null) {\n          return RSGroupInfoList;\n        }\n        for(String znode: children) {\n          byte[] data = ZKUtil.getData(watcher, ZNodePaths.joinZNode(groupBasePath, znode));\n          if(data.length > 0) {\n            ProtobufUtil.expectPBMagicPrefix(data);\n            ByteArrayInputStream bis = new ByteArrayInputStream(\n                data, ProtobufUtil.lengthOfPBMagic(), data.length);\n            RSGroupInfoList.add(RSGroupProtobufUtil.toGroupInfo(\n                RSGroupProtos.RSGroupInfo.parseFrom(bis)));\n          }\n        }\n        LOG.debug(\"Read ZK GroupInfo count:\" + RSGroupInfoList.size());\n      }\n    } catch (KeeperException|DeserializationException|InterruptedException e) {\n      throw new IOException(\"Failed to read rsGroupZNode\",e);\n    }\n    return RSGroupInfoList;\n  }",
            " 349  \n 350 +\n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  ",
            "  List<RSGroupInfo> retrieveGroupListFromZookeeper() throws IOException {\n    String groupBasePath = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, rsGroupZNode);\n    List<RSGroupInfo> RSGroupInfoList = Lists.newArrayList();\n    //Overwrite any info stored by table, this takes precedence\n    try {\n      if(ZKUtil.checkExists(watcher, groupBasePath) != -1) {\n        List<String> children = ZKUtil.listChildrenAndWatchForNewChildren(watcher, groupBasePath);\n        if (children == null) {\n          return RSGroupInfoList;\n        }\n        for(String znode: children) {\n          byte[] data = ZKUtil.getData(watcher, ZNodePaths.joinZNode(groupBasePath, znode));\n          if(data.length > 0) {\n            ProtobufUtil.expectPBMagicPrefix(data);\n            ByteArrayInputStream bis = new ByteArrayInputStream(\n                data, ProtobufUtil.lengthOfPBMagic(), data.length);\n            RSGroupInfoList.add(RSGroupProtobufUtil.toGroupInfo(\n                RSGroupProtos.RSGroupInfo.parseFrom(bis)));\n          }\n        }\n        LOG.debug(\"Read ZK GroupInfo count:\" + RSGroupInfoList.size());\n      }\n    } catch (KeeperException|DeserializationException|InterruptedException e) {\n      throw new IOException(\"Failed to read rsGroupZNode\",e);\n    }\n    return RSGroupInfoList;\n  }"
        ],
        [
            "ClusterStatusTracker::setClusterUp()",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68 -\n  69  \n  70  \n  71 -\n  72  \n  73 -\n  74  \n  75  ",
            "  /**\n   * Sets the cluster as up.\n   * @throws KeeperException unexpected zk exception\n   */\n  public void setClusterUp()\n  throws KeeperException {\n    byte [] upData = toByteArray();\n    try {\n      ZKUtil.createAndWatch(watcher, watcher.znodePaths.clusterStateZNode, upData);\n    } catch(KeeperException.NodeExistsException nee) {\n      ZKUtil.setData(watcher, watcher.znodePaths.clusterStateZNode, upData);\n    }\n  }",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69  \n  70  \n  71 +\n  72  \n  73 +\n  74  \n  75  ",
            "  /**\n   * Sets the cluster as up.\n   * @throws KeeperException unexpected zk exception\n   */\n  public void setClusterUp()\n    throws KeeperException {\n    byte [] upData = toByteArray();\n    try {\n      ZKUtil.createAndWatch(watcher, watcher.getZNodePaths().clusterStateZNode, upData);\n    } catch(KeeperException.NodeExistsException nee) {\n      ZKUtil.setData(watcher, watcher.getZNodePaths().clusterStateZNode, upData);\n    }\n  }"
        ],
        [
            "TestActiveMasterManager::testRestartMaster()",
            "  78  \n  79  \n  80  \n  81  \n  82 -\n  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "  @Test public void testRestartMaster() throws IOException, KeeperException {\n    ZKWatcher zk = new ZKWatcher(TEST_UTIL.getConfiguration(),\n      \"testActiveMasterManagerFromZK\", null, true);\n    try {\n      ZKUtil.deleteNode(zk, zk.znodePaths.masterAddressZNode);\n      ZKUtil.deleteNode(zk, zk.znodePaths.clusterStateZNode);\n    } catch(KeeperException.NoNodeException nne) {}\n\n    // Create the master node with a dummy address\n    ServerName master = ServerName.valueOf(\"localhost\", 1, System.currentTimeMillis());\n    // Should not have a master yet\n    DummyMaster dummyMaster = new DummyMaster(zk,master);\n    ClusterStatusTracker clusterStatusTracker =\n      dummyMaster.getClusterStatusTracker();\n    ActiveMasterManager activeMasterManager =\n      dummyMaster.getActiveMasterManager();\n    assertFalse(activeMasterManager.clusterHasActiveMaster.get());\n\n    // First test becoming the active master uninterrupted\n    MonitoredTask status = Mockito.mock(MonitoredTask.class);\n    clusterStatusTracker.setClusterUp();\n\n    activeMasterManager.blockUntilBecomingActiveMaster(100, status);\n    assertTrue(activeMasterManager.clusterHasActiveMaster.get());\n    assertMaster(zk, master);\n\n    // Now pretend master restart\n    DummyMaster secondDummyMaster = new DummyMaster(zk,master);\n    ActiveMasterManager secondActiveMasterManager =\n      secondDummyMaster.getActiveMasterManager();\n    assertFalse(secondActiveMasterManager.clusterHasActiveMaster.get());\n    activeMasterManager.blockUntilBecomingActiveMaster(100, status);\n    assertTrue(activeMasterManager.clusterHasActiveMaster.get());\n    assertMaster(zk, master);\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82 +\n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "  @Test public void testRestartMaster() throws IOException, KeeperException {\n    ZKWatcher zk = new ZKWatcher(TEST_UTIL.getConfiguration(),\n      \"testActiveMasterManagerFromZK\", null, true);\n    try {\n      ZKUtil.deleteNode(zk, zk.getZNodePaths().masterAddressZNode);\n      ZKUtil.deleteNode(zk, zk.getZNodePaths().clusterStateZNode);\n    } catch(KeeperException.NoNodeException nne) {}\n\n    // Create the master node with a dummy address\n    ServerName master = ServerName.valueOf(\"localhost\", 1, System.currentTimeMillis());\n    // Should not have a master yet\n    DummyMaster dummyMaster = new DummyMaster(zk,master);\n    ClusterStatusTracker clusterStatusTracker =\n      dummyMaster.getClusterStatusTracker();\n    ActiveMasterManager activeMasterManager =\n      dummyMaster.getActiveMasterManager();\n    assertFalse(activeMasterManager.clusterHasActiveMaster.get());\n\n    // First test becoming the active master uninterrupted\n    MonitoredTask status = Mockito.mock(MonitoredTask.class);\n    clusterStatusTracker.setClusterUp();\n\n    activeMasterManager.blockUntilBecomingActiveMaster(100, status);\n    assertTrue(activeMasterManager.clusterHasActiveMaster.get());\n    assertMaster(zk, master);\n\n    // Now pretend master restart\n    DummyMaster secondDummyMaster = new DummyMaster(zk,master);\n    ActiveMasterManager secondActiveMasterManager =\n      secondDummyMaster.getActiveMasterManager();\n    assertFalse(secondActiveMasterManager.clusterHasActiveMaster.get());\n    activeMasterManager.blockUntilBecomingActiveMaster(100, status);\n    assertTrue(activeMasterManager.clusterHasActiveMaster.get());\n    assertMaster(zk, master);\n  }"
        ],
        [
            "DrainingServerTracker::start()",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 -\n  87  \n  88  ",
            "  /**\n   * Starts the tracking of draining RegionServers.\n   *\n   * <p>All Draining RSs will be tracked after this method is called.\n   *\n   * @throws KeeperException\n   */\n  public void start() throws KeeperException, IOException {\n    watcher.registerListener(this);\n    // Add a ServerListener to check if a server is draining when it's added.\n    serverManager.registerListener(new ServerListener() {\n      @Override\n      public void serverAdded(ServerName sn) {\n        if (drainingServers.contains(sn)){\n          serverManager.addServerToDrainList(sn);\n        }\n      }\n    });\n    List<String> servers =\n      ZKUtil.listChildrenAndWatchThem(watcher, watcher.znodePaths.drainingZNode);\n    add(servers);\n  }",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 +\n  87  \n  88  ",
            "  /**\n   * Starts the tracking of draining RegionServers.\n   *\n   * <p>All Draining RSs will be tracked after this method is called.\n   *\n   * @throws KeeperException\n   */\n  public void start() throws KeeperException, IOException {\n    watcher.registerListener(this);\n    // Add a ServerListener to check if a server is draining when it's added.\n    serverManager.registerListener(new ServerListener() {\n      @Override\n      public void serverAdded(ServerName sn) {\n        if (drainingServers.contains(sn)){\n          serverManager.addServerToDrainList(sn);\n        }\n      }\n    });\n    List<String> servers =\n      ZKUtil.listChildrenAndWatchThem(watcher, watcher.getZNodePaths().drainingZNode);\n    add(servers);\n  }"
        ],
        [
            "TestZKMulti::testComplexMulti()",
            " 113  \n 114  \n 115 -\n 116 -\n 117 -\n 118 -\n 119 -\n 120 -\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "  @Test\n  public void testComplexMulti() throws Exception {\n    String path1 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testComplexMulti1\");\n    String path2 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testComplexMulti2\");\n    String path3 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testComplexMulti3\");\n    String path4 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testComplexMulti4\");\n    String path5 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testComplexMulti5\");\n    String path6 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testComplexMulti6\");\n    // create 4 nodes that we'll setData on or delete later\n    LinkedList<ZKUtilOp> create4Nodes = new LinkedList<>();\n    create4Nodes.add(ZKUtilOp.createAndFailSilent(path1, Bytes.toBytes(path1)));\n    create4Nodes.add(ZKUtilOp.createAndFailSilent(path2, Bytes.toBytes(path2)));\n    create4Nodes.add(ZKUtilOp.createAndFailSilent(path3, Bytes.toBytes(path3)));\n    create4Nodes.add(ZKUtilOp.createAndFailSilent(path4, Bytes.toBytes(path4)));\n    ZKUtil.multiOrSequential(zkw, create4Nodes, false);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path1), Bytes.toBytes(path1)));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path2), Bytes.toBytes(path2)));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path3), Bytes.toBytes(path3)));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path4), Bytes.toBytes(path4)));\n\n    // do multiple of each operation (setData, delete, create)\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    // setData\n    ops.add(ZKUtilOp.setData(path1, Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1))));\n    ops.add(ZKUtilOp.setData(path2, Bytes.add(Bytes.toBytes(path2), Bytes.toBytes(path2))));\n    // delete\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path3));\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path4));\n    // create\n    ops.add(ZKUtilOp.createAndFailSilent(path5, Bytes.toBytes(path5)));\n    ops.add(ZKUtilOp.createAndFailSilent(path6, Bytes.toBytes(path6)));\n    ZKUtil.multiOrSequential(zkw, ops, false);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path1),\n      Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1))));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path2),\n      Bytes.add(Bytes.toBytes(path2), Bytes.toBytes(path2))));\n    assertTrue(ZKUtil.checkExists(zkw, path3) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, path4) == -1);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path5), Bytes.toBytes(path5)));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path6), Bytes.toBytes(path6)));\n  }",
            " 113  \n 114  \n 115 +\n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "  @Test\n  public void testComplexMulti() throws Exception {\n    String path1 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testComplexMulti1\");\n    String path2 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testComplexMulti2\");\n    String path3 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testComplexMulti3\");\n    String path4 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testComplexMulti4\");\n    String path5 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testComplexMulti5\");\n    String path6 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testComplexMulti6\");\n    // create 4 nodes that we'll setData on or delete later\n    LinkedList<ZKUtilOp> create4Nodes = new LinkedList<>();\n    create4Nodes.add(ZKUtilOp.createAndFailSilent(path1, Bytes.toBytes(path1)));\n    create4Nodes.add(ZKUtilOp.createAndFailSilent(path2, Bytes.toBytes(path2)));\n    create4Nodes.add(ZKUtilOp.createAndFailSilent(path3, Bytes.toBytes(path3)));\n    create4Nodes.add(ZKUtilOp.createAndFailSilent(path4, Bytes.toBytes(path4)));\n    ZKUtil.multiOrSequential(zkw, create4Nodes, false);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path1), Bytes.toBytes(path1)));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path2), Bytes.toBytes(path2)));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path3), Bytes.toBytes(path3)));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path4), Bytes.toBytes(path4)));\n\n    // do multiple of each operation (setData, delete, create)\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    // setData\n    ops.add(ZKUtilOp.setData(path1, Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1))));\n    ops.add(ZKUtilOp.setData(path2, Bytes.add(Bytes.toBytes(path2), Bytes.toBytes(path2))));\n    // delete\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path3));\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path4));\n    // create\n    ops.add(ZKUtilOp.createAndFailSilent(path5, Bytes.toBytes(path5)));\n    ops.add(ZKUtilOp.createAndFailSilent(path6, Bytes.toBytes(path6)));\n    ZKUtil.multiOrSequential(zkw, ops, false);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path1),\n      Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1))));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path2),\n      Bytes.add(Bytes.toBytes(path2), Bytes.toBytes(path2))));\n    assertTrue(ZKUtil.checkExists(zkw, path3) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, path4) == -1);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path5), Bytes.toBytes(path5)));\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path6), Bytes.toBytes(path6)));\n  }"
        ],
        [
            "TestZKNodeTracker::testCleanZNode()",
            " 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 -\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  ",
            "  @Test\n  public void testCleanZNode() throws Exception {\n    ZKWatcher zkw = new ZKWatcher(TEST_UTIL.getConfiguration(),\n        \"testNodeTracker\", new TestZKNodeTracker.StubAbortable());\n\n    final ServerName sn = ServerName.valueOf(\"127.0.0.1:52\", 45L);\n\n    ZKUtil.createAndFailSilent(zkw,\n        TEST_UTIL.getConfiguration().get(HConstants.ZOOKEEPER_ZNODE_PARENT,\n            HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));\n\n    final String nodeName =  zkw.znodePaths.masterAddressZNode;\n\n    // Check that we manage the case when there is no data\n    ZKUtil.createAndFailSilent(zkw, nodeName);\n    MasterAddressTracker.deleteIfEquals(zkw, sn.toString());\n    assertNotNull(ZKUtil.getData(zkw, nodeName));\n\n    // Check that we don't delete if we're not supposed to\n    ZKUtil.setData(zkw, nodeName, MasterAddressTracker.toByteArray(sn, 0));\n    MasterAddressTracker.deleteIfEquals(zkw, ServerName.valueOf(\"127.0.0.2:52\", 45L).toString());\n    assertNotNull(ZKUtil.getData(zkw, nodeName));\n\n    // Check that we delete when we're supposed to\n    ZKUtil.setData(zkw, nodeName,MasterAddressTracker.toByteArray(sn, 0));\n    MasterAddressTracker.deleteIfEquals(zkw, sn.toString());\n    assertNull(ZKUtil.getData(zkw, nodeName));\n\n    // Check that we support the case when the znode does not exist\n    MasterAddressTracker.deleteIfEquals(zkw, sn.toString()); // must not throw an exception\n  }",
            " 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 +\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  ",
            "  @Test\n  public void testCleanZNode() throws Exception {\n    ZKWatcher zkw = new ZKWatcher(TEST_UTIL.getConfiguration(),\n        \"testNodeTracker\", new TestZKNodeTracker.StubAbortable());\n\n    final ServerName sn = ServerName.valueOf(\"127.0.0.1:52\", 45L);\n\n    ZKUtil.createAndFailSilent(zkw,\n        TEST_UTIL.getConfiguration().get(HConstants.ZOOKEEPER_ZNODE_PARENT,\n            HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT));\n\n    final String nodeName =  zkw.getZNodePaths().masterAddressZNode;\n\n    // Check that we manage the case when there is no data\n    ZKUtil.createAndFailSilent(zkw, nodeName);\n    MasterAddressTracker.deleteIfEquals(zkw, sn.toString());\n    assertNotNull(ZKUtil.getData(zkw, nodeName));\n\n    // Check that we don't delete if we're not supposed to\n    ZKUtil.setData(zkw, nodeName, MasterAddressTracker.toByteArray(sn, 0));\n    MasterAddressTracker.deleteIfEquals(zkw, ServerName.valueOf(\"127.0.0.2:52\", 45L).toString());\n    assertNotNull(ZKUtil.getData(zkw, nodeName));\n\n    // Check that we delete when we're supposed to\n    ZKUtil.setData(zkw, nodeName,MasterAddressTracker.toByteArray(sn, 0));\n    MasterAddressTracker.deleteIfEquals(zkw, sn.toString());\n    assertNull(ZKUtil.getData(zkw, nodeName));\n\n    // Check that we support the case when the znode does not exist\n    MasterAddressTracker.deleteIfEquals(zkw, sn.toString()); // must not throw an exception\n  }"
        ],
        [
            "HBaseFsck::setMasterInMaintenanceMode()",
            " 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725 -\n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  ",
            "  /**\n   * This method maintains an ephemeral znode. If the creation fails we return false or throw\n   * exception\n   *\n   * @return true if creating znode succeeds; false otherwise\n   * @throws IOException if IO failure occurs\n   */\n  private boolean setMasterInMaintenanceMode() throws IOException {\n    RetryCounter retryCounter = createZNodeRetryCounterFactory.create();\n    hbckEphemeralNodePath = ZNodePaths.joinZNode(\n      zkw.znodePaths.masterMaintZNode,\n      \"hbck-\" + Long.toString(EnvironmentEdgeManager.currentTime()));\n    do {\n      try {\n        hbckZodeCreated = ZKUtil.createEphemeralNodeAndWatch(zkw, hbckEphemeralNodePath, null);\n        if (hbckZodeCreated) {\n          break;\n        }\n      } catch (KeeperException e) {\n        if (retryCounter.getAttemptTimes() >= retryCounter.getMaxAttempts()) {\n           throw new IOException(\"Can't create znode \" + hbckEphemeralNodePath, e);\n        }\n        // fall through and retry\n      }\n\n      LOG.warn(\"Fail to create znode \" + hbckEphemeralNodePath + \", try=\" +\n          (retryCounter.getAttemptTimes() + 1) + \" of \" + retryCounter.getMaxAttempts());\n\n      try {\n        retryCounter.sleepUntilNextRetry();\n      } catch (InterruptedException ie) {\n        throw (InterruptedIOException) new InterruptedIOException(\n              \"Can't create znode \" + hbckEphemeralNodePath).initCause(ie);\n      }\n    } while (retryCounter.shouldRetry());\n    return hbckZodeCreated;\n  }",
            " 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725 +\n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  ",
            "  /**\n   * This method maintains an ephemeral znode. If the creation fails we return false or throw\n   * exception\n   *\n   * @return true if creating znode succeeds; false otherwise\n   * @throws IOException if IO failure occurs\n   */\n  private boolean setMasterInMaintenanceMode() throws IOException {\n    RetryCounter retryCounter = createZNodeRetryCounterFactory.create();\n    hbckEphemeralNodePath = ZNodePaths.joinZNode(\n      zkw.getZNodePaths().masterMaintZNode,\n      \"hbck-\" + Long.toString(EnvironmentEdgeManager.currentTime()));\n    do {\n      try {\n        hbckZodeCreated = ZKUtil.createEphemeralNodeAndWatch(zkw, hbckEphemeralNodePath, null);\n        if (hbckZodeCreated) {\n          break;\n        }\n      } catch (KeeperException e) {\n        if (retryCounter.getAttemptTimes() >= retryCounter.getMaxAttempts()) {\n           throw new IOException(\"Can't create znode \" + hbckEphemeralNodePath, e);\n        }\n        // fall through and retry\n      }\n\n      LOG.warn(\"Fail to create znode \" + hbckEphemeralNodePath + \", try=\" +\n          (retryCounter.getAttemptTimes() + 1) + \" of \" + retryCounter.getMaxAttempts());\n\n      try {\n        retryCounter.sleepUntilNextRetry();\n      } catch (InterruptedException ie) {\n        throw (InterruptedIOException) new InterruptedIOException(\n              \"Can't create znode \" + hbckEphemeralNodePath).initCause(ie);\n      }\n    } while (retryCounter.shouldRetry());\n    return hbckZodeCreated;\n  }"
        ],
        [
            "ZKDataMigrator::getTableState(ZKWatcher,TableName)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "  /**\n   * Gets table state from ZK.\n   * @param zkw ZKWatcher instance to use\n   * @param tableName table we're checking\n   * @return Null or {@link ZooKeeperProtos.DeprecatedTableState.State} found in znode.\n   * @throws KeeperException\n   * @deprecated Since 2.0.0. To be removed in hbase-3.0.0.\n   */\n  @Deprecated\n  private static  ZooKeeperProtos.DeprecatedTableState.State getTableState(\n          final ZKWatcher zkw, final TableName tableName)\n      throws KeeperException, InterruptedException {\n    String znode = ZNodePaths.joinZNode(zkw.znodePaths.tableZNode, tableName.getNameAsString());\n    byte [] data = ZKUtil.getData(zkw, znode);\n    if (data == null || data.length <= 0) return null;\n    try {\n      ProtobufUtil.expectPBMagicPrefix(data);\n      ZooKeeperProtos.DeprecatedTableState.Builder builder =\n          ZooKeeperProtos.DeprecatedTableState.newBuilder();\n      int magicLen = ProtobufUtil.lengthOfPBMagic();\n      ProtobufUtil.mergeFrom(builder, data, magicLen, data.length - magicLen);\n      return builder.getState();\n    } catch (IOException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    } catch (DeserializationException e) {\n      throw ZKUtil.convert(e);\n    }\n  }",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105 +\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  ",
            "  /**\n   * Gets table state from ZK.\n   * @param zkw ZKWatcher instance to use\n   * @param tableName table we're checking\n   * @return Null or {@link ZooKeeperProtos.DeprecatedTableState.State} found in znode.\n   * @throws KeeperException\n   * @deprecated Since 2.0.0. To be removed in hbase-3.0.0.\n   */\n  @Deprecated\n  private static  ZooKeeperProtos.DeprecatedTableState.State getTableState(\n          final ZKWatcher zkw, final TableName tableName)\n      throws KeeperException, InterruptedException {\n    String znode = ZNodePaths.joinZNode(zkw.getZNodePaths().tableZNode,\n            tableName.getNameAsString());\n    byte [] data = ZKUtil.getData(zkw, znode);\n    if (data == null || data.length <= 0) return null;\n    try {\n      ProtobufUtil.expectPBMagicPrefix(data);\n      ZooKeeperProtos.DeprecatedTableState.Builder builder =\n          ZooKeeperProtos.DeprecatedTableState.newBuilder();\n      int magicLen = ProtobufUtil.lengthOfPBMagic();\n      ProtobufUtil.mergeFrom(builder, data, magicLen, data.length - magicLen);\n      return builder.getState();\n    } catch (IOException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    } catch (DeserializationException e) {\n      throw ZKUtil.convert(e);\n    }\n  }"
        ],
        [
            "ZKClusterId::setClusterId(ZKWatcher,ClusterId)",
            "  84  \n  85  \n  86 -\n  87  ",
            "  public static void setClusterId(ZKWatcher watcher, ClusterId id)\n      throws KeeperException {\n    ZKUtil.createSetData(watcher, watcher.znodePaths.clusterIdZNode, id.toByteArray());\n  }",
            "  84  \n  85  \n  86 +\n  87  ",
            "  public static void setClusterId(ZKWatcher watcher, ClusterId id)\n      throws KeeperException {\n    ZKUtil.createSetData(watcher, watcher.getZNodePaths().clusterIdZNode, id.toByteArray());\n  }"
        ],
        [
            "MasterMaintenanceModeTracker::update(String)",
            "  42  \n  43 -\n  44  \n  45  \n  46  ",
            "  private void update(String path) {\n    if (path.startsWith(watcher.znodePaths.masterMaintZNode)) {\n      update();\n    }\n  }",
            "  42  \n  43 +\n  44  \n  45  \n  46  ",
            "  private void update(String path) {\n    if (path.startsWith(watcher.getZNodePaths().masterMaintZNode)) {\n      update();\n    }\n  }"
        ],
        [
            "ZKUtil::getReplicationZnodesDump(ZKWatcher,StringBuilder)",
            "1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802 -\n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813 -\n1814  \n1815 -\n1816  \n1817 -\n1818  \n1819  \n1820  \n1821  ",
            "  /**\n   * Appends replication znodes to the passed StringBuilder.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param sb the {@link StringBuilder} to append to\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  private static void getReplicationZnodesDump(ZKWatcher zkw, StringBuilder sb)\n      throws KeeperException {\n    String replicationZnode = zkw.znodePaths.replicationZNode;\n\n    if (ZKUtil.checkExists(zkw, replicationZnode) == -1) {\n      return;\n    }\n\n    // do a ls -r on this znode\n    sb.append(\"\\n\").append(replicationZnode).append(\": \");\n    List<String> children = ZKUtil.listChildrenNoWatch(zkw, replicationZnode);\n    for (String child : children) {\n      String znode = ZNodePaths.joinZNode(replicationZnode, child);\n      if (znode.equals(zkw.znodePaths.peersZNode)) {\n        appendPeersZnodes(zkw, znode, sb);\n      } else if (znode.equals(zkw.znodePaths.queuesZNode)) {\n        appendRSZnodes(zkw, znode, sb);\n      } else if (znode.equals(zkw.znodePaths.hfileRefsZNode)) {\n        appendHFileRefsZnodes(zkw, znode, sb);\n      }\n    }\n  }",
            "1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803 +\n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814 +\n1815  \n1816 +\n1817  \n1818 +\n1819  \n1820  \n1821  \n1822  ",
            "  /**\n   * Appends replication znodes to the passed StringBuilder.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param sb the {@link StringBuilder} to append to\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  private static void getReplicationZnodesDump(ZKWatcher zkw, StringBuilder sb)\n      throws KeeperException {\n    String replicationZnode = zkw.getZNodePaths().replicationZNode;\n\n    if (ZKUtil.checkExists(zkw, replicationZnode) == -1) {\n      return;\n    }\n\n    // do a ls -r on this znode\n    sb.append(\"\\n\").append(replicationZnode).append(\": \");\n    List<String> children = ZKUtil.listChildrenNoWatch(zkw, replicationZnode);\n    for (String child : children) {\n      String znode = ZNodePaths.joinZNode(replicationZnode, child);\n      if (znode.equals(zkw.getZNodePaths().peersZNode)) {\n        appendPeersZnodes(zkw, znode, sb);\n      } else if (znode.equals(zkw.getZNodePaths().queuesZNode)) {\n        appendRSZnodes(zkw, znode, sb);\n      } else if (znode.equals(zkw.getZNodePaths().hfileRefsZNode)) {\n        appendHFileRefsZnodes(zkw, znode, sb);\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::nodeHasChildren(ZKWatcher,String)",
            " 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553 -\n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  ",
            "  /**\n   * Checks if the specified znode has any children.  Sets no watches.\n   *\n   * Returns true if the node exists and has children.  Returns false if the\n   * node does not exist or if the node does not have any children.\n   *\n   * Used during master initialization to determine if the master is a\n   * failed-over-to master or the first master during initial cluster startup.\n   * If the directory for regionserver ephemeral nodes is empty then this is\n   * a cluster startup, if not then it is not cluster startup.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to check for children of\n   * @return true if node has children, false if not or node does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean nodeHasChildren(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    try {\n      return !zkw.getRecoverableZooKeeper().getChildren(znode, null).isEmpty();\n    } catch(KeeperException.NoNodeException ke) {\n      LOG.debug(zkw.prefix(\"Unable to list children of znode \" + znode +\n              \" because node does not exist (not an error)\"));\n      return false;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode), e);\n      zkw.keeperException(e);\n      return false;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode), e);\n      zkw.interruptedException(e);\n      return false;\n    }\n  }",
            " 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553 +\n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  ",
            "  /**\n   * Checks if the specified znode has any children.  Sets no watches.\n   *\n   * Returns true if the node exists and has children.  Returns false if the\n   * node does not exist or if the node does not have any children.\n   *\n   * Used during master initialization to determine if the master is a\n   * failed-over-to master or the first master during initial cluster startup.\n   * If the directory for regionserver ephemeral nodes is empty then this is\n   * a cluster startup, if not then it is not cluster startup.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to check for children of\n   * @return true if node has children, false if not or node does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean nodeHasChildren(ZKWatcher zkw, String znode)\n    throws KeeperException {\n    try {\n      return !zkw.getRecoverableZooKeeper().getChildren(znode, null).isEmpty();\n    } catch(KeeperException.NoNodeException ke) {\n      LOG.debug(zkw.prefix(\"Unable to list children of znode \" + znode +\n              \" because node does not exist (not an error)\"));\n      return false;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode), e);\n      zkw.keeperException(e);\n      return false;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode), e);\n      zkw.interruptedException(e);\n      return false;\n    }\n  }"
        ],
        [
            "MetaTableLocator::waitMetaRegionLocation(ZKWatcher,long)",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 -\n 195  \n 196  ",
            "  /**\n   * Gets the meta region location, if available, and waits for up to the\n   * specified timeout if not immediately available.\n   * Given the zookeeper notification could be delayed, we will try to\n   * get the latest data.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param timeout maximum time to wait, in millis\n   * @return server name for server hosting meta region formatted as per\n   * {@link ServerName}, or null if none available\n   * @throws InterruptedException if interrupted while waiting\n   * @throws NotAllMetaRegionsOnlineException if a meta or root region is not online\n   */\n  public ServerName waitMetaRegionLocation(ZKWatcher zkw, long timeout)\n  throws InterruptedException, NotAllMetaRegionsOnlineException {\n    return waitMetaRegionLocation(zkw, RegionInfo.DEFAULT_REPLICA_ID, timeout);\n  }",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194 +\n 195  \n 196  ",
            "  /**\n   * Gets the meta region location, if available, and waits for up to the\n   * specified timeout if not immediately available.\n   * Given the zookeeper notification could be delayed, we will try to\n   * get the latest data.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param timeout maximum time to wait, in millis\n   * @return server name for server hosting meta region formatted as per\n   * {@link ServerName}, or null if none available\n   * @throws InterruptedException if interrupted while waiting\n   * @throws NotAllMetaRegionsOnlineException if a meta or root region is not online\n   */\n  public ServerName waitMetaRegionLocation(ZKWatcher zkw, long timeout)\n    throws InterruptedException, NotAllMetaRegionsOnlineException {\n    return waitMetaRegionLocation(zkw, RegionInfo.DEFAULT_REPLICA_ID, timeout);\n  }"
        ],
        [
            "TestRegionServerHostname::testRegionServerHostname()",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  ",
            "  @Test\n  public void testRegionServerHostname() throws Exception {\n    Enumeration<NetworkInterface> netInterfaceList = NetworkInterface.getNetworkInterfaces();\n    while (netInterfaceList.hasMoreElements()) {\n      NetworkInterface ni = netInterfaceList.nextElement();\n      Enumeration<InetAddress> addrList = ni.getInetAddresses();\n      // iterate through host addresses and use each as hostname\n      while (addrList.hasMoreElements()) {\n        InetAddress addr = addrList.nextElement();\n        if (addr.isLoopbackAddress() || addr.isLinkLocalAddress() || addr.isMulticastAddress()) {\n          continue;\n        }\n        String hostName = addr.getHostName();\n        LOG.info(\"Found \" + hostName + \" on \" + ni);\n\n        TEST_UTIL.getConfiguration().set(HRegionServer.MASTER_HOSTNAME_KEY, hostName);\n        TEST_UTIL.getConfiguration().set(HRegionServer.RS_HOSTNAME_KEY, hostName);\n        TEST_UTIL.startMiniCluster(NUM_MASTERS, NUM_RS);\n        try {\n          ZKWatcher zkw = TEST_UTIL.getZooKeeperWatcher();\n          List<String> servers = ZKUtil.listChildrenNoWatch(zkw, zkw.znodePaths.rsZNode);\n          // there would be NUM_RS+1 children - one for the master\n          assertTrue(servers.size() ==\n            NUM_RS + (LoadBalancer.isTablesOnMaster(TEST_UTIL.getConfiguration())? 1: 0));\n          for (String server : servers) {\n            assertTrue(\"From zookeeper: \" + server + \" hostname: \" + hostName,\n              server.startsWith(hostName.toLowerCase(Locale.ROOT)+\",\"));\n          }\n          zkw.close();\n        } finally {\n          TEST_UTIL.shutdownMiniCluster();\n        }\n      }\n    }\n  }",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  ",
            "  @Test\n  public void testRegionServerHostname() throws Exception {\n    Enumeration<NetworkInterface> netInterfaceList = NetworkInterface.getNetworkInterfaces();\n    while (netInterfaceList.hasMoreElements()) {\n      NetworkInterface ni = netInterfaceList.nextElement();\n      Enumeration<InetAddress> addrList = ni.getInetAddresses();\n      // iterate through host addresses and use each as hostname\n      while (addrList.hasMoreElements()) {\n        InetAddress addr = addrList.nextElement();\n        if (addr.isLoopbackAddress() || addr.isLinkLocalAddress() || addr.isMulticastAddress()) {\n          continue;\n        }\n        String hostName = addr.getHostName();\n        LOG.info(\"Found \" + hostName + \" on \" + ni);\n\n        TEST_UTIL.getConfiguration().set(HRegionServer.MASTER_HOSTNAME_KEY, hostName);\n        TEST_UTIL.getConfiguration().set(HRegionServer.RS_HOSTNAME_KEY, hostName);\n        TEST_UTIL.startMiniCluster(NUM_MASTERS, NUM_RS);\n        try {\n          ZKWatcher zkw = TEST_UTIL.getZooKeeperWatcher();\n          List<String> servers = ZKUtil.listChildrenNoWatch(zkw, zkw.getZNodePaths().rsZNode);\n          // there would be NUM_RS+1 children - one for the master\n          assertTrue(servers.size() ==\n            NUM_RS + (LoadBalancer.isTablesOnMaster(TEST_UTIL.getConfiguration())? 1: 0));\n          for (String server : servers) {\n            assertTrue(\"From zookeeper: \" + server + \" hostname: \" + hostName,\n              server.startsWith(hostName.toLowerCase(Locale.ROOT)+\",\"));\n          }\n          zkw.close();\n        } finally {\n          TEST_UTIL.shutdownMiniCluster();\n        }\n      }\n    }\n  }"
        ],
        [
            "HBaseFsck::unassignMetaReplica(HbckInfo)",
            "3672  \n3673  \n3674  \n3675 -\n3676  ",
            "  private void unassignMetaReplica(HbckInfo hi) throws IOException, InterruptedException,\n  KeeperException {\n    undeployRegions(hi);\n    ZKUtil.deleteNode(zkw, zkw.znodePaths.getZNodeForReplica(hi.metaEntry.getReplicaId()));\n  }",
            "3672  \n3673  \n3674  \n3675 +\n3676  ",
            "  private void unassignMetaReplica(HbckInfo hi) throws IOException, InterruptedException,\n  KeeperException {\n    undeployRegions(hi);\n    ZKUtil.deleteNode(zkw, zkw.getZNodePaths().getZNodeForReplica(hi.metaEntry.getReplicaId()));\n  }"
        ],
        [
            "TestZKMulti::testSimpleMulti()",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "  @Test\n  public void testSimpleMulti() throws Exception {\n    // null multi\n    ZKUtil.multiOrSequential(zkw, null, false);\n\n    // empty multi\n    ZKUtil.multiOrSequential(zkw, new LinkedList<>(), false);\n\n    // single create\n    String path = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testSimpleMulti\");\n    LinkedList<ZKUtilOp> singleCreate = new LinkedList<>();\n    singleCreate.add(ZKUtilOp.createAndFailSilent(path, new byte[0]));\n    ZKUtil.multiOrSequential(zkw, singleCreate, false);\n    assertTrue(ZKUtil.checkExists(zkw, path) != -1);\n\n    // single setdata\n    LinkedList<ZKUtilOp> singleSetData = new LinkedList<>();\n    byte [] data = Bytes.toBytes(\"foobar\");\n    singleSetData.add(ZKUtilOp.setData(path, data));\n    ZKUtil.multiOrSequential(zkw, singleSetData, false);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path), data));\n\n    // single delete\n    LinkedList<ZKUtilOp> singleDelete = new LinkedList<>();\n    singleDelete.add(ZKUtilOp.deleteNodeFailSilent(path));\n    ZKUtil.multiOrSequential(zkw, singleDelete, false);\n    assertTrue(ZKUtil.checkExists(zkw, path) == -1);\n  }",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "  @Test\n  public void testSimpleMulti() throws Exception {\n    // null multi\n    ZKUtil.multiOrSequential(zkw, null, false);\n\n    // empty multi\n    ZKUtil.multiOrSequential(zkw, new LinkedList<>(), false);\n\n    // single create\n    String path = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testSimpleMulti\");\n    LinkedList<ZKUtilOp> singleCreate = new LinkedList<>();\n    singleCreate.add(ZKUtilOp.createAndFailSilent(path, new byte[0]));\n    ZKUtil.multiOrSequential(zkw, singleCreate, false);\n    assertTrue(ZKUtil.checkExists(zkw, path) != -1);\n\n    // single setdata\n    LinkedList<ZKUtilOp> singleSetData = new LinkedList<>();\n    byte [] data = Bytes.toBytes(\"foobar\");\n    singleSetData.add(ZKUtilOp.setData(path, data));\n    ZKUtil.multiOrSequential(zkw, singleSetData, false);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path), data));\n\n    // single delete\n    LinkedList<ZKUtilOp> singleDelete = new LinkedList<>();\n    singleDelete.add(ZKUtilOp.deleteNodeFailSilent(path));\n    ZKUtil.multiOrSequential(zkw, singleDelete, false);\n    assertTrue(ZKUtil.checkExists(zkw, path) == -1);\n  }"
        ],
        [
            "ZKTableArchiveClient::getArchiveZNode(Configuration,ZKWatcher)",
            " 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 -\n 153  \n 154  ",
            "  /**\n   * @param conf conf to read for the base archive node\n   * @param zooKeeper zookeeper to used for building the full path\n   * @return get the znode for long-term archival of a table for\n   */\n  public static String getArchiveZNode(Configuration conf, ZKWatcher zooKeeper) {\n    return ZNodePaths.joinZNode(zooKeeper.znodePaths.baseZNode, conf.get(\n      ZOOKEEPER_ZNODE_HFILE_ARCHIVE_KEY, TableHFileArchiveTracker.HFILE_ARCHIVE_ZNODE_PARENT));\n  }",
            " 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 +\n 153  \n 154  ",
            "  /**\n   * @param conf conf to read for the base archive node\n   * @param zooKeeper zookeeper to used for building the full path\n   * @return get the znode for long-term archival of a table for\n   */\n  public static String getArchiveZNode(Configuration conf, ZKWatcher zooKeeper) {\n    return ZNodePaths.joinZNode(zooKeeper.getZNodePaths().baseZNode, conf.get(\n      ZOOKEEPER_ZNODE_HFILE_ARCHIVE_KEY, TableHFileArchiveTracker.HFILE_ARCHIVE_ZNODE_PARENT));\n  }"
        ],
        [
            "MetaLocationSyncer::getNodesToWatch()",
            "  42  \n  43  \n  44 -\n  45  ",
            "  @Override\n  Collection<String> getNodesToWatch() {\n    return watcher.znodePaths.metaReplicaZNodes.values();\n  }",
            "  42  \n  43  \n  44 +\n  45  ",
            "  @Override\n  Collection<String> getNodesToWatch() {\n    return watcher.getZNodePaths().metaReplicaZNodes.values();\n  }"
        ],
        [
            "ZKUtil::getDataNoWatch(ZKWatcher,String,Stat)",
            " 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698 -\n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  ",
            "  /**\n   * Get the data at the specified znode without setting a watch.\n   *\n   * Returns the data if the node exists.  Returns null if the node does not\n   * exist.\n   *\n   * Sets the stats of the node in the passed Stat object.  Pass a null stat if\n   * not interested.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param stat node status to get if node exists\n   * @return data of the specified znode, or null if node does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static byte [] getDataNoWatch(ZKWatcher zkw, String znode,\n                                       Stat stat)\n  throws KeeperException {\n    try {\n      byte [] data = zkw.getRecoverableZooKeeper().getData(znode, null, stat);\n      logRetrievedMsg(zkw, znode, data, false);\n      return data;\n    } catch (KeeperException.NoNodeException e) {\n      LOG.debug(zkw.prefix(\"Unable to get data of znode \" + znode + \" \" +\n          \"because node does not exist (not necessarily an error)\"));\n      return null;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to get data of znode \" + znode), e);\n      zkw.keeperException(e);\n      return null;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to get data of znode \" + znode), e);\n      zkw.interruptedException(e);\n      return null;\n    }\n  }",
            " 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698 +\n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  ",
            "  /**\n   * Get the data at the specified znode without setting a watch.\n   *\n   * Returns the data if the node exists.  Returns null if the node does not\n   * exist.\n   *\n   * Sets the stats of the node in the passed Stat object.  Pass a null stat if\n   * not interested.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param stat node status to get if node exists\n   * @return data of the specified znode, or null if node does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static byte [] getDataNoWatch(ZKWatcher zkw, String znode,\n                                       Stat stat)\n    throws KeeperException {\n    try {\n      byte [] data = zkw.getRecoverableZooKeeper().getData(znode, null, stat);\n      logRetrievedMsg(zkw, znode, data, false);\n      return data;\n    } catch (KeeperException.NoNodeException e) {\n      LOG.debug(zkw.prefix(\"Unable to get data of znode \" + znode + \" \" +\n          \"because node does not exist (not necessarily an error)\"));\n      return null;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to get data of znode \" + znode), e);\n      zkw.keeperException(e);\n      return null;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to get data of znode \" + znode), e);\n      zkw.interruptedException(e);\n      return null;\n    }\n  }"
        ],
        [
            "RegionServerTracker::nodeCreated(String)",
            " 115  \n 116  \n 117 -\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  ",
            "  @Override\n  public void nodeCreated(String path) {\n    if (path.startsWith(watcher.znodePaths.rsZNode)) {\n      String serverName = ZKUtil.getNodeName(path);\n      LOG.info(\"RegionServer ephemeral node created, adding [\" + serverName + \"]\");\n      if (server.isInitialized()) {\n        // Only call the check to move servers if a RegionServer was added to the cluster; in this\n        // case it could be a server with a new version so it makes sense to run the check.\n        server.checkIfShouldMoveSystemRegionAsync();\n      }\n    }\n  }",
            " 115  \n 116  \n 117 +\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  ",
            "  @Override\n  public void nodeCreated(String path) {\n    if (path.startsWith(watcher.getZNodePaths().rsZNode)) {\n      String serverName = ZKUtil.getNodeName(path);\n      LOG.info(\"RegionServer ephemeral node created, adding [\" + serverName + \"]\");\n      if (server.isInitialized()) {\n        // Only call the check to move servers if a RegionServer was added to the cluster; in this\n        // case it could be a server with a new version so it makes sense to run the check.\n        server.checkIfShouldMoveSystemRegionAsync();\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::createAndFailSilent(ZKWatcher,String,byte)",
            "1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135 -\n1136  \n1137  \n1138  ",
            "  /**\n   * Creates the specified node containing specified data, iff the node does not exist.  Does\n   * not set a watch and fails silently if the node already exists.\n   *\n   * The node created is persistent and open access.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data a byte array data to store in the znode\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static void createAndFailSilent(ZKWatcher zkw,\n      String znode, byte[] data)\n  throws KeeperException {\n    createAndFailSilent(zkw,\n        (CreateAndFailSilent)ZKUtilOp.createAndFailSilent(znode, data));\n  }",
            "1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135 +\n1136  \n1137  \n1138  ",
            "  /**\n   * Creates the specified node containing specified data, iff the node does not exist.  Does\n   * not set a watch and fails silently if the node already exists.\n   *\n   * The node created is persistent and open access.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data a byte array data to store in the znode\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static void createAndFailSilent(ZKWatcher zkw,\n      String znode, byte[] data)\n    throws KeeperException {\n    createAndFailSilent(zkw,\n        (CreateAndFailSilent)ZKUtilOp.createAndFailSilent(znode, data));\n  }"
        ],
        [
            "MetaTableLocator::waitMetaRegionLocation(ZKWatcher,int,long)",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 -\n 213  \n 214 -\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "  /**\n   * Gets the meta region location, if available, and waits for up to the specified timeout if not\n   * immediately available. Given the zookeeper notification could be delayed, we will try to\n   * get the latest data.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param replicaId the ID of the replica\n   * @param timeout maximum time to wait, in millis\n   * @return server name for server hosting meta region formatted as per\n   * {@link ServerName}, or null if none available\n   * @throws InterruptedException if waiting for the socket operation fails\n   * @throws NotAllMetaRegionsOnlineException if a meta or root region is not online\n   */\n  public ServerName waitMetaRegionLocation(ZKWatcher zkw, int replicaId, long timeout)\n  throws InterruptedException, NotAllMetaRegionsOnlineException {\n    try {\n      if (ZKUtil.checkExists(zkw, zkw.znodePaths.baseZNode) == -1) {\n        String errorMsg = \"Check the value configured in 'zookeeper.znode.parent'. \"\n            + \"There could be a mismatch with the one configured in the master.\";\n        LOG.error(errorMsg);\n        throw new IllegalArgumentException(errorMsg);\n      }\n    } catch (KeeperException e) {\n      throw new IllegalStateException(\"KeeperException while trying to check baseZNode:\", e);\n    }\n    ServerName sn = blockUntilAvailable(zkw, replicaId, timeout);\n\n    if (sn == null) {\n      throw new NotAllMetaRegionsOnlineException(\"Timed out; \" + timeout + \"ms\");\n    }\n\n    return sn;\n  }",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 +\n 213  \n 214 +\n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "  /**\n   * Gets the meta region location, if available, and waits for up to the specified timeout if not\n   * immediately available. Given the zookeeper notification could be delayed, we will try to\n   * get the latest data.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param replicaId the ID of the replica\n   * @param timeout maximum time to wait, in millis\n   * @return server name for server hosting meta region formatted as per\n   * {@link ServerName}, or null if none available\n   * @throws InterruptedException if waiting for the socket operation fails\n   * @throws NotAllMetaRegionsOnlineException if a meta or root region is not online\n   */\n  public ServerName waitMetaRegionLocation(ZKWatcher zkw, int replicaId, long timeout)\n    throws InterruptedException, NotAllMetaRegionsOnlineException {\n    try {\n      if (ZKUtil.checkExists(zkw, zkw.getZNodePaths().baseZNode) == -1) {\n        String errorMsg = \"Check the value configured in 'zookeeper.znode.parent'. \"\n            + \"There could be a mismatch with the one configured in the master.\";\n        LOG.error(errorMsg);\n        throw new IllegalArgumentException(errorMsg);\n      }\n    } catch (KeeperException e) {\n      throw new IllegalStateException(\"KeeperException while trying to check baseZNode:\", e);\n    }\n    ServerName sn = blockUntilAvailable(zkw, replicaId, timeout);\n\n    if (sn == null) {\n      throw new NotAllMetaRegionsOnlineException(\"Timed out; \" + timeout + \"ms\");\n    }\n\n    return sn;\n  }"
        ],
        [
            "IntegrationTestZKAndFSPermissions::testZNodeACLs()",
            " 142  \n 143  \n 144  \n 145  \n 146  \n 147 -\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  ",
            "  private void testZNodeACLs() throws IOException, KeeperException, InterruptedException {\n\n    ZKWatcher watcher = new ZKWatcher(conf, \"IntegrationTestZnodeACLs\", null);\n    RecoverableZooKeeper zk = ZKUtil.connect(this.conf, watcher);\n\n    String baseZNode = watcher.znodePaths.baseZNode;\n\n    LOG.info(\"\");\n    LOG.info(\"***********************************************************************************\");\n    LOG.info(\"Checking ZK permissions, root znode: \" + baseZNode);\n    LOG.info(\"***********************************************************************************\");\n    LOG.info(\"\");\n\n    checkZnodePermsRecursive(watcher, zk, baseZNode);\n\n    LOG.info(\"Checking ZK permissions: SUCCESS\");\n  }",
            " 142  \n 143  \n 144  \n 145  \n 146  \n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  ",
            "  private void testZNodeACLs() throws IOException, KeeperException, InterruptedException {\n\n    ZKWatcher watcher = new ZKWatcher(conf, \"IntegrationTestZnodeACLs\", null);\n    RecoverableZooKeeper zk = ZKUtil.connect(this.conf, watcher);\n\n    String baseZNode = watcher.getZNodePaths().baseZNode;\n\n    LOG.info(\"\");\n    LOG.info(\"***********************************************************************************\");\n    LOG.info(\"Checking ZK permissions, root znode: \" + baseZNode);\n    LOG.info(\"***********************************************************************************\");\n    LOG.info(\"\");\n\n    checkZnodePermsRecursive(watcher, zk, baseZNode);\n\n    LOG.info(\"Checking ZK permissions: SUCCESS\");\n  }"
        ],
        [
            "ZkSplitLogWorkerCoordination::getNumExpectedTasksPerRS(int)",
            " 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 -\n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  ",
            "  /**\n   * This function calculates how many splitters this RS should create based on expected average\n   * tasks per RS and the hard limit upper bound(maxConcurrentTasks) set by configuration. <br>\n   * At any given time, a RS allows spawn MIN(Expected Tasks/RS, Hard Upper Bound)\n   * @param numTasks total number of split tasks available\n   * @return number of tasks this RS can grab\n   */\n  private int getNumExpectedTasksPerRS(int numTasks) {\n    // at lease one RS(itself) available\n    int availableRSs = 1;\n    try {\n      List<String> regionServers =\n          ZKUtil.listChildrenNoWatch(watcher, watcher.znodePaths.rsZNode);\n      availableRSs = Math.max(availableRSs, (regionServers == null) ? 0 : regionServers.size());\n    } catch (KeeperException e) {\n      // do nothing\n      LOG.debug(\"getAvailableRegionServers got ZooKeeper exception\", e);\n    }\n    int expectedTasksPerRS = (numTasks / availableRSs) + ((numTasks % availableRSs == 0) ? 0 : 1);\n    return Math.max(1, expectedTasksPerRS); // at least be one\n  }",
            " 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 +\n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  ",
            "  /**\n   * This function calculates how many splitters this RS should create based on expected average\n   * tasks per RS and the hard limit upper bound(maxConcurrentTasks) set by configuration. <br>\n   * At any given time, a RS allows spawn MIN(Expected Tasks/RS, Hard Upper Bound)\n   * @param numTasks total number of split tasks available\n   * @return number of tasks this RS can grab\n   */\n  private int getNumExpectedTasksPerRS(int numTasks) {\n    // at lease one RS(itself) available\n    int availableRSs = 1;\n    try {\n      List<String> regionServers =\n          ZKUtil.listChildrenNoWatch(watcher, watcher.getZNodePaths().rsZNode);\n      availableRSs = Math.max(availableRSs, (regionServers == null) ? 0 : regionServers.size());\n    } catch (KeeperException e) {\n      // do nothing\n      LOG.debug(\"getAvailableRegionServers got ZooKeeper exception\", e);\n    }\n    int expectedTasksPerRS = (numTasks / availableRSs) + ((numTasks % availableRSs == 0) ? 0 : 1);\n    return Math.max(1, expectedTasksPerRS); // at least be one\n  }"
        ],
        [
            "ZKUtil::deleteNode(ZKWatcher,String)",
            "1215  \n1216  \n1217  \n1218  \n1219 -\n1220  \n1221  ",
            "  /**\n   * Delete the specified node.  Sets no watches.  Throws all exceptions.\n   */\n  public static void deleteNode(ZKWatcher zkw, String node)\n  throws KeeperException {\n    deleteNode(zkw, node, -1);\n  }",
            "1215  \n1216  \n1217  \n1218  \n1219 +\n1220  \n1221  ",
            "  /**\n   * Delete the specified node.  Sets no watches.  Throws all exceptions.\n   */\n  public static void deleteNode(ZKWatcher zkw, String node)\n    throws KeeperException {\n    deleteNode(zkw, node, -1);\n  }"
        ],
        [
            "MetaTableLocator::getMetaRegionState(ZKWatcher,int)",
            " 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495 -\n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  ",
            "  /**\n   * Load the meta region state from the meta server ZNode.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param replicaId the ID of the replica\n   * @return regionstate\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static RegionState getMetaRegionState(ZKWatcher zkw, int replicaId)\n          throws KeeperException {\n    RegionState.State state = RegionState.State.OPEN;\n    ServerName serverName = null;\n    try {\n      byte[] data = ZKUtil.getData(zkw, zkw.znodePaths.getZNodeForReplica(replicaId));\n      if (data != null && data.length > 0 && ProtobufUtil.isPBMagicPrefix(data)) {\n        try {\n          int prefixLen = ProtobufUtil.lengthOfPBMagic();\n          ZooKeeperProtos.MetaRegionServer rl =\n            ZooKeeperProtos.MetaRegionServer.PARSER.parseFrom(data, prefixLen,\n                    data.length - prefixLen);\n          if (rl.hasState()) {\n            state = RegionState.State.convert(rl.getState());\n          }\n          HBaseProtos.ServerName sn = rl.getServer();\n          serverName = ServerName.valueOf(\n            sn.getHostName(), sn.getPort(), sn.getStartCode());\n        } catch (InvalidProtocolBufferException e) {\n          throw new DeserializationException(\"Unable to parse meta region location\");\n        }\n      } else {\n        // old style of meta region location?\n        serverName = ProtobufUtil.parseServerNameFrom(data);\n      }\n    } catch (DeserializationException e) {\n      throw ZKUtil.convert(e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    if (serverName == null) {\n      state = RegionState.State.OFFLINE;\n    }\n    return new RegionState(\n        RegionReplicaUtil.getRegionInfoForReplica(\n            RegionInfoBuilder.FIRST_META_REGIONINFO, replicaId),\n        state, serverName);\n  }",
            " 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496 +\n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  ",
            "  /**\n   * Load the meta region state from the meta server ZNode.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param replicaId the ID of the replica\n   * @return regionstate\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static RegionState getMetaRegionState(ZKWatcher zkw, int replicaId)\n          throws KeeperException {\n    RegionState.State state = RegionState.State.OPEN;\n    ServerName serverName = null;\n    try {\n      byte[] data = ZKUtil.getData(zkw, zkw.getZNodePaths().getZNodeForReplica(replicaId));\n      if (data != null && data.length > 0 && ProtobufUtil.isPBMagicPrefix(data)) {\n        try {\n          int prefixLen = ProtobufUtil.lengthOfPBMagic();\n          ZooKeeperProtos.MetaRegionServer rl =\n            ZooKeeperProtos.MetaRegionServer.PARSER.parseFrom(data, prefixLen,\n                    data.length - prefixLen);\n          if (rl.hasState()) {\n            state = RegionState.State.convert(rl.getState());\n          }\n          HBaseProtos.ServerName sn = rl.getServer();\n          serverName = ServerName.valueOf(\n            sn.getHostName(), sn.getPort(), sn.getStartCode());\n        } catch (InvalidProtocolBufferException e) {\n          throw new DeserializationException(\"Unable to parse meta region location\");\n        }\n      } else {\n        // old style of meta region location?\n        serverName = ProtobufUtil.parseServerNameFrom(data);\n      }\n    } catch (DeserializationException e) {\n      throw ZKUtil.convert(e);\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt();\n    }\n    if (serverName == null) {\n      state = RegionState.State.OFFLINE;\n    }\n    return new RegionState(\n        RegionReplicaUtil.getRegionInfoForReplica(\n            RegionInfoBuilder.FIRST_META_REGIONINFO, replicaId),\n        state, serverName);\n  }"
        ],
        [
            "ZKUtil::logRetrievedMsg(ZKWatcher,String,byte,boolean)",
            "1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992 -\n1993  \n1994 -\n1995  \n1996  \n1997  ",
            "  private static void logRetrievedMsg(final ZKWatcher zkw,\n      final String znode, final byte [] data, final boolean watcherSet) {\n    if (!LOG.isTraceEnabled()) {\n      return;\n    }\n\n    LOG.trace(zkw.prefix(\"Retrieved \" + ((data == null)? 0: data.length) +\n      \" byte(s) of data from znode \" + znode +\n      (watcherSet? \" and set watcher; \": \"; data=\") +\n      (data == null? \"null\": data.length == 0? \"empty\": (\n          znode.startsWith(zkw.znodePaths.metaZNodePrefix)?\n            getServerNameOrEmptyString(data):\n          znode.startsWith(zkw.znodePaths.backupMasterAddressesZNode)?\n            getServerNameOrEmptyString(data):\n          StringUtils.abbreviate(Bytes.toStringBinary(data), 32)))));\n  }",
            "1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  \n1992  \n1993 +\n1994  \n1995 +\n1996  \n1997  \n1998  ",
            "  private static void logRetrievedMsg(final ZKWatcher zkw,\n      final String znode, final byte [] data, final boolean watcherSet) {\n    if (!LOG.isTraceEnabled()) {\n      return;\n    }\n\n    LOG.trace(zkw.prefix(\"Retrieved \" + ((data == null)? 0: data.length) +\n      \" byte(s) of data from znode \" + znode +\n      (watcherSet? \" and set watcher; \": \"; data=\") +\n      (data == null? \"null\": data.length == 0? \"empty\": (\n          znode.startsWith(zkw.getZNodePaths().metaZNodePrefix)?\n            getServerNameOrEmptyString(data):\n          znode.startsWith(zkw.getZNodePaths().backupMasterAddressesZNode)?\n            getServerNameOrEmptyString(data):\n          StringUtils.abbreviate(Bytes.toStringBinary(data), 32)))));\n  }"
        ],
        [
            "TestSplitLogManager::setup()",
            " 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131 -\n 132 -\n 133 -\n 134 -\n 135 -\n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "  @Before\n  public void setup() throws Exception {\n    TEST_UTIL = new HBaseTestingUtility();\n    TEST_UTIL.startMiniZKCluster();\n    conf = TEST_UTIL.getConfiguration();\n    // Use a different ZK wrapper instance for each tests.\n    zkw =\n        new ZKWatcher(conf, \"split-log-manager-tests\" + UUID.randomUUID().toString(), null);\n    master = new DummyMasterServices(zkw, conf);\n\n    ZKUtil.deleteChildrenRecursively(zkw, zkw.znodePaths.baseZNode);\n    ZKUtil.createAndFailSilent(zkw, zkw.znodePaths.baseZNode);\n    assertTrue(ZKUtil.checkExists(zkw, zkw.znodePaths.baseZNode) != -1);\n    LOG.debug(zkw.znodePaths.baseZNode + \" created\");\n    ZKUtil.createAndFailSilent(zkw, zkw.znodePaths.splitLogZNode);\n    assertTrue(ZKUtil.checkExists(zkw, zkw.znodePaths.splitLogZNode) != -1);\n    LOG.debug(zkw.znodePaths.splitLogZNode + \" created\");\n\n    resetCounters();\n\n    // By default, we let the test manage the error as before, so the server\n    // does not appear as dead from the master point of view, only from the split log pov.\n    Mockito.when(sm.isServerOnline(Mockito.any())).thenReturn(true);\n\n    to = 12000;\n    conf.setInt(HConstants.HBASE_SPLITLOG_MANAGER_TIMEOUT, to);\n    conf.setInt(\"hbase.splitlog.manager.unassigned.timeout\", 2 * to);\n\n    conf.setInt(\"hbase.splitlog.manager.timeoutmonitor.period\", 100);\n    to = to + 16 * 100;\n  }",
            " 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 +\n 131 +\n 132 +\n 133 +\n 134 +\n 135 +\n 136 +\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "  @Before\n  public void setup() throws Exception {\n    TEST_UTIL = new HBaseTestingUtility();\n    TEST_UTIL.startMiniZKCluster();\n    conf = TEST_UTIL.getConfiguration();\n    // Use a different ZK wrapper instance for each tests.\n    zkw =\n        new ZKWatcher(conf, \"split-log-manager-tests\" + UUID.randomUUID().toString(), null);\n    master = new DummyMasterServices(zkw, conf);\n\n    ZKUtil.deleteChildrenRecursively(zkw, zkw.getZNodePaths().baseZNode);\n    ZKUtil.createAndFailSilent(zkw, zkw.getZNodePaths().baseZNode);\n    assertTrue(ZKUtil.checkExists(zkw, zkw.getZNodePaths().baseZNode) != -1);\n    LOG.debug(zkw.getZNodePaths().baseZNode + \" created\");\n    ZKUtil.createAndFailSilent(zkw, zkw.getZNodePaths().splitLogZNode);\n    assertTrue(ZKUtil.checkExists(zkw, zkw.getZNodePaths().splitLogZNode) != -1);\n    LOG.debug(zkw.getZNodePaths().splitLogZNode + \" created\");\n\n    resetCounters();\n\n    // By default, we let the test manage the error as before, so the server\n    // does not appear as dead from the master point of view, only from the split log pov.\n    Mockito.when(sm.isServerOnline(Mockito.any())).thenReturn(true);\n\n    to = 12000;\n    conf.setInt(HConstants.HBASE_SPLITLOG_MANAGER_TIMEOUT, to);\n    conf.setInt(\"hbase.splitlog.manager.unassigned.timeout\", 2 * to);\n\n    conf.setInt(\"hbase.splitlog.manager.timeoutmonitor.period\", 100);\n    to = to + 16 * 100;\n  }"
        ],
        [
            "TestReplicationStateZKImpl::setUpBeforeClass()",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 -\n  62  \n  63  \n  64  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    utility = new HBaseZKTestingUtility();\n    utility.startMiniZKCluster();\n    conf = utility.getConfiguration();\n    conf.setBoolean(HConstants.REPLICATION_BULKLOAD_ENABLE_KEY, true);\n    zkw = utility.getZooKeeperWatcher();\n    String replicationZNodeName = conf.get(\"zookeeper.znode.replication\", \"replication\");\n    replicationZNode = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, replicationZNodeName);\n    KEY_ONE = initPeerClusterState(\"/hbase1\");\n    KEY_TWO = initPeerClusterState(\"/hbase2\");\n  }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62  \n  63  \n  64  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    utility = new HBaseZKTestingUtility();\n    utility.startMiniZKCluster();\n    conf = utility.getConfiguration();\n    conf.setBoolean(HConstants.REPLICATION_BULKLOAD_ENABLE_KEY, true);\n    zkw = utility.getZooKeeperWatcher();\n    String replicationZNodeName = conf.get(\"zookeeper.znode.replication\", \"replication\");\n    replicationZNode = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, replicationZNodeName);\n    KEY_ONE = initPeerClusterState(\"/hbase1\");\n    KEY_TWO = initPeerClusterState(\"/hbase2\");\n  }"
        ],
        [
            "ZKUtil::createAndWatch(ZKWatcher,String,byte)",
            "1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069 -\n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  ",
            "  /**\n   * Creates the specified node with the specified data and watches it.\n   *\n   * <p>Throws an exception if the node already exists.\n   *\n   * <p>The node created is persistent and open access.\n   *\n   * <p>Returns the version number of the created node if successful.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to create\n   * @param data data of node to create\n   * @return version of node created\n   * @throws KeeperException if unexpected zookeeper exception\n   * @throws KeeperException.NodeExistsException if node already exists\n   */\n  public static int createAndWatch(ZKWatcher zkw,\n      String znode, byte [] data)\n  throws KeeperException, KeeperException.NodeExistsException {\n    try {\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.PERSISTENT);\n      Stat stat = zkw.getRecoverableZooKeeper().exists(znode, zkw);\n      if (stat == null){\n        // Likely a race condition. Someone deleted the znode.\n        throw KeeperException.create(KeeperException.Code.SYSTEMERROR,\n            \"ZK.exists returned null (i.e.: znode does not exist) for znode=\" + znode);\n      }\n\n      return stat.getVersion();\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return -1;\n    }\n  }",
            "1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069 +\n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  ",
            "  /**\n   * Creates the specified node with the specified data and watches it.\n   *\n   * <p>Throws an exception if the node already exists.\n   *\n   * <p>The node created is persistent and open access.\n   *\n   * <p>Returns the version number of the created node if successful.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to create\n   * @param data data of node to create\n   * @return version of node created\n   * @throws KeeperException if unexpected zookeeper exception\n   * @throws KeeperException.NodeExistsException if node already exists\n   */\n  public static int createAndWatch(ZKWatcher zkw,\n      String znode, byte [] data)\n    throws KeeperException, KeeperException.NodeExistsException {\n    try {\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.PERSISTENT);\n      Stat stat = zkw.getRecoverableZooKeeper().exists(znode, zkw);\n      if (stat == null){\n        // Likely a race condition. Someone deleted the znode.\n        throw KeeperException.create(KeeperException.Code.SYSTEMERROR,\n            \"ZK.exists returned null (i.e.: znode does not exist) for znode=\" + znode);\n      }\n\n      return stat.getVersion();\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return -1;\n    }\n  }"
        ],
        [
            "ReplicationTrackerZKImpl::OtherRegionServerWatcher::refreshListIfRightPath(String)",
            " 140  \n 141 -\n 142  \n 143  \n 144  \n 145  ",
            "    private boolean refreshListIfRightPath(String path) {\n      if (!path.startsWith(this.watcher.znodePaths.rsZNode)) {\n        return false;\n      }\n      return refreshOtherRegionServersList(true);\n    }",
            " 140  \n 141 +\n 142  \n 143  \n 144  \n 145  ",
            "    private boolean refreshListIfRightPath(String path) {\n      if (!path.startsWith(this.watcher.getZNodePaths().rsZNode)) {\n        return false;\n      }\n      return refreshOtherRegionServersList(true);\n    }"
        ],
        [
            "ZKNodeTracker::checkIfBaseNodeAvailable()",
            " 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 -\n 242  \n 243  \n 244  \n 245 -\n 246  \n 247  \n 248  \n 249  \n 250  ",
            "  /**\n   * Checks if the baseznode set as per the property 'zookeeper.znode.parent'\n   * exists.\n   * @return true if baseznode exists.\n   *         false if doesnot exists.\n   */\n  public boolean checkIfBaseNodeAvailable() {\n    try {\n      if (ZKUtil.checkExists(watcher, watcher.znodePaths.baseZNode) == -1) {\n        return false;\n      }\n    } catch (KeeperException e) {\n      abortable.abort(\"Exception while checking if basenode (\" + watcher.znodePaths.baseZNode\n          + \") exists in ZooKeeper.\",\n        e);\n    }\n    return true;\n  }",
            " 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 +\n 242  \n 243  \n 244  \n 245 +\n 246  \n 247  \n 248  \n 249  \n 250  ",
            "  /**\n   * Checks if the baseznode set as per the property 'zookeeper.znode.parent'\n   * exists.\n   * @return true if baseznode exists.\n   *         false if doesnot exists.\n   */\n  public boolean checkIfBaseNodeAvailable() {\n    try {\n      if (ZKUtil.checkExists(watcher, watcher.getZNodePaths().baseZNode) == -1) {\n        return false;\n      }\n    } catch (KeeperException e) {\n      abortable.abort(\"Exception while checking if basenode (\" + watcher.getZNodePaths().baseZNode\n          + \") exists in ZooKeeper.\",\n        e);\n    }\n    return true;\n  }"
        ],
        [
            "ZKSplitLog::getRescanNode(ZKWatcher)",
            "  74  \n  75 -\n  76  ",
            "  public static String getRescanNode(ZKWatcher zkw) {\n    return ZNodePaths.joinZNode(zkw.znodePaths.splitLogZNode, \"RESCAN\");\n  }",
            "  74  \n  75 +\n  76  ",
            "  public static String getRescanNode(ZKWatcher zkw) {\n    return ZNodePaths.joinZNode(zkw.getZNodePaths().splitLogZNode, \"RESCAN\");\n  }"
        ],
        [
            "ZkSplitLogWorkerCoordination::getTaskList()",
            " 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475 -\n 476  \n 477  \n 478  \n 479  \n 480 -\n 481  \n 482 -\n 483  \n 484  \n 485  \n 486  \n 487  ",
            "  private List<String> getTaskList() throws InterruptedException {\n    List<String> childrenPaths = null;\n    long sleepTime = 1000;\n    // It will be in loop till it gets the list of children or\n    // it will come out if worker thread exited.\n    while (!shouldStop) {\n      try {\n        childrenPaths = ZKUtil.listChildrenAndWatchForNewChildren(watcher,\n          watcher.znodePaths.splitLogZNode);\n        if (childrenPaths != null) {\n          return childrenPaths;\n        }\n      } catch (KeeperException e) {\n        LOG.warn(\"Could not get children of znode \" + watcher.znodePaths.splitLogZNode, e);\n      }\n      LOG.debug(\"Retry listChildren of znode \" + watcher.znodePaths.splitLogZNode\n          + \" after sleep for \" + sleepTime + \"ms!\");\n      Thread.sleep(sleepTime);\n    }\n    return childrenPaths;\n  }",
            " 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475 +\n 476  \n 477  \n 478  \n 479  \n 480 +\n 481  \n 482 +\n 483  \n 484  \n 485  \n 486  \n 487  ",
            "  private List<String> getTaskList() throws InterruptedException {\n    List<String> childrenPaths = null;\n    long sleepTime = 1000;\n    // It will be in loop till it gets the list of children or\n    // it will come out if worker thread exited.\n    while (!shouldStop) {\n      try {\n        childrenPaths = ZKUtil.listChildrenAndWatchForNewChildren(watcher,\n          watcher.getZNodePaths().splitLogZNode);\n        if (childrenPaths != null) {\n          return childrenPaths;\n        }\n      } catch (KeeperException e) {\n        LOG.warn(\"Could not get children of znode \" + watcher.getZNodePaths().splitLogZNode, e);\n      }\n      LOG.debug(\"Retry listChildren of znode \" + watcher.getZNodePaths().splitLogZNode\n          + \" after sleep for \" + sleepTime + \"ms!\");\n      Thread.sleep(sleepTime);\n    }\n    return childrenPaths;\n  }"
        ],
        [
            "ActiveMasterManager::hasActiveMaster()",
            " 243  \n 244  \n 245  \n 246  \n 247  \n 248 -\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "  /**\n   * @return True if cluster has an active master.\n   */\n  boolean hasActiveMaster() {\n    try {\n      if (ZKUtil.checkExists(watcher, watcher.znodePaths.masterAddressZNode) >= 0) {\n        return true;\n      }\n    }\n    catch (KeeperException ke) {\n      LOG.info(\"Received an unexpected KeeperException when checking \" +\n          \"isActiveMaster : \"+ ke);\n    }\n    return false;\n  }",
            " 243  \n 244  \n 245  \n 246  \n 247  \n 248 +\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "  /**\n   * @return True if cluster has an active master.\n   */\n  boolean hasActiveMaster() {\n    try {\n      if (ZKUtil.checkExists(watcher, watcher.getZNodePaths().masterAddressZNode) >= 0) {\n        return true;\n      }\n    }\n    catch (KeeperException ke) {\n      LOG.info(\"Received an unexpected KeeperException when checking \" +\n          \"isActiveMaster : \"+ ke);\n    }\n    return false;\n  }"
        ],
        [
            "MetaTableLocator::setMetaLocation(ZKWatcher,ServerName,int,RegionState)",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463 -\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471 -\n 472  \n 473  ",
            "  /**\n   * Sets the location of <code>hbase:meta</code> in ZooKeeper to the specified server address.\n   * @param zookeeper reference to the {@link ZKWatcher} which also contains configuration and\n   *                  operation\n   * @param serverName the name of the server\n   * @param replicaId the ID of the replica\n   * @param state the state of the region\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static void setMetaLocation(ZKWatcher zookeeper, ServerName serverName, int replicaId,\n      RegionState.State state) throws KeeperException {\n    if (serverName == null) {\n      LOG.warn(\"Tried to set null ServerName in hbase:meta; skipping -- ServerName required\");\n      return;\n    }\n    LOG.info(\"Setting hbase:meta (replicaId=\" + replicaId + \") location in ZooKeeper as \" +\n      serverName);\n    // Make the MetaRegionServer pb and then get its bytes and save this as\n    // the znode content.\n    MetaRegionServer pbrsr = MetaRegionServer.newBuilder()\n      .setServer(ProtobufUtil.toServerName(serverName))\n      .setRpcVersion(HConstants.RPC_CURRENT_VERSION)\n      .setState(state.convert()).build();\n    byte[] data = ProtobufUtil.prependPBMagic(pbrsr.toByteArray());\n    try {\n      ZKUtil.setData(zookeeper,\n          zookeeper.znodePaths.getZNodeForReplica(replicaId), data);\n    } catch(KeeperException.NoNodeException nne) {\n      if (replicaId == RegionInfo.DEFAULT_REPLICA_ID) {\n        LOG.debug(\"META region location doesn't exist, create it\");\n      } else {\n        LOG.debug(\"META region location doesn't exist for replicaId=\" + replicaId +\n            \", create it\");\n      }\n      ZKUtil.createAndWatch(zookeeper, zookeeper.znodePaths.getZNodeForReplica(replicaId), data);\n    }\n  }",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463 +\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471 +\n 472 +\n 473  \n 474  ",
            "  /**\n   * Sets the location of <code>hbase:meta</code> in ZooKeeper to the specified server address.\n   * @param zookeeper reference to the {@link ZKWatcher} which also contains configuration and\n   *                  operation\n   * @param serverName the name of the server\n   * @param replicaId the ID of the replica\n   * @param state the state of the region\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static void setMetaLocation(ZKWatcher zookeeper, ServerName serverName, int replicaId,\n      RegionState.State state) throws KeeperException {\n    if (serverName == null) {\n      LOG.warn(\"Tried to set null ServerName in hbase:meta; skipping -- ServerName required\");\n      return;\n    }\n    LOG.info(\"Setting hbase:meta (replicaId=\" + replicaId + \") location in ZooKeeper as \" +\n      serverName);\n    // Make the MetaRegionServer pb and then get its bytes and save this as\n    // the znode content.\n    MetaRegionServer pbrsr = MetaRegionServer.newBuilder()\n      .setServer(ProtobufUtil.toServerName(serverName))\n      .setRpcVersion(HConstants.RPC_CURRENT_VERSION)\n      .setState(state.convert()).build();\n    byte[] data = ProtobufUtil.prependPBMagic(pbrsr.toByteArray());\n    try {\n      ZKUtil.setData(zookeeper,\n          zookeeper.getZNodePaths().getZNodeForReplica(replicaId), data);\n    } catch(KeeperException.NoNodeException nne) {\n      if (replicaId == RegionInfo.DEFAULT_REPLICA_ID) {\n        LOG.debug(\"META region location doesn't exist, create it\");\n      } else {\n        LOG.debug(\"META region location doesn't exist for replicaId=\" + replicaId +\n            \", create it\");\n      }\n      ZKUtil.createAndWatch(zookeeper, zookeeper.getZNodePaths().getZNodeForReplica(replicaId),\n              data);\n    }\n  }"
        ],
        [
            "TestHMasterRPCException::setUp()",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  ",
            "  @Before\n  public void setUp() throws Exception {\n    Configuration conf = testUtil.getConfiguration();\n    conf.set(HConstants.MASTER_PORT, \"0\");\n    conf.setInt(HConstants.ZK_SESSION_TIMEOUT, 2000);\n    testUtil.startMiniZKCluster();\n\n    ZKWatcher watcher = testUtil.getZooKeeperWatcher();\n    ZKUtil.createWithParents(watcher, watcher.znodePaths.masterAddressZNode, Bytes.toBytes(\"fake:123\"));\n    master = new HMaster(conf);\n    rpcClient = RpcClientFactory.createClient(conf, HConstants.CLUSTER_ID_DEFAULT);\n  }",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76 +\n  77  \n  78  \n  79  ",
            "  @Before\n  public void setUp() throws Exception {\n    Configuration conf = testUtil.getConfiguration();\n    conf.set(HConstants.MASTER_PORT, \"0\");\n    conf.setInt(HConstants.ZK_SESSION_TIMEOUT, 2000);\n    testUtil.startMiniZKCluster();\n\n    ZKWatcher watcher = testUtil.getZooKeeperWatcher();\n    ZKUtil.createWithParents(watcher, watcher.getZNodePaths().masterAddressZNode,\n            Bytes.toBytes(\"fake:123\"));\n    master = new HMaster(conf);\n    rpcClient = RpcClientFactory.createClient(conf, HConstants.CLUSTER_ID_DEFAULT);\n  }"
        ],
        [
            "ZkSplitLogWorkerCoordination::isReady()",
            " 494  \n 495  \n 496  \n 497  \n 498 -\n 499  \n 500  \n 501 -\n 502  \n 503  \n 504  \n 505 -\n 506  \n 507  \n 508  \n 509  \n 510  ",
            "  @Override\n  public boolean isReady() throws InterruptedException {\n    int result = -1;\n    try {\n      result = ZKUtil.checkExists(watcher, watcher.znodePaths.splitLogZNode);\n    } catch (KeeperException e) {\n      // ignore\n      LOG.warn(\"Exception when checking for \" + watcher.znodePaths.splitLogZNode\n          + \" ... retrying\", e);\n    }\n    if (result == -1) {\n      LOG.info(watcher.znodePaths.splitLogZNode\n          + \" znode does not exist, waiting for master to create\");\n      Thread.sleep(1000);\n    }\n    return (result != -1);\n  }",
            " 494  \n 495  \n 496  \n 497  \n 498 +\n 499  \n 500  \n 501 +\n 502  \n 503  \n 504  \n 505 +\n 506  \n 507  \n 508  \n 509  \n 510  ",
            "  @Override\n  public boolean isReady() throws InterruptedException {\n    int result = -1;\n    try {\n      result = ZKUtil.checkExists(watcher, watcher.getZNodePaths().splitLogZNode);\n    } catch (KeeperException e) {\n      // ignore\n      LOG.warn(\"Exception when checking for \" + watcher.getZNodePaths().splitLogZNode\n          + \" ... retrying\", e);\n    }\n    if (result == -1) {\n      LOG.info(watcher.getZNodePaths().splitLogZNode\n          + \" znode does not exist, waiting for master to create\");\n      Thread.sleep(1000);\n    }\n    return (result != -1);\n  }"
        ],
        [
            "ZKUtil::createNodeIfNotExistsAndWatch(ZKWatcher,String,byte)",
            " 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002 -\n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  ",
            "  /**\n   * Creates the specified znode to be a persistent node carrying the specified\n   * data.\n   *\n   * Returns true if the node was successfully created, false if the node\n   * already existed.\n   *\n   * If the node is created successfully, a watcher is also set on the node.\n   *\n   * If the node is not created successfully because it already exists, this\n   * method will also set a watcher on the node but return false.\n   *\n   * If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data of node\n   * @return true if node created, false if not, watch set in both cases\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean createNodeIfNotExistsAndWatch(\n          ZKWatcher zkw, String znode, byte [] data)\n  throws KeeperException {\n    boolean ret = true;\n    try {\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.PERSISTENT);\n    } catch (KeeperException.NodeExistsException nee) {\n      ret = false;\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return false;\n    }\n    try {\n      zkw.getRecoverableZooKeeper().exists(znode, zkw);\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return false;\n    }\n    return ret;\n  }",
            " 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002 +\n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  ",
            "  /**\n   * Creates the specified znode to be a persistent node carrying the specified\n   * data.\n   *\n   * Returns true if the node was successfully created, false if the node\n   * already existed.\n   *\n   * If the node is created successfully, a watcher is also set on the node.\n   *\n   * If the node is not created successfully because it already exists, this\n   * method will also set a watcher on the node but return false.\n   *\n   * If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data of node\n   * @return true if node created, false if not, watch set in both cases\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean createNodeIfNotExistsAndWatch(\n          ZKWatcher zkw, String znode, byte [] data)\n    throws KeeperException {\n    boolean ret = true;\n    try {\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.PERSISTENT);\n    } catch (KeeperException.NodeExistsException nee) {\n      ret = false;\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return false;\n    }\n    try {\n      zkw.getRecoverableZooKeeper().exists(znode, zkw);\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return false;\n    }\n    return ret;\n  }"
        ],
        [
            "ZKUtil::deleteNodeFailSilent(ZKWatcher,String)",
            "1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249 -\n1250  \n1251  \n1252  ",
            "  /**\n   * Deletes the specified node.  Fails silent if the node does not exist.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param node the node to delete\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static void deleteNodeFailSilent(ZKWatcher zkw, String node)\n  throws KeeperException {\n    deleteNodeFailSilent(zkw,\n      (DeleteNodeFailSilent)ZKUtilOp.deleteNodeFailSilent(node));\n  }",
            "1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249 +\n1250  \n1251  \n1252  ",
            "  /**\n   * Deletes the specified node.  Fails silent if the node does not exist.\n   *\n   * @param zkw reference to the {@link ZKWatcher} which also contains configuration and operation\n   * @param node the node to delete\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static void deleteNodeFailSilent(ZKWatcher zkw, String node)\n    throws KeeperException {\n    deleteNodeFailSilent(zkw,\n      (DeleteNodeFailSilent)ZKUtilOp.deleteNodeFailSilent(node));\n  }"
        ],
        [
            "ClusterStatusTracker::setClusterDown()",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82 -\n  83  \n  84 -\n  85  \n  86  \n  87 -\n  88  \n  89  ",
            "  /**\n   * Sets the cluster as down by deleting the znode.\n   * @throws KeeperException unexpected zk exception\n   */\n  public void setClusterDown()\n  throws KeeperException {\n    try {\n      ZKUtil.deleteNode(watcher, watcher.znodePaths.clusterStateZNode);\n    } catch(KeeperException.NoNodeException nne) {\n      LOG.warn(\"Attempted to set cluster as down but already down, cluster \" +\n          \"state node (\" + watcher.znodePaths.clusterStateZNode + \") not found\");\n    }\n  }",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83  \n  84 +\n  85  \n  86  \n  87 +\n  88  \n  89  ",
            "  /**\n   * Sets the cluster as down by deleting the znode.\n   * @throws KeeperException unexpected zk exception\n   */\n  public void setClusterDown()\n    throws KeeperException {\n    try {\n      ZKUtil.deleteNode(watcher, watcher.getZNodePaths().clusterStateZNode);\n    } catch(KeeperException.NoNodeException nne) {\n      LOG.warn(\"Attempted to set cluster as down but already down, cluster \" +\n          \"state node (\" + watcher.getZNodePaths().clusterStateZNode + \") not found\");\n    }\n  }"
        ],
        [
            "MetaTableLocator::getCachedConnection(ClusterConnection,ServerName)",
            " 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391 -\n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410 -\n 411  \n 412 -\n 413  \n 414  \n 415 -\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  ",
            "  /**\n   * @param sn ServerName to get a connection against.\n   * @return The AdminProtocol we got when we connected to <code>sn</code>\n   *         May have come from cache, may not be good, may have been setup by this invocation, or\n   *         may be null.\n   * @throws IOException if the number of retries for getting the connection is exceeded\n   */\n  private static AdminService.BlockingInterface getCachedConnection(ClusterConnection connection,\n      ServerName sn) throws IOException {\n    if (sn == null) {\n      return null;\n    }\n    AdminService.BlockingInterface service = null;\n    try {\n      service = connection.getAdmin(sn);\n    } catch (RetriesExhaustedException e) {\n      if (e.getCause() != null && e.getCause() instanceof ConnectException) {\n        // Catch this; presume it means the cached connection has gone bad.\n      } else {\n        throw e;\n      }\n    } catch (SocketTimeoutException e) {\n      LOG.debug(\"Timed out connecting to \" + sn);\n    } catch (NoRouteToHostException e) {\n      LOG.debug(\"Connecting to \" + sn, e);\n    } catch (SocketException e) {\n      LOG.debug(\"Exception connecting to \" + sn);\n    } catch (UnknownHostException e) {\n      LOG.debug(\"Unknown host exception connecting to  \" + sn);\n    } catch (FailedServerException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Server \" + sn + \" is in failed server list.\");\n      }\n    } catch (IOException ioe) {\n      Throwable cause = ioe.getCause();\n      if (ioe instanceof ConnectException) {\n        // Catch. Connect refused.\n      } else if (cause != null && cause instanceof EOFException) {\n        // Catch. Other end disconnected us.\n      } else if (cause != null && cause.getMessage() != null &&\n        cause.getMessage().toLowerCase(Locale.ROOT).contains(\"connection reset\")) {\n        // Catch. Connection reset.\n      } else {\n        throw ioe;\n      }\n\n    }\n    return service;\n  }",
            " 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391 +\n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410 +\n 411  \n 412 +\n 413  \n 414  \n 415 +\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  ",
            "  /**\n   * @param sn ServerName to get a connection against.\n   * @return The AdminProtocol we got when we connected to <code>sn</code>\n   *         May have come from cache, may not be good, may have been setup by this invocation, or\n   *         may be null.\n   * @throws IOException if the number of retries for getting the connection is exceeded\n   */\n  private static AdminService.BlockingInterface getCachedConnection(ClusterConnection connection,\n      ServerName sn) throws IOException {\n    if (sn == null) {\n      return null;\n    }\n    AdminService.BlockingInterface service = null;\n    try {\n      service = connection.getAdmin(sn);\n    } catch (RetriesExhaustedException e) {\n      if (e.getCause() != null && e.getCause() instanceof ConnectException) {\n        LOG.debug(\"Catch this; presume it means the cached connection has gone bad.\");\n      } else {\n        throw e;\n      }\n    } catch (SocketTimeoutException e) {\n      LOG.debug(\"Timed out connecting to \" + sn);\n    } catch (NoRouteToHostException e) {\n      LOG.debug(\"Connecting to \" + sn, e);\n    } catch (SocketException e) {\n      LOG.debug(\"Exception connecting to \" + sn);\n    } catch (UnknownHostException e) {\n      LOG.debug(\"Unknown host exception connecting to  \" + sn);\n    } catch (FailedServerException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Server \" + sn + \" is in failed server list.\");\n      }\n    } catch (IOException ioe) {\n      Throwable cause = ioe.getCause();\n      if (ioe instanceof ConnectException) {\n        LOG.debug(\"Catch. Connect refused.\");\n      } else if (cause != null && cause instanceof EOFException) {\n        LOG.debug(\"Catch. Other end disconnected us.\");\n      } else if (cause != null && cause.getMessage() != null &&\n        cause.getMessage().toLowerCase(Locale.ROOT).contains(\"connection reset\")) {\n        LOG.debug(\"Catch. Connection reset.\");\n      } else {\n        throw ioe;\n      }\n\n    }\n    return service;\n  }"
        ],
        [
            "RSGroupInfoManagerImpl::flushConfig(Map)",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491 -\n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  ",
            "  private synchronized void flushConfig(Map<String, RSGroupInfo> newGroupMap)\n  throws IOException {\n    Map<TableName, String> newTableMap;\n\n    // For offline mode persistence is still unavailable\n    // We're refreshing in-memory state but only for default servers\n    if (!isOnline()) {\n      Map<String, RSGroupInfo> m = Maps.newHashMap(rsGroupMap);\n      RSGroupInfo oldDefaultGroup = m.remove(RSGroupInfo.DEFAULT_GROUP);\n      RSGroupInfo newDefaultGroup = newGroupMap.remove(RSGroupInfo.DEFAULT_GROUP);\n      if (!m.equals(newGroupMap) ||\n          !oldDefaultGroup.getTables().equals(newDefaultGroup.getTables())) {\n        throw new IOException(\"Only default servers can be updated during offline mode\");\n      }\n      newGroupMap.put(RSGroupInfo.DEFAULT_GROUP, newDefaultGroup);\n      rsGroupMap = newGroupMap;\n      return;\n    }\n\n    newTableMap = flushConfigTable(newGroupMap);\n\n    // Make changes visible after having been persisted to the source of truth\n    resetRSGroupAndTableMaps(newGroupMap, newTableMap);\n\n    try {\n      String groupBasePath = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, rsGroupZNode);\n      ZKUtil.createAndFailSilent(watcher, groupBasePath, ProtobufMagic.PB_MAGIC);\n\n      List<ZKUtil.ZKUtilOp> zkOps = new ArrayList<>(newGroupMap.size());\n      for(String groupName : prevRSGroups) {\n        if(!newGroupMap.containsKey(groupName)) {\n          String znode = ZNodePaths.joinZNode(groupBasePath, groupName);\n          zkOps.add(ZKUtil.ZKUtilOp.deleteNodeFailSilent(znode));\n        }\n      }\n\n\n      for (RSGroupInfo RSGroupInfo : newGroupMap.values()) {\n        String znode = ZNodePaths.joinZNode(groupBasePath, RSGroupInfo.getName());\n        RSGroupProtos.RSGroupInfo proto = RSGroupProtobufUtil.toProtoGroupInfo(RSGroupInfo);\n        LOG.debug(\"Updating znode: \"+znode);\n        ZKUtil.createAndFailSilent(watcher, znode);\n        zkOps.add(ZKUtil.ZKUtilOp.deleteNodeFailSilent(znode));\n        zkOps.add(ZKUtil.ZKUtilOp.createAndFailSilent(znode,\n            ProtobufUtil.prependPBMagic(proto.toByteArray())));\n      }\n      LOG.debug(\"Writing ZK GroupInfo count: \" + zkOps.size());\n\n      ZKUtil.multiOrSequential(watcher, zkOps, false);\n    } catch (KeeperException e) {\n      LOG.error(\"Failed to write to rsGroupZNode\", e);\n      masterServices.abort(\"Failed to write to rsGroupZNode\", e);\n      throw new IOException(\"Failed to write to rsGroupZNode\",e);\n    }\n    updateCacheOfRSGroups(newGroupMap.keySet());\n  }",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491 +\n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  ",
            "  private synchronized void flushConfig(Map<String, RSGroupInfo> newGroupMap)\n  throws IOException {\n    Map<TableName, String> newTableMap;\n\n    // For offline mode persistence is still unavailable\n    // We're refreshing in-memory state but only for default servers\n    if (!isOnline()) {\n      Map<String, RSGroupInfo> m = Maps.newHashMap(rsGroupMap);\n      RSGroupInfo oldDefaultGroup = m.remove(RSGroupInfo.DEFAULT_GROUP);\n      RSGroupInfo newDefaultGroup = newGroupMap.remove(RSGroupInfo.DEFAULT_GROUP);\n      if (!m.equals(newGroupMap) ||\n          !oldDefaultGroup.getTables().equals(newDefaultGroup.getTables())) {\n        throw new IOException(\"Only default servers can be updated during offline mode\");\n      }\n      newGroupMap.put(RSGroupInfo.DEFAULT_GROUP, newDefaultGroup);\n      rsGroupMap = newGroupMap;\n      return;\n    }\n\n    newTableMap = flushConfigTable(newGroupMap);\n\n    // Make changes visible after having been persisted to the source of truth\n    resetRSGroupAndTableMaps(newGroupMap, newTableMap);\n\n    try {\n      String groupBasePath = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, rsGroupZNode);\n      ZKUtil.createAndFailSilent(watcher, groupBasePath, ProtobufMagic.PB_MAGIC);\n\n      List<ZKUtil.ZKUtilOp> zkOps = new ArrayList<>(newGroupMap.size());\n      for(String groupName : prevRSGroups) {\n        if(!newGroupMap.containsKey(groupName)) {\n          String znode = ZNodePaths.joinZNode(groupBasePath, groupName);\n          zkOps.add(ZKUtil.ZKUtilOp.deleteNodeFailSilent(znode));\n        }\n      }\n\n\n      for (RSGroupInfo RSGroupInfo : newGroupMap.values()) {\n        String znode = ZNodePaths.joinZNode(groupBasePath, RSGroupInfo.getName());\n        RSGroupProtos.RSGroupInfo proto = RSGroupProtobufUtil.toProtoGroupInfo(RSGroupInfo);\n        LOG.debug(\"Updating znode: \"+znode);\n        ZKUtil.createAndFailSilent(watcher, znode);\n        zkOps.add(ZKUtil.ZKUtilOp.deleteNodeFailSilent(znode));\n        zkOps.add(ZKUtil.ZKUtilOp.createAndFailSilent(znode,\n            ProtobufUtil.prependPBMagic(proto.toByteArray())));\n      }\n      LOG.debug(\"Writing ZK GroupInfo count: \" + zkOps.size());\n\n      ZKUtil.multiOrSequential(watcher, zkOps, false);\n    } catch (KeeperException e) {\n      LOG.error(\"Failed to write to rsGroupZNode\", e);\n      masterServices.abort(\"Failed to write to rsGroupZNode\", e);\n      throw new IOException(\"Failed to write to rsGroupZNode\",e);\n    }\n    updateCacheOfRSGroups(newGroupMap.keySet());\n  }"
        ],
        [
            "RecoverableZooKeeper::RecoverableZooKeeper(String,int,Watcher,int,int,int,String)",
            "  93  \n  94  \n  95  \n  96  \n  97 -\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"DE_MIGHT_IGNORE\",\n      justification=\"None. Its always been this way.\")\n  public RecoverableZooKeeper(String quorumServers, int sessionTimeout,\n      Watcher watcher, int maxRetries, int retryIntervalMillis, int maxSleepTime, String identifier)\n  throws IOException {\n    // TODO: Add support for zk 'chroot'; we don't add it to the quorumServers String as we should.\n    this.retryCounterFactory =\n      new RetryCounterFactory(maxRetries+1, retryIntervalMillis, maxSleepTime);\n\n    if (identifier == null || identifier.length() == 0) {\n      // the identifier = processID@hostName\n      identifier = ManagementFactory.getRuntimeMXBean().getName();\n    }\n    LOG.info(\"Process identifier=\" + identifier +\n      \" connecting to ZooKeeper ensemble=\" + quorumServers);\n    this.identifier = identifier;\n    this.id = Bytes.toBytes(identifier);\n\n    this.watcher = watcher;\n    this.sessionTimeout = sessionTimeout;\n    this.quorumServers = quorumServers;\n\n    try {\n      checkZk();\n    } catch (Exception x) {\n      /* ignore */\n    }\n  }",
            "  93  \n  94  \n  95  \n  96  \n  97 +\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"DE_MIGHT_IGNORE\",\n      justification=\"None. Its always been this way.\")\n  public RecoverableZooKeeper(String quorumServers, int sessionTimeout,\n      Watcher watcher, int maxRetries, int retryIntervalMillis, int maxSleepTime, String identifier)\n    throws IOException {\n    // TODO: Add support for zk 'chroot'; we don't add it to the quorumServers String as we should.\n    this.retryCounterFactory =\n      new RetryCounterFactory(maxRetries+1, retryIntervalMillis, maxSleepTime);\n\n    if (identifier == null || identifier.length() == 0) {\n      // the identifier = processID@hostName\n      identifier = ManagementFactory.getRuntimeMXBean().getName();\n    }\n    LOG.info(\"Process identifier=\" + identifier +\n      \" connecting to ZooKeeper ensemble=\" + quorumServers);\n    this.identifier = identifier;\n    this.id = Bytes.toBytes(identifier);\n\n    this.watcher = watcher;\n    this.sessionTimeout = sessionTimeout;\n    this.quorumServers = quorumServers;\n\n    try {\n      checkZk();\n    } catch (Exception x) {\n      /* ignore */\n    }\n  }"
        ],
        [
            "ZKVisibilityLabelWatcher::ZKVisibilityLabelWatcher(ZKWatcher,VisibilityLabelsCache,Configuration)",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58 -\n  59  ",
            "  public ZKVisibilityLabelWatcher(ZKWatcher watcher, VisibilityLabelsCache labelsCache,\n                                  Configuration conf) {\n    super(watcher);\n    this.labelsCache = labelsCache;\n    String labelZnodeParent = conf.get(VISIBILITY_LABEL_ZK_PATH, DEFAULT_VISIBILITY_LABEL_NODE);\n    String userAuthsZnodeParent = conf.get(VISIBILITY_USER_AUTHS_ZK_PATH,\n        DEFAULT_VISIBILITY_USER_AUTHS_NODE);\n    this.labelZnode = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, labelZnodeParent);\n    this.userAuthsZnode = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, userAuthsZnodeParent);\n  }",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58 +\n  59 +\n  60  ",
            "  public ZKVisibilityLabelWatcher(ZKWatcher watcher, VisibilityLabelsCache labelsCache,\n                                  Configuration conf) {\n    super(watcher);\n    this.labelsCache = labelsCache;\n    String labelZnodeParent = conf.get(VISIBILITY_LABEL_ZK_PATH, DEFAULT_VISIBILITY_LABEL_NODE);\n    String userAuthsZnodeParent = conf.get(VISIBILITY_USER_AUTHS_ZK_PATH,\n        DEFAULT_VISIBILITY_USER_AUTHS_NODE);\n    this.labelZnode = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, labelZnodeParent);\n    this.userAuthsZnode = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode,\n            userAuthsZnodeParent);\n  }"
        ],
        [
            "RegionServerTracker::refresh(List)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 -\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  ",
            "  private void refresh(final List<String> servers) throws IOException {\n    synchronized(this.regionServers) {\n      this.regionServers.clear();\n      for (String n: servers) {\n        ServerName sn = ServerName.parseServerName(ZKUtil.getNodeName(n));\n        if (regionServers.get(sn) == null) {\n          RegionServerInfo.Builder rsInfoBuilder = RegionServerInfo.newBuilder();\n          try {\n            String nodePath = ZNodePaths.joinZNode(watcher.znodePaths.rsZNode, n);\n            byte[] data = ZKUtil.getData(watcher, nodePath);\n            if (data != null && data.length > 0 && ProtobufUtil.isPBMagicPrefix(data)) {\n              int magicLen = ProtobufUtil.lengthOfPBMagic();\n              ProtobufUtil.mergeFrom(rsInfoBuilder, data, magicLen, data.length - magicLen);\n            }\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Added tracking of RS \" + nodePath);\n            }\n          } catch (KeeperException e) {\n            LOG.warn(\"Get Rs info port from ephemeral node\", e);\n          } catch (IOException e) {\n            LOG.warn(\"Illegal data from ephemeral node\", e);\n          } catch (InterruptedException e) {\n            throw new InterruptedIOException();\n          }\n          this.regionServers.put(sn, rsInfoBuilder.build());\n        }\n      }\n    }\n  }",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  ",
            "  private void refresh(final List<String> servers) throws IOException {\n    synchronized(this.regionServers) {\n      this.regionServers.clear();\n      for (String n: servers) {\n        ServerName sn = ServerName.parseServerName(ZKUtil.getNodeName(n));\n        if (regionServers.get(sn) == null) {\n          RegionServerInfo.Builder rsInfoBuilder = RegionServerInfo.newBuilder();\n          try {\n            String nodePath = ZNodePaths.joinZNode(watcher.getZNodePaths().rsZNode, n);\n            byte[] data = ZKUtil.getData(watcher, nodePath);\n            if (data != null && data.length > 0 && ProtobufUtil.isPBMagicPrefix(data)) {\n              int magicLen = ProtobufUtil.lengthOfPBMagic();\n              ProtobufUtil.mergeFrom(rsInfoBuilder, data, magicLen, data.length - magicLen);\n            }\n            if (LOG.isTraceEnabled()) {\n              LOG.trace(\"Added tracking of RS \" + nodePath);\n            }\n          } catch (KeeperException e) {\n            LOG.warn(\"Get Rs info port from ephemeral node\", e);\n          } catch (IOException e) {\n            LOG.warn(\"Illegal data from ephemeral node\", e);\n          } catch (InterruptedException e) {\n            throw new InterruptedIOException();\n          }\n          this.regionServers.put(sn, rsInfoBuilder.build());\n        }\n      }\n    }\n  }"
        ],
        [
            "ZKNamespaceManager::ZKNamespaceManager(ZKWatcher)",
            "  56  \n  57  \n  58 -\n  59  \n  60  ",
            "  public ZKNamespaceManager(ZKWatcher zkw) throws IOException {\n    super(zkw);\n    nsZNode = zkw.znodePaths.namespaceZNode;\n    cache = new ConcurrentSkipListMap<>();\n  }",
            "  56  \n  57  \n  58 +\n  59  \n  60  ",
            "  public ZKNamespaceManager(ZKWatcher zkw) throws IOException {\n    super(zkw);\n    nsZNode = zkw.getZNodePaths().namespaceZNode;\n    cache = new ConcurrentSkipListMap<>();\n  }"
        ],
        [
            "ActiveMasterManager::blockUntilBecomingActiveMaster(int,MonitoredTask)",
            " 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 -\n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 -\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 -\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "  /**\n   * Block until becoming the active master.\n   *\n   * Method blocks until there is not another active master and our attempt\n   * to become the new active master is successful.\n   *\n   * This also makes sure that we are watching the master znode so will be\n   * notified if another master dies.\n   * @param checkInterval the interval to check if the master is stopped\n   * @param startupStatus the monitor status to track the progress\n   * @return True if no issue becoming active master else false if another\n   * master was running or if some other problem (zookeeper, stop flag has been\n   * set on this Master)\n   */\n  boolean blockUntilBecomingActiveMaster(\n      int checkInterval, MonitoredTask startupStatus) {\n    String backupZNode = ZNodePaths.joinZNode(\n      this.watcher.znodePaths.backupMasterAddressesZNode, this.sn.toString());\n    while (!(master.isAborted() || master.isStopped())) {\n      startupStatus.setStatus(\"Trying to register in ZK as active master\");\n      // Try to become the active master, watch if there is another master.\n      // Write out our ServerName as versioned bytes.\n      try {\n        if (MasterAddressTracker.setMasterAddress(this.watcher,\n            this.watcher.znodePaths.masterAddressZNode, this.sn, infoPort)) {\n\n          // If we were a backup master before, delete our ZNode from the backup\n          // master directory since we are the active now)\n          if (ZKUtil.checkExists(this.watcher, backupZNode) != -1) {\n            LOG.info(\"Deleting ZNode for \" + backupZNode + \" from backup master directory\");\n            ZKUtil.deleteNodeFailSilent(this.watcher, backupZNode);\n          }\n          // Save the znode in a file, this will allow to check if we crash in the launch scripts\n          ZNodeClearer.writeMyEphemeralNodeOnDisk(this.sn.toString());\n\n          // We are the master, return\n          startupStatus.setStatus(\"Successfully registered as active master.\");\n          this.clusterHasActiveMaster.set(true);\n          LOG.info(\"Registered as active master=\" + this.sn);\n          return true;\n        }\n\n        // There is another active master running elsewhere or this is a restart\n        // and the master ephemeral node has not expired yet.\n        this.clusterHasActiveMaster.set(true);\n\n        String msg;\n        byte[] bytes =\n          ZKUtil.getDataAndWatch(this.watcher, this.watcher.znodePaths.masterAddressZNode);\n        if (bytes == null) {\n          msg = (\"A master was detected, but went down before its address \" +\n            \"could be read.  Attempting to become the next active master\");\n        } else {\n          ServerName currentMaster;\n          try {\n            currentMaster = ProtobufUtil.parseServerNameFrom(bytes);\n          } catch (DeserializationException e) {\n            LOG.warn(\"Failed parse\", e);\n            // Hopefully next time around we won't fail the parse.  Dangerous.\n            continue;\n          }\n          if (ServerName.isSameAddress(currentMaster, this.sn)) {\n            msg = (\"Current master has this master's address, \" +\n              currentMaster + \"; master was restarted? Deleting node.\");\n            // Hurry along the expiration of the znode.\n            ZKUtil.deleteNode(this.watcher, this.watcher.znodePaths.masterAddressZNode);\n\n            // We may have failed to delete the znode at the previous step, but\n            //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n            ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n          } else {\n            msg = \"Another master is the active master, \" + currentMaster +\n              \"; waiting to become the next active master\";\n          }\n        }\n        LOG.info(msg);\n        startupStatus.setStatus(msg);\n      } catch (KeeperException ke) {\n        master.abort(\"Received an unexpected KeeperException, aborting\", ke);\n        return false;\n      }\n      synchronized (this.clusterHasActiveMaster) {\n        while (clusterHasActiveMaster.get() && !master.isStopped()) {\n          try {\n            clusterHasActiveMaster.wait(checkInterval);\n          } catch (InterruptedException e) {\n            // We expect to be interrupted when a master dies,\n            //  will fall out if so\n            LOG.debug(\"Interrupted waiting for master to die\", e);\n          }\n        }\n        if (clusterShutDown.get()) {\n          this.master.stop(\n            \"Cluster went down before this master became active\");\n        }\n      }\n    }\n    return false;\n  }",
            " 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 +\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 +\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "  /**\n   * Block until becoming the active master.\n   *\n   * Method blocks until there is not another active master and our attempt\n   * to become the new active master is successful.\n   *\n   * This also makes sure that we are watching the master znode so will be\n   * notified if another master dies.\n   * @param checkInterval the interval to check if the master is stopped\n   * @param startupStatus the monitor status to track the progress\n   * @return True if no issue becoming active master else false if another\n   * master was running or if some other problem (zookeeper, stop flag has been\n   * set on this Master)\n   */\n  boolean blockUntilBecomingActiveMaster(\n      int checkInterval, MonitoredTask startupStatus) {\n    String backupZNode = ZNodePaths.joinZNode(\n      this.watcher.getZNodePaths().backupMasterAddressesZNode, this.sn.toString());\n    while (!(master.isAborted() || master.isStopped())) {\n      startupStatus.setStatus(\"Trying to register in ZK as active master\");\n      // Try to become the active master, watch if there is another master.\n      // Write out our ServerName as versioned bytes.\n      try {\n        if (MasterAddressTracker.setMasterAddress(this.watcher,\n            this.watcher.getZNodePaths().masterAddressZNode, this.sn, infoPort)) {\n\n          // If we were a backup master before, delete our ZNode from the backup\n          // master directory since we are the active now)\n          if (ZKUtil.checkExists(this.watcher, backupZNode) != -1) {\n            LOG.info(\"Deleting ZNode for \" + backupZNode + \" from backup master directory\");\n            ZKUtil.deleteNodeFailSilent(this.watcher, backupZNode);\n          }\n          // Save the znode in a file, this will allow to check if we crash in the launch scripts\n          ZNodeClearer.writeMyEphemeralNodeOnDisk(this.sn.toString());\n\n          // We are the master, return\n          startupStatus.setStatus(\"Successfully registered as active master.\");\n          this.clusterHasActiveMaster.set(true);\n          LOG.info(\"Registered as active master=\" + this.sn);\n          return true;\n        }\n\n        // There is another active master running elsewhere or this is a restart\n        // and the master ephemeral node has not expired yet.\n        this.clusterHasActiveMaster.set(true);\n\n        String msg;\n        byte[] bytes =\n          ZKUtil.getDataAndWatch(this.watcher, this.watcher.getZNodePaths().masterAddressZNode);\n        if (bytes == null) {\n          msg = (\"A master was detected, but went down before its address \" +\n            \"could be read.  Attempting to become the next active master\");\n        } else {\n          ServerName currentMaster;\n          try {\n            currentMaster = ProtobufUtil.parseServerNameFrom(bytes);\n          } catch (DeserializationException e) {\n            LOG.warn(\"Failed parse\", e);\n            // Hopefully next time around we won't fail the parse.  Dangerous.\n            continue;\n          }\n          if (ServerName.isSameAddress(currentMaster, this.sn)) {\n            msg = (\"Current master has this master's address, \" +\n              currentMaster + \"; master was restarted? Deleting node.\");\n            // Hurry along the expiration of the znode.\n            ZKUtil.deleteNode(this.watcher, this.watcher.getZNodePaths().masterAddressZNode);\n\n            // We may have failed to delete the znode at the previous step, but\n            //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n            ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n          } else {\n            msg = \"Another master is the active master, \" + currentMaster +\n              \"; waiting to become the next active master\";\n          }\n        }\n        LOG.info(msg);\n        startupStatus.setStatus(msg);\n      } catch (KeeperException ke) {\n        master.abort(\"Received an unexpected KeeperException, aborting\", ke);\n        return false;\n      }\n      synchronized (this.clusterHasActiveMaster) {\n        while (clusterHasActiveMaster.get() && !master.isStopped()) {\n          try {\n            clusterHasActiveMaster.wait(checkInterval);\n          } catch (InterruptedException e) {\n            // We expect to be interrupted when a master dies,\n            //  will fall out if so\n            LOG.debug(\"Interrupted waiting for master to die\", e);\n          }\n        }\n        if (clusterShutDown.get()) {\n          this.master.stop(\n            \"Cluster went down before this master became active\");\n        }\n      }\n    }\n    return false;\n  }"
        ],
        [
            "TestMetaWithReplicas::testZookeeperNodesForReplicas()",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182 -\n 183  \n 184  \n 185  \n 186  \n 187  \n 188  ",
            "  @Test\n  public void testZookeeperNodesForReplicas() throws Exception {\n    // Checks all the znodes exist when meta's replicas are enabled\n    ZKWatcher zkw = TEST_UTIL.getZooKeeperWatcher();\n    Configuration conf = TEST_UTIL.getConfiguration();\n    String baseZNode = conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT,\n        HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT);\n    String primaryMetaZnode = ZNodePaths.joinZNode(baseZNode,\n        conf.get(\"zookeeper.znode.metaserver\", \"meta-region-server\"));\n    // check that the data in the znode is parseable (this would also mean the znode exists)\n    byte[] data = ZKUtil.getData(zkw, primaryMetaZnode);\n    ProtobufUtil.toServerName(data);\n    for (int i = 1; i < 3; i++) {\n      String secZnode = ZNodePaths.joinZNode(baseZNode,\n          conf.get(\"zookeeper.znode.metaserver\", \"meta-region-server\") + \"-\" + i);\n      String str = zkw.znodePaths.getZNodeForReplica(i);\n      assertTrue(str.equals(secZnode));\n      // check that the data in the znode is parseable (this would also mean the znode exists)\n      data = ZKUtil.getData(zkw, secZnode);\n      ProtobufUtil.toServerName(data);\n    }\n  }",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182 +\n 183  \n 184  \n 185  \n 186  \n 187  \n 188  ",
            "  @Test\n  public void testZookeeperNodesForReplicas() throws Exception {\n    // Checks all the znodes exist when meta's replicas are enabled\n    ZKWatcher zkw = TEST_UTIL.getZooKeeperWatcher();\n    Configuration conf = TEST_UTIL.getConfiguration();\n    String baseZNode = conf.get(HConstants.ZOOKEEPER_ZNODE_PARENT,\n        HConstants.DEFAULT_ZOOKEEPER_ZNODE_PARENT);\n    String primaryMetaZnode = ZNodePaths.joinZNode(baseZNode,\n        conf.get(\"zookeeper.znode.metaserver\", \"meta-region-server\"));\n    // check that the data in the znode is parseable (this would also mean the znode exists)\n    byte[] data = ZKUtil.getData(zkw, primaryMetaZnode);\n    ProtobufUtil.toServerName(data);\n    for (int i = 1; i < 3; i++) {\n      String secZnode = ZNodePaths.joinZNode(baseZNode,\n          conf.get(\"zookeeper.znode.metaserver\", \"meta-region-server\") + \"-\" + i);\n      String str = zkw.getZNodePaths().getZNodeForReplica(i);\n      assertTrue(str.equals(secZnode));\n      // check that the data in the znode is parseable (this would also mean the znode exists)\n      data = ZKUtil.getData(zkw, secZnode);\n      ProtobufUtil.toServerName(data);\n    }\n  }"
        ],
        [
            "RegionServerTracker::start()",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  ",
            "  /**\n   * Starts the tracking of online RegionServers.\n   *\n   * <p>All RSs will be tracked after this method is called.\n   *\n   * @throws KeeperException\n   * @throws IOException\n   */\n  public void start() throws KeeperException, IOException {\n    watcher.registerListener(this);\n    List<String> servers =\n      ZKUtil.listChildrenAndWatchThem(watcher, watcher.znodePaths.rsZNode);\n    refresh(servers);\n  }",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76  \n  77  ",
            "  /**\n   * Starts the tracking of online RegionServers.\n   *\n   * <p>All RSs will be tracked after this method is called.\n   *\n   * @throws KeeperException\n   * @throws IOException\n   */\n  public void start() throws KeeperException, IOException {\n    watcher.registerListener(this);\n    List<String> servers =\n      ZKUtil.listChildrenAndWatchThem(watcher, watcher.getZNodePaths().rsZNode);\n    refresh(servers);\n  }"
        ],
        [
            "RecoverableZooKeeper::setData(String,byte,int)",
            " 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398 -\n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  ",
            "  /**\n   * setData is NOT an idempotent operation. Retry may cause BadVersion Exception\n   * Adding an identifier field into the data to check whether\n   * badversion is caused by the result of previous correctly setData\n   * @return Stat instance\n   */\n  public Stat setData(String path, byte[] data, int version)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.setData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      byte[] newData = ZKMetadata.appendMetaData(id, data);\n      boolean isRetry = false;\n      long startTime;\n      while (true) {\n        try {\n          startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().setData(path, newData, version);\n          return nodeStat;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"setData\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"setData\");\n              break;\n            case BADVERSION:\n              if (isRetry) {\n                // try to verify whether the previous setData success or not\n                try{\n                  Stat stat = new Stat();\n                  byte[] revData = checkZk().getData(path, false, stat);\n                  if(Bytes.compareTo(revData, newData) == 0) {\n                    // the bad version is caused by previous successful setData\n                    return stat;\n                  }\n                } catch(KeeperException keeperException){\n                  // the ZK is not reliable at this moment. just throwing exception\n                  throw keeperException;\n                }\n              }\n            // throw other exceptions and verified bad version exceptions\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n        isRetry = true;\n      }\n    }\n  }",
            " 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398 +\n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  ",
            "  /**\n   * setData is NOT an idempotent operation. Retry may cause BadVersion Exception\n   * Adding an identifier field into the data to check whether\n   * badversion is caused by the result of previous correctly setData\n   * @return Stat instance\n   */\n  public Stat setData(String path, byte[] data, int version)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.setData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      byte[] newData = ZKMetadata.appendMetaData(id, data);\n      boolean isRetry = false;\n      long startTime;\n      while (true) {\n        try {\n          startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().setData(path, newData, version);\n          return nodeStat;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"setData\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"setData\");\n              break;\n            case BADVERSION:\n              if (isRetry) {\n                // try to verify whether the previous setData success or not\n                try{\n                  Stat stat = new Stat();\n                  byte[] revData = checkZk().getData(path, false, stat);\n                  if(Bytes.compareTo(revData, newData) == 0) {\n                    // the bad version is caused by previous successful setData\n                    return stat;\n                  }\n                } catch(KeeperException keeperException){\n                  // the ZK is not reliable at this moment. just throwing exception\n                  throw keeperException;\n                }\n              }\n            // throw other exceptions and verified bad version exceptions\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n        isRetry = true;\n      }\n    }\n  }"
        ],
        [
            "TestReplicationStateZKImpl::initPeerClusterState(String)",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72 -\n  73  \n  74  \n  75  \n  76  ",
            "  private static String initPeerClusterState(String baseZKNode)\n      throws IOException, KeeperException {\n    // Add a dummy region server and set up the cluster id\n    Configuration testConf = new Configuration(conf);\n    testConf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, baseZKNode);\n    ZKWatcher zkw1 = new ZKWatcher(testConf, \"test1\", null);\n    String fakeRs = ZNodePaths.joinZNode(zkw1.znodePaths.rsZNode, \"hostname1.example.org:1234\");\n    ZKUtil.createWithParents(zkw1, fakeRs);\n    ZKClusterId.setClusterId(zkw1, new ClusterId());\n    return ZKConfig.getZooKeeperClusterKey(testConf);\n  }",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72 +\n  73 +\n  74  \n  75  \n  76  \n  77  ",
            "  private static String initPeerClusterState(String baseZKNode)\n      throws IOException, KeeperException {\n    // Add a dummy region server and set up the cluster id\n    Configuration testConf = new Configuration(conf);\n    testConf.set(HConstants.ZOOKEEPER_ZNODE_PARENT, baseZKNode);\n    ZKWatcher zkw1 = new ZKWatcher(testConf, \"test1\", null);\n    String fakeRs = ZNodePaths.joinZNode(zkw1.getZNodePaths().rsZNode,\n            \"hostname1.example.org:1234\");\n    ZKUtil.createWithParents(zkw1, fakeRs);\n    ZKClusterId.setClusterId(zkw1, new ClusterId());\n    return ZKConfig.getZooKeeperClusterKey(testConf);\n  }"
        ],
        [
            "TestReplicationTrackerZKImpl::testRegionServerRemovedEvent()",
            " 137  \n 138  \n 139  \n 140 -\n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "  @Test\n  public void testRegionServerRemovedEvent() throws Exception {\n    ZKUtil.createAndWatch(zkw,\n      ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, \"hostname2.example.org:1234\"),\n      HConstants.EMPTY_BYTE_ARRAY);\n    rt.registerListener(new DummyReplicationListener());\n    // delete one\n    ZKUtil.deleteNode(zkw,\n      ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, \"hostname2.example.org:1234\"));\n    // wait for event\n    while (rsRemovedCount.get() < 1) {\n      Thread.sleep(5);\n    }\n    assertEquals(\"hostname2.example.org:1234\", rsRemovedData);\n  }",
            " 138  \n 139  \n 140  \n 141 +\n 142  \n 143  \n 144  \n 145  \n 146 +\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "  @Test\n  public void testRegionServerRemovedEvent() throws Exception {\n    ZKUtil.createAndWatch(zkw,\n      ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, \"hostname2.example.org:1234\"),\n      HConstants.EMPTY_BYTE_ARRAY);\n    rt.registerListener(new DummyReplicationListener());\n    // delete one\n    ZKUtil.deleteNode(zkw,\n      ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, \"hostname2.example.org:1234\"));\n    // wait for event\n    while (rsRemovedCount.get() < 1) {\n      Thread.sleep(5);\n    }\n    assertEquals(\"hostname2.example.org:1234\", rsRemovedData);\n  }"
        ],
        [
            "ZNodeClearer::clear(Configuration)",
            " 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 -\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  ",
            "  /**\n   * Delete the master znode if its content (ServerName string) is the same\n   *  as the one in the znode file. (env: HBASE_ZNODE_FILE). I case of master-rs\n   *  colloaction we extract ServerName string from rsZnode path.(HBASE-14861)\n   * @return true on successful deletion, false otherwise.\n   */\n  public static boolean clear(Configuration conf) {\n    Configuration tempConf = new Configuration(conf);\n    tempConf.setInt(\"zookeeper.recovery.retry\", 0);\n\n    ZKWatcher zkw;\n    try {\n      zkw = new ZKWatcher(tempConf, \"clean znode for master\",\n          new Abortable() {\n            @Override public void abort(String why, Throwable e) {}\n            @Override public boolean isAborted() { return false; }\n          });\n    } catch (IOException e) {\n      LOG.warn(\"Can't connect to zookeeper to read the master znode\", e);\n      return false;\n    }\n\n    String znodeFileContent;\n    try {\n      znodeFileContent = ZNodeClearer.readMyEphemeralNodeOnDisk();\n      if (ZNodeClearer.tablesOnMaster(conf)) {\n        // In case of master crash also remove rsZnode since master is also regionserver\n        ZKUtil.deleteNodeFailSilent(zkw,\n          ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, znodeFileContent));\n        return MasterAddressTracker.deleteIfEquals(zkw,\n          ZNodeClearer.parseMasterServerName(znodeFileContent));\n      } else {\n        return MasterAddressTracker.deleteIfEquals(zkw, znodeFileContent);\n      }\n    } catch (FileNotFoundException fnfe) {\n      // If no file, just keep going -- return success.\n      LOG.warn(\"Can't find the znode file; presume non-fatal\", fnfe);\n      return true;\n    } catch (IOException e) {\n      LOG.warn(\"Can't read the content of the znode file\", e);\n      return false;\n    } catch (KeeperException e) {\n      LOG.warn(\"ZooKeeper exception deleting znode\", e);\n      return false;\n    } finally {\n      zkw.close();\n    }\n  }",
            " 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  ",
            "  /**\n   * Delete the master znode if its content (ServerName string) is the same\n   *  as the one in the znode file. (env: HBASE_ZNODE_FILE). I case of master-rs\n   *  colloaction we extract ServerName string from rsZnode path.(HBASE-14861)\n   * @return true on successful deletion, false otherwise.\n   */\n  public static boolean clear(Configuration conf) {\n    Configuration tempConf = new Configuration(conf);\n    tempConf.setInt(\"zookeeper.recovery.retry\", 0);\n\n    ZKWatcher zkw;\n    try {\n      zkw = new ZKWatcher(tempConf, \"clean znode for master\",\n          new Abortable() {\n            @Override public void abort(String why, Throwable e) {}\n            @Override public boolean isAborted() { return false; }\n          });\n    } catch (IOException e) {\n      LOG.warn(\"Can't connect to zookeeper to read the master znode\", e);\n      return false;\n    }\n\n    String znodeFileContent;\n    try {\n      znodeFileContent = ZNodeClearer.readMyEphemeralNodeOnDisk();\n      if (ZNodeClearer.tablesOnMaster(conf)) {\n        // In case of master crash also remove rsZnode since master is also regionserver\n        ZKUtil.deleteNodeFailSilent(zkw,\n          ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, znodeFileContent));\n        return MasterAddressTracker.deleteIfEquals(zkw,\n          ZNodeClearer.parseMasterServerName(znodeFileContent));\n      } else {\n        return MasterAddressTracker.deleteIfEquals(zkw, znodeFileContent);\n      }\n    } catch (FileNotFoundException fnfe) {\n      // If no file, just keep going -- return success.\n      LOG.warn(\"Can't find the znode file; presume non-fatal\", fnfe);\n      return true;\n    } catch (IOException e) {\n      LOG.warn(\"Can't read the content of the znode file\", e);\n      return false;\n    } catch (KeeperException e) {\n      LOG.warn(\"ZooKeeper exception deleting znode\", e);\n      return false;\n    } finally {\n      zkw.close();\n    }\n  }"
        ],
        [
            "TestMirroringTableStateManager::getTableStateInZK(ZKWatcher,TableName)",
            "  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "  private TableState.State getTableStateInZK(ZKWatcher watcher, final TableName tableName)\n      throws KeeperException, IOException, InterruptedException {\n    String znode = ZNodePaths.joinZNode(watcher.znodePaths.tableZNode, tableName.getNameAsString());\n    byte [] data = ZKUtil.getData(watcher, znode);\n    if (data == null || data.length <= 0) {\n      return null;\n    }\n    try {\n      ProtobufUtil.expectPBMagicPrefix(data);\n      ZooKeeperProtos.DeprecatedTableState.Builder builder =\n          ZooKeeperProtos.DeprecatedTableState.newBuilder();\n      int magicLen = ProtobufUtil.lengthOfPBMagic();\n      ProtobufUtil.mergeFrom(builder, data, magicLen, data.length - magicLen);\n      return TableState.State.valueOf(builder.getState().toString());\n    } catch (IOException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    } catch (DeserializationException e) {\n      throw ZKUtil.convert(e);\n    }\n  }",
            "  83  \n  84  \n  85 +\n  86 +\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  ",
            "  private TableState.State getTableStateInZK(ZKWatcher watcher, final TableName tableName)\n      throws KeeperException, IOException, InterruptedException {\n    String znode = ZNodePaths.joinZNode(watcher.getZNodePaths().tableZNode,\n            tableName.getNameAsString());\n    byte [] data = ZKUtil.getData(watcher, znode);\n    if (data == null || data.length <= 0) {\n      return null;\n    }\n    try {\n      ProtobufUtil.expectPBMagicPrefix(data);\n      ZooKeeperProtos.DeprecatedTableState.Builder builder =\n          ZooKeeperProtos.DeprecatedTableState.newBuilder();\n      int magicLen = ProtobufUtil.lengthOfPBMagic();\n      ProtobufUtil.mergeFrom(builder, data, magicLen, data.length - magicLen);\n      return TableState.State.valueOf(builder.getState().toString());\n    } catch (IOException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    } catch (DeserializationException e) {\n      throw ZKUtil.convert(e);\n    }\n  }"
        ],
        [
            "TestZKMulti::testRunSequentialOnMultiFailure()",
            " 270  \n 271  \n 272 -\n 273 -\n 274 -\n 275 -\n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  ",
            "  @Test\n  public void testRunSequentialOnMultiFailure() throws Exception {\n    String path1 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"runSequential1\");\n    String path2 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"runSequential2\");\n    String path3 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"runSequential3\");\n    String path4 = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"runSequential4\");\n\n    // create some nodes that we will use later\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(path1, Bytes.toBytes(path1)));\n    ops.add(ZKUtilOp.createAndFailSilent(path2, Bytes.toBytes(path2)));\n    ZKUtil.multiOrSequential(zkw, ops, false);\n\n    // test that, even with operations that fail, the ones that would pass will pass\n    // with runSequentialOnMultiFailure\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.setData(path1, Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1)))); // pass\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path2)); // pass\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path3)); // fail -- node doesn't exist\n    ops.add(ZKUtilOp.createAndFailSilent(path4,\n      Bytes.add(Bytes.toBytes(path4), Bytes.toBytes(path4)))); // pass\n    ZKUtil.multiOrSequential(zkw, ops, true);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path1),\n      Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1))));\n    assertTrue(ZKUtil.checkExists(zkw, path2) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, path3) == -1);\n    assertFalse(ZKUtil.checkExists(zkw, path4) == -1);\n  }",
            " 270  \n 271  \n 272 +\n 273 +\n 274 +\n 275 +\n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  ",
            "  @Test\n  public void testRunSequentialOnMultiFailure() throws Exception {\n    String path1 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"runSequential1\");\n    String path2 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"runSequential2\");\n    String path3 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"runSequential3\");\n    String path4 = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"runSequential4\");\n\n    // create some nodes that we will use later\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(path1, Bytes.toBytes(path1)));\n    ops.add(ZKUtilOp.createAndFailSilent(path2, Bytes.toBytes(path2)));\n    ZKUtil.multiOrSequential(zkw, ops, false);\n\n    // test that, even with operations that fail, the ones that would pass will pass\n    // with runSequentialOnMultiFailure\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.setData(path1, Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1)))); // pass\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path2)); // pass\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path3)); // fail -- node doesn't exist\n    ops.add(ZKUtilOp.createAndFailSilent(path4,\n      Bytes.add(Bytes.toBytes(path4), Bytes.toBytes(path4)))); // pass\n    ZKUtil.multiOrSequential(zkw, ops, true);\n    assertTrue(Bytes.equals(ZKUtil.getData(zkw, path1),\n      Bytes.add(Bytes.toBytes(path1), Bytes.toBytes(path1))));\n    assertTrue(ZKUtil.checkExists(zkw, path2) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, path3) == -1);\n    assertFalse(ZKUtil.checkExists(zkw, path4) == -1);\n  }"
        ],
        [
            "RecoverableZooKeeper::create(String,byte,List,CreateMode)",
            " 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522 -\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  ",
            "  /**\n   * <p>\n   * NONSEQUENTIAL create is idempotent operation.\n   * Retry before throwing exceptions.\n   * But this function will not throw the NodeExist exception back to the\n   * application.\n   * </p>\n   * <p>\n   * But SEQUENTIAL is NOT idempotent operation. It is necessary to add\n   * identifier to the path to verify, whether the previous one is successful\n   * or not.\n   * </p>\n   *\n   * @return Path\n   */\n  public String create(String path, byte[] data, List<ACL> acl,\n      CreateMode createMode)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.create\")) {\n      byte[] newData = ZKMetadata.appendMetaData(id, data);\n      switch (createMode) {\n        case EPHEMERAL:\n        case PERSISTENT:\n          return createNonSequential(path, newData, acl, createMode);\n\n        case EPHEMERAL_SEQUENTIAL:\n        case PERSISTENT_SEQUENTIAL:\n          return createSequential(path, newData, acl, createMode);\n\n        default:\n          throw new IllegalArgumentException(\"Unrecognized CreateMode: \" +\n              createMode);\n      }\n    }\n  }",
            " 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522 +\n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  ",
            "  /**\n   * <p>\n   * NONSEQUENTIAL create is idempotent operation.\n   * Retry before throwing exceptions.\n   * But this function will not throw the NodeExist exception back to the\n   * application.\n   * </p>\n   * <p>\n   * But SEQUENTIAL is NOT idempotent operation. It is necessary to add\n   * identifier to the path to verify, whether the previous one is successful\n   * or not.\n   * </p>\n   *\n   * @return Path\n   */\n  public String create(String path, byte[] data, List<ACL> acl,\n      CreateMode createMode)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.create\")) {\n      byte[] newData = ZKMetadata.appendMetaData(id, data);\n      switch (createMode) {\n        case EPHEMERAL:\n        case PERSISTENT:\n          return createNonSequential(path, newData, acl, createMode);\n\n        case EPHEMERAL_SEQUENTIAL:\n        case PERSISTENT_SEQUENTIAL:\n          return createSequential(path, newData, acl, createMode);\n\n        default:\n          throw new IllegalArgumentException(\"Unrecognized CreateMode: \" +\n              createMode);\n      }\n    }\n  }"
        ],
        [
            "MetaTableLocator::deleteMetaLocation(ZKWatcher,int)",
            " 539  \n 540 -\n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548 -\n 549  \n 550  \n 551  \n 552  ",
            "  public void deleteMetaLocation(ZKWatcher zookeeper, int replicaId)\n  throws KeeperException {\n    if (replicaId == RegionInfo.DEFAULT_REPLICA_ID) {\n      LOG.info(\"Deleting hbase:meta region location in ZooKeeper\");\n    } else {\n      LOG.info(\"Deleting hbase:meta for \" + replicaId + \" region location in ZooKeeper\");\n    }\n    try {\n      // Just delete the node.  Don't need any watches.\n      ZKUtil.deleteNode(zookeeper, zookeeper.znodePaths.getZNodeForReplica(replicaId));\n    } catch(KeeperException.NoNodeException nne) {\n      // Has already been deleted\n    }\n  }",
            " 540  \n 541 +\n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549 +\n 550  \n 551  \n 552  \n 553  ",
            "  public void deleteMetaLocation(ZKWatcher zookeeper, int replicaId)\n    throws KeeperException {\n    if (replicaId == RegionInfo.DEFAULT_REPLICA_ID) {\n      LOG.info(\"Deleting hbase:meta region location in ZooKeeper\");\n    } else {\n      LOG.info(\"Deleting hbase:meta for \" + replicaId + \" region location in ZooKeeper\");\n    }\n    try {\n      // Just delete the node.  Don't need any watches.\n      ZKUtil.deleteNode(zookeeper, zookeeper.getZNodePaths().getZNodeForReplica(replicaId));\n    } catch(KeeperException.NoNodeException nne) {\n      // Has already been deleted\n    }\n  }"
        ],
        [
            "ZKUtil::listChildrenAndWatchForNewChildren(ZKWatcher,String)",
            " 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441 -\n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  ",
            "  /**\n   * Lists the children znodes of the specified znode.  Also sets a watch on\n   * the specified znode which will capture a NodeDeleted event on the specified\n   * znode as well as NodeChildrenChanged if any children of the specified znode\n   * are created or deleted.\n   *\n   * Returns null if the specified node does not exist.  Otherwise returns a\n   * list of children of the specified node.  If the node exists but it has no\n   * children, an empty list will be returned.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to list and watch children of\n   * @return list of children of the specified node, an empty list if the node\n   *          exists but has no children, and null if the node does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static List<String> listChildrenAndWatchForNewChildren(\n          ZKWatcher zkw, String znode)\n  throws KeeperException {\n    try {\n      List<String> children = zkw.getRecoverableZooKeeper().getChildren(znode, zkw);\n      return children;\n    } catch(KeeperException.NoNodeException ke) {\n      LOG.debug(zkw.prefix(\"Unable to list children of znode \" + znode + \" \" +\n          \"because node does not exist (not an error)\"));\n      return null;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode + \" \"), e);\n      zkw.keeperException(e);\n      return null;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode + \" \"), e);\n      zkw.interruptedException(e);\n      return null;\n    }\n  }",
            " 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441 +\n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  ",
            "  /**\n   * Lists the children znodes of the specified znode.  Also sets a watch on\n   * the specified znode which will capture a NodeDeleted event on the specified\n   * znode as well as NodeChildrenChanged if any children of the specified znode\n   * are created or deleted.\n   *\n   * Returns null if the specified node does not exist.  Otherwise returns a\n   * list of children of the specified node.  If the node exists but it has no\n   * children, an empty list will be returned.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to list and watch children of\n   * @return list of children of the specified node, an empty list if the node\n   *          exists but has no children, and null if the node does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static List<String> listChildrenAndWatchForNewChildren(\n          ZKWatcher zkw, String znode)\n    throws KeeperException {\n    try {\n      List<String> children = zkw.getRecoverableZooKeeper().getChildren(znode, zkw);\n      return children;\n    } catch(KeeperException.NoNodeException ke) {\n      LOG.debug(zkw.prefix(\"Unable to list children of znode \" + znode + \" \" +\n          \"because node does not exist (not an error)\"));\n      return null;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode + \" \"), e);\n      zkw.keeperException(e);\n      return null;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to list children of znode \" + znode + \" \"), e);\n      zkw.interruptedException(e);\n      return null;\n    }\n  }"
        ],
        [
            "TestMasterAddressTracker::setupMasterTracker(ServerName,int)",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103  \n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "  /**\n   * create an address tracker instance\n   * @param sn if not-null set the active master\n   * @param infoPort if there is an active master, set its info port.\n   */\n  private MasterAddressTracker setupMasterTracker(final ServerName sn, final int infoPort)\n      throws Exception {\n    ZKWatcher zk = new ZKWatcher(TEST_UTIL.getConfiguration(),\n        name.getMethodName(), null);\n    ZKUtil.createAndFailSilent(zk, zk.znodePaths.baseZNode);\n\n    // Should not have a master yet\n    MasterAddressTracker addressTracker = new MasterAddressTracker(zk, null);\n    addressTracker.start();\n    assertFalse(addressTracker.hasMaster());\n    zk.registerListener(addressTracker);\n\n    // Use a listener to capture when the node is actually created\n    NodeCreationListener listener = new NodeCreationListener(zk, zk.znodePaths.masterAddressZNode);\n    zk.registerListener(listener);\n\n    if (sn != null) {\n      LOG.info(\"Creating master node\");\n      MasterAddressTracker.setMasterAddress(zk, zk.znodePaths.masterAddressZNode, sn, infoPort);\n\n      // Wait for the node to be created\n      LOG.info(\"Waiting for master address manager to be notified\");\n      listener.waitForCreation();\n      LOG.info(\"Master node created\");\n    }\n    return addressTracker;\n  }",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 +\n 100 +\n 101  \n 102  \n 103  \n 104  \n 105 +\n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  ",
            "  /**\n   * create an address tracker instance\n   * @param sn if not-null set the active master\n   * @param infoPort if there is an active master, set its info port.\n   */\n  private MasterAddressTracker setupMasterTracker(final ServerName sn, final int infoPort)\n      throws Exception {\n    ZKWatcher zk = new ZKWatcher(TEST_UTIL.getConfiguration(),\n        name.getMethodName(), null);\n    ZKUtil.createAndFailSilent(zk, zk.getZNodePaths().baseZNode);\n\n    // Should not have a master yet\n    MasterAddressTracker addressTracker = new MasterAddressTracker(zk, null);\n    addressTracker.start();\n    assertFalse(addressTracker.hasMaster());\n    zk.registerListener(addressTracker);\n\n    // Use a listener to capture when the node is actually created\n    NodeCreationListener listener = new NodeCreationListener(zk,\n            zk.getZNodePaths().masterAddressZNode);\n    zk.registerListener(listener);\n\n    if (sn != null) {\n      LOG.info(\"Creating master node\");\n      MasterAddressTracker.setMasterAddress(zk, zk.getZNodePaths().masterAddressZNode,\n              sn, infoPort);\n\n      // Wait for the node to be created\n      LOG.info(\"Waiting for master address manager to be notified\");\n      listener.waitForCreation();\n      LOG.info(\"Master node created\");\n    }\n    return addressTracker;\n  }"
        ],
        [
            "IntegrationTestZKAndFSPermissions::checkZnodePermsRecursive(ZKWatcher,RecoverableZooKeeper,String)",
            " 160  \n 161  \n 162  \n 163 -\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  ",
            "  private void checkZnodePermsRecursive(ZKWatcher watcher,\n      RecoverableZooKeeper zk, String znode) throws KeeperException, InterruptedException {\n\n    boolean expectedWorldReadable = watcher.znodePaths.isClientReadable(znode);\n\n    assertZnodePerms(zk, znode, expectedWorldReadable);\n\n    try {\n      List<String> children = zk.getChildren(znode, false);\n\n      for (String child : children) {\n        checkZnodePermsRecursive(watcher, zk, ZNodePaths.joinZNode(znode, child));\n      }\n    } catch (KeeperException ke) {\n      // if we are not authenticated for listChildren, it is fine.\n      if (ke.code() != Code.NOAUTH && ke.code() != Code.NONODE) {\n        throw ke;\n      }\n    }\n  }",
            " 160  \n 161  \n 162  \n 163 +\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  ",
            "  private void checkZnodePermsRecursive(ZKWatcher watcher,\n      RecoverableZooKeeper zk, String znode) throws KeeperException, InterruptedException {\n\n    boolean expectedWorldReadable = watcher.getZNodePaths().isClientReadable(znode);\n\n    assertZnodePerms(zk, znode, expectedWorldReadable);\n\n    try {\n      List<String> children = zk.getChildren(znode, false);\n\n      for (String child : children) {\n        checkZnodePermsRecursive(watcher, zk, ZNodePaths.joinZNode(znode, child));\n      }\n    } catch (KeeperException ke) {\n      // if we are not authenticated for listChildren, it is fine.\n      if (ke.code() != Code.NOAUTH && ke.code() != Code.NONODE) {\n        throw ke;\n      }\n    }\n  }"
        ],
        [
            "ZKReplicationStorageBase::ZKReplicationStorageBase(ZKWatcher,Configuration)",
            "  49  \n  50  \n  51  \n  52  \n  53 -\n  54  \n  55  ",
            "  protected ZKReplicationStorageBase(ZKWatcher zookeeper, Configuration conf) {\n    this.zookeeper = zookeeper;\n    this.conf = conf;\n\n    this.replicationZNode = ZNodePaths.joinZNode(this.zookeeper.znodePaths.baseZNode,\n      conf.get(REPLICATION_ZNODE, REPLICATION_ZNODE_DEFAULT));\n  }",
            "  49  \n  50  \n  51  \n  52  \n  53 +\n  54  \n  55  ",
            "  protected ZKReplicationStorageBase(ZKWatcher zookeeper, Configuration conf) {\n    this.zookeeper = zookeeper;\n    this.conf = conf;\n\n    this.replicationZNode = ZNodePaths.joinZNode(this.zookeeper.getZNodePaths().baseZNode,\n      conf.get(REPLICATION_ZNODE, REPLICATION_ZNODE_DEFAULT));\n  }"
        ],
        [
            "RegionNormalizerTracker::setNormalizerOn(boolean)",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 -\n  69  \n  70 -\n  71  \n  72 -\n  73  ",
            "  /**\n   * Set region normalizer on/off\n   * @param normalizerOn whether normalizer should be on or off\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public void setNormalizerOn(boolean normalizerOn) throws KeeperException {\n    byte [] upData = toByteArray(normalizerOn);\n    try {\n      ZKUtil.setData(watcher, watcher.znodePaths.regionNormalizerZNode, upData);\n    } catch(KeeperException.NoNodeException nne) {\n      ZKUtil.createAndWatch(watcher, watcher.znodePaths.regionNormalizerZNode, upData);\n    }\n    super.nodeDataChanged(watcher.znodePaths.regionNormalizerZNode);\n  }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69  \n  70 +\n  71  \n  72 +\n  73  ",
            "  /**\n   * Set region normalizer on/off\n   * @param normalizerOn whether normalizer should be on or off\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public void setNormalizerOn(boolean normalizerOn) throws KeeperException {\n    byte [] upData = toByteArray(normalizerOn);\n    try {\n      ZKUtil.setData(watcher, watcher.getZNodePaths().regionNormalizerZNode, upData);\n    } catch(KeeperException.NoNodeException nne) {\n      ZKUtil.createAndWatch(watcher, watcher.getZNodePaths().regionNormalizerZNode, upData);\n    }\n    super.nodeDataChanged(watcher.getZNodePaths().regionNormalizerZNode);\n  }"
        ],
        [
            "ZKPermissionWatcher::ZKPermissionWatcher(ZKWatcher,TableAuthManager,Configuration)",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70 -\n  71  \n  72  \n  73  ",
            "  public ZKPermissionWatcher(ZKWatcher watcher,\n      TableAuthManager authManager, Configuration conf) {\n    super(watcher);\n    this.authManager = authManager;\n    String aclZnodeParent = conf.get(\"zookeeper.znode.acl.parent\", ACL_NODE);\n    this.aclZNode = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, aclZnodeParent);\n    executor = Executors.newSingleThreadExecutor(\n      new DaemonThreadFactory(\"zk-permission-watcher\"));\n  }",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70 +\n  71  \n  72  \n  73  ",
            "  public ZKPermissionWatcher(ZKWatcher watcher,\n      TableAuthManager authManager, Configuration conf) {\n    super(watcher);\n    this.authManager = authManager;\n    String aclZnodeParent = conf.get(\"zookeeper.znode.acl.parent\", ACL_NODE);\n    this.aclZNode = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, aclZnodeParent);\n    executor = Executors.newSingleThreadExecutor(\n      new DaemonThreadFactory(\"zk-permission-watcher\"));\n  }"
        ],
        [
            "MasterAddressTracker::setMasterAddress(ZKWatcher,String,ServerName,int)",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 -\n 212  \n 213  ",
            "  /**\n   * Set master address into the <code>master</code> znode or into the backup\n   * subdirectory of backup masters; switch off the passed in <code>znode</code>\n   * path.\n   * @param zkw The ZKWatcher to use.\n   * @param znode Where to create the znode; could be at the top level or it\n   *              could be under backup masters\n   * @param master ServerName of the current master must not be null.\n   * @return true if node created, false if not; a watch is set in both cases\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static boolean setMasterAddress(final ZKWatcher zkw,\n      final String znode, final ServerName master, int infoPort)\n  throws KeeperException {\n    return ZKUtil.createEphemeralNodeAndWatch(zkw, znode, toByteArray(master, infoPort));\n  }",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 +\n 212  \n 213  ",
            "  /**\n   * Set master address into the <code>master</code> znode or into the backup\n   * subdirectory of backup masters; switch off the passed in <code>znode</code>\n   * path.\n   * @param zkw The ZKWatcher to use.\n   * @param znode Where to create the znode; could be at the top level or it\n   *              could be under backup masters\n   * @param master ServerName of the current master must not be null.\n   * @return true if node created, false if not; a watch is set in both cases\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public static boolean setMasterAddress(final ZKWatcher zkw,\n      final String znode, final ServerName master, int infoPort)\n    throws KeeperException {\n    return ZKUtil.createEphemeralNodeAndWatch(zkw, znode, toByteArray(master, infoPort));\n  }"
        ],
        [
            "MasterAddressTracker::getMasterInfoPort(ZKWatcher)",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 -\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "  /**\n   * Get master info port.\n   * Use this instead of {@link #getMasterInfoPort()} if you do not have an\n   * instance of this tracker in your context.\n   * @param zkw ZKWatcher to use\n   * @return master info port in the the master address znode or null if no\n   *         znode present.\n   *         // TODO can't return null for 'int' return type. non-static verison returns 0\n   * @throws KeeperException if a ZooKeeper operation fails\n   * @throws IOException if the address of the ZooKeeper master cannot be retrieved\n   */\n  public static int getMasterInfoPort(final ZKWatcher zkw) throws KeeperException, IOException {\n    byte[] data;\n    try {\n      data = ZKUtil.getData(zkw, zkw.znodePaths.masterAddressZNode);\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException();\n    }\n    // TODO javadoc claims we return null in this case. :/\n    if (data == null) {\n      throw new IOException(\"Can't get master address from ZooKeeper; znode data == null\");\n    }\n    try {\n      return parse(data).getInfoPort();\n    } catch (DeserializationException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    }\n  }",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 +\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "  /**\n   * Get master info port.\n   * Use this instead of {@link #getMasterInfoPort()} if you do not have an\n   * instance of this tracker in your context.\n   * @param zkw ZKWatcher to use\n   * @return master info port in the the master address znode or null if no\n   *         znode present.\n   *         // TODO can't return null for 'int' return type. non-static verison returns 0\n   * @throws KeeperException if a ZooKeeper operation fails\n   * @throws IOException if the address of the ZooKeeper master cannot be retrieved\n   */\n  public static int getMasterInfoPort(final ZKWatcher zkw) throws KeeperException, IOException {\n    byte[] data;\n    try {\n      data = ZKUtil.getData(zkw, zkw.getZNodePaths().masterAddressZNode);\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException();\n    }\n    // TODO javadoc claims we return null in this case. :/\n    if (data == null) {\n      throw new IOException(\"Can't get master address from ZooKeeper; znode data == null\");\n    }\n    try {\n      return parse(data).getInfoPort();\n    } catch (DeserializationException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    }\n  }"
        ],
        [
            "RegionServerTracker::nodeChildrenChanged(String)",
            " 145  \n 146  \n 147 -\n 148  \n 149  \n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  ",
            "  @Override\n  public void nodeChildrenChanged(String path) {\n    if (path.equals(watcher.znodePaths.rsZNode)\n        && !server.isAborted() && !server.isStopped()) {\n      try {\n        List<String> servers =\n          ZKUtil.listChildrenAndWatchThem(watcher, watcher.znodePaths.rsZNode);\n        refresh(servers);\n      } catch (IOException e) {\n        server.abort(\"Unexpected zk exception getting RS nodes\", e);\n      } catch (KeeperException e) {\n        server.abort(\"Unexpected zk exception getting RS nodes\", e);\n      }\n    }\n  }",
            " 145  \n 146  \n 147 +\n 148  \n 149  \n 150  \n 151 +\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  ",
            "  @Override\n  public void nodeChildrenChanged(String path) {\n    if (path.equals(watcher.getZNodePaths().rsZNode)\n        && !server.isAborted() && !server.isStopped()) {\n      try {\n        List<String> servers =\n          ZKUtil.listChildrenAndWatchThem(watcher, watcher.getZNodePaths().rsZNode);\n        refresh(servers);\n      } catch (IOException e) {\n        server.abort(\"Unexpected zk exception getting RS nodes\", e);\n      } catch (KeeperException e) {\n        server.abort(\"Unexpected zk exception getting RS nodes\", e);\n      }\n    }\n  }"
        ],
        [
            "SplitOrMergeTracker::SplitOrMergeTracker(ZKWatcher,Configuration,Abortable)",
            "  49  \n  50  \n  51  \n  52 -\n  53 -\n  54  \n  55  \n  56  \n  57  \n  58 -\n  59  \n  60 -\n  61  \n  62  \n  63  \n  64  ",
            "  public SplitOrMergeTracker(ZKWatcher watcher, Configuration conf,\n                             Abortable abortable) {\n    try {\n      if (ZKUtil.checkExists(watcher, watcher.znodePaths.switchZNode) < 0) {\n        ZKUtil.createAndFailSilent(watcher, watcher.znodePaths.switchZNode);\n      }\n    } catch (KeeperException e) {\n      throw new RuntimeException(e);\n    }\n    splitZnode = ZNodePaths.joinZNode(watcher.znodePaths.switchZNode,\n      conf.get(\"zookeeper.znode.switch.split\", \"split\"));\n    mergeZnode = ZNodePaths.joinZNode(watcher.znodePaths.switchZNode,\n      conf.get(\"zookeeper.znode.switch.merge\", \"merge\"));\n    splitStateTracker = new SwitchStateTracker(watcher, splitZnode, abortable);\n    mergeStateTracker = new SwitchStateTracker(watcher, mergeZnode, abortable);\n  }",
            "  49  \n  50  \n  51  \n  52 +\n  53 +\n  54  \n  55  \n  56  \n  57  \n  58 +\n  59  \n  60 +\n  61  \n  62  \n  63  \n  64  ",
            "  public SplitOrMergeTracker(ZKWatcher watcher, Configuration conf,\n                             Abortable abortable) {\n    try {\n      if (ZKUtil.checkExists(watcher, watcher.getZNodePaths().switchZNode) < 0) {\n        ZKUtil.createAndFailSilent(watcher, watcher.getZNodePaths().switchZNode);\n      }\n    } catch (KeeperException e) {\n      throw new RuntimeException(e);\n    }\n    splitZnode = ZNodePaths.joinZNode(watcher.getZNodePaths().switchZNode,\n      conf.get(\"zookeeper.znode.switch.split\", \"split\"));\n    mergeZnode = ZNodePaths.joinZNode(watcher.getZNodePaths().switchZNode,\n      conf.get(\"zookeeper.znode.switch.merge\", \"merge\"));\n    splitStateTracker = new SwitchStateTracker(watcher, splitZnode, abortable);\n    mergeStateTracker = new SwitchStateTracker(watcher, mergeZnode, abortable);\n  }"
        ],
        [
            "MasterMaintenanceModeTracker::update()",
            "  48  \n  49  \n  50  \n  51 -\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  ",
            "  private void update() {\n    try {\n      List<String> children =\n          ZKUtil.listChildrenAndWatchForNewChildren(watcher, watcher.znodePaths.masterMaintZNode);\n      hasChildren = (children != null && children.size() > 0);\n    } catch (KeeperException e) {\n      // Ignore the ZK keeper exception\n      hasChildren = false;\n    }\n  }",
            "  48  \n  49  \n  50  \n  51 +\n  52 +\n  53  \n  54  \n  55  \n  56  \n  57  \n  58  ",
            "  private void update() {\n    try {\n      List<String> children =\n          ZKUtil.listChildrenAndWatchForNewChildren(watcher,\n                  watcher.getZNodePaths().masterMaintZNode);\n      hasChildren = (children != null && children.size() > 0);\n    } catch (KeeperException e) {\n      // Ignore the ZK keeper exception\n      hasChildren = false;\n    }\n  }"
        ],
        [
            "MasterMetaBootstrap::unassignExcessMetaReplica(int)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135 -\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "  private void unassignExcessMetaReplica(int numMetaReplicasConfigured) {\n    final ZKWatcher zooKeeper = master.getZooKeeper();\n    // unassign the unneeded replicas (for e.g., if the previous master was configured\n    // with a replication of 3 and now it is 2, we need to unassign the 1 unneeded replica)\n    try {\n      List<String> metaReplicaZnodes = zooKeeper.getMetaReplicaNodes();\n      for (String metaReplicaZnode : metaReplicaZnodes) {\n        int replicaId = zooKeeper.znodePaths.getMetaReplicaIdFromZnode(metaReplicaZnode);\n        if (replicaId >= numMetaReplicasConfigured) {\n          RegionState r = MetaTableLocator.getMetaRegionState(zooKeeper, replicaId);\n          LOG.info(\"Closing excess replica of meta region \" + r.getRegion());\n          // send a close and wait for a max of 30 seconds\n          ServerManager.closeRegionSilentlyAndWait(master.getClusterConnection(),\n              r.getServerName(), r.getRegion(), 30000);\n          ZKUtil.deleteNode(zooKeeper, zooKeeper.znodePaths.getZNodeForReplica(replicaId));\n        }\n      }\n    } catch (Exception ex) {\n      // ignore the exception since we don't want the master to be wedged due to potential\n      // issues in the cleanup of the extra regions. We can do that cleanup via hbck or manually\n      LOG.warn(\"Ignoring exception \" + ex);\n    }\n  }",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135 +\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "  private void unassignExcessMetaReplica(int numMetaReplicasConfigured) {\n    final ZKWatcher zooKeeper = master.getZooKeeper();\n    // unassign the unneeded replicas (for e.g., if the previous master was configured\n    // with a replication of 3 and now it is 2, we need to unassign the 1 unneeded replica)\n    try {\n      List<String> metaReplicaZnodes = zooKeeper.getMetaReplicaNodes();\n      for (String metaReplicaZnode : metaReplicaZnodes) {\n        int replicaId = zooKeeper.getZNodePaths().getMetaReplicaIdFromZnode(metaReplicaZnode);\n        if (replicaId >= numMetaReplicasConfigured) {\n          RegionState r = MetaTableLocator.getMetaRegionState(zooKeeper, replicaId);\n          LOG.info(\"Closing excess replica of meta region \" + r.getRegion());\n          // send a close and wait for a max of 30 seconds\n          ServerManager.closeRegionSilentlyAndWait(master.getClusterConnection(),\n              r.getServerName(), r.getRegion(), 30000);\n          ZKUtil.deleteNode(zooKeeper, zooKeeper.getZNodePaths().getZNodeForReplica(replicaId));\n        }\n      }\n    } catch (Exception ex) {\n      // ignore the exception since we don't want the master to be wedged due to potential\n      // issues in the cleanup of the extra regions. We can do that cleanup via hbck or manually\n      LOG.warn(\"Ignoring exception \" + ex);\n    }\n  }"
        ],
        [
            "TestReplicationTrackerZKImpl::testGetListOfRegionServers()",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119  \n 120  \n 121  \n 122  \n 123 -\n 124  \n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133 -\n 134  \n 135  ",
            "  @Test\n  public void testGetListOfRegionServers() throws Exception {\n    // 0 region servers\n    assertEquals(0, rt.getListOfRegionServers().size());\n\n    // 1 region server\n    ZKUtil.createWithParents(zkw,\n      ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, \"hostname1.example.org:1234\"));\n    assertEquals(1, rt.getListOfRegionServers().size());\n\n    // 2 region servers\n    ZKUtil.createWithParents(zkw,\n      ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, \"hostname2.example.org:1234\"));\n    assertEquals(2, rt.getListOfRegionServers().size());\n\n    // 1 region server\n    ZKUtil.deleteNode(zkw,\n      ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, \"hostname2.example.org:1234\"));\n    assertEquals(1, rt.getListOfRegionServers().size());\n\n    // 0 region server\n    ZKUtil.deleteNode(zkw,\n      ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, \"hostname1.example.org:1234\"));\n    assertEquals(0, rt.getListOfRegionServers().size());\n  }",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 +\n 120  \n 121  \n 122  \n 123  \n 124 +\n 125  \n 126  \n 127  \n 128  \n 129 +\n 130  \n 131  \n 132  \n 133  \n 134 +\n 135  \n 136  ",
            "  @Test\n  public void testGetListOfRegionServers() throws Exception {\n    // 0 region servers\n    assertEquals(0, rt.getListOfRegionServers().size());\n\n    // 1 region server\n    ZKUtil.createWithParents(zkw,\n      ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, \"hostname1.example.org:1234\"));\n    assertEquals(1, rt.getListOfRegionServers().size());\n\n    // 2 region servers\n    ZKUtil.createWithParents(zkw,\n      ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, \"hostname2.example.org:1234\"));\n    assertEquals(2, rt.getListOfRegionServers().size());\n\n    // 1 region server\n    ZKUtil.deleteNode(zkw,\n      ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, \"hostname2.example.org:1234\"));\n    assertEquals(1, rt.getListOfRegionServers().size());\n\n    // 0 region server\n    ZKUtil.deleteNode(zkw,\n      ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode, \"hostname1.example.org:1234\"));\n    assertEquals(0, rt.getListOfRegionServers().size());\n  }"
        ],
        [
            "ZKNodeTracker::blockUntilAvailable()",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  ",
            "  /**\n   * Gets the data of the node, blocking until the node is available.\n   *\n   * @return data of the node\n   * @throws InterruptedException if the waiting thread is interrupted\n   */\n  public synchronized byte [] blockUntilAvailable()\n  throws InterruptedException {\n    return blockUntilAvailable(0, false);\n  }",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 +\n 106  \n 107  ",
            "  /**\n   * Gets the data of the node, blocking until the node is available.\n   *\n   * @return data of the node\n   * @throws InterruptedException if the waiting thread is interrupted\n   */\n  public synchronized byte [] blockUntilAvailable()\n    throws InterruptedException {\n    return blockUntilAvailable(0, false);\n  }"
        ],
        [
            "ZKUtil::createWithParents(ZKWatcher,String,byte)",
            "1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194 -\n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  ",
            "  /**\n   * Creates the specified node and all parent nodes required for it to exist.  The creation of\n   * parent znodes is not atomic with the leafe znode creation but the data is written atomically\n   * when the leaf node is created.\n   *\n   * No watches are set and no errors are thrown if the node already exists.\n   *\n   * The nodes created are persistent and open access.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static void createWithParents(ZKWatcher zkw, String znode, byte[] data)\n  throws KeeperException {\n    try {\n      if(znode == null) {\n        return;\n      }\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.PERSISTENT);\n    } catch(KeeperException.NodeExistsException nee) {\n      return;\n    } catch(KeeperException.NoNodeException nne) {\n      createWithParents(zkw, getParent(znode));\n      createWithParents(zkw, znode, data);\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n  }",
            "1180  \n1181  \n1182  \n1183  \n1184  \n1185  \n1186  \n1187  \n1188  \n1189  \n1190  \n1191  \n1192  \n1193  \n1194 +\n1195  \n1196  \n1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  ",
            "  /**\n   * Creates the specified node and all parent nodes required for it to exist.  The creation of\n   * parent znodes is not atomic with the leafe znode creation but the data is written atomically\n   * when the leaf node is created.\n   *\n   * No watches are set and no errors are thrown if the node already exists.\n   *\n   * The nodes created are persistent and open access.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static void createWithParents(ZKWatcher zkw, String znode, byte[] data)\n    throws KeeperException {\n    try {\n      if(znode == null) {\n        return;\n      }\n      zkw.getRecoverableZooKeeper().create(znode, data, createACL(zkw, znode),\n          CreateMode.PERSISTENT);\n    } catch(KeeperException.NodeExistsException nee) {\n      return;\n    } catch(KeeperException.NoNodeException nne) {\n      createWithParents(zkw, getParent(znode));\n      createWithParents(zkw, znode, data);\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n  }"
        ],
        [
            "TestZKMulti::testSingleFailure()",
            " 155  \n 156  \n 157  \n 158  \n 159 -\n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  ",
            "  @Test\n  public void testSingleFailure() throws Exception {\n    // try to delete a node that doesn't exist\n    boolean caughtNoNode = false;\n    String path = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testSingleFailureZ\");\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path));\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NoNodeException nne) {\n      caughtNoNode = true;\n    }\n    assertTrue(caughtNoNode);\n\n    // try to setData on a node that doesn't exist\n    caughtNoNode = false;\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.setData(path, Bytes.toBytes(path)));\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NoNodeException nne) {\n      caughtNoNode = true;\n    }\n    assertTrue(caughtNoNode);\n\n    // try to create on a node that already exists\n    boolean caughtNodeExists = false;\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(path, Bytes.toBytes(path)));\n    ZKUtil.multiOrSequential(zkw, ops, false);\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NodeExistsException nee) {\n      caughtNodeExists = true;\n    }\n    assertTrue(caughtNodeExists);\n  }",
            " 155  \n 156  \n 157  \n 158  \n 159 +\n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  ",
            "  @Test\n  public void testSingleFailure() throws Exception {\n    // try to delete a node that doesn't exist\n    boolean caughtNoNode = false;\n    String path = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testSingleFailureZ\");\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    ops.add(ZKUtilOp.deleteNodeFailSilent(path));\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NoNodeException nne) {\n      caughtNoNode = true;\n    }\n    assertTrue(caughtNoNode);\n\n    // try to setData on a node that doesn't exist\n    caughtNoNode = false;\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.setData(path, Bytes.toBytes(path)));\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NoNodeException nne) {\n      caughtNoNode = true;\n    }\n    assertTrue(caughtNoNode);\n\n    // try to create on a node that already exists\n    boolean caughtNodeExists = false;\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(path, Bytes.toBytes(path)));\n    ZKUtil.multiOrSequential(zkw, ops, false);\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NodeExistsException nee) {\n      caughtNodeExists = true;\n    }\n    assertTrue(caughtNodeExists);\n  }"
        ],
        [
            "MasterAddressTracker::getMasterAddress(ZKWatcher)",
            " 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 -\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "  /**\n   * Get master address.\n   * Use this instead of {@link #getMasterAddress()} if you do not have an\n   * instance of this tracker in your context.\n   * @param zkw ZKWatcher to use\n   * @return ServerName stored in the the master address znode or null if no\n   *         znode present.\n   * @throws KeeperException if a ZooKeeper operation fails\n   * @throws IOException if the address of the ZooKeeper master cannot be retrieved\n   */\n  public static ServerName getMasterAddress(final ZKWatcher zkw)\n          throws KeeperException, IOException {\n    byte [] data;\n    try {\n      data = ZKUtil.getData(zkw, zkw.znodePaths.masterAddressZNode);\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException();\n    }\n    // TODO javadoc claims we return null in this case. :/\n    if (data == null){\n      throw new IOException(\"Can't get master address from ZooKeeper; znode data == null\");\n    }\n    try {\n      return ProtobufUtil.parseServerNameFrom(data);\n    } catch (DeserializationException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    }\n  }",
            " 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 +\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "  /**\n   * Get master address.\n   * Use this instead of {@link #getMasterAddress()} if you do not have an\n   * instance of this tracker in your context.\n   * @param zkw ZKWatcher to use\n   * @return ServerName stored in the the master address znode or null if no\n   *         znode present.\n   * @throws KeeperException if a ZooKeeper operation fails\n   * @throws IOException if the address of the ZooKeeper master cannot be retrieved\n   */\n  public static ServerName getMasterAddress(final ZKWatcher zkw)\n          throws KeeperException, IOException {\n    byte [] data;\n    try {\n      data = ZKUtil.getData(zkw, zkw.getZNodePaths().masterAddressZNode);\n    } catch (InterruptedException e) {\n      throw new InterruptedIOException();\n    }\n    // TODO javadoc claims we return null in this case. :/\n    if (data == null){\n      throw new IOException(\"Can't get master address from ZooKeeper; znode data == null\");\n    }\n    try {\n      return ProtobufUtil.parseServerNameFrom(data);\n    } catch (DeserializationException e) {\n      KeeperException ke = new KeeperException.DataInconsistencyException();\n      ke.initCause(e);\n      throw ke;\n    }\n  }"
        ],
        [
            "ClientZKSyncer::start()",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 -\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  ",
            "  /**\n   * Starts the syncer\n   * @throws KeeperException if error occurs when trying to create base nodes on client ZK\n   */\n  public void start() throws KeeperException {\n    LOG.debug(\"Starting \" + getClass().getSimpleName());\n    this.watcher.registerListener(this);\n    // create base znode on remote ZK\n    ZKUtil.createWithParents(clientZkWatcher, watcher.znodePaths.baseZNode);\n    // set meta znodes for client ZK\n    Collection<String> nodes = getNodesToWatch();\n    LOG.debug(\"Znodes to watch: \" + nodes);\n    // initialize queues and threads\n    for (String node : nodes) {\n      BlockingQueue<byte[]> queue = new ArrayBlockingQueue<>(1);\n      queues.put(node, queue);\n      Thread updater = new ClientZkUpdater(node, queue);\n      updater.setDaemon(true);\n      updater.start();\n      watchAndCheckExists(node);\n    }\n  }",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  ",
            "  /**\n   * Starts the syncer\n   * @throws KeeperException if error occurs when trying to create base nodes on client ZK\n   */\n  public void start() throws KeeperException {\n    LOG.debug(\"Starting \" + getClass().getSimpleName());\n    this.watcher.registerListener(this);\n    // create base znode on remote ZK\n    ZKUtil.createWithParents(clientZkWatcher, watcher.getZNodePaths().baseZNode);\n    // set meta znodes for client ZK\n    Collection<String> nodes = getNodesToWatch();\n    LOG.debug(\"Znodes to watch: \" + nodes);\n    // initialize queues and threads\n    for (String node : nodes) {\n      BlockingQueue<byte[]> queue = new ArrayBlockingQueue<>(1);\n      queues.put(node, queue);\n      Thread updater = new ClientZkUpdater(node, queue);\n      updater.setDaemon(true);\n      updater.start();\n      watchAndCheckExists(node);\n    }\n  }"
        ],
        [
            "ZKUtil::connect(Configuration,Watcher)",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 -\n 112  \n 113  \n 114  ",
            "  /**\n   * Creates a new connection to ZooKeeper, pulling settings and ensemble config\n   * from the specified configuration object using methods from {@link ZKConfig}.\n   *\n   * Sets the connection status monitoring watcher to the specified watcher.\n   *\n   * @param conf configuration to pull ensemble and other settings from\n   * @param watcher watcher to monitor connection changes\n   * @return connection to zookeeper\n   * @throws IOException if unable to connect to zk or config problem\n   */\n  public static RecoverableZooKeeper connect(Configuration conf, Watcher watcher)\n  throws IOException {\n    String ensemble = ZKConfig.getZKQuorumServersString(conf);\n    return connect(conf, ensemble, watcher);\n  }",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 +\n 112  \n 113  \n 114  ",
            "  /**\n   * Creates a new connection to ZooKeeper, pulling settings and ensemble config\n   * from the specified configuration object using methods from {@link ZKConfig}.\n   *\n   * Sets the connection status monitoring watcher to the specified watcher.\n   *\n   * @param conf configuration to pull ensemble and other settings from\n   * @param watcher watcher to monitor connection changes\n   * @return connection to zookeeper\n   * @throws IOException if unable to connect to zk or config problem\n   */\n  public static RecoverableZooKeeper connect(Configuration conf, Watcher watcher)\n    throws IOException {\n    String ensemble = ZKConfig.getZKQuorumServersString(conf);\n    return connect(conf, ensemble, watcher);\n  }"
        ],
        [
            "RecoverableZooKeeper::multi(Iterable)",
            " 652  \n 653  \n 654  \n 655  \n 656 -\n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  ",
            "  /**\n   * Run multiple operations in a transactional manner. Retry before throwing exception\n   */\n  public List<OpResult> multi(Iterable<Op> ops)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.multi\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      Iterable<Op> multiOps = prepareZKMulti(ops);\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<OpResult> opResults = checkZk().multi(multiOps);\n          return opResults;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"multi\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"multi\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 652  \n 653  \n 654  \n 655  \n 656 +\n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  ",
            "  /**\n   * Run multiple operations in a transactional manner. Retry before throwing exception\n   */\n  public List<OpResult> multi(Iterable<Op> ops)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.multi\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      Iterable<Op> multiOps = prepareZKMulti(ops);\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<OpResult> opResults = checkZk().multi(multiOps);\n          return opResults;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"multi\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"multi\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "TestReplicationTrackerZKImpl::setUp()",
            "  89  \n  90  \n  91  \n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "  @Before\n  public void setUp() throws Exception {\n    zkw = HBaseTestingUtility.getZooKeeperWatcher(utility);\n    String fakeRs1 = ZNodePaths.joinZNode(zkw.znodePaths.rsZNode, \"hostname1.example.org:1234\");\n    try {\n      ZKClusterId.setClusterId(zkw, new ClusterId());\n      rp = ReplicationFactory.getReplicationPeers(zkw, conf);\n      rp.init();\n      rt = ReplicationFactory.getReplicationTracker(zkw, new DummyServer(fakeRs1),\n        new DummyServer(fakeRs1));\n    } catch (Exception e) {\n      fail(\"Exception during test setup: \" + e);\n    }\n    rsRemovedCount = new AtomicInteger(0);\n    rsRemovedData = \"\";\n  }",
            "  89  \n  90  \n  91  \n  92 +\n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  ",
            "  @Before\n  public void setUp() throws Exception {\n    zkw = HBaseTestingUtility.getZooKeeperWatcher(utility);\n    String fakeRs1 = ZNodePaths.joinZNode(zkw.getZNodePaths().rsZNode,\n            \"hostname1.example.org:1234\");\n    try {\n      ZKClusterId.setClusterId(zkw, new ClusterId());\n      rp = ReplicationFactory.getReplicationPeers(zkw, conf);\n      rp.init();\n      rt = ReplicationFactory.getReplicationTracker(zkw, new DummyServer(fakeRs1),\n        new DummyServer(fakeRs1));\n    } catch (Exception e) {\n      fail(\"Exception during test setup: \" + e);\n    }\n    rsRemovedCount = new AtomicInteger(0);\n    rsRemovedData = \"\";\n  }"
        ],
        [
            "DrainingServerTracker::nodeChildrenChanged(String)",
            " 121  \n 122  \n 123 -\n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "  @Override\n  public void nodeChildrenChanged(final String path) {\n    if(path.equals(watcher.znodePaths.drainingZNode)) {\n      try {\n        final List<String> newNodes =\n          ZKUtil.listChildrenAndWatchThem(watcher, watcher.znodePaths.drainingZNode);\n        add(newNodes);\n      } catch (KeeperException e) {\n        abortable.abort(\"Unexpected zk exception getting RS nodes\", e);\n      } catch (IOException e) {\n        abortable.abort(\"Unexpected zk exception getting RS nodes\", e);\n      }\n    }\n  }",
            " 121  \n 122  \n 123 +\n 124  \n 125  \n 126 +\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "  @Override\n  public void nodeChildrenChanged(final String path) {\n    if(path.equals(watcher.getZNodePaths().drainingZNode)) {\n      try {\n        final List<String> newNodes =\n          ZKUtil.listChildrenAndWatchThem(watcher, watcher.getZNodePaths().drainingZNode);\n        add(newNodes);\n      } catch (KeeperException e) {\n        abortable.abort(\"Unexpected zk exception getting RS nodes\", e);\n      } catch (IOException e) {\n        abortable.abort(\"Unexpected zk exception getting RS nodes\", e);\n      }\n    }\n  }"
        ],
        [
            "VerifyingRSGroupAdminClient::verify()",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137 -\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  ",
            "  public void verify() throws IOException {\n    Map<String, RSGroupInfo> groupMap = Maps.newHashMap();\n    Set<RSGroupInfo> zList = Sets.newHashSet();\n\n    for (Result result : table.getScanner(new Scan())) {\n      RSGroupProtos.RSGroupInfo proto =\n          RSGroupProtos.RSGroupInfo.parseFrom(\n              result.getValue(\n                  RSGroupInfoManager.META_FAMILY_BYTES,\n                  RSGroupInfoManager.META_QUALIFIER_BYTES));\n      groupMap.put(proto.getName(), RSGroupProtobufUtil.toGroupInfo(proto));\n    }\n    Assert.assertEquals(Sets.newHashSet(groupMap.values()),\n        Sets.newHashSet(wrapped.listRSGroups()));\n    try {\n      String groupBasePath = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"rsgroup\");\n      for(String znode: ZKUtil.listChildrenNoWatch(zkw, groupBasePath)) {\n        byte[] data = ZKUtil.getData(zkw, ZNodePaths.joinZNode(groupBasePath, znode));\n        if(data.length > 0) {\n          ProtobufUtil.expectPBMagicPrefix(data);\n          ByteArrayInputStream bis = new ByteArrayInputStream(\n              data, ProtobufUtil.lengthOfPBMagic(), data.length);\n          zList.add(RSGroupProtobufUtil.toGroupInfo(RSGroupProtos.RSGroupInfo.parseFrom(bis)));\n        }\n      }\n      Assert.assertEquals(zList.size(), groupMap.size());\n      for(RSGroupInfo RSGroupInfo : zList) {\n        Assert.assertTrue(groupMap.get(RSGroupInfo.getName()).equals(RSGroupInfo));\n      }\n    } catch (KeeperException e) {\n      throw new IOException(\"ZK verification failed\", e);\n    } catch (DeserializationException e) {\n      throw new IOException(\"ZK verification failed\", e);\n    } catch (InterruptedException e) {\n      throw new IOException(\"ZK verification failed\", e);\n    }\n  }",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137 +\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  ",
            "  public void verify() throws IOException {\n    Map<String, RSGroupInfo> groupMap = Maps.newHashMap();\n    Set<RSGroupInfo> zList = Sets.newHashSet();\n\n    for (Result result : table.getScanner(new Scan())) {\n      RSGroupProtos.RSGroupInfo proto =\n          RSGroupProtos.RSGroupInfo.parseFrom(\n              result.getValue(\n                  RSGroupInfoManager.META_FAMILY_BYTES,\n                  RSGroupInfoManager.META_QUALIFIER_BYTES));\n      groupMap.put(proto.getName(), RSGroupProtobufUtil.toGroupInfo(proto));\n    }\n    Assert.assertEquals(Sets.newHashSet(groupMap.values()),\n        Sets.newHashSet(wrapped.listRSGroups()));\n    try {\n      String groupBasePath = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"rsgroup\");\n      for(String znode: ZKUtil.listChildrenNoWatch(zkw, groupBasePath)) {\n        byte[] data = ZKUtil.getData(zkw, ZNodePaths.joinZNode(groupBasePath, znode));\n        if(data.length > 0) {\n          ProtobufUtil.expectPBMagicPrefix(data);\n          ByteArrayInputStream bis = new ByteArrayInputStream(\n              data, ProtobufUtil.lengthOfPBMagic(), data.length);\n          zList.add(RSGroupProtobufUtil.toGroupInfo(RSGroupProtos.RSGroupInfo.parseFrom(bis)));\n        }\n      }\n      Assert.assertEquals(zList.size(), groupMap.size());\n      for(RSGroupInfo RSGroupInfo : zList) {\n        Assert.assertTrue(groupMap.get(RSGroupInfo.getName()).equals(RSGroupInfo));\n      }\n    } catch (KeeperException e) {\n      throw new IOException(\"ZK verification failed\", e);\n    } catch (DeserializationException e) {\n      throw new IOException(\"ZK verification failed\", e);\n    } catch (InterruptedException e) {\n      throw new IOException(\"ZK verification failed\", e);\n    }\n  }"
        ],
        [
            "TestMetaWithReplicas::testHBaseFsckWithFewerMetaReplicaZnodes()",
            " 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416 -\n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  ",
            "  @Ignore @Test // The close silently doesn't work any more since HBASE-14614. Fix.\n  public void testHBaseFsckWithFewerMetaReplicaZnodes() throws Exception {\n    ClusterConnection c = (ClusterConnection)ConnectionFactory.createConnection(\n        TEST_UTIL.getConfiguration());\n    RegionLocations rl = c.locateRegion(TableName.META_TABLE_NAME, HConstants.EMPTY_START_ROW,\n        false, false);\n    HBaseFsckRepair.closeRegionSilentlyAndWait(c,\n        rl.getRegionLocation(2).getServerName(), rl.getRegionLocation(2).getRegionInfo());\n    ZKWatcher zkw = TEST_UTIL.getZooKeeperWatcher();\n    ZKUtil.deleteNode(zkw, zkw.znodePaths.getZNodeForReplica(2));\n    // check that problem exists\n    HBaseFsck hbck = doFsck(TEST_UTIL.getConfiguration(), false);\n    assertErrors(hbck, new ERROR_CODE[]{ERROR_CODE.UNKNOWN,ERROR_CODE.NO_META_REGION});\n    // fix the problem\n    hbck = doFsck(TEST_UTIL.getConfiguration(), true);\n    // run hbck again to make sure we don't see any errors\n    hbck = doFsck(TEST_UTIL.getConfiguration(), false);\n    assertErrors(hbck, new ERROR_CODE[]{});\n  }",
            " 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416 +\n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  ",
            "  @Ignore @Test // The close silently doesn't work any more since HBASE-14614. Fix.\n  public void testHBaseFsckWithFewerMetaReplicaZnodes() throws Exception {\n    ClusterConnection c = (ClusterConnection)ConnectionFactory.createConnection(\n        TEST_UTIL.getConfiguration());\n    RegionLocations rl = c.locateRegion(TableName.META_TABLE_NAME, HConstants.EMPTY_START_ROW,\n        false, false);\n    HBaseFsckRepair.closeRegionSilentlyAndWait(c,\n        rl.getRegionLocation(2).getServerName(), rl.getRegionLocation(2).getRegionInfo());\n    ZKWatcher zkw = TEST_UTIL.getZooKeeperWatcher();\n    ZKUtil.deleteNode(zkw, zkw.getZNodePaths().getZNodeForReplica(2));\n    // check that problem exists\n    HBaseFsck hbck = doFsck(TEST_UTIL.getConfiguration(), false);\n    assertErrors(hbck, new ERROR_CODE[]{ERROR_CODE.UNKNOWN,ERROR_CODE.NO_META_REGION});\n    // fix the problem\n    hbck = doFsck(TEST_UTIL.getConfiguration(), true);\n    // run hbck again to make sure we don't see any errors\n    hbck = doFsck(TEST_UTIL.getConfiguration(), false);\n    assertErrors(hbck, new ERROR_CODE[]{});\n  }"
        ],
        [
            "ServerManager::getRegionServersInZK(ZKWatcher)",
            " 552  \n 553  \n 554 -\n 555  ",
            "  private List<String> getRegionServersInZK(final ZKWatcher zkw)\n  throws KeeperException {\n    return ZKUtil.listChildrenNoWatch(zkw, zkw.znodePaths.rsZNode);\n  }",
            " 552  \n 553  \n 554 +\n 555  ",
            "  private List<String> getRegionServersInZK(final ZKWatcher zkw)\n  throws KeeperException {\n    return ZKUtil.listChildrenNoWatch(zkw, zkw.getZNodePaths().rsZNode);\n  }"
        ],
        [
            "TestRegionServerHostname::testRegionServerHostnameReportedToMaster()",
            " 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171  \n 172  \n 173  ",
            "  @Test\n  public void testRegionServerHostnameReportedToMaster() throws Exception {\n    TEST_UTIL.getConfiguration().setBoolean(HRegionServer.RS_HOSTNAME_DISABLE_MASTER_REVERSEDNS_KEY,\n    true);\n    TEST_UTIL.startMiniCluster(NUM_MASTERS, NUM_RS);\n    boolean tablesOnMaster = LoadBalancer.isTablesOnMaster(TEST_UTIL.getConfiguration());\n    int expectedRS = NUM_RS + (tablesOnMaster? 1: 0);\n    try (ZKWatcher zkw = TEST_UTIL.getZooKeeperWatcher()) {\n      List<String> servers = ZKUtil.listChildrenNoWatch(zkw, zkw.znodePaths.rsZNode);\n      assertEquals(expectedRS, servers.size());\n    }\n  }",
            " 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 +\n 171  \n 172  \n 173  ",
            "  @Test\n  public void testRegionServerHostnameReportedToMaster() throws Exception {\n    TEST_UTIL.getConfiguration().setBoolean(HRegionServer.RS_HOSTNAME_DISABLE_MASTER_REVERSEDNS_KEY,\n    true);\n    TEST_UTIL.startMiniCluster(NUM_MASTERS, NUM_RS);\n    boolean tablesOnMaster = LoadBalancer.isTablesOnMaster(TEST_UTIL.getConfiguration());\n    int expectedRS = NUM_RS + (tablesOnMaster? 1: 0);\n    try (ZKWatcher zkw = TEST_UTIL.getZooKeeperWatcher()) {\n      List<String> servers = ZKUtil.listChildrenNoWatch(zkw, zkw.getZNodePaths().rsZNode);\n      assertEquals(expectedRS, servers.size());\n    }\n  }"
        ],
        [
            "HMaster::startActiveMasterManager(int)",
            "2026  \n2027  \n2028 -\n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  ",
            "  private void startActiveMasterManager(int infoPort) throws KeeperException {\n    String backupZNode = ZNodePaths.joinZNode(\n      zooKeeper.znodePaths.backupMasterAddressesZNode, serverName.toString());\n    /*\n    * Add a ZNode for ourselves in the backup master directory since we\n    * may not become the active master. If so, we want the actual active\n    * master to know we are backup masters, so that it won't assign\n    * regions to us if so configured.\n    *\n    * If we become the active master later, ActiveMasterManager will delete\n    * this node explicitly.  If we crash before then, ZooKeeper will delete\n    * this node for us since it is ephemeral.\n    */\n    LOG.info(\"Adding backup master ZNode \" + backupZNode);\n    if (!MasterAddressTracker.setMasterAddress(zooKeeper, backupZNode, serverName, infoPort)) {\n      LOG.warn(\"Failed create of \" + backupZNode + \" by \" + serverName);\n    }\n    this.activeMasterManager.setInfoPort(infoPort);\n    int timeout = conf.getInt(HConstants.ZK_SESSION_TIMEOUT, HConstants.DEFAULT_ZK_SESSION_TIMEOUT);\n    // If we're a backup master, stall until a primary to write this address\n    if (conf.getBoolean(HConstants.MASTER_TYPE_BACKUP, HConstants.DEFAULT_MASTER_TYPE_BACKUP)) {\n      LOG.debug(\"HMaster started in backup mode. Stalling until master znode is written.\");\n      // This will only be a minute or so while the cluster starts up,\n      // so don't worry about setting watches on the parent znode\n      while (!activeMasterManager.hasActiveMaster()) {\n        LOG.debug(\"Waiting for master address and cluster state znode to be written.\");\n        Threads.sleep(timeout);\n      }\n    }\n    MonitoredTask status = TaskMonitor.get().createStatus(\"Master startup\");\n    status.setDescription(\"Master startup\");\n    try {\n      if (activeMasterManager.blockUntilBecomingActiveMaster(timeout, status)) {\n        finishActiveMasterInitialization(status);\n      }\n    } catch (Throwable t) {\n      status.setStatus(\"Failed to become active: \" + t.getMessage());\n      LOG.error(HBaseMarkers.FATAL, \"Failed to become active master\", t);\n      // HBASE-5680: Likely hadoop23 vs hadoop 20.x/1.x incompatibility\n      if (t instanceof NoClassDefFoundError && t.getMessage().\n          contains(\"org/apache/hadoop/hdfs/protocol/HdfsConstants$SafeModeAction\")) {\n        // improved error message for this special case\n        abort(\"HBase is having a problem with its Hadoop jars.  You may need to recompile \" +\n          \"HBase against Hadoop version \" + org.apache.hadoop.util.VersionInfo.getVersion() +\n          \" or change your hadoop jars to start properly\", t);\n      } else {\n        abort(\"Unhandled exception. Starting shutdown.\", t);\n      }\n    } finally {\n      status.cleanup();\n    }\n  }",
            "2026  \n2027  \n2028 +\n2029  \n2030  \n2031  \n2032  \n2033  \n2034  \n2035  \n2036  \n2037  \n2038  \n2039  \n2040  \n2041  \n2042  \n2043  \n2044  \n2045  \n2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  ",
            "  private void startActiveMasterManager(int infoPort) throws KeeperException {\n    String backupZNode = ZNodePaths.joinZNode(\n      zooKeeper.getZNodePaths().backupMasterAddressesZNode, serverName.toString());\n    /*\n    * Add a ZNode for ourselves in the backup master directory since we\n    * may not become the active master. If so, we want the actual active\n    * master to know we are backup masters, so that it won't assign\n    * regions to us if so configured.\n    *\n    * If we become the active master later, ActiveMasterManager will delete\n    * this node explicitly.  If we crash before then, ZooKeeper will delete\n    * this node for us since it is ephemeral.\n    */\n    LOG.info(\"Adding backup master ZNode \" + backupZNode);\n    if (!MasterAddressTracker.setMasterAddress(zooKeeper, backupZNode, serverName, infoPort)) {\n      LOG.warn(\"Failed create of \" + backupZNode + \" by \" + serverName);\n    }\n    this.activeMasterManager.setInfoPort(infoPort);\n    int timeout = conf.getInt(HConstants.ZK_SESSION_TIMEOUT, HConstants.DEFAULT_ZK_SESSION_TIMEOUT);\n    // If we're a backup master, stall until a primary to write this address\n    if (conf.getBoolean(HConstants.MASTER_TYPE_BACKUP, HConstants.DEFAULT_MASTER_TYPE_BACKUP)) {\n      LOG.debug(\"HMaster started in backup mode. Stalling until master znode is written.\");\n      // This will only be a minute or so while the cluster starts up,\n      // so don't worry about setting watches on the parent znode\n      while (!activeMasterManager.hasActiveMaster()) {\n        LOG.debug(\"Waiting for master address and cluster state znode to be written.\");\n        Threads.sleep(timeout);\n      }\n    }\n    MonitoredTask status = TaskMonitor.get().createStatus(\"Master startup\");\n    status.setDescription(\"Master startup\");\n    try {\n      if (activeMasterManager.blockUntilBecomingActiveMaster(timeout, status)) {\n        finishActiveMasterInitialization(status);\n      }\n    } catch (Throwable t) {\n      status.setStatus(\"Failed to become active: \" + t.getMessage());\n      LOG.error(HBaseMarkers.FATAL, \"Failed to become active master\", t);\n      // HBASE-5680: Likely hadoop23 vs hadoop 20.x/1.x incompatibility\n      if (t instanceof NoClassDefFoundError && t.getMessage().\n          contains(\"org/apache/hadoop/hdfs/protocol/HdfsConstants$SafeModeAction\")) {\n        // improved error message for this special case\n        abort(\"HBase is having a problem with its Hadoop jars.  You may need to recompile \" +\n          \"HBase against Hadoop version \" + org.apache.hadoop.util.VersionInfo.getVersion() +\n          \" or change your hadoop jars to start properly\", t);\n      } else {\n        abort(\"Unhandled exception. Starting shutdown.\", t);\n      }\n    } finally {\n      status.cleanup();\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::getChildren(String,boolean)",
            " 298  \n 299  \n 300  \n 301  \n 302  \n 303 -\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  ",
            "  /**\n   * getChildren is an idempotent operation. Retry before throwing exception\n   * @return List of children znodes\n   */\n  public List<String> getChildren(String path, boolean watch)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getChildren\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<String> children = checkZk().getChildren(path, watch);\n          return children;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 298  \n 299  \n 300  \n 301  \n 302  \n 303 +\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  ",
            "  /**\n   * getChildren is an idempotent operation. Retry before throwing exception\n   * @return List of children znodes\n   */\n  public List<String> getChildren(String path, boolean watch)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getChildren\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<String> children = checkZk().getChildren(path, watch);\n          return children;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"getChildren\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "TestZKMulti::testMultiFailure()",
            " 216  \n 217  \n 218 -\n 219 -\n 220 -\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 -\n 228 -\n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  ",
            "  @Test\n  public void testMultiFailure() throws Exception {\n    String pathX = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testMultiFailureX\");\n    String pathY = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testMultiFailureY\");\n    String pathZ = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testMultiFailureZ\");\n    // create X that we will use to fail create later\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX)));\n    ZKUtil.multiOrSequential(zkw, ops, false);\n\n    // fail one of each create ,setData, delete\n    String pathV = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testMultiFailureV\");\n    String pathW = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testMultiFailureW\");\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX))); // fail  -- already exists\n    ops.add(ZKUtilOp.setData(pathY, Bytes.toBytes(pathY))); // fail -- doesn't exist\n    ops.add(ZKUtilOp.deleteNodeFailSilent(pathZ)); // fail -- doesn't exist\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathV))); // pass\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathW))); // pass\n    boolean caughtNodeExists = false;\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NodeExistsException nee) {\n      // check first operation that fails throws exception\n      caughtNodeExists = true;\n    }\n    assertTrue(caughtNodeExists);\n    // check that no modifications were made\n    assertFalse(ZKUtil.checkExists(zkw, pathX) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathY) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathZ) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathW) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathV) == -1);\n\n    // test that with multiple failures, throws an exception corresponding to first failure in list\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.setData(pathY, Bytes.toBytes(pathY))); // fail -- doesn't exist\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX))); // fail -- exists\n    boolean caughtNoNode = false;\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NoNodeException nne) {\n      // check first operation that fails throws exception\n      caughtNoNode = true;\n    }\n    assertTrue(caughtNoNode);\n    // check that no modifications were made\n    assertFalse(ZKUtil.checkExists(zkw, pathX) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathY) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathZ) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathW) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathV) == -1);\n  }",
            " 216  \n 217  \n 218 +\n 219 +\n 220 +\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 +\n 228 +\n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  ",
            "  @Test\n  public void testMultiFailure() throws Exception {\n    String pathX = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testMultiFailureX\");\n    String pathY = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testMultiFailureY\");\n    String pathZ = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testMultiFailureZ\");\n    // create X that we will use to fail create later\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX)));\n    ZKUtil.multiOrSequential(zkw, ops, false);\n\n    // fail one of each create ,setData, delete\n    String pathV = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testMultiFailureV\");\n    String pathW = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testMultiFailureW\");\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX))); // fail  -- already exists\n    ops.add(ZKUtilOp.setData(pathY, Bytes.toBytes(pathY))); // fail -- doesn't exist\n    ops.add(ZKUtilOp.deleteNodeFailSilent(pathZ)); // fail -- doesn't exist\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathV))); // pass\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathW))); // pass\n    boolean caughtNodeExists = false;\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NodeExistsException nee) {\n      // check first operation that fails throws exception\n      caughtNodeExists = true;\n    }\n    assertTrue(caughtNodeExists);\n    // check that no modifications were made\n    assertFalse(ZKUtil.checkExists(zkw, pathX) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathY) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathZ) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathW) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathV) == -1);\n\n    // test that with multiple failures, throws an exception corresponding to first failure in list\n    ops = new LinkedList<>();\n    ops.add(ZKUtilOp.setData(pathY, Bytes.toBytes(pathY))); // fail -- doesn't exist\n    ops.add(ZKUtilOp.createAndFailSilent(pathX, Bytes.toBytes(pathX))); // fail -- exists\n    boolean caughtNoNode = false;\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NoNodeException nne) {\n      // check first operation that fails throws exception\n      caughtNoNode = true;\n    }\n    assertTrue(caughtNoNode);\n    // check that no modifications were made\n    assertFalse(ZKUtil.checkExists(zkw, pathX) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathY) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathZ) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathW) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathV) == -1);\n  }"
        ],
        [
            "ZKUtil::connect(Configuration,String,Watcher)",
            " 116  \n 117  \n 118 -\n 119  \n 120  ",
            "  public static RecoverableZooKeeper connect(Configuration conf, String ensemble,\n      Watcher watcher)\n  throws IOException {\n    return connect(conf, ensemble, watcher, null);\n  }",
            " 116  \n 117  \n 118 +\n 119  \n 120  ",
            "  public static RecoverableZooKeeper connect(Configuration conf, String ensemble,\n      Watcher watcher)\n    throws IOException {\n    return connect(conf, ensemble, watcher, null);\n  }"
        ],
        [
            "ZKUtil::createAndFailSilent(ZKWatcher,CreateAndFailSilent)",
            "1140  \n1141 -\n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  ",
            "  private static void createAndFailSilent(ZKWatcher zkw, CreateAndFailSilent cafs)\n  throws KeeperException {\n    CreateRequest create = (CreateRequest)toZooKeeperOp(zkw, cafs).toRequestRecord();\n    String znode = create.getPath();\n    try {\n      RecoverableZooKeeper zk = zkw.getRecoverableZooKeeper();\n      if (zk.exists(znode, false) == null) {\n        zk.create(znode, create.getData(), create.getAcl(), CreateMode.fromFlag(create.getFlags()));\n      }\n    } catch(KeeperException.NodeExistsException nee) {\n    } catch(KeeperException.NoAuthException nee){\n      try {\n        if (null == zkw.getRecoverableZooKeeper().exists(znode, false)) {\n          // If we failed to create the file and it does not already exist.\n          throw(nee);\n        }\n      } catch (InterruptedException ie) {\n        zkw.interruptedException(ie);\n      }\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n  }",
            "1140  \n1141 +\n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  ",
            "  private static void createAndFailSilent(ZKWatcher zkw, CreateAndFailSilent cafs)\n    throws KeeperException {\n    CreateRequest create = (CreateRequest)toZooKeeperOp(zkw, cafs).toRequestRecord();\n    String znode = create.getPath();\n    try {\n      RecoverableZooKeeper zk = zkw.getRecoverableZooKeeper();\n      if (zk.exists(znode, false) == null) {\n        zk.create(znode, create.getData(), create.getAcl(), CreateMode.fromFlag(create.getFlags()));\n      }\n    } catch(KeeperException.NodeExistsException nee) {\n    } catch(KeeperException.NoAuthException nee){\n      try {\n        if (null == zkw.getRecoverableZooKeeper().exists(znode, false)) {\n          // If we failed to create the file and it does not already exist.\n          throw(nee);\n        }\n      } catch (InterruptedException ie) {\n        zkw.interruptedException(ie);\n      }\n    } catch(InterruptedException ie) {\n      zkw.interruptedException(ie);\n    }\n  }"
        ],
        [
            "ZKUtil::deleteNodeRecursively(ZKWatcher,String)",
            "1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275 -\n1276  \n1277  ",
            "  /**\n   * Delete the specified node and all of it's children.\n   * <p>\n   * If the node does not exist, just returns.\n   * <p>\n   * Sets no watches. Throws all exceptions besides dealing with deletion of\n   * children.\n   */\n  public static void deleteNodeRecursively(ZKWatcher zkw, String node)\n  throws KeeperException {\n    deleteNodeRecursivelyMultiOrSequential(zkw, true, node);\n  }",
            "1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275 +\n1276  \n1277  ",
            "  /**\n   * Delete the specified node and all of it's children.\n   * <p>\n   * If the node does not exist, just returns.\n   * <p>\n   * Sets no watches. Throws all exceptions besides dealing with deletion of\n   * children.\n   */\n  public static void deleteNodeRecursively(ZKWatcher zkw, String node)\n    throws KeeperException {\n    deleteNodeRecursivelyMultiOrSequential(zkw, true, node);\n  }"
        ],
        [
            "RegionServerTracker::nodeDeleted(String)",
            " 128  \n 129  \n 130 -\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "  @Override\n  public void nodeDeleted(String path) {\n    if (path.startsWith(watcher.znodePaths.rsZNode)) {\n      String serverName = ZKUtil.getNodeName(path);\n      LOG.info(\"RegionServer ephemeral node deleted, processing expiration [\" +\n        serverName + \"]\");\n      ServerName sn = ServerName.parseServerName(serverName);\n      if (!serverManager.isServerOnline(sn)) {\n        LOG.warn(serverName.toString() + \" is not online or isn't known to the master.\"+\n         \"The latter could be caused by a DNS misconfiguration.\");\n        return;\n      }\n      remove(sn);\n      this.serverManager.expireServer(sn);\n    }\n  }",
            " 128  \n 129  \n 130 +\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "  @Override\n  public void nodeDeleted(String path) {\n    if (path.startsWith(watcher.getZNodePaths().rsZNode)) {\n      String serverName = ZKUtil.getNodeName(path);\n      LOG.info(\"RegionServer ephemeral node deleted, processing expiration [\" +\n        serverName + \"]\");\n      ServerName sn = ServerName.parseServerName(serverName);\n      if (!serverManager.isServerOnline(sn)) {\n        LOG.warn(serverName.toString() + \" is not online or isn't known to the master.\"+\n         \"The latter could be caused by a DNS misconfiguration.\");\n        return;\n      }\n      remove(sn);\n      this.serverManager.expireServer(sn);\n    }\n  }"
        ],
        [
            "TestReplicationTrackerZKImpl::setUpBeforeClass()",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 -\n  87  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    utility = new HBaseTestingUtility();\n    utility.startMiniZKCluster();\n    conf = utility.getConfiguration();\n    ZKWatcher zk = HBaseTestingUtility.getZooKeeperWatcher(utility);\n    ZKUtil.createWithParents(zk, zk.znodePaths.rsZNode);\n  }",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 +\n  87  ",
            "  @BeforeClass\n  public static void setUpBeforeClass() throws Exception {\n    utility = new HBaseTestingUtility();\n    utility.startMiniZKCluster();\n    conf = utility.getConfiguration();\n    ZKWatcher zk = HBaseTestingUtility.getZooKeeperWatcher(utility);\n    ZKUtil.createWithParents(zk, zk.getZNodePaths().rsZNode);\n  }"
        ],
        [
            "RecoverableZooKeeper::RecoverableZooKeeper(String,int,Watcher,int,int,int)",
            "  86  \n  87  \n  88 -\n  89  \n  90  \n  91  ",
            "  public RecoverableZooKeeper(String quorumServers, int sessionTimeout,\n      Watcher watcher, int maxRetries, int retryIntervalMillis, int maxSleepTime)\n  throws IOException {\n    this(quorumServers, sessionTimeout, watcher, maxRetries, retryIntervalMillis, maxSleepTime,\n        null);\n  }",
            "  86  \n  87  \n  88 +\n  89  \n  90  \n  91  ",
            "  public RecoverableZooKeeper(String quorumServers, int sessionTimeout,\n      Watcher watcher, int maxRetries, int retryIntervalMillis, int maxSleepTime)\n    throws IOException {\n    this(quorumServers, sessionTimeout, watcher, maxRetries, retryIntervalMillis, maxSleepTime,\n        null);\n  }"
        ],
        [
            "ActiveMasterManager::handle(String)",
            " 103  \n 104 -\n 105  \n 106  \n 107  ",
            "  void handle(final String path) {\n    if (path.equals(watcher.znodePaths.masterAddressZNode) && !master.isStopped()) {\n      handleMasterNodeChange();\n    }\n  }",
            " 103  \n 104 +\n 105  \n 106  \n 107  ",
            "  void handle(final String path) {\n    if (path.equals(watcher.getZNodePaths().masterAddressZNode) && !master.isStopped()) {\n      handleMasterNodeChange();\n    }\n  }"
        ],
        [
            "ZKProcedureUtil::ZKProcedureUtil(ZKWatcher,String)",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 -\n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "  /**\n   * Top-level watcher/controller for procedures across the cluster.\n   * <p>\n   * On instantiation, this ensures the procedure znodes exist.  This however requires the passed in\n   *  watcher has been started.\n   * @param watcher watcher for the cluster ZK. Owned by <tt>this</tt> and closed via\n   *          {@link #close()}\n   * @param procDescription name of the znode describing the procedure to run\n   * @throws KeeperException when the procedure znodes cannot be created\n   */\n  public ZKProcedureUtil(ZKWatcher watcher, String procDescription)\n      throws KeeperException {\n    super(watcher);\n    // make sure we are listening for events\n    watcher.registerListener(this);\n    // setup paths for the zknodes used in procedures\n    this.baseZNode = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, procDescription);\n    acquiredZnode = ZNodePaths.joinZNode(baseZNode, ACQUIRED_BARRIER_ZNODE_DEFAULT);\n    reachedZnode = ZNodePaths.joinZNode(baseZNode, REACHED_BARRIER_ZNODE_DEFAULT);\n    abortZnode = ZNodePaths.joinZNode(baseZNode, ABORT_ZNODE_DEFAULT);\n\n    // first make sure all the ZK nodes exist\n    // make sure all the parents exist (sometimes not the case in tests)\n    ZKUtil.createWithParents(watcher, acquiredZnode);\n    // regular create because all the parents exist\n    ZKUtil.createAndFailSilent(watcher, reachedZnode);\n    ZKUtil.createAndFailSilent(watcher, abortZnode);\n  }",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "  /**\n   * Top-level watcher/controller for procedures across the cluster.\n   * <p>\n   * On instantiation, this ensures the procedure znodes exist.  This however requires the passed in\n   *  watcher has been started.\n   * @param watcher watcher for the cluster ZK. Owned by <tt>this</tt> and closed via\n   *          {@link #close()}\n   * @param procDescription name of the znode describing the procedure to run\n   * @throws KeeperException when the procedure znodes cannot be created\n   */\n  public ZKProcedureUtil(ZKWatcher watcher, String procDescription)\n      throws KeeperException {\n    super(watcher);\n    // make sure we are listening for events\n    watcher.registerListener(this);\n    // setup paths for the zknodes used in procedures\n    this.baseZNode = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, procDescription);\n    acquiredZnode = ZNodePaths.joinZNode(baseZNode, ACQUIRED_BARRIER_ZNODE_DEFAULT);\n    reachedZnode = ZNodePaths.joinZNode(baseZNode, REACHED_BARRIER_ZNODE_DEFAULT);\n    abortZnode = ZNodePaths.joinZNode(baseZNode, ABORT_ZNODE_DEFAULT);\n\n    // first make sure all the ZK nodes exist\n    // make sure all the parents exist (sometimes not the case in tests)\n    ZKUtil.createWithParents(watcher, acquiredZnode);\n    // regular create because all the parents exist\n    ZKUtil.createAndFailSilent(watcher, reachedZnode);\n    ZKUtil.createAndFailSilent(watcher, abortZnode);\n  }"
        ],
        [
            "ZKSecretWatcher::ZKSecretWatcher(Configuration,ZKWatcher,AuthenticationTokenSecretManager)",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58  \n  59  ",
            "  public ZKSecretWatcher(Configuration conf,\n      ZKWatcher watcher,\n      AuthenticationTokenSecretManager secretManager) {\n    super(watcher);\n    this.secretManager = secretManager;\n    String keyZNodeParent = conf.get(\"zookeeper.znode.tokenauth.parent\", DEFAULT_ROOT_NODE);\n    this.baseKeyZNode = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, keyZNodeParent);\n    this.keysParentZNode = ZNodePaths.joinZNode(baseKeyZNode, DEFAULT_KEYS_PARENT);\n  }",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58  \n  59  ",
            "  public ZKSecretWatcher(Configuration conf,\n      ZKWatcher watcher,\n      AuthenticationTokenSecretManager secretManager) {\n    super(watcher);\n    this.secretManager = secretManager;\n    String keyZNodeParent = conf.get(\"zookeeper.znode.tokenauth.parent\", DEFAULT_ROOT_NODE);\n    this.baseKeyZNode = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, keyZNodeParent);\n    this.keysParentZNode = ZNodePaths.joinZNode(baseKeyZNode, DEFAULT_KEYS_PARENT);\n  }"
        ],
        [
            "LoadBalancerTracker::setBalancerOn(boolean)",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 -\n  71  \n  72 -\n  73  \n  74 -\n  75  ",
            "  /**\n   * Set the balancer on/off.\n   *\n   * @param balancerOn true if the balancher should be on, false otherwise\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public void setBalancerOn(boolean balancerOn) throws KeeperException {\n    byte [] upData = toByteArray(balancerOn);\n\n    try {\n      ZKUtil.setData(watcher, watcher.znodePaths.balancerZNode, upData);\n    } catch(KeeperException.NoNodeException nne) {\n      ZKUtil.createAndWatch(watcher, watcher.znodePaths.balancerZNode, upData);\n    }\n    super.nodeDataChanged(watcher.znodePaths.balancerZNode);\n  }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 +\n  71  \n  72 +\n  73  \n  74 +\n  75  ",
            "  /**\n   * Set the balancer on/off.\n   *\n   * @param balancerOn true if the balancher should be on, false otherwise\n   * @throws KeeperException if a ZooKeeper operation fails\n   */\n  public void setBalancerOn(boolean balancerOn) throws KeeperException {\n    byte [] upData = toByteArray(balancerOn);\n\n    try {\n      ZKUtil.setData(watcher, watcher.getZNodePaths().balancerZNode, upData);\n    } catch(KeeperException.NoNodeException nne) {\n      ZKUtil.createAndWatch(watcher, watcher.getZNodePaths().balancerZNode, upData);\n    }\n    super.nodeDataChanged(watcher.getZNodePaths().balancerZNode);\n  }"
        ],
        [
            "LoadBalancerTracker::LoadBalancerTracker(ZKWatcher,Abortable)",
            "  40  \n  41  \n  42 -\n  43  ",
            "  public LoadBalancerTracker(ZKWatcher watcher,\n      Abortable abortable) {\n    super(watcher, watcher.znodePaths.balancerZNode, abortable);\n  }",
            "  40  \n  41  \n  42 +\n  43  ",
            "  public LoadBalancerTracker(ZKWatcher watcher,\n      Abortable abortable) {\n    super(watcher, watcher.getZNodePaths().balancerZNode, abortable);\n  }"
        ],
        [
            "MasterAddressTracker::getBackupMasterInfoPort(ServerName)",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "  /**\n   * Get the info port of the backup master if it is available.\n   * Return 0 if no backup master or zookeeper is unavailable\n   * @param sn server name of backup master\n   * @return info port or 0 if timed out or exceptions\n   */\n  public int getBackupMasterInfoPort(final ServerName sn) {\n    String backupZNode = ZNodePaths.joinZNode(watcher.znodePaths.backupMasterAddressesZNode,\n      sn.toString());\n    try {\n      byte[] data = ZKUtil.getData(watcher, backupZNode);\n      final ZooKeeperProtos.Master backup = parse(data);\n      if (backup == null) {\n        return 0;\n      }\n      return backup.getInfoPort();\n    } catch (Exception e) {\n      LOG.warn(\"Failed to get backup master: \" + sn + \"'s info port.\", e);\n      return 0;\n    }\n  }",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "  /**\n   * Get the info port of the backup master if it is available.\n   * Return 0 if no backup master or zookeeper is unavailable\n   * @param sn server name of backup master\n   * @return info port or 0 if timed out or exceptions\n   */\n  public int getBackupMasterInfoPort(final ServerName sn) {\n    String backupZNode = ZNodePaths.joinZNode(watcher.getZNodePaths().backupMasterAddressesZNode,\n      sn.toString());\n    try {\n      byte[] data = ZKUtil.getData(watcher, backupZNode);\n      final ZooKeeperProtos.Master backup = parse(data);\n      if (backup == null) {\n        return 0;\n      }\n      return backup.getInfoPort();\n    } catch (Exception e) {\n      LOG.warn(\"Failed to get backup master: \" + sn + \"'s info port.\", e);\n      return 0;\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::getData(String,Watcher,Stat)",
            " 329  \n 330  \n 331  \n 332  \n 333  \n 334 -\n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  ",
            "  /**\n   * getData is an idempotent operation. Retry before throwing exception\n   * @return Data\n   */\n  public byte[] getData(String path, Watcher watcher, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          byte[] revData = checkZk().getData(path, watcher, stat);\n          return ZKMetadata.removeMetaData(revData);\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 329  \n 330  \n 331  \n 332  \n 333  \n 334 +\n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  ",
            "  /**\n   * getData is an idempotent operation. Retry before throwing exception\n   * @return Data\n   */\n  public byte[] getData(String path, Watcher watcher, Stat stat)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          byte[] revData = checkZk().getData(path, watcher, stat);\n          return ZKMetadata.removeMetaData(revData);\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "ZKSplitLogManagerCoordination::lookForOrphans()",
            " 466  \n 467  \n 468  \n 469 -\n 470  \n 471 -\n 472  \n 473  \n 474  \n 475 -\n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483 -\n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  ",
            "  private void lookForOrphans() {\n    List<String> orphans;\n    try {\n      orphans = ZKUtil.listChildrenNoWatch(this.watcher, this.watcher.znodePaths.splitLogZNode);\n      if (orphans == null) {\n        LOG.warn(\"Could not get children of \" + this.watcher.znodePaths.splitLogZNode);\n        return;\n      }\n    } catch (KeeperException e) {\n      LOG.warn(\"Could not get children of \" + this.watcher.znodePaths.splitLogZNode + \" \"\n          + StringUtils.stringifyException(e));\n      return;\n    }\n    int rescan_nodes = 0;\n    int listSize = orphans.size();\n    for (int i = 0; i < listSize; i++) {\n      String path = orphans.get(i);\n      String nodepath = ZNodePaths.joinZNode(watcher.znodePaths.splitLogZNode, path);\n      if (ZKSplitLog.isRescanNode(watcher, nodepath)) {\n        rescan_nodes++;\n        LOG.debug(\"Found orphan rescan node \" + path);\n      } else {\n        LOG.info(\"Found orphan task \" + path);\n      }\n      getDataSetWatch(nodepath, zkretries);\n    }\n    LOG.info(\"Found \" + (orphans.size() - rescan_nodes) + \" orphan tasks and \" + rescan_nodes\n        + \" rescan nodes\");\n  }",
            " 467  \n 468  \n 469  \n 470 +\n 471 +\n 472  \n 473 +\n 474  \n 475  \n 476  \n 477 +\n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 +\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  ",
            "  private void lookForOrphans() {\n    List<String> orphans;\n    try {\n      orphans = ZKUtil.listChildrenNoWatch(this.watcher,\n              this.watcher.getZNodePaths().splitLogZNode);\n      if (orphans == null) {\n        LOG.warn(\"Could not get children of \" + this.watcher.getZNodePaths().splitLogZNode);\n        return;\n      }\n    } catch (KeeperException e) {\n      LOG.warn(\"Could not get children of \" + this.watcher.getZNodePaths().splitLogZNode + \" \"\n          + StringUtils.stringifyException(e));\n      return;\n    }\n    int rescan_nodes = 0;\n    int listSize = orphans.size();\n    for (int i = 0; i < listSize; i++) {\n      String path = orphans.get(i);\n      String nodepath = ZNodePaths.joinZNode(watcher.getZNodePaths().splitLogZNode, path);\n      if (ZKSplitLog.isRescanNode(watcher, nodepath)) {\n        rescan_nodes++;\n        LOG.debug(\"Found orphan rescan node \" + path);\n      } else {\n        LOG.info(\"Found orphan task \" + path);\n      }\n      getDataSetWatch(nodepath, zkretries);\n    }\n    LOG.info(\"Found \" + (orphans.size() - rescan_nodes) + \" orphan tasks and \" + rescan_nodes\n        + \" rescan nodes\");\n  }"
        ],
        [
            "DrainingServerTracker::nodeDeleted(String)",
            " 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  ",
            "  @Override\n  public void nodeDeleted(final String path) {\n    if(path.startsWith(watcher.znodePaths.drainingZNode)) {\n      final ServerName sn = ServerName.valueOf(ZKUtil.getNodeName(path));\n      LOG.info(\"Draining RS node deleted, removing from list [\" +\n          sn + \"]\");\n      remove(sn);\n    }\n  }",
            " 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  ",
            "  @Override\n  public void nodeDeleted(final String path) {\n    if(path.startsWith(watcher.getZNodePaths().drainingZNode)) {\n      final ServerName sn = ServerName.valueOf(ZKUtil.getNodeName(path));\n      LOG.info(\"Draining RS node deleted, removing from list [\" +\n          sn + \"]\");\n      remove(sn);\n    }\n  }"
        ],
        [
            "MasterAddressTracker::MasterAddressTracker(ZKWatcher,Abortable)",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 -\n  69  ",
            "  /**\n   * Construct a master address listener with the specified\n   * <code>zookeeper</code> reference.\n   * <p>\n   * This constructor does not trigger any actions, you must call methods\n   * explicitly.  Normally you will just want to execute {@link #start()} to\n   * begin tracking of the master address.\n   *\n   * @param watcher zk reference and watcher\n   * @param abortable abortable in case of fatal error\n   */\n  public MasterAddressTracker(ZKWatcher watcher, Abortable abortable) {\n    super(watcher, watcher.znodePaths.masterAddressZNode, abortable);\n  }",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69  ",
            "  /**\n   * Construct a master address listener with the specified\n   * <code>zookeeper</code> reference.\n   * <p>\n   * This constructor does not trigger any actions, you must call methods\n   * explicitly.  Normally you will just want to execute {@link #start()} to\n   * begin tracking of the master address.\n   *\n   * @param watcher zk reference and watcher\n   * @param abortable abortable in case of fatal error\n   */\n  public MasterAddressTracker(ZKWatcher watcher, Abortable abortable) {\n    super(watcher, watcher.getZNodePaths().masterAddressZNode, abortable);\n  }"
        ],
        [
            "HBaseReplicationEndpoint::fetchSlavesAddresses(ZKWatcher)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 -\n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "  /**\n   * Get the list of all the region servers from the specified peer\n   * @param zkw zk connection to use\n   * @return list of region server addresses or an empty list if the slave is unavailable\n   */\n  protected static List<ServerName> fetchSlavesAddresses(ZKWatcher zkw)\n      throws KeeperException {\n    List<String> children = ZKUtil.listChildrenAndWatchForNewChildren(zkw, zkw.znodePaths.rsZNode);\n    if (children == null) {\n      return Collections.emptyList();\n    }\n    List<ServerName> addresses = new ArrayList<>(children.size());\n    for (String child : children) {\n      addresses.add(ServerName.parseServerName(child));\n    }\n    return addresses;\n  }",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159 +\n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  ",
            "  /**\n   * Get the list of all the region servers from the specified peer\n   * @param zkw zk connection to use\n   * @return list of region server addresses or an empty list if the slave is unavailable\n   */\n  protected static List<ServerName> fetchSlavesAddresses(ZKWatcher zkw)\n      throws KeeperException {\n    List<String> children = ZKUtil.listChildrenAndWatchForNewChildren(zkw,\n            zkw.getZNodePaths().rsZNode);\n    if (children == null) {\n      return Collections.emptyList();\n    }\n    List<ServerName> addresses = new ArrayList<>(children.size());\n    for (String child : children) {\n      addresses.add(ServerName.parseServerName(child));\n    }\n    return addresses;\n  }"
        ],
        [
            "ZKPermissionWatcher::deleteTableACLNode(TableName)",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281 -\n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  ",
            "  /***\n   * Delete the acl notify node of table\n   * @param tableName\n   */\n  public void deleteTableACLNode(final TableName tableName) {\n    String zkNode = ZNodePaths.joinZNode(watcher.znodePaths.baseZNode, ACL_NODE);\n    zkNode = ZNodePaths.joinZNode(zkNode, tableName.getNameAsString());\n\n    try {\n      ZKUtil.deleteNode(watcher, zkNode);\n    } catch (KeeperException.NoNodeException e) {\n      LOG.warn(\"No acl notify node of table '\" + tableName + \"'\");\n    } catch (KeeperException e) {\n      LOG.error(\"Failed deleting acl node of table '\" + tableName + \"'\", e);\n      watcher.abort(\"Failed deleting node \" + zkNode, e);\n    }\n  }",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281 +\n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  ",
            "  /***\n   * Delete the acl notify node of table\n   * @param tableName\n   */\n  public void deleteTableACLNode(final TableName tableName) {\n    String zkNode = ZNodePaths.joinZNode(watcher.getZNodePaths().baseZNode, ACL_NODE);\n    zkNode = ZNodePaths.joinZNode(zkNode, tableName.getNameAsString());\n\n    try {\n      ZKUtil.deleteNode(watcher, zkNode);\n    } catch (KeeperException.NoNodeException e) {\n      LOG.warn(\"No acl notify node of table '\" + tableName + \"'\");\n    } catch (KeeperException e) {\n      LOG.error(\"Failed deleting acl node of table '\" + tableName + \"'\", e);\n      watcher.abort(\"Failed deleting node \" + zkNode, e);\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::createSequential(String,byte,List,CreateMode)",
            " 588  \n 589  \n 590 -\n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  ",
            "  private String createSequential(String path, byte[] data,\n      List<ACL> acl, CreateMode createMode)\n  throws KeeperException, InterruptedException {\n    RetryCounter retryCounter = retryCounterFactory.create();\n    boolean first = true;\n    String newPath = path+this.identifier;\n    while (true) {\n      try {\n        if (!first) {\n          // Check if we succeeded on a previous attempt\n          String previousResult = findPreviousSequentialNode(newPath);\n          if (previousResult != null) {\n            return previousResult;\n          }\n        }\n        first = false;\n        long startTime = EnvironmentEdgeManager.currentTime();\n        String nodePath = checkZk().create(newPath, data, acl, createMode);\n        return nodePath;\n      } catch (KeeperException e) {\n        switch (e.code()) {\n          case CONNECTIONLOSS:\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n          case OPERATIONTIMEOUT:\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n\n          default:\n            throw e;\n        }\n      }\n      retryCounter.sleepUntilNextRetry();\n    }\n  }",
            " 588  \n 589  \n 590 +\n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  ",
            "  private String createSequential(String path, byte[] data,\n      List<ACL> acl, CreateMode createMode)\n    throws KeeperException, InterruptedException {\n    RetryCounter retryCounter = retryCounterFactory.create();\n    boolean first = true;\n    String newPath = path+this.identifier;\n    while (true) {\n      try {\n        if (!first) {\n          // Check if we succeeded on a previous attempt\n          String previousResult = findPreviousSequentialNode(newPath);\n          if (previousResult != null) {\n            return previousResult;\n          }\n        }\n        first = false;\n        long startTime = EnvironmentEdgeManager.currentTime();\n        String nodePath = checkZk().create(newPath, data, acl, createMode);\n        return nodePath;\n      } catch (KeeperException e) {\n        switch (e.code()) {\n          case CONNECTIONLOSS:\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n          case OPERATIONTIMEOUT:\n            retryOrThrow(retryCounter, e, \"create\");\n            break;\n\n          default:\n            throw e;\n        }\n      }\n      retryCounter.sleepUntilNextRetry();\n    }\n  }"
        ],
        [
            "ZKUtil::createWithParents(ZKWatcher,String)",
            "1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176 -\n1177  \n1178  ",
            "  /**\n   * Creates the specified node and all parent nodes required for it to exist.\n   *\n   * No watches are set and no errors are thrown if the node already exists.\n   *\n   * The nodes created are persistent and open access.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static void createWithParents(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    createWithParents(zkw, znode, new byte[0]);\n  }",
            "1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  \n1175  \n1176 +\n1177  \n1178  ",
            "  /**\n   * Creates the specified node and all parent nodes required for it to exist.\n   *\n   * No watches are set and no errors are thrown if the node already exists.\n   *\n   * The nodes created are persistent and open access.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static void createWithParents(ZKWatcher zkw, String znode)\n    throws KeeperException {\n    createWithParents(zkw, znode, new byte[0]);\n  }"
        ],
        [
            "LoadBalancerTracker::parseFrom(byte)",
            "  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "  private LoadBalancerProtos.LoadBalancerState parseFrom(byte [] pbBytes)\n  throws DeserializationException {\n    ProtobufUtil.expectPBMagicPrefix(pbBytes);\n    LoadBalancerProtos.LoadBalancerState.Builder builder =\n      LoadBalancerProtos.LoadBalancerState.newBuilder();\n    try {\n      int magicLen = ProtobufUtil.lengthOfPBMagic();\n      ProtobufUtil.mergeFrom(builder, pbBytes, magicLen, pbBytes.length - magicLen);\n    } catch (IOException e) {\n      throw new DeserializationException(e);\n    }\n    return builder.build();\n  }",
            "  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "  private LoadBalancerProtos.LoadBalancerState parseFrom(byte [] pbBytes)\n    throws DeserializationException {\n    ProtobufUtil.expectPBMagicPrefix(pbBytes);\n    LoadBalancerProtos.LoadBalancerState.Builder builder =\n      LoadBalancerProtos.LoadBalancerState.newBuilder();\n    try {\n      int magicLen = ProtobufUtil.lengthOfPBMagic();\n      ProtobufUtil.mergeFrom(builder, pbBytes, magicLen, pbBytes.length - magicLen);\n    } catch (IOException e) {\n      throw new DeserializationException(e);\n    }\n    return builder.build();\n  }"
        ],
        [
            "ZKUtil::setData(ZKWatcher,SetData)",
            " 852  \n 853 -\n 854  \n 855  \n 856  ",
            "  private static void setData(ZKWatcher zkw, SetData setData)\n  throws KeeperException, KeeperException.NoNodeException {\n    SetDataRequest sd = (SetDataRequest)toZooKeeperOp(zkw, setData).toRequestRecord();\n    setData(zkw, sd.getPath(), sd.getData(), sd.getVersion());\n  }",
            " 852  \n 853 +\n 854  \n 855  \n 856  ",
            "  private static void setData(ZKWatcher zkw, SetData setData)\n    throws KeeperException, KeeperException.NoNodeException {\n    SetDataRequest sd = (SetDataRequest)toZooKeeperOp(zkw, setData).toRequestRecord();\n    setData(zkw, sd.getPath(), sd.getData(), sd.getVersion());\n  }"
        ],
        [
            "ZKUtil::getDataAndWatch(ZKWatcher,String)",
            " 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636 -\n 637  \n 638  ",
            "  /**\n   * Get the data at the specified znode and set a watch.\n   *\n   * Returns the data and sets a watch if the node exists.  Returns null and no\n   * watch is set if the node does not exist or there is an exception.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @return data of the specified znode, or null\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static byte [] getDataAndWatch(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    return getDataInternal(zkw, znode, null, true);\n  }",
            " 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636 +\n 637  \n 638  ",
            "  /**\n   * Get the data at the specified znode and set a watch.\n   *\n   * Returns the data and sets a watch if the node exists.  Returns null and no\n   * watch is set if the node does not exist or there is an exception.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @return data of the specified znode, or null\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static byte [] getDataAndWatch(ZKWatcher zkw, String znode)\n    throws KeeperException {\n    return getDataInternal(zkw, znode, null, true);\n  }"
        ],
        [
            "ZKUtil::setData(ZKWatcher,String,byte)",
            " 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848 -\n 849  \n 850  ",
            "  /**\n   * Sets the data of the existing znode to be the specified data.  The node\n   * must exist but no checks are done on the existing data or version.\n   *\n   * <p>If the node does not exist, a {@link NoNodeException} will be thrown.\n   *\n   * <p>No watches are set but setting data will trigger other watchers of this\n   * node.\n   *\n   * <p>If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data to set for node\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static void setData(ZKWatcher zkw, String znode, byte [] data)\n  throws KeeperException, KeeperException.NoNodeException {\n    setData(zkw, (SetData)ZKUtilOp.setData(znode, data));\n  }",
            " 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848 +\n 849  \n 850  ",
            "  /**\n   * Sets the data of the existing znode to be the specified data.  The node\n   * must exist but no checks are done on the existing data or version.\n   *\n   * <p>If the node does not exist, a {@link NoNodeException} will be thrown.\n   *\n   * <p>No watches are set but setting data will trigger other watchers of this\n   * node.\n   *\n   * <p>If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data to set for node\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static void setData(ZKWatcher zkw, String znode, byte [] data)\n    throws KeeperException, KeeperException.NoNodeException {\n    setData(zkw, (SetData)ZKUtilOp.setData(znode, data));\n  }"
        ],
        [
            "RecoverableZooKeeper::getData(String,boolean,Stat)",
            " 360  \n 361  \n 362  \n 363  \n 364  \n 365 -\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "  /**\n   * getData is an idempotent operation. Retry before throwing exception\n   * @return Data\n   */\n  public byte[] getData(String path, boolean watch, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          byte[] revData = checkZk().getData(path, watch, stat);\n          return ZKMetadata.removeMetaData(revData);\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 360  \n 361  \n 362  \n 363  \n 364  \n 365 +\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "  /**\n   * getData is an idempotent operation. Retry before throwing exception\n   * @return Data\n   */\n  public byte[] getData(String path, boolean watch, Stat stat)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getData\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          byte[] revData = checkZk().getData(path, watch, stat);\n          return ZKMetadata.removeMetaData(revData);\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"getData\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "TestMetaShutdownHandler::testExpireMetaRegionServer()",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  ",
            "  /**\n   * This test will test the expire handling of a meta-carrying\n   * region server.\n   * After HBaseMiniCluster is up, we will delete the ephemeral\n   * node of the meta-carrying region server, which will trigger\n   * the expire of this region server on the master.\n   * On the other hand, we will slow down the abort process on\n   * the region server so that it is still up during the master SSH.\n   * We will check that the master SSH is still successfully done.\n   */\n  @Test\n  public void testExpireMetaRegionServer() throws Exception {\n    MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();\n    HMaster master = cluster.getMaster();\n    RegionStates regionStates = master.getAssignmentManager().getRegionStates();\n    ServerName metaServerName = regionStates.getRegionServerOfRegion(\n      HRegionInfo.FIRST_META_REGIONINFO);\n    if (master.getServerName().equals(metaServerName) || metaServerName == null\n        || !metaServerName.equals(cluster.getServerHoldingMeta())) {\n      // Move meta off master\n      metaServerName =\n          cluster.getLiveRegionServerThreads().get(0).getRegionServer().getServerName();\n      master.move(HRegionInfo.FIRST_META_REGIONINFO.getEncodedNameAsBytes(),\n        Bytes.toBytes(metaServerName.getServerName()));\n      TEST_UTIL.waitUntilNoRegionsInTransition(60000);\n      metaServerName = regionStates.getRegionServerOfRegion(HRegionInfo.FIRST_META_REGIONINFO);\n    }\n    RegionState metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper());\n    assertEquals(\"Wrong state for meta!\", RegionState.State.OPEN, metaState.getState());\n    assertNotEquals(\"Meta is on master!\", metaServerName, master.getServerName());\n\n    // Delete the ephemeral node of the meta-carrying region server.\n    // This is trigger the expire of this region server on the master.\n    String rsEphemeralNodePath =\n        ZNodePaths.joinZNode(master.getZooKeeper().znodePaths.rsZNode, metaServerName.toString());\n    ZKUtil.deleteNode(master.getZooKeeper(), rsEphemeralNodePath);\n    LOG.info(\"Deleted the znode for the RegionServer hosting hbase:meta; waiting on SSH\");\n    // Wait for SSH to finish\n    final ServerManager serverManager = master.getServerManager();\n    final ServerName priorMetaServerName = metaServerName;\n    TEST_UTIL.waitFor(120000, 200, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        return !serverManager.isServerOnline(priorMetaServerName)\n            && !serverManager.areDeadServersInProgress();\n      }\n    });\n    LOG.info(\"Past wait on RIT\");\n    TEST_UTIL.waitUntilNoRegionsInTransition(60000);\n    // Now, make sure meta is assigned\n    assertTrue(\"Meta should be assigned\",\n      regionStates.isRegionOnline(HRegionInfo.FIRST_META_REGIONINFO));\n    // Now, make sure meta is registered in zk\n    metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper());\n    assertEquals(\"Meta should not be in transition\", RegionState.State.OPEN,\n        metaState.getState());\n    assertEquals(\"Meta should be assigned\", metaState.getServerName(),\n      regionStates.getRegionServerOfRegion(HRegionInfo.FIRST_META_REGIONINFO));\n    assertNotEquals(\"Meta should be assigned on a different server\",\n      metaState.getServerName(), metaServerName);\n  }",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 +\n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  ",
            "  /**\n   * This test will test the expire handling of a meta-carrying\n   * region server.\n   * After HBaseMiniCluster is up, we will delete the ephemeral\n   * node of the meta-carrying region server, which will trigger\n   * the expire of this region server on the master.\n   * On the other hand, we will slow down the abort process on\n   * the region server so that it is still up during the master SSH.\n   * We will check that the master SSH is still successfully done.\n   */\n  @Test\n  public void testExpireMetaRegionServer() throws Exception {\n    MiniHBaseCluster cluster = TEST_UTIL.getHBaseCluster();\n    HMaster master = cluster.getMaster();\n    RegionStates regionStates = master.getAssignmentManager().getRegionStates();\n    ServerName metaServerName = regionStates.getRegionServerOfRegion(\n      HRegionInfo.FIRST_META_REGIONINFO);\n    if (master.getServerName().equals(metaServerName) || metaServerName == null\n        || !metaServerName.equals(cluster.getServerHoldingMeta())) {\n      // Move meta off master\n      metaServerName =\n          cluster.getLiveRegionServerThreads().get(0).getRegionServer().getServerName();\n      master.move(HRegionInfo.FIRST_META_REGIONINFO.getEncodedNameAsBytes(),\n        Bytes.toBytes(metaServerName.getServerName()));\n      TEST_UTIL.waitUntilNoRegionsInTransition(60000);\n      metaServerName = regionStates.getRegionServerOfRegion(HRegionInfo.FIRST_META_REGIONINFO);\n    }\n    RegionState metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper());\n    assertEquals(\"Wrong state for meta!\", RegionState.State.OPEN, metaState.getState());\n    assertNotEquals(\"Meta is on master!\", metaServerName, master.getServerName());\n\n    // Delete the ephemeral node of the meta-carrying region server.\n    // This is trigger the expire of this region server on the master.\n    String rsEphemeralNodePath =\n        ZNodePaths.joinZNode(master.getZooKeeper().getZNodePaths().rsZNode,\n                metaServerName.toString());\n    ZKUtil.deleteNode(master.getZooKeeper(), rsEphemeralNodePath);\n    LOG.info(\"Deleted the znode for the RegionServer hosting hbase:meta; waiting on SSH\");\n    // Wait for SSH to finish\n    final ServerManager serverManager = master.getServerManager();\n    final ServerName priorMetaServerName = metaServerName;\n    TEST_UTIL.waitFor(120000, 200, new Waiter.Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        return !serverManager.isServerOnline(priorMetaServerName)\n            && !serverManager.areDeadServersInProgress();\n      }\n    });\n    LOG.info(\"Past wait on RIT\");\n    TEST_UTIL.waitUntilNoRegionsInTransition(60000);\n    // Now, make sure meta is assigned\n    assertTrue(\"Meta should be assigned\",\n      regionStates.isRegionOnline(HRegionInfo.FIRST_META_REGIONINFO));\n    // Now, make sure meta is registered in zk\n    metaState = MetaTableLocator.getMetaRegionState(master.getZooKeeper());\n    assertEquals(\"Meta should not be in transition\", RegionState.State.OPEN,\n        metaState.getState());\n    assertEquals(\"Meta should be assigned\", metaState.getServerName(),\n      regionStates.getRegionServerOfRegion(HRegionInfo.FIRST_META_REGIONINFO));\n    assertNotEquals(\"Meta should be assigned on a different server\",\n      metaState.getServerName(), metaServerName);\n  }"
        ],
        [
            "RecoverableZooKeeper::setAcl(String,List,int)",
            " 474  \n 475  \n 476  \n 477  \n 478  \n 479 -\n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  ",
            "  /**\n   * setAcl is an idempotent operation. Retry before throwing exception\n   * @return list of ACLs\n   */\n  public Stat setAcl(String path, List<ACL> acls, int version)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.setAcl\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().setACL(path, acls, version);\n          return nodeStat;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"setAcl\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"setAcl\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 474  \n 475  \n 476  \n 477  \n 478  \n 479 +\n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  ",
            "  /**\n   * setAcl is an idempotent operation. Retry before throwing exception\n   * @return list of ACLs\n   */\n  public Stat setAcl(String path, List<ACL> acls, int version)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.setAcl\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          Stat nodeStat = checkZk().setACL(path, acls, version);\n          return nodeStat;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"setAcl\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"setAcl\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "TestSplitLogWorker::testRescan()",
            " 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433 -\n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441 -\n 442  \n 443  \n 444  \n 445  \n 446  \n 447  ",
            "  @Test\n  public void testRescan() throws Exception {\n    LOG.info(\"testRescan\");\n    SplitLogCounters.resetCounters();\n    final ServerName SRV = ServerName.valueOf(\"svr,1,1\");\n    RegionServerServices mockedRS = getRegionServer(SRV);\n    slw = new SplitLogWorker(ds, TEST_UTIL.getConfiguration(), mockedRS, neverEndingTask);\n    slw.start();\n    Thread.yield(); // let the worker start\n    Thread.sleep(100);\n\n    String task = ZKSplitLog.getEncodedNodeName(zkw, \"task\");\n    SplitLogTask slt = new SplitLogTask.Unassigned(MANAGER);\n    zkw.getRecoverableZooKeeper().create(task,slt.toByteArray(), Ids.OPEN_ACL_UNSAFE,\n      CreateMode.PERSISTENT);\n\n    waitForCounter(SplitLogCounters.tot_wkr_task_acquired, 0, 1, WAIT_TIME);\n    // now the worker is busy doing the above task\n\n    // preempt the task, have it owned by another worker\n    ZKUtil.setData(zkw, task, slt.toByteArray());\n    waitForCounter(SplitLogCounters.tot_wkr_preempt_task, 0, 1, WAIT_TIME);\n\n    // create a RESCAN node\n    String rescan = ZKSplitLog.getEncodedNodeName(zkw, \"RESCAN\");\n    rescan = zkw.getRecoverableZooKeeper().create(rescan, slt.toByteArray(), Ids.OPEN_ACL_UNSAFE,\n      CreateMode.PERSISTENT_SEQUENTIAL);\n\n    waitForCounter(SplitLogCounters.tot_wkr_task_acquired, 1, 2, WAIT_TIME);\n    // RESCAN node might not have been processed if the worker became busy\n    // with the above task. preempt the task again so that now the RESCAN\n    // node is processed\n    ZKUtil.setData(zkw, task, slt.toByteArray());\n    waitForCounter(SplitLogCounters.tot_wkr_preempt_task, 1, 2, WAIT_TIME);\n    waitForCounter(SplitLogCounters.tot_wkr_task_acquired_rescan, 0, 1, WAIT_TIME);\n\n    List<String> nodes = ZKUtil.listChildrenNoWatch(zkw, zkw.znodePaths.splitLogZNode);\n    LOG.debug(Objects.toString(nodes));\n    int num = 0;\n    for (String node : nodes) {\n      num++;\n      if (node.startsWith(\"RESCAN\")) {\n        String name = ZKSplitLog.getEncodedNodeName(zkw, node);\n        String fn = ZKSplitLog.getFileName(name);\n        byte [] data = ZKUtil.getData(zkw, ZNodePaths.joinZNode(zkw.znodePaths.splitLogZNode, fn));\n        slt = SplitLogTask.parseFrom(data);\n        assertTrue(slt.toString(), slt.isDone(SRV));\n      }\n    }\n    assertEquals(2, num);\n  }",
            " 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433 +\n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441 +\n 442 +\n 443  \n 444  \n 445  \n 446  \n 447  \n 448  ",
            "  @Test\n  public void testRescan() throws Exception {\n    LOG.info(\"testRescan\");\n    SplitLogCounters.resetCounters();\n    final ServerName SRV = ServerName.valueOf(\"svr,1,1\");\n    RegionServerServices mockedRS = getRegionServer(SRV);\n    slw = new SplitLogWorker(ds, TEST_UTIL.getConfiguration(), mockedRS, neverEndingTask);\n    slw.start();\n    Thread.yield(); // let the worker start\n    Thread.sleep(100);\n\n    String task = ZKSplitLog.getEncodedNodeName(zkw, \"task\");\n    SplitLogTask slt = new SplitLogTask.Unassigned(MANAGER);\n    zkw.getRecoverableZooKeeper().create(task,slt.toByteArray(), Ids.OPEN_ACL_UNSAFE,\n      CreateMode.PERSISTENT);\n\n    waitForCounter(SplitLogCounters.tot_wkr_task_acquired, 0, 1, WAIT_TIME);\n    // now the worker is busy doing the above task\n\n    // preempt the task, have it owned by another worker\n    ZKUtil.setData(zkw, task, slt.toByteArray());\n    waitForCounter(SplitLogCounters.tot_wkr_preempt_task, 0, 1, WAIT_TIME);\n\n    // create a RESCAN node\n    String rescan = ZKSplitLog.getEncodedNodeName(zkw, \"RESCAN\");\n    rescan = zkw.getRecoverableZooKeeper().create(rescan, slt.toByteArray(), Ids.OPEN_ACL_UNSAFE,\n      CreateMode.PERSISTENT_SEQUENTIAL);\n\n    waitForCounter(SplitLogCounters.tot_wkr_task_acquired, 1, 2, WAIT_TIME);\n    // RESCAN node might not have been processed if the worker became busy\n    // with the above task. preempt the task again so that now the RESCAN\n    // node is processed\n    ZKUtil.setData(zkw, task, slt.toByteArray());\n    waitForCounter(SplitLogCounters.tot_wkr_preempt_task, 1, 2, WAIT_TIME);\n    waitForCounter(SplitLogCounters.tot_wkr_task_acquired_rescan, 0, 1, WAIT_TIME);\n\n    List<String> nodes = ZKUtil.listChildrenNoWatch(zkw, zkw.getZNodePaths().splitLogZNode);\n    LOG.debug(Objects.toString(nodes));\n    int num = 0;\n    for (String node : nodes) {\n      num++;\n      if (node.startsWith(\"RESCAN\")) {\n        String name = ZKSplitLog.getEncodedNodeName(zkw, node);\n        String fn = ZKSplitLog.getFileName(name);\n        byte [] data = ZKUtil.getData(zkw,\n                ZNodePaths.joinZNode(zkw.getZNodePaths().splitLogZNode, fn));\n        slt = SplitLogTask.parseFrom(data);\n        assertTrue(slt.toString(), slt.isDone(SRV));\n      }\n    }\n    assertEquals(2, num);\n  }"
        ],
        [
            "ZKUtil::createACL(ZKWatcher,String,boolean)",
            " 892  \n 893  \n 894 -\n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926 -\n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  ",
            "  public static ArrayList<ACL> createACL(ZKWatcher zkw, String node,\n                                         boolean isSecureZooKeeper) {\n    if (!node.startsWith(zkw.znodePaths.baseZNode)) {\n      return Ids.OPEN_ACL_UNSAFE;\n    }\n    if (isSecureZooKeeper) {\n      ArrayList<ACL> acls = new ArrayList<>();\n      // add permission to hbase supper user\n      String[] superUsers = zkw.getConfiguration().getStrings(Superusers.SUPERUSER_CONF_KEY);\n      String hbaseUser = null;\n      try {\n        hbaseUser = UserGroupInformation.getCurrentUser().getShortUserName();\n      } catch (IOException e) {\n        LOG.debug(\"Could not acquire current User.\", e);\n      }\n      if (superUsers != null) {\n        List<String> groups = new ArrayList<>();\n        for (String user : superUsers) {\n          if (AuthUtil.isGroupPrincipal(user)) {\n            // TODO: Set node ACL for groups when ZK supports this feature\n            groups.add(user);\n          } else {\n            if(!user.equals(hbaseUser)) {\n              acls.add(new ACL(Perms.ALL, new Id(\"sasl\", user)));\n            }\n          }\n        }\n        if (!groups.isEmpty()) {\n          LOG.warn(\"Znode ACL setting for group \" + groups\n              + \" is skipped, ZooKeeper doesn't support this feature presently.\");\n        }\n      }\n      // Certain znodes are accessed directly by the client,\n      // so they must be readable by non-authenticated clients\n      if (zkw.znodePaths.isClientReadable(node)) {\n        acls.addAll(Ids.CREATOR_ALL_ACL);\n        acls.addAll(Ids.READ_ACL_UNSAFE);\n      } else {\n        acls.addAll(Ids.CREATOR_ALL_ACL);\n      }\n      return acls;\n    } else {\n      return Ids.OPEN_ACL_UNSAFE;\n    }\n  }",
            " 892  \n 893  \n 894 +\n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926 +\n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  ",
            "  public static ArrayList<ACL> createACL(ZKWatcher zkw, String node,\n                                         boolean isSecureZooKeeper) {\n    if (!node.startsWith(zkw.getZNodePaths().baseZNode)) {\n      return Ids.OPEN_ACL_UNSAFE;\n    }\n    if (isSecureZooKeeper) {\n      ArrayList<ACL> acls = new ArrayList<>();\n      // add permission to hbase supper user\n      String[] superUsers = zkw.getConfiguration().getStrings(Superusers.SUPERUSER_CONF_KEY);\n      String hbaseUser = null;\n      try {\n        hbaseUser = UserGroupInformation.getCurrentUser().getShortUserName();\n      } catch (IOException e) {\n        LOG.debug(\"Could not acquire current User.\", e);\n      }\n      if (superUsers != null) {\n        List<String> groups = new ArrayList<>();\n        for (String user : superUsers) {\n          if (AuthUtil.isGroupPrincipal(user)) {\n            // TODO: Set node ACL for groups when ZK supports this feature\n            groups.add(user);\n          } else {\n            if(!user.equals(hbaseUser)) {\n              acls.add(new ACL(Perms.ALL, new Id(\"sasl\", user)));\n            }\n          }\n        }\n        if (!groups.isEmpty()) {\n          LOG.warn(\"Znode ACL setting for group \" + groups\n              + \" is skipped, ZooKeeper doesn't support this feature presently.\");\n        }\n      }\n      // Certain znodes are accessed directly by the client,\n      // so they must be readable by non-authenticated clients\n      if (zkw.getZNodePaths().isClientReadable(node)) {\n        acls.addAll(Ids.CREATOR_ALL_ACL);\n        acls.addAll(Ids.READ_ACL_UNSAFE);\n      } else {\n        acls.addAll(Ids.CREATOR_ALL_ACL);\n      }\n      return acls;\n    } else {\n      return Ids.OPEN_ACL_UNSAFE;\n    }\n  }"
        ],
        [
            "ZkSplitLogWorkerCoordination::taskLoop()",
            " 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415 -\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442 -\n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  ",
            "  /**\n   * Wait for tasks to become available at /hbase/splitlog zknode. Grab a task one at a time. This\n   * policy puts an upper-limit on the number of simultaneous log splitting that could be happening\n   * in a cluster.\n   * <p>\n   * Synchronization using <code>taskReadySeq</code> ensures that it will try to grab every task\n   * that has been put up\n   * @throws InterruptedException\n   */\n  @Override\n  public void taskLoop() throws InterruptedException {\n    while (!shouldStop) {\n      int seq_start = taskReadySeq.get();\n      List<String> paths;\n      paths = getTaskList();\n      if (paths == null) {\n        LOG.warn(\"Could not get tasks, did someone remove \" + watcher.znodePaths.splitLogZNode\n            + \" ... worker thread exiting.\");\n        return;\n      }\n      // shuffle the paths to prevent different split log worker start from the same log file after\n      // meta log (if any)\n      Collections.shuffle(paths);\n      // pick meta wal firstly\n      int offset = 0;\n      for (int i = 0; i < paths.size(); i++) {\n        if (AbstractFSWALProvider.isMetaFile(paths.get(i))) {\n          offset = i;\n          break;\n        }\n      }\n      int numTasks = paths.size();\n      int expectedTasksPerRS = getNumExpectedTasksPerRS(numTasks);\n      boolean taskGrabbed = false;\n      for (int i = 0; i < numTasks; i++) {\n        while (!shouldStop) {\n          if (this.areSplittersAvailable(expectedTasksPerRS)) {\n            LOG.debug(\"Current region server \" + server.getServerName()\n                + \" is ready to take more tasks, will get task list and try grab tasks again.\");\n            int idx = (i + offset) % paths.size();\n            // don't call ZKSplitLog.getNodeName() because that will lead to\n            // double encoding of the path name\n            taskGrabbed |= grabTask(ZNodePaths.joinZNode(\n                watcher.znodePaths.splitLogZNode, paths.get(idx)));\n            break;\n          } else {\n            LOG.debug(\"Current region server \" + server.getServerName() + \" has \"\n                + this.tasksInProgress.get() + \" tasks in progress and can't take more.\");\n            Thread.sleep(100);\n          }\n        }\n        if (shouldStop) {\n          return;\n        }\n      }\n      if (!taskGrabbed && !shouldStop) {\n        // do not grab any tasks, sleep a little bit to reduce zk request.\n        Thread.sleep(1000);\n      }\n      SplitLogCounters.tot_wkr_task_grabing.increment();\n      synchronized (taskReadySeq) {\n        while (seq_start == taskReadySeq.get()) {\n          taskReadySeq.wait(checkInterval);\n        }\n      }\n    }\n  }",
            " 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415 +\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442 +\n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  ",
            "  /**\n   * Wait for tasks to become available at /hbase/splitlog zknode. Grab a task one at a time. This\n   * policy puts an upper-limit on the number of simultaneous log splitting that could be happening\n   * in a cluster.\n   * <p>\n   * Synchronization using <code>taskReadySeq</code> ensures that it will try to grab every task\n   * that has been put up\n   * @throws InterruptedException\n   */\n  @Override\n  public void taskLoop() throws InterruptedException {\n    while (!shouldStop) {\n      int seq_start = taskReadySeq.get();\n      List<String> paths;\n      paths = getTaskList();\n      if (paths == null) {\n        LOG.warn(\"Could not get tasks, did someone remove \" + watcher.getZNodePaths().splitLogZNode\n            + \" ... worker thread exiting.\");\n        return;\n      }\n      // shuffle the paths to prevent different split log worker start from the same log file after\n      // meta log (if any)\n      Collections.shuffle(paths);\n      // pick meta wal firstly\n      int offset = 0;\n      for (int i = 0; i < paths.size(); i++) {\n        if (AbstractFSWALProvider.isMetaFile(paths.get(i))) {\n          offset = i;\n          break;\n        }\n      }\n      int numTasks = paths.size();\n      int expectedTasksPerRS = getNumExpectedTasksPerRS(numTasks);\n      boolean taskGrabbed = false;\n      for (int i = 0; i < numTasks; i++) {\n        while (!shouldStop) {\n          if (this.areSplittersAvailable(expectedTasksPerRS)) {\n            LOG.debug(\"Current region server \" + server.getServerName()\n                + \" is ready to take more tasks, will get task list and try grab tasks again.\");\n            int idx = (i + offset) % paths.size();\n            // don't call ZKSplitLog.getNodeName() because that will lead to\n            // double encoding of the path name\n            taskGrabbed |= grabTask(ZNodePaths.joinZNode(\n                watcher.getZNodePaths().splitLogZNode, paths.get(idx)));\n            break;\n          } else {\n            LOG.debug(\"Current region server \" + server.getServerName() + \" has \"\n                + this.tasksInProgress.get() + \" tasks in progress and can't take more.\");\n            Thread.sleep(100);\n          }\n        }\n        if (shouldStop) {\n          return;\n        }\n      }\n      if (!taskGrabbed && !shouldStop) {\n        // do not grab any tasks, sleep a little bit to reduce zk request.\n        Thread.sleep(1000);\n      }\n      SplitLogCounters.tot_wkr_task_grabing.increment();\n      synchronized (taskReadySeq) {\n        while (seq_start == taskReadySeq.get()) {\n          taskReadySeq.wait(checkInterval);\n        }\n      }\n    }\n  }"
        ],
        [
            "TestSplitLogWorker::setup()",
            " 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208 -\n 209 -\n 210 -\n 211 -\n 212 -\n 213  \n 214 -\n 215 -\n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  ",
            "  @Before\n  public void setup() throws Exception {\n    TEST_UTIL.startMiniZKCluster();\n    Configuration conf = TEST_UTIL.getConfiguration();\n    zkw = new ZKWatcher(TEST_UTIL.getConfiguration(),\n        \"split-log-worker-tests\", null);\n    ds = new DummyServer(zkw, conf);\n    ZKUtil.deleteChildrenRecursively(zkw, zkw.znodePaths.baseZNode);\n    ZKUtil.createAndFailSilent(zkw, zkw.znodePaths.baseZNode);\n    assertThat(ZKUtil.checkExists(zkw, zkw.znodePaths.baseZNode), not (is(-1)));\n    LOG.debug(zkw.znodePaths.baseZNode + \" created\");\n    ZKUtil.createAndFailSilent(zkw, zkw.znodePaths.splitLogZNode);\n    assertThat(ZKUtil.checkExists(zkw, zkw.znodePaths.splitLogZNode), not (is(-1)));\n\n    LOG.debug(zkw.znodePaths.splitLogZNode + \" created\");\n    ZKUtil.createAndFailSilent(zkw, zkw.znodePaths.rsZNode);\n    assertThat(ZKUtil.checkExists(zkw, zkw.znodePaths.rsZNode), not (is(-1)));\n\n    SplitLogCounters.resetCounters();\n    executorService = new ExecutorService(\"TestSplitLogWorker\");\n    executorService.startExecutorService(ExecutorType.RS_LOG_REPLAY_OPS, 10);\n  }",
            " 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213  \n 214 +\n 215 +\n 216 +\n 217  \n 218  \n 219  \n 220  \n 221  ",
            "  @Before\n  public void setup() throws Exception {\n    TEST_UTIL.startMiniZKCluster();\n    Configuration conf = TEST_UTIL.getConfiguration();\n    zkw = new ZKWatcher(TEST_UTIL.getConfiguration(),\n        \"split-log-worker-tests\", null);\n    ds = new DummyServer(zkw, conf);\n    ZKUtil.deleteChildrenRecursively(zkw, zkw.getZNodePaths().baseZNode);\n    ZKUtil.createAndFailSilent(zkw, zkw.getZNodePaths().baseZNode);\n    assertThat(ZKUtil.checkExists(zkw, zkw.getZNodePaths().baseZNode), not(is(-1)));\n    LOG.debug(zkw.getZNodePaths().baseZNode + \" created\");\n    ZKUtil.createAndFailSilent(zkw, zkw.getZNodePaths().splitLogZNode);\n    assertThat(ZKUtil.checkExists(zkw, zkw.getZNodePaths().splitLogZNode), not(is(-1)));\n\n    LOG.debug(zkw.getZNodePaths().splitLogZNode + \" created\");\n    ZKUtil.createAndFailSilent(zkw, zkw.getZNodePaths().rsZNode);\n    assertThat(ZKUtil.checkExists(zkw, zkw.getZNodePaths().rsZNode), not(is(-1)));\n\n    SplitLogCounters.resetCounters();\n    executorService = new ExecutorService(\"TestSplitLogWorker\");\n    executorService.startExecutorService(ExecutorType.RS_LOG_REPLAY_OPS, 10);\n  }"
        ],
        [
            "ActiveMasterManager::handleMasterNodeChange()",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "  /**\n   * Handle a change in the master node.  Doesn't matter whether this was called\n   * from a nodeCreated or nodeDeleted event because there are no guarantees\n   * that the current state of the master node matches the event at the time of\n   * our next ZK request.\n   *\n   * <p>Uses the watchAndCheckExists method which watches the master address node\n   * regardless of whether it exists or not.  If it does exist (there is an\n   * active master), it returns true.  Otherwise it returns false.\n   *\n   * <p>A watcher is set which guarantees that this method will get called again if\n   * there is another change in the master node.\n   */\n  private void handleMasterNodeChange() {\n    // Watch the node and check if it exists.\n    try {\n      synchronized(clusterHasActiveMaster) {\n        if (ZKUtil.watchAndCheckExists(watcher, watcher.znodePaths.masterAddressZNode)) {\n          // A master node exists, there is an active master\n          LOG.trace(\"A master is now available\");\n          clusterHasActiveMaster.set(true);\n        } else {\n          // Node is no longer there, cluster does not have an active master\n          LOG.debug(\"No master available. Notifying waiting threads\");\n          clusterHasActiveMaster.set(false);\n          // Notify any thread waiting to become the active master\n          clusterHasActiveMaster.notifyAll();\n        }\n      }\n    } catch (KeeperException ke) {\n      master.abort(\"Received an unexpected KeeperException, aborting\", ke);\n    }\n  }",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 +\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  ",
            "  /**\n   * Handle a change in the master node.  Doesn't matter whether this was called\n   * from a nodeCreated or nodeDeleted event because there are no guarantees\n   * that the current state of the master node matches the event at the time of\n   * our next ZK request.\n   *\n   * <p>Uses the watchAndCheckExists method which watches the master address node\n   * regardless of whether it exists or not.  If it does exist (there is an\n   * active master), it returns true.  Otherwise it returns false.\n   *\n   * <p>A watcher is set which guarantees that this method will get called again if\n   * there is another change in the master node.\n   */\n  private void handleMasterNodeChange() {\n    // Watch the node and check if it exists.\n    try {\n      synchronized(clusterHasActiveMaster) {\n        if (ZKUtil.watchAndCheckExists(watcher, watcher.getZNodePaths().masterAddressZNode)) {\n          // A master node exists, there is an active master\n          LOG.trace(\"A master is now available\");\n          clusterHasActiveMaster.set(true);\n        } else {\n          // Node is no longer there, cluster does not have an active master\n          LOG.debug(\"No master available. Notifying waiting threads\");\n          clusterHasActiveMaster.set(false);\n          // Notify any thread waiting to become the active master\n          clusterHasActiveMaster.notifyAll();\n        }\n      }\n    } catch (KeeperException ke) {\n      master.abort(\"Received an unexpected KeeperException, aborting\", ke);\n    }\n  }"
        ],
        [
            "HBaseReplicationEndpoint::PeerRegionServerListener::PeerRegionServerListener(HBaseReplicationEndpoint)",
            " 216  \n 217  \n 218  \n 219 -\n 220  ",
            "    public PeerRegionServerListener(HBaseReplicationEndpoint replicationPeer) {\n      super(replicationPeer.getZkw());\n      this.replicationEndpoint = replicationPeer;\n      this.regionServerListNode = replicationEndpoint.getZkw().znodePaths.rsZNode;\n    }",
            " 217  \n 218  \n 219  \n 220 +\n 221  ",
            "    public PeerRegionServerListener(HBaseReplicationEndpoint replicationPeer) {\n      super(replicationPeer.getZkw());\n      this.replicationEndpoint = replicationPeer;\n      this.regionServerListNode = replicationEndpoint.getZkw().getZNodePaths().rsZNode;\n    }"
        ],
        [
            "IntegrationTestMetaReplicas::waitUntilZnodeAvailable(int)",
            "  83  \n  84 -\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "  private static void waitUntilZnodeAvailable(int replicaId) throws Exception {\n    String znode = util.getZooKeeperWatcher().znodePaths.getZNodeForReplica(replicaId);\n    int i = 0;\n    while (i < 1000) {\n      if (ZKUtil.checkExists(util.getZooKeeperWatcher(), znode) == -1) {\n        Thread.sleep(100);\n        i++;\n      } else break;\n    }\n    if (i == 1000) throw new IOException(\"znode for meta replica \" + replicaId + \" not available\");\n  }",
            "  83  \n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  ",
            "  private static void waitUntilZnodeAvailable(int replicaId) throws Exception {\n    String znode = util.getZooKeeperWatcher().getZNodePaths().getZNodeForReplica(replicaId);\n    int i = 0;\n    while (i < 1000) {\n      if (ZKUtil.checkExists(util.getZooKeeperWatcher(), znode) == -1) {\n        Thread.sleep(100);\n        i++;\n      } else break;\n    }\n    if (i == 1000) throw new IOException(\"znode for meta replica \" + replicaId + \" not available\");\n  }"
        ],
        [
            "ZKUtil::setData(ZKWatcher,String,byte,int)",
            " 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804 -\n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  ",
            "  /**\n   * Sets the data of the existing znode to be the specified data.  Ensures that\n   * the current data has the specified expected version.\n   *\n   * <p>If the node does not exist, a {@link NoNodeException} will be thrown.\n   *\n   * <p>If their is a version mismatch, method returns null.\n   *\n   * <p>No watches are set but setting data will trigger other watchers of this\n   * node.\n   *\n   * <p>If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data to set for node\n   * @param expectedVersion version expected when setting data\n   * @return true if data set, false if version mismatch\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean setData(ZKWatcher zkw, String znode,\n                                byte [] data, int expectedVersion)\n  throws KeeperException, KeeperException.NoNodeException {\n    try {\n      return zkw.getRecoverableZooKeeper().setData(znode, data, expectedVersion) != null;\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return false;\n    }\n  }",
            " 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804 +\n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  ",
            "  /**\n   * Sets the data of the existing znode to be the specified data.  Ensures that\n   * the current data has the specified expected version.\n   *\n   * <p>If the node does not exist, a {@link NoNodeException} will be thrown.\n   *\n   * <p>If their is a version mismatch, method returns null.\n   *\n   * <p>No watches are set but setting data will trigger other watchers of this\n   * node.\n   *\n   * <p>If there is another problem, a KeeperException will be thrown.\n   *\n   * @param zkw zk reference\n   * @param znode path of node\n   * @param data data to set for node\n   * @param expectedVersion version expected when setting data\n   * @return true if data set, false if version mismatch\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static boolean setData(ZKWatcher zkw, String znode,\n                                byte [] data, int expectedVersion)\n    throws KeeperException, KeeperException.NoNodeException {\n    try {\n      return zkw.getRecoverableZooKeeper().setData(znode, data, expectedVersion) != null;\n    } catch (InterruptedException e) {\n      zkw.interruptedException(e);\n      return false;\n    }\n  }"
        ],
        [
            "RecoverableZooKeeper::getAcl(String,Stat)",
            " 443  \n 444  \n 445  \n 446  \n 447  \n 448 -\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "  /**\n   * getAcl is an idempotent operation. Retry before throwing exception\n   * @return list of ACLs\n   */\n  public List<ACL> getAcl(String path, Stat stat)\n  throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getAcl\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<ACL> nodeACL = checkZk().getACL(path, stat);\n          return nodeACL;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"getAcl\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"getAcl\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }",
            " 443  \n 444  \n 445  \n 446  \n 447  \n 448 +\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  ",
            "  /**\n   * getAcl is an idempotent operation. Retry before throwing exception\n   * @return list of ACLs\n   */\n  public List<ACL> getAcl(String path, Stat stat)\n    throws KeeperException, InterruptedException {\n    try (TraceScope scope = TraceUtil.createTrace(\"RecoverableZookeeper.getAcl\")) {\n      RetryCounter retryCounter = retryCounterFactory.create();\n      while (true) {\n        try {\n          long startTime = EnvironmentEdgeManager.currentTime();\n          List<ACL> nodeACL = checkZk().getACL(path, stat);\n          return nodeACL;\n        } catch (KeeperException e) {\n          switch (e.code()) {\n            case CONNECTIONLOSS:\n              retryOrThrow(retryCounter, e, \"getAcl\");\n              break;\n            case OPERATIONTIMEOUT:\n              retryOrThrow(retryCounter, e, \"getAcl\");\n              break;\n\n            default:\n              throw e;\n          }\n        }\n        retryCounter.sleepUntilNextRetry();\n      }\n    }\n  }"
        ],
        [
            "ZKUtil::checkExists(ZKWatcher,String)",
            " 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404 -\n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "  /**\n   * Check if the specified node exists.  Sets no watches.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to watch\n   * @return version of the node if it exists, -1 if does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static int checkExists(ZKWatcher zkw, String znode)\n  throws KeeperException {\n    try {\n      Stat s = zkw.getRecoverableZooKeeper().exists(znode, null);\n      return s != null ? s.getVersion() : -1;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to set watcher on znode (\" + znode + \")\"), e);\n      zkw.keeperException(e);\n      return -1;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to set watcher on znode (\" + znode + \")\"), e);\n      zkw.interruptedException(e);\n      return -1;\n    }\n  }",
            " 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404 +\n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "  /**\n   * Check if the specified node exists.  Sets no watches.\n   *\n   * @param zkw zk reference\n   * @param znode path of node to watch\n   * @return version of the node if it exists, -1 if does not exist\n   * @throws KeeperException if unexpected zookeeper exception\n   */\n  public static int checkExists(ZKWatcher zkw, String znode)\n    throws KeeperException {\n    try {\n      Stat s = zkw.getRecoverableZooKeeper().exists(znode, null);\n      return s != null ? s.getVersion() : -1;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to set watcher on znode (\" + znode + \")\"), e);\n      zkw.keeperException(e);\n      return -1;\n    } catch (InterruptedException e) {\n      LOG.warn(zkw.prefix(\"Unable to set watcher on znode (\" + znode + \")\"), e);\n      zkw.interruptedException(e);\n      return -1;\n    }\n  }"
        ],
        [
            "TestZKMulti::testSingleFailureInMulti()",
            " 193  \n 194  \n 195  \n 196 -\n 197 -\n 198 -\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  ",
            "  @Test\n  public void testSingleFailureInMulti() throws Exception {\n    // try a multi where all but one operation succeeds\n    String pathA = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testSingleFailureInMultiA\");\n    String pathB = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testSingleFailureInMultiB\");\n    String pathC = ZNodePaths.joinZNode(zkw.znodePaths.baseZNode, \"testSingleFailureInMultiC\");\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(pathA, Bytes.toBytes(pathA)));\n    ops.add(ZKUtilOp.createAndFailSilent(pathB, Bytes.toBytes(pathB)));\n    ops.add(ZKUtilOp.deleteNodeFailSilent(pathC));\n    boolean caughtNoNode = false;\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NoNodeException nne) {\n      caughtNoNode = true;\n    }\n    assertTrue(caughtNoNode);\n    // assert that none of the operations succeeded\n    assertTrue(ZKUtil.checkExists(zkw, pathA) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathB) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathC) == -1);\n  }",
            " 193  \n 194  \n 195  \n 196 +\n 197 +\n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  ",
            "  @Test\n  public void testSingleFailureInMulti() throws Exception {\n    // try a multi where all but one operation succeeds\n    String pathA = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testSingleFailureInMultiA\");\n    String pathB = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testSingleFailureInMultiB\");\n    String pathC = ZNodePaths.joinZNode(zkw.getZNodePaths().baseZNode, \"testSingleFailureInMultiC\");\n    LinkedList<ZKUtilOp> ops = new LinkedList<>();\n    ops.add(ZKUtilOp.createAndFailSilent(pathA, Bytes.toBytes(pathA)));\n    ops.add(ZKUtilOp.createAndFailSilent(pathB, Bytes.toBytes(pathB)));\n    ops.add(ZKUtilOp.deleteNodeFailSilent(pathC));\n    boolean caughtNoNode = false;\n    try {\n      ZKUtil.multiOrSequential(zkw, ops, false);\n    } catch (KeeperException.NoNodeException nne) {\n      caughtNoNode = true;\n    }\n    assertTrue(caughtNoNode);\n    // assert that none of the operations succeeded\n    assertTrue(ZKUtil.checkExists(zkw, pathA) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathB) == -1);\n    assertTrue(ZKUtil.checkExists(zkw, pathC) == -1);\n  }"
        ],
        [
            "TestHMasterRPCException::testRPCException()",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  ",
            "  @Test\n  public void testRPCException() throws IOException, InterruptedException, KeeperException {\n    ServerName sm = master.getServerName();\n    boolean fakeZNodeDelete = false;\n    for (int i = 0; i < 20; i++) {\n      try {\n        BlockingRpcChannel channel = rpcClient.createBlockingRpcChannel(sm, User.getCurrent(), 0);\n        MasterProtos.MasterService.BlockingInterface stub =\n            MasterProtos.MasterService.newBlockingStub(channel);\n        assertTrue(stub.isMasterRunning(null, IsMasterRunningRequest.getDefaultInstance())\n            .getIsMasterRunning());\n        return;\n      } catch (ServiceException ex) {\n        IOException ie = ProtobufUtil.handleRemoteException(ex);\n        // No SocketTimeoutException here. RpcServer is already started after the construction of\n        // HMaster.\n        assertTrue(ie.getMessage().startsWith(\n          \"org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet\"));\n        LOG.info(\"Expected exception: \", ie);\n        if (!fakeZNodeDelete) {\n          testUtil.getZooKeeperWatcher().getRecoverableZooKeeper()\n              .delete(testUtil.getZooKeeperWatcher().znodePaths.masterAddressZNode, -1);\n          fakeZNodeDelete = true;\n        }\n      }\n      Thread.sleep(1000);\n    }\n  }",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  ",
            "  @Test\n  public void testRPCException() throws IOException, InterruptedException, KeeperException {\n    ServerName sm = master.getServerName();\n    boolean fakeZNodeDelete = false;\n    for (int i = 0; i < 20; i++) {\n      try {\n        BlockingRpcChannel channel = rpcClient.createBlockingRpcChannel(sm, User.getCurrent(), 0);\n        MasterProtos.MasterService.BlockingInterface stub =\n            MasterProtos.MasterService.newBlockingStub(channel);\n        assertTrue(stub.isMasterRunning(null, IsMasterRunningRequest.getDefaultInstance())\n            .getIsMasterRunning());\n        return;\n      } catch (ServiceException ex) {\n        IOException ie = ProtobufUtil.handleRemoteException(ex);\n        // No SocketTimeoutException here. RpcServer is already started after the construction of\n        // HMaster.\n        assertTrue(ie.getMessage().startsWith(\n          \"org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet\"));\n        LOG.info(\"Expected exception: \", ie);\n        if (!fakeZNodeDelete) {\n          testUtil.getZooKeeperWatcher().getRecoverableZooKeeper()\n              .delete(testUtil.getZooKeeperWatcher().getZNodePaths().masterAddressZNode, -1);\n          fakeZNodeDelete = true;\n        }\n      }\n      Thread.sleep(1000);\n    }\n  }"
        ],
        [
            "ZKWatcher::ZKWatcher(Configuration,String,Abortable,boolean)",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119  \n 120  ",
            "  /**\n   * Instantiate a ZooKeeper connection and watcher.\n   * @param conf the configuration to use\n   * @param identifier string that is passed to RecoverableZookeeper to be used as identifier for\n   *          this instance. Use null for default.\n   * @param abortable Can be null if there is on error there is no host to abort: e.g. client\n   *          context.\n   * @param canCreateBaseZNode true if a base ZNode can be created\n   * @throws IOException if the connection to ZooKeeper fails\n   * @throws ZooKeeperConnectionException\n   */\n  public ZKWatcher(Configuration conf, String identifier,\n                   Abortable abortable, boolean canCreateBaseZNode)\n  throws IOException, ZooKeeperConnectionException {\n    this(conf, identifier, abortable, canCreateBaseZNode, false);\n  }",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 +\n 119  \n 120  ",
            "  /**\n   * Instantiate a ZooKeeper connection and watcher.\n   * @param conf the configuration to use\n   * @param identifier string that is passed to RecoverableZookeeper to be used as identifier for\n   *          this instance. Use null for default.\n   * @param abortable Can be null if there is on error there is no host to abort: e.g. client\n   *          context.\n   * @param canCreateBaseZNode true if a base ZNode can be created\n   * @throws IOException if the connection to ZooKeeper fails\n   * @throws ZooKeeperConnectionException if the client can't connect to ZooKeeper\n   */\n  public ZKWatcher(Configuration conf, String identifier,\n                   Abortable abortable, boolean canCreateBaseZNode)\n    throws IOException, ZooKeeperConnectionException {\n    this(conf, identifier, abortable, canCreateBaseZNode, false);\n  }"
        ]
    ],
    "cc766df28b80827f8951c2c1364443a8c5a39b35": [
        [
            "LruBlockCache::iterator()",
            " 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990 -\n 991  \n 992  \n 993  \n 994  \n 995  \n 996 -\n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  ",
            "  @Override\n  public Iterator<CachedBlock> iterator() {\n    final Iterator<LruCachedBlock> iterator = map.values().iterator();\n\n    return new Iterator<CachedBlock>() {\n      private final long now = System.nanoTime();\n\n      @Override\n      public boolean hasNext() {\n        return iterator.hasNext();\n      }\n\n      @Override\n      public CachedBlock next() {\n        final LruCachedBlock b = iterator.next();\n        return new CachedBlock() {\n          @Override\n          public String toString() {\n            return BlockCacheUtil.toString(this, now);\n          }\n\n          @Override\n          public BlockPriority getBlockPriority() {\n            return b.getPriority();\n          }\n\n          @Override\n          public BlockType getBlockType() {\n            return b.getBuffer().getBlockType();\n          }\n\n          @Override\n          public long getOffset() {\n            return b.getCacheKey().getOffset();\n          }\n\n          @Override\n          public long getSize() {\n            return b.getBuffer().heapSize();\n          }\n\n          @Override\n          public long getCachedTime() {\n            return b.getCachedTime();\n          }\n\n          @Override\n          public String getFilename() {\n            return b.getCacheKey().getHfileName();\n          }\n\n          @Override\n          public int compareTo(CachedBlock other) {\n            int diff = this.getFilename().compareTo(other.getFilename());\n            if (diff != 0) return diff;\n            diff = (int)(this.getOffset() - other.getOffset());\n            if (diff != 0) return diff;\n            if (other.getCachedTime() < 0 || this.getCachedTime() < 0) {\n              throw new IllegalStateException(\"\" + this.getCachedTime() + \", \" +\n                other.getCachedTime());\n            }\n            return (int)(other.getCachedTime() - this.getCachedTime());\n          }\n\n          @Override\n          public int hashCode() {\n            return b.hashCode();\n          }\n\n          @Override\n          public boolean equals(Object obj) {\n            if (obj instanceof CachedBlock) {\n              CachedBlock cb = (CachedBlock)obj;\n              return compareTo(cb) == 0;\n            } else {\n              return false;\n            }\n          }\n        };\n      }\n\n      @Override\n      public void remove() {\n        throw new UnsupportedOperationException();\n      }\n    };\n  }",
            " 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989 +\n 990  \n 991  \n 992  \n 993  \n 994  \n 995 +\n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  ",
            "  @Override\n  public Iterator<CachedBlock> iterator() {\n    final Iterator<LruCachedBlock> iterator = map.values().iterator();\n\n    return new Iterator<CachedBlock>() {\n      private final long now = System.nanoTime();\n\n      @Override\n      public boolean hasNext() {\n        return iterator.hasNext();\n      }\n\n      @Override\n      public CachedBlock next() {\n        final LruCachedBlock b = iterator.next();\n        return new CachedBlock() {\n          @Override\n          public String toString() {\n            return BlockCacheUtil.toString(this, now);\n          }\n\n          @Override\n          public BlockPriority getBlockPriority() {\n            return b.getPriority();\n          }\n\n          @Override\n          public BlockType getBlockType() {\n            return b.getBuffer().getBlockType();\n          }\n\n          @Override\n          public long getOffset() {\n            return b.getCacheKey().getOffset();\n          }\n\n          @Override\n          public long getSize() {\n            return b.getBuffer().heapSize();\n          }\n\n          @Override\n          public long getCachedTime() {\n            return b.getCachedTime();\n          }\n\n          @Override\n          public String getFilename() {\n            return b.getCacheKey().getHfileName();\n          }\n\n          @Override\n          public int compareTo(CachedBlock other) {\n            int diff = this.getFilename().compareTo(other.getFilename());\n            if (diff != 0) return diff;\n            diff = Long.compare(this.getOffset(), other.getOffset());\n            if (diff != 0) return diff;\n            if (other.getCachedTime() < 0 || this.getCachedTime() < 0) {\n              throw new IllegalStateException(\"\" + this.getCachedTime() + \", \" +\n                other.getCachedTime());\n            }\n            return Long.compare(other.getCachedTime(), this.getCachedTime());\n          }\n\n          @Override\n          public int hashCode() {\n            return b.hashCode();\n          }\n\n          @Override\n          public boolean equals(Object obj) {\n            if (obj instanceof CachedBlock) {\n              CachedBlock cb = (CachedBlock)obj;\n              return compareTo(cb) == 0;\n            } else {\n              return false;\n            }\n          }\n        };\n      }\n\n      @Override\n      public void remove() {\n        throw new UnsupportedOperationException();\n      }\n    };\n  }"
        ],
        [
            "BucketCache::BucketEntryGroup::compareTo(BucketEntryGroup)",
            "1272  \n1273  \n1274 -\n1275 -\n1276 -\n1277  ",
            "    @Override\n    public int compareTo(BucketEntryGroup that) {\n      if (this.overflow() == that.overflow())\n        return 0;\n      return this.overflow() > that.overflow() ? 1 : -1;\n    }",
            "1270  \n1271  \n1272 +\n1273  ",
            "    @Override\n    public int compareTo(BucketEntryGroup that) {\n      return Long.compare(this.overflow(), that.overflow());\n    }"
        ],
        [
            "LruBlockCache::BlockBucket::compareTo(BlockBucket)",
            " 751  \n 752 -\n 753 -\n 754  ",
            "    public int compareTo(BlockBucket that) {\n      if(this.overflow() == that.overflow()) return 0;\n      return this.overflow() > that.overflow() ? 1 : -1;\n    }",
            " 751  \n 752 +\n 753  ",
            "    public int compareTo(BlockBucket that) {\n      return Long.compare(this.overflow(), that.overflow());\n    }"
        ],
        [
            "BucketCache::BucketEntry::compare(BucketEntry,BucketEntry)",
            "1142  \n1143  \n1144 -\n1145 -\n1146 -\n1147  ",
            "      @Override\n      public int compare(BucketEntry o1, BucketEntry o2) {\n        long accessCounter1 = o1.accessCounter;\n        long accessCounter2 = o2.accessCounter;\n        return accessCounter1 < accessCounter2 ? 1 : accessCounter1 == accessCounter2 ? 0 : -1;\n      }",
            "1142  \n1143  \n1144 +\n1145  ",
            "      @Override\n      public int compare(BucketEntry o1, BucketEntry o2) {\n        return Long.compare(o2.accessCounter, o1.accessCounter);\n      }"
        ],
        [
            "BucketCache::iterator()",
            "1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423 -\n1424  \n1425  \n1426  \n1427  \n1428  \n1429 -\n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  ",
            "  @Override\n  public Iterator<CachedBlock> iterator() {\n    // Don't bother with ramcache since stuff is in here only a little while.\n    final Iterator<Map.Entry<BlockCacheKey, BucketEntry>> i =\n        this.backingMap.entrySet().iterator();\n    return new Iterator<CachedBlock>() {\n      private final long now = System.nanoTime();\n\n      @Override\n      public boolean hasNext() {\n        return i.hasNext();\n      }\n\n      @Override\n      public CachedBlock next() {\n        final Map.Entry<BlockCacheKey, BucketEntry> e = i.next();\n        return new CachedBlock() {\n          @Override\n          public String toString() {\n            return BlockCacheUtil.toString(this, now);\n          }\n\n          @Override\n          public BlockPriority getBlockPriority() {\n            return e.getValue().getPriority();\n          }\n\n          @Override\n          public BlockType getBlockType() {\n            // Not held by BucketEntry.  Could add it if wanted on BucketEntry creation.\n            return null;\n          }\n\n          @Override\n          public long getOffset() {\n            return e.getKey().getOffset();\n          }\n\n          @Override\n          public long getSize() {\n            return e.getValue().getLength();\n          }\n\n          @Override\n          public long getCachedTime() {\n            return e.getValue().getCachedTime();\n          }\n\n          @Override\n          public String getFilename() {\n            return e.getKey().getHfileName();\n          }\n\n          @Override\n          public int compareTo(CachedBlock other) {\n            int diff = this.getFilename().compareTo(other.getFilename());\n            if (diff != 0) return diff;\n            diff = (int)(this.getOffset() - other.getOffset());\n            if (diff != 0) return diff;\n            if (other.getCachedTime() < 0 || this.getCachedTime() < 0) {\n              throw new IllegalStateException(\"\" + this.getCachedTime() + \", \" +\n                other.getCachedTime());\n            }\n            return (int)(other.getCachedTime() - this.getCachedTime());\n          }\n\n          @Override\n          public int hashCode() {\n            return e.getKey().hashCode();\n          }\n\n          @Override\n          public boolean equals(Object obj) {\n            if (obj instanceof CachedBlock) {\n              CachedBlock cb = (CachedBlock)obj;\n              return compareTo(cb) == 0;\n            } else {\n              return false;\n            }\n          }\n        };\n      }\n\n      @Override\n      public void remove() {\n        throw new UnsupportedOperationException();\n      }\n    };\n  }",
            "1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377  \n1378  \n1379  \n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419 +\n1420 +\n1421  \n1422  \n1423  \n1424  \n1425  \n1426 +\n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  ",
            "  @Override\n  public Iterator<CachedBlock> iterator() {\n    // Don't bother with ramcache since stuff is in here only a little while.\n    final Iterator<Map.Entry<BlockCacheKey, BucketEntry>> i =\n        this.backingMap.entrySet().iterator();\n    return new Iterator<CachedBlock>() {\n      private final long now = System.nanoTime();\n\n      @Override\n      public boolean hasNext() {\n        return i.hasNext();\n      }\n\n      @Override\n      public CachedBlock next() {\n        final Map.Entry<BlockCacheKey, BucketEntry> e = i.next();\n        return new CachedBlock() {\n          @Override\n          public String toString() {\n            return BlockCacheUtil.toString(this, now);\n          }\n\n          @Override\n          public BlockPriority getBlockPriority() {\n            return e.getValue().getPriority();\n          }\n\n          @Override\n          public BlockType getBlockType() {\n            // Not held by BucketEntry.  Could add it if wanted on BucketEntry creation.\n            return null;\n          }\n\n          @Override\n          public long getOffset() {\n            return e.getKey().getOffset();\n          }\n\n          @Override\n          public long getSize() {\n            return e.getValue().getLength();\n          }\n\n          @Override\n          public long getCachedTime() {\n            return e.getValue().getCachedTime();\n          }\n\n          @Override\n          public String getFilename() {\n            return e.getKey().getHfileName();\n          }\n\n          @Override\n          public int compareTo(CachedBlock other) {\n            int diff = this.getFilename().compareTo(other.getFilename());\n            if (diff != 0) return diff;\n\n            diff = Long.compare(this.getOffset(), other.getOffset());\n            if (diff != 0) return diff;\n            if (other.getCachedTime() < 0 || this.getCachedTime() < 0) {\n              throw new IllegalStateException(\"\" + this.getCachedTime() + \", \" +\n                other.getCachedTime());\n            }\n            return Long.compare(other.getCachedTime(), this.getCachedTime());\n          }\n\n          @Override\n          public int hashCode() {\n            return e.getKey().hashCode();\n          }\n\n          @Override\n          public boolean equals(Object obj) {\n            if (obj instanceof CachedBlock) {\n              CachedBlock cb = (CachedBlock)obj;\n              return compareTo(cb) == 0;\n            } else {\n              return false;\n            }\n          }\n        };\n      }\n\n      @Override\n      public void remove() {\n        throw new UnsupportedOperationException();\n      }\n    };\n  }"
        ],
        [
            "SimpleRpcScheduler::CallPriorityComparator::compare(CallRunner,CallRunner)",
            " 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 -\n 147  ",
            "    @Override\n    public int compare(CallRunner a, CallRunner b) {\n      RpcServer.Call callA = a.getCall();\n      RpcServer.Call callB = b.getCall();\n      long deadlineA = priority.getDeadline(callA.getHeader(), callA.param);\n      long deadlineB = priority.getDeadline(callB.getHeader(), callB.param);\n      deadlineA = callA.timestamp + Math.min(deadlineA, maxDelay);\n      deadlineB = callB.timestamp + Math.min(deadlineB, maxDelay);\n      return (int)(deadlineA - deadlineB);\n    }",
            " 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147  ",
            "    @Override\n    public int compare(CallRunner a, CallRunner b) {\n      RpcServer.Call callA = a.getCall();\n      RpcServer.Call callB = b.getCall();\n      long deadlineA = priority.getDeadline(callA.getHeader(), callA.param);\n      long deadlineB = priority.getDeadline(callB.getHeader(), callB.param);\n      deadlineA = callA.timestamp + Math.min(deadlineA, maxDelay);\n      deadlineB = callB.timestamp + Math.min(deadlineB, maxDelay);\n      return Long.compare(deadlineA, deadlineB);\n    }"
        ],
        [
            "HBaseFsck::loadMetaEntries()",
            "3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378  \n3379  \n3380  \n3381  \n3382  \n3383 -\n3384  \n3385  \n3386  \n3387  \n3388  \n3389  \n3390  \n3391  \n3392  \n3393  \n3394  \n3395  \n3396  \n3397  \n3398  \n3399  \n3400  \n3401  \n3402  \n3403  \n3404  \n3405  \n3406  \n3407  \n3408  \n3409  \n3410  \n3411  \n3412  \n3413  \n3414  \n3415  \n3416  \n3417  \n3418  \n3419  \n3420  \n3421  \n3422  \n3423  \n3424  \n3425  \n3426  \n3427  \n3428  \n3429  \n3430  \n3431  \n3432  \n3433  \n3434  \n3435  \n3436  \n3437  \n3438  \n3439  \n3440  \n3441  \n3442  \n3443  \n3444  \n3445  \n3446  \n3447  \n3448  \n3449  \n3450  \n3451  \n3452  \n3453  \n3454  \n3455  \n3456  \n3457  \n3458  \n3459  \n3460  \n3461  \n3462  \n3463  \n3464  \n3465  ",
            "  /**\n   * Scan hbase:meta, adding all regions found to the regionInfo map.\n   * @throws IOException if an error is encountered\n   */\n  boolean loadMetaEntries() throws IOException {\n    MetaTableAccessor.Visitor visitor = new MetaTableAccessor.Visitor() {\n      int countRecord = 1;\n\n      // comparator to sort KeyValues with latest modtime\n      final Comparator<Cell> comp = new Comparator<Cell>() {\n        @Override\n        public int compare(Cell k1, Cell k2) {\n          return (int)(k1.getTimestamp() - k2.getTimestamp());\n        }\n      };\n\n      @Override\n      public boolean visit(Result result) throws IOException {\n        try {\n\n          // record the latest modification of this META record\n          long ts =  Collections.max(result.listCells(), comp).getTimestamp();\n          RegionLocations rl = MetaTableAccessor.getRegionLocations(result);\n          if (rl == null) {\n            emptyRegionInfoQualifiers.add(result);\n            errors.reportError(ERROR_CODE.EMPTY_META_CELL,\n              \"Empty REGIONINFO_QUALIFIER found in hbase:meta\");\n            return true;\n          }\n          ServerName sn = null;\n          if (rl.getRegionLocation(HRegionInfo.DEFAULT_REPLICA_ID) == null ||\n              rl.getRegionLocation(HRegionInfo.DEFAULT_REPLICA_ID).getRegionInfo() == null) {\n            emptyRegionInfoQualifiers.add(result);\n            errors.reportError(ERROR_CODE.EMPTY_META_CELL,\n              \"Empty REGIONINFO_QUALIFIER found in hbase:meta\");\n            return true;\n          }\n          HRegionInfo hri = rl.getRegionLocation(HRegionInfo.DEFAULT_REPLICA_ID).getRegionInfo();\n          if (!(isTableIncluded(hri.getTable())\n              || hri.isMetaRegion())) {\n            return true;\n          }\n          PairOfSameType<HRegionInfo> daughters = MetaTableAccessor.getDaughterRegions(result);\n          for (HRegionLocation h : rl.getRegionLocations()) {\n            if (h == null || h.getRegionInfo() == null) {\n              continue;\n            }\n            sn = h.getServerName();\n            hri = h.getRegionInfo();\n\n            MetaEntry m = null;\n            if (hri.getReplicaId() == HRegionInfo.DEFAULT_REPLICA_ID) {\n              m = new MetaEntry(hri, sn, ts, daughters.getFirst(), daughters.getSecond());\n            } else {\n              m = new MetaEntry(hri, sn, ts, null, null);\n            }\n            HbckInfo previous = regionInfoMap.get(hri.getEncodedName());\n            if (previous == null) {\n              regionInfoMap.put(hri.getEncodedName(), new HbckInfo(m));\n            } else if (previous.metaEntry == null) {\n              previous.metaEntry = m;\n            } else {\n              throw new IOException(\"Two entries in hbase:meta are same \" + previous);\n            }\n          }\n          PairOfSameType<HRegionInfo> mergeRegions = MetaTableAccessor.getMergeRegions(result);\n          for (HRegionInfo mergeRegion : new HRegionInfo[] {\n              mergeRegions.getFirst(), mergeRegions.getSecond() }) {\n            if (mergeRegion != null) {\n              // This region is already been merged\n              HbckInfo hbInfo = getOrCreateInfo(mergeRegion.getEncodedName());\n              hbInfo.setMerged(true);\n            }\n          }\n\n          // show proof of progress to the user, once for every 100 records.\n          if (countRecord % 100 == 0) {\n            errors.progress();\n          }\n          countRecord++;\n          return true;\n        } catch (RuntimeException e) {\n          LOG.error(\"Result=\" + result);\n          throw e;\n        }\n      }\n    };\n    if (!checkMetaOnly) {\n      // Scan hbase:meta to pick up user regions\n      MetaTableAccessor.fullScanRegions(connection, visitor);\n    }\n\n    errors.print(\"\");\n    return true;\n  }",
            "3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378  \n3379  \n3380  \n3381  \n3382  \n3383 +\n3384  \n3385  \n3386  \n3387  \n3388  \n3389  \n3390  \n3391  \n3392  \n3393  \n3394  \n3395  \n3396  \n3397  \n3398  \n3399  \n3400  \n3401  \n3402  \n3403  \n3404  \n3405  \n3406  \n3407  \n3408  \n3409  \n3410  \n3411  \n3412  \n3413  \n3414  \n3415  \n3416  \n3417  \n3418  \n3419  \n3420  \n3421  \n3422  \n3423  \n3424  \n3425  \n3426  \n3427  \n3428  \n3429  \n3430  \n3431  \n3432  \n3433  \n3434  \n3435  \n3436  \n3437  \n3438  \n3439  \n3440  \n3441  \n3442  \n3443  \n3444  \n3445  \n3446  \n3447  \n3448  \n3449  \n3450  \n3451  \n3452  \n3453  \n3454  \n3455  \n3456  \n3457  \n3458  \n3459  \n3460  \n3461  \n3462  \n3463  \n3464  \n3465  ",
            "  /**\n   * Scan hbase:meta, adding all regions found to the regionInfo map.\n   * @throws IOException if an error is encountered\n   */\n  boolean loadMetaEntries() throws IOException {\n    MetaTableAccessor.Visitor visitor = new MetaTableAccessor.Visitor() {\n      int countRecord = 1;\n\n      // comparator to sort KeyValues with latest modtime\n      final Comparator<Cell> comp = new Comparator<Cell>() {\n        @Override\n        public int compare(Cell k1, Cell k2) {\n          return Long.compare(k1.getTimestamp(), k2.getTimestamp());\n        }\n      };\n\n      @Override\n      public boolean visit(Result result) throws IOException {\n        try {\n\n          // record the latest modification of this META record\n          long ts =  Collections.max(result.listCells(), comp).getTimestamp();\n          RegionLocations rl = MetaTableAccessor.getRegionLocations(result);\n          if (rl == null) {\n            emptyRegionInfoQualifiers.add(result);\n            errors.reportError(ERROR_CODE.EMPTY_META_CELL,\n              \"Empty REGIONINFO_QUALIFIER found in hbase:meta\");\n            return true;\n          }\n          ServerName sn = null;\n          if (rl.getRegionLocation(HRegionInfo.DEFAULT_REPLICA_ID) == null ||\n              rl.getRegionLocation(HRegionInfo.DEFAULT_REPLICA_ID).getRegionInfo() == null) {\n            emptyRegionInfoQualifiers.add(result);\n            errors.reportError(ERROR_CODE.EMPTY_META_CELL,\n              \"Empty REGIONINFO_QUALIFIER found in hbase:meta\");\n            return true;\n          }\n          HRegionInfo hri = rl.getRegionLocation(HRegionInfo.DEFAULT_REPLICA_ID).getRegionInfo();\n          if (!(isTableIncluded(hri.getTable())\n              || hri.isMetaRegion())) {\n            return true;\n          }\n          PairOfSameType<HRegionInfo> daughters = MetaTableAccessor.getDaughterRegions(result);\n          for (HRegionLocation h : rl.getRegionLocations()) {\n            if (h == null || h.getRegionInfo() == null) {\n              continue;\n            }\n            sn = h.getServerName();\n            hri = h.getRegionInfo();\n\n            MetaEntry m = null;\n            if (hri.getReplicaId() == HRegionInfo.DEFAULT_REPLICA_ID) {\n              m = new MetaEntry(hri, sn, ts, daughters.getFirst(), daughters.getSecond());\n            } else {\n              m = new MetaEntry(hri, sn, ts, null, null);\n            }\n            HbckInfo previous = regionInfoMap.get(hri.getEncodedName());\n            if (previous == null) {\n              regionInfoMap.put(hri.getEncodedName(), new HbckInfo(m));\n            } else if (previous.metaEntry == null) {\n              previous.metaEntry = m;\n            } else {\n              throw new IOException(\"Two entries in hbase:meta are same \" + previous);\n            }\n          }\n          PairOfSameType<HRegionInfo> mergeRegions = MetaTableAccessor.getMergeRegions(result);\n          for (HRegionInfo mergeRegion : new HRegionInfo[] {\n              mergeRegions.getFirst(), mergeRegions.getSecond() }) {\n            if (mergeRegion != null) {\n              // This region is already been merged\n              HbckInfo hbInfo = getOrCreateInfo(mergeRegion.getEncodedName());\n              hbInfo.setMerged(true);\n            }\n          }\n\n          // show proof of progress to the user, once for every 100 records.\n          if (countRecord % 100 == 0) {\n            errors.progress();\n          }\n          countRecord++;\n          return true;\n        } catch (RuntimeException e) {\n          LOG.error(\"Result=\" + result);\n          throw e;\n        }\n      }\n    };\n    if (!checkMetaOnly) {\n      // Scan hbase:meta to pick up user regions\n      MetaTableAccessor.fullScanRegions(connection, visitor);\n    }\n\n    errors.print(\"\");\n    return true;\n  }"
        ],
        [
            "ServerName::compareTo(ServerName)",
            " 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 -\n 304  ",
            "  @Override\n  public int compareTo(ServerName other) {\n    int compare = this.getHostname().compareToIgnoreCase(other.getHostname());\n    if (compare != 0) return compare;\n    compare = this.getPort() - other.getPort();\n    if (compare != 0) return compare;\n    return (int)(this.getStartcode() - other.getStartcode());\n  }",
            " 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 +\n 304 +\n 305  ",
            "  @Override\n  public int compareTo(ServerName other) {\n    int compare = this.getHostname().compareToIgnoreCase(other.getHostname());\n    if (compare != 0) return compare;\n    compare = this.getPort() - other.getPort();\n    if (compare != 0) return compare;\n\n    return Long.compare(this.getStartcode(), other.getStartcode());\n  }"
        ]
    ],
    "c67983ebf88d449a67bccd8b213237362a4093f6": [
        [
            "MultiRowRangeFilter::getNextCellHint(Cell)",
            " 138  \n 139  \n 140  \n 141 -\n 142  ",
            "  @Override\n  public Cell getNextCellHint(Cell currentKV) {\n    // skip to the next range's start row\n    return KeyValueUtil.createFirstOnRow(range.startRow);\n  }",
            " 137  \n 138  \n 139  \n 140 +\n 141 +\n 142  ",
            "  @Override\n  public Cell getNextCellHint(Cell currentKV) {\n    // skip to the next range's start row\n    return CellUtil.createFirstOnRow(range.startRow, 0,\n        (short) range.startRow.length);\n  }"
        ],
        [
            "StoreFileScanner::seekToLastRow()",
            " 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509 -\n 510  \n 511  \n 512  \n 513  \n 514  \n 515  ",
            "  @Override\n  public boolean seekToLastRow() throws IOException {\n    byte[] lastRow = reader.getLastRowKey();\n    if (lastRow == null) {\n      return false;\n    }\n    KeyValue seekKey = KeyValueUtil.createFirstOnRow(lastRow);\n    if (seek(seekKey)) {\n      return true;\n    } else {\n      return seekToPreviousRow(seekKey);\n    }\n  }",
            " 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509 +\n 510 +\n 511  \n 512  \n 513  \n 514  \n 515  \n 516  ",
            "  @Override\n  public boolean seekToLastRow() throws IOException {\n    byte[] lastRow = reader.getLastRowKey();\n    if (lastRow == null) {\n      return false;\n    }\n    Cell seekKey = CellUtil\n        .createFirstOnRow(lastRow, 0, (short) lastRow.length);\n    if (seek(seekKey)) {\n      return true;\n    } else {\n      return seekToPreviousRow(seekKey);\n    }\n  }"
        ],
        [
            "HRegion::RegionScannerImpl::reseek(byte)",
            "6212  \n6213  \n6214  \n6215  \n6216  \n6217  \n6218  \n6219 -\n6220  \n6221  \n6222  \n6223  \n6224  \n6225  \n6226  \n6227  \n6228  \n6229  \n6230  \n6231  \n6232  ",
            "    @Override\n    public synchronized boolean reseek(byte[] row) throws IOException {\n      if (row == null) {\n        throw new IllegalArgumentException(\"Row cannot be null.\");\n      }\n      boolean result = false;\n      startRegionOperation();\n      KeyValue kv = KeyValueUtil.createFirstOnRow(row);\n      try {\n        // use request seek to make use of the lazy seek option. See HBASE-5520\n        result = this.storeHeap.requestSeek(kv, true, true);\n        if (this.joinedHeap != null) {\n          result = this.joinedHeap.requestSeek(kv, true, true) || result;\n        }\n      } catch (FileNotFoundException e) {\n        throw handleFileNotFound(e);\n      } finally {\n        closeRegionOperation();\n      }\n      return result;\n    }",
            "6212  \n6213  \n6214  \n6215  \n6216  \n6217  \n6218  \n6219 +\n6220  \n6221  \n6222  \n6223  \n6224  \n6225  \n6226  \n6227  \n6228  \n6229  \n6230  \n6231  \n6232  ",
            "    @Override\n    public synchronized boolean reseek(byte[] row) throws IOException {\n      if (row == null) {\n        throw new IllegalArgumentException(\"Row cannot be null.\");\n      }\n      boolean result = false;\n      startRegionOperation();\n      Cell kv = CellUtil.createFirstOnRow(row, 0, (short) row.length);\n      try {\n        // use request seek to make use of the lazy seek option. See HBASE-5520\n        result = this.storeHeap.requestSeek(kv, true, true);\n        if (this.joinedHeap != null) {\n          result = this.joinedHeap.requestSeek(kv, true, true) || result;\n        }\n      } catch (FileNotFoundException e) {\n        throw handleFileNotFound(e);\n      } finally {\n        closeRegionOperation();\n      }\n      return result;\n    }"
        ],
        [
            "FuzzyRowFilter::getNextCellHint(Cell)",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 -\n 179  ",
            "  @Override\n  public Cell getNextCellHint(Cell currentCell) {\n    boolean result = tracker.updateTracker(currentCell);\n    if (result == false) {\n      done = true;\n      return null;\n    }\n    byte[] nextRowKey = tracker.nextRow();\n    return KeyValueUtil.createFirstOnRow(nextRowKey);\n  }",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  ",
            "  @Override\n  public Cell getNextCellHint(Cell currentCell) {\n    boolean result = tracker.updateTracker(currentCell);\n    if (result == false) {\n      done = true;\n      return null;\n    }\n    byte[] nextRowKey = tracker.nextRow();\n    return CellUtil.createFirstOnRow(nextRowKey, 0, (short) nextRowKey.length);\n  }"
        ],
        [
            "ReversedRegionScannerImpl::nextRow(ScannerContext,Cell)",
            "  66  \n  67  \n  68  \n  69  \n  70 -\n  71 -\n  72 -\n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  ",
            "  @Override\n  protected boolean nextRow(ScannerContext scannerContext, Cell curRowCell)\n      throws IOException {\n    assert super.joinedContinuationRow == null : \"Trying to go to next row during joinedHeap read.\";\n    byte[] row = new byte[curRowCell.getRowLength()];\n    CellUtil.copyRowTo(curRowCell, row, 0);\n    this.storeHeap.seekToPreviousRow(KeyValueUtil.createFirstOnRow(row));\n    resetFilters();\n    // Calling the hook in CP which allows it to do a fast forward\n    if (this.region.getCoprocessorHost() != null) {\n      return this.region.getCoprocessorHost().postScannerFilterRow(this, curRowCell);\n    }\n    return true;\n  }",
            "  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  ",
            "  @Override\n  protected boolean nextRow(ScannerContext scannerContext, Cell curRowCell)\n      throws IOException {\n    assert super.joinedContinuationRow == null : \"Trying to go to next row during joinedHeap read.\";\n    this.storeHeap.seekToPreviousRow(CellUtil.createFirstOnRow(curRowCell));\n    resetFilters();\n    // Calling the hook in CP which allows it to do a fast forward\n    if (this.region.getCoprocessorHost() != null) {\n      return this.region.getCoprocessorHost().postScannerFilterRow(this, curRowCell);\n    }\n    return true;\n  }"
        ]
    ],
    "43a8ac00158e92c3015af7753edd8e835dc6054b": [
        [
            "MemStoreSize::equals(Object)",
            "  63  \n  64  \n  65 -\n  66  \n  67  \n  68  \n  69  \n  70  ",
            "  @Override\n  public boolean equals(Object obj) {\n    if (obj == null || !(obj instanceof MemStoreSize)) {\n      return false;\n    }\n    MemStoreSize other = (MemStoreSize) obj;\n    return this.dataSize == other.dataSize && this.heapSize == other.heapSize;\n  }",
            "  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  ",
            "  @Override\n  public boolean equals(Object obj) {\n    if (obj == null || getClass() != obj.getClass()) {\n      return false;\n    }\n    MemStoreSize other = (MemStoreSize) obj;\n    return this.dataSize == other.dataSize && this.heapSize == other.heapSize;\n  }"
        ],
        [
            "MemStoreSizing::equals(Object)",
            "  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  @Override\n  public boolean equals(Object obj) {\n    if (obj == null || !(obj instanceof MemStoreSizing)) {\n      return false;\n    }\n    MemStoreSizing other = (MemStoreSizing) obj;\n    return this.dataSize == other.dataSize && this.heapSize == other.heapSize;\n  }",
            "  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  @Override\n  public boolean equals(Object obj) {\n    if (obj == null || (getClass() != obj.getClass())) {\n      return false;\n    }\n    MemStoreSizing other = (MemStoreSizing) obj;\n    return this.dataSize == other.dataSize && this.heapSize == other.heapSize;\n  }"
        ]
    ],
    "3b444a066c0c699aff749713209950198f1b21e4": [
        [
            "TestSpnegoHttpServer::buildSpnegoConfiguration(String,File)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171 -\n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  ",
            "  private static Configuration buildSpnegoConfiguration(String serverPrincipal, File\n      serverKeytab) {\n    Configuration conf = new Configuration();\n    KerberosName.setRules(\"DEFAULT\");\n\n    conf.setInt(HttpServer.HTTP_MAX_THREADS, 10);\n\n    // Enable Kerberos (pre-req)\n    conf.set(\"hbase.security.authentication\", \"kerberos\");\n    conf.set(HttpServer.HTTP_UI_AUTHENTICATION, \"kerberos\");\n    conf.set(HttpServer.HTTP_SPNEGO_AUTHENTICATION_PRINCIPAL_KEY, serverPrincipal);\n    conf.set(HttpServer.HTTP_SPNEGO_AUTHENTICATION_KEYTAB_KEY, serverKeytab.getAbsolutePath());\n\n    return conf;\n  }",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  ",
            "  private static Configuration buildSpnegoConfiguration(String serverPrincipal, File\n      serverKeytab) {\n    Configuration conf = new Configuration();\n    KerberosName.setRules(\"DEFAULT\");\n\n    conf.setInt(HttpServer.HTTP_MAX_THREADS, TestHttpServer.MAX_THREADS);\n\n    // Enable Kerberos (pre-req)\n    conf.set(\"hbase.security.authentication\", \"kerberos\");\n    conf.set(HttpServer.HTTP_UI_AUTHENTICATION, \"kerberos\");\n    conf.set(HttpServer.HTTP_SPNEGO_AUTHENTICATION_PRINCIPAL_KEY, serverPrincipal);\n    conf.set(HttpServer.HTTP_SPNEGO_AUTHENTICATION_KEYTAB_KEY, serverKeytab.getAbsolutePath());\n\n    return conf;\n  }"
        ],
        [
            "TestHttpServer::setup()",
            " 151  \n 152  \n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "  @BeforeClass public static void setup() throws Exception {\n    Configuration conf = new Configuration();\n    conf.setInt(HttpServer.HTTP_MAX_THREADS, 10);\n    server = createTestServer(conf);\n    server.addServlet(\"echo\", \"/echo\", EchoServlet.class);\n    server.addServlet(\"echomap\", \"/echomap\", EchoMapServlet.class);\n    server.addServlet(\"htmlcontent\", \"/htmlcontent\", HtmlContentServlet.class);\n    server.addServlet(\"longheader\", \"/longheader\", LongHeaderServlet.class);\n    server.addJerseyResourcePackage(\n        JerseyResource.class.getPackage().getName(), \"/jersey/*\");\n    server.start();\n    baseUrl = getServerURL(server);\n    LOG.info(\"HTTP server started: \"+ baseUrl);\n  }",
            " 152  \n 153  \n 154 +\n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "  @BeforeClass public static void setup() throws Exception {\n    Configuration conf = new Configuration();\n    conf.setInt(HttpServer.HTTP_MAX_THREADS, MAX_THREADS);\n    server = createTestServer(conf);\n    server.addServlet(\"echo\", \"/echo\", EchoServlet.class);\n    server.addServlet(\"echomap\", \"/echomap\", EchoMapServlet.class);\n    server.addServlet(\"htmlcontent\", \"/htmlcontent\", HtmlContentServlet.class);\n    server.addServlet(\"longheader\", \"/longheader\", LongHeaderServlet.class);\n    server.addJerseyResourcePackage(\n        JerseyResource.class.getPackage().getName(), \"/jersey/*\");\n    server.start();\n    baseUrl = getServerURL(server);\n    LOG.info(\"HTTP server started: \"+ baseUrl);\n  }"
        ],
        [
            "TestThriftHttpServer::testRunThriftServerWithHeaderBufferLength()",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 -\n 116  \n 117  \n 118  \n 119  \n 120 -\n 121  \n 122  ",
            "  @Test(timeout=600000)\n  public void testRunThriftServerWithHeaderBufferLength() throws Exception {\n\n    // Test thrift server with HTTP header length less than 64k\n    try {\n      runThriftServer(1024 * 63);\n    } catch (TTransportException tex) {\n      assertFalse(tex.getMessage().equals(\"HTTP Response code: 413\"));\n    }\n\n    // Test thrift server with HTTP header length more than 64k, expect an exception\n    exception.expect(TTransportException.class);\n    exception.expectMessage(\"HTTP Response code: 413\");\n    runThriftServer(1024 * 64);\n  }",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 +\n 116  \n 117  \n 118  \n 119  \n 120 +\n 121  \n 122  ",
            "  @Test(timeout=600000)\n  public void testRunThriftServerWithHeaderBufferLength() throws Exception {\n\n    // Test thrift server with HTTP header length less than 64k\n    try {\n      runThriftServer(1024 * 63);\n    } catch (TTransportException tex) {\n      assertFalse(tex.getMessage().equals(\"HTTP Response code: 431\"));\n    }\n\n    // Test thrift server with HTTP header length more than 64k, expect an exception\n    exception.expect(TTransportException.class);\n    exception.expectMessage(\"HTTP Response code: 431\");\n    runThriftServer(1024 * 64);\n  }"
        ],
        [
            "TestSSLHttpServer::setup()",
            "  62  \n  63  \n  64  \n  65 -\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "  @BeforeClass\n  public static void setup() throws Exception {\n    conf = new Configuration();\n    conf.setInt(HttpServer.HTTP_MAX_THREADS, 10);\n\n    File base = new File(BASEDIR);\n    FileUtil.fullyDelete(base);\n    base.mkdirs();\n    keystoresDir = new File(BASEDIR).getAbsolutePath();\n    sslConfDir = KeyStoreTestUtil.getClasspathDir(TestSSLHttpServer.class);\n\n    KeyStoreTestUtil.setupSSLConfig(keystoresDir, sslConfDir, conf, false);\n    Configuration sslConf = new Configuration(false);\n    sslConf.addResource(\"ssl-server.xml\");\n    sslConf.addResource(\"ssl-client.xml\");\n\n    clientSslFactory = new SSLFactory(SSLFactory.Mode.CLIENT, sslConf);\n    clientSslFactory.init();\n\n    server = new HttpServer.Builder()\n        .setName(\"test\")\n        .addEndpoint(new URI(\"https://localhost\"))\n        .setConf(conf)\n        .keyPassword(HBaseConfiguration.getPassword(sslConf, \"ssl.server.keystore.keypassword\",\n            null))\n        .keyStore(sslConf.get(\"ssl.server.keystore.location\"),\n            HBaseConfiguration.getPassword(sslConf, \"ssl.server.keystore.password\", null),\n            sslConf.get(\"ssl.server.keystore.type\", \"jks\"))\n        .trustStore(sslConf.get(\"ssl.server.truststore.location\"),\n            HBaseConfiguration.getPassword(sslConf, \"ssl.server.truststore.password\", null),\n            sslConf.get(\"ssl.server.truststore.type\", \"jks\")).build();\n    server.addServlet(\"echo\", \"/echo\", TestHttpServer.EchoServlet.class);\n    server.start();\n    baseUrl = new URL(\"https://\"\n        + NetUtils.getHostPortString(server.getConnectorAddress(0)));\n    LOG.info(\"HTTP server started: \" + baseUrl);\n  }",
            "  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "  @BeforeClass\n  public static void setup() throws Exception {\n    conf = new Configuration();\n    conf.setInt(HttpServer.HTTP_MAX_THREADS, TestHttpServer.MAX_THREADS);\n\n    File base = new File(BASEDIR);\n    FileUtil.fullyDelete(base);\n    base.mkdirs();\n    keystoresDir = new File(BASEDIR).getAbsolutePath();\n    sslConfDir = KeyStoreTestUtil.getClasspathDir(TestSSLHttpServer.class);\n\n    KeyStoreTestUtil.setupSSLConfig(keystoresDir, sslConfDir, conf, false);\n    Configuration sslConf = new Configuration(false);\n    sslConf.addResource(\"ssl-server.xml\");\n    sslConf.addResource(\"ssl-client.xml\");\n\n    clientSslFactory = new SSLFactory(SSLFactory.Mode.CLIENT, sslConf);\n    clientSslFactory.init();\n\n    server = new HttpServer.Builder()\n        .setName(\"test\")\n        .addEndpoint(new URI(\"https://localhost\"))\n        .setConf(conf)\n        .keyPassword(HBaseConfiguration.getPassword(sslConf, \"ssl.server.keystore.keypassword\",\n            null))\n        .keyStore(sslConf.get(\"ssl.server.keystore.location\"),\n            HBaseConfiguration.getPassword(sslConf, \"ssl.server.keystore.password\", null),\n            sslConf.get(\"ssl.server.keystore.type\", \"jks\"))\n        .trustStore(sslConf.get(\"ssl.server.truststore.location\"),\n            HBaseConfiguration.getPassword(sslConf, \"ssl.server.truststore.password\", null),\n            sslConf.get(\"ssl.server.truststore.type\", \"jks\")).build();\n    server.addServlet(\"echo\", \"/echo\", TestHttpServer.EchoServlet.class);\n    server.start();\n    baseUrl = new URL(\"https://\"\n        + NetUtils.getHostPortString(server.getConnectorAddress(0)));\n    LOG.info(\"HTTP server started: \" + baseUrl);\n  }"
        ],
        [
            "TestSpnegoHttpServer::testMissingConfigurationThrowsException()",
            " 245  \n 246  \n 247  \n 248 -\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "  @Test(expected = IllegalArgumentException.class)\n  public void testMissingConfigurationThrowsException() throws Exception {\n    Configuration conf = new Configuration();\n    conf.setInt(HttpServer.HTTP_MAX_THREADS, 10);\n    // Enable Kerberos (pre-req)\n    conf.set(\"hbase.security.authentication\", \"kerberos\");\n    // Intentionally skip keytab and principal\n\n    HttpServer customServer = createTestServerWithSecurity(conf);\n    customServer.addServlet(\"echo\", \"/echo\", EchoServlet.class);\n    customServer.addJerseyResourcePackage(JerseyResource.class.getPackage().getName(), \"/jersey/*\");\n    customServer.start();\n  }",
            " 245  \n 246  \n 247  \n 248 +\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "  @Test(expected = IllegalArgumentException.class)\n  public void testMissingConfigurationThrowsException() throws Exception {\n    Configuration conf = new Configuration();\n    conf.setInt(HttpServer.HTTP_MAX_THREADS, TestHttpServer.MAX_THREADS);\n    // Enable Kerberos (pre-req)\n    conf.set(\"hbase.security.authentication\", \"kerberos\");\n    // Intentionally skip keytab and principal\n\n    HttpServer customServer = createTestServerWithSecurity(conf);\n    customServer.addServlet(\"echo\", \"/echo\", EchoServlet.class);\n    customServer.addJerseyResourcePackage(JerseyResource.class.getPackage().getName(), \"/jersey/*\");\n    customServer.start();\n  }"
        ],
        [
            "FavoredNodeLoadBalancer::balanceCluster(Map)",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 -\n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  ",
            "  @Override\n  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState)  {\n    //TODO. Look at is whether Stochastic loadbalancer can be integrated with this\n    List<RegionPlan> plans = new ArrayList<>();\n    //perform a scan of the meta to get the latest updates (if any)\n    SnapshotOfRegionAssignmentFromMeta snaphotOfRegionAssignment =\n        new SnapshotOfRegionAssignmentFromMeta(super.services.getConnection());\n    try {\n      snaphotOfRegionAssignment.initialize();\n    } catch (IOException ie) {\n      LOG.warn(\"Not running balancer since exception was thrown \" + ie);\n      return plans;\n    }\n    Map<ServerName, ServerName> serverNameToServerNameWithoutCode = new HashMap<>();\n    Map<ServerName, ServerName> serverNameWithoutCodeToServerName = new HashMap<>();\n    ServerManager serverMgr = super.services.getServerManager();\n    for (ServerName sn: serverMgr.getOnlineServersList()) {\n      ServerName s = ServerName.valueOf(sn.getHostname(), sn.getPort(), ServerName.NON_STARTCODE);\n      serverNameToServerNameWithoutCode.put(sn, s);\n      serverNameWithoutCodeToServerName.put(s, sn);\n    }\n    for (Map.Entry<ServerName, List<HRegionInfo>> entry : clusterState.entrySet()) {\n      ServerName currentServer = entry.getKey();\n      //get a server without the startcode for the currentServer\n      ServerName currentServerWithoutStartCode = ServerName.valueOf(currentServer.getHostname(),\n          currentServer.getPort(), ServerName.NON_STARTCODE);\n      List<HRegionInfo> list = entry.getValue();\n      for (HRegionInfo region : list) {\n        if(!FavoredNodesManager.isFavoredNodeApplicable(region)) {\n          continue;\n        }\n        List<ServerName> favoredNodes = fnm.getFavoredNodes(region);\n        if (favoredNodes == null || favoredNodes.get(0).equals(currentServerWithoutStartCode)) {\n          continue; //either favorednodes does not exist or we are already on the primary node\n        }\n        ServerName destination = null;\n        //check whether the primary is available\n        destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(0));\n        if (destination == null) {\n          //check whether the region is on secondary/tertiary\n          if (currentServerWithoutStartCode.equals(favoredNodes.get(1)) ||\n              currentServerWithoutStartCode.equals(favoredNodes.get(2))) {\n            continue;\n          }\n          //the region is currently on none of the favored nodes\n          //get it on one of them if possible\n          ServerLoad l1 = super.services.getServerManager().getLoad(\n              serverNameWithoutCodeToServerName.get(favoredNodes.get(1)));\n          ServerLoad l2 = super.services.getServerManager().getLoad(\n              serverNameWithoutCodeToServerName.get(favoredNodes.get(2)));\n          if (l1 != null && l2 != null) {\n            if (l1.getLoad() > l2.getLoad()) {\n              destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(2));\n            } else {\n              destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(1));\n            }\n          } else if (l1 != null) {\n            destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(1));\n          } else if (l2 != null) {\n            destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(2));\n          }\n        }\n\n        if (destination != null) {\n          RegionPlan plan = new RegionPlan(region, currentServer, destination);\n          plans.add(plan);\n        }\n      }\n    }\n    return plans;\n  }",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105 +\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  ",
            "  @Override\n  public List<RegionPlan> balanceCluster(Map<ServerName, List<HRegionInfo>> clusterState)  {\n    //TODO. Look at is whether Stochastic loadbalancer can be integrated with this\n    List<RegionPlan> plans = new ArrayList<>();\n    //perform a scan of the meta to get the latest updates (if any)\n    SnapshotOfRegionAssignmentFromMeta snaphotOfRegionAssignment =\n        new SnapshotOfRegionAssignmentFromMeta(super.services.getConnection());\n    try {\n      snaphotOfRegionAssignment.initialize();\n    } catch (IOException ie) {\n      LOG.warn(\"Not running balancer since exception was thrown \" + ie);\n      return plans;\n    }\n    // This is not used? Findbugs says so: Map<ServerName, ServerName> serverNameToServerNameWithoutCode = new HashMap<>();\n    Map<ServerName, ServerName> serverNameWithoutCodeToServerName = new HashMap<>();\n    ServerManager serverMgr = super.services.getServerManager();\n    for (ServerName sn: serverMgr.getOnlineServersList()) {\n      ServerName s = ServerName.valueOf(sn.getHostname(), sn.getPort(), ServerName.NON_STARTCODE);\n      // FindBugs complains about useless store! serverNameToServerNameWithoutCode.put(sn, s);\n      serverNameWithoutCodeToServerName.put(s, sn);\n    }\n    for (Map.Entry<ServerName, List<HRegionInfo>> entry : clusterState.entrySet()) {\n      ServerName currentServer = entry.getKey();\n      //get a server without the startcode for the currentServer\n      ServerName currentServerWithoutStartCode = ServerName.valueOf(currentServer.getHostname(),\n          currentServer.getPort(), ServerName.NON_STARTCODE);\n      List<HRegionInfo> list = entry.getValue();\n      for (HRegionInfo region : list) {\n        if(!FavoredNodesManager.isFavoredNodeApplicable(region)) {\n          continue;\n        }\n        List<ServerName> favoredNodes = fnm.getFavoredNodes(region);\n        if (favoredNodes == null || favoredNodes.get(0).equals(currentServerWithoutStartCode)) {\n          continue; //either favorednodes does not exist or we are already on the primary node\n        }\n        ServerName destination = null;\n        //check whether the primary is available\n        destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(0));\n        if (destination == null) {\n          //check whether the region is on secondary/tertiary\n          if (currentServerWithoutStartCode.equals(favoredNodes.get(1)) ||\n              currentServerWithoutStartCode.equals(favoredNodes.get(2))) {\n            continue;\n          }\n          //the region is currently on none of the favored nodes\n          //get it on one of them if possible\n          ServerLoad l1 = super.services.getServerManager().getLoad(\n              serverNameWithoutCodeToServerName.get(favoredNodes.get(1)));\n          ServerLoad l2 = super.services.getServerManager().getLoad(\n              serverNameWithoutCodeToServerName.get(favoredNodes.get(2)));\n          if (l1 != null && l2 != null) {\n            if (l1.getLoad() > l2.getLoad()) {\n              destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(2));\n            } else {\n              destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(1));\n            }\n          } else if (l1 != null) {\n            destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(1));\n          } else if (l2 != null) {\n            destination = serverNameWithoutCodeToServerName.get(favoredNodes.get(2));\n          }\n        }\n\n        if (destination != null) {\n          RegionPlan plan = new RegionPlan(region, currentServer, destination);\n          plans.add(plan);\n        }\n      }\n    }\n    return plans;\n  }"
        ]
    ],
    "0732ef5ebfea8e254d4a5dad6fe16ecccca3b799": [
        [
            "TestAssignmentManagerMetrics::testRITAssignmentManagerMetrics()",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133 -\n 134 -\n 135 -\n 136  \n 137 -\n 138  \n 139  \n 140 -\n 141 -\n 142 -\n 143  \n 144  \n 145  \n 146  \n 147 -\n 148 -\n 149 -\n 150 -\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  ",
            "  @Test\n  public void testRITAssignmentManagerMetrics() throws Exception {\n    final TableName TABLENAME = TableName.valueOf(name.getMethodName());\n    final byte[] FAMILY = Bytes.toBytes(\"family\");\n\n    Table table = null;\n    try {\n      table = TEST_UTIL.createTable(TABLENAME, FAMILY);\n\n      final byte[] row = Bytes.toBytes(\"row\");\n      final byte[] qualifier = Bytes.toBytes(\"qualifier\");\n      final byte[] value = Bytes.toBytes(\"value\");\n\n      Put put = new Put(row);\n      put.addColumn(FAMILY, qualifier, value);\n      table.put(put);\n\n      // Sleep 3 seconds, wait for doMetrics chore catching up\n      Thread.sleep(msgInterval * 3);\n\n      // check the RIT is 0\n      MetricsAssignmentManagerSource amSource =\n          master.getAssignmentManager().getAssignmentManagerMetrics().getMetricsProcSource();\n\n      metricsHelper.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_NAME, 0, amSource);\n      metricsHelper.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_OVER_THRESHOLD_NAME, 0,\n          amSource);\n\n      // alter table with a non-existing coprocessor\n      ColumnFamilyDescriptor hcd = ColumnFamilyDescriptorBuilder.newBuilder(FAMILY).build();\n      TableDescriptor htd = TableDescriptorBuilder.newBuilder(TABLENAME).addColumnFamily(hcd).\n          addCoprocessorWithSpec(\"hdfs:///foo.jar|com.foo.FooRegionObserver|1001|arg1=1,arg2=2\").\n          build();\n      try {\n        TEST_UTIL.getAdmin().modifyTable(htd);\n        fail(\"Expected region failed to open\");\n      } catch (IOException e) {\n        // Expected, the RS will crash and the assignment will spin forever waiting for a RS\n        // to assign the region.\n        LOG.info(\"Expected exception\", e);\n      }\n\n      // Sleep 3 seconds, wait for doMetrics chore catching up\n      Thread.sleep(msgInterval * 3);\n      // Two regions in RIT -- meta and the testRITAssignementManagerMetrics table region.\n      metricsHelper.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_NAME, 2, amSource);\n      // Both are over the threshold because no RegionServer to assign to.\n      metricsHelper.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_OVER_THRESHOLD_NAME, 2,\n          amSource);\n\n    } finally {\n      if (table != null) {\n        table.close();\n      }\n    }\n  }",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135 +\n 136 +\n 137 +\n 138 +\n 139 +\n 140 +\n 141 +\n 142 +\n 143  \n 144 +\n 145  \n 146  \n 147 +\n 148 +\n 149 +\n 150  \n 151  \n 152  \n 153  \n 154 +\n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "  @Test\n  public void testRITAssignmentManagerMetrics() throws Exception {\n    final TableName TABLENAME = TableName.valueOf(name.getMethodName());\n    final byte[] FAMILY = Bytes.toBytes(\"family\");\n\n    Table table = null;\n    try {\n      table = TEST_UTIL.createTable(TABLENAME, FAMILY);\n\n      final byte[] row = Bytes.toBytes(\"row\");\n      final byte[] qualifier = Bytes.toBytes(\"qualifier\");\n      final byte[] value = Bytes.toBytes(\"value\");\n\n      Put put = new Put(row);\n      put.addColumn(FAMILY, qualifier, value);\n      table.put(put);\n\n      // Sleep 3 seconds, wait for doMetrics chore catching up\n      Thread.sleep(msgInterval * 3);\n\n      // check the RIT is 0\n      MetricsAssignmentManagerSource amSource =\n          master.getAssignmentManager().getAssignmentManagerMetrics().getMetricsProcSource();\n\n      metricsHelper.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_NAME, 0, amSource);\n      metricsHelper.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_OVER_THRESHOLD_NAME, 0,\n          amSource);\n\n      // alter table with a non-existing coprocessor\n      HTableDescriptor htd = new HTableDescriptor(TABLENAME);\n      HColumnDescriptor hcd = new HColumnDescriptor(FAMILY);\n\n      htd.addFamily(hcd);\n\n      String spec = \"hdfs:///foo.jar|com.foo.FooRegionObserver|1001|arg1=1,arg2=2\";\n      htd.addCoprocessorWithSpec(spec);\n\n      try {\n        TEST_UTIL.getAdmin().modifyTable(TABLENAME, htd);\n        fail(\"Expected region failed to open\");\n      } catch (IOException e) {\n        // expected, the RS will crash and the assignment will spin forever waiting for a RS\n        // to assign the region. the region will not go to FAILED_OPEN because in this case\n        // we have just one RS and it will do one retry.\n      }\n\n      // Sleep 3 seconds, wait for doMetrics chore catching up\n      Thread.sleep(msgInterval * 3);\n      metricsHelper.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_NAME, 1, amSource);\n      metricsHelper.assertGauge(MetricsAssignmentManagerSource.RIT_COUNT_OVER_THRESHOLD_NAME, 1,\n          amSource);\n\n    } finally {\n      if (table != null) {\n        table.close();\n      }\n    }\n  }"
        ],
        [
            "SplitLogManager::splitLogDistributed(Set,List,PathFilter)",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 -\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  ",
            "  /**\n   * The caller will block until all the hbase:meta log files of the given region server have been\n   * processed - successfully split or an error is encountered - by an available worker region\n   * server. This method must only be called after the region servers have been brought online.\n   * @param logDirs List of log dirs to split\n   * @param filter the Path filter to select specific files for considering\n   * @throws IOException If there was an error while splitting any log file\n   * @return cumulative size of the logfiles split\n   */\n  public long splitLogDistributed(final Set<ServerName> serverNames, final List<Path> logDirs,\n      PathFilter filter) throws IOException {\n    MonitoredTask status = TaskMonitor.get().createStatus(\"Doing distributed log split in \" +\n      logDirs + \" for serverName=\" + serverNames);\n    FileStatus[] logfiles = getFileList(logDirs, filter);\n    status.setStatus(\"Checking directory contents...\");\n    SplitLogCounters.tot_mgr_log_split_batch_start.increment();\n    LOG.info(\"Started splitting \" + logfiles.length + \" logs in \" + logDirs +\n      \" for \" + serverNames);\n    long t = EnvironmentEdgeManager.currentTime();\n    long totalSize = 0;\n    TaskBatch batch = new TaskBatch();\n    for (FileStatus lf : logfiles) {\n      // TODO If the log file is still being written to - which is most likely\n      // the case for the last log file - then its length will show up here\n      // as zero. The size of such a file can only be retrieved after\n      // recover-lease is done. totalSize will be under in most cases and the\n      // metrics that it drives will also be under-reported.\n      totalSize += lf.getLen();\n      String pathToLog = FSUtils.removeWALRootPath(lf.getPath(), conf);\n      if (!enqueueSplitTask(pathToLog, batch)) {\n        throw new IOException(\"duplicate log split scheduled for \" + lf.getPath());\n      }\n    }\n    waitForSplittingCompletion(batch, status);\n\n    if (batch.done != batch.installed) {\n      batch.isDead = true;\n      SplitLogCounters.tot_mgr_log_split_batch_err.increment();\n      String msg = \"Error or interrupted while splitting WALs in \" + logDirs + \"; task=\" + batch;\n      status.abort(msg);\n      throw new IOException(msg);\n    }\n    for (Path logDir : logDirs) {\n      status.setStatus(\"Cleaning up log directory...\");\n      final FileSystem fs = logDir.getFileSystem(conf);\n      try {\n        if (fs.exists(logDir) && !fs.delete(logDir, false)) {\n          LOG.warn(\"Unable to delete log src dir. Ignoring. \" + logDir);\n        }\n      } catch (IOException ioe) {\n        FileStatus[] files = fs.listStatus(logDir);\n        if (files != null && files.length > 0) {\n          LOG.warn(\"Returning success without actually splitting and \"\n              + \"deleting all the log files in path \" + logDir + \": \"\n              + Arrays.toString(files), ioe);\n        } else {\n          LOG.warn(\"Unable to delete log src dir. Ignoring. \" + logDir, ioe);\n        }\n      }\n      SplitLogCounters.tot_mgr_log_split_batch_success.increment();\n    }\n    String msg =\n        \"finished splitting (more than or equal to) \" + totalSize + \" bytes in \" + batch.installed\n            + \" log files in \" + logDirs + \" in \"\n            + (EnvironmentEdgeManager.currentTime() - t) + \"ms\";\n    status.markComplete(msg);\n    LOG.info(msg);\n    return totalSize;\n  }",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 +\n 264 +\n 265 +\n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  ",
            "  /**\n   * The caller will block until all the hbase:meta log files of the given region server have been\n   * processed - successfully split or an error is encountered - by an available worker region\n   * server. This method must only be called after the region servers have been brought online.\n   * @param logDirs List of log dirs to split\n   * @param filter the Path filter to select specific files for considering\n   * @throws IOException If there was an error while splitting any log file\n   * @return cumulative size of the logfiles split\n   */\n  public long splitLogDistributed(final Set<ServerName> serverNames, final List<Path> logDirs,\n      PathFilter filter) throws IOException {\n    MonitoredTask status = TaskMonitor.get().createStatus(\"Doing distributed log split in \" +\n      logDirs + \" for serverName=\" + serverNames);\n    FileStatus[] logfiles = getFileList(logDirs, filter);\n    status.setStatus(\"Checking directory contents...\");\n    SplitLogCounters.tot_mgr_log_split_batch_start.increment();\n    LOG.info(\"Started splitting \" + logfiles.length + \" logs in \" + logDirs +\n      \" for \" + serverNames);\n    long t = EnvironmentEdgeManager.currentTime();\n    long totalSize = 0;\n    TaskBatch batch = new TaskBatch();\n    for (FileStatus lf : logfiles) {\n      // TODO If the log file is still being written to - which is most likely\n      // the case for the last log file - then its length will show up here\n      // as zero. The size of such a file can only be retrieved after\n      // recover-lease is done. totalSize will be under in most cases and the\n      // metrics that it drives will also be under-reported.\n      totalSize += lf.getLen();\n      String pathToLog = FSUtils.removeWALRootPath(lf.getPath(), conf);\n      if (!enqueueSplitTask(pathToLog, batch)) {\n        throw new IOException(\"duplicate log split scheduled for \" + lf.getPath());\n      }\n    }\n    waitForSplittingCompletion(batch, status);\n\n    if (batch.done != batch.installed) {\n      batch.isDead = true;\n      SplitLogCounters.tot_mgr_log_split_batch_err.increment();\n      LOG.warn(\"error while splitting logs in \" + logDirs + \" installed = \" + batch.installed\n          + \" but only \" + batch.done + \" done\");\n      String msg = \"error or interrupted while splitting logs in \" + logDirs + \" Task = \" + batch;\n      status.abort(msg);\n      throw new IOException(msg);\n    }\n    for (Path logDir : logDirs) {\n      status.setStatus(\"Cleaning up log directory...\");\n      final FileSystem fs = logDir.getFileSystem(conf);\n      try {\n        if (fs.exists(logDir) && !fs.delete(logDir, false)) {\n          LOG.warn(\"Unable to delete log src dir. Ignoring. \" + logDir);\n        }\n      } catch (IOException ioe) {\n        FileStatus[] files = fs.listStatus(logDir);\n        if (files != null && files.length > 0) {\n          LOG.warn(\"Returning success without actually splitting and \"\n              + \"deleting all the log files in path \" + logDir + \": \"\n              + Arrays.toString(files), ioe);\n        } else {\n          LOG.warn(\"Unable to delete log src dir. Ignoring. \" + logDir, ioe);\n        }\n      }\n      SplitLogCounters.tot_mgr_log_split_batch_success.increment();\n    }\n    String msg =\n        \"finished splitting (more than or equal to) \" + totalSize + \" bytes in \" + batch.installed\n            + \" log files in \" + logDirs + \" in \"\n            + (EnvironmentEdgeManager.currentTime() - t) + \"ms\";\n    status.markComplete(msg);\n    LOG.info(msg);\n    return totalSize;\n  }"
        ],
        [
            "HMaster::waitForMasterActive()",
            " 655  \n 656  \n 657  \n 658  \n 659  \n 660 -\n 661 -\n 662  \n 663  \n 664  ",
            "  /**\n   * Wait here if backup Master. This avoids showing backup masters as regionservers in master\n   * web UI, or assigning any region to them.\n   */\n  @Override\n  protected void waitForMasterActive() {\n    while (!this.activeMaster && keepLooping()) {\n      sleeper.sleep();\n    }\n  }",
            " 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661 +\n 662 +\n 663 +\n 664  \n 665  \n 666  ",
            "  /**\n   * If configured to put regions on active master,\n   * wait till a backup master becomes active.\n   * Otherwise, loop till the server is stopped or aborted.\n   */\n  @Override\n  protected void waitForMasterActive(){\n    boolean tablesOnMaster = LoadBalancer.isTablesOnMaster(conf);\n    while (!(tablesOnMaster && activeMaster) && !isStopped() && !isAborted()) {\n      sleeper.sleep();\n    }\n  }"
        ],
        [
            "AssignmentManager::processAssignQueue()",
            "1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730 -\n1731 -\n1732  \n1733 -\n1734  \n1735  \n1736 -\n1737  \n1738 -\n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751 -\n1752 -\n1753 -\n1754  \n1755  \n1756  \n1757 -\n1758  \n1759  \n1760  \n1761 -\n1762  \n1763  \n1764  \n1765 -\n1766  \n1767 -\n1768 -\n1769 -\n1770 -\n1771  \n1772  \n1773  \n1774  \n1775 -\n1776  \n1777 -\n1778 -\n1779 -\n1780  \n1781  \n1782 -\n1783  ",
            "  private void processAssignQueue() {\n    final HashMap<RegionInfo, RegionStateNode> regions = waitOnAssignQueue();\n    if (regions == null || regions.size() == 0 || !isRunning()) {\n      return;\n    }\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PROCESS ASSIGN QUEUE regionCount=\" + regions.size());\n    }\n\n    // TODO: Optimize balancer. pass a RegionPlan?\n    final HashMap<RegionInfo, ServerName> retainPlan = new HashMap<>();\n    final List<RegionInfo> userRegionInfos = new ArrayList<>();\n    // Regions for system tables requiring reassignment\n    final List<RegionInfo> systemRegionInfos = new ArrayList<>();\n    for (RegionStateNode regionStateNode: regions.values()) {\n      boolean sysTable = regionStateNode.isSystemTable();\n      final List<RegionInfo> hris = sysTable? systemRegionInfos: userRegionInfos;\n      if (regionStateNode.getRegionLocation() != null) {\n        retainPlan.put(regionStateNode.getRegionInfo(), regionStateNode.getRegionLocation());\n      } else {\n        hris.add(regionStateNode.getRegionInfo());\n      }\n    }\n\n    // TODO: connect with the listener to invalidate the cache\n\n    // TODO use events\n    List<ServerName> servers = master.getServerManager().createDestinationServersList();\n    for (int i = 0; servers.size() < 1; ++i) {\n      // Report every fourth time around this loop; try not to flood log.\n      if (i % 4 == 0) {\n        // Log every 4th time; we wait 250ms below so means every second.\n        LOG.warn(\"No server available; unable to find a location for \" + regions.size() +\n            \" regions. waiting...\");\n      }\n\n      if (!isRunning()) {\n        LOG.debug(\"Aborting assignment-queue with \" + regions.size() + \" unassigned\");\n        return;\n      }\n      Threads.sleep(250);\n      // Refresh server list.\n      servers = master.getServerManager().createDestinationServersList();\n    }\n\n    if (!systemRegionInfos.isEmpty()) {\n      // System table regions requiring reassignment are present, get region servers\n      // not available for system table regions. Here we are filtering out any regionservers\n      // that might be running older versions of the RegionServer; we want system tables on any\n      // newer servers that may be present. Newer servers means we are probably doing a rolling\n      // upgrade.\n      final List<ServerName> excludeServers = getExcludedServersForSystemTable();\n      List<ServerName> serversForSysTables = servers.stream()\n          .filter(s -> !excludeServers.contains(s)).collect(Collectors.toList());\n      if (serversForSysTables.isEmpty()) {\n        LOG.warn(\"All servers excluded! Considering all servers!\");\n      }\n      LOG.debug(\"Candidate servers to host system regions=\" + serversForSysTables.size() +\n          \"; totalServersCount=\" + servers.size());\n      processAssignmentPlans(regions, null, systemRegionInfos,\n          serversForSysTables.isEmpty()? servers: serversForSysTables);\n    }\n    processAssignmentPlans(regions, retainPlan, userRegionInfos, servers);\n  }",
            "1719  \n1720  \n1721  \n1722  \n1723  \n1724  \n1725  \n1726  \n1727  \n1728  \n1729  \n1730 +\n1731 +\n1732  \n1733 +\n1734  \n1735  \n1736 +\n1737  \n1738 +\n1739  \n1740  \n1741  \n1742  \n1743  \n1744  \n1745  \n1746  \n1747  \n1748  \n1749  \n1750  \n1751 +\n1752  \n1753  \n1754  \n1755 +\n1756  \n1757  \n1758  \n1759  \n1760  \n1761  \n1762 +\n1763  \n1764 +\n1765  \n1766  \n1767  \n1768  \n1769 +\n1770 +\n1771  \n1772 +\n1773 +\n1774 +\n1775  \n1776  \n1777 +\n1778 +\n1779  ",
            "  private void processAssignQueue() {\n    final HashMap<RegionInfo, RegionStateNode> regions = waitOnAssignQueue();\n    if (regions == null || regions.size() == 0 || !isRunning()) {\n      return;\n    }\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"PROCESS ASSIGN QUEUE regionCount=\" + regions.size());\n    }\n\n    // TODO: Optimize balancer. pass a RegionPlan?\n    final HashMap<RegionInfo, ServerName> retainMap = new HashMap<>();\n    final List<RegionInfo> userHRIs = new ArrayList<>(regions.size());\n    // Regions for system tables requiring reassignment\n    final List<RegionInfo> systemHRIs = new ArrayList<>();\n    for (RegionStateNode regionStateNode: regions.values()) {\n      boolean sysTable = regionStateNode.isSystemTable();\n      final List<RegionInfo> hris = sysTable? systemHRIs: userHRIs;\n      if (regionStateNode.getRegionLocation() != null) {\n        retainMap.put(regionStateNode.getRegionInfo(), regionStateNode.getRegionLocation());\n      } else {\n        hris.add(regionStateNode.getRegionInfo());\n      }\n    }\n\n    // TODO: connect with the listener to invalidate the cache\n\n    // TODO use events\n    List<ServerName> servers = master.getServerManager().createDestinationServersList();\n    for (int i = 0; servers.size() < 1; ++i) {\n      // Report every fourth time around this loop; try not to flood log.\n      if (i % 4 == 0) {\n        LOG.warn(\"No servers available; cannot place \" + regions.size() + \" unassigned regions.\");\n      }\n\n      if (!isRunning()) {\n        LOG.debug(\"Stopped! Dropping assign of \" + regions.size() + \" queued regions.\");\n        return;\n      }\n      Threads.sleep(250);\n      servers = master.getServerManager().createDestinationServersList();\n    }\n\n    if (!systemHRIs.isEmpty()) {\n      // System table regions requiring reassignment are present, get region servers\n      // not available for system table regions\n      final List<ServerName> excludeServers = getExcludedServersForSystemTable();\n      List<ServerName> serversForSysTables = servers.stream()\n          .filter(s -> !excludeServers.contains(s)).collect(Collectors.toList());\n      if (serversForSysTables.isEmpty()) {\n        LOG.warn(\"Filtering old server versions and the excluded produced an empty set; \" +\n            \"instead considering all candidate servers!\");\n      }\n      LOG.debug(\"Processing assignQueue; systemServersCount=\" + serversForSysTables.size() +\n          \", allServersCount=\" + servers.size());\n      processAssignmentPlans(regions, null, systemHRIs,\n          serversForSysTables.isEmpty()? servers: serversForSysTables);\n    }\n\n    processAssignmentPlans(regions, retainMap, userHRIs, servers);\n  }"
        ],
        [
            "BaseLoadBalancer::roundRobinAssignment(List,List)",
            "1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232 -\n1233 -\n1234 -\n1235 -\n1236 -\n1237 -\n1238  \n1239 -\n1240  \n1241 -\n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  \n1288  \n1289  \n1290  \n1291  \n1292  \n1293  ",
            "  /**\n   * Generates a bulk assignment plan to be used on cluster startup using a\n   * simple round-robin assignment.\n   * <p>\n   * Takes a list of all the regions and all the servers in the cluster and\n   * returns a map of each server to the regions that it should be assigned.\n   * <p>\n   * Currently implemented as a round-robin assignment. Same invariant as load\n   * balancing, all servers holding floor(avg) or ceiling(avg).\n   *\n   * TODO: Use block locations from HDFS to place regions with their blocks\n   *\n   * @param regions all regions\n   * @param servers all servers\n   * @return map of server to the regions it should take, or null if no\n   *         assignment is possible (ie. no regions or no servers)\n   */\n  @Override\n  public Map<ServerName, List<RegionInfo>> roundRobinAssignment(List<RegionInfo> regions,\n      List<ServerName> servers) throws HBaseIOException {\n    metricsBalancer.incrMiscInvocations();\n    Map<ServerName, List<RegionInfo>> assignments = assignMasterSystemRegions(regions, servers);\n    if (assignments != null && !assignments.isEmpty()) {\n      servers = new ArrayList<>(servers);\n      // Guarantee not to put other regions on master\n      servers.remove(masterServerName);\n      List<RegionInfo> masterRegions = assignments.get(masterServerName);\n      if (!masterRegions.isEmpty()) {\n        regions = new ArrayList<>(regions);\n        regions.removeAll(masterRegions);\n      }\n    }\n    if (regions == null || regions.isEmpty()) {\n      return assignments;\n    }\n    if (!this.tablesOnMaster) {\n      // Make sure Master is not in set of possible servers.\n      if (servers != null && !servers.isEmpty()) {\n        servers.remove(this.masterServerName);\n      }\n    }\n\n    int numServers = servers == null? 0: servers.size();\n    if (numServers == 0) {\n      LOG.warn(\"Wanted to round-robin assignment but no server(s) to assign to.\");\n      return null;\n    }\n\n    // TODO: instead of retainAssignment() and roundRobinAssignment(), we should just run the\n    // normal LB.balancerCluster() with unassignedRegions. We only need to have a candidate\n    // generator for AssignRegionAction. The LB will ensure the regions are mostly local\n    // and balanced. This should also run fast with fewer number of iterations.\n\n    if (numServers == 1) { // Only one server, nothing fancy we can do here\n      ServerName server = servers.get(0);\n      assignments.put(server, new ArrayList<>(regions));\n      return assignments;\n    }\n\n    Cluster cluster = createCluster(servers, regions, false);\n    List<RegionInfo> unassignedRegions = new ArrayList<>();\n\n    roundRobinAssignment(cluster, regions, unassignedRegions,\n      servers, assignments);\n\n    List<RegionInfo> lastFewRegions = new ArrayList<>();\n    // assign the remaining by going through the list and try to assign to servers one-by-one\n    int serverIdx = RANDOM.nextInt(numServers);\n    for (RegionInfo region : unassignedRegions) {\n      boolean assigned = false;\n      for (int j = 0; j < numServers; j++) { // try all servers one by one\n        ServerName serverName = servers.get((j + serverIdx) % numServers);\n        if (!cluster.wouldLowerAvailability(region, serverName)) {\n          List<RegionInfo> serverRegions =\n              assignments.computeIfAbsent(serverName, k -> new ArrayList<>());\n          serverRegions.add(region);\n          cluster.doAssignRegion(region, serverName);\n          serverIdx = (j + serverIdx + 1) % numServers; //remain from next server\n          assigned = true;\n          break;\n        }\n      }\n      if (!assigned) {\n        lastFewRegions.add(region);\n      }\n    }\n    // just sprinkle the rest of the regions on random regionservers. The balanceCluster will\n    // make it optimal later. we can end up with this if numReplicas > numServers.\n    for (RegionInfo region : lastFewRegions) {\n      int i = RANDOM.nextInt(numServers);\n      ServerName server = servers.get(i);\n      List<RegionInfo> serverRegions = assignments.computeIfAbsent(server, k -> new ArrayList<>());\n      serverRegions.add(region);\n      cluster.doAssignRegion(region, server);\n    }\n    return assignments;\n  }",
            "1197  \n1198  \n1199  \n1200  \n1201  \n1202  \n1203  \n1204  \n1205  \n1206  \n1207  \n1208  \n1209  \n1210  \n1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233 +\n1234  \n1235 +\n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  ",
            "  /**\n   * Generates a bulk assignment plan to be used on cluster startup using a\n   * simple round-robin assignment.\n   * <p>\n   * Takes a list of all the regions and all the servers in the cluster and\n   * returns a map of each server to the regions that it should be assigned.\n   * <p>\n   * Currently implemented as a round-robin assignment. Same invariant as load\n   * balancing, all servers holding floor(avg) or ceiling(avg).\n   *\n   * TODO: Use block locations from HDFS to place regions with their blocks\n   *\n   * @param regions all regions\n   * @param servers all servers\n   * @return map of server to the regions it should take, or null if no\n   *         assignment is possible (ie. no regions or no servers)\n   */\n  @Override\n  public Map<ServerName, List<RegionInfo>> roundRobinAssignment(List<RegionInfo> regions,\n      List<ServerName> servers) throws HBaseIOException {\n    metricsBalancer.incrMiscInvocations();\n    Map<ServerName, List<RegionInfo>> assignments = assignMasterSystemRegions(regions, servers);\n    if (assignments != null && !assignments.isEmpty()) {\n      servers = new ArrayList<>(servers);\n      // Guarantee not to put other regions on master\n      servers.remove(masterServerName);\n      List<RegionInfo> masterRegions = assignments.get(masterServerName);\n      if (!masterRegions.isEmpty()) {\n        regions = new ArrayList<>(regions);\n        regions.removeAll(masterRegions);\n      }\n    }\n    if (regions == null || regions.isEmpty()) {\n      return assignments;\n    }\n\n    int numServers = servers == null ? 0 : servers.size();\n    if (numServers == 0) {\n      LOG.warn(\"Wanted to do round robin assignment but no servers to assign to\");\n      return null;\n    }\n\n    // TODO: instead of retainAssignment() and roundRobinAssignment(), we should just run the\n    // normal LB.balancerCluster() with unassignedRegions. We only need to have a candidate\n    // generator for AssignRegionAction. The LB will ensure the regions are mostly local\n    // and balanced. This should also run fast with fewer number of iterations.\n\n    if (numServers == 1) { // Only one server, nothing fancy we can do here\n      ServerName server = servers.get(0);\n      assignments.put(server, new ArrayList<>(regions));\n      return assignments;\n    }\n\n    Cluster cluster = createCluster(servers, regions, false);\n    List<RegionInfo> unassignedRegions = new ArrayList<>();\n\n    roundRobinAssignment(cluster, regions, unassignedRegions,\n      servers, assignments);\n\n    List<RegionInfo> lastFewRegions = new ArrayList<>();\n    // assign the remaining by going through the list and try to assign to servers one-by-one\n    int serverIdx = RANDOM.nextInt(numServers);\n    for (RegionInfo region : unassignedRegions) {\n      boolean assigned = false;\n      for (int j = 0; j < numServers; j++) { // try all servers one by one\n        ServerName serverName = servers.get((j + serverIdx) % numServers);\n        if (!cluster.wouldLowerAvailability(region, serverName)) {\n          List<RegionInfo> serverRegions =\n              assignments.computeIfAbsent(serverName, k -> new ArrayList<>());\n          serverRegions.add(region);\n          cluster.doAssignRegion(region, serverName);\n          serverIdx = (j + serverIdx + 1) % numServers; //remain from next server\n          assigned = true;\n          break;\n        }\n      }\n      if (!assigned) {\n        lastFewRegions.add(region);\n      }\n    }\n    // just sprinkle the rest of the regions on random regionservers. The balanceCluster will\n    // make it optimal later. we can end up with this if numReplicas > numServers.\n    for (RegionInfo region : lastFewRegions) {\n      int i = RANDOM.nextInt(numServers);\n      ServerName server = servers.get(i);\n      List<RegionInfo> serverRegions = assignments.computeIfAbsent(server, k -> new ArrayList<>());\n      serverRegions.add(region);\n      cluster.doAssignRegion(region, server);\n    }\n    return assignments;\n  }"
        ],
        [
            "AssignmentManager::processAssignmentPlans(HashMap,HashMap,List,List)",
            "1785  \n1786 -\n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795 -\n1796  \n1797 -\n1798  \n1799  \n1800 -\n1801  \n1802  \n1803 -\n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812 -\n1813  \n1814  \n1815  \n1816  \n1817 -\n1818  \n1819  \n1820  \n1821  ",
            "  private void processAssignmentPlans(final HashMap<RegionInfo, RegionStateNode> regions,\n      final HashMap<RegionInfo, ServerName> retain, final List<RegionInfo> hris,\n      final List<ServerName> servers) {\n    boolean isTraceEnabled = LOG.isTraceEnabled();\n    if (isTraceEnabled) {\n      LOG.trace(\"Available servers count=\" + servers.size() + \": \" + servers);\n    }\n\n    final LoadBalancer balancer = getBalancer();\n    // ask the balancer where to place regions\n    if (retain != null && !retain.isEmpty()) {\n      if (isTraceEnabled) {\n        LOG.trace(\"Retain assign regions=\" + retain);\n      }\n      try {\n        acceptPlan(regions, balancer.retainAssignment(retain, servers));\n      } catch (HBaseIOException e) {\n        LOG.warn(\"unable to retain assignment\", e);\n        addToPendingAssignment(regions, retain.keySet());\n      }\n    }\n\n    // TODO: Do we need to split retain and round-robin?\n    // the retain seems to fallback to round-robin/random if the region is not in the map.\n    if (!hris.isEmpty()) {\n      Collections.sort(hris, RegionInfo.COMPARATOR);\n      if (isTraceEnabled) {\n        LOG.trace(\"Round-robin regions=\" + hris);\n      }\n      try {\n        acceptPlan(regions, balancer.roundRobinAssignment(hris, servers));\n      } catch (HBaseIOException e) {\n        LOG.warn(\"Unable to round-robin assignment\", e);\n        addToPendingAssignment(regions, hris);\n      }\n    }\n  }",
            "1781  \n1782 +\n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791 +\n1792  \n1793 +\n1794  \n1795  \n1796 +\n1797  \n1798  \n1799 +\n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808 +\n1809  \n1810  \n1811  \n1812  \n1813 +\n1814  \n1815  \n1816  \n1817  ",
            "  private void processAssignmentPlans(final HashMap<RegionInfo, RegionStateNode> regions,\n      final HashMap<RegionInfo, ServerName> retainMap, final List<RegionInfo> hris,\n      final List<ServerName> servers) {\n    boolean isTraceEnabled = LOG.isTraceEnabled();\n    if (isTraceEnabled) {\n      LOG.trace(\"Available servers count=\" + servers.size() + \": \" + servers);\n    }\n\n    final LoadBalancer balancer = getBalancer();\n    // ask the balancer where to place regions\n    if (retainMap != null && !retainMap.isEmpty()) {\n      if (isTraceEnabled) {\n        LOG.trace(\"retain assign regions=\" + retainMap);\n      }\n      try {\n        acceptPlan(regions, balancer.retainAssignment(retainMap, servers));\n      } catch (HBaseIOException e) {\n        LOG.warn(\"unable to retain assignment\", e);\n        addToPendingAssignment(regions, retainMap.keySet());\n      }\n    }\n\n    // TODO: Do we need to split retain and round-robin?\n    // the retain seems to fallback to round-robin/random if the region is not in the map.\n    if (!hris.isEmpty()) {\n      Collections.sort(hris, RegionInfo.COMPARATOR);\n      if (isTraceEnabled) {\n        LOG.trace(\"round robin regions=\" + hris);\n      }\n      try {\n        acceptPlan(regions, balancer.roundRobinAssignment(hris, servers));\n      } catch (HBaseIOException e) {\n        LOG.warn(\"unable to round-robin assignment\", e);\n        addToPendingAssignment(regions, hris);\n      }\n    }\n  }"
        ],
        [
            "TestClientClusterStatus::testLiveAndDeadServersStatus()",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 -\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "  @Test\n  public void testLiveAndDeadServersStatus() throws Exception {\n    // Count the number of live regionservers\n    List<RegionServerThread> regionserverThreads = CLUSTER.getLiveRegionServerThreads();\n    int numRs = 0;\n    int len = regionserverThreads.size();\n    for (int i = 0; i < len; i++) {\n      if (regionserverThreads.get(i).isAlive()) {\n        numRs++;\n      }\n    }\n    // Depending on the (random) order of unit execution we may run this unit before the\n    // minicluster is fully up and recovered from the RS shutdown done during test init.\n    Waiter.waitFor(CLUSTER.getConfiguration(), 10 * 1000, 100, new Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        ClusterStatus status\n          = new ClusterStatus(ADMIN.getClusterMetrics(EnumSet.of(Option.LIVE_SERVERS)));\n        Assert.assertNotNull(status);\n        return status.getRegionsCount() > 0;\n      }\n    });\n    // Retrieve live servers and dead servers info.\n    EnumSet<Option> options = EnumSet.of(Option.LIVE_SERVERS, Option.DEAD_SERVERS);\n    ClusterStatus status = new ClusterStatus(ADMIN.getClusterMetrics(options));\n    checkPbObjectNotNull(status);\n    Assert.assertNotNull(status);\n    Assert.assertNotNull(status.getServers());\n    // exclude a dead region server\n    Assert.assertEquals(SLAVES -1, numRs);\n    // live servers = nums of regionservers\n    // By default, HMaster don't carry any regions so it won't report its load.\n    // Hence, it won't be in the server list.\n    Assert.assertEquals(status.getServers().size(), numRs + 1/*Master*/);\n    Assert.assertTrue(status.getRegionsCount() > 0);\n    Assert.assertNotNull(status.getDeadServerNames());\n    Assert.assertEquals(1, status.getDeadServersSize());\n    ServerName deadServerName = status.getDeadServerNames().iterator().next();\n    Assert.assertEquals(DEAD.getServerName(), deadServerName);\n  }",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "  @Test\n  public void testLiveAndDeadServersStatus() throws Exception {\n    // Count the number of live regionservers\n    List<RegionServerThread> regionserverThreads = CLUSTER.getLiveRegionServerThreads();\n    int numRs = 0;\n    int len = regionserverThreads.size();\n    for (int i = 0; i < len; i++) {\n      if (regionserverThreads.get(i).isAlive()) {\n        numRs++;\n      }\n    }\n    // Depending on the (random) order of unit execution we may run this unit before the\n    // minicluster is fully up and recovered from the RS shutdown done during test init.\n    Waiter.waitFor(CLUSTER.getConfiguration(), 10 * 1000, 100, new Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        ClusterStatus status\n          = new ClusterStatus(ADMIN.getClusterMetrics(EnumSet.of(Option.LIVE_SERVERS)));\n        Assert.assertNotNull(status);\n        return status.getRegionsCount() > 0;\n      }\n    });\n    // Retrieve live servers and dead servers info.\n    EnumSet<Option> options = EnumSet.of(Option.LIVE_SERVERS, Option.DEAD_SERVERS);\n    ClusterStatus status = new ClusterStatus(ADMIN.getClusterMetrics(options));\n    checkPbObjectNotNull(status);\n    Assert.assertNotNull(status);\n    Assert.assertNotNull(status.getServers());\n    // exclude a dead region server\n    Assert.assertEquals(SLAVES -1, numRs);\n    // live servers = nums of regionservers\n    // By default, HMaster don't carry any regions so it won't report its load.\n    // Hence, it won't be in the server list.\n    Assert.assertEquals(status.getServers().size(), numRs);\n    Assert.assertTrue(status.getRegionsCount() > 0);\n    Assert.assertNotNull(status.getDeadServerNames());\n    Assert.assertEquals(1, status.getDeadServersSize());\n    ServerName deadServerName = status.getDeadServerNames().iterator().next();\n    Assert.assertEquals(DEAD.getServerName(), deadServerName);\n  }"
        ],
        [
            "HRegionServer::run()",
            " 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926 -\n 927 -\n 928 -\n 929 -\n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  ",
            "  /**\n   * The HRegionServer sticks in this loop until closed.\n   */\n  @Override\n  public void run() {\n    try {\n      // Do pre-registration initializations; zookeeper, lease threads, etc.\n      preRegistrationInitialization();\n    } catch (Throwable e) {\n      abort(\"Fatal exception during initialization\", e);\n    }\n\n    try {\n      // If we are backup server instance, wait till we become active master before proceeding.\n      waitForMasterActive();\n\n      if (keepLooping()) {\n        ShutdownHook.install(conf, fs, this, Thread.currentThread());\n        // Initialize the RegionServerCoprocessorHost now that our ephemeral\n        // node was created, in case any coprocessors want to use ZooKeeper\n        this.rsHost = new RegionServerCoprocessorHost(this, this.conf);\n      }\n\n      // Try and register with the Master; tell it we are here.  Break if\n      // server is stopped or the clusterup flag is down or hdfs went wacky.\n      // Once registered successfully, go ahead and start up all Services.\n      while (keepLooping()) {\n        RegionServerStartupResponse w = reportForDuty();\n        if (w == null) {\n          LOG.warn(\"reportForDuty failed; sleeping and then retrying.\");\n          this.sleeper.sleep();\n        } else {\n          handleReportForDutyResponse(w);\n          break;\n        }\n      }\n\n      if (!isStopped() && isHealthy()) {\n        // start the snapshot handler and other procedure handlers,\n        // since the server is ready to run\n        if (this.rspmHost != null) {\n          this.rspmHost.start();\n        }\n        // Start the Quota Manager\n        if (this.rsQuotaManager != null) {\n          rsQuotaManager.start(getRpcServer().getScheduler());\n        }\n        if (this.rsSpaceQuotaManager != null) {\n          this.rsSpaceQuotaManager.start();\n        }\n      }\n\n      // We registered with the Master.  Go into run mode.\n      long lastMsg = System.currentTimeMillis();\n      long oldRequestCount = -1;\n      // The main run loop.\n      while (!isStopped() && isHealthy()) {\n        if (!isClusterUp()) {\n          if (isOnlineRegionsEmpty()) {\n            stop(\"Exiting; cluster shutdown set and not carrying any regions\");\n          } else if (!this.stopping) {\n            this.stopping = true;\n            LOG.info(\"Closing user regions\");\n            closeUserRegions(this.abortRequested);\n          } else if (this.stopping) {\n            boolean allUserRegionsOffline = areAllUserRegionsOffline();\n            if (allUserRegionsOffline) {\n              // Set stopped if no more write requests tp meta tables\n              // since last time we went around the loop.  Any open\n              // meta regions will be closed on our way out.\n              if (oldRequestCount == getWriteRequestCount()) {\n                stop(\"Stopped; only catalog regions remaining online\");\n                break;\n              }\n              oldRequestCount = getWriteRequestCount();\n            } else {\n              // Make sure all regions have been closed -- some regions may\n              // have not got it because we were splitting at the time of\n              // the call to closeUserRegions.\n              closeUserRegions(this.abortRequested);\n            }\n            LOG.debug(\"Waiting on \" + getOnlineRegionsAsPrintableString());\n          }\n        }\n        long now = System.currentTimeMillis();\n        if ((now - lastMsg) >= msgInterval) {\n          tryRegionServerReport(lastMsg, now);\n          lastMsg = System.currentTimeMillis();\n        }\n        if (!isStopped() && !isAborted()) {\n          this.sleeper.sleep();\n        }\n      } // for\n    } catch (Throwable t) {\n      if (!rpcServices.checkOOME(t)) {\n        String prefix = t instanceof YouAreDeadException? \"\": \"Unhandled: \";\n        abort(prefix + t.getMessage(), t);\n      }\n    }\n    // Run shutdown.\n    if (mxBean != null) {\n      MBeans.unregister(mxBean);\n      mxBean = null;\n    }\n    if (this.leases != null) this.leases.closeAfterLeasesExpire();\n    if (this.splitLogWorker != null) {\n      splitLogWorker.stop();\n    }\n    if (this.infoServer != null) {\n      LOG.info(\"Stopping infoServer\");\n      try {\n        this.infoServer.stop();\n      } catch (Exception e) {\n        LOG.error(\"Failed to stop infoServer\", e);\n      }\n    }\n    // Send cache a shutdown.\n    if (cacheConfig != null && cacheConfig.isBlockCacheEnabled()) {\n      cacheConfig.getBlockCache().shutdown();\n    }\n    mobCacheConfig.getMobFileCache().shutdown();\n\n    if (movedRegionsCleaner != null) {\n      movedRegionsCleaner.stop(\"Region Server stopping\");\n    }\n\n    // Send interrupts to wake up threads if sleeping so they notice shutdown.\n    // TODO: Should we check they are alive? If OOME could have exited already\n    if (this.hMemManager != null) this.hMemManager.stop();\n    if (this.cacheFlusher != null) this.cacheFlusher.interruptIfNecessary();\n    if (this.compactSplitThread != null) this.compactSplitThread.interruptIfNecessary();\n    if (this.compactionChecker != null) this.compactionChecker.cancel(true);\n    if (this.healthCheckChore != null) this.healthCheckChore.cancel(true);\n    if (this.nonceManagerChore != null) this.nonceManagerChore.cancel(true);\n    if (this.storefileRefresher != null) this.storefileRefresher.cancel(true);\n    sendShutdownInterrupt();\n\n    // Stop the quota manager\n    if (rsQuotaManager != null) {\n      rsQuotaManager.stop();\n    }\n    if (rsSpaceQuotaManager != null) {\n      rsSpaceQuotaManager.stop();\n      rsSpaceQuotaManager = null;\n    }\n\n    // Stop the snapshot and other procedure handlers, forcefully killing all running tasks\n    if (rspmHost != null) {\n      rspmHost.stop(this.abortRequested || this.killed);\n    }\n\n    if (this.killed) {\n      // Just skip out w/o closing regions.  Used when testing.\n    } else if (abortRequested) {\n      if (this.fsOk) {\n        closeUserRegions(abortRequested); // Don't leave any open file handles\n      }\n      LOG.info(\"aborting server \" + this.serverName);\n    } else {\n      closeUserRegions(abortRequested);\n      LOG.info(\"stopping server \" + this.serverName);\n    }\n\n    // so callers waiting for meta without timeout can stop\n    if (this.metaTableLocator != null) this.metaTableLocator.stop();\n    if (this.clusterConnection != null && !clusterConnection.isClosed()) {\n      try {\n        this.clusterConnection.close();\n      } catch (IOException e) {\n        // Although the {@link Closeable} interface throws an {@link\n        // IOException}, in reality, the implementation would never do that.\n        LOG.warn(\"Attempt to close server's short circuit ClusterConnection failed.\", e);\n      }\n    }\n\n    // Closing the compactSplit thread before closing meta regions\n    if (!this.killed && containsMetaTableRegions()) {\n      if (!abortRequested || this.fsOk) {\n        if (this.compactSplitThread != null) {\n          this.compactSplitThread.join();\n          this.compactSplitThread = null;\n        }\n        closeMetaTableRegions(abortRequested);\n      }\n    }\n\n    if (!this.killed && this.fsOk) {\n      waitOnAllRegionsToClose(abortRequested);\n      LOG.info(\"stopping server \" + this.serverName + \"; all regions closed.\");\n    }\n\n    //fsOk flag may be changed when closing regions throws exception.\n    if (this.fsOk) {\n      shutdownWAL(!abortRequested);\n    }\n\n    // Make sure the proxy is down.\n    if (this.rssStub != null) {\n      this.rssStub = null;\n    }\n    if (this.lockStub != null) {\n      this.lockStub = null;\n    }\n    if (this.rpcClient != null) {\n      this.rpcClient.close();\n    }\n    if (this.leases != null) {\n      this.leases.close();\n    }\n    if (this.pauseMonitor != null) {\n      this.pauseMonitor.stop();\n    }\n\n    if (!killed) {\n      stopServiceThreads();\n    }\n\n    if (this.rpcServices != null) {\n      this.rpcServices.stop();\n    }\n\n    try {\n      deleteMyEphemeralNode();\n    } catch (KeeperException.NoNodeException nn) {\n    } catch (KeeperException e) {\n      LOG.warn(\"Failed deleting my ephemeral node\", e);\n    }\n    // We may have failed to delete the znode at the previous step, but\n    //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n    ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n\n    if (this.zooKeeper != null) {\n      this.zooKeeper.close();\n    }\n    LOG.info(\"Exiting; stopping=\" + this.serverName + \"; zookeeper connection closed.\");\n  }",
            " 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933 +\n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  ",
            "  /**\n   * The HRegionServer sticks in this loop until closed.\n   */\n  @Override\n  public void run() {\n    try {\n      // Do pre-registration initializations; zookeeper, lease threads, etc.\n      preRegistrationInitialization();\n    } catch (Throwable e) {\n      abort(\"Fatal exception during initialization\", e);\n    }\n\n    try {\n      if (!isStopped() && !isAborted()) {\n        ShutdownHook.install(conf, fs, this, Thread.currentThread());\n        // Initialize the RegionServerCoprocessorHost now that our ephemeral\n        // node was created, in case any coprocessors want to use ZooKeeper\n        this.rsHost = new RegionServerCoprocessorHost(this, this.conf);\n      }\n\n      // Try and register with the Master; tell it we are here.  Break if\n      // server is stopped or the clusterup flag is down or hdfs went wacky.\n      // Once registered successfully, go ahead and start up all Services.\n      while (keepLooping()) {\n        RegionServerStartupResponse w = reportForDuty();\n        if (w == null) {\n          LOG.warn(\"reportForDuty failed; sleeping and then retrying.\");\n          this.sleeper.sleep();\n        } else {\n          handleReportForDutyResponse(w);\n          break;\n        }\n      }\n\n      if (!isStopped() && isHealthy()) {\n        // start the snapshot handler and other procedure handlers,\n        // since the server is ready to run\n        if (this.rspmHost != null) {\n          this.rspmHost.start();\n        }\n        // Start the Quota Manager\n        if (this.rsQuotaManager != null) {\n          rsQuotaManager.start(getRpcServer().getScheduler());\n        }\n        if (this.rsSpaceQuotaManager != null) {\n          this.rsSpaceQuotaManager.start();\n        }\n      }\n\n      // We registered with the Master.  Go into run mode.\n      long lastMsg = System.currentTimeMillis();\n      long oldRequestCount = -1;\n      // The main run loop.\n      while (!isStopped() && isHealthy()) {\n        if (!isClusterUp()) {\n          if (isOnlineRegionsEmpty()) {\n            stop(\"Exiting; cluster shutdown set and not carrying any regions\");\n          } else if (!this.stopping) {\n            this.stopping = true;\n            LOG.info(\"Closing user regions\");\n            closeUserRegions(this.abortRequested);\n          } else if (this.stopping) {\n            boolean allUserRegionsOffline = areAllUserRegionsOffline();\n            if (allUserRegionsOffline) {\n              // Set stopped if no more write requests tp meta tables\n              // since last time we went around the loop.  Any open\n              // meta regions will be closed on our way out.\n              if (oldRequestCount == getWriteRequestCount()) {\n                stop(\"Stopped; only catalog regions remaining online\");\n                break;\n              }\n              oldRequestCount = getWriteRequestCount();\n            } else {\n              // Make sure all regions have been closed -- some regions may\n              // have not got it because we were splitting at the time of\n              // the call to closeUserRegions.\n              closeUserRegions(this.abortRequested);\n            }\n            LOG.debug(\"Waiting on \" + getOnlineRegionsAsPrintableString());\n          }\n        }\n        long now = System.currentTimeMillis();\n        if ((now - lastMsg) >= msgInterval) {\n          tryRegionServerReport(lastMsg, now);\n          lastMsg = System.currentTimeMillis();\n        }\n        if (!isStopped() && !isAborted()) {\n          this.sleeper.sleep();\n        }\n      } // for\n    } catch (Throwable t) {\n      if (!rpcServices.checkOOME(t)) {\n        String prefix = t instanceof YouAreDeadException? \"\": \"Unhandled: \";\n        abort(prefix + t.getMessage(), t);\n      }\n    }\n    // Run shutdown.\n    if (mxBean != null) {\n      MBeans.unregister(mxBean);\n      mxBean = null;\n    }\n    if (this.leases != null) this.leases.closeAfterLeasesExpire();\n    if (this.splitLogWorker != null) {\n      splitLogWorker.stop();\n    }\n    if (this.infoServer != null) {\n      LOG.info(\"Stopping infoServer\");\n      try {\n        this.infoServer.stop();\n      } catch (Exception e) {\n        LOG.error(\"Failed to stop infoServer\", e);\n      }\n    }\n    // Send cache a shutdown.\n    if (cacheConfig != null && cacheConfig.isBlockCacheEnabled()) {\n      cacheConfig.getBlockCache().shutdown();\n    }\n    mobCacheConfig.getMobFileCache().shutdown();\n\n    if (movedRegionsCleaner != null) {\n      movedRegionsCleaner.stop(\"Region Server stopping\");\n    }\n\n    // Send interrupts to wake up threads if sleeping so they notice shutdown.\n    // TODO: Should we check they are alive? If OOME could have exited already\n    if (this.hMemManager != null) this.hMemManager.stop();\n    if (this.cacheFlusher != null) this.cacheFlusher.interruptIfNecessary();\n    if (this.compactSplitThread != null) this.compactSplitThread.interruptIfNecessary();\n    if (this.compactionChecker != null) this.compactionChecker.cancel(true);\n    if (this.healthCheckChore != null) this.healthCheckChore.cancel(true);\n    if (this.nonceManagerChore != null) this.nonceManagerChore.cancel(true);\n    if (this.storefileRefresher != null) this.storefileRefresher.cancel(true);\n    sendShutdownInterrupt();\n\n    // Stop the quota manager\n    if (rsQuotaManager != null) {\n      rsQuotaManager.stop();\n    }\n    if (rsSpaceQuotaManager != null) {\n      rsSpaceQuotaManager.stop();\n      rsSpaceQuotaManager = null;\n    }\n\n    // Stop the snapshot and other procedure handlers, forcefully killing all running tasks\n    if (rspmHost != null) {\n      rspmHost.stop(this.abortRequested || this.killed);\n    }\n\n    if (this.killed) {\n      // Just skip out w/o closing regions.  Used when testing.\n    } else if (abortRequested) {\n      if (this.fsOk) {\n        closeUserRegions(abortRequested); // Don't leave any open file handles\n      }\n      LOG.info(\"aborting server \" + this.serverName);\n    } else {\n      closeUserRegions(abortRequested);\n      LOG.info(\"stopping server \" + this.serverName);\n    }\n\n    // so callers waiting for meta without timeout can stop\n    if (this.metaTableLocator != null) this.metaTableLocator.stop();\n    if (this.clusterConnection != null && !clusterConnection.isClosed()) {\n      try {\n        this.clusterConnection.close();\n      } catch (IOException e) {\n        // Although the {@link Closeable} interface throws an {@link\n        // IOException}, in reality, the implementation would never do that.\n        LOG.warn(\"Attempt to close server's short circuit ClusterConnection failed.\", e);\n      }\n    }\n\n    // Closing the compactSplit thread before closing meta regions\n    if (!this.killed && containsMetaTableRegions()) {\n      if (!abortRequested || this.fsOk) {\n        if (this.compactSplitThread != null) {\n          this.compactSplitThread.join();\n          this.compactSplitThread = null;\n        }\n        closeMetaTableRegions(abortRequested);\n      }\n    }\n\n    if (!this.killed && this.fsOk) {\n      waitOnAllRegionsToClose(abortRequested);\n      LOG.info(\"stopping server \" + this.serverName + \"; all regions closed.\");\n    }\n\n    //fsOk flag may be changed when closing regions throws exception.\n    if (this.fsOk) {\n      shutdownWAL(!abortRequested);\n    }\n\n    // Make sure the proxy is down.\n    if (this.rssStub != null) {\n      this.rssStub = null;\n    }\n    if (this.lockStub != null) {\n      this.lockStub = null;\n    }\n    if (this.rpcClient != null) {\n      this.rpcClient.close();\n    }\n    if (this.leases != null) {\n      this.leases.close();\n    }\n    if (this.pauseMonitor != null) {\n      this.pauseMonitor.stop();\n    }\n\n    if (!killed) {\n      stopServiceThreads();\n    }\n\n    if (this.rpcServices != null) {\n      this.rpcServices.stop();\n    }\n\n    try {\n      deleteMyEphemeralNode();\n    } catch (KeeperException.NoNodeException nn) {\n    } catch (KeeperException e) {\n      LOG.warn(\"Failed deleting my ephemeral node\", e);\n    }\n    // We may have failed to delete the znode at the previous step, but\n    //  we delete the file anyway: a second attempt to delete the znode is likely to fail again.\n    ZNodeClearer.deleteMyEphemeralNodeOnDisk();\n\n    if (this.zooKeeper != null) {\n      this.zooKeeper.close();\n    }\n    LOG.info(\"Exiting; stopping=\" + this.serverName + \"; zookeeper connection closed.\");\n  }"
        ],
        [
            "SplitLogManager::TaskBatch::toString()",
            " 475  \n 476  \n 477 -\n 478  ",
            "    @Override\n    public String toString() {\n      return (\"installed=\" + installed + \", done=\" + done + \", error=\" + error);\n    }",
            " 477  \n 478  \n 479 +\n 480  ",
            "    @Override\n    public String toString() {\n      return (\"installed = \" + installed + \" done = \" + done + \" error = \" + error);\n    }"
        ],
        [
            "ServerManager::waitForRegionServers(MonitoredTask)",
            " 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887 -\n 888  \n 889  \n 890  ",
            "  /**\n   * Wait for the region servers to report in.\n   * We will wait until one of this condition is met:\n   *  - the master is stopped\n   *  - the 'hbase.master.wait.on.regionservers.maxtostart' number of\n   *    region servers is reached\n   *  - the 'hbase.master.wait.on.regionservers.mintostart' is reached AND\n   *   there have been no new region server in for\n   *      'hbase.master.wait.on.regionservers.interval' time AND\n   *   the 'hbase.master.wait.on.regionservers.timeout' is reached\n   *\n   * @throws InterruptedException\n   */\n  public void waitForRegionServers(MonitoredTask status) throws InterruptedException {\n    final long interval = this.master.getConfiguration().\n        getLong(WAIT_ON_REGIONSERVERS_INTERVAL, 1500);\n    final long timeout = this.master.getConfiguration().\n        getLong(WAIT_ON_REGIONSERVERS_TIMEOUT, 4500);\n    // Min is not an absolute; just a friction making us wait longer on server checkin.\n    int minToStart = getMinToStart();\n    int maxToStart = this.master.getConfiguration().\n        getInt(WAIT_ON_REGIONSERVERS_MAXTOSTART, Integer.MAX_VALUE);\n    if (maxToStart < minToStart) {\n      LOG.warn(String.format(\"The value of '%s' (%d) is set less than '%s' (%d), ignoring.\",\n          WAIT_ON_REGIONSERVERS_MAXTOSTART, maxToStart,\n          WAIT_ON_REGIONSERVERS_MINTOSTART, minToStart));\n      maxToStart = Integer.MAX_VALUE;\n    }\n\n    long now =  System.currentTimeMillis();\n    final long startTime = now;\n    long slept = 0;\n    long lastLogTime = 0;\n    long lastCountChange = startTime;\n    int count = countOfRegionServers();\n    int oldCount = 0;\n    // This while test is a little hard to read. We try to comment it in below but in essence:\n    // Wait if Master is not stopped and the number of regionservers that have checked-in is\n    // less than the maxToStart. Both of these conditions will be true near universally.\n    // Next, we will keep cycling if ANY of the following three conditions are true:\n    // 1. The time since a regionserver registered is < interval (means servers are actively checking in).\n    // 2. We are under the total timeout.\n    // 3. The count of servers is < minimum.\n    for (ServerListener listener: this.listeners) {\n      listener.waiting();\n    }\n    while (!this.master.isStopped() && !isClusterShutdown() && count < maxToStart &&\n        ((lastCountChange + interval) > now || timeout > slept || count < minToStart)) {\n      // Log some info at every interval time or if there is a change\n      if (oldCount != count || lastLogTime + interval < now) {\n        lastLogTime = now;\n        String msg =\n            \"Waiting on regionserver count=\" + count + \"; waited=\"+\n                slept + \"ms, expecting min=\" + minToStart + \" server(s), max=\"+ getStrForMax(maxToStart) +\n                \" server(s), \" + \"timeout=\" + timeout + \"ms, lastChange=\" + (lastCountChange - now) + \"ms\";\n        LOG.info(msg);\n        status.setStatus(msg);\n      }\n\n      // We sleep for some time\n      final long sleepTime = 50;\n      Thread.sleep(sleepTime);\n      now =  System.currentTimeMillis();\n      slept = now - startTime;\n\n      oldCount = count;\n      count = countOfRegionServers();\n      if (count != oldCount) {\n        lastCountChange = now;\n      }\n    }\n    // Did we exit the loop because cluster is going down?\n    if (isClusterShutdown()) {\n      this.master.stop(\"Cluster shutdown\");\n    }\n    LOG.info(\"RegionServer count=\" + count + \"; waited=\" + slept + \"ms,\" +\n        \" expected min=\" + minToStart + \" server(s), max=\" +  getStrForMax(maxToStart) + \" server(s),\"+\n        \" master is \"+ (this.master.isStopped() ? \"stopped.\": \"running\"));\n  }",
            " 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887 +\n 888  \n 889  \n 890  ",
            "  /**\n   * Wait for the region servers to report in.\n   * We will wait until one of this condition is met:\n   *  - the master is stopped\n   *  - the 'hbase.master.wait.on.regionservers.maxtostart' number of\n   *    region servers is reached\n   *  - the 'hbase.master.wait.on.regionservers.mintostart' is reached AND\n   *   there have been no new region server in for\n   *      'hbase.master.wait.on.regionservers.interval' time AND\n   *   the 'hbase.master.wait.on.regionservers.timeout' is reached\n   *\n   * @throws InterruptedException\n   */\n  public void waitForRegionServers(MonitoredTask status) throws InterruptedException {\n    final long interval = this.master.getConfiguration().\n        getLong(WAIT_ON_REGIONSERVERS_INTERVAL, 1500);\n    final long timeout = this.master.getConfiguration().\n        getLong(WAIT_ON_REGIONSERVERS_TIMEOUT, 4500);\n    // Min is not an absolute; just a friction making us wait longer on server checkin.\n    int minToStart = getMinToStart();\n    int maxToStart = this.master.getConfiguration().\n        getInt(WAIT_ON_REGIONSERVERS_MAXTOSTART, Integer.MAX_VALUE);\n    if (maxToStart < minToStart) {\n      LOG.warn(String.format(\"The value of '%s' (%d) is set less than '%s' (%d), ignoring.\",\n          WAIT_ON_REGIONSERVERS_MAXTOSTART, maxToStart,\n          WAIT_ON_REGIONSERVERS_MINTOSTART, minToStart));\n      maxToStart = Integer.MAX_VALUE;\n    }\n\n    long now =  System.currentTimeMillis();\n    final long startTime = now;\n    long slept = 0;\n    long lastLogTime = 0;\n    long lastCountChange = startTime;\n    int count = countOfRegionServers();\n    int oldCount = 0;\n    // This while test is a little hard to read. We try to comment it in below but in essence:\n    // Wait if Master is not stopped and the number of regionservers that have checked-in is\n    // less than the maxToStart. Both of these conditions will be true near universally.\n    // Next, we will keep cycling if ANY of the following three conditions are true:\n    // 1. The time since a regionserver registered is < interval (means servers are actively checking in).\n    // 2. We are under the total timeout.\n    // 3. The count of servers is < minimum.\n    for (ServerListener listener: this.listeners) {\n      listener.waiting();\n    }\n    while (!this.master.isStopped() && !isClusterShutdown() && count < maxToStart &&\n        ((lastCountChange + interval) > now || timeout > slept || count < minToStart)) {\n      // Log some info at every interval time or if there is a change\n      if (oldCount != count || lastLogTime + interval < now) {\n        lastLogTime = now;\n        String msg =\n            \"Waiting on regionserver count=\" + count + \"; waited=\"+\n                slept + \"ms, expecting min=\" + minToStart + \" server(s), max=\"+ getStrForMax(maxToStart) +\n                \" server(s), \" + \"timeout=\" + timeout + \"ms, lastChange=\" + (lastCountChange - now) + \"ms\";\n        LOG.info(msg);\n        status.setStatus(msg);\n      }\n\n      // We sleep for some time\n      final long sleepTime = 50;\n      Thread.sleep(sleepTime);\n      now =  System.currentTimeMillis();\n      slept = now - startTime;\n\n      oldCount = count;\n      count = countOfRegionServers();\n      if (count != oldCount) {\n        lastCountChange = now;\n      }\n    }\n    // Did we exit the loop because cluster is going down?\n    if (isClusterShutdown()) {\n      this.master.stop(\"Cluster shutdown\");\n    }\n    LOG.info(\"Finished waiting on RegionServer count=\" + count + \"; waited=\" + slept + \"ms,\" +\n        \" expected min=\" + minToStart + \" server(s), max=\" +  getStrForMax(maxToStart) + \" server(s),\"+\n        \" master is \"+ (this.master.isStopped() ? \"stopped.\": \"running\"));\n  }"
        ],
        [
            "HRegionServer::initializeZooKeeper()",
            " 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877 -\n 878 -\n 879 -\n 880 -\n 881 -\n 882 -\n 883 -\n 884 -\n 885 -\n 886  \n 887  ",
            "  /**\n   * Bring up connection to zk ensemble and then wait until a master for this cluster and then after\n   * that, wait until cluster 'up' flag has been set. This is the order in which master does things.\n   * <p>\n   * Finally open long-living server short-circuit connection.\n   */\n  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"RV_RETURN_VALUE_IGNORED_BAD_PRACTICE\",\n    justification=\"cluster Id znode read would give us correct response\")\n  private void initializeZooKeeper() throws IOException, InterruptedException {\n    // Nothing to do in here if no Master in the mix.\n    if (this.masterless) {\n      return;\n    }\n\n    // Create the master address tracker, register with zk, and start it.  Then\n    // block until a master is available.  No point in starting up if no master\n    // running.\n    blockAndCheckIfStopped(this.masterAddressTracker);\n\n    // Wait on cluster being up.  Master will set this flag up in zookeeper\n    // when ready.\n    blockAndCheckIfStopped(this.clusterStatusTracker);\n\n    // If we are HMaster then the cluster id should have already been set.\n    if (clusterId == null) {\n      // Retrieve clusterId\n      // Since cluster status is now up\n      // ID should have already been set by HMaster\n      try {\n        clusterId = ZKClusterId.readClusterIdZNode(this.zooKeeper);\n        if (clusterId == null) {\n          this.abort(\"Cluster ID has not been set\");\n        }\n        LOG.info(\"ClusterId : \" + clusterId);\n      } catch (KeeperException e) {\n        this.abort(\"Failed to retrieve Cluster ID\", e);\n      }\n    }\n\n    // Watch for snapshots and other procedures. Check we have not been stopped before proceeding.\n    if (keepLooping()) {\n      try {\n        rspmHost = new RegionServerProcedureManagerHost();\n        rspmHost.loadProcedures(conf);\n        rspmHost.initialize(this);\n      } catch (KeeperException e) {\n        this.abort(\"Failed setup of RegionServerProcedureManager.\", e);\n      }\n    }\n  }",
            " 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877 +\n 878 +\n 879 +\n 880 +\n 881 +\n 882 +\n 883 +\n 884 +\n 885 +\n 886 +\n 887 +\n 888 +\n 889 +\n 890 +\n 891 +\n 892 +\n 893  \n 894  ",
            "  /**\n   * Bring up connection to zk ensemble and then wait until a master for this cluster and then after\n   * that, wait until cluster 'up' flag has been set. This is the order in which master does things.\n   * <p>\n   * Finally open long-living server short-circuit connection.\n   */\n  @edu.umd.cs.findbugs.annotations.SuppressWarnings(value=\"RV_RETURN_VALUE_IGNORED_BAD_PRACTICE\",\n    justification=\"cluster Id znode read would give us correct response\")\n  private void initializeZooKeeper() throws IOException, InterruptedException {\n    // Nothing to do in here if no Master in the mix.\n    if (this.masterless) {\n      return;\n    }\n\n    // Create the master address tracker, register with zk, and start it.  Then\n    // block until a master is available.  No point in starting up if no master\n    // running.\n    blockAndCheckIfStopped(this.masterAddressTracker);\n\n    // Wait on cluster being up.  Master will set this flag up in zookeeper\n    // when ready.\n    blockAndCheckIfStopped(this.clusterStatusTracker);\n\n    // If we are HMaster then the cluster id should have already been set.\n    if (clusterId == null) {\n      // Retrieve clusterId\n      // Since cluster status is now up\n      // ID should have already been set by HMaster\n      try {\n        clusterId = ZKClusterId.readClusterIdZNode(this.zooKeeper);\n        if (clusterId == null) {\n          this.abort(\"Cluster ID has not been set\");\n        }\n        LOG.info(\"ClusterId : \" + clusterId);\n      } catch (KeeperException e) {\n        this.abort(\"Failed to retrieve Cluster ID\", e);\n      }\n    }\n\n    // In case colocated master, wait here till it's active.\n    // So backup masters won't start as regionservers.\n    // This is to avoid showing backup masters as regionservers\n    // in master web UI, or assigning any region to them.\n    waitForMasterActive();\n    if (isStopped() || isAborted()) {\n      return; // No need for further initialization\n    }\n\n    // watch for snapshots and other procedures\n    try {\n      rspmHost = new RegionServerProcedureManagerHost();\n      rspmHost.loadProcedures(conf);\n      rspmHost.initialize(this);\n    } catch (KeeperException e) {\n      this.abort(\"Failed to reach coordination cluster when creating procedure handler.\", e);\n    }\n  }"
        ],
        [
            "HRegionServer::keepLooping()",
            "2566  \n2567  \n2568  \n2569  \n2570 -\n2571  \n2572  ",
            "  /**\n   * @return True if we should break loop because cluster is going down or\n   * this server has been stopped or hdfs has gone bad.\n   */\n  protected boolean keepLooping() {\n    return !this.stopped && isClusterUp();\n  }",
            "2570  \n2571  \n2572  \n2573  \n2574 +\n2575  \n2576  ",
            "  /**\n   * @return True if we should break loop because cluster is going down or\n   * this server has been stopped or hdfs has gone bad.\n   */\n  private boolean keepLooping() {\n    return !this.stopped && isClusterUp();\n  }"
        ],
        [
            "AssignmentManager::acceptPlan(HashMap,Map)",
            "1823  \n1824  \n1825  \n1826  \n1827  \n1828  \n1829 -\n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  \n1847  \n1848  \n1849  \n1850  ",
            "  private void acceptPlan(final HashMap<RegionInfo, RegionStateNode> regions,\n      final Map<ServerName, List<RegionInfo>> plan) throws HBaseIOException {\n    final ProcedureEvent[] events = new ProcedureEvent[regions.size()];\n    final long st = System.currentTimeMillis();\n\n    if (plan == null) {\n      throw new HBaseIOException(\"Unable to compute plans for \" + regions.size() + \" regions\");\n    }\n\n    if (plan.isEmpty()) return;\n\n    int evcount = 0;\n    for (Map.Entry<ServerName, List<RegionInfo>> entry: plan.entrySet()) {\n      final ServerName server = entry.getKey();\n      for (RegionInfo hri: entry.getValue()) {\n        final RegionStateNode regionNode = regions.get(hri);\n        regionNode.setRegionLocation(server);\n        events[evcount++] = regionNode.getProcedureEvent();\n      }\n    }\n    ProcedureEvent.wakeEvents(getProcedureScheduler(), events);\n\n    final long et = System.currentTimeMillis();\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"ASSIGN ACCEPT \" + events.length + \" -> \" +\n          StringUtils.humanTimeDiff(et - st));\n    }\n  }",
            "1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825 +\n1826  \n1827  \n1828  \n1829  \n1830  \n1831  \n1832  \n1833  \n1834  \n1835  \n1836  \n1837  \n1838  \n1839  \n1840  \n1841  \n1842  \n1843  \n1844  \n1845  \n1846  ",
            "  private void acceptPlan(final HashMap<RegionInfo, RegionStateNode> regions,\n      final Map<ServerName, List<RegionInfo>> plan) throws HBaseIOException {\n    final ProcedureEvent[] events = new ProcedureEvent[regions.size()];\n    final long st = System.currentTimeMillis();\n\n    if (plan == null) {\n      throw new HBaseIOException(\"unable to compute plans for regions=\" + regions.size());\n    }\n\n    if (plan.isEmpty()) return;\n\n    int evcount = 0;\n    for (Map.Entry<ServerName, List<RegionInfo>> entry: plan.entrySet()) {\n      final ServerName server = entry.getKey();\n      for (RegionInfo hri: entry.getValue()) {\n        final RegionStateNode regionNode = regions.get(hri);\n        regionNode.setRegionLocation(server);\n        events[evcount++] = regionNode.getProcedureEvent();\n      }\n    }\n    ProcedureEvent.wakeEvents(getProcedureScheduler(), events);\n\n    final long et = System.currentTimeMillis();\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"ASSIGN ACCEPT \" + events.length + \" -> \" +\n          StringUtils.humanTimeDiff(et - st));\n    }\n  }"
        ],
        [
            "TestAssignmentManagerMetrics::startCluster()",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 -\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  ",
            "  @BeforeClass\n  public static void startCluster() throws Exception {\n    LOG.info(\"Starting cluster\");\n    TEST_UTIL = new HBaseTestingUtility();\n    conf = TEST_UTIL.getConfiguration();\n\n    // Disable sanity check for coprocessor\n    conf.setBoolean(\"hbase.table.sanity.checks\", false);\n\n    // set RIT stuck warning threshold to a small value\n    conf.setInt(HConstants.METRICS_RIT_STUCK_WARNING_THRESHOLD, 20);\n\n    // set msgInterval to 1 second\n    conf.setInt(\"hbase.regionserver.msginterval\", msgInterval);\n\n    // Set client sync wait timeout to 5sec\n    conf.setInt(\"hbase.client.sync.wait.timeout.msec\", 2500);\n    conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);\n    conf.setInt(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT, 2500);\n\n    TEST_UTIL.startMiniCluster(1);\n    cluster = TEST_UTIL.getHBaseCluster();\n    master = cluster.getMaster();\n  }",
            "  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 +\n  87 +\n  88 +\n  89 +\n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "  @BeforeClass\n  public static void startCluster() throws Exception {\n    LOG.info(\"Starting cluster\");\n    TEST_UTIL = new HBaseTestingUtility();\n    conf = TEST_UTIL.getConfiguration();\n\n    // Disable sanity check for coprocessor\n    conf.setBoolean(\"hbase.table.sanity.checks\", false);\n\n    // set RIT stuck warning threshold to a small value\n    conf.setInt(HConstants.METRICS_RIT_STUCK_WARNING_THRESHOLD, 20);\n\n    // set msgInterval to 1 second\n    conf.setInt(\"hbase.regionserver.msginterval\", msgInterval);\n\n    // set tablesOnMaster to none\n    conf.set(\"hbase.balancer.tablesOnMaster\", \"none\");\n\n    // set client sync wait timeout to 5sec\n    conf.setInt(\"hbase.client.sync.wait.timeout.msec\", 2500);\n    conf.setInt(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);\n    conf.setInt(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT, 2500);\n\n    TEST_UTIL.startMiniCluster(1);\n    cluster = TEST_UTIL.getHBaseCluster();\n    master = cluster.getMaster();\n  }"
        ],
        [
            "TestClientClusterMetrics::testLiveAndDeadServersStatus()",
            " 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  ",
            "  @Test\n  public void testLiveAndDeadServersStatus() throws Exception {\n    // Count the number of live regionservers\n    List<RegionServerThread> regionserverThreads = CLUSTER.getLiveRegionServerThreads();\n    int numRs = 0;\n    int len = regionserverThreads.size();\n    for (int i = 0; i < len; i++) {\n      if (regionserverThreads.get(i).isAlive()) {\n        numRs++;\n      }\n    }\n    // Depending on the (random) order of unit execution we may run this unit before the\n    // minicluster is fully up and recovered from the RS shutdown done during test init.\n    Waiter.waitFor(CLUSTER.getConfiguration(), 10 * 1000, 100, new Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        ClusterMetrics metrics = ADMIN.getClusterMetrics(EnumSet.of(Option.LIVE_SERVERS));\n        Assert.assertNotNull(metrics);\n        return metrics.getRegionCount() > 0;\n      }\n    });\n    // Retrieve live servers and dead servers info.\n    EnumSet<Option> options = EnumSet.of(Option.LIVE_SERVERS, Option.DEAD_SERVERS);\n    ClusterMetrics metrics = ADMIN.getClusterMetrics(options);\n    Assert.assertNotNull(metrics);\n    // exclude a dead region server\n    Assert.assertEquals(SLAVES -1, numRs);\n    Assert.assertEquals(numRs + 1 /*Master*/, metrics.getLiveServerMetrics().size());\n    Assert.assertTrue(metrics.getRegionCount() > 0);\n    Assert.assertNotNull(metrics.getDeadServerNames());\n    Assert.assertEquals(1, metrics.getDeadServerNames().size());\n    ServerName deadServerName = metrics.getDeadServerNames().iterator().next();\n    Assert.assertEquals(DEAD.getServerName(), deadServerName);\n  }",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 +\n 153 +\n 154 +\n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  \n 161  ",
            "  @Test\n  public void testLiveAndDeadServersStatus() throws Exception {\n    // Count the number of live regionservers\n    List<RegionServerThread> regionserverThreads = CLUSTER.getLiveRegionServerThreads();\n    int numRs = 0;\n    int len = regionserverThreads.size();\n    for (int i = 0; i < len; i++) {\n      if (regionserverThreads.get(i).isAlive()) {\n        numRs++;\n      }\n    }\n    // Depending on the (random) order of unit execution we may run this unit before the\n    // minicluster is fully up and recovered from the RS shutdown done during test init.\n    Waiter.waitFor(CLUSTER.getConfiguration(), 10 * 1000, 100, new Predicate<Exception>() {\n      @Override\n      public boolean evaluate() throws Exception {\n        ClusterMetrics metrics = ADMIN.getClusterMetrics(EnumSet.of(Option.LIVE_SERVERS));\n        Assert.assertNotNull(metrics);\n        return metrics.getRegionCount() > 0;\n      }\n    });\n    // Retrieve live servers and dead servers info.\n    EnumSet<Option> options = EnumSet.of(Option.LIVE_SERVERS, Option.DEAD_SERVERS);\n    ClusterMetrics metrics = ADMIN.getClusterMetrics(options);\n    Assert.assertNotNull(metrics);\n    // exclude a dead region server\n    Assert.assertEquals(SLAVES -1, numRs);\n    // live servers = nums of regionservers\n    // By default, HMaster don't carry any regions so it won't report its load.\n    // Hence, it won't be in the server list.\n    Assert.assertEquals(numRs, metrics.getLiveServerMetrics().size());\n    Assert.assertTrue(metrics.getRegionCount() > 0);\n    Assert.assertNotNull(metrics.getDeadServerNames());\n    Assert.assertEquals(1, metrics.getDeadServerNames().size());\n    ServerName deadServerName = metrics.getDeadServerNames().iterator().next();\n    Assert.assertEquals(DEAD.getServerName(), deadServerName);\n  }"
        ],
        [
            "HRegionServer::stop(String,boolean,User)",
            "2130  \n2131  \n2132  \n2133  \n2134  \n2135  \n2136  \n2137  \n2138 -\n2139  \n2140  \n2141  \n2142  \n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  ",
            "  /**\n   * Stops the regionserver.\n   * @param msg Status message\n   * @param force True if this is a regionserver abort\n   * @param user The user executing the stop request, or null if no user is associated\n   */\n  public void stop(final String msg, final boolean force, final User user) {\n    if (!this.stopped) {\n      LOG.info(\"STOPPING server '\" + this);\n      if (this.rsHost != null) {\n        // when forced via abort don't allow CPs to override\n        try {\n          this.rsHost.preStop(msg, user);\n        } catch (IOException ioe) {\n          if (!force) {\n            LOG.warn(\"The region server did not stop\", ioe);\n            return;\n          }\n          LOG.warn(\"Skipping coprocessor exception on preStop() due to forced shutdown\", ioe);\n        }\n      }\n      this.stopped = true;\n      LOG.info(\"STOPPED: \" + msg);\n      // Wakes run() if it is sleeping\n      sleeper.skipSleepCycle();\n    }\n  }",
            "2134  \n2135  \n2136  \n2137  \n2138  \n2139  \n2140  \n2141  \n2142 +\n2143  \n2144  \n2145  \n2146  \n2147  \n2148  \n2149  \n2150  \n2151  \n2152  \n2153  \n2154  \n2155  \n2156  \n2157  \n2158  \n2159  \n2160  ",
            "  /**\n   * Stops the regionserver.\n   * @param msg Status message\n   * @param force True if this is a regionserver abort\n   * @param user The user executing the stop request, or null if no user is associated\n   */\n  public void stop(final String msg, final boolean force, final User user) {\n    if (!this.stopped) {\n      LOG.info(\"***** STOPPING region server '\" + this + \"' *****\");\n      if (this.rsHost != null) {\n        // when forced via abort don't allow CPs to override\n        try {\n          this.rsHost.preStop(msg, user);\n        } catch (IOException ioe) {\n          if (!force) {\n            LOG.warn(\"The region server did not stop\", ioe);\n            return;\n          }\n          LOG.warn(\"Skipping coprocessor exception on preStop() due to forced shutdown\", ioe);\n        }\n      }\n      this.stopped = true;\n      LOG.info(\"STOPPED: \" + msg);\n      // Wakes run() if it is sleeping\n      sleeper.skipSleepCycle();\n    }\n  }"
        ],
        [
            "RpcExecutor::Handler::run(CallRunner)",
            " 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330 -\n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  ",
            "    private void run(CallRunner cr) {\n      MonitoredRPCHandler status = RpcServer.getStatus();\n      cr.setStatus(status);\n      try {\n        this.activeHandlerCount.incrementAndGet();\n        cr.run();\n      } catch (Throwable e) {\n        if (e instanceof Error) {\n          int failedCount = failedHandlerCount.incrementAndGet();\n          if (this.handlerFailureThreshhold >= 0\n              && failedCount > handlerCount * this.handlerFailureThreshhold) {\n            String message = \"Number of failed RpcServer handler runs exceeded threshold \"\n                + this.handlerFailureThreshhold + \"; reason: \" + StringUtils.stringifyException(e);\n            if (abortable != null) {\n              abortable.abort(message, e);\n            } else {\n              LOG.error(\"Error but can't abort because abortable is null: \"\n                  + StringUtils.stringifyException(e));\n              throw e;\n            }\n          } else {\n            LOG.warn(\"Handler errors \" + StringUtils.stringifyException(e));\n          }\n        } else {\n          LOG.warn(\"Handler  exception \" + StringUtils.stringifyException(e));\n        }\n      } finally {\n        this.activeHandlerCount.decrementAndGet();\n      }\n    }",
            " 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330 +\n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  ",
            "    private void run(CallRunner cr) {\n      MonitoredRPCHandler status = RpcServer.getStatus();\n      cr.setStatus(status);\n      try {\n        this.activeHandlerCount.incrementAndGet();\n        cr.run();\n      } catch (Throwable e) {\n        if (e instanceof Error) {\n          int failedCount = failedHandlerCount.incrementAndGet();\n          if (this.handlerFailureThreshhold >= 0\n              && failedCount > handlerCount * this.handlerFailureThreshhold) {\n            String message = \"Number of failed RpcServer handler runs exceeded threshhold \"\n                + this.handlerFailureThreshhold + \"; reason: \" + StringUtils.stringifyException(e);\n            if (abortable != null) {\n              abortable.abort(message, e);\n            } else {\n              LOG.error(\"Error but can't abort because abortable is null: \"\n                  + StringUtils.stringifyException(e));\n              throw e;\n            }\n          } else {\n            LOG.warn(\"Handler errors \" + StringUtils.stringifyException(e));\n          }\n        } else {\n          LOG.warn(\"Handler  exception \" + StringUtils.stringifyException(e));\n        }\n      } finally {\n        this.activeHandlerCount.decrementAndGet();\n      }\n    }"
        ],
        [
            "ZKUtil::getData(ZKWatcher,String)",
            " 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614 -\n 615 -\n 616  \n 617  \n 618 -\n 619 -\n 620 -\n 621  \n 622  ",
            "  /**\n   * Get znode data. Does not set a watcher.\n   *\n   * @return ZNode data, null if the node does not exist or if there is an error.\n   */\n  public static byte [] getData(ZKWatcher zkw, String znode)\n      throws KeeperException, InterruptedException {\n    try {\n      byte [] data = zkw.getRecoverableZooKeeper().getData(znode, null, null);\n      logRetrievedMsg(zkw, znode, data, false);\n      return data;\n    } catch (KeeperException.NoNodeException e) {\n      LOG.debug(zkw.prefix(\"failed to get data of \" + znode + \" \" +\n          \"; does not exist (not an error)\"));\n      return null;\n    } catch (KeeperException e) {\n      LOG.debug(zkw.prefix(\"failed to get data of \" + znode + \"; \" + e.getMessage()));\n      // Rethrow\n      throw e;\n    }\n  }",
            " 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614 +\n 615 +\n 616  \n 617  \n 618 +\n 619 +\n 620 +\n 621  \n 622  ",
            "  /**\n   * Get znode data. Does not set a watcher.\n   *\n   * @return ZNode data, null if the node does not exist or if there is an error.\n   */\n  public static byte [] getData(ZKWatcher zkw, String znode)\n      throws KeeperException, InterruptedException {\n    try {\n      byte [] data = zkw.getRecoverableZooKeeper().getData(znode, null, null);\n      logRetrievedMsg(zkw, znode, data, false);\n      return data;\n    } catch (KeeperException.NoNodeException e) {\n      LOG.debug(zkw.prefix(\"Unable to get data of znode \" + znode + \" \" +\n          \"because node does not exist (not an error)\"));\n      return null;\n    } catch (KeeperException e) {\n      LOG.warn(zkw.prefix(\"Unable to get data of znode \" + znode), e);\n      zkw.keeperException(e);\n      return null;\n    }\n  }"
        ]
    ],
    "d957f0fa1926c13355c8cca01bbfd7133866e05d": [
        [
            "ReadOnlyZKClient::get(String)",
            " 260  \n 261  \n 262 -\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "  public CompletableFuture<byte[]> get(String path) {\n    if (closed.get()) {\n      return failed(new IOException(\"Client already closed\"));\n    }\n    CompletableFuture<byte[]> future = new CompletableFuture<>();\n    tasks.add(new ZKTask<byte[]>(path, future, \"get\") {\n\n      @Override\n      protected void doExec(ZooKeeper zk) {\n        zk.getData(path, false,\n            (rc, path, ctx, data, stat) -> onComplete(zk, rc, data, true), null);\n      }\n    });\n    return future;\n  }",
            " 261  \n 262  \n 263 +\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "  public CompletableFuture<byte[]> get(String path) {\n    if (closed.get()) {\n      return failed(new DoNotRetryIOException(\"Client already closed\"));\n    }\n    CompletableFuture<byte[]> future = new CompletableFuture<>();\n    tasks.add(new ZKTask<byte[]>(path, future, \"get\") {\n\n      @Override\n      protected void doExec(ZooKeeper zk) {\n        zk.getData(path, false,\n            (rc, path, ctx, data, stat) -> onComplete(zk, rc, data, true), null);\n      }\n    });\n    return future;\n  }"
        ],
        [
            "ReadOnlyZKClient::run()",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342 -\n 343  \n 344  \n 345  ",
            "  private void run() {\n    for (;;) {\n      Task task;\n      try {\n        task = tasks.poll(keepAliveTimeMs, TimeUnit.MILLISECONDS);\n      } catch (InterruptedException e) {\n        continue;\n      }\n      if (task == CLOSE) {\n        break;\n      }\n      if (task == null) {\n        if (pendingRequests == 0) {\n          LOG.trace(\"{} to {} inactive for {}ms; closing (Will reconnect when new requests)\",\n            getId(), connectString, keepAliveTimeMs);\n          closeZk();\n        }\n        continue;\n      }\n      if (!task.needZk()) {\n        task.exec(null);\n      } else {\n        ZooKeeper zk;\n        try {\n          zk = getZk();\n        } catch (IOException e) {\n          task.connectFailed(e);\n          continue;\n        }\n        task.exec(zk);\n      }\n    }\n    closeZk();\n    IOException error = new IOException(\"Client already closed\");\n    Arrays.stream(tasks.toArray(new Task[0])).forEach(t -> t.closed(error));\n    tasks.clear();\n  }",
            " 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343 +\n 344  \n 345  \n 346  ",
            "  private void run() {\n    for (;;) {\n      Task task;\n      try {\n        task = tasks.poll(keepAliveTimeMs, TimeUnit.MILLISECONDS);\n      } catch (InterruptedException e) {\n        continue;\n      }\n      if (task == CLOSE) {\n        break;\n      }\n      if (task == null) {\n        if (pendingRequests == 0) {\n          LOG.trace(\"{} to {} inactive for {}ms; closing (Will reconnect when new requests)\",\n            getId(), connectString, keepAliveTimeMs);\n          closeZk();\n        }\n        continue;\n      }\n      if (!task.needZk()) {\n        task.exec(null);\n      } else {\n        ZooKeeper zk;\n        try {\n          zk = getZk();\n        } catch (IOException e) {\n          task.connectFailed(e);\n          continue;\n        }\n        task.exec(zk);\n      }\n    }\n    closeZk();\n    DoNotRetryIOException error = new DoNotRetryIOException(\"Client already closed\");\n    Arrays.stream(tasks.toArray(new Task[0])).forEach(t -> t.closed(error));\n    tasks.clear();\n  }"
        ],
        [
            "ReadOnlyZKClient::exists(String)",
            " 276  \n 277  \n 278 -\n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  ",
            "  public CompletableFuture<Stat> exists(String path) {\n    if (closed.get()) {\n      return failed(new IOException(\"Client already closed\"));\n    }\n    CompletableFuture<Stat> future = new CompletableFuture<>();\n    tasks.add(new ZKTask<Stat>(path, future, \"exists\") {\n\n      @Override\n      protected void doExec(ZooKeeper zk) {\n        zk.exists(path, false, (rc, path, ctx, stat) -> onComplete(zk, rc, stat, false), null);\n      }\n    });\n    return future;\n  }",
            " 277  \n 278  \n 279 +\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  ",
            "  public CompletableFuture<Stat> exists(String path) {\n    if (closed.get()) {\n      return failed(new DoNotRetryIOException(\"Client already closed\"));\n    }\n    CompletableFuture<Stat> future = new CompletableFuture<>();\n    tasks.add(new ZKTask<Stat>(path, future, \"exists\") {\n\n      @Override\n      protected void doExec(ZooKeeper zk) {\n        zk.exists(path, false, (rc, path, ctx, stat) -> onComplete(zk, rc, stat, false), null);\n      }\n    });\n    return future;\n  }"
        ]
    ],
    "1c477b2df9f3cb10063d66d8f14ba9eb83bf9f4c": [
        [
            "RSRpcServices::doNonAtomicRegionMutation(Region,OperationQuota,RegionAction,CellScanner,RegionActionResult,List,long,RegionScannersCloseCallBack,RpcCallContext)",
            " 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723 -\n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752 -\n 753 -\n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777 -\n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835 -\n 836 -\n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844 -\n 845 -\n 846  \n 847 -\n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  ",
            "  /**\n   * Run through the regionMutation <code>rm</code> and per Mutation, do the work, and then when\n   * done, add an instance of a {@link ResultOrException} that corresponds to each Mutation.\n   * @param region\n   * @param actions\n   * @param cellScanner\n   * @param builder\n   * @param cellsToReturn  Could be null. May be allocated in this method.  This is what this\n   * method returns as a 'result'.\n   * @param closeCallBack the callback to be used with multigets\n   * @param context the current RpcCallContext\n   * @return Return the <code>cellScanner</code> passed\n   */\n  private List<CellScannable> doNonAtomicRegionMutation(final Region region,\n      final OperationQuota quota, final RegionAction actions, final CellScanner cellScanner,\n      final RegionActionResult.Builder builder, List<CellScannable> cellsToReturn, long nonceGroup,\n      final RegionScannersCloseCallBack closeCallBack, RpcCallContext context) {\n    // Gather up CONTIGUOUS Puts and Deletes in this mutations List.  Idea is that rather than do\n    // one at a time, we instead pass them in batch.  Be aware that the corresponding\n    // ResultOrException instance that matches each Put or Delete is then added down in the\n    // doBatchOp call.  We should be staying aligned though the Put and Delete are deferred/batched\n    List<ClientProtos.Action> mutations = null;\n    long maxQuotaResultSize = Math.min(maxScannerResultSize, quota.getReadAvailable());\n    IOException sizeIOE = null;\n    Object lastBlock = null;\n    for (ClientProtos.Action action : actions.getActionList()) {\n      ClientProtos.ResultOrException.Builder resultOrExceptionBuilder = null;\n      try {\n        Result r = null;\n\n        if (context != null\n            && context.isRetryImmediatelySupported()\n            && (context.getResponseCellSize() > maxQuotaResultSize\n              || context.getResponseBlockSize() > maxQuotaResultSize)) {\n\n          // We're storing the exception since the exception and reason string won't\n          // change after the response size limit is reached.\n          if (sizeIOE == null ) {\n            // We don't need the stack un-winding do don't throw the exception.\n            // Throwing will kill the JVM's JIT.\n            //\n            // Instead just create the exception and then store it.\n            sizeIOE = new MultiActionResultTooLarge(\"Max size exceeded\"\n                + \" CellSize: \" + context.getResponseCellSize()\n                + \" BlockSize: \" + context.getResponseBlockSize());\n\n            // Only report the exception once since there's only one request that\n            // caused the exception. Otherwise this number will dominate the exceptions count.\n            rpcServer.getMetrics().exception(sizeIOE);\n          }\n\n          // Now that there's an exception is known to be created\n          // use it for the response.\n          //\n          // This will create a copy in the builder.\n          resultOrExceptionBuilder = ResultOrException.newBuilder().\n              setException(ResponseConverter.buildException(sizeIOE));\n          resultOrExceptionBuilder.setIndex(action.getIndex());\n          builder.addResultOrException(resultOrExceptionBuilder.build());\n          if (cellScanner != null) {\n            skipCellsForMutation(action, cellScanner);\n          }\n          continue;\n        }\n        if (action.hasGet()) {\n          long before = EnvironmentEdgeManager.currentTime();\n          try {\n            Get get = ProtobufUtil.toGet(action.getGet());\n            if (context != null) {\n              r = get(get, ((HRegion) region), closeCallBack, context);\n            } else {\n              r = region.get(get);\n            }\n          } finally {\n            if (regionServer.metricsRegionServer != null) {\n              regionServer.metricsRegionServer.updateGet(\n                EnvironmentEdgeManager.currentTime() - before);\n            }\n          }\n        } else if (action.hasServiceCall()) {\n          resultOrExceptionBuilder = ResultOrException.newBuilder();\n          try {\n            com.google.protobuf.Message result =\n                execServiceOnRegion(region, action.getServiceCall());\n            ClientProtos.CoprocessorServiceResult.Builder serviceResultBuilder =\n                ClientProtos.CoprocessorServiceResult.newBuilder();\n            resultOrExceptionBuilder.setServiceResult(\n                serviceResultBuilder.setValue(\n                  serviceResultBuilder.getValueBuilder()\n                    .setName(result.getClass().getName())\n                    // TODO: Copy!!!\n                    .setValue(UnsafeByteOperations.unsafeWrap(result.toByteArray()))));\n          } catch (IOException ioe) {\n            rpcServer.getMetrics().exception(ioe);\n            resultOrExceptionBuilder.setException(ResponseConverter.buildException(ioe));\n          }\n        } else if (action.hasMutation()) {\n          MutationType type = action.getMutation().getMutateType();\n          if (type != MutationType.PUT && type != MutationType.DELETE && mutations != null &&\n              !mutations.isEmpty()) {\n            // Flush out any Puts or Deletes already collected.\n            doBatchOp(builder, region, quota, mutations, cellScanner);\n            mutations.clear();\n          }\n          switch (type) {\n            case APPEND:\n              r = append(region, quota, action.getMutation(), cellScanner, nonceGroup);\n              break;\n            case INCREMENT:\n              r = increment(region, quota, action.getMutation(), cellScanner, nonceGroup);\n              break;\n            case PUT:\n            case DELETE:\n              // Collect the individual mutations and apply in a batch\n              if (mutations == null) {\n                mutations = new ArrayList<ClientProtos.Action>(actions.getActionCount());\n              }\n              mutations.add(action);\n              break;\n            default:\n              throw new DoNotRetryIOException(\"Unsupported mutate type: \" + type.name());\n          }\n        } else {\n          throw new HBaseIOException(\"Unexpected Action type\");\n        }\n        if (r != null) {\n          ClientProtos.Result pbResult = null;\n          if (isClientCellBlockSupport(context)) {\n            pbResult = ProtobufUtil.toResultNoData(r);\n            //  Hard to guess the size here.  Just make a rough guess.\n            if (cellsToReturn == null) {\n              cellsToReturn = new ArrayList<CellScannable>();\n            }\n            cellsToReturn.add(r);\n          } else {\n            pbResult = ProtobufUtil.toResult(r);\n          }\n          lastBlock = addSize(context, r, lastBlock);\n          resultOrExceptionBuilder =\n            ClientProtos.ResultOrException.newBuilder().setResult(pbResult);\n        }\n        // Could get to here and there was no result and no exception.  Presumes we added\n        // a Put or Delete to the collecting Mutations List for adding later.  In this\n        // case the corresponding ResultOrException instance for the Put or Delete will be added\n        // down in the doBatchOp method call rather than up here.\n      } catch (IOException ie) {\n        rpcServer.getMetrics().exception(ie);\n        resultOrExceptionBuilder = ResultOrException.newBuilder().\n          setException(ResponseConverter.buildException(ie));\n      }\n      if (resultOrExceptionBuilder != null) {\n        // Propagate index.\n        resultOrExceptionBuilder.setIndex(action.getIndex());\n        builder.addResultOrException(resultOrExceptionBuilder.build());\n      }\n    }\n    // Finish up any outstanding mutations\n    if (mutations != null && !mutations.isEmpty()) {\n      doBatchOp(builder, region, quota, mutations, cellScanner);\n    }\n    return cellsToReturn;\n  }",
            " 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722 +\n 723 +\n 724  \n 725 +\n 726 +\n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755 +\n 756 +\n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780 +\n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838 +\n 839 +\n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847 +\n 848 +\n 849  \n 850 +\n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  ",
            "  /**\n   * Run through the regionMutation <code>rm</code> and per Mutation, do the work, and then when\n   * done, add an instance of a {@link ResultOrException} that corresponds to each Mutation.\n   * @param region\n   * @param actions\n   * @param cellScanner\n   * @param builder\n   * @param cellsToReturn  Could be null. May be allocated in this method.  This is what this\n   * method returns as a 'result'.\n   * @param closeCallBack the callback to be used with multigets\n   * @param context the current RpcCallContext\n   * @return Return the <code>cellScanner</code> passed\n   */\n  private List<CellScannable> doNonAtomicRegionMutation(final Region region,\n      final OperationQuota quota, final RegionAction actions, final CellScanner cellScanner,\n      final RegionActionResult.Builder builder, List<CellScannable> cellsToReturn, long nonceGroup,\n      final RegionScannersCloseCallBack closeCallBack, RpcCallContext context) {\n    // Gather up CONTIGUOUS Puts and Deletes in this mutations List.  Idea is that rather than do\n    // one at a time, we instead pass them in batch.  Be aware that the corresponding\n    // ResultOrException instance that matches each Put or Delete is then added down in the\n    // doBatchOp call.  We should be staying aligned though the Put and Delete are deferred/batched\n    List<ClientProtos.Action> mutations = null;\n    long maxQuotaResultSize = Math.min(maxScannerResultSize, quota.getReadAvailable());\n    IOException sizeIOE = null;\n    Object lastBlock = null;\n    ClientProtos.ResultOrException.Builder resultOrExceptionBuilder = ResultOrException.newBuilder();\n    boolean hasResultOrException = false;\n    for (ClientProtos.Action action : actions.getActionList()) {\n      hasResultOrException = false;\n      resultOrExceptionBuilder.clear();\n      try {\n        Result r = null;\n\n        if (context != null\n            && context.isRetryImmediatelySupported()\n            && (context.getResponseCellSize() > maxQuotaResultSize\n              || context.getResponseBlockSize() > maxQuotaResultSize)) {\n\n          // We're storing the exception since the exception and reason string won't\n          // change after the response size limit is reached.\n          if (sizeIOE == null ) {\n            // We don't need the stack un-winding do don't throw the exception.\n            // Throwing will kill the JVM's JIT.\n            //\n            // Instead just create the exception and then store it.\n            sizeIOE = new MultiActionResultTooLarge(\"Max size exceeded\"\n                + \" CellSize: \" + context.getResponseCellSize()\n                + \" BlockSize: \" + context.getResponseBlockSize());\n\n            // Only report the exception once since there's only one request that\n            // caused the exception. Otherwise this number will dominate the exceptions count.\n            rpcServer.getMetrics().exception(sizeIOE);\n          }\n\n          // Now that there's an exception is known to be created\n          // use it for the response.\n          //\n          // This will create a copy in the builder.\n          hasResultOrException = true;\n          resultOrExceptionBuilder.setException(ResponseConverter.buildException(sizeIOE));\n          resultOrExceptionBuilder.setIndex(action.getIndex());\n          builder.addResultOrException(resultOrExceptionBuilder.build());\n          if (cellScanner != null) {\n            skipCellsForMutation(action, cellScanner);\n          }\n          continue;\n        }\n        if (action.hasGet()) {\n          long before = EnvironmentEdgeManager.currentTime();\n          try {\n            Get get = ProtobufUtil.toGet(action.getGet());\n            if (context != null) {\n              r = get(get, ((HRegion) region), closeCallBack, context);\n            } else {\n              r = region.get(get);\n            }\n          } finally {\n            if (regionServer.metricsRegionServer != null) {\n              regionServer.metricsRegionServer.updateGet(\n                EnvironmentEdgeManager.currentTime() - before);\n            }\n          }\n        } else if (action.hasServiceCall()) {\n          hasResultOrException = true;\n          try {\n            com.google.protobuf.Message result =\n                execServiceOnRegion(region, action.getServiceCall());\n            ClientProtos.CoprocessorServiceResult.Builder serviceResultBuilder =\n                ClientProtos.CoprocessorServiceResult.newBuilder();\n            resultOrExceptionBuilder.setServiceResult(\n                serviceResultBuilder.setValue(\n                  serviceResultBuilder.getValueBuilder()\n                    .setName(result.getClass().getName())\n                    // TODO: Copy!!!\n                    .setValue(UnsafeByteOperations.unsafeWrap(result.toByteArray()))));\n          } catch (IOException ioe) {\n            rpcServer.getMetrics().exception(ioe);\n            resultOrExceptionBuilder.setException(ResponseConverter.buildException(ioe));\n          }\n        } else if (action.hasMutation()) {\n          MutationType type = action.getMutation().getMutateType();\n          if (type != MutationType.PUT && type != MutationType.DELETE && mutations != null &&\n              !mutations.isEmpty()) {\n            // Flush out any Puts or Deletes already collected.\n            doBatchOp(builder, region, quota, mutations, cellScanner);\n            mutations.clear();\n          }\n          switch (type) {\n            case APPEND:\n              r = append(region, quota, action.getMutation(), cellScanner, nonceGroup);\n              break;\n            case INCREMENT:\n              r = increment(region, quota, action.getMutation(), cellScanner, nonceGroup);\n              break;\n            case PUT:\n            case DELETE:\n              // Collect the individual mutations and apply in a batch\n              if (mutations == null) {\n                mutations = new ArrayList<ClientProtos.Action>(actions.getActionCount());\n              }\n              mutations.add(action);\n              break;\n            default:\n              throw new DoNotRetryIOException(\"Unsupported mutate type: \" + type.name());\n          }\n        } else {\n          throw new HBaseIOException(\"Unexpected Action type\");\n        }\n        if (r != null) {\n          ClientProtos.Result pbResult = null;\n          if (isClientCellBlockSupport(context)) {\n            pbResult = ProtobufUtil.toResultNoData(r);\n            //  Hard to guess the size here.  Just make a rough guess.\n            if (cellsToReturn == null) {\n              cellsToReturn = new ArrayList<CellScannable>();\n            }\n            cellsToReturn.add(r);\n          } else {\n            pbResult = ProtobufUtil.toResult(r);\n          }\n          lastBlock = addSize(context, r, lastBlock);\n          hasResultOrException = true;\n          resultOrExceptionBuilder.setResult(pbResult);\n        }\n        // Could get to here and there was no result and no exception.  Presumes we added\n        // a Put or Delete to the collecting Mutations List for adding later.  In this\n        // case the corresponding ResultOrException instance for the Put or Delete will be added\n        // down in the doBatchOp method call rather than up here.\n      } catch (IOException ie) {\n        rpcServer.getMetrics().exception(ie);\n        hasResultOrException = true;\n        resultOrExceptionBuilder.setException(ResponseConverter.buildException(ie));\n      }\n      if (hasResultOrException) {\n        // Propagate index.\n        resultOrExceptionBuilder.setIndex(action.getIndex());\n        builder.addResultOrException(resultOrExceptionBuilder.build());\n      }\n    }\n    // Finish up any outstanding mutations\n    if (mutations != null && !mutations.isEmpty()) {\n      doBatchOp(builder, region, quota, mutations, cellScanner);\n    }\n    return cellsToReturn;\n  }"
        ]
    ],
    "104afd74a664d90fe8e3aa57b0722ab04908525c": [
        [
            "NamespaceExistException::NamespaceExistException(String)",
            "  34  \n  35 -\n  36  ",
            "  public NamespaceExistException(String namespace) {\n    super(namespace);\n  }",
            "  34  \n  35 +\n  36  ",
            "  public NamespaceExistException(String namespace) {\n    super(\"Namespace \" + namespace + \" already exists\");\n  }"
        ]
    ],
    "c90e9ff5efe26427cb489e455a29f05e7efd0e1b": [
        [
            "AbstractFSWAL::postSync(long,int)",
            " 991 -\n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  ",
            "  protected final void postSync(final long timeInNanos, final int handlerSyncs) {\n    if (timeInNanos > this.slowSyncNs) {\n      String msg = new StringBuilder().append(\"Slow sync cost: \").append(timeInNanos / 1000000)\n          .append(\" ms, current pipeline: \").append(Arrays.toString(getPipeline())).toString();\n      TraceUtil.addTimelineAnnotation(msg);\n      LOG.info(msg);\n    }\n    if (!listeners.isEmpty()) {\n      for (WALActionsListener listener : listeners) {\n        listener.postSync(timeInNanos, handlerSyncs);\n      }\n    }\n  }",
            "1000 +\n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012 +\n1013 +\n1014 +\n1015 +\n1016 +\n1017  ",
            "  protected final boolean postSync(long timeInNanos, int handlerSyncs) {\n    if (timeInNanos > this.slowSyncNs) {\n      String msg = new StringBuilder().append(\"Slow sync cost: \").append(timeInNanos / 1000000)\n          .append(\" ms, current pipeline: \").append(Arrays.toString(getPipeline())).toString();\n      TraceUtil.addTimelineAnnotation(msg);\n      LOG.info(msg);\n    }\n    if (!listeners.isEmpty()) {\n      for (WALActionsListener listener : listeners) {\n        listener.postSync(timeInNanos, handlerSyncs);\n      }\n    }\n    if (timeInNanos > this.rollOnSyncNs) {\n      LOG.info(\"Trying to request a roll due to a very long sync ({} ms)\", timeInNanos / 1000000);\n      return true;\n    }\n    return false;\n  }"
        ],
        [
            "FSHLog::SyncRunner::run()",
            " 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601 -\n 602  \n 603  \n 604 -\n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  ",
            "    @Override\n    public void run() {\n      long currentSequence;\n      while (!isInterrupted()) {\n        int syncCount = 0;\n\n        try {\n          while (true) {\n            takeSyncFuture = null;\n            // We have to process what we 'take' from the queue\n            takeSyncFuture = this.syncFutures.take();\n            currentSequence = this.sequence;\n            long syncFutureSequence = takeSyncFuture.getTxid();\n            if (syncFutureSequence > currentSequence) {\n              throw new IllegalStateException(\"currentSequence=\" + currentSequence\n                  + \", syncFutureSequence=\" + syncFutureSequence);\n            }\n            // See if we can process any syncfutures BEFORE we go sync.\n            long currentHighestSyncedSequence = highestSyncedTxid.get();\n            if (currentSequence < currentHighestSyncedSequence) {\n              syncCount += releaseSyncFuture(takeSyncFuture, currentHighestSyncedSequence, null);\n              // Done with the 'take'. Go around again and do a new 'take'.\n              continue;\n            }\n            break;\n          }\n          // I got something. Lets run. Save off current sequence number in case it changes\n          // while we run.\n          //TODO handle htrace API change, see HBASE-18895\n          //TraceScope scope = Trace.continueSpan(takeSyncFuture.getSpan());\n          long start = System.nanoTime();\n          Throwable lastException = null;\n          try {\n            TraceUtil.addTimelineAnnotation(\"syncing writer\");\n            writer.sync(useHsync);\n            TraceUtil.addTimelineAnnotation(\"writer synced\");\n            currentSequence = updateHighestSyncedSequence(currentSequence);\n          } catch (IOException e) {\n            LOG.error(\"Error syncing, request close of WAL\", e);\n            lastException = e;\n          } catch (Exception e) {\n            LOG.warn(\"UNEXPECTED\", e);\n            lastException = e;\n          } finally {\n            // reattach the span to the future before releasing.\n            //TODO handle htrace API change, see HBASE-18895\n            // takeSyncFuture.setSpan(scope.getSpan());\n            // First release what we 'took' from the queue.\n            syncCount += releaseSyncFuture(takeSyncFuture, currentSequence, lastException);\n            // Can we release other syncs?\n            syncCount += releaseSyncFutures(currentSequence, lastException);\n            if (lastException != null) {\n              requestLogRoll();\n            } else {\n              checkLogRoll();\n            }\n          }\n          postSync(System.nanoTime() - start, syncCount);\n        } catch (InterruptedException e) {\n          // Presume legit interrupt.\n          Thread.currentThread().interrupt();\n        } catch (Throwable t) {\n          LOG.warn(\"UNEXPECTED, continuing\", t);\n        }\n      }\n    }",
            " 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582 +\n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603 +\n 604  \n 605  \n 606 +\n 607  \n 608  \n 609 +\n 610 +\n 611 +\n 612 +\n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  ",
            "    @Override\n    public void run() {\n      long currentSequence;\n      while (!isInterrupted()) {\n        int syncCount = 0;\n\n        try {\n          while (true) {\n            takeSyncFuture = null;\n            // We have to process what we 'take' from the queue\n            takeSyncFuture = this.syncFutures.take();\n            currentSequence = this.sequence;\n            long syncFutureSequence = takeSyncFuture.getTxid();\n            if (syncFutureSequence > currentSequence) {\n              throw new IllegalStateException(\"currentSequence=\" + currentSequence\n                  + \", syncFutureSequence=\" + syncFutureSequence);\n            }\n            // See if we can process any syncfutures BEFORE we go sync.\n            long currentHighestSyncedSequence = highestSyncedTxid.get();\n            if (currentSequence < currentHighestSyncedSequence) {\n              syncCount += releaseSyncFuture(takeSyncFuture, currentHighestSyncedSequence, null);\n              // Done with the 'take'. Go around again and do a new 'take'.\n              continue;\n            }\n            break;\n          }\n          // I got something. Lets run. Save off current sequence number in case it changes\n          // while we run.\n          //TODO handle htrace API change, see HBASE-18895\n          //TraceScope scope = Trace.continueSpan(takeSyncFuture.getSpan());\n          long start = System.nanoTime();\n          Throwable lastException = null;\n          boolean wasRollRequested = false;\n          try {\n            TraceUtil.addTimelineAnnotation(\"syncing writer\");\n            writer.sync(useHsync);\n            TraceUtil.addTimelineAnnotation(\"writer synced\");\n            currentSequence = updateHighestSyncedSequence(currentSequence);\n          } catch (IOException e) {\n            LOG.error(\"Error syncing, request close of WAL\", e);\n            lastException = e;\n          } catch (Exception e) {\n            LOG.warn(\"UNEXPECTED\", e);\n            lastException = e;\n          } finally {\n            // reattach the span to the future before releasing.\n            //TODO handle htrace API change, see HBASE-18895\n            // takeSyncFuture.setSpan(scope.getSpan());\n            // First release what we 'took' from the queue.\n            syncCount += releaseSyncFuture(takeSyncFuture, currentSequence, lastException);\n            // Can we release other syncs?\n            syncCount += releaseSyncFutures(currentSequence, lastException);\n            if (lastException != null) {\n              wasRollRequested = true;\n              requestLogRoll();\n            } else {\n              wasRollRequested = checkLogRoll();\n            }\n          }\n          boolean doRequestRoll = postSync(System.nanoTime() - start, syncCount);\n          if (!wasRollRequested && doRequestRoll) {\n            requestLogRoll();\n          }\n        } catch (InterruptedException e) {\n          // Presume legit interrupt.\n          Thread.currentThread().interrupt();\n        } catch (Throwable t) {\n          LOG.warn(\"UNEXPECTED, continuing\", t);\n        }\n      }\n    }"
        ],
        [
            "AsyncFSWAL::syncCompleted(AsyncWriter,long,long)",
            " 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331 -\n 332  \n 333  \n 334  \n 335  \n 336  \n 337 -\n 338  \n 339  \n 340  \n 341  \n 342  ",
            "  private void syncCompleted(AsyncWriter writer, long processedTxid, long startTimeNs) {\n    highestSyncedTxid.set(processedTxid);\n    for (Iterator<FSWALEntry> iter = unackedAppends.iterator(); iter.hasNext();) {\n      if (iter.next().getTxid() <= processedTxid) {\n        iter.remove();\n      } else {\n        break;\n      }\n    }\n    postSync(System.nanoTime() - startTimeNs, finishSync(true));\n    if (trySetReadyForRolling()) {\n      // we have just finished a roll, then do not need to check for log rolling, the writer will be\n      // closed soon.\n      return;\n    }\n    if (writer.getLength() < logrollsize || rollRequested) {\n      return;\n    }\n    rollRequested = true;\n    requestLogRoll();\n  }",
            " 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331 +\n 332 +\n 333  \n 334  \n 335  \n 336  \n 337  \n 338 +\n 339  \n 340  \n 341  \n 342  \n 343  ",
            "  private void syncCompleted(AsyncWriter writer, long processedTxid, long startTimeNs) {\n    highestSyncedTxid.set(processedTxid);\n    for (Iterator<FSWALEntry> iter = unackedAppends.iterator(); iter.hasNext();) {\n      if (iter.next().getTxid() <= processedTxid) {\n        iter.remove();\n      } else {\n        break;\n      }\n    }\n\n    boolean doRequestRoll = postSync(System.nanoTime() - startTimeNs, finishSync(true));\n    if (trySetReadyForRolling()) {\n      // we have just finished a roll, then do not need to check for log rolling, the writer will be\n      // closed soon.\n      return;\n    }\n    if ((!doRequestRoll && writer.getLength() < logrollsize) || rollRequested) {\n      return;\n    }\n    rollRequested = true;\n    requestLogRoll();\n  }"
        ],
        [
            "AbstractFSWAL::AbstractFSWAL(FileSystem,Path,String,String,Configuration,List,boolean,String,String)",
            " 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432 -\n 433 -\n 434 -\n 435 -\n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  ",
            "  protected AbstractFSWAL(final FileSystem fs, final Path rootDir, final String logDir,\n      final String archiveDir, final Configuration conf, final List<WALActionsListener> listeners,\n      final boolean failIfWALExists, final String prefix, final String suffix)\n      throws FailedLogCloseException, IOException {\n    this.fs = fs;\n    this.walDir = new Path(rootDir, logDir);\n    this.walArchiveDir = new Path(rootDir, archiveDir);\n    this.conf = conf;\n\n    if (!fs.exists(walDir) && !fs.mkdirs(walDir)) {\n      throw new IOException(\"Unable to mkdir \" + walDir);\n    }\n\n    if (!fs.exists(this.walArchiveDir)) {\n      if (!fs.mkdirs(this.walArchiveDir)) {\n        throw new IOException(\"Unable to mkdir \" + this.walArchiveDir);\n      }\n    }\n\n    // If prefix is null||empty then just name it wal\n    this.walFilePrefix =\n      prefix == null || prefix.isEmpty() ? \"wal\" : URLEncoder.encode(prefix, \"UTF8\");\n    // we only correctly differentiate suffices when numeric ones start with '.'\n    if (suffix != null && !(suffix.isEmpty()) && !(suffix.startsWith(WAL_FILE_NAME_DELIMITER))) {\n      throw new IllegalArgumentException(\"WAL suffix must start with '\" + WAL_FILE_NAME_DELIMITER +\n        \"' but instead was '\" + suffix + \"'\");\n    }\n    // Now that it exists, set the storage policy for the entire directory of wal files related to\n    // this FSHLog instance\n    String storagePolicy =\n        conf.get(HConstants.WAL_STORAGE_POLICY, HConstants.DEFAULT_WAL_STORAGE_POLICY);\n    CommonFSUtils.setStoragePolicy(fs, this.walDir, storagePolicy);\n    this.walFileSuffix = (suffix == null) ? \"\" : URLEncoder.encode(suffix, \"UTF8\");\n    this.prefixPathStr = new Path(walDir, walFilePrefix + WAL_FILE_NAME_DELIMITER).toString();\n\n    this.ourFiles = new PathFilter() {\n      @Override\n      public boolean accept(final Path fileName) {\n        // The path should start with dir/<prefix> and end with our suffix\n        final String fileNameString = fileName.toString();\n        if (!fileNameString.startsWith(prefixPathStr)) {\n          return false;\n        }\n        if (walFileSuffix.isEmpty()) {\n          // in the case of the null suffix, we need to ensure the filename ends with a timestamp.\n          return org.apache.commons.lang3.StringUtils\n              .isNumeric(fileNameString.substring(prefixPathStr.length()));\n        } else if (!fileNameString.endsWith(walFileSuffix)) {\n          return false;\n        }\n        return true;\n      }\n    };\n\n    if (failIfWALExists) {\n      final FileStatus[] walFiles = CommonFSUtils.listStatus(fs, walDir, ourFiles);\n      if (null != walFiles && 0 != walFiles.length) {\n        throw new IOException(\"Target WAL already exists within directory \" + walDir);\n      }\n    }\n\n    // Register listeners. TODO: Should this exist anymore? We have CPs?\n    if (listeners != null) {\n      for (WALActionsListener i : listeners) {\n        registerWALActionsListener(i);\n      }\n    }\n    this.coprocessorHost = new WALCoprocessorHost(this, conf);\n\n    // Schedule a WAL roll when the WAL is 50% of the HDFS block size. Scheduling at 50% of block\n    // size should make it so WAL rolls before we get to the end-of-block (Block transitions cost\n    // some latency). In hbase-1 we did this differently. We scheduled a roll when we hit 95% of\n    // the block size but experience from the field has it that this was not enough time for the\n    // roll to happen before end-of-block. So the new accounting makes WALs of about the same\n    // size as those made in hbase-1 (to prevent surprise), we now have default block size as\n    // 2 times the DFS default: i.e. 2 * DFS default block size rolling at 50% full will generally\n    // make similar size logs to 1 * DFS default block size rolling at 95% full. See HBASE-19148.\n    this.blocksize = WALUtil.getWALBlockSize(this.conf, this.fs, this.walDir);\n    float multiplier = conf.getFloat(\"hbase.regionserver.logroll.multiplier\", 0.5f);\n    this.logrollsize = (long)(this.blocksize * multiplier);\n    this.maxLogs = conf.getInt(\"hbase.regionserver.maxlogs\",\n      Math.max(32, calculateMaxLogFiles(conf, logrollsize)));\n\n    LOG.info(\"WAL configuration: blocksize=\" + StringUtils.byteDesc(blocksize) + \", rollsize=\" +\n      StringUtils.byteDesc(this.logrollsize) + \", prefix=\" + this.walFilePrefix + \", suffix=\" +\n      walFileSuffix + \", logDir=\" + this.walDir + \", archiveDir=\" + this.walArchiveDir);\n    this.slowSyncNs = TimeUnit.MILLISECONDS\n        .toNanos(conf.getInt(\"hbase.regionserver.hlog.slowsync.ms\", DEFAULT_SLOW_SYNC_TIME_MS));\n    this.walSyncTimeoutNs = TimeUnit.MILLISECONDS\n        .toNanos(conf.getLong(\"hbase.regionserver.hlog.sync.timeout\", DEFAULT_WAL_SYNC_TIMEOUT_MS));\n    this.cachedSyncFutures = new ThreadLocal<SyncFuture>() {\n      @Override\n      protected SyncFuture initialValue() {\n        return new SyncFuture();\n      }\n    };\n    this.implClassName = getClass().getSimpleName();\n    this.walTooOldNs = TimeUnit.SECONDS.toNanos(conf.getInt(\n            SURVIVED_TOO_LONG_SEC_KEY, SURVIVED_TOO_LONG_SEC_DEFAULT));\n  }",
            " 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438 +\n 439 +\n 440 +\n 441 +\n 442 +\n 443 +\n 444 +\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  ",
            "  protected AbstractFSWAL(final FileSystem fs, final Path rootDir, final String logDir,\n      final String archiveDir, final Configuration conf, final List<WALActionsListener> listeners,\n      final boolean failIfWALExists, final String prefix, final String suffix)\n      throws FailedLogCloseException, IOException {\n    this.fs = fs;\n    this.walDir = new Path(rootDir, logDir);\n    this.walArchiveDir = new Path(rootDir, archiveDir);\n    this.conf = conf;\n\n    if (!fs.exists(walDir) && !fs.mkdirs(walDir)) {\n      throw new IOException(\"Unable to mkdir \" + walDir);\n    }\n\n    if (!fs.exists(this.walArchiveDir)) {\n      if (!fs.mkdirs(this.walArchiveDir)) {\n        throw new IOException(\"Unable to mkdir \" + this.walArchiveDir);\n      }\n    }\n\n    // If prefix is null||empty then just name it wal\n    this.walFilePrefix =\n      prefix == null || prefix.isEmpty() ? \"wal\" : URLEncoder.encode(prefix, \"UTF8\");\n    // we only correctly differentiate suffices when numeric ones start with '.'\n    if (suffix != null && !(suffix.isEmpty()) && !(suffix.startsWith(WAL_FILE_NAME_DELIMITER))) {\n      throw new IllegalArgumentException(\"WAL suffix must start with '\" + WAL_FILE_NAME_DELIMITER +\n        \"' but instead was '\" + suffix + \"'\");\n    }\n    // Now that it exists, set the storage policy for the entire directory of wal files related to\n    // this FSHLog instance\n    String storagePolicy =\n        conf.get(HConstants.WAL_STORAGE_POLICY, HConstants.DEFAULT_WAL_STORAGE_POLICY);\n    CommonFSUtils.setStoragePolicy(fs, this.walDir, storagePolicy);\n    this.walFileSuffix = (suffix == null) ? \"\" : URLEncoder.encode(suffix, \"UTF8\");\n    this.prefixPathStr = new Path(walDir, walFilePrefix + WAL_FILE_NAME_DELIMITER).toString();\n\n    this.ourFiles = new PathFilter() {\n      @Override\n      public boolean accept(final Path fileName) {\n        // The path should start with dir/<prefix> and end with our suffix\n        final String fileNameString = fileName.toString();\n        if (!fileNameString.startsWith(prefixPathStr)) {\n          return false;\n        }\n        if (walFileSuffix.isEmpty()) {\n          // in the case of the null suffix, we need to ensure the filename ends with a timestamp.\n          return org.apache.commons.lang3.StringUtils\n              .isNumeric(fileNameString.substring(prefixPathStr.length()));\n        } else if (!fileNameString.endsWith(walFileSuffix)) {\n          return false;\n        }\n        return true;\n      }\n    };\n\n    if (failIfWALExists) {\n      final FileStatus[] walFiles = CommonFSUtils.listStatus(fs, walDir, ourFiles);\n      if (null != walFiles && 0 != walFiles.length) {\n        throw new IOException(\"Target WAL already exists within directory \" + walDir);\n      }\n    }\n\n    // Register listeners. TODO: Should this exist anymore? We have CPs?\n    if (listeners != null) {\n      for (WALActionsListener i : listeners) {\n        registerWALActionsListener(i);\n      }\n    }\n    this.coprocessorHost = new WALCoprocessorHost(this, conf);\n\n    // Schedule a WAL roll when the WAL is 50% of the HDFS block size. Scheduling at 50% of block\n    // size should make it so WAL rolls before we get to the end-of-block (Block transitions cost\n    // some latency). In hbase-1 we did this differently. We scheduled a roll when we hit 95% of\n    // the block size but experience from the field has it that this was not enough time for the\n    // roll to happen before end-of-block. So the new accounting makes WALs of about the same\n    // size as those made in hbase-1 (to prevent surprise), we now have default block size as\n    // 2 times the DFS default: i.e. 2 * DFS default block size rolling at 50% full will generally\n    // make similar size logs to 1 * DFS default block size rolling at 95% full. See HBASE-19148.\n    this.blocksize = WALUtil.getWALBlockSize(this.conf, this.fs, this.walDir);\n    float multiplier = conf.getFloat(\"hbase.regionserver.logroll.multiplier\", 0.5f);\n    this.logrollsize = (long)(this.blocksize * multiplier);\n    this.maxLogs = conf.getInt(\"hbase.regionserver.maxlogs\",\n      Math.max(32, calculateMaxLogFiles(conf, logrollsize)));\n\n    LOG.info(\"WAL configuration: blocksize=\" + StringUtils.byteDesc(blocksize) + \", rollsize=\" +\n      StringUtils.byteDesc(this.logrollsize) + \", prefix=\" + this.walFilePrefix + \", suffix=\" +\n      walFileSuffix + \", logDir=\" + this.walDir + \", archiveDir=\" + this.walArchiveDir);\n    this.slowSyncNs = TimeUnit.MILLISECONDS.toNanos(\n      conf.getInt(SLOW_SYNC_TIME_MS, DEFAULT_SLOW_SYNC_TIME_MS));\n    this.rollOnSyncNs = TimeUnit.MILLISECONDS.toNanos(\n      conf.getInt(ROLL_ON_SYNC_TIME_MS, DEFAULT_ROLL_ON_SYNC_TIME_MS));\n    this.walSyncTimeoutNs = TimeUnit.MILLISECONDS.toNanos(\n      conf.getLong(WAL_SYNC_TIMEOUT_MS, DEFAULT_WAL_SYNC_TIMEOUT_MS));\n\n    this.cachedSyncFutures = new ThreadLocal<SyncFuture>() {\n      @Override\n      protected SyncFuture initialValue() {\n        return new SyncFuture();\n      }\n    };\n    this.implClassName = getClass().getSimpleName();\n    this.walTooOldNs = TimeUnit.SECONDS.toNanos(conf.getInt(\n            SURVIVED_TOO_LONG_SEC_KEY, SURVIVED_TOO_LONG_SEC_DEFAULT));\n  }"
        ],
        [
            "FanOutOneBlockAsyncDFSOutput::flushBuffer(CompletableFuture,ByteBuf,long,boolean)",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  ",
            "  private void flushBuffer(CompletableFuture<Long> future, ByteBuf dataBuf,\n      long nextPacketOffsetInBlock, boolean syncBlock) {\n    int dataLen = dataBuf.readableBytes();\n    int chunkLen = summer.getBytesPerChecksum();\n    int trailingPartialChunkLen = dataLen % chunkLen;\n    int numChecks = dataLen / chunkLen + (trailingPartialChunkLen != 0 ? 1 : 0);\n    int checksumLen = numChecks * summer.getChecksumSize();\n    ByteBuf checksumBuf = alloc.directBuffer(checksumLen);\n    summer.calculateChunkedSums(dataBuf.nioBuffer(), checksumBuf.nioBuffer(0, checksumLen));\n    checksumBuf.writerIndex(checksumLen);\n    PacketHeader header = new PacketHeader(4 + checksumLen + dataLen, nextPacketOffsetInBlock,\n        nextPacketSeqno, false, dataLen, syncBlock);\n    int headerLen = header.getSerializedSize();\n    ByteBuf headerBuf = alloc.buffer(headerLen);\n    header.putInBuffer(headerBuf.nioBuffer(0, headerLen));\n    headerBuf.writerIndex(headerLen);\n    Callback c = new Callback(future, nextPacketOffsetInBlock + dataLen, datanodeList);\n    waitingAckQueue.addLast(c);\n    // recheck again after we pushed the callback to queue\n    if (state != State.STREAMING && waitingAckQueue.peekFirst() == c) {\n      future.completeExceptionally(new IOException(\"stream already broken\"));\n      // it's the one we have just pushed or just a no-op\n      waitingAckQueue.removeFirst();\n      return;\n    }\n    datanodeList.forEach(ch -> {\n      ch.write(headerBuf.retainedDuplicate());\n      ch.write(checksumBuf.retainedDuplicate());\n      ch.writeAndFlush(dataBuf.retainedDuplicate());\n    });\n    checksumBuf.release();\n    headerBuf.release();\n    dataBuf.release();\n    nextPacketSeqno++;\n  }",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410 +\n 411 +\n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  ",
            "  private void flushBuffer(CompletableFuture<Long> future, ByteBuf dataBuf,\n      long nextPacketOffsetInBlock, boolean syncBlock) {\n    int dataLen = dataBuf.readableBytes();\n    int chunkLen = summer.getBytesPerChecksum();\n    int trailingPartialChunkLen = dataLen % chunkLen;\n    int numChecks = dataLen / chunkLen + (trailingPartialChunkLen != 0 ? 1 : 0);\n    int checksumLen = numChecks * summer.getChecksumSize();\n    ByteBuf checksumBuf = alloc.directBuffer(checksumLen);\n    summer.calculateChunkedSums(dataBuf.nioBuffer(), checksumBuf.nioBuffer(0, checksumLen));\n    checksumBuf.writerIndex(checksumLen);\n    PacketHeader header = new PacketHeader(4 + checksumLen + dataLen, nextPacketOffsetInBlock,\n        nextPacketSeqno, false, dataLen, syncBlock);\n    int headerLen = header.getSerializedSize();\n    ByteBuf headerBuf = alloc.buffer(headerLen);\n    header.putInBuffer(headerBuf.nioBuffer(0, headerLen));\n    headerBuf.writerIndex(headerLen);\n    Callback c = new Callback(future, nextPacketOffsetInBlock + dataLen, datanodeList);\n    waitingAckQueue.addLast(c);\n    // recheck again after we pushed the callback to queue\n    if (state != State.STREAMING && waitingAckQueue.peekFirst() == c) {\n      future.completeExceptionally(new IOException(\"stream already broken\"));\n      // it's the one we have just pushed or just a no-op\n      waitingAckQueue.removeFirst();\n      return;\n    }\n    // TODO: we should perhaps measure time taken per DN here;\n    //       we could collect statistics per DN, and/or exclude bad nodes in createOutput.\n    datanodeList.forEach(ch -> {\n      ch.write(headerBuf.retainedDuplicate());\n      ch.write(checksumBuf.retainedDuplicate());\n      ch.writeAndFlush(dataBuf.retainedDuplicate());\n    });\n    checksumBuf.release();\n    headerBuf.release();\n    dataBuf.release();\n    nextPacketSeqno++;\n  }"
        ],
        [
            "FSHLog::checkLogRoll()",
            " 615  \n 616  \n 617  \n 618 -\n 619  \n 620  \n 621 -\n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "  /**\n   * Schedule a log roll if needed.\n   */\n  private void checkLogRoll() {\n    // Will return immediately if we are in the middle of a WAL log roll currently.\n    if (!rollWriterLock.tryLock()) {\n      return;\n    }\n    boolean lowReplication;\n    try {\n      lowReplication = doCheckLogLowReplication();\n    } finally {\n      rollWriterLock.unlock();\n    }\n    if (lowReplication || (writer != null && writer.getLength() > logrollsize)) {\n      requestLogRoll(lowReplication);\n    }\n  }",
            " 623  \n 624  \n 625  \n 626 +\n 627  \n 628  \n 629 +\n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639 +\n 640  \n 641 +\n 642  ",
            "  /**\n   * Schedule a log roll if needed.\n   */\n  private boolean checkLogRoll() {\n    // Will return immediately if we are in the middle of a WAL log roll currently.\n    if (!rollWriterLock.tryLock()) {\n      return false;\n    }\n    boolean lowReplication;\n    try {\n      lowReplication = doCheckLogLowReplication();\n    } finally {\n      rollWriterLock.unlock();\n    }\n    if (lowReplication || (writer != null && writer.getLength() > logrollsize)) {\n      requestLogRoll(lowReplication);\n      return true;\n    }\n    return false;\n  }"
        ],
        [
            "FSHLog::preemptiveSync(ProtobufLogWriter)",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260 -\n 261  \n 262  \n 263  \n 264  \n 265  ",
            "  /**\n   * Run a sync after opening to set up the pipeline.\n   */\n  private void preemptiveSync(final ProtobufLogWriter nextWriter) {\n    long startTimeNanos = System.nanoTime();\n    try {\n      nextWriter.sync(useHsync);\n      postSync(System.nanoTime() - startTimeNanos, 0);\n    } catch (IOException e) {\n      // optimization failed, no need to abort here.\n      LOG.warn(\"pre-sync failed but an optimization so keep going\", e);\n    }\n  }",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260 +\n 261 +\n 262 +\n 263 +\n 264  \n 265  \n 266  \n 267  \n 268  ",
            "  /**\n   * Run a sync after opening to set up the pipeline.\n   */\n  private void preemptiveSync(final ProtobufLogWriter nextWriter) {\n    long startTimeNanos = System.nanoTime();\n    try {\n      nextWriter.sync(useHsync);\n      boolean doRequestRoll = postSync(System.nanoTime() - startTimeNanos, 0);\n      if (doRequestRoll) {\n        LOG.info(\"Ignoring a roll request after a sync for a new file\");\n      }\n    } catch (IOException e) {\n      // optimization failed, no need to abort here.\n      LOG.warn(\"pre-sync failed but an optimization so keep going\", e);\n    }\n  }"
        ]
    ],
    "e10d7836ed420754fcf13e3ab7ea1c1c2f1c2098": [
        [
            "TableMapReduceUtil::addHBaseDependencyJars(Configuration)",
            " 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790 -\n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801 -\n 802  ",
            "  /**\n   * Add HBase and its dependencies (only) to the job configuration.\n   * <p>\n   * This is intended as a low-level API, facilitating code reuse between this\n   * class and its mapred counterpart. It also of use to external tools that\n   * need to build a MapReduce job that interacts with HBase but want\n   * fine-grained control over the jars shipped to the cluster.\n   * </p>\n   * @param conf The Configuration object to extend with dependencies.\n   * @see org.apache.hadoop.hbase.mapred.TableMapReduceUtil\n   * @see <a href=\"https://issues.apache.org/jira/browse/PIG-3285\">PIG-3285</a>\n   */\n  public static void addHBaseDependencyJars(Configuration conf) throws IOException {\n\n    // PrefixTreeCodec is part of the hbase-prefix-tree module. If not included in MR jobs jar\n    // dependencies, MR jobs that write encoded hfiles will fail.\n    // We used reflection here so to prevent a circular module dependency.\n    // TODO - if we extract the MR into a module, make it depend on hbase-prefix-tree.\n    Class prefixTreeCodecClass = null;\n    try {\n      prefixTreeCodecClass =\n          Class.forName(\"org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeCodec\");\n    } catch (ClassNotFoundException e) {\n      // this will show up in unit tests but should not show in real deployments\n      LOG.warn(\"The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.\" +\n          \"  Continuing without it.\");\n    }\n\n    addDependencyJarsForClasses(conf,\n      // explicitly pull a class from each module\n      org.apache.hadoop.hbase.HConstants.class,                      // hbase-common\n      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.class, // hbase-protocol\n      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded\n      org.apache.hadoop.hbase.client.Put.class,                      // hbase-client\n      org.apache.hadoop.hbase.ipc.RpcServer.class,                   // hbase-server\n      org.apache.hadoop.hbase.CompatibilityFactory.class,            // hbase-hadoop-compat\n      org.apache.hadoop.hbase.mapreduce.JobUtil.class,               // hbase-hadoop2-compat\n      org.apache.hadoop.hbase.mapreduce.TableMapper.class,           // hbase-server\n      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class,  // hbase-metrics\n      org.apache.hadoop.hbase.metrics.Snapshot.class,                // hbase-metrics-api\n      prefixTreeCodecClass, //  hbase-prefix-tree (if null will be skipped)\n      // pull necessary dependencies\n      org.apache.zookeeper.ZooKeeper.class,\n      org.apache.hadoop.hbase.shaded.io.netty.channel.Channel.class,\n      com.google.protobuf.Message.class,\n      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnsafeByteOperations.class,\n      org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists.class,\n      org.apache.htrace.Trace.class,\n      com.codahale.metrics.MetricRegistry.class);\n  }",
            " 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790 +\n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801 +\n 802 +\n 803  ",
            "  /**\n   * Add HBase and its dependencies (only) to the job configuration.\n   * <p>\n   * This is intended as a low-level API, facilitating code reuse between this\n   * class and its mapred counterpart. It also of use to external tools that\n   * need to build a MapReduce job that interacts with HBase but want\n   * fine-grained control over the jars shipped to the cluster.\n   * </p>\n   * @param conf The Configuration object to extend with dependencies.\n   * @see org.apache.hadoop.hbase.mapred.TableMapReduceUtil\n   * @see <a href=\"https://issues.apache.org/jira/browse/PIG-3285\">PIG-3285</a>\n   */\n  public static void addHBaseDependencyJars(Configuration conf) throws IOException {\n\n    // PrefixTreeCodec is part of the hbase-prefix-tree module. If not included in MR jobs jar\n    // dependencies, MR jobs that write encoded hfiles will fail.\n    // We used reflection here so to prevent a circular module dependency.\n    // TODO - if we extract the MR into a module, make it depend on hbase-prefix-tree.\n    Class prefixTreeCodecClass = null;\n    try {\n      prefixTreeCodecClass =\n          Class.forName(\"org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeCodec\");\n    } catch (ClassNotFoundException e) {\n      // this will show up in unit tests but should not show in real deployments\n      LOG.warn(\"The hbase-prefix-tree module jar containing PrefixTreeCodec is not present.\" +\n          \"  Continuing without it.\");\n    }\n\n    addDependencyJarsForClasses(conf,\n      // explicitly pull a class from each module\n      org.apache.hadoop.hbase.HConstants.class,                      // hbase-common\n      org.apache.hadoop.hbase.protobuf.generated.ClientProtos.class, // hbase-protocol\n      org.apache.hadoop.hbase.shaded.protobuf.generated.ClientProtos.class, // hbase-protocol-shaded\n      org.apache.hadoop.hbase.client.Put.class,                      // hbase-client\n      org.apache.hadoop.hbase.ipc.RpcServer.class,                   // hbase-server\n      org.apache.hadoop.hbase.CompatibilityFactory.class,            // hbase-hadoop-compat\n      org.apache.hadoop.hbase.mapreduce.JobUtil.class,               // hbase-hadoop2-compat\n      org.apache.hadoop.hbase.mapreduce.TableMapper.class,           // hbase-mapreduce\n      org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.class,  // hbase-metrics\n      org.apache.hadoop.hbase.metrics.Snapshot.class,                // hbase-metrics-api\n      prefixTreeCodecClass, //  hbase-prefix-tree (if null will be skipped)\n      // pull necessary dependencies\n      org.apache.zookeeper.ZooKeeper.class,\n      org.apache.hadoop.hbase.shaded.io.netty.channel.Channel.class,\n      com.google.protobuf.Message.class,\n      org.apache.hadoop.hbase.shaded.com.google.protobuf.UnsafeByteOperations.class,\n      org.apache.hadoop.hbase.shaded.com.google.common.collect.Lists.class,\n      org.apache.htrace.Trace.class,\n      com.codahale.metrics.MetricRegistry.class,\n      org.apache.commons.lang3.ArrayUtils.class);\n  }"
        ]
    ],
    "69431c75c16d8d863932815f0460322153a25dbb": [
        [
            "BaseLoadBalancer::retainAssignment(Map,List)",
            "1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460 -\n1461 -\n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  ",
            "  /**\n   * Generates a bulk assignment startup plan, attempting to reuse the existing\n   * assignment information from META, but adjusting for the specified list of\n   * available/online servers available for assignment.\n   * <p>\n   * Takes a map of all regions to their existing assignment from META. Also\n   * takes a list of online servers for regions to be assigned to. Attempts to\n   * retain all assignment, so in some instances initial assignment will not be\n   * completely balanced.\n   * <p>\n   * Any leftover regions without an existing server to be assigned to will be\n   * assigned randomly to available servers.\n   *\n   * @param regions regions and existing assignment from meta\n   * @param servers available servers\n   * @return map of servers and regions to be assigned to them\n   */\n  @Override\n  public Map<ServerName, List<RegionInfo>> retainAssignment(Map<RegionInfo, ServerName> regions,\n      List<ServerName> servers) throws HBaseIOException {\n    // Update metrics\n    metricsBalancer.incrMiscInvocations();\n    Map<ServerName, List<RegionInfo>> assignments = assignMasterSystemRegions(regions.keySet(), servers);\n    if (assignments != null && !assignments.isEmpty()) {\n      servers = new ArrayList<>(servers);\n      // Guarantee not to put other regions on master\n      servers.remove(masterServerName);\n      List<RegionInfo> masterRegions = assignments.get(masterServerName);\n      regions = regions.entrySet().stream().filter(e -> !masterRegions.contains(e.getKey()))\n          .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n    }\n    if (regions.isEmpty()) {\n      return assignments;\n    }\n\n    int numServers = servers == null ? 0 : servers.size();\n    if (numServers == 0) {\n      LOG.warn(\"Wanted to do retain assignment but no servers to assign to\");\n      return null;\n    }\n    if (numServers == 1) { // Only one server, nothing fancy we can do here\n      ServerName server = servers.get(0);\n      assignments.put(server, new ArrayList<>(regions.keySet()));\n      return assignments;\n    }\n\n    // Group all of the old assignments by their hostname.\n    // We can't group directly by ServerName since the servers all have\n    // new start-codes.\n\n    // Group the servers by their hostname. It's possible we have multiple\n    // servers on the same host on different ports.\n    ArrayListMultimap<String, ServerName> serversByHostname = ArrayListMultimap.create();\n    for (ServerName server : servers) {\n      assignments.put(server, new ArrayList<>());\n      serversByHostname.put(server.getHostnameLowerCase(), server);\n    }\n\n    // Collection of the hostnames that used to have regions\n    // assigned, but for which we no longer have any RS running\n    // after the cluster restart.\n    Set<String> oldHostsNoLongerPresent = Sets.newTreeSet();\n\n    // If the old servers aren't present, lets assign those regions later.\n    List<RegionInfo> randomAssignRegions = Lists.newArrayList();\n\n    int numRandomAssignments = 0;\n    int numRetainedAssigments = 0;\n    boolean hasRegionReplica = false;\n    for (Map.Entry<RegionInfo, ServerName> entry : regions.entrySet()) {\n      RegionInfo region = entry.getKey();\n      ServerName oldServerName = entry.getValue();\n      // In the current set of regions even if one has region replica let us go with\n      // getting the entire snapshot\n      if (this.services != null && this.services.getAssignmentManager() != null) { // for tests\n        if (!hasRegionReplica && this.services.getAssignmentManager().getRegionStates()\n            .isReplicaAvailableForRegion(region)) {\n          hasRegionReplica = true;\n        }\n      }\n      List<ServerName> localServers = new ArrayList<>();\n      if (oldServerName != null) {\n        localServers = serversByHostname.get(oldServerName.getHostnameLowerCase());\n      }\n      if (localServers.isEmpty()) {\n        // No servers on the new cluster match up with this hostname, assign randomly, later.\n        randomAssignRegions.add(region);\n        if (oldServerName != null) {\n          oldHostsNoLongerPresent.add(oldServerName.getHostnameLowerCase());\n        }\n      } else if (localServers.size() == 1) {\n        // the usual case - one new server on same host\n        ServerName target = localServers.get(0);\n        assignments.get(target).add(region);\n        numRetainedAssigments++;\n      } else {\n        // multiple new servers in the cluster on this same host\n        if (localServers.contains(oldServerName)) {\n          assignments.get(oldServerName).add(region);\n          numRetainedAssigments++;\n        } else {\n          ServerName target = null;\n          for (ServerName tmp : localServers) {\n            if (tmp.getPort() == oldServerName.getPort()) {\n              target = tmp;\n              assignments.get(tmp).add(region);\n              numRetainedAssigments++;\n              break;\n            }\n          }\n          if (target == null) {\n            randomAssignRegions.add(region);\n          }\n        }\n      }\n    }\n\n    // If servers from prior assignment aren't present, then lets do randomAssignment on regions.\n    if (randomAssignRegions.size() > 0) {\n      Cluster cluster = createCluster(servers, regions.keySet(), hasRegionReplica);\n      for (Map.Entry<ServerName, List<RegionInfo>> entry : assignments.entrySet()) {\n        ServerName sn = entry.getKey();\n        for (RegionInfo region : entry.getValue()) {\n          cluster.doAssignRegion(region, sn);\n        }\n      }\n      for (RegionInfo region : randomAssignRegions) {\n        ServerName target = randomAssignment(cluster, region, servers);\n        assignments.get(target).add(region);\n        numRandomAssignments++;\n      }\n    }\n\n    String randomAssignMsg = \"\";\n    if (numRandomAssignments > 0) {\n      randomAssignMsg =\n          numRandomAssignments + \" regions were assigned \"\n              + \"to random hosts, since the old hosts for these regions are no \"\n              + \"longer present in the cluster. These hosts were:\\n  \"\n              + Joiner.on(\"\\n  \").join(oldHostsNoLongerPresent);\n    }\n\n    LOG.info(\"Reassigned \" + regions.size() + \" regions. \" + numRetainedAssigments\n        + \" retained the pre-restart assignment. \" + randomAssignMsg);\n    return assignments;\n  }",
            "1386  \n1387  \n1388  \n1389  \n1390  \n1391  \n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461 +\n1462 +\n1463 +\n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  \n1476  \n1477  \n1478  \n1479  \n1480  \n1481  \n1482  \n1483  \n1484  \n1485  \n1486  \n1487  \n1488  \n1489  \n1490  \n1491  \n1492  \n1493  \n1494  \n1495  \n1496  \n1497  \n1498  \n1499  \n1500  \n1501  \n1502  \n1503  \n1504  \n1505  \n1506  \n1507  \n1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  ",
            "  /**\n   * Generates a bulk assignment startup plan, attempting to reuse the existing\n   * assignment information from META, but adjusting for the specified list of\n   * available/online servers available for assignment.\n   * <p>\n   * Takes a map of all regions to their existing assignment from META. Also\n   * takes a list of online servers for regions to be assigned to. Attempts to\n   * retain all assignment, so in some instances initial assignment will not be\n   * completely balanced.\n   * <p>\n   * Any leftover regions without an existing server to be assigned to will be\n   * assigned randomly to available servers.\n   *\n   * @param regions regions and existing assignment from meta\n   * @param servers available servers\n   * @return map of servers and regions to be assigned to them\n   */\n  @Override\n  public Map<ServerName, List<RegionInfo>> retainAssignment(Map<RegionInfo, ServerName> regions,\n      List<ServerName> servers) throws HBaseIOException {\n    // Update metrics\n    metricsBalancer.incrMiscInvocations();\n    Map<ServerName, List<RegionInfo>> assignments = assignMasterSystemRegions(regions.keySet(), servers);\n    if (assignments != null && !assignments.isEmpty()) {\n      servers = new ArrayList<>(servers);\n      // Guarantee not to put other regions on master\n      servers.remove(masterServerName);\n      List<RegionInfo> masterRegions = assignments.get(masterServerName);\n      regions = regions.entrySet().stream().filter(e -> !masterRegions.contains(e.getKey()))\n          .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n    }\n    if (regions.isEmpty()) {\n      return assignments;\n    }\n\n    int numServers = servers == null ? 0 : servers.size();\n    if (numServers == 0) {\n      LOG.warn(\"Wanted to do retain assignment but no servers to assign to\");\n      return null;\n    }\n    if (numServers == 1) { // Only one server, nothing fancy we can do here\n      ServerName server = servers.get(0);\n      assignments.put(server, new ArrayList<>(regions.keySet()));\n      return assignments;\n    }\n\n    // Group all of the old assignments by their hostname.\n    // We can't group directly by ServerName since the servers all have\n    // new start-codes.\n\n    // Group the servers by their hostname. It's possible we have multiple\n    // servers on the same host on different ports.\n    ArrayListMultimap<String, ServerName> serversByHostname = ArrayListMultimap.create();\n    for (ServerName server : servers) {\n      assignments.put(server, new ArrayList<>());\n      serversByHostname.put(server.getHostnameLowerCase(), server);\n    }\n\n    // Collection of the hostnames that used to have regions\n    // assigned, but for which we no longer have any RS running\n    // after the cluster restart.\n    Set<String> oldHostsNoLongerPresent = Sets.newTreeSet();\n\n    // If the old servers aren't present, lets assign those regions later.\n    List<RegionInfo> randomAssignRegions = Lists.newArrayList();\n\n    int numRandomAssignments = 0;\n    int numRetainedAssigments = 0;\n    boolean hasRegionReplica = false;\n    for (Map.Entry<RegionInfo, ServerName> entry : regions.entrySet()) {\n      RegionInfo region = entry.getKey();\n      ServerName oldServerName = entry.getValue();\n      // In the current set of regions even if one has region replica let us go with\n      // getting the entire snapshot\n      if (this.services != null && this.services.getAssignmentManager() != null) { // for tests\n        RegionStates states = this.services.getAssignmentManager().getRegionStates();\n        if (!hasRegionReplica && states != null &&\n            states.isReplicaAvailableForRegion(region)) {\n          hasRegionReplica = true;\n        }\n      }\n      List<ServerName> localServers = new ArrayList<>();\n      if (oldServerName != null) {\n        localServers = serversByHostname.get(oldServerName.getHostnameLowerCase());\n      }\n      if (localServers.isEmpty()) {\n        // No servers on the new cluster match up with this hostname, assign randomly, later.\n        randomAssignRegions.add(region);\n        if (oldServerName != null) {\n          oldHostsNoLongerPresent.add(oldServerName.getHostnameLowerCase());\n        }\n      } else if (localServers.size() == 1) {\n        // the usual case - one new server on same host\n        ServerName target = localServers.get(0);\n        assignments.get(target).add(region);\n        numRetainedAssigments++;\n      } else {\n        // multiple new servers in the cluster on this same host\n        if (localServers.contains(oldServerName)) {\n          assignments.get(oldServerName).add(region);\n          numRetainedAssigments++;\n        } else {\n          ServerName target = null;\n          for (ServerName tmp : localServers) {\n            if (tmp.getPort() == oldServerName.getPort()) {\n              target = tmp;\n              assignments.get(tmp).add(region);\n              numRetainedAssigments++;\n              break;\n            }\n          }\n          if (target == null) {\n            randomAssignRegions.add(region);\n          }\n        }\n      }\n    }\n\n    // If servers from prior assignment aren't present, then lets do randomAssignment on regions.\n    if (randomAssignRegions.size() > 0) {\n      Cluster cluster = createCluster(servers, regions.keySet(), hasRegionReplica);\n      for (Map.Entry<ServerName, List<RegionInfo>> entry : assignments.entrySet()) {\n        ServerName sn = entry.getKey();\n        for (RegionInfo region : entry.getValue()) {\n          cluster.doAssignRegion(region, sn);\n        }\n      }\n      for (RegionInfo region : randomAssignRegions) {\n        ServerName target = randomAssignment(cluster, region, servers);\n        assignments.get(target).add(region);\n        numRandomAssignments++;\n      }\n    }\n\n    String randomAssignMsg = \"\";\n    if (numRandomAssignments > 0) {\n      randomAssignMsg =\n          numRandomAssignments + \" regions were assigned \"\n              + \"to random hosts, since the old hosts for these regions are no \"\n              + \"longer present in the cluster. These hosts were:\\n  \"\n              + Joiner.on(\"\\n  \").join(oldHostsNoLongerPresent);\n    }\n\n    LOG.info(\"Reassigned \" + regions.size() + \" regions. \" + numRetainedAssigments\n        + \" retained the pre-restart assignment. \" + randomAssignMsg);\n    return assignments;\n  }"
        ]
    ],
    "4489598a8e4cd920d2fa36d7f84f195d8aa40736": [
        [
            "SnapshotFileCache::refreshCache()",
            " 215 -\n 216 -\n 217 -\n 218 -\n 219 -\n 220 -\n 221 -\n 222 -\n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229  \n 230 -\n 231 -\n 232 -\n 233 -\n 234 -\n 235 -\n 236 -\n 237 -\n 238 -\n 239  \n 240 -\n 241 -\n 242 -\n 243 -\n 244 -\n 245  \n 246  \n 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253 -\n 254 -\n 255 -\n 256 -\n 257 -\n 258 -\n 259 -\n 260 -\n 261 -\n 262 -\n 263 -\n 264 -\n 265 -\n 266 -\n 267 -\n 268 -\n 269 -\n 270 -\n 271 -\n 272  \n 273  \n 274 -\n 275 -\n 276  \n 277 -\n 278  ",
            "  private synchronized void refreshCache() throws IOException {\n    // get the status of the snapshots directory and check if it is has changes\n    FileStatus dirStatus;\n    try {\n      dirStatus = fs.getFileStatus(snapshotDir);\n    } catch (FileNotFoundException e) {\n      if (this.cache.size() > 0) {\n        LOG.error(\"Snapshot directory: \" + snapshotDir + \" doesn't exist\");\n      }\n      return;\n    }\n\n    // if the snapshot directory wasn't modified since we last check, we are done\n    if (dirStatus.getModificationTime() <= this.lastModifiedTime) return;\n\n    // directory was modified, so we need to reload our cache\n    // there could be a slight race here where we miss the cache, check the directory modification\n    // time, then someone updates the directory, causing us to not scan the directory again.\n    // However, snapshot directories are only created once, so this isn't an issue.\n\n    // 1. update the modified time\n    this.lastModifiedTime = dirStatus.getModificationTime();\n\n    // 2.clear the cache\n    this.cache.clear();\n    Map<String, SnapshotDirectoryInfo> known = new HashMap<>();\n\n    // 3. check each of the snapshot directories\n    FileStatus[] snapshots = FSUtils.listStatus(fs, snapshotDir);\n    if (snapshots == null) {\n      // remove all the remembered snapshots because we don't have any left\n      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {\n        LOG.debug(\"No snapshots on-disk, cache empty\");\n      }\n      this.snapshots.clear();\n      return;\n    }\n\n    // 3.1 iterate through the on-disk snapshots\n    for (FileStatus snapshot : snapshots) {\n      String name = snapshot.getPath().getName();\n      // its not the tmp dir,\n      if (!name.equals(SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME)) {\n        SnapshotDirectoryInfo files = this.snapshots.remove(name);\n        // 3.1.1 if we don't know about the snapshot or its been modified, we need to update the\n        // files the latter could occur where I create a snapshot, then delete it, and then make a\n        // new snapshot with the same name. We will need to update the cache the information from\n        // that new snapshot, even though it has the same name as the files referenced have\n        // probably changed.\n        if (files == null || files.hasBeenModified(snapshot.getModificationTime())) {\n          // get all files for the snapshot and create a new info\n          Collection<String> storedFiles = fileInspector.filesUnderSnapshot(snapshot.getPath());\n          files = new SnapshotDirectoryInfo(snapshot.getModificationTime(), storedFiles);\n        }\n        // 3.2 add all the files to cache\n        this.cache.addAll(files.getFiles());\n        known.put(name, files);\n      }\n    }\n\n    // 4. set the snapshots we are tracking\n    this.snapshots.clear();\n    this.snapshots.putAll(known);\n  }",
            " 211 +\n 212 +\n 213 +\n 214 +\n 215 +\n 216 +\n 217  \n 218 +\n 219 +\n 220  \n 221 +\n 222  \n 223  \n 224 +\n 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238 +\n 239 +\n 240 +\n 241 +\n 242 +\n 243 +\n 244  \n 245 +\n 246 +\n 247 +\n 248  \n 249 +\n 250  \n 251 +\n 252  ",
            "  private void refreshCache() throws IOException {\n    // just list the snapshot directory directly, do not check the modification time for the root\n    // snapshot directory, as some file system implementations do not modify the parent directory's\n    // modTime when there are new sub items, for example, S3.\n    FileStatus[] snapshotDirs = FSUtils.listStatus(fs, snapshotDir,\n      p -> !p.getName().equals(SnapshotDescriptionUtils.SNAPSHOT_TMP_DIR_NAME));\n\n    // clear the cache, as in the below code, either we will also clear the snapshots, or we will\n    // refill the file name cache again.\n    this.cache.clear();\n    if (ArrayUtils.isEmpty(snapshotDirs)) {\n      // remove all the remembered snapshots because we don't have any left\n      if (LOG.isDebugEnabled() && this.snapshots.size() > 0) {\n        LOG.debug(\"No snapshots on-disk, clear cache\");\n      }\n      this.snapshots.clear();\n      return;\n    }\n\n    // iterate over all the cached snapshots and see if we need to update some, it is not an\n    // expensive operation if we do not reload the manifest of snapshots.\n    Map<String, SnapshotDirectoryInfo> newSnapshots = new HashMap<>();\n    for (FileStatus snapshotDir : snapshotDirs) {\n      String name = snapshotDir.getPath().getName();\n      SnapshotDirectoryInfo files = this.snapshots.remove(name);\n      // if we don't know about the snapshot or its been modified, we need to update the\n      // files the latter could occur where I create a snapshot, then delete it, and then make a\n      // new snapshot with the same name. We will need to update the cache the information from\n      // that new snapshot, even though it has the same name as the files referenced have\n      // probably changed.\n      if (files == null || files.hasBeenModified(snapshotDir.getModificationTime())) {\n        Collection<String> storedFiles = fileInspector.filesUnderSnapshot(snapshotDir.getPath());\n        files = new SnapshotDirectoryInfo(snapshotDir.getModificationTime(), storedFiles);\n      }\n      // add all the files to cache\n      this.cache.addAll(files.getFiles());\n      newSnapshots.put(name, files);\n    }\n    // set the snapshots we are tracking\n    this.snapshots.clear();\n    this.snapshots.putAll(newSnapshots);\n  }"
        ],
        [
            "SnapshotFileCache::SnapshotFileCache(Configuration,long,String,SnapshotFileInspector)",
            " 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117 -\n 118  ",
            "  /**\n   * Create a snapshot file cache for all snapshots under the specified [root]/.snapshot on the\n   * filesystem.\n   * <p>\n   * Immediately loads the file cache.\n   * @param conf to extract the configured {@link FileSystem} where the snapshots are stored and\n   *          hbase root directory\n   * @param cacheRefreshPeriod frequency (ms) with which the cache should be refreshed\n   * @param refreshThreadName name of the cache refresh thread\n   * @param inspectSnapshotFiles Filter to apply to each snapshot to extract the files.\n   * @throws IOException if the {@link FileSystem} or root directory cannot be loaded\n   */\n  public SnapshotFileCache(Configuration conf, long cacheRefreshPeriod, String refreshThreadName,\n      SnapshotFileInspector inspectSnapshotFiles) throws IOException {\n    this(FSUtils.getCurrentFileSystem(conf), FSUtils.getRootDir(conf), 0, cacheRefreshPeriod,\n        refreshThreadName, inspectSnapshotFiles);\n  }",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 +\n 115  ",
            "  /**\n   * Create a snapshot file cache for all snapshots under the specified [root]/.snapshot on the\n   * filesystem.\n   * <p>\n   * Immediately loads the file cache.\n   * @param conf to extract the configured {@link FileSystem} where the snapshots are stored and\n   *          hbase root directory\n   * @param cacheRefreshPeriod frequency (ms) with which the cache should be refreshed\n   * @param refreshThreadName name of the cache refresh thread\n   * @param inspectSnapshotFiles Filter to apply to each snapshot to extract the files.\n   * @throws IOException if the {@link FileSystem} or root directory cannot be loaded\n   */\n  public SnapshotFileCache(Configuration conf, long cacheRefreshPeriod, String refreshThreadName,\n      SnapshotFileInspector inspectSnapshotFiles) throws IOException {\n    this(FSUtils.getCurrentFileSystem(conf), FSUtils.getRootDir(conf), 0, cacheRefreshPeriod,\n      refreshThreadName, inspectSnapshotFiles);\n  }"
        ],
        [
            "SnapshotFileCache::getUnreferencedFiles(Iterable,SnapshotManager)",
            " 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 -\n 181 -\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192 -\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "   * is refreshed and the cache checked again for that file.\n   * This ensures that we never return files that exist.\n   * <p>\n   * Note this may lead to periodic false positives for the file being referenced. Periodically, the\n   * cache is refreshed even if there are no requests to ensure that the false negatives get removed\n   * eventually. For instance, suppose you have a file in the snapshot and it gets loaded into the\n   * cache. Then at some point later that snapshot is deleted. If the cache has not been refreshed\n   * at that point, cache will still think the file system contains that file and return\n   * <tt>true</tt>, even if it is no longer present (false positive). However, if the file never was\n   * on the filesystem, we will never find it and always return <tt>false</tt>.\n   * @param files file to check, NOTE: Relies that files are loaded from hdfs before method\n   *              is called (NOT LAZY)\n   * @return <tt>unReferencedFiles</tt> the collection of files that do not have snapshot references\n   * @throws IOException if there is an unexpected error reaching the filesystem.\n   */\n  // XXX this is inefficient to synchronize on the method, when what we really need to guard against\n  // is an illegal access to the cache. Really we could do a mutex-guarded pointer swap on the\n  // cache, but that seems overkill at the moment and isn't necessarily a bottleneck.\n  public synchronized Iterable<FileStatus> getUnreferencedFiles(Iterable<FileStatus> files,\n      final SnapshotManager snapshotManager)\n      throws IOException {\n    List<FileStatus> unReferencedFiles = Lists.newArrayList();\n    boolean refreshed = false;\n    Lock lock = null;\n    if (snapshotManager != null) {\n      lock = snapshotManager.getTakingSnapshotLock().writeLock();\n    }\n    if (lock == null || lock.tryLock()) {\n      try {\n        if (snapshotManager != null && snapshotManager.isTakingAnySnapshot()) {\n          LOG.warn(\"Not checking unreferenced files since snapshot is running, it will \"\n              + \"skip to clean the HFiles this time\");\n          return unReferencedFiles;\n        }\n        for (FileStatus file : files) {\n          String fileName = file.getPath().getName();\n          if (!refreshed && !cache.contains(fileName)) {\n            refreshCache();\n            refreshed = true;\n          }\n          if (cache.contains(fileName)) {\n            continue;\n          }\n          unReferencedFiles.add(file);\n        }\n      } finally {\n        if (lock != null) {\n          lock.unlock();\n        }\n      }\n    }\n    return unReferencedFiles;\n  }",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177 +\n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 +\n 188 +\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  ",
            "   * and the cache checked again for that file. This ensures that we never return files that exist.\n   * <p>\n   * Note this may lead to periodic false positives for the file being referenced. Periodically, the\n   * cache is refreshed even if there are no requests to ensure that the false negatives get removed\n   * eventually. For instance, suppose you have a file in the snapshot and it gets loaded into the\n   * cache. Then at some point later that snapshot is deleted. If the cache has not been refreshed\n   * at that point, cache will still think the file system contains that file and return\n   * <tt>true</tt>, even if it is no longer present (false positive). However, if the file never was\n   * on the filesystem, we will never find it and always return <tt>false</tt>.\n   * @param files file to check, NOTE: Relies that files are loaded from hdfs before method is\n   *          called (NOT LAZY)\n   * @return <tt>unReferencedFiles</tt> the collection of files that do not have snapshot references\n   * @throws IOException if there is an unexpected error reaching the filesystem.\n   */\n  // XXX this is inefficient to synchronize on the method, when what we really need to guard against\n  // is an illegal access to the cache. Really we could do a mutex-guarded pointer swap on the\n  // cache, but that seems overkill at the moment and isn't necessarily a bottleneck.\n  public synchronized Iterable<FileStatus> getUnreferencedFiles(Iterable<FileStatus> files,\n      final SnapshotManager snapshotManager) throws IOException {\n    List<FileStatus> unReferencedFiles = Lists.newArrayList();\n    boolean refreshed = false;\n    Lock lock = null;\n    if (snapshotManager != null) {\n      lock = snapshotManager.getTakingSnapshotLock().writeLock();\n    }\n    if (lock == null || lock.tryLock()) {\n      try {\n        if (snapshotManager != null && snapshotManager.isTakingAnySnapshot()) {\n          LOG.warn(\"Not checking unreferenced files since snapshot is running, it will \" +\n            \"skip to clean the HFiles this time\");\n          return unReferencedFiles;\n        }\n        for (FileStatus file : files) {\n          String fileName = file.getPath().getName();\n          if (!refreshed && !cache.contains(fileName)) {\n            refreshCache();\n            refreshed = true;\n          }\n          if (cache.contains(fileName)) {\n            continue;\n          }\n          unReferencedFiles.add(file);\n        }\n      } finally {\n        if (lock != null) {\n          lock.unlock();\n        }\n      }\n    }\n    return unReferencedFiles;\n  }"
        ],
        [
            "SnapshotFileCache::SnapshotFileCache(FileSystem,Path,long,long,String,SnapshotFileInspector)",
            " 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 -\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  ",
            "  /**\n   * Create a snapshot file cache for all snapshots under the specified [root]/.snapshot on the\n   * filesystem\n   * @param fs {@link FileSystem} where the snapshots are stored\n   * @param rootDir hbase root directory\n   * @param cacheRefreshPeriod period (ms) with which the cache should be refreshed\n   * @param cacheRefreshDelay amount of time to wait for the cache to be refreshed\n   * @param refreshThreadName name of the cache refresh thread\n   * @param inspectSnapshotFiles Filter to apply to each snapshot to extract the files.\n   */\n  public SnapshotFileCache(FileSystem fs, Path rootDir, long cacheRefreshPeriod,\n      long cacheRefreshDelay, String refreshThreadName, SnapshotFileInspector inspectSnapshotFiles) {\n    this.fs = fs;\n    this.fileInspector = inspectSnapshotFiles;\n    this.snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);\n    // periodically refresh the file cache to make sure we aren't superfluously saving files.\n    this.refreshTimer = new Timer(refreshThreadName, true);\n    this.refreshTimer.scheduleAtFixedRate(new RefreshCacheTask(), cacheRefreshDelay,\n      cacheRefreshPeriod);\n  }",
            " 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "  /**\n   * Create a snapshot file cache for all snapshots under the specified [root]/.snapshot on the\n   * filesystem\n   * @param fs {@link FileSystem} where the snapshots are stored\n   * @param rootDir hbase root directory\n   * @param cacheRefreshPeriod period (ms) with which the cache should be refreshed\n   * @param cacheRefreshDelay amount of time to wait for the cache to be refreshed\n   * @param refreshThreadName name of the cache refresh thread\n   * @param inspectSnapshotFiles Filter to apply to each snapshot to extract the files.\n   */\n  public SnapshotFileCache(FileSystem fs, Path rootDir, long cacheRefreshPeriod,\n      long cacheRefreshDelay, String refreshThreadName,\n      SnapshotFileInspector inspectSnapshotFiles) {\n    this.fs = fs;\n    this.fileInspector = inspectSnapshotFiles;\n    this.snapshotDir = SnapshotDescriptionUtils.getSnapshotsDir(rootDir);\n    // periodically refresh the file cache to make sure we aren't superfluously saving files.\n    this.refreshTimer = new Timer(refreshThreadName, true);\n    this.refreshTimer.scheduleAtFixedRate(new RefreshCacheTask(), cacheRefreshDelay,\n      cacheRefreshPeriod);\n  }"
        ],
        [
            "SnapshotFileCache::RefreshCacheTask::run()",
            " 284  \n 285  \n 286 -\n 287 -\n 288 -\n 289 -\n 290  \n 291  ",
            "    @Override\n    public void run() {\n      try {\n        SnapshotFileCache.this.refreshCache();\n      } catch (IOException e) {\n        LOG.warn(\"Failed to refresh snapshot hfile cache!\", e);\n      }\n    }",
            " 258  \n 259  \n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269  \n 270 +\n 271  ",
            "    @Override\n    public void run() {\n      synchronized (SnapshotFileCache.this) {\n        try {\n          SnapshotFileCache.this.refreshCache();\n        } catch (IOException e) {\n          LOG.warn(\"Failed to refresh snapshot hfile cache!\", e);\n          // clear all the cached entries if we meet an error\n          cache.clear();\n          snapshots.clear();\n        }\n      }\n\n    }"
        ],
        [
            "SnapshotFileCache::triggerCacheRefreshForTesting()",
            " 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 -\n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  ",
            "  /**\n   * Trigger a cache refresh, even if its before the next cache refresh. Does not affect pending\n   * cache refreshes.\n   * <p>\n   * Blocks until the cache is refreshed.\n   * <p>\n   * Exposed for TESTING.\n   */\n  public void triggerCacheRefreshForTesting() {\n    try {\n      SnapshotFileCache.this.refreshCache();\n    } catch (IOException e) {\n      LOG.warn(\"Failed to refresh snapshot hfile cache!\", e);\n    }\n    LOG.debug(\"Current cache:\" + cache);\n  }",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 +\n 148  \n 149 +\n 150  \n 151  \n 152  \n 153  \n 154  ",
            "  /**\n   * Trigger a cache refresh, even if its before the next cache refresh. Does not affect pending\n   * cache refreshes.\n   * <p/>\n   * Blocks until the cache is refreshed.\n   * <p/>\n   * Exposed for TESTING.\n   */\n  public synchronized void triggerCacheRefreshForTesting() {\n    try {\n      refreshCache();\n    } catch (IOException e) {\n      LOG.warn(\"Failed to refresh snapshot hfile cache!\", e);\n    }\n    LOG.debug(\"Current cache:\" + cache);\n  }"
        ]
    ],
    "3434e99e6c728d59ba99141df3730f3e70e0059c": [
        [
            "TestClientClusterStatus::testNone()",
            " 101  \n 102  \n 103 -\n 104 -\n 105 -\n 106 -\n 107 -\n 108 -\n 109 -\n 110  ",
            "  @Test\n  public void testNone() throws Exception {\n    ClusterStatus status0\n      = new ClusterStatus(ADMIN.getClusterMetrics(EnumSet.allOf(Option.class)));\n    ClusterStatus status1\n      = new ClusterStatus(ADMIN.getClusterMetrics(EnumSet.noneOf(Option.class)));\n    Assert.assertEquals(status0, status1);\n    checkPbObjectNotNull(status0);\n    checkPbObjectNotNull(status1);\n  }",
            " 102  \n 103  \n 104 +\n 105 +\n 106 +\n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112  ",
            "  @Test\n  public void testNone() throws Exception {\n    ClusterMetrics status0 = ADMIN.getClusterMetrics(EnumSet.allOf(Option.class));\n    ClusterMetrics status1 = ADMIN.getClusterMetrics(EnumSet.noneOf(Option.class));\n    // Do a rough compare. More specific compares can fail because all regions not deployed yet\n    // or more requests than expected.\n    Assert.assertEquals(status0.getLiveServerMetrics().size(),\n        status1.getLiveServerMetrics().size());\n    checkPbObjectNotNull(new ClusterStatus(status0));\n    checkPbObjectNotNull(new ClusterStatus(status1));\n  }"
        ]
    ],
    "125767b44e93f1094b77a6cf8c2a5ca19b5cabd2": [
        [
            "TestClientOperationTimeout::setUpClass()",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "  @BeforeClass\n  public static void setUpClass() throws Exception {\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);\n\n    TESTING_UTIL.startMiniCluster(1, 1, null, null, DelayedRegionServer.class);\n  }",
            "  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  ",
            "  @BeforeClass\n  public static void setUpClass() throws Exception {\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_OPERATION_TIMEOUT, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_META_OPERATION_TIMEOUT, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_SCANNER_TIMEOUT_PERIOD, 500);\n    TESTING_UTIL.getConfiguration().setLong(HConstants.HBASE_CLIENT_RETRIES_NUMBER, 1);\n\n    TESTING_UTIL.startMiniCluster(1, 1, null, null, DelayedRegionServer.class);\n  }"
        ]
    ],
    "6bc7089f9e0793efc9bdd46a84f5ccd9bc4579ad": [
        [
            "ProcedureUtil::getBackoffTimeMs(int)",
            " 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 -\n 347  ",
            "  /**\n   * Get an exponential backoff time, in milliseconds. The base unit is 1 second, and the max\n   * backoff time is 10 minutes. This is the general backoff policy for most procedure\n   * implementation.\n   */\n  public static long getBackoffTimeMs(int attempts) {\n    long maxBackoffTime = 10L * 60 * 1000; // Ten minutes, hard coded for now.\n    // avoid overflow\n    if (attempts >= 30) {\n      return maxBackoffTime;\n    }\n    return Math.min((long) (1000 * Math.pow(2, attempts)), maxBackoffTime);\n  }",
            " 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347 +\n 348 +\n 349 +\n 350 +\n 351  ",
            "  /**\n   * Get an exponential backoff time, in milliseconds. The base unit is 1 second, and the max\n   * backoff time is 10 minutes. This is the general backoff policy for most procedure\n   * implementation.\n   */\n  public static long getBackoffTimeMs(int attempts) {\n    long maxBackoffTime = 10L * 60 * 1000; // Ten minutes, hard coded for now.\n    // avoid overflow\n    if (attempts >= 30) {\n      return maxBackoffTime;\n    }\n    long backoffTimeMs = Math.min((long) (1000 * Math.pow(2, attempts)), maxBackoffTime);\n    // 1% possible jitter\n    long jitter = (long) (backoffTimeMs * ThreadLocalRandom.current().nextFloat() * 0.01f);\n    return backoffTimeMs + jitter;\n  }"
        ],
        [
            "TestProcedureUtil::testGetBackoffTimeMs()",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66 -\n  67 -\n  68 -\n  69  ",
            "  @Test\n  public void testGetBackoffTimeMs() {\n    for (int i = 30; i < 1000; i++) {\n      assertEquals(TimeUnit.MINUTES.toMillis(10), ProcedureUtil.getBackoffTimeMs(30));\n    }\n    assertEquals(1000, ProcedureUtil.getBackoffTimeMs(0));\n    assertEquals(2000, ProcedureUtil.getBackoffTimeMs(1));\n    assertEquals(32000, ProcedureUtil.getBackoffTimeMs(5));\n  }",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67 +\n  68 +\n  69 +\n  70 +\n  71 +\n  72 +\n  73 +\n  74 +\n  75 +\n  76 +\n  77 +\n  78  ",
            "  @Test\n  public void testGetBackoffTimeMs() {\n    for (int i = 30; i < 1000; i++) {\n      assertEquals(TimeUnit.MINUTES.toMillis(10), ProcedureUtil.getBackoffTimeMs(30));\n    }\n    long backoffTimeMs = ProcedureUtil.getBackoffTimeMs(0);\n    assertTrue(backoffTimeMs >= 1000);\n    assertTrue(backoffTimeMs <= 1000 * 1.01f);\n\n    backoffTimeMs = ProcedureUtil.getBackoffTimeMs(1);\n    assertTrue(backoffTimeMs >= 2000);\n    assertTrue(backoffTimeMs <= 2000 * 1.01f);\n\n    backoffTimeMs = ProcedureUtil.getBackoffTimeMs(5);\n    assertTrue(backoffTimeMs >= 32000);\n    assertTrue(backoffTimeMs <= 32000 * 1.01f);\n  }"
        ]
    ],
    "64f88906f7cc7265fe0c42a4c42530dbd660c70b": [
        [
            "SimpleRegionNormalizer::computePlanForTable(TableName)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 -\n 193 -\n 194 -\n 195 -\n 196 -\n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  ",
            "  /**\n   * Computes next most \"urgent\" normalization action on the table.\n   * Action may be either a split, or a merge, or no action.\n   *\n   * @param table table to normalize\n   * @return normalization plan to execute\n   */\n  @Override\n  public List<NormalizationPlan> computePlanForTable(TableName table) throws HBaseIOException {\n    if (table == null || table.isSystemTable()) {\n      LOG.debug(\"Normalization of system table \" + table + \" isn't allowed\");\n      return null;\n    }\n\n    List<NormalizationPlan> plans = new ArrayList<>();\n    List<RegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().\n      getRegionsOfTable(table);\n\n    //TODO: should we make min number of regions a config param?\n    if (tableRegions == null || tableRegions.size() < minRegionCount) {\n      int nrRegions = tableRegions == null ? 0 : tableRegions.size();\n      LOG.debug(\"Table \" + table + \" has \" + nrRegions + \" regions, required min number\"\n        + \" of regions for normalizer to run is \" + minRegionCount + \", not running normalizer\");\n      return null;\n    }\n\n    LOG.debug(\"Computing normalization plan for table: \" + table +\n      \", number of regions: \" + tableRegions.size());\n\n    long totalSizeMb = 0;\n    int acutalRegionCnt = 0;\n\n    for (int i = 0; i < tableRegions.size(); i++) {\n      RegionInfo hri = tableRegions.get(i);\n      long regionSize = getRegionSize(hri);\n      if (regionSize > 0) {\n        acutalRegionCnt++;\n        totalSizeMb += regionSize;\n      }\n    }\n    int targetRegionCount = -1;\n    long targetRegionSize = -1;\n    try {\n      TableDescriptor tableDescriptor = masterServices.getTableDescriptors().get(table);\n      if(tableDescriptor != null) {\n        targetRegionCount =\n            tableDescriptor.getNormalizerTargetRegionCount();\n        targetRegionSize =\n            tableDescriptor.getNormalizerTargetRegionSize();\n        LOG.debug(\"Table {}:  target region count is {}, target region size is {}\", table,\n            targetRegionCount, targetRegionSize);\n      }\n    } catch (IOException e) {\n      LOG.warn(\n        \"cannot get the target number and target size of table {}, they will be default value -1.\",\n        table);\n    }\n\n    double avgRegionSize;\n    if (targetRegionSize > 0) {\n      avgRegionSize = targetRegionSize;\n    } else if (targetRegionCount > 0) {\n      avgRegionSize = totalSizeMb / (double) targetRegionCount;\n    } else {\n      avgRegionSize = acutalRegionCnt == 0 ? 0 : totalSizeMb / (double) acutalRegionCnt;\n    }\n\n    LOG.debug(\"Table \" + table + \", total aggregated regions size: \" + totalSizeMb);\n    LOG.debug(\"Table \" + table + \", average region size: \" + avgRegionSize);\n\n    int candidateIdx = 0;\n    boolean splitEnabled = true, mergeEnabled = true;\n    try {\n      splitEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.SPLIT)).getEnabled();\n    } catch (org.apache.hbase.thirdparty.com.google.protobuf.ServiceException e) {\n      LOG.debug(\"Unable to determine whether split is enabled\", e);\n    }\n    try {\n      mergeEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.MERGE)).getEnabled();\n    } catch (org.apache.hbase.thirdparty.com.google.protobuf.ServiceException e) {\n      LOG.debug(\"Unable to determine whether split is enabled\", e);\n    }\n    while (candidateIdx < tableRegions.size()) {\n      RegionInfo hri = tableRegions.get(candidateIdx);\n      long regionSize = getRegionSize(hri);\n      // if the region is > 2 times larger than average, we split it, split\n      // is more high priority normalization action than merge.\n      if (regionSize > 2 * avgRegionSize) {\n        if (splitEnabled) {\n          LOG.info(\"Table \" + table + \", large region \" + hri.getRegionNameAsString() + \" has size \"\n              + regionSize + \", more than twice avg size, splitting\");\n          plans.add(new SplitNormalizationPlan(hri, null));\n        }\n      } else {\n        if (candidateIdx == tableRegions.size()-1) {\n          break;\n        }\n        if (mergeEnabled) {\n          RegionInfo hri2 = tableRegions.get(candidateIdx+1);\n          long regionSize2 = getRegionSize(hri2);\n          if (regionSize >= 0 && regionSize2 >= 0 && regionSize + regionSize2 < avgRegionSize) {\n            LOG.info(\"Table \" + table + \", small region size: \" + regionSize\n              + \" plus its neighbor size: \" + regionSize2\n              + \", less than the avg size \" + avgRegionSize + \", merging them\");\n            plans.add(new MergeNormalizationPlan(hri, hri2));\n            candidateIdx++;\n          }\n        }\n      }\n      candidateIdx++;\n    }\n    if (plans.isEmpty()) {\n      LOG.debug(\"No normalization needed, regions look good for table: \" + table);\n      return null;\n    }\n    Collections.sort(plans, planComparator);\n    return plans;\n  }",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135 +\n 136 +\n 137 +\n 138 +\n 139 +\n 140 +\n 141 +\n 142 +\n 143 +\n 144 +\n 145 +\n 146 +\n 147 +\n 148 +\n 149 +\n 150 +\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "  /**\n   * Computes next most \"urgent\" normalization action on the table.\n   * Action may be either a split, or a merge, or no action.\n   *\n   * @param table table to normalize\n   * @return normalization plan to execute\n   */\n  @Override\n  public List<NormalizationPlan> computePlanForTable(TableName table) throws HBaseIOException {\n    if (table == null || table.isSystemTable()) {\n      LOG.debug(\"Normalization of system table \" + table + \" isn't allowed\");\n      return null;\n    }\n    boolean splitEnabled = true, mergeEnabled = true;\n    try {\n      splitEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.SPLIT)).getEnabled();\n    } catch (org.apache.hbase.thirdparty.com.google.protobuf.ServiceException e) {\n      LOG.debug(\"Unable to determine whether split is enabled\", e);\n    }\n    try {\n      mergeEnabled = masterRpcServices.isSplitOrMergeEnabled(null,\n        RequestConverter.buildIsSplitOrMergeEnabledRequest(MasterSwitchType.MERGE)).getEnabled();\n    } catch (org.apache.hbase.thirdparty.com.google.protobuf.ServiceException e) {\n      LOG.debug(\"Unable to determine whether split is enabled\", e);\n    }\n    if (!mergeEnabled && !splitEnabled) {\n      LOG.debug(\"Both split and merge are disabled for table: \" + table);\n      return null;\n    }\n    List<NormalizationPlan> plans = new ArrayList<>();\n    List<RegionInfo> tableRegions = masterServices.getAssignmentManager().getRegionStates().\n      getRegionsOfTable(table);\n\n    //TODO: should we make min number of regions a config param?\n    if (tableRegions == null || tableRegions.size() < minRegionCount) {\n      int nrRegions = tableRegions == null ? 0 : tableRegions.size();\n      LOG.debug(\"Table \" + table + \" has \" + nrRegions + \" regions, required min number\"\n        + \" of regions for normalizer to run is \" + minRegionCount + \", not running normalizer\");\n      return null;\n    }\n\n    LOG.debug(\"Computing normalization plan for table: \" + table +\n      \", number of regions: \" + tableRegions.size());\n\n    long totalSizeMb = 0;\n    int acutalRegionCnt = 0;\n\n    for (int i = 0; i < tableRegions.size(); i++) {\n      RegionInfo hri = tableRegions.get(i);\n      long regionSize = getRegionSize(hri);\n      if (regionSize > 0) {\n        acutalRegionCnt++;\n        totalSizeMb += regionSize;\n      }\n    }\n    int targetRegionCount = -1;\n    long targetRegionSize = -1;\n    try {\n      TableDescriptor tableDescriptor = masterServices.getTableDescriptors().get(table);\n      if(tableDescriptor != null) {\n        targetRegionCount =\n            tableDescriptor.getNormalizerTargetRegionCount();\n        targetRegionSize =\n            tableDescriptor.getNormalizerTargetRegionSize();\n        LOG.debug(\"Table {}:  target region count is {}, target region size is {}\", table,\n            targetRegionCount, targetRegionSize);\n      }\n    } catch (IOException e) {\n      LOG.warn(\n        \"cannot get the target number and target size of table {}, they will be default value -1.\",\n        table);\n    }\n\n    double avgRegionSize;\n    if (targetRegionSize > 0) {\n      avgRegionSize = targetRegionSize;\n    } else if (targetRegionCount > 0) {\n      avgRegionSize = totalSizeMb / (double) targetRegionCount;\n    } else {\n      avgRegionSize = acutalRegionCnt == 0 ? 0 : totalSizeMb / (double) acutalRegionCnt;\n    }\n\n    LOG.debug(\"Table \" + table + \", total aggregated regions size: \" + totalSizeMb);\n    LOG.debug(\"Table \" + table + \", average region size: \" + avgRegionSize);\n\n    int candidateIdx = 0;\n    while (candidateIdx < tableRegions.size()) {\n      RegionInfo hri = tableRegions.get(candidateIdx);\n      long regionSize = getRegionSize(hri);\n      // if the region is > 2 times larger than average, we split it, split\n      // is more high priority normalization action than merge.\n      if (regionSize > 2 * avgRegionSize) {\n        if (splitEnabled) {\n          LOG.info(\"Table \" + table + \", large region \" + hri.getRegionNameAsString() + \" has size \"\n              + regionSize + \", more than twice avg size, splitting\");\n          plans.add(new SplitNormalizationPlan(hri, null));\n        }\n      } else {\n        if (candidateIdx == tableRegions.size()-1) {\n          break;\n        }\n        if (mergeEnabled) {\n          RegionInfo hri2 = tableRegions.get(candidateIdx+1);\n          long regionSize2 = getRegionSize(hri2);\n          if (regionSize >= 0 && regionSize2 >= 0 && regionSize + regionSize2 < avgRegionSize) {\n            LOG.info(\"Table \" + table + \", small region size: \" + regionSize\n              + \" plus its neighbor size: \" + regionSize2\n              + \", less than the avg size \" + avgRegionSize + \", merging them\");\n            plans.add(new MergeNormalizationPlan(hri, hri2));\n            candidateIdx++;\n          }\n        }\n      }\n      candidateIdx++;\n    }\n    if (plans.isEmpty()) {\n      LOG.debug(\"No normalization needed, regions look good for table: \" + table);\n      return null;\n    }\n    Collections.sort(plans, planComparator);\n    return plans;\n  }"
        ]
    ]
}