{
    "6a86e9d62045386b76026f4deead8baa559f008e": [
        [
            "OperatorStateBackendTest::testSnapshotRestore()",
            " 144  \n 145  \n 146 -\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 -\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "\t@Test\n\tpublic void testSnapshotRestore() throws Exception {\n\t\tOperatorStateBackend operatorStateBackend = createNewOperatorStateBackend();\n\t\tListStateDescriptor<Serializable> stateDescriptor1 = new ListStateDescriptor<>(\"test1\", new JavaSerializer<>());\n\t\tListStateDescriptor<Serializable> stateDescriptor2 = new ListStateDescriptor<>(\"test2\", new JavaSerializer<>());\n\t\tListStateDescriptor<Serializable> stateDescriptor3 = new ListStateDescriptor<>(\"test3\", new JavaSerializer<>());\n\t\tListState<Serializable> listState1 = operatorStateBackend.getOperatorState(stateDescriptor1);\n\t\tListState<Serializable> listState2 = operatorStateBackend.getOperatorState(stateDescriptor2);\n\t\tListState<Serializable> listState3 = operatorStateBackend.getBroadcastOperatorState(stateDescriptor3);\n\n\t\tlistState1.add(42);\n\t\tlistState1.add(4711);\n\n\t\tlistState2.add(7);\n\t\tlistState2.add(13);\n\t\tlistState2.add(23);\n\n\t\tlistState3.add(17);\n\t\tlistState3.add(18);\n\t\tlistState3.add(19);\n\t\tlistState3.add(20);\n\n\t\tCheckpointStreamFactory streamFactory = abstractStateBackend.createStreamFactory(new JobID(), \"testOperator\");\n\t\tOperatorStateHandle stateHandle = operatorStateBackend.snapshot(1, 1, streamFactory).get();\n\n\t\ttry {\n\n\t\t\toperatorStateBackend.close();\n\t\t\toperatorStateBackend.dispose();\n\n\t\t\toperatorStateBackend = abstractStateBackend.createOperatorStateBackend(\n\t\t\t\t\tcreateMockEnvironment(),\n\t\t\t\t\t\"testOperator\");\n\n\t\t\toperatorStateBackend.restore(Collections.singletonList(stateHandle));\n\n\t\t\tassertEquals(3, operatorStateBackend.getRegisteredStateNames().size());\n\n\t\t\tlistState1 = operatorStateBackend.getOperatorState(stateDescriptor1);\n\t\t\tlistState2 = operatorStateBackend.getOperatorState(stateDescriptor2);\n\t\t\tlistState3 = operatorStateBackend.getBroadcastOperatorState(stateDescriptor3);\n\n\t\t\tassertEquals(3, operatorStateBackend.getRegisteredStateNames().size());\n\n\t\t\tIterator<Serializable> it = listState1.get().iterator();\n\t\t\tassertEquals(42, it.next());\n\t\t\tassertEquals(4711, it.next());\n\t\t\tassertTrue(!it.hasNext());\n\n\t\t\tit = listState2.get().iterator();\n\t\t\tassertEquals(7, it.next());\n\t\t\tassertEquals(13, it.next());\n\t\t\tassertEquals(23, it.next());\n\t\t\tassertTrue(!it.hasNext());\n\n\t\t\tit = listState3.get().iterator();\n\t\t\tassertEquals(17, it.next());\n\t\t\tassertEquals(18, it.next());\n\t\t\tassertEquals(19, it.next());\n\t\t\tassertEquals(20, it.next());\n\t\t\tassertTrue(!it.hasNext());\n\n\t\t\toperatorStateBackend.close();\n\t\t\toperatorStateBackend.dispose();\n\t\t} finally {\n\t\t\tstateHandle.discardState();\n\t\t}\n\t}",
            " 145  \n 146  \n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175 +\n 176 +\n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "\t@Test\n\tpublic void testSnapshotRestore() throws Exception {\n\t\tDefaultOperatorStateBackend operatorStateBackend = createNewOperatorStateBackend();\n\t\tListStateDescriptor<Serializable> stateDescriptor1 = new ListStateDescriptor<>(\"test1\", new JavaSerializer<>());\n\t\tListStateDescriptor<Serializable> stateDescriptor2 = new ListStateDescriptor<>(\"test2\", new JavaSerializer<>());\n\t\tListStateDescriptor<Serializable> stateDescriptor3 = new ListStateDescriptor<>(\"test3\", new JavaSerializer<>());\n\t\tListState<Serializable> listState1 = operatorStateBackend.getOperatorState(stateDescriptor1);\n\t\tListState<Serializable> listState2 = operatorStateBackend.getOperatorState(stateDescriptor2);\n\t\tListState<Serializable> listState3 = operatorStateBackend.getBroadcastOperatorState(stateDescriptor3);\n\n\t\tlistState1.add(42);\n\t\tlistState1.add(4711);\n\n\t\tlistState2.add(7);\n\t\tlistState2.add(13);\n\t\tlistState2.add(23);\n\n\t\tlistState3.add(17);\n\t\tlistState3.add(18);\n\t\tlistState3.add(19);\n\t\tlistState3.add(20);\n\n\t\tCheckpointStreamFactory streamFactory = abstractStateBackend.createStreamFactory(new JobID(), \"testOperator\");\n\t\tOperatorStateHandle stateHandle = operatorStateBackend.snapshot(1, 1, streamFactory).get();\n\n\t\ttry {\n\n\t\t\toperatorStateBackend.close();\n\t\t\toperatorStateBackend.dispose();\n\n\t\t\t//TODO this is temporarily casted to test already functionality that we do not yet expose through public API\n\t\t\toperatorStateBackend = (DefaultOperatorStateBackend) abstractStateBackend.createOperatorStateBackend(\n\t\t\t\t\tcreateMockEnvironment(),\n\t\t\t\t\t\"testOperator\");\n\n\t\t\toperatorStateBackend.restore(Collections.singletonList(stateHandle));\n\n\t\t\tassertEquals(3, operatorStateBackend.getRegisteredStateNames().size());\n\n\t\t\tlistState1 = operatorStateBackend.getOperatorState(stateDescriptor1);\n\t\t\tlistState2 = operatorStateBackend.getOperatorState(stateDescriptor2);\n\t\t\tlistState3 = operatorStateBackend.getBroadcastOperatorState(stateDescriptor3);\n\n\t\t\tassertEquals(3, operatorStateBackend.getRegisteredStateNames().size());\n\n\t\t\tIterator<Serializable> it = listState1.get().iterator();\n\t\t\tassertEquals(42, it.next());\n\t\t\tassertEquals(4711, it.next());\n\t\t\tassertTrue(!it.hasNext());\n\n\t\t\tit = listState2.get().iterator();\n\t\t\tassertEquals(7, it.next());\n\t\t\tassertEquals(13, it.next());\n\t\t\tassertEquals(23, it.next());\n\t\t\tassertTrue(!it.hasNext());\n\n\t\t\tit = listState3.get().iterator();\n\t\t\tassertEquals(17, it.next());\n\t\t\tassertEquals(18, it.next());\n\t\t\tassertEquals(19, it.next());\n\t\t\tassertEquals(20, it.next());\n\t\t\tassertTrue(!it.hasNext());\n\n\t\t\toperatorStateBackend.close();\n\t\t\toperatorStateBackend.dispose();\n\t\t} finally {\n\t\t\tstateHandle.discardState();\n\t\t}\n\t}"
        ],
        [
            "OperatorStateBackendTest::testRegisterStates()",
            "  61  \n  62  \n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  ",
            "\t@Test\n\tpublic void testRegisterStates() throws Exception {\n\t\tOperatorStateBackend operatorStateBackend = createNewOperatorStateBackend();\n\t\tListStateDescriptor<Serializable> stateDescriptor1 = new ListStateDescriptor<>(\"test1\", new JavaSerializer<>());\n\t\tListStateDescriptor<Serializable> stateDescriptor2 = new ListStateDescriptor<>(\"test2\", new JavaSerializer<>());\n\t\tListStateDescriptor<Serializable> stateDescriptor3 = new ListStateDescriptor<>(\"test3\", new JavaSerializer<>());\n\t\tListState<Serializable> listState1 = operatorStateBackend.getOperatorState(stateDescriptor1);\n\t\tassertNotNull(listState1);\n\t\tassertEquals(1, operatorStateBackend.getRegisteredStateNames().size());\n\t\tIterator<Serializable> it = listState1.get().iterator();\n\t\tassertTrue(!it.hasNext());\n\t\tlistState1.add(42);\n\t\tlistState1.add(4711);\n\n\t\tit = listState1.get().iterator();\n\t\tassertEquals(42, it.next());\n\t\tassertEquals(4711, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tListState<Serializable> listState2 = operatorStateBackend.getOperatorState(stateDescriptor2);\n\t\tassertNotNull(listState2);\n\t\tassertEquals(2, operatorStateBackend.getRegisteredStateNames().size());\n\t\tassertTrue(!it.hasNext());\n\t\tlistState2.add(7);\n\t\tlistState2.add(13);\n\t\tlistState2.add(23);\n\n\t\tit = listState2.get().iterator();\n\t\tassertEquals(7, it.next());\n\t\tassertEquals(13, it.next());\n\t\tassertEquals(23, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tListState<Serializable> listState3 = operatorStateBackend.getBroadcastOperatorState(stateDescriptor3);\n\t\tassertNotNull(listState3);\n\t\tassertEquals(3, operatorStateBackend.getRegisteredStateNames().size());\n\t\tassertTrue(!it.hasNext());\n\t\tlistState3.add(17);\n\t\tlistState3.add(3);\n\t\tlistState3.add(123);\n\n\t\tit = listState3.get().iterator();\n\t\tassertEquals(17, it.next());\n\t\tassertEquals(3, it.next());\n\t\tassertEquals(123, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tListState<Serializable> listState1b = operatorStateBackend.getOperatorState(stateDescriptor1);\n\t\tassertNotNull(listState1b);\n\t\tlistState1b.add(123);\n\t\tit = listState1b.get().iterator();\n\t\tassertEquals(42, it.next());\n\t\tassertEquals(4711, it.next());\n\t\tassertEquals(123, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tit = listState1.get().iterator();\n\t\tassertEquals(42, it.next());\n\t\tassertEquals(4711, it.next());\n\t\tassertEquals(123, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tit = listState1b.get().iterator();\n\t\tassertEquals(42, it.next());\n\t\tassertEquals(4711, it.next());\n\t\tassertEquals(123, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\ttry {\n\t\t\toperatorStateBackend.getBroadcastOperatorState(stateDescriptor2);\n\t\t\tfail(\"Did not detect changed mode\");\n\t\t} catch (IllegalStateException ignored) {\n\n\t\t}\n\n\t\ttry {\n\t\t\toperatorStateBackend.getOperatorState(stateDescriptor3);\n\t\t\tfail(\"Did not detect changed mode\");\n\t\t} catch (IllegalStateException ignored) {\n\n\t\t}\n\t}",
            "  62  \n  63  \n  64 +\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "\t@Test\n\tpublic void testRegisterStates() throws Exception {\n\t\tDefaultOperatorStateBackend operatorStateBackend = createNewOperatorStateBackend();\n\t\tListStateDescriptor<Serializable> stateDescriptor1 = new ListStateDescriptor<>(\"test1\", new JavaSerializer<>());\n\t\tListStateDescriptor<Serializable> stateDescriptor2 = new ListStateDescriptor<>(\"test2\", new JavaSerializer<>());\n\t\tListStateDescriptor<Serializable> stateDescriptor3 = new ListStateDescriptor<>(\"test3\", new JavaSerializer<>());\n\t\tListState<Serializable> listState1 = operatorStateBackend.getOperatorState(stateDescriptor1);\n\t\tassertNotNull(listState1);\n\t\tassertEquals(1, operatorStateBackend.getRegisteredStateNames().size());\n\t\tIterator<Serializable> it = listState1.get().iterator();\n\t\tassertTrue(!it.hasNext());\n\t\tlistState1.add(42);\n\t\tlistState1.add(4711);\n\n\t\tit = listState1.get().iterator();\n\t\tassertEquals(42, it.next());\n\t\tassertEquals(4711, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tListState<Serializable> listState2 = operatorStateBackend.getOperatorState(stateDescriptor2);\n\t\tassertNotNull(listState2);\n\t\tassertEquals(2, operatorStateBackend.getRegisteredStateNames().size());\n\t\tassertTrue(!it.hasNext());\n\t\tlistState2.add(7);\n\t\tlistState2.add(13);\n\t\tlistState2.add(23);\n\n\t\tit = listState2.get().iterator();\n\t\tassertEquals(7, it.next());\n\t\tassertEquals(13, it.next());\n\t\tassertEquals(23, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tListState<Serializable> listState3 = operatorStateBackend.getBroadcastOperatorState(stateDescriptor3);\n\t\tassertNotNull(listState3);\n\t\tassertEquals(3, operatorStateBackend.getRegisteredStateNames().size());\n\t\tassertTrue(!it.hasNext());\n\t\tlistState3.add(17);\n\t\tlistState3.add(3);\n\t\tlistState3.add(123);\n\n\t\tit = listState3.get().iterator();\n\t\tassertEquals(17, it.next());\n\t\tassertEquals(3, it.next());\n\t\tassertEquals(123, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tListState<Serializable> listState1b = operatorStateBackend.getOperatorState(stateDescriptor1);\n\t\tassertNotNull(listState1b);\n\t\tlistState1b.add(123);\n\t\tit = listState1b.get().iterator();\n\t\tassertEquals(42, it.next());\n\t\tassertEquals(4711, it.next());\n\t\tassertEquals(123, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tit = listState1.get().iterator();\n\t\tassertEquals(42, it.next());\n\t\tassertEquals(4711, it.next());\n\t\tassertEquals(123, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\tit = listState1b.get().iterator();\n\t\tassertEquals(42, it.next());\n\t\tassertEquals(4711, it.next());\n\t\tassertEquals(123, it.next());\n\t\tassertTrue(!it.hasNext());\n\n\t\ttry {\n\t\t\toperatorStateBackend.getBroadcastOperatorState(stateDescriptor2);\n\t\t\tfail(\"Did not detect changed mode\");\n\t\t} catch (IllegalStateException ignored) {\n\n\t\t}\n\n\t\ttry {\n\t\t\toperatorStateBackend.getOperatorState(stateDescriptor3);\n\t\t\tfail(\"Did not detect changed mode\");\n\t\t} catch (IllegalStateException ignored) {\n\n\t\t}\n\t}"
        ],
        [
            "OperatorStateBackendTest::createNewOperatorStateBackend()",
            "  48 -\n  49 -\n  50  \n  51  \n  52  ",
            "\tprivate OperatorStateBackend createNewOperatorStateBackend() throws Exception {\n\t\treturn abstractStateBackend.createOperatorStateBackend(\n\t\t\t\tcreateMockEnvironment(),\n\t\t\t\t\"test-operator\");\n\t}",
            "  48 +\n  49 +\n  50 +\n  51  \n  52  \n  53  ",
            "\tprivate DefaultOperatorStateBackend createNewOperatorStateBackend() throws Exception {\n\t\t//TODO this is temporarily casted to test already functionality that we do not yet expose through public API\n\t\treturn (DefaultOperatorStateBackend) abstractStateBackend.createOperatorStateBackend(\n\t\t\t\tcreateMockEnvironment(),\n\t\t\t\t\"test-operator\");\n\t}"
        ],
        [
            "RescalingITCase::PartitionedStateSource::initializeState(FunctionInitializationContext)",
            " 968  \n 969  \n 970  \n 971  \n 972  \n 973 -\n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  ",
            "\t\t@Override\n\t\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n\n\t\t\tif (broadcast) {\n\t\t\t\tthis.counterPartitions =\n\t\t\t\t\t\tcontext.getOperatorStateStore().getBroadcastSerializableListState(\"counter_partitions\");\n\t\t\t} else {\n\t\t\t\tthis.counterPartitions =\n\t\t\t\t\t\tcontext.getOperatorStateStore().getSerializableListState(\"counter_partitions\");\n\t\t\t}\n\n\t\t\tif (context.isRestored()) {\n\t\t\t\tfor (int v : counterPartitions.get()) {\n\t\t\t\t\tcounter += v;\n\t\t\t\t}\n\t\t\t\tCHECK_CORRECT_RESTORE[getRuntimeContext().getIndexOfThisSubtask()] = counter;\n\t\t\t}\n\t\t}",
            " 969  \n 970  \n 971  \n 972  \n 973 +\n 974 +\n 975  \n 976 +\n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  ",
            "\t\t@Override\n\t\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n\n\t\t\tif (broadcast) {\n\t\t\t\t//TODO this is temporarily casted to test already functionality that we do not yet expose through public API\n\t\t\t\tDefaultOperatorStateBackend operatorStateStore = (DefaultOperatorStateBackend) context.getOperatorStateStore();\n\t\t\t\tthis.counterPartitions =\n\t\t\t\t\t\toperatorStateStore.getBroadcastSerializableListState(\"counter_partitions\");\n\t\t\t} else {\n\t\t\t\tthis.counterPartitions =\n\t\t\t\t\t\tcontext.getOperatorStateStore().getSerializableListState(\"counter_partitions\");\n\t\t\t}\n\n\t\t\tif (context.isRestored()) {\n\t\t\t\tfor (int v : counterPartitions.get()) {\n\t\t\t\t\tcounter += v;\n\t\t\t\t}\n\t\t\t\tCHECK_CORRECT_RESTORE[getRuntimeContext().getIndexOfThisSubtask()] = counter;\n\t\t\t}\n\t\t}"
        ]
    ],
    "10e4e321b335b6f9376501f90715e31b71b02da8": [
        [
            "Execution::getAssignedResourceLocation()",
            " 190  \n 191  \n 192 -\n 193  ",
            "\t@Override\n\tpublic TaskManagerLocation getAssignedResourceLocation() {\n\t\treturn assignedResourceLocation;\n\t}",
            " 190  \n 191  \n 192 +\n 193 +\n 194  ",
            "\t@Override\n\tpublic TaskManagerLocation getAssignedResourceLocation() {\n\t\t// returns non-null only when a location is already assigned\n\t\treturn assignedResource != null ? assignedResource.getTaskManagerLocation() : null;\n\t}"
        ],
        [
            "Execution::deployToSlot(SimpleSlot)",
            " 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 -\n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356 -\n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376 -\n 377 -\n 378 -\n 379  \n 380  \n 381 -\n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  ",
            "\tpublic void deployToSlot(final SimpleSlot slot) throws JobException {\n\t\t// sanity checks\n\t\tif (slot == null) {\n\t\t\tthrow new NullPointerException();\n\t\t}\n\t\tif (!slot.isAlive()) {\n\t\t\tthrow new JobException(\"Target slot for deployment is not alive.\");\n\t\t}\n\n\t\t// make sure exactly one deployment call happens from the correct state\n\t\t// note: the transition from CREATED to DEPLOYING is for testing purposes only\n\t\tExecutionState previous = this.state;\n\t\tif (previous == SCHEDULED || previous == CREATED) {\n\t\t\tif (!transitionState(previous, DEPLOYING)) {\n\t\t\t\t// race condition, someone else beat us to the deploying call.\n\t\t\t\t// this should actually not happen and indicates a race somewhere else\n\t\t\t\tthrow new IllegalStateException(\"Cannot deploy task: Concurrent deployment call race.\");\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t// vertex may have been cancelled, or it was already scheduled\n\t\t\tthrow new IllegalStateException(\"The vertex must be in CREATED or SCHEDULED state to be deployed. Found state \" + previous);\n\t\t}\n\n\t\ttry {\n\t\t\t// good, we are allowed to deploy\n\t\t\tif (!slot.setExecutedVertex(this)) {\n\t\t\t\tthrow new JobException(\"Could not assign the ExecutionVertex to the slot \" + slot);\n\t\t\t}\n\t\t\tthis.assignedResource = slot;\n\t\t\tthis.assignedResourceLocation = slot.getTaskManagerLocation();\n\n\t\t\t// race double check, did we fail/cancel and do we need to release the slot?\n\t\t\tif (this.state != DEPLOYING) {\n\t\t\t\tslot.releaseSlot();\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(String.format(\"Deploying %s (attempt #%d) to %s\", vertex.getSimpleName(),\n\t\t\t\t\t\tattemptNumber, assignedResourceLocation.getHostname()));\n\t\t\t}\n\n\t\t\tfinal TaskDeploymentDescriptor deployment = vertex.createDeploymentDescriptor(\n\t\t\t\tattemptId,\n\t\t\t\tslot,\n\t\t\t\ttaskStateHandles,\n\t\t\t\tattemptNumber);\n\n\t\t\t// register this execution at the execution graph, to receive call backs\n\t\t\tvertex.getExecutionGraph().registerExecution(this);\n\t\t\t\n\t\t\tfinal TaskManagerGateway taskManagerGateway = slot.getTaskManagerGateway();\n\n\t\t\tfinal Future<Acknowledge> submitResultFuture = taskManagerGateway.submitTask(deployment, timeout);\n\n\t\t\tsubmitResultFuture.exceptionallyAsync(new ApplyFunction<Throwable, Void>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Void apply(Throwable failure) {\n\t\t\t\t\tif (failure instanceof TimeoutException) {\n\t\t\t\t\t\tString taskname = vertex.getTaskName() + '(' +\n\t\t\t\t\t\t\t(getParallelSubtaskIndex() + 1) + '/' +\n\t\t\t\t\t\t\tvertex.getTotalNumberOfParallelSubtasks() + \") (\" + attemptId + ')';\n\n\t\t\t\t\t\tmarkFailed(new Exception(\n\t\t\t\t\t\t\t\"Cannot deploy task \" + taskname + \" - TaskManager (\" + assignedResourceLocation\n\t\t\t\t\t\t\t\t+ \") not responding after a timeout of \" + timeout, failure));\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tmarkFailed(failure);\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}, executor);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tmarkFailed(t);\n\t\t\tExceptionUtils.rethrow(t);\n\t\t}\n\t}",
            " 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352 +\n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 +\n 373  \n 374  \n 375 +\n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "\tpublic void deployToSlot(final SimpleSlot slot) throws JobException {\n\t\t// sanity checks\n\t\tif (slot == null) {\n\t\t\tthrow new NullPointerException();\n\t\t}\n\t\tif (!slot.isAlive()) {\n\t\t\tthrow new JobException(\"Target slot for deployment is not alive.\");\n\t\t}\n\n\t\t// make sure exactly one deployment call happens from the correct state\n\t\t// note: the transition from CREATED to DEPLOYING is for testing purposes only\n\t\tExecutionState previous = this.state;\n\t\tif (previous == SCHEDULED || previous == CREATED) {\n\t\t\tif (!transitionState(previous, DEPLOYING)) {\n\t\t\t\t// race condition, someone else beat us to the deploying call.\n\t\t\t\t// this should actually not happen and indicates a race somewhere else\n\t\t\t\tthrow new IllegalStateException(\"Cannot deploy task: Concurrent deployment call race.\");\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t// vertex may have been cancelled, or it was already scheduled\n\t\t\tthrow new IllegalStateException(\"The vertex must be in CREATED or SCHEDULED state to be deployed. Found state \" + previous);\n\t\t}\n\n\t\ttry {\n\t\t\t// good, we are allowed to deploy\n\t\t\tif (!slot.setExecutedVertex(this)) {\n\t\t\t\tthrow new JobException(\"Could not assign the ExecutionVertex to the slot \" + slot);\n\t\t\t}\n\t\t\tthis.assignedResource = slot;\n\n\t\t\t// race double check, did we fail/cancel and do we need to release the slot?\n\t\t\tif (this.state != DEPLOYING) {\n\t\t\t\tslot.releaseSlot();\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tif (LOG.isInfoEnabled()) {\n\t\t\t\tLOG.info(String.format(\"Deploying %s (attempt #%d) to %s\", vertex.getSimpleName(),\n\t\t\t\t\t\tattemptNumber, getAssignedResourceLocation().getHostname()));\n\t\t\t}\n\n\t\t\tfinal TaskDeploymentDescriptor deployment = vertex.createDeploymentDescriptor(\n\t\t\t\tattemptId,\n\t\t\t\tslot,\n\t\t\t\ttaskStateHandles,\n\t\t\t\tattemptNumber);\n\n\t\t\t// register this execution at the execution graph, to receive call backs\n\t\t\tvertex.getExecutionGraph().registerExecution(this);\n\t\t\t\n\t\t\tfinal TaskManagerGateway taskManagerGateway = slot.getTaskManagerGateway();\n\n\t\t\tfinal Future<Acknowledge> submitResultFuture = taskManagerGateway.submitTask(deployment, timeout);\n\n\t\t\tsubmitResultFuture.exceptionallyAsync(new ApplyFunction<Throwable, Void>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Void apply(Throwable failure) {\n\t\t\t\t\tif (failure instanceof TimeoutException) {\n\t\t\t\t\t\tString taskname = vertex.getTaskNameWithSubtaskIndex()+ \" (\" + attemptId + ')';\n\n\t\t\t\t\t\tmarkFailed(new Exception(\n\t\t\t\t\t\t\t\"Cannot deploy task \" + taskname + \" - TaskManager (\" + getAssignedResourceLocation()\n\t\t\t\t\t\t\t\t+ \") not responding after a timeout of \" + timeout, failure));\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tmarkFailed(failure);\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}, executor);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tmarkFailed(t);\n\t\t\tExceptionUtils.rethrow(t);\n\t\t}\n\t}"
        ],
        [
            "Execution::setInitialState(TaskStateHandles)",
            " 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229 -\n 230 -\n 231 -\n 232 -\n 233 -\n 234  \n 235  ",
            "\t/**\n\t * Sets the initial state for the execution. The serialized state is then shipped via the\n\t * {@link TaskDeploymentDescriptor} to the TaskManagers.\n\t *\n\t * @param checkpointStateHandles all checkpointed operator state\n\t */\n\tpublic void setInitialState(TaskStateHandles checkpointStateHandles) {\n\n\t\tif (state != ExecutionState.CREATED) {\n\t\t\tthrow new IllegalArgumentException(\"Can only assign operator state when execution attempt is in CREATED\");\n\t\t}\n\n\t\tthis.taskStateHandles = checkpointStateHandles;\n\t}",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231  \n 232  ",
            "\t/**\n\t * Sets the initial state for the execution. The serialized state is then shipped via the\n\t * {@link TaskDeploymentDescriptor} to the TaskManagers.\n\t *\n\t * @param checkpointStateHandles all checkpointed operator state\n\t */\n\tpublic void setInitialState(TaskStateHandles checkpointStateHandles) {\n\t\tcheckState(state == CREATED, \"Can only assign operator state when execution attempt is in CREATED\");\n\t\tthis.taskStateHandles = checkpointStateHandles;\n\t}"
        ]
    ],
    "f24514339c78d809a28731fa18e8df638b382e3b": [
        [
            "HttpRequestHandler::channelRead0(ChannelHandlerContext,HttpObject)",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 -\n 111 -\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  ",
            "\t@Override\n\tpublic void channelRead0(ChannelHandlerContext ctx, HttpObject msg) {\n\t\ttry {\n\t\t\tif (msg instanceof HttpRequest) {\n\t\t\t\tcurrentRequest = (HttpRequest) msg;\n\t\t\t\tcurrentRequestPath = null;\n\n\t\t\t\tif (currentDecoder != null) {\n\t\t\t\t\tcurrentDecoder.destroy();\n\t\t\t\t\tcurrentDecoder = null;\n\t\t\t\t}\n\n\t\t\t\tif (currentRequest.getMethod() == HttpMethod.GET || currentRequest.getMethod() == HttpMethod.DELETE) {\n\t\t\t\t\t// directly delegate to the router\n\t\t\t\t\tctx.fireChannelRead(currentRequest);\n\t\t\t\t}\n\t\t\t\telse if (currentRequest.getMethod() == HttpMethod.POST) {\n\t\t\t\t\t// POST comes in multiple objects. First the request, then the contents\n\t\t\t\t\t// keep the request and path for the remaining objects of the POST request\n\t\t\t\t\tcurrentRequestPath = new QueryStringDecoder(currentRequest.getUri()).path();\n\t\t\t\t\tcurrentDecoder = new HttpPostRequestDecoder(DATA_FACTORY, currentRequest);\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new IOException(\"Unsupported HTTP method: \" + currentRequest.getMethod().name());\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if (currentDecoder != null && msg instanceof HttpContent) {\n\t\t\t\t// received new chunk, give it to the current decoder\n\t\t\t\tHttpContent chunk = (HttpContent) msg;\n\t\t\t\tcurrentDecoder.offer(chunk);\n\n\t\t\t\ttry {\n\t\t\t\t\twhile (currentDecoder.hasNext()) {\n\t\t\t\t\t\tInterfaceHttpData data = currentDecoder.next();\n\n\t\t\t\t\t\t// IF SOMETHING EVER NEEDS POST PARAMETERS, THIS WILL BE THE PLACE TO HANDLE IT\n\t\t\t\t\t\t// all fields values will be passed with type Attribute.\n\n\t\t\t\t\t\tif (data.getHttpDataType() == HttpDataType.FileUpload) {\n\t\t\t\t\t\t\tDiskFileUpload file = (DiskFileUpload) data;\n\t\t\t\t\t\t\tif (file.isCompleted()) {\n\t\t\t\t\t\t\t\tString name = file.getFilename();\n\n\t\t\t\t\t\t\t\tFile target = new File(tmpDir, UUID.randomUUID() + \"_\" + name);\n\t\t\t\t\t\t\t\tfile.renameTo(target);\n\n\t\t\t\t\t\t\t\tQueryStringEncoder encoder = new QueryStringEncoder(currentRequestPath);\n\t\t\t\t\t\t\t\tencoder.addParam(\"filepath\", target.getAbsolutePath());\n\t\t\t\t\t\t\t\tencoder.addParam(\"filename\", name);\n\n\t\t\t\t\t\t\t\tcurrentRequest.setUri(encoder.toString());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tdata.release();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcatch (EndOfDataDecoderException ignored) {}\n\n\t\t\t\tif (chunk instanceof LastHttpContent) {\n\t\t\t\t\tHttpRequest request = currentRequest;\n\t\t\t\t\tcurrentRequest = null;\n\t\t\t\t\tcurrentRequestPath = null;\n\n\t\t\t\t\tcurrentDecoder.destroy();\n\t\t\t\t\tcurrentDecoder = null;\n\n\t\t\t\t\t// fire next channel handler\n\t\t\t\t\tctx.fireChannelRead(request);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tcurrentRequest = null;\n\t\t\tcurrentRequestPath = null;\n\n\t\t\tif (currentDecoder != null) {\n\t\t\t\tcurrentDecoder.destroy();\n\t\t\t\tcurrentDecoder = null;\n\t\t\t}\n\n\t\t\tif (ctx.channel().isActive()) {\n\t\t\t\tbyte[] bytes = ExceptionUtils.stringifyException(t).getBytes(ENCODING);\n\n\t\t\t\tDefaultFullHttpResponse response = new DefaultFullHttpResponse(\n\t\t\t\t\tHttpVersion.HTTP_1_1, HttpResponseStatus.INTERNAL_SERVER_ERROR,\n\t\t\t\t\tUnpooled.wrappedBuffer(bytes));\n\n\t\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_TYPE, \"text/plain\");\n\t\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_LENGTH, response.content().readableBytes());\n\n\t\t\t\tctx.writeAndFlush(response);\n\t\t\t}\n\t\t}\n\t}",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111 +\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  ",
            "\t@Override\n\tpublic void channelRead0(ChannelHandlerContext ctx, HttpObject msg) {\n\t\ttry {\n\t\t\tif (msg instanceof HttpRequest) {\n\t\t\t\tcurrentRequest = (HttpRequest) msg;\n\t\t\t\tcurrentRequestPath = null;\n\n\t\t\t\tif (currentDecoder != null) {\n\t\t\t\t\tcurrentDecoder.destroy();\n\t\t\t\t\tcurrentDecoder = null;\n\t\t\t\t}\n\n\t\t\t\tif (currentRequest.getMethod() == HttpMethod.GET || currentRequest.getMethod() == HttpMethod.DELETE) {\n\t\t\t\t\t// directly delegate to the router\n\t\t\t\t\tctx.fireChannelRead(currentRequest);\n\t\t\t\t}\n\t\t\t\telse if (currentRequest.getMethod() == HttpMethod.POST) {\n\t\t\t\t\t// POST comes in multiple objects. First the request, then the contents\n\t\t\t\t\t// keep the request and path for the remaining objects of the POST request\n\t\t\t\t\tcurrentRequestPath = new QueryStringDecoder(currentRequest.getUri(), ENCODING).path();\n\t\t\t\t\tcurrentDecoder = new HttpPostRequestDecoder(DATA_FACTORY, currentRequest, ENCODING);\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tthrow new IOException(\"Unsupported HTTP method: \" + currentRequest.getMethod().name());\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if (currentDecoder != null && msg instanceof HttpContent) {\n\t\t\t\t// received new chunk, give it to the current decoder\n\t\t\t\tHttpContent chunk = (HttpContent) msg;\n\t\t\t\tcurrentDecoder.offer(chunk);\n\n\t\t\t\ttry {\n\t\t\t\t\twhile (currentDecoder.hasNext()) {\n\t\t\t\t\t\tInterfaceHttpData data = currentDecoder.next();\n\n\t\t\t\t\t\t// IF SOMETHING EVER NEEDS POST PARAMETERS, THIS WILL BE THE PLACE TO HANDLE IT\n\t\t\t\t\t\t// all fields values will be passed with type Attribute.\n\n\t\t\t\t\t\tif (data.getHttpDataType() == HttpDataType.FileUpload) {\n\t\t\t\t\t\t\tDiskFileUpload file = (DiskFileUpload) data;\n\t\t\t\t\t\t\tif (file.isCompleted()) {\n\t\t\t\t\t\t\t\tString name = file.getFilename();\n\n\t\t\t\t\t\t\t\tFile target = new File(tmpDir, UUID.randomUUID() + \"_\" + name);\n\t\t\t\t\t\t\t\tfile.renameTo(target);\n\n\t\t\t\t\t\t\t\tQueryStringEncoder encoder = new QueryStringEncoder(currentRequestPath);\n\t\t\t\t\t\t\t\tencoder.addParam(\"filepath\", target.getAbsolutePath());\n\t\t\t\t\t\t\t\tencoder.addParam(\"filename\", name);\n\n\t\t\t\t\t\t\t\tcurrentRequest.setUri(encoder.toString());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tdata.release();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tcatch (EndOfDataDecoderException ignored) {}\n\n\t\t\t\tif (chunk instanceof LastHttpContent) {\n\t\t\t\t\tHttpRequest request = currentRequest;\n\t\t\t\t\tcurrentRequest = null;\n\t\t\t\t\tcurrentRequestPath = null;\n\n\t\t\t\t\tcurrentDecoder.destroy();\n\t\t\t\t\tcurrentDecoder = null;\n\n\t\t\t\t\t// fire next channel handler\n\t\t\t\t\tctx.fireChannelRead(request);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tcurrentRequest = null;\n\t\t\tcurrentRequestPath = null;\n\n\t\t\tif (currentDecoder != null) {\n\t\t\t\tcurrentDecoder.destroy();\n\t\t\t\tcurrentDecoder = null;\n\t\t\t}\n\n\t\t\tif (ctx.channel().isActive()) {\n\t\t\t\tbyte[] bytes = ExceptionUtils.stringifyException(t).getBytes(ENCODING);\n\n\t\t\t\tDefaultFullHttpResponse response = new DefaultFullHttpResponse(\n\t\t\t\t\tHttpVersion.HTTP_1_1, HttpResponseStatus.INTERNAL_SERVER_ERROR,\n\t\t\t\t\tUnpooled.wrappedBuffer(bytes));\n\n\t\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_TYPE, \"text/plain\");\n\t\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_LENGTH, response.content().readableBytes());\n\n\t\t\t\tctx.writeAndFlush(response);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "RuntimeMonitorHandler::respondAsLeader(ChannelHandlerContext,Routed,ActorGateway)",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "\t@Override\n\tprotected void respondAsLeader(ChannelHandlerContext ctx, Routed routed, ActorGateway jobManager) {\n\t\tFullHttpResponse response;\n\n\t\ttry {\n\t\t\t// we only pass the first element in the list to the handlers.\n\t\t\tMap<String, String> queryParams = new HashMap<>();\n\t\t\tfor (String key : routed.queryParams().keySet()) {\n\t\t\t\tqueryParams.put(key, routed.queryParam(key));\n\t\t\t}\n\n\t\t\tMap<String, String> pathParams = new HashMap<>(routed.pathParams().size());\n\t\t\tfor (String key : routed.pathParams().keySet()) {\n\t\t\t\tpathParams.put(key, URLDecoder.decode(routed.pathParams().get(key), ENCODING.toString()));\n\t\t\t}\n\n\t\t\tInetSocketAddress address = (InetSocketAddress) ctx.channel().localAddress();\n\t\t\tqueryParams.put(WEB_MONITOR_ADDRESS_KEY,\n\t\t\t\t(httpsEnabled ? \"https://\" : \"http://\") + address.getHostName() + \":\" + address.getPort());\n\n\t\t\tresponse = handler.handleRequest(pathParams, queryParams, jobManager);\n\t\t}\n\t\tcatch (NotFoundException e) {\n\t\t\t// this should result in a 404 error code (not found)\n\t\t\tByteBuf message = e.getMessage() == null ? Unpooled.buffer(0)\n\t\t\t\t\t: Unpooled.wrappedBuffer(e.getMessage().getBytes(ENCODING));\n\t\t\tresponse = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.NOT_FOUND, message);\n\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_TYPE, \"text/plain\");\n\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_LENGTH, response.content().readableBytes());\n\t\t\tLOG.debug(\"Error while handling request\", e);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tbyte[] bytes = ExceptionUtils.stringifyException(e).getBytes(ENCODING);\n\t\t\tresponse = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1,\n\t\t\t\t\tHttpResponseStatus.INTERNAL_SERVER_ERROR, Unpooled.wrappedBuffer(bytes));\n\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_TYPE, \"text/plain\");\n\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_LENGTH, response.content().readableBytes());\n\n\t\t\tLOG.debug(\"Error while handling request\", e);\n\t\t}\n\n\t\tresponse.headers().set(HttpHeaders.Names.ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n\n\t\tKeepAliveWrite.flush(ctx, routed.request(), response);\n\t}",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 +\n 121 +\n 122  \n 123  \n 124  ",
            "\t@Override\n\tprotected void respondAsLeader(ChannelHandlerContext ctx, Routed routed, ActorGateway jobManager) {\n\t\tFullHttpResponse response;\n\n\t\ttry {\n\t\t\t// we only pass the first element in the list to the handlers.\n\t\t\tMap<String, String> queryParams = new HashMap<>();\n\t\t\tfor (String key : routed.queryParams().keySet()) {\n\t\t\t\tqueryParams.put(key, routed.queryParam(key));\n\t\t\t}\n\n\t\t\tMap<String, String> pathParams = new HashMap<>(routed.pathParams().size());\n\t\t\tfor (String key : routed.pathParams().keySet()) {\n\t\t\t\tpathParams.put(key, URLDecoder.decode(routed.pathParams().get(key), ENCODING.toString()));\n\t\t\t}\n\n\t\t\tInetSocketAddress address = (InetSocketAddress) ctx.channel().localAddress();\n\t\t\tqueryParams.put(WEB_MONITOR_ADDRESS_KEY,\n\t\t\t\t(httpsEnabled ? \"https://\" : \"http://\") + address.getHostName() + \":\" + address.getPort());\n\n\t\t\tresponse = handler.handleRequest(pathParams, queryParams, jobManager);\n\t\t}\n\t\tcatch (NotFoundException e) {\n\t\t\t// this should result in a 404 error code (not found)\n\t\t\tByteBuf message = e.getMessage() == null ? Unpooled.buffer(0)\n\t\t\t\t\t: Unpooled.wrappedBuffer(e.getMessage().getBytes(ENCODING));\n\t\t\tresponse = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1, HttpResponseStatus.NOT_FOUND, message);\n\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_TYPE, \"text/plain\");\n\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_LENGTH, response.content().readableBytes());\n\t\t\tLOG.debug(\"Error while handling request\", e);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tbyte[] bytes = ExceptionUtils.stringifyException(e).getBytes(ENCODING);\n\t\t\tresponse = new DefaultFullHttpResponse(HttpVersion.HTTP_1_1,\n\t\t\t\t\tHttpResponseStatus.INTERNAL_SERVER_ERROR, Unpooled.wrappedBuffer(bytes));\n\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_TYPE, \"text/plain\");\n\t\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_LENGTH, response.content().readableBytes());\n\n\t\t\tLOG.debug(\"Error while handling request\", e);\n\t\t}\n\n\t\tresponse.headers().set(HttpHeaders.Names.ACCESS_CONTROL_ALLOW_ORIGIN, \"*\");\n\t\t// Content-Encoding:utf-8\n\t\tresponse.headers().set(HttpHeaders.Names.CONTENT_ENCODING, ENCODING.name());\n\n\t\tKeepAliveWrite.flush(ctx, routed.request(), response);\n\t}"
        ]
    ],
    "30bb958a73e74ced94548897962740a59677dc12": [
        [
            "PythonSender::sendBuffer(Iterator,int)",
            " 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 -\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "\t/**\n\t * Extracts records from an iterator and writes them to the memory-mapped file. This method assumes that all values\n\t * in the iterator are of the same type. This method does NOT take care of synchronization. The caller must\n\t * guarantee that the file may be written to before calling this method.\n\t *\n\t * @param i iterator containing records\n\t * @param group group to which the iterator belongs, most notably used by CoGroup-functions.\n\t * @return size of the written buffer\n\t * @throws IOException\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic int sendBuffer(Iterator i, int group) throws IOException {\n\t\tfileBuffer.clear();\n\n\t\tObject value;\n\t\tByteBuffer bb;\n\t\tif (serializer[group] == null) {\n\t\t\tvalue = i.next();\n\t\t\tserializer[group] = getSerializer(value);\n\t\t\tbb = serializer[group].serialize(value);\n\t\t\tif (bb.remaining() > MAPPED_FILE_SIZE) {\n\t\t\t\tthrow new RuntimeException(\"Serialized object does not fit into a single buffer.\");\n\t\t\t}\n\t\t\tfileBuffer.put(bb);\n\n\t\t}\n\t\tif (saved[group] != null) {\n\t\t\tfileBuffer.put(saved[group]);\n\t\t\tsaved[group] = null;\n\t\t}\n\t\twhile (i.hasNext() && saved[group] == null) {\n\t\t\tvalue = i.next();\n\t\t\tbb = serializer[group].serialize(value);\n\t\t\tif (bb.remaining() > MAPPED_FILE_SIZE) {\n\t\t\t\tthrow new RuntimeException(\"Serialized object does not fit into a single buffer.\");\n\t\t\t}\n\t\t\tif (bb.remaining() <= fileBuffer.remaining()) {\n\t\t\t\tfileBuffer.put(bb);\n\t\t\t} else {\n\t\t\t\tsaved[group] = bb;\n\t\t\t}\n\t\t}\n\n\t\tint size = fileBuffer.position();\n\t\treturn size;\n\t}",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 +\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "\t/**\n\t * Extracts records from an iterator and writes them to the memory-mapped file. This method assumes that all values\n\t * in the iterator are of the same type. This method does NOT take care of synchronization. The caller must\n\t * guarantee that the file may be written to before calling this method.\n\t *\n\t * @param i iterator containing records\n\t * @param group group to which the iterator belongs, most notably used by CoGroup-functions.\n\t * @return size of the written buffer\n\t * @throws IOException\n\t */\n\t@SuppressWarnings(\"unchecked\")\n\tpublic int sendBuffer(Iterator<?> i, int group) throws IOException {\n\t\tfileBuffer.clear();\n\n\t\tObject value;\n\t\tByteBuffer bb;\n\t\tif (serializer[group] == null) {\n\t\t\tvalue = i.next();\n\t\t\tserializer[group] = getSerializer(value);\n\t\t\tbb = serializer[group].serialize(value);\n\t\t\tif (bb.remaining() > MAPPED_FILE_SIZE) {\n\t\t\t\tthrow new RuntimeException(\"Serialized object does not fit into a single buffer.\");\n\t\t\t}\n\t\t\tfileBuffer.put(bb);\n\n\t\t}\n\t\tif (saved[group] != null) {\n\t\t\tfileBuffer.put(saved[group]);\n\t\t\tsaved[group] = null;\n\t\t}\n\t\twhile (i.hasNext() && saved[group] == null) {\n\t\t\tvalue = i.next();\n\t\t\tbb = serializer[group].serialize(value);\n\t\t\tif (bb.remaining() > MAPPED_FILE_SIZE) {\n\t\t\t\tthrow new RuntimeException(\"Serialized object does not fit into a single buffer.\");\n\t\t\t}\n\t\t\tif (bb.remaining() <= fileBuffer.remaining()) {\n\t\t\t\tfileBuffer.put(bb);\n\t\t\t} else {\n\t\t\t\tsaved[group] = bb;\n\t\t\t}\n\t\t}\n\n\t\tint size = fileBuffer.position();\n\t\treturn size;\n\t}"
        ],
        [
            "PythonPlanStreamer::close()",
            "  93  \n  94  \n  95  \n  96 -\n  97 -\n  98  \n  99  \n 100  ",
            "\tpublic void close() {\n\t\ttry {\n\t\t\tprocess.exitValue();\n\t\t} catch (NullPointerException npe) { //exception occurred before process was started\n\t\t} catch (IllegalThreadStateException ise) { //process still active\n\t\t\tprocess.destroy();\n\t\t}\n\t}",
            "  97  \n  98  \n  99  \n 100 +\n 101 +\n 102  \n 103 +\n 104 +\n 105 +\n 106 +\n 107 +\n 108 +\n 109  \n 110  ",
            "\tpublic void close() {\n\t\ttry {\n\t\t\tprocess.exitValue();\n\t\t} catch (NullPointerException ignored) { //exception occurred before process was started\n\t\t} catch (IllegalThreadStateException ignored) { //process still active\n\t\t\tprocess.destroy();\n\t\t} finally {\n\t\t\ttry {\n\t\t\t\tsocket.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.error(\"Failed to close socket.\", e);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "PythonSender::setupMappedFile(String)",
            "  51 -\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  ",
            "\tprivate void setupMappedFile(String outputFilePath) throws FileNotFoundException, IOException {\n\t\tFile x = new File(FLINK_TMP_DATA_DIR);\n\t\tx.mkdirs();\n\n\t\toutputFile = new File(outputFilePath);\n\t\tif (outputFile.exists()) {\n\t\t\toutputFile.delete();\n\t\t}\n\t\toutputFile.createNewFile();\n\t\toutputRAF = new RandomAccessFile(outputFilePath, \"rw\");\n\t\toutputRAF.setLength(MAPPED_FILE_SIZE);\n\t\toutputRAF.seek(MAPPED_FILE_SIZE - 1);\n\t\toutputRAF.writeByte(0);\n\t\toutputRAF.seek(0);\n\t\toutputChannel = outputRAF.getChannel();\n\t\tfileBuffer = outputChannel.map(FileChannel.MapMode.READ_WRITE, 0, MAPPED_FILE_SIZE);\n\t}",
            "  55 +\n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  ",
            "\tprivate void setupMappedFile(String outputFilePath) throws IOException {\n\t\tFile x = new File(FLINK_TMP_DATA_DIR);\n\t\tx.mkdirs();\n\n\t\toutputFile = new File(outputFilePath);\n\t\tif (outputFile.exists()) {\n\t\t\toutputFile.delete();\n\t\t}\n\t\toutputFile.createNewFile();\n\t\toutputRAF = new RandomAccessFile(outputFilePath, \"rw\");\n\t\toutputRAF.setLength(MAPPED_FILE_SIZE);\n\t\toutputRAF.seek(MAPPED_FILE_SIZE - 1);\n\t\toutputRAF.writeByte(0);\n\t\toutputRAF.seek(0);\n\t\toutputChannel = outputRAF.getChannel();\n\t\tfileBuffer = outputChannel.map(FileChannel.MapMode.READ_WRITE, 0, MAPPED_FILE_SIZE);\n\t}"
        ],
        [
            "PythonSender::getSerializer(Object)",
            " 168 -\n 169  \n 170  \n 171  \n 172 -\n 173  \n 174  \n 175 -\n 176  \n 177  \n 178 -\n 179  ",
            "\tprivate Serializer getSerializer(Object value) {\n\t\tif (value instanceof byte[]) {\n\t\t\treturn new ArraySerializer();\n\t\t}\n\t\tif (((Tuple2) value).f0 instanceof byte[]) {\n\t\t\treturn new ValuePairSerializer();\n\t\t}\n\t\tif (((Tuple2) value).f0 instanceof Tuple) {\n\t\t\treturn new KeyValuePairSerializer();\n\t\t}\n\t\tthrow new IllegalArgumentException(\"This object can't be serialized: \" + value.toString());\n\t}",
            " 173 +\n 174  \n 175  \n 176  \n 177 +\n 178  \n 179  \n 180 +\n 181  \n 182  \n 183 +\n 184  ",
            "\tprivate Serializer<?> getSerializer(Object value) {\n\t\tif (value instanceof byte[]) {\n\t\t\treturn new ArraySerializer();\n\t\t}\n\t\tif (((Tuple2<?, ?>) value).f0 instanceof byte[]) {\n\t\t\treturn new ValuePairSerializer();\n\t\t}\n\t\tif (((Tuple2<?, ?>) value).f0 instanceof Tuple) {\n\t\t\treturn new KeyValuePairSerializer();\n\t\t}\n\t\tthrow new IllegalArgumentException(\"This object can't be serialized: \" + value);\n\t}"
        ],
        [
            "PythonMapPartition::PythonMapPartition(int,TypeInformation)",
            "  35  \n  36  \n  37 -\n  38  ",
            "\tpublic PythonMapPartition(int id, TypeInformation<OUT> typeInformation) {\n\t\tthis.typeInformation = typeInformation;\n\t\tstreamer = new PythonStreamer(this, id, typeInformation instanceof PrimitiveArrayTypeInfo);\n\t}",
            "  38  \n  39  \n  40 +\n  41  ",
            "\tpublic PythonMapPartition(int id, TypeInformation<OUT> typeInformation) {\n\t\tthis.typeInformation = typeInformation;\n\t\tstreamer = new PythonStreamer<>(this, id, typeInformation instanceof PrimitiveArrayTypeInfo);\n\t}"
        ],
        [
            "PythonStreamer::startPython()",
            "  99  \n 100 -\n 101 -\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113 -\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  \n 150 -\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "\tprivate void startPython() throws IOException {\n\t\tthis.outputFilePath = FLINK_TMP_DATA_DIR + \"/\" + id + this.function.getRuntimeContext().getIndexOfThisSubtask() + \"output\";\n\t\tthis.inputFilePath = FLINK_TMP_DATA_DIR + \"/\" + id + this.function.getRuntimeContext().getIndexOfThisSubtask() + \"input\";\n\n\t\tsender.open(inputFilePath);\n\t\treceiver.open(outputFilePath);\n\n\t\tString path = function.getRuntimeContext().getDistributedCache().getFile(FLINK_PYTHON_DC_ID).getAbsolutePath();\n\t\tString planPath = path + FLINK_PYTHON_PLAN_NAME;\n\n\t\tString pythonBinaryPath = usePython3 ? FLINK_PYTHON3_BINARY_PATH : FLINK_PYTHON2_BINARY_PATH;\n\n\t\ttry {\n\t\t\tRuntime.getRuntime().exec(pythonBinaryPath);\n\t\t} catch (IOException ex) {\n\t\t\tthrow new RuntimeException(pythonBinaryPath + \" does not point to a valid python binary.\");\n\t\t}\n\n\t\tprocess = Runtime.getRuntime().exec(pythonBinaryPath + \" -O -B \" + planPath + planArguments);\n\t\tnew StreamPrinter(process.getInputStream()).start();\n\t\tnew StreamPrinter(process.getErrorStream(), true, msg).start();\n\n\t\tshutdownThread = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tdestroyProcess();\n\t\t\t\t} catch (IOException ex) {\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\n\t\tRuntime.getRuntime().addShutdownHook(shutdownThread);\n\n\t\tOutputStream processOutput = process.getOutputStream();\n\t\tprocessOutput.write(\"operator\\n\".getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((\"\" + server.getLocalPort() + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((id + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((this.function.getRuntimeContext().getIndexOfThisSubtask() + \"\\n\")\n\t\t\t.getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((inputFilePath + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((outputFilePath + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.flush();\n\n\t\ttry { // wait a bit to catch syntax errors\n\t\t\tThread.sleep(2000);\n\t\t} catch (InterruptedException ex) {\n\t\t}\n\t\ttry {\n\t\t\tprocess.exitValue();\n\t\t\tthrow new RuntimeException(\"External process for task \" + function.getRuntimeContext().getTaskName() + \" terminated prematurely.\" + msg);\n\t\t} catch (IllegalThreadStateException ise) { //process still active -> start receiving data\n\t\t}\n\n\t\twhile (true) {\n\t\t\ttry {\n\t\t\t\tsocket = server.accept();\n\t\t\t\tbreak;\n\t\t\t} catch (SocketTimeoutException ignored) {\n\t\t\t\tcheckPythonProcessHealth();\n\t\t\t}\n\t\t}\n\t\tin = new DataInputStream(socket.getInputStream());\n\t\tout = new DataOutputStream(socket.getOutputStream());\n\t}",
            "  98  \n  99 +\n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 +\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144 +\n 145  \n 146  \n 147  \n 148  \n 149 +\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "\tprivate void startPython() throws IOException {\n\t\tString outputFilePath = FLINK_TMP_DATA_DIR + \"/\" + id + this.function.getRuntimeContext().getIndexOfThisSubtask() + \"output\";\n\t\tString inputFilePath = FLINK_TMP_DATA_DIR + \"/\" + id + this.function.getRuntimeContext().getIndexOfThisSubtask() + \"input\";\n\n\t\tsender.open(inputFilePath);\n\t\treceiver.open(outputFilePath);\n\n\t\tString path = function.getRuntimeContext().getDistributedCache().getFile(FLINK_PYTHON_DC_ID).getAbsolutePath();\n\t\tString planPath = path + FLINK_PYTHON_PLAN_NAME;\n\n\t\tString pythonBinaryPath = usePython3 ? FLINK_PYTHON3_BINARY_PATH : FLINK_PYTHON2_BINARY_PATH;\n\n\t\ttry {\n\t\t\tRuntime.getRuntime().exec(pythonBinaryPath);\n\t\t} catch (IOException ignored) {\n\t\t\tthrow new RuntimeException(pythonBinaryPath + \" does not point to a valid python binary.\");\n\t\t}\n\n\t\tprocess = Runtime.getRuntime().exec(pythonBinaryPath + \" -O -B \" + planPath + planArguments);\n\t\tnew StreamPrinter(process.getInputStream()).start();\n\t\tnew StreamPrinter(process.getErrorStream(), true, msg).start();\n\n\t\tshutdownThread = new Thread() {\n\t\t\t@Override\n\t\t\tpublic void run() {\n\t\t\t\ttry {\n\t\t\t\t\tdestroyProcess();\n\t\t\t\t} catch (IOException ignored) {\n\t\t\t\t}\n\t\t\t}\n\t\t};\n\n\t\tRuntime.getRuntime().addShutdownHook(shutdownThread);\n\n\t\tOutputStream processOutput = process.getOutputStream();\n\t\tprocessOutput.write(\"operator\\n\".getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((\"\" + server.getLocalPort() + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((id + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((this.function.getRuntimeContext().getIndexOfThisSubtask() + \"\\n\")\n\t\t\t.getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((inputFilePath + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.write((outputFilePath + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocessOutput.flush();\n\n\t\ttry { // wait a bit to catch syntax errors\n\t\t\tThread.sleep(2000);\n\t\t} catch (InterruptedException ignored) {\n\t\t}\n\t\ttry {\n\t\t\tprocess.exitValue();\n\t\t\tthrow new RuntimeException(\"External process for task \" + function.getRuntimeContext().getTaskName() + \" terminated prematurely.\" + msg);\n\t\t} catch (IllegalThreadStateException ignored) { //process still active -> start receiving data\n\t\t}\n\n\t\twhile (true) {\n\t\t\ttry {\n\t\t\t\tsocket = server.accept();\n\t\t\t\tbreak;\n\t\t\t} catch (SocketTimeoutException ignored) {\n\t\t\t\tcheckPythonProcessHealth();\n\t\t\t}\n\t\t}\n\t\tin = new DataInputStream(socket.getInputStream());\n\t\tout = new DataOutputStream(socket.getOutputStream());\n\t}"
        ],
        [
            "PythonSender::closeMappedFile()",
            "  73  \n  74  \n  75  \n  76  ",
            "\tprivate void closeMappedFile() throws IOException {\n\t\toutputChannel.close();\n\t\toutputRAF.close();\n\t}",
            "  77  \n  78  \n  79  \n  80 +\n  81  ",
            "\tprivate void closeMappedFile() throws IOException {\n\t\toutputChannel.close();\n\t\toutputRAF.close();\n\t\toutputFile.delete();\n\t}"
        ],
        [
            "PythonPlanStreamer::checkPythonProcessHealth()",
            " 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108 -\n 109  \n 110  \n 111 -\n 112  \n 113  ",
            "\tprivate void checkPythonProcessHealth() {\n\t\ttry {\n\t\t\tint value = process.exitValue();\n\t\t\tif (value != 0) {\n\t\t\t\tthrow new RuntimeException(\"Plan file caused an error. Check log-files for details.\");\n\t\t\t}\n\t\t\tif (value == 0) {\n\t\t\t\tthrow new RuntimeException(\"Plan file exited prematurely without an error.\");\n\t\t\t}\n\t\t} catch (IllegalThreadStateException ise) {//Process still running\n\t\t}\n\t}",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117 +\n 118  \n 119  \n 120 +\n 121  \n 122  ",
            "\tprivate void checkPythonProcessHealth() {\n\t\ttry {\n\t\t\tint value = process.exitValue();\n\t\t\tif (value != 0) {\n\t\t\t\tthrow new RuntimeException(\"Plan file caused an error. Check log-files for details.\");\n\t\t\t} else {\n\t\t\t\tthrow new RuntimeException(\"Plan file exited prematurely without an error.\");\n\t\t\t}\n\t\t} catch (IllegalThreadStateException ignored) {//Process still running\n\t\t}\n\t}"
        ],
        [
            "PythonCoGroup::PythonCoGroup(int,TypeInformation)",
            "  34  \n  35  \n  36 -\n  37  ",
            "\tpublic PythonCoGroup(int id, TypeInformation<OUT> typeInformation) {\n\t\tthis.typeInformation = typeInformation;\n\t\tstreamer = new PythonStreamer(this, id, true);\n\t}",
            "  37  \n  38  \n  39 +\n  40  ",
            "\tpublic PythonCoGroup(int id, TypeInformation<OUT> typeInformation) {\n\t\tthis.typeInformation = typeInformation;\n\t\tstreamer = new PythonStreamer<>(this, id, true);\n\t}"
        ],
        [
            "PythonReceiver::collectBuffer(Collector,int)",
            "  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "\t/**\n\t * Reads a buffer of the given size from the memory-mapped file, and collects all records contained. This method\n\t * assumes that all values in the buffer are of the same type. This method does NOT take care of synchronization.\n\t * The user must guarantee that the buffer was completely written before calling this method.\n\t *\n\t * @param c Collector to collect records\n\t * @param bufferSize size of the buffer\n\t * @throws IOException\n\t */\n\t@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n\tpublic void collectBuffer(Collector c, int bufferSize) throws IOException {\n\t\tfileBuffer.position(0);\n\n\t\twhile (fileBuffer.position() < bufferSize) {\n\t\t\tc.collect(deserializer.deserialize());\n\t\t}\n\t}",
            "  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 +\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  ",
            "\t/**\n\t * Reads a buffer of the given size from the memory-mapped file, and collects all records contained. This method\n\t * assumes that all values in the buffer are of the same type. This method does NOT take care of synchronization.\n\t * The user must guarantee that the buffer was completely written before calling this method.\n\t *\n\t * @param c Collector to collect records\n\t * @param bufferSize size of the buffer\n\t * @throws IOException\n\t */\n\t@SuppressWarnings({ \"rawtypes\", \"unchecked\" })\n\tpublic void collectBuffer(Collector<OUT> c, int bufferSize) throws IOException {\n\t\tfileBuffer.position(0);\n\n\t\twhile (fileBuffer.position() < bufferSize) {\n\t\t\tc.collect(deserializer.deserialize());\n\t\t}\n\t}"
        ],
        [
            "PythonStreamer::checkPythonProcessHealth()",
            " 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171 -\n 172  \n 173  \n 174 -\n 175  \n 176  ",
            "\tprivate void checkPythonProcessHealth() {\n\t\ttry {\n\t\t\tint value = process.exitValue();\n\t\t\tif (value != 0) {\n\t\t\t\tthrow new RuntimeException(\"Plan file caused an error. Check log-files for details.\");\n\t\t\t}\n\t\t\tif (value == 0) {\n\t\t\t\tthrow new RuntimeException(\"Plan file exited prematurely without an error.\");\n\t\t\t}\n\t\t} catch (IllegalThreadStateException ise) {//Process still running\n\t\t}\n\t}",
            " 164  \n 165  \n 166  \n 167  \n 168  \n 169 +\n 170  \n 171  \n 172 +\n 173  \n 174  ",
            "\tprivate void checkPythonProcessHealth() {\n\t\ttry {\n\t\t\tint value = process.exitValue();\n\t\t\tif (value != 0) {\n\t\t\t\tthrow new RuntimeException(\"Plan file caused an error. Check log-files for details.\");\n\t\t\t} else {\n\t\t\t\tthrow new RuntimeException(\"Plan file exited prematurely without an error.\");\n\t\t\t}\n\t\t} catch (IllegalThreadStateException ignored) {//Process still running\n\t\t}\n\t}"
        ],
        [
            "PythonPlanStreamer::startPython(String,String)",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "\tprivate void startPython(String tmpPath, String args) throws IOException {\n\t\tString pythonBinaryPath = usePython3 ? FLINK_PYTHON3_BINARY_PATH : FLINK_PYTHON2_BINARY_PATH;\n\n\t\ttry {\n\t\t\tRuntime.getRuntime().exec(pythonBinaryPath);\n\t\t} catch (IOException ex) {\n\t\t\tthrow new RuntimeException(pythonBinaryPath + \" does not point to a valid python binary.\");\n\t\t}\n\t\tprocess = Runtime.getRuntime().exec(pythonBinaryPath + \" -B \" + tmpPath + FLINK_PYTHON_PLAN_NAME + args);\n\n\t\tnew StreamPrinter(process.getInputStream()).start();\n\t\tnew StreamPrinter(process.getErrorStream()).start();\n\n\t\ttry {\n\t\t\tThread.sleep(2000);\n\t\t} catch (InterruptedException ex) {\n\t\t}\n\n\t\tcheckPythonProcessHealth();\n\n\t\tprocess.getOutputStream().write(\"plan\\n\".getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocess.getOutputStream().write((server.getLocalPort() + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocess.getOutputStream().flush();\n\t}",
            "  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tprivate void startPython(String tmpPath, String args) throws IOException {\n\t\tString pythonBinaryPath = usePython3 ? FLINK_PYTHON3_BINARY_PATH : FLINK_PYTHON2_BINARY_PATH;\n\n\t\ttry {\n\t\t\tRuntime.getRuntime().exec(pythonBinaryPath);\n\t\t} catch (IOException ignored) {\n\t\t\tthrow new RuntimeException(pythonBinaryPath + \" does not point to a valid python binary.\");\n\t\t}\n\t\tprocess = Runtime.getRuntime().exec(pythonBinaryPath + \" -B \" + tmpPath + FLINK_PYTHON_PLAN_NAME + args);\n\n\t\tnew StreamPrinter(process.getInputStream()).start();\n\t\tnew StreamPrinter(process.getErrorStream()).start();\n\n\t\ttry {\n\t\t\tThread.sleep(2000);\n\t\t} catch (InterruptedException ignored) {\n\t\t}\n\n\t\tcheckPythonProcessHealth();\n\n\t\tprocess.getOutputStream().write(\"plan\\n\".getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocess.getOutputStream().write((server.getLocalPort() + \"\\n\").getBytes(ConfigConstants.DEFAULT_CHARSET));\n\t\tprocess.getOutputStream().flush();\n\t}"
        ],
        [
            "PythonStreamer::sendBroadCastVariables(Configuration)",
            " 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250 -\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  ",
            "\t/**\n\t * Sends all broadcast-variables encoded in the configuration to the external process.\n\t *\n\t * @param config configuration object containing broadcast-variable count and names\n\t * @throws IOException\n\t */\n\tpublic final void sendBroadCastVariables(Configuration config) throws IOException {\n\t\ttry {\n\t\t\tint broadcastCount = config.getInteger(PLANBINDER_CONFIG_BCVAR_COUNT, 0);\n\n\t\t\tString[] names = new String[broadcastCount];\n\n\t\t\tfor (int x = 0; x < names.length; x++) {\n\t\t\t\tnames[x] = config.getString(PLANBINDER_CONFIG_BCVAR_NAME_PREFIX + x, null);\n\t\t\t}\n\n\t\t\tout.write(new IntSerializer().serializeWithoutTypeInfo(broadcastCount));\n\n\t\t\tStringSerializer stringSerializer = new StringSerializer();\n\t\t\tfor (String name : names) {\n\t\t\t\tIterator bcv = function.getRuntimeContext().getBroadcastVariable(name).iterator();\n\n\t\t\t\tout.write(stringSerializer.serializeWithoutTypeInfo(name));\n\n\t\t\t\twhile (bcv.hasNext()) {\n\t\t\t\t\tout.writeByte(1);\n\t\t\t\t\tout.write((byte[]) bcv.next());\n\t\t\t\t}\n\t\t\t\tout.writeByte(0);\n\t\t\t}\n\t\t} catch (SocketTimeoutException ste) {\n\t\t\tthrow new RuntimeException(\"External process for task \" + function.getRuntimeContext().getTaskName() + \" stopped responding.\" + msg);\n\t\t}\n\t}",
            " 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248 +\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  ",
            "\t/**\n\t * Sends all broadcast-variables encoded in the configuration to the external process.\n\t *\n\t * @param config configuration object containing broadcast-variable count and names\n\t * @throws IOException\n\t */\n\tpublic final void sendBroadCastVariables(Configuration config) throws IOException {\n\t\ttry {\n\t\t\tint broadcastCount = config.getInteger(PLANBINDER_CONFIG_BCVAR_COUNT, 0);\n\n\t\t\tString[] names = new String[broadcastCount];\n\n\t\t\tfor (int x = 0; x < names.length; x++) {\n\t\t\t\tnames[x] = config.getString(PLANBINDER_CONFIG_BCVAR_NAME_PREFIX + x, null);\n\t\t\t}\n\n\t\t\tout.write(new IntSerializer().serializeWithoutTypeInfo(broadcastCount));\n\n\t\t\tStringSerializer stringSerializer = new StringSerializer();\n\t\t\tfor (String name : names) {\n\t\t\t\tIterator<?> bcv = function.getRuntimeContext().getBroadcastVariable(name).iterator();\n\n\t\t\t\tout.write(stringSerializer.serializeWithoutTypeInfo(name));\n\n\t\t\t\twhile (bcv.hasNext()) {\n\t\t\t\t\tout.writeByte(1);\n\t\t\t\t\tout.write((byte[]) bcv.next());\n\t\t\t\t}\n\t\t\t\tout.writeByte(0);\n\t\t\t}\n\t\t} catch (SocketTimeoutException ste) {\n\t\t\tthrow new RuntimeException(\"External process for task \" + function.getRuntimeContext().getTaskName() + \" stopped responding.\" + msg);\n\t\t}\n\t}"
        ],
        [
            "PythonPlanReceiver::getDeserializer()",
            "  52  \n  53 -\n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\tprivate Deserializer getDeserializer() throws IOException {\n\t\tbyte type = (byte) input.readByte();\n\t\tif (type >= 0 && type < 26) {\n\t\t\t\tDeserializer[] d = new Deserializer[type];\n\t\t\t\tfor (int x = 0; x < d.length; x++) {\n\t\t\t\t\td[x] = getDeserializer();\n\t\t\t\t}\n\t\t\t\treturn new TupleDeserializer(d);\n\t\t}\n\t\tswitch (type) {\n\t\t\tcase TYPE_BOOLEAN:\n\t\t\t\treturn new BooleanDeserializer();\n\t\t\tcase TYPE_BYTE:\n\t\t\t\treturn new ByteDeserializer();\n\t\t\tcase TYPE_INTEGER:\n\t\t\t\treturn new IntDeserializer();\n\t\t\tcase TYPE_LONG:\n\t\t\t\treturn new LongDeserializer();\n\t\t\tcase TYPE_FLOAT:\n\t\t\t\treturn new FloatDeserializer();\n\t\t\tcase TYPE_DOUBLE:\n\t\t\t\treturn new DoubleDeserializer();\n\t\t\tcase TYPE_STRING:\n\t\t\t\treturn new StringDeserializer();\n\t\t\tcase TYPE_BYTES:\n\t\t\t\treturn new BytesDeserializer();\n\t\t\tcase TYPE_NULL:\n\t\t\t\treturn new NullDeserializer();\n\t\t\tdefault:\n\t\t\t\treturn new CustomTypeDeserializer(type);\n\t\t}\n\t}",
            "  51  \n  52 +\n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "\tprivate Deserializer getDeserializer() throws IOException {\n\t\tbyte type = input.readByte();\n\t\tif (type >= 0 && type < 26) {\n\t\t\t\tDeserializer[] d = new Deserializer[type];\n\t\t\t\tfor (int x = 0; x < d.length; x++) {\n\t\t\t\t\td[x] = getDeserializer();\n\t\t\t\t}\n\t\t\t\treturn new TupleDeserializer(d);\n\t\t}\n\t\tswitch (type) {\n\t\t\tcase TYPE_BOOLEAN:\n\t\t\t\treturn new BooleanDeserializer();\n\t\t\tcase TYPE_BYTE:\n\t\t\t\treturn new ByteDeserializer();\n\t\t\tcase TYPE_INTEGER:\n\t\t\t\treturn new IntDeserializer();\n\t\t\tcase TYPE_LONG:\n\t\t\t\treturn new LongDeserializer();\n\t\t\tcase TYPE_FLOAT:\n\t\t\t\treturn new FloatDeserializer();\n\t\t\tcase TYPE_DOUBLE:\n\t\t\t\treturn new DoubleDeserializer();\n\t\t\tcase TYPE_STRING:\n\t\t\t\treturn new StringDeserializer();\n\t\t\tcase TYPE_BYTES:\n\t\t\t\treturn new BytesDeserializer();\n\t\t\tcase TYPE_NULL:\n\t\t\t\treturn new NullDeserializer();\n\t\t\tdefault:\n\t\t\t\treturn new CustomTypeDeserializer(type);\n\t\t}\n\t}"
        ],
        [
            "PythonStreamer::streamBufferWithoutGroups(Iterator,Collector)",
            " 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 -\n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292 -\n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 -\n 304  \n 305  \n 306  ",
            "\t/**\n\t * Sends all values contained in the iterator to the external process and collects all results.\n\t *\n\t * @param i iterator\n\t * @param c collector\n\t * @throws IOException\n\t */\n\tpublic final void streamBufferWithoutGroups(Iterator i, Collector c) throws IOException {\n\t\ttry {\n\t\t\tint size;\n\t\t\tif (i.hasNext()) {\n\t\t\t\twhile (true) {\n\t\t\t\t\tint sig = in.readInt();\n\t\t\t\t\tswitch (sig) {\n\t\t\t\t\t\tcase SIGNAL_BUFFER_REQUEST:\n\t\t\t\t\t\t\tif (i.hasNext() || sender.hasRemaining(0)) {\n\t\t\t\t\t\t\t\tsize = sender.sendBuffer(i, 0);\n\t\t\t\t\t\t\t\tsendWriteNotification(size, sender.hasRemaining(0) || i.hasNext());\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tthrow new RuntimeException(\"External process requested data even though none is available.\");\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase SIGNAL_FINISHED:\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\tcase SIGNAL_ERROR:\n\t\t\t\t\t\t\ttry { //wait before terminating to ensure that the complete error message is printed\n\t\t\t\t\t\t\t\tThread.sleep(2000);\n\t\t\t\t\t\t\t} catch (InterruptedException ex) {\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\t\"External process for task \" + function.getRuntimeContext().getTaskName() + \" terminated prematurely due to an error.\" + msg);\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\treceiver.collectBuffer(c, sig);\n\t\t\t\t\t\t\tsendReadConfirmation();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (SocketTimeoutException ste) {\n\t\t\tthrow new RuntimeException(\"External process for task \" + function.getRuntimeContext().getTaskName() + \" stopped responding.\" + msg);\n\t\t}\n\t}",
            " 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270 +\n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290 +\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301 +\n 302  \n 303  \n 304  ",
            "\t/**\n\t * Sends all values contained in the iterator to the external process and collects all results.\n\t *\n\t * @param i iterator\n\t * @param c collector\n\t * @throws IOException\n\t */\n\tpublic final void streamBufferWithoutGroups(Iterator<IN1> i, Collector<OUT> c) throws IOException {\n\t\ttry {\n\t\t\tint size;\n\t\t\tif (i.hasNext()) {\n\t\t\t\twhile (true) {\n\t\t\t\t\tint sig = in.readInt();\n\t\t\t\t\tswitch (sig) {\n\t\t\t\t\t\tcase SIGNAL_BUFFER_REQUEST:\n\t\t\t\t\t\t\tif (i.hasNext() || sender.hasRemaining(0)) {\n\t\t\t\t\t\t\t\tsize = sender.sendBuffer(i, 0);\n\t\t\t\t\t\t\t\tsendWriteNotification(size, sender.hasRemaining(0) || i.hasNext());\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tthrow new RuntimeException(\"External process requested data even though none is available.\");\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase SIGNAL_FINISHED:\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\tcase SIGNAL_ERROR:\n\t\t\t\t\t\t\ttry { //wait before terminating to ensure that the complete error message is printed\n\t\t\t\t\t\t\t\tThread.sleep(2000);\n\t\t\t\t\t\t\t} catch (InterruptedException ignored) {\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\t\"External process for task \" + function.getRuntimeContext().getTaskName() + \" terminated prematurely due to an error.\" + msg);\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\treceiver.collectBuffer(c, sig);\n\t\t\t\t\t\t\tsendReadConfirmation();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (SocketTimeoutException ignored) {\n\t\t\tthrow new RuntimeException(\"External process for task \" + function.getRuntimeContext().getTaskName() + \" stopped responding.\" + msg);\n\t\t}\n\t}"
        ],
        [
            "PythonStreamer::close()",
            " 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 -\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  ",
            "\t/**\n\t * Closes this streamer.\n\t *\n\t * @throws IOException\n\t */\n\tpublic void close() throws IOException {\n\t\ttry {\n\t\t\tsocket.close();\n\t\t\tsender.close();\n\t\t\treceiver.close();\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"Exception occurred while closing Streamer. :\" + e.getMessage());\n\t\t}\n\t\tdestroyProcess();\n\t\tif (shutdownThread != null) {\n\t\t\tRuntime.getRuntime().removeShutdownHook(shutdownThread);\n\t\t}\n\t}",
            " 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 +\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "\t/**\n\t * Closes this streamer.\n\t *\n\t * @throws IOException\n\t */\n\tpublic void close() throws IOException {\n\t\ttry {\n\t\t\tsocket.close();\n\t\t\tsender.close();\n\t\t\treceiver.close();\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"Exception occurred while closing Streamer. :{}\", e.getMessage());\n\t\t}\n\t\tdestroyProcess();\n\t\tif (shutdownThread != null) {\n\t\t\tRuntime.getRuntime().removeShutdownHook(shutdownThread);\n\t\t}\n\t}"
        ],
        [
            "PythonStreamer::destroyProcess()",
            " 197  \n 198  \n 199  \n 200 -\n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211 -\n 212  \n 213  \n 214  \n 215  \n 216  \n 217  ",
            "\tprivate void destroyProcess() throws IOException {\n\t\ttry {\n\t\t\tprocess.exitValue();\n\t\t} catch (IllegalThreadStateException ise) { //process still active\n\t\t\tif (process.getClass().getName().equals(\"java.lang.UNIXProcess\")) {\n\t\t\t\tint pid;\n\t\t\t\ttry {\n\t\t\t\t\tField f = process.getClass().getDeclaredField(\"pid\");\n\t\t\t\t\tf.setAccessible(true);\n\t\t\t\t\tpid = f.getInt(process);\n\t\t\t\t} catch (Throwable e) {\n\t\t\t\t\tprocess.destroy();\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tString[] args = new String[]{\"kill\", \"-9\", \"\" + pid};\n\t\t\t\tRuntime.getRuntime().exec(args);\n\t\t\t} else {\n\t\t\t\tprocess.destroy();\n\t\t\t}\n\t\t}\n\t}",
            " 195  \n 196  \n 197  \n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209 +\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  ",
            "\tprivate void destroyProcess() throws IOException {\n\t\ttry {\n\t\t\tprocess.exitValue();\n\t\t} catch (IllegalThreadStateException ignored) { //process still active\n\t\t\tif (process.getClass().getName().equals(\"java.lang.UNIXProcess\")) {\n\t\t\t\tint pid;\n\t\t\t\ttry {\n\t\t\t\t\tField f = process.getClass().getDeclaredField(\"pid\");\n\t\t\t\t\tf.setAccessible(true);\n\t\t\t\t\tpid = f.getInt(process);\n\t\t\t\t} catch (Throwable ignore) {\n\t\t\t\t\tprocess.destroy();\n\t\t\t\t\treturn;\n\t\t\t\t}\n\t\t\t\tString[] args = new String[]{\"kill\", \"-9\", String.valueOf(pid)};\n\t\t\t\tRuntime.getRuntime().exec(args);\n\t\t\t} else {\n\t\t\t\tprocess.destroy();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "PythonReceiver::setupMappedFile(String)",
            "  53 -\n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  ",
            "\tprivate void setupMappedFile(String inputFilePath) throws FileNotFoundException, IOException {\n\t\tFile x = new File(FLINK_TMP_DATA_DIR);\n\t\tx.mkdirs();\n\n\t\tinputFile = new File(inputFilePath);\n\t\tif (inputFile.exists()) {\n\t\t\tinputFile.delete();\n\t\t}\n\t\tinputFile.createNewFile();\n\t\tinputRAF = new RandomAccessFile(inputFilePath, \"rw\");\n\t\tinputRAF.setLength(MAPPED_FILE_SIZE);\n\t\tinputRAF.seek(MAPPED_FILE_SIZE - 1);\n\t\tinputRAF.writeByte(0);\n\t\tinputRAF.seek(0);\n\t\tinputChannel = inputRAF.getChannel();\n\t\tfileBuffer = inputChannel.map(FileChannel.MapMode.READ_WRITE, 0, MAPPED_FILE_SIZE);\n\t}",
            "  52 +\n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  ",
            "\tprivate void setupMappedFile(String inputFilePath) throws IOException {\n\t\tFile x = new File(FLINK_TMP_DATA_DIR);\n\t\tx.mkdirs();\n\n\t\tinputFile = new File(inputFilePath);\n\t\tif (inputFile.exists()) {\n\t\t\tinputFile.delete();\n\t\t}\n\t\tinputFile.createNewFile();\n\t\tinputRAF = new RandomAccessFile(inputFilePath, \"rw\");\n\t\tinputRAF.setLength(MAPPED_FILE_SIZE);\n\t\tinputRAF.seek(MAPPED_FILE_SIZE - 1);\n\t\tinputRAF.writeByte(0);\n\t\tinputRAF.seek(0);\n\t\tinputChannel = inputRAF.getChannel();\n\t\tfileBuffer = inputChannel.map(FileChannel.MapMode.READ_WRITE, 0, MAPPED_FILE_SIZE);\n\t}"
        ],
        [
            "PythonReceiver::closeMappedFile()",
            "  75  \n  76  \n  77  \n  78  ",
            "\tprivate void closeMappedFile() throws IOException {\n\t\tinputChannel.close();\n\t\tinputRAF.close();\n\t}",
            "  74  \n  75  \n  76  \n  77 +\n  78  ",
            "\tprivate void closeMappedFile() throws IOException {\n\t\tinputChannel.close();\n\t\tinputRAF.close();\n\t\tinputFile.delete();\n\t}"
        ],
        [
            "PythonSender::open(String)",
            "  47  \n  48  \n  49  ",
            "\tpublic void open(String path) throws IOException {\n\t\tsetupMappedFile(path);\n\t}",
            "  49  \n  50 +\n  51 +\n  52  \n  53  ",
            "\tpublic void open(String path) throws IOException {\n\t\tsaved = new ByteBuffer[2];\n\t\tserializer = new Serializer[2];\n\t\tsetupMappedFile(path);\n\t}"
        ],
        [
            "PythonStreamer::streamBufferWithGroups(Iterator,Iterator,Collector)",
            " 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316 -\n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340 -\n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351 -\n 352  \n 353  \n 354  ",
            "\t/**\n\t * Sends all values contained in both iterators to the external process and collects all results.\n\t *\n\t * @param i1 iterator\n\t * @param i2 iterator\n\t * @param c collector\n\t * @throws IOException\n\t */\n\tpublic final void streamBufferWithGroups(Iterator i1, Iterator i2, Collector c) throws IOException {\n\t\ttry {\n\t\t\tint size;\n\t\t\tif (i1.hasNext() || i2.hasNext()) {\n\t\t\t\twhile (true) {\n\t\t\t\t\tint sig = in.readInt();\n\t\t\t\t\tswitch (sig) {\n\t\t\t\t\t\tcase SIGNAL_BUFFER_REQUEST_G0:\n\t\t\t\t\t\t\tif (i1.hasNext() || sender.hasRemaining(0)) {\n\t\t\t\t\t\t\t\tsize = sender.sendBuffer(i1, 0);\n\t\t\t\t\t\t\t\tsendWriteNotification(size, sender.hasRemaining(0) || i1.hasNext());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase SIGNAL_BUFFER_REQUEST_G1:\n\t\t\t\t\t\t\tif (i2.hasNext() || sender.hasRemaining(1)) {\n\t\t\t\t\t\t\t\tsize = sender.sendBuffer(i2, 1);\n\t\t\t\t\t\t\t\tsendWriteNotification(size, sender.hasRemaining(1) || i2.hasNext());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase SIGNAL_FINISHED:\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\tcase SIGNAL_ERROR:\n\t\t\t\t\t\t\ttry { //wait before terminating to ensure that the complete error message is printed\n\t\t\t\t\t\t\t\tThread.sleep(2000);\n\t\t\t\t\t\t\t} catch (InterruptedException ex) {\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\t\"External process for task \" + function.getRuntimeContext().getTaskName() + \" terminated prematurely due to an error.\" + msg);\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\treceiver.collectBuffer(c, sig);\n\t\t\t\t\t\t\tsendReadConfirmation();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (SocketTimeoutException ste) {\n\t\t\tthrow new RuntimeException(\"External process for task \" + function.getRuntimeContext().getTaskName() + \" stopped responding.\" + msg);\n\t\t}\n\t}",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314 +\n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338 +\n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349 +\n 350  \n 351  \n 352  ",
            "\t/**\n\t * Sends all values contained in both iterators to the external process and collects all results.\n\t *\n\t * @param i1 iterator\n\t * @param i2 iterator\n\t * @param c collector\n\t * @throws IOException\n\t */\n\tpublic final void streamBufferWithGroups(Iterator<IN1> i1, Iterator<IN2> i2, Collector<OUT> c) throws IOException {\n\t\ttry {\n\t\t\tint size;\n\t\t\tif (i1.hasNext() || i2.hasNext()) {\n\t\t\t\twhile (true) {\n\t\t\t\t\tint sig = in.readInt();\n\t\t\t\t\tswitch (sig) {\n\t\t\t\t\t\tcase SIGNAL_BUFFER_REQUEST_G0:\n\t\t\t\t\t\t\tif (i1.hasNext() || sender.hasRemaining(0)) {\n\t\t\t\t\t\t\t\tsize = sender.sendBuffer(i1, 0);\n\t\t\t\t\t\t\t\tsendWriteNotification(size, sender.hasRemaining(0) || i1.hasNext());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase SIGNAL_BUFFER_REQUEST_G1:\n\t\t\t\t\t\t\tif (i2.hasNext() || sender.hasRemaining(1)) {\n\t\t\t\t\t\t\t\tsize = sender.sendBuffer(i2, 1);\n\t\t\t\t\t\t\t\tsendWriteNotification(size, sender.hasRemaining(1) || i2.hasNext());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tcase SIGNAL_FINISHED:\n\t\t\t\t\t\t\treturn;\n\t\t\t\t\t\tcase SIGNAL_ERROR:\n\t\t\t\t\t\t\ttry { //wait before terminating to ensure that the complete error message is printed\n\t\t\t\t\t\t\t\tThread.sleep(2000);\n\t\t\t\t\t\t\t} catch (InterruptedException ignored) {\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tthrow new RuntimeException(\n\t\t\t\t\t\t\t\t\t\"External process for task \" + function.getRuntimeContext().getTaskName() + \" terminated prematurely due to an error.\" + msg);\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\treceiver.collectBuffer(c, sig);\n\t\t\t\t\t\t\tsendReadConfirmation();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (SocketTimeoutException ignored) {\n\t\t\tthrow new RuntimeException(\"External process for task \" + function.getRuntimeContext().getTaskName() + \" stopped responding.\" + msg);\n\t\t}\n\t}"
        ],
        [
            "PythonReceiver::open(String)",
            "  48  \n  49  \n  50 -\n  51  ",
            "\tpublic void open(String path) throws IOException {\n\t\tsetupMappedFile(path);\n\t\tdeserializer = readAsByteArray ? new ByteArrayDeserializer() : new TupleDeserializer();\n\t}",
            "  47  \n  48  \n  49 +\n  50  ",
            "\tpublic void open(String path) throws IOException {\n\t\tsetupMappedFile(path);\n\t\tdeserializer = (Deserializer<OUT>) (readAsByteArray ? new ByteArrayDeserializer() : new TupleDeserializer());\n\t}"
        ]
    ],
    "ded464b8bd60d8f19221c0f1589346684c11c78d": [
        [
            "KryoRegistrationSerializerConfigSnapshot::KryoRegistrationSerializationProxy::read(DataInputView)",
            " 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 -\n 188 -\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "\t\t@SuppressWarnings(\"unchecked\")\n\t\t@Override\n\t\tpublic void read(DataInputView in) throws IOException {\n\t\t\tString registeredClassname = in.readUTF();\n\n\t\t\tClass<RC> registeredClass;\n\t\t\ttry {\n\t\t\t\tregisteredClass = (Class<RC>) Class.forName(registeredClassname, true, userCodeClassLoader);\n\t\t\t} catch (ClassNotFoundException e) {\n\t\t\t\tLOG.warn(\"Cannot find registered class \" + registeredClassname + \" for Kryo serialization in classpath;\" +\n\t\t\t\t\t\" using a dummy class as a placeholder.\", e);\n\n\t\t\t\tregisteredClass = (Class) DummyRegisteredClass.class;\n\t\t\t}\n\n\t\t\tfinal KryoRegistration.SerializerDefinitionType serializerDefinitionType =\n\t\t\t\tKryoRegistration.SerializerDefinitionType.values()[in.readInt()];\n\n\t\t\tswitch (serializerDefinitionType) {\n\t\t\t\tcase UNSPECIFIED:\n\t\t\t\t\tkryoRegistration = new KryoRegistration(registeredClass);\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase CLASS:\n\t\t\t\t\tString serializerClassname = in.readUTF();\n\n\t\t\t\t\tClass serializerClass;\n\t\t\t\t\ttry {\n\t\t\t\t\t\tserializerClass = Class.forName(serializerClassname, true, userCodeClassLoader);\n\t\t\t\t\t} catch (ClassNotFoundException e) {\n\t\t\t\t\t\tLOG.warn(\"Cannot find registered Kryo serializer class for class \" + registeredClassname +\n\t\t\t\t\t\t\t\t\" in classpath; using a dummy Kryo serializer that should be replaced as soon as\" +\n\t\t\t\t\t\t\t\t\" a new Kryo serializer for the class is present\", e);\n\n\t\t\t\t\t\tserializerClass = DummyKryoSerializerClass.class;\n\t\t\t\t\t}\n\n\t\t\t\t\tkryoRegistration = new KryoRegistration(registeredClass, serializerClass);\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase INSTANCE:\n\t\t\t\t\tExecutionConfig.SerializableSerializer<? extends Serializer<RC>> serializerInstance;\n\t\t\t\t\ttry {\n\t\t\t\t\t\tserializerInstance = InstantiationUtil.deserializeObject(new DataInputViewStream(in), userCodeClassLoader);\n\t\t\t\t\t} catch (ClassNotFoundException e) {\n\t\t\t\t\t\tLOG.warn(\"Cannot find registered Kryo serializer class for class \" + registeredClassname +\n\t\t\t\t\t\t\t\t\" in classpath; using a dummy Kryo serializer that should be replaced as soon as\" +\n\t\t\t\t\t\t\t\t\" a new Kryo serializer for the class is present\", e);\n\n\t\t\t\t\t\tserializerInstance = new ExecutionConfig.SerializableSerializer<>(new DummyKryoSerializerClass<RC>());\n\t\t\t\t\t} catch (InvalidClassException e) {\n\t\t\t\t\t\tLOG.warn(\"The registered Kryo serializer class for class \" + registeredClassname +\n\t\t\t\t\t\t\t\t\" has changed and is no longer valid; using a dummy Kryo serializer that should be replaced\" +\n\t\t\t\t\t\t\t\t\" as soon as a new Kryo serializer for the class is present.\", e);\n\n\t\t\t\t\t\tserializerInstance = new ExecutionConfig.SerializableSerializer<>(new DummyKryoSerializerClass<RC>());\n\t\t\t\t\t}\n\n\t\t\t\t\tkryoRegistration = new KryoRegistration(registeredClass, serializerInstance);\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t// this should not happen; adding as a guard for the future\n\t\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\t\"Unrecognized Kryo registration serializer definition type: \" + serializerDefinitionType);\n\t\t\t}\n\t\t}",
            " 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189 +\n 190 +\n 191 +\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  ",
            "\t\t@SuppressWarnings(\"unchecked\")\n\t\t@Override\n\t\tpublic void read(DataInputView in) throws IOException {\n\t\t\tString registeredClassname = in.readUTF();\n\n\t\t\tClass<RC> registeredClass;\n\t\t\ttry {\n\t\t\t\tregisteredClass = (Class<RC>) Class.forName(registeredClassname, true, userCodeClassLoader);\n\t\t\t} catch (ClassNotFoundException e) {\n\t\t\t\tLOG.warn(\"Cannot find registered class \" + registeredClassname + \" for Kryo serialization in classpath;\" +\n\t\t\t\t\t\" using a dummy class as a placeholder.\", e);\n\n\t\t\t\tregisteredClass = (Class) DummyRegisteredClass.class;\n\t\t\t}\n\n\t\t\tfinal KryoRegistration.SerializerDefinitionType serializerDefinitionType =\n\t\t\t\tKryoRegistration.SerializerDefinitionType.values()[in.readInt()];\n\n\t\t\tswitch (serializerDefinitionType) {\n\t\t\t\tcase UNSPECIFIED:\n\t\t\t\t\tkryoRegistration = new KryoRegistration(registeredClass);\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase CLASS:\n\t\t\t\t\tString serializerClassname = in.readUTF();\n\n\t\t\t\t\tClass serializerClass;\n\t\t\t\t\ttry {\n\t\t\t\t\t\tserializerClass = Class.forName(serializerClassname, true, userCodeClassLoader);\n\t\t\t\t\t} catch (ClassNotFoundException e) {\n\t\t\t\t\t\tLOG.warn(\"Cannot find registered Kryo serializer class for class \" + registeredClassname +\n\t\t\t\t\t\t\t\t\" in classpath; using a dummy Kryo serializer that should be replaced as soon as\" +\n\t\t\t\t\t\t\t\t\" a new Kryo serializer for the class is present\", e);\n\n\t\t\t\t\t\tserializerClass = DummyKryoSerializerClass.class;\n\t\t\t\t\t}\n\n\t\t\t\t\tkryoRegistration = new KryoRegistration(registeredClass, serializerClass);\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase INSTANCE:\n\t\t\t\t\tExecutionConfig.SerializableSerializer<? extends Serializer<RC>> serializerInstance;\n\n\t\t\t\t\ttry (final DataInputViewStream inViewWrapper = new DataInputViewStream(in)) {\n\t\t\t\t\t\tserializerInstance = InstantiationUtil.deserializeObject(inViewWrapper, userCodeClassLoader);\n\t\t\t\t\t} catch (ClassNotFoundException e) {\n\t\t\t\t\t\tLOG.warn(\"Cannot find registered Kryo serializer class for class \" + registeredClassname +\n\t\t\t\t\t\t\t\t\" in classpath; using a dummy Kryo serializer that should be replaced as soon as\" +\n\t\t\t\t\t\t\t\t\" a new Kryo serializer for the class is present\", e);\n\n\t\t\t\t\t\tserializerInstance = new ExecutionConfig.SerializableSerializer<>(new DummyKryoSerializerClass<RC>());\n\t\t\t\t\t} catch (InvalidClassException e) {\n\t\t\t\t\t\tLOG.warn(\"The registered Kryo serializer class for class \" + registeredClassname +\n\t\t\t\t\t\t\t\t\" has changed and is no longer valid; using a dummy Kryo serializer that should be replaced\" +\n\t\t\t\t\t\t\t\t\" as soon as a new Kryo serializer for the class is present.\", e);\n\n\t\t\t\t\t\tserializerInstance = new ExecutionConfig.SerializableSerializer<>(new DummyKryoSerializerClass<RC>());\n\t\t\t\t\t}\n\n\t\t\t\t\tkryoRegistration = new KryoRegistration(registeredClass, serializerInstance);\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t// this should not happen; adding as a guard for the future\n\t\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\t\"Unrecognized Kryo registration serializer definition type: \" + serializerDefinitionType);\n\t\t\t}\n\t\t}"
        ],
        [
            "JavaSerializer::deserialize(DataInputView)",
            "  72  \n  73  \n  74 -\n  75  \n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\t@Override\n\tpublic T deserialize(DataInputView source) throws IOException {\n\t\ttry {\n\t\t\treturn InstantiationUtil.deserializeObject(\n\t\t\t\t\tnew DataInputViewStream(source),\n\t\t\t\t\tThread.currentThread().getContextClassLoader());\n\t\t} catch (ClassNotFoundException e) {\n\t\t\tthrow new IOException(\"Could not deserialize object.\", e);\n\t\t}\n\t}",
            "  74  \n  75  \n  76 +\n  77  \n  78 +\n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\t@Override\n\tpublic T deserialize(DataInputView source) throws IOException {\n\t\ttry (final DataInputViewStream inViewWrapper = new DataInputViewStream(source)) {\n\t\t\treturn InstantiationUtil.deserializeObject(\n\t\t\t\t\tinViewWrapper,\n\t\t\t\t\tThread.currentThread().getContextClassLoader());\n\t\t} catch (ClassNotFoundException e) {\n\t\t\tthrow new IOException(\"Could not deserialize object.\", e);\n\t\t}\n\t}"
        ],
        [
            "GenericArraySerializerConfigSnapshot::write(DataOutputView)",
            "  57  \n  58  \n  59  \n  60  \n  61 -\n  62  ",
            "\t@Override\n\tpublic void write(DataOutputView out) throws IOException {\n\t\tsuper.write(out);\n\n\t\tInstantiationUtil.serializeObject(new DataOutputViewStream(out), componentClass);\n\t}",
            "  57  \n  58  \n  59  \n  60  \n  61 +\n  62 +\n  63 +\n  64  ",
            "\t@Override\n\tpublic void write(DataOutputView out) throws IOException {\n\t\tsuper.write(out);\n\n\t\ttry (final DataOutputViewStream outViewWrapper = new DataOutputViewStream(out)) {\n\t\t\tInstantiationUtil.serializeObject(outViewWrapper, componentClass);\n\t\t}\n\t}"
        ],
        [
            "GenericArraySerializerConfigSnapshot::read(DataInputView)",
            "  64  \n  65  \n  66  \n  67  \n  68 -\n  69 -\n  70  \n  71  \n  72  \n  73  ",
            "\t@Override\n\tpublic void read(DataInputView in) throws IOException {\n\t\tsuper.read(in);\n\n\t\ttry {\n\t\t\tcomponentClass = InstantiationUtil.deserializeObject(new DataInputViewStream(in), getUserCodeClassLoader());\n\t\t} catch (ClassNotFoundException e) {\n\t\t\tthrow new IOException(\"Could not find requested element class in classpath.\", e);\n\t\t}\n\t}",
            "  66  \n  67  \n  68  \n  69  \n  70 +\n  71 +\n  72  \n  73  \n  74  \n  75  ",
            "\t@Override\n\tpublic void read(DataInputView in) throws IOException {\n\t\tsuper.read(in);\n\n\t\ttry (final DataInputViewStream inViewWrapper = new DataInputViewStream(in)) {\n\t\t\tcomponentClass = InstantiationUtil.deserializeObject(inViewWrapper, getUserCodeClassLoader());\n\t\t} catch (ClassNotFoundException e) {\n\t\t\tthrow new IOException(\"Could not find requested element class in classpath.\", e);\n\t\t}\n\t}"
        ],
        [
            "JavaSerializer::serialize(T,DataOutputView)",
            "  67  \n  68  \n  69 -\n  70  ",
            "\t@Override\n\tpublic void serialize(T record, DataOutputView target) throws IOException {\n\t\tInstantiationUtil.serializeObject(new DataOutputViewStream(target), record);\n\t}",
            "  67  \n  68  \n  69 +\n  70 +\n  71 +\n  72  ",
            "\t@Override\n\tpublic void serialize(T record, DataOutputView target) throws IOException {\n\t\ttry (final DataOutputViewStream outViewWrapper = new DataOutputViewStream(target)) {\n\t\t\tInstantiationUtil.serializeObject(outViewWrapper, record);\n\t\t}\n\t}"
        ],
        [
            "TupleSerializerConfigSnapshot::read(DataInputView)",
            "  57  \n  58  \n  59  \n  60  \n  61 -\n  62 -\n  63  \n  64  \n  65  \n  66  ",
            "\t@Override\n\tpublic void read(DataInputView in) throws IOException {\n\t\tsuper.read(in);\n\n\t\ttry {\n\t\t\ttupleClass = InstantiationUtil.deserializeObject(new DataInputViewStream(in), getUserCodeClassLoader());\n\t\t} catch (ClassNotFoundException e) {\n\t\t\tthrow new IOException(\"Could not find requested tuple class in classpath.\", e);\n\t\t}\n\t}",
            "  59  \n  60  \n  61  \n  62  \n  63 +\n  64 +\n  65  \n  66  \n  67  \n  68  ",
            "\t@Override\n\tpublic void read(DataInputView in) throws IOException {\n\t\tsuper.read(in);\n\n\t\ttry (final DataInputViewStream inViewWrapper = new DataInputViewStream(in)) {\n\t\t\ttupleClass = InstantiationUtil.deserializeObject(inViewWrapper, getUserCodeClassLoader());\n\t\t} catch (ClassNotFoundException e) {\n\t\t\tthrow new IOException(\"Could not find requested tuple class in classpath.\", e);\n\t\t}\n\t}"
        ],
        [
            "TupleSerializerConfigSnapshot::write(DataOutputView)",
            "  50  \n  51  \n  52  \n  53  \n  54 -\n  55  ",
            "\t@Override\n\tpublic void write(DataOutputView out) throws IOException {\n\t\tsuper.write(out);\n\n\t\tInstantiationUtil.serializeObject(new DataOutputViewStream(out), tupleClass);\n\t}",
            "  50  \n  51  \n  52  \n  53  \n  54 +\n  55 +\n  56 +\n  57  ",
            "\t@Override\n\tpublic void write(DataOutputView out) throws IOException {\n\t\tsuper.write(out);\n\n\t\ttry (final DataOutputViewStream outViewWrapper = new DataOutputViewStream(out)) {\n\t\t\tInstantiationUtil.serializeObject(outViewWrapper, tupleClass);\n\t\t}\n\t}"
        ],
        [
            "KryoRegistrationSerializerConfigSnapshot::KryoRegistrationSerializationProxy::write(DataOutputView)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "\t\t@Override\n\t\tpublic void write(DataOutputView out) throws IOException {\n\t\t\tout.writeUTF(kryoRegistration.getRegisteredClass().getName());\n\n\t\t\tfinal KryoRegistration.SerializerDefinitionType serializerDefinitionType = kryoRegistration.getSerializerDefinitionType();\n\n\t\t\tout.writeInt(serializerDefinitionType.ordinal());\n\t\t\tswitch (serializerDefinitionType) {\n\t\t\t\tcase UNSPECIFIED:\n\t\t\t\t\t// nothing else to write\n\t\t\t\t\tbreak;\n\t\t\t\tcase CLASS:\n\t\t\t\t\tout.writeUTF(kryoRegistration.getSerializerClass().getName());\n\t\t\t\t\tbreak;\n\t\t\t\tcase INSTANCE:\n\t\t\t\t\tInstantiationUtil.serializeObject(new DataOutputViewStream(out), kryoRegistration.getSerializableSerializerInstance());\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\t// this should not happen; adding as a guard for the future\n\t\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\t\"Unrecognized Kryo registration serializer definition type: \" + serializerDefinitionType);\n\t\t\t}\n\t\t}",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 +\n 137 +\n 138 +\n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  ",
            "\t\t@Override\n\t\tpublic void write(DataOutputView out) throws IOException {\n\t\t\tout.writeUTF(kryoRegistration.getRegisteredClass().getName());\n\n\t\t\tfinal KryoRegistration.SerializerDefinitionType serializerDefinitionType = kryoRegistration.getSerializerDefinitionType();\n\n\t\t\tout.writeInt(serializerDefinitionType.ordinal());\n\t\t\tswitch (serializerDefinitionType) {\n\t\t\t\tcase UNSPECIFIED:\n\t\t\t\t\t// nothing else to write\n\t\t\t\t\tbreak;\n\t\t\t\tcase CLASS:\n\t\t\t\t\tout.writeUTF(kryoRegistration.getSerializerClass().getName());\n\t\t\t\t\tbreak;\n\t\t\t\tcase INSTANCE:\n\t\t\t\t\ttry (final DataOutputViewStream outViewWrapper = new DataOutputViewStream(out)) {\n\t\t\t\t\t\tInstantiationUtil.serializeObject(outViewWrapper, kryoRegistration.getSerializableSerializerInstance());\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\t// this should not happen; adding as a guard for the future\n\t\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\t\"Unrecognized Kryo registration serializer definition type: \" + serializerDefinitionType);\n\t\t\t}\n\t\t}"
        ]
    ],
    "789ed8a8246d140e1621a5860645a747132d6618": [
        [
            "EnumTrianglesDataTypes::Edge::getSecondVertex()",
            "  42 -",
            "\t\tpublic Integer getSecondVertex() { return this.getField(V2); }",
            "  50 +\n  51 +\n  52 +",
            "\t\tpublic Integer getSecondVertex() {\n\t\t\treturn this.getField(V2);\n\t\t}"
        ],
        [
            "PageRank::main(String)",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103 -\n 104  \n 105 -\n 106  \n 107 -\n 108  \n 109  \n 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119  \n 120 -\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135 -\n 136  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tfinal int numPages = params.getInt(\"numPages\", PageRankData.getNumberOfPages());\n\t\tfinal int maxIterations = params.getInt(\"iterations\", 10);\n\t\t\n\t\t// set up execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make the parameters available to the web ui\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\t\t\n\t\t// get input data\n\t\tDataSet<Long> pagesInput = getPagesDataSet(env, params);\n\t\tDataSet<Tuple2<Long, Long>> linksInput = getLinksDataSet(env, params);\n\t\t\n\t\t// assign initial rank to pages\n\t\tDataSet<Tuple2<Long, Double>> pagesWithRanks = pagesInput.\n\t\t\t\tmap(new RankAssigner((1.0d / numPages)));\n\t\t\n\t\t// build adjacency list from link input\n\t\tDataSet<Tuple2<Long, Long[]>> adjacencyListInput = \n\t\t\t\tlinksInput.groupBy(0).reduceGroup(new BuildOutgoingEdgeList());\n\t\t\n\t\t// set iterative data set\n\t\tIterativeDataSet<Tuple2<Long, Double>> iteration = pagesWithRanks.iterate(maxIterations);\n\t\t\n\t\tDataSet<Tuple2<Long, Double>> newRanks = iteration\n\t\t\t\t// join pages with outgoing edges and distribute rank\n\t\t\t\t.join(adjacencyListInput).where(0).equalTo(0).flatMap(new JoinVertexWithEdgesMatch())\n\t\t\t\t// collect and sum ranks\n\t\t\t\t.groupBy(0).aggregate(SUM, 1)\n\t\t\t\t// apply dampening factor\n\t\t\t\t.map(new Dampener(DAMPENING_FACTOR, numPages));\n\t\t\n\t\tDataSet<Tuple2<Long, Double>> finalPageRanks = iteration.closeWith(\n\t\t\t\tnewRanks, \n\t\t\t\tnewRanks.join(iteration).where(0).equalTo(0)\n\t\t\t\t// termination condition\n\t\t\t\t.filter(new EpsilonFilter()));\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tfinalPageRanks.writeAsCsv(params.get(\"output\"), \"\\n\", \" \");\n\t\t\t// execute program\n\t\t\tenv.execute(\"Basic Page Rank Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tfinalPageRanks.print();\n\t\t}\n\n\t\t\n\t}",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91 +\n  92  \n  93  \n  94  \n  95 +\n  96  \n  97  \n  98  \n  99 +\n 100  \n 101 +\n 102  \n 103 +\n 104  \n 105  \n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 +\n 115  \n 116 +\n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tfinal int numPages = params.getInt(\"numPages\", PageRankData.getNumberOfPages());\n\t\tfinal int maxIterations = params.getInt(\"iterations\", 10);\n\n\t\t// set up execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make the parameters available to the web ui\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input data\n\t\tDataSet<Long> pagesInput = getPagesDataSet(env, params);\n\t\tDataSet<Tuple2<Long, Long>> linksInput = getLinksDataSet(env, params);\n\n\t\t// assign initial rank to pages\n\t\tDataSet<Tuple2<Long, Double>> pagesWithRanks = pagesInput.\n\t\t\t\tmap(new RankAssigner((1.0d / numPages)));\n\n\t\t// build adjacency list from link input\n\t\tDataSet<Tuple2<Long, Long[]>> adjacencyListInput =\n\t\t\t\tlinksInput.groupBy(0).reduceGroup(new BuildOutgoingEdgeList());\n\n\t\t// set iterative data set\n\t\tIterativeDataSet<Tuple2<Long, Double>> iteration = pagesWithRanks.iterate(maxIterations);\n\n\t\tDataSet<Tuple2<Long, Double>> newRanks = iteration\n\t\t\t\t// join pages with outgoing edges and distribute rank\n\t\t\t\t.join(adjacencyListInput).where(0).equalTo(0).flatMap(new JoinVertexWithEdgesMatch())\n\t\t\t\t// collect and sum ranks\n\t\t\t\t.groupBy(0).aggregate(SUM, 1)\n\t\t\t\t// apply dampening factor\n\t\t\t\t.map(new Dampener(DAMPENING_FACTOR, numPages));\n\n\t\tDataSet<Tuple2<Long, Double>> finalPageRanks = iteration.closeWith(\n\t\t\t\tnewRanks,\n\t\t\t\tnewRanks.join(iteration).where(0).equalTo(0)\n\t\t\t\t// termination condition\n\t\t\t\t.filter(new EpsilonFilter()));\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tfinalPageRanks.writeAsCsv(params.get(\"output\"), \"\\n\", \" \");\n\t\t\t// execute program\n\t\t\tenv.execute(\"Basic Page Rank Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tfinalPageRanks.print();\n\t\t}\n\t}"
        ],
        [
            "TPCHQuery3::ShippingPriorityItem::getOrderkey()",
            " 223 -",
            "\t\tpublic Long getOrderkey() { return this.f0; }",
            " 244 +\n 245 +\n 246 +",
            "\t\tpublic Long getOrderkey() {\n\t\t\treturn this.f0;\n\t\t}"
        ],
        [
            "WebLogAnalysis::getDocumentsDataSet(ExecutionEnvironment,ParameterTool)",
            " 264  \n 265  \n 266 -\n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "\tprivate static DataSet<Tuple2<String, String>> getDocumentsDataSet(ExecutionEnvironment env, ParameterTool params) {\n\t\t// Create DataSet for documents relation (URL, Doc-Text)\n\t\tif(params.has(\"documents\")) {\n\t\t\treturn env.readCsvFile(params.get(\"documents\"))\n\t\t\t\t\t\t.fieldDelimiter(\"|\")\n\t\t\t\t\t\t.types(String.class, String.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing WebLogAnalysis example with default documents data set.\");\n\t\t\tSystem.out.println(\"Use --documents to specify file input.\");\n\t\t\treturn WebLogData.getDocumentDataSet(env);\n\t\t}\n\t}",
            " 260  \n 261  \n 262 +\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  ",
            "\tprivate static DataSet<Tuple2<String, String>> getDocumentsDataSet(ExecutionEnvironment env, ParameterTool params) {\n\t\t// Create DataSet for documents relation (URL, Doc-Text)\n\t\tif (params.has(\"documents\")) {\n\t\t\treturn env.readCsvFile(params.get(\"documents\"))\n\t\t\t\t\t\t.fieldDelimiter(\"|\")\n\t\t\t\t\t\t.types(String.class, String.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing WebLogAnalysis example with default documents data set.\");\n\t\t\tSystem.out.println(\"Use --documents to specify file input.\");\n\t\t\treturn WebLogData.getDocumentDataSet(env);\n\t\t}\n\t}"
        ],
        [
            "EnumTrianglesData::getDefaultEdgeDataSet(ExecutionEnvironment)",
            "  50  \n  51 -\n  52  \n  53 -\n  54 -\n  55  \n  56 -\n  57  \n  58  ",
            "\tpublic static DataSet<EnumTrianglesDataTypes.Edge> getDefaultEdgeDataSet(ExecutionEnvironment env) {\n\t\t\n\t\tList<EnumTrianglesDataTypes.Edge> edges = new ArrayList<EnumTrianglesDataTypes.Edge>();\n\t\tfor(Object[] e : EDGES) {\n\t\t\tedges.add(new Edge((Integer)e[0], (Integer)e[1]));\n\t\t}\n\t\t\n\t\treturn env.fromCollection(edges);\n\t}",
            "  49  \n  50 +\n  51  \n  52 +\n  53 +\n  54  \n  55 +\n  56  \n  57  ",
            "\tpublic static DataSet<EnumTrianglesDataTypes.Edge> getDefaultEdgeDataSet(ExecutionEnvironment env) {\n\n\t\tList<EnumTrianglesDataTypes.Edge> edges = new ArrayList<EnumTrianglesDataTypes.Edge>();\n\t\tfor (Object[] e : EDGES) {\n\t\t\tedges.add(new Edge((Integer) e[0], (Integer) e[1]));\n\t\t}\n\n\t\treturn env.fromCollection(edges);\n\t}"
        ],
        [
            "KMeansDataGenerator::writeCenter(long,double,StringBuilder,BufferedWriter)",
            " 171  \n 172  \n 173 -\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 -\n 182  \n 183  \n 184  \n 185 -\n 186  \n 187  \n 188  ",
            "\tprivate static void writeCenter(long id, double[] coordinates, StringBuilder buffer, BufferedWriter out) throws IOException {\n\t\tbuffer.setLength(0);\n\t\t\n\t\t// write id\n\t\tbuffer.append(id);\n\t\tbuffer.append(DELIMITER);\n\n\t\t// write coordinates\n\t\tfor (int j = 0; j < coordinates.length; j++) {\n\t\t\tbuffer.append(FORMAT.format(coordinates[j]));\n\t\t\tif(j < coordinates.length - 1) {\n\t\t\t\tbuffer.append(DELIMITER);\n\t\t\t}\n\t\t}\n\t\t\n\t\tout.write(buffer.toString());\n\t\tout.newLine();\n\t}",
            " 169  \n 170  \n 171 +\n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179 +\n 180  \n 181  \n 182  \n 183 +\n 184  \n 185  \n 186  ",
            "\tprivate static void writeCenter(long id, double[] coordinates, StringBuilder buffer, BufferedWriter out) throws IOException {\n\t\tbuffer.setLength(0);\n\n\t\t// write id\n\t\tbuffer.append(id);\n\t\tbuffer.append(DELIMITER);\n\n\t\t// write coordinates\n\t\tfor (int j = 0; j < coordinates.length; j++) {\n\t\t\tbuffer.append(FORMAT.format(coordinates[j]));\n\t\t\tif (j < coordinates.length - 1) {\n\t\t\t\tbuffer.append(DELIMITER);\n\t\t\t}\n\t\t}\n\n\t\tout.write(buffer.toString());\n\t\tout.newLine();\n\t}"
        ],
        [
            "TPCHQuery3::Customer::getMktsegment()",
            " 200 -",
            "\t\tpublic String getMktsegment() { return this.f1; }",
            " 208 +\n 209 +\n 210 +",
            "\t\tpublic String getMktsegment() {\n\t\t\treturn this.f1;\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::Edge::setFirstVertex(Integer)",
            "  44 -",
            "\t\tpublic void setFirstVertex(final Integer vertex1) { this.setField(vertex1, V1); }",
            "  54 +\n  55 +\n  56 +",
            "\t\tpublic void setFirstVertex(final Integer vertex1) {\n\t\t\tthis.setField(vertex1, V1);\n\t\t}"
        ],
        [
            "TPCHQuery3::Customer::getCustKey()",
            " 199 -",
            "\t\tpublic Long getCustKey() { return this.f0; }",
            " 204 +\n 205 +\n 206 +",
            "\t\tpublic Long getCustKey() {\n\t\t\treturn this.f0;\n\t\t}"
        ],
        [
            "WebLogAnalysis::main(String)",
            " 100  \n 101 -\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131 -\n 132  \n 133  \n 134 -\n 135  \n 136  \n 137  \n 138  \n 139  \n 140 -\n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input data\n\t\tDataSet<Tuple2<String, String>> documents = getDocumentsDataSet(env, params);\n\t\tDataSet<Tuple3<Integer, String, Integer>> ranks = getRanksDataSet(env, params);\n\t\tDataSet<Tuple2<String, String>> visits = getVisitsDataSet(env, params);\n\t\t\n\t\t// Retain documents with keywords\n\t\tDataSet<Tuple1<String>> filterDocs = documents\n\t\t\t\t.filter(new FilterDocByKeyWords())\n\t\t\t\t.project(0);\n\n\t\t// Filter ranks by minimum rank\n\t\tDataSet<Tuple3<Integer, String, Integer>> filterRanks = ranks\n\t\t\t\t.filter(new FilterByRank());\n\n\t\t// Filter visits by visit date\n\t\tDataSet<Tuple1<String>> filterVisits = visits\n\t\t\t\t.filter(new FilterVisitsByDate())\n\t\t\t\t.project(0);\n\n\t\t// Join the filtered documents and ranks, i.e., get all URLs with min rank and keywords\n\t\tDataSet<Tuple3<Integer, String, Integer>> joinDocsRanks = \n\t\t\t\tfilterDocs.join(filterRanks)\n\t\t\t\t\t\t\t.where(0).equalTo(1)\n\t\t\t\t\t\t\t.projectSecond(0,1,2);\n\n\t\t// Anti-join urls with visits, i.e., retain all URLs which have NOT been visited in a certain time\n\t\tDataSet<Tuple3<Integer, String, Integer>> result = \n\t\t\t\tjoinDocsRanks.coGroup(filterVisits)\n\t\t\t\t\t\t\t\t.where(1).equalTo(0)\n\t\t\t\t\t\t\t\t.with(new AntiJoinVisits());\n\n\t\t// emit result\n\t\tif(params.has(\"output\")) {\n\t\t\tresult.writeAsCsv(params.get(\"output\"), \"\\n\", \"|\");\n\t\t\t// execute program\n\t\t\tenv.execute(\"WebLogAnalysis Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\t}",
            "  96  \n  97 +\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 +\n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125  \n 126  \n 127 +\n 128  \n 129  \n 130 +\n 131  \n 132  \n 133  \n 134  \n 135  \n 136 +\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input data\n\t\tDataSet<Tuple2<String, String>> documents = getDocumentsDataSet(env, params);\n\t\tDataSet<Tuple3<Integer, String, Integer>> ranks = getRanksDataSet(env, params);\n\t\tDataSet<Tuple2<String, String>> visits = getVisitsDataSet(env, params);\n\n\t\t// Retain documents with keywords\n\t\tDataSet<Tuple1<String>> filterDocs = documents\n\t\t\t\t.filter(new FilterDocByKeyWords())\n\t\t\t\t.project(0);\n\n\t\t// Filter ranks by minimum rank\n\t\tDataSet<Tuple3<Integer, String, Integer>> filterRanks = ranks\n\t\t\t\t.filter(new FilterByRank());\n\n\t\t// Filter visits by visit date\n\t\tDataSet<Tuple1<String>> filterVisits = visits\n\t\t\t\t.filter(new FilterVisitsByDate())\n\t\t\t\t.project(0);\n\n\t\t// Join the filtered documents and ranks, i.e., get all URLs with min rank and keywords\n\t\tDataSet<Tuple3<Integer, String, Integer>> joinDocsRanks =\n\t\t\t\tfilterDocs.join(filterRanks)\n\t\t\t\t\t\t\t.where(0).equalTo(1)\n\t\t\t\t\t\t\t.projectSecond(0, 1, 2);\n\n\t\t// Anti-join urls with visits, i.e., retain all URLs which have NOT been visited in a certain time\n\t\tDataSet<Tuple3<Integer, String, Integer>> result =\n\t\t\t\tjoinDocsRanks.coGroup(filterVisits)\n\t\t\t\t\t\t\t\t.where(1).equalTo(0)\n\t\t\t\t\t\t\t\t.with(new AntiJoinVisits());\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tresult.writeAsCsv(params.get(\"output\"), \"\\n\", \"|\");\n\t\t\t// execute program\n\t\t\tenv.execute(\"WebLogAnalysis Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\t}"
        ],
        [
            "TPCHQuery3::Lineitem::getOrderkey()",
            " 191 -",
            "\t\tpublic Long getOrderkey() { return this.f0; }",
            " 185 +\n 186 +\n 187 +",
            "\t\tpublic Long getOrderkey() {\n\t\t\treturn this.f0;\n\t\t}"
        ],
        [
            "TPCHQuery3::ShippingPriorityItem::getOrderdate()",
            " 228 -",
            "\t\tpublic String getOrderdate() { return this.f2; }",
            " 260 +\n 261 +\n 262 +",
            "\t\tpublic String getOrderdate() {\n\t\t\treturn this.f2;\n\t\t}"
        ],
        [
            "CollectionExecutionExample::main(String)",
            "  70  \n  71  \n  72  \n  73 -\n  74  \n  75  \n  76 -\n  77  \n  78  \n  79  \n  80 -\n  81  \n  82  \n  83  \n  84 -\n  85  \n  86 -\n  87 -\n  88  \n  89 -\n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t// initialize a new Collection-based execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.createCollectionsEnvironment();\n\t\t\n\t\t// create objects for users and emails\n\t\tUser[] usersArray = { new User(1, \"Peter\"), new User(2, \"John\"), new User(3, \"Bill\") };\n\t\t\n\t\tEMail[] emailsArray = {new EMail(1, \"Re: Meeting\", \"How about 1pm?\"),\n\t\t\t\t\t\t\tnew EMail(1, \"Re: Meeting\", \"Sorry, I'm not availble\"),\n\t\t\t\t\t\t\tnew EMail(3, \"Re: Re: Project proposal\", \"Give me a few more days to think about it.\")};\n\t\t\n\t\t// convert objects into a DataSet\n\t\tDataSet<User> users = env.fromElements(usersArray);\n\t\tDataSet<EMail> emails = env.fromElements(emailsArray);\n\t\t\n\t\t// join the two DataSets\n\t\tDataSet<Tuple2<User,EMail>> joined = users.join(emails).where(\"userIdentifier\").equalTo(\"userId\");\n\t\t\n\t\t// retrieve the resulting Tuple2 elements into a ArrayList.\n\t\tList<Tuple2<User,EMail>> result = joined.collect();\n\t\t\n\t\t// Do some work with the resulting ArrayList (=Collection).\n\t\tfor (Tuple2<User, EMail> t : result) {\n\t\t\tSystem.err.println(\"Result = \" + t);\n\t\t}\n\t}",
            "  76  \n  77  \n  78  \n  79 +\n  80  \n  81  \n  82 +\n  83  \n  84  \n  85  \n  86 +\n  87  \n  88  \n  89  \n  90 +\n  91  \n  92 +\n  93 +\n  94  \n  95 +\n  96 +\n  97  \n  98  \n  99  \n 100  \n 101  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t// initialize a new Collection-based execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.createCollectionsEnvironment();\n\n\t\t// create objects for users and emails\n\t\tUser[] usersArray = { new User(1, \"Peter\"), new User(2, \"John\"), new User(3, \"Bill\") };\n\n\t\tEMail[] emailsArray = {new EMail(1, \"Re: Meeting\", \"How about 1pm?\"),\n\t\t\t\t\t\t\tnew EMail(1, \"Re: Meeting\", \"Sorry, I'm not availble\"),\n\t\t\t\t\t\t\tnew EMail(3, \"Re: Re: Project proposal\", \"Give me a few more days to think about it.\")};\n\n\t\t// convert objects into a DataSet\n\t\tDataSet<User> users = env.fromElements(usersArray);\n\t\tDataSet<EMail> emails = env.fromElements(emailsArray);\n\n\t\t// join the two DataSets\n\t\tDataSet<Tuple2<User, EMail>> joined = users.join(emails).where(\"userIdentifier\").equalTo(\"userId\");\n\n\t\t// retrieve the resulting Tuple2 elements into a ArrayList.\n\t\tList<Tuple2<User, EMail>> result = joined.collect();\n\n\t\t// Do some work with the resulting ArrayList (=Collection).\n\t\tfor (Tuple2<User, EMail> t : result) {\n\t\t\tSystem.err.println(\"Result = \" + t);\n\t\t}\n\t}"
        ],
        [
            "CollectionExecutionExample::User::toString()",
            "  49  \n  50 -\n  51  ",
            "\t\tpublic String toString() {\n\t\t\treturn \"User{userIdentifier=\"+userIdentifier+\" name=\"+name+\"}\";\n\t\t}",
            "  51  \n  52 +\n  53  ",
            "\t\tpublic String toString() {\n\t\t\treturn \"User{userIdentifier=\" + userIdentifier + \" name=\" + name + \"}\";\n\t\t}"
        ],
        [
            "KMeansDataGenerator::writePoint(double,StringBuilder,BufferedWriter)",
            " 156  \n 157  \n 158 -\n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166 -\n 167  \n 168  \n 169  ",
            "\tprivate static void writePoint(double[] coordinates, StringBuilder buffer, BufferedWriter out) throws IOException {\n\t\tbuffer.setLength(0);\n\t\t\n\t\t// write coordinates\n\t\tfor (int j = 0; j < coordinates.length; j++) {\n\t\t\tbuffer.append(FORMAT.format(coordinates[j]));\n\t\t\tif(j < coordinates.length - 1) {\n\t\t\t\tbuffer.append(DELIMITER);\n\t\t\t}\n\t\t}\n\t\t\n\t\tout.write(buffer.toString());\n\t\tout.newLine();\n\t}",
            " 154  \n 155  \n 156 +\n 157  \n 158  \n 159  \n 160 +\n 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  ",
            "\tprivate static void writePoint(double[] coordinates, StringBuilder buffer, BufferedWriter out) throws IOException {\n\t\tbuffer.setLength(0);\n\n\t\t// write coordinates\n\t\tfor (int j = 0; j < coordinates.length; j++) {\n\t\t\tbuffer.append(FORMAT.format(coordinates[j]));\n\t\t\tif (j < coordinates.length - 1) {\n\t\t\t\tbuffer.append(DELIMITER);\n\t\t\t}\n\t\t}\n\n\t\tout.write(buffer.toString());\n\t\tout.newLine();\n\t}"
        ],
        [
            "TPCHQuery3::Lineitem::getExtendedprice()",
            " 193 -",
            "\t\tpublic Double getExtendedprice() { return this.f1; }",
            " 193 +\n 194 +\n 195 +",
            "\t\tpublic Double getExtendedprice() {\n\t\t\treturn this.f1;\n\t\t}"
        ],
        [
            "LinearRegression::Data::Data(double,double)",
            " 139 -\n 140  \n 141  \n 142  ",
            "\t\tpublic Data(double x ,double y){\n\t\t\tthis.x = x;\n\t\t\tthis.y = y;\n\t\t}",
            " 135 +\n 136  \n 137  \n 138  ",
            "\t\tpublic Data(double x, double y) {\n\t\t\tthis.x = x;\n\t\t\tthis.y = y;\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::Triad::setThirdVertex(Integer)",
            "  78 -",
            "\t\tpublic void setThirdVertex(final Integer vertex3) { this.setField(vertex3, V3); }",
            "  99 +\n 100 +\n 101 +",
            "\t\tpublic void setThirdVertex(final Integer vertex3) {\n\t\t\tthis.setField(vertex3, V3);\n\t\t}"
        ],
        [
            "LinearRegression::UpdateAccumulator::reduce(Tuple2,Tuple2)",
            " 234  \n 235  \n 236  \n 237 -\n 238 -\n 239 -\n 240 -\n 241  \n 242  ",
            "\t\t@Override\n\t\tpublic Tuple2<Params, Integer> reduce(Tuple2<Params, Integer> val1, Tuple2<Params, Integer> val2) {\n\n\t\t\tdouble new_theta0 = val1.f0.theta0 + val2.f0.theta0;\n\t\t\tdouble new_theta1 = val1.f0.theta1 + val2.f0.theta1;\n\t\t\tParams result = new Params(new_theta0,new_theta1);\n\t\t\treturn new Tuple2<Params, Integer>( result, val1.f1 + val2.f1);\n\n\t\t}",
            " 230  \n 231  \n 232  \n 233 +\n 234 +\n 235 +\n 236 +\n 237  \n 238  ",
            "\t\t@Override\n\t\tpublic Tuple2<Params, Integer> reduce(Tuple2<Params, Integer> val1, Tuple2<Params, Integer> val2) {\n\n\t\t\tdouble newTheta0 = val1.f0.theta0 + val2.f0.theta0;\n\t\t\tdouble newTheta1 = val1.f0.theta1 + val2.f0.theta1;\n\t\t\tParams result = new Params(newTheta0, newTheta1);\n\t\t\treturn new Tuple2<Params, Integer>(result, val1.f1 + val2.f1);\n\n\t\t}"
        ],
        [
            "PiEstimation::main(String)",
            "  50  \n  51  \n  52  \n  53 -\n  54  \n  55 -\n  56  \n  57  \n  58 -\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal long numSamples = args.length > 0 ? Long.parseLong(args[0]) : 1000000;\n\t\t\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\n\t\t// count how many of the samples would randomly fall into\n\t\t// the unit circle\n\t\tDataSet<Long> count = \n\t\t\t\tenv.generateSequence(1, numSamples)\n\t\t\t\t.map(new Sampler())\n\t\t\t\t.reduce(new SumReducer());\n\n\t\tlong theCount = count.collect().get(0);\n\n\t\tSystem.out.println(\"We estimate Pi to be: \" + (theCount * 4.0 / numSamples));\n\t}",
            "  49  \n  50  \n  51  \n  52 +\n  53  \n  54 +\n  55  \n  56  \n  57 +\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal long numSamples = args.length > 0 ? Long.parseLong(args[0]) : 1000000;\n\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// count how many of the samples would randomly fall into\n\t\t// the unit circle\n\t\tDataSet<Long> count =\n\t\t\t\tenv.generateSequence(1, numSamples)\n\t\t\t\t.map(new Sampler())\n\t\t\t\t.reduce(new SumReducer());\n\n\t\tlong theCount = count.collect().get(0);\n\n\t\tSystem.out.println(\"We estimate Pi to be: \" + (theCount * 4.0 / numSamples));\n\t}"
        ],
        [
            "KMeansDataGenerator::uniformRandomCenters(Random,int,int,double)",
            " 144  \n 145  \n 146  \n 147 -\n 148  \n 149 -\n 150  \n 151  \n 152  \n 153  \n 154  ",
            "\tprivate static double[][] uniformRandomCenters(Random rnd, int num, int dimensionality, double range) {\n\t\tfinal double halfRange = range / 2;\n\t\tfinal double[][] points = new double[num][dimensionality];\n\t\t\n\t\tfor (int i = 0; i < num; i++) {\n\t\t\tfor (int dim = 0; dim < dimensionality; dim ++) {\n\t\t\t\tpoints[i][dim] = (rnd.nextDouble() * range) - halfRange;\n\t\t\t}\n\t\t}\n\t\treturn points;\n\t}",
            " 142  \n 143  \n 144  \n 145 +\n 146  \n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  ",
            "\tprivate static double[][] uniformRandomCenters(Random rnd, int num, int dimensionality, double range) {\n\t\tfinal double halfRange = range / 2;\n\t\tfinal double[][] points = new double[num][dimensionality];\n\n\t\tfor (int i = 0; i < num; i++) {\n\t\t\tfor (int dim = 0; dim < dimensionality; dim++) {\n\t\t\t\tpoints[i][dim] = (rnd.nextDouble() * range) - halfRange;\n\t\t\t}\n\t\t}\n\t\treturn points;\n\t}"
        ],
        [
            "TPCHQuery3::Order::getOrderKey()",
            " 205 -",
            "\t\tpublic Long getOrderKey() { return this.f0; }",
            " 215 +\n 216 +\n 217 +",
            "\t\tpublic Long getOrderKey() {\n\t\t\treturn this.f0;\n\t\t}"
        ],
        [
            "KMeansDataGenerator::main(String)",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93 -\n  94  \n  95  \n  96 -\n  97  \n  98  \n  99  \n 100 -\n 101  \n 102 -\n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 -\n 122  \n 123  \n 124  \n 125 -\n 126  \n 127 -\n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140 -\n 141 -\n 142  ",
            "\t/**\n\t * Main method to generate data for the {@link KMeans} example program.\n\t * <p>\n\t * The generator creates to files:\n\t * <ul>\n\t * <li><code>&lt; output-path &gt;/points</code> for the data points\n\t * <li><code>&lt; output-path &gt;/centers</code> for the cluster centers\n\t * </ul> \n\t * \n\t * @param args \n\t * <ol>\n\t * <li>Int: Number of data points\n\t * <li>Int: Number of cluster centers\n\t * <li><b>Optional</b> String: Output path, default value is {tmp.dir}\n\t * <li><b>Optional</b> Double: Standard deviation of data points\n\t * <li><b>Optional</b> Double: Value range of cluster centers\n\t * <li><b>Optional</b> Long: Random seed\n\t * </ol>\n\t *\n\t * @throws IOException\n\t */\n\tpublic static void main(String[] args) throws IOException {\n\n\t\t// check parameter count\n\t\tif (args.length < 2) {\n\t\t\tSystem.out.println(\"KMeansDataGenerator -points <num> -k <num clusters> [-output <output-path>] [-stddev <relative stddev>] [-range <centroid range>] [-seed <seed>]\");\n\t\t\tSystem.exit(1);\n\t\t}\n\n\t\t// parse parameters\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\tfinal int numDataPoints = params.getInt(\"points\");\n\t\tfinal int k = params.getInt(\"k\");\n\t\tfinal String outDir = params.get(\"output\", System.getProperty(\"java.io.tmpdir\"));\n\t\tfinal double stddev = params.getDouble(\"stddev\", RELATIVE_STDDEV);\n\t\tfinal double range = params.getDouble(\"range\", DEFAULT_VALUE_RANGE);\n\t\tfinal long firstSeed = params.getLong(\"seed\", DEFAULT_SEED);\n\n\t\t\n\t\tfinal double absoluteStdDev = stddev * range;\n\t\tfinal Random random = new Random(firstSeed);\n\t\t\n\t\t// the means around which data points are distributed\n\t\tfinal double[][] means = uniformRandomCenters(random, k, DIMENSIONALITY, range);\n\t\t\n\t\t// write the points out\n\t\tBufferedWriter pointsOut = null;\n\t\ttry {\n\t\t\tpointsOut = new BufferedWriter(new FileWriter(new File(outDir+\"/\"+POINTS_FILE)));\n\t\t\tStringBuilder buffer = new StringBuilder();\n\t\t\t\n\t\t\tdouble[] point = new double[DIMENSIONALITY];\n\t\t\tint nextCentroid = 0;\n\t\t\t\n\t\t\tfor (int i = 1; i <= numDataPoints; i++) {\n\t\t\t\t// generate a point for the current centroid\n\t\t\t\tdouble[] centroid = means[nextCentroid];\n\t\t\t\tfor (int d = 0; d < DIMENSIONALITY; d++) {\n\t\t\t\t\tpoint[d] = (random.nextGaussian() * absoluteStdDev) + centroid[d];\n\t\t\t\t}\n\t\t\t\twritePoint(point, buffer, pointsOut);\n\t\t\t\tnextCentroid = (nextCentroid + 1) % k;\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tif (pointsOut != null) {\n\t\t\t\tpointsOut.close();\n\t\t\t}\n\t\t}\n\t\t\n\t\t// write the uniformly distributed centers to a file\n\t\tBufferedWriter centersOut = null;\n\t\ttry {\n\t\t\tcentersOut = new BufferedWriter(new FileWriter(new File(outDir+\"/\"+CENTERS_FILE)));\n\t\t\tStringBuilder buffer = new StringBuilder();\n\t\t\t\n\t\t\tdouble[][] centers = uniformRandomCenters(random, k, DIMENSIONALITY, range);\n\t\t\t\n\t\t\tfor (int i = 0; i < k; i++) {\n\t\t\t\twriteCenter(i + 1, centers[i], buffer, centersOut);\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tif (centersOut != null) {\n\t\t\t\tcentersOut.close();\n\t\t\t}\n\t\t}\n\t\t\n\t\tSystem.out.println(\"Wrote \"+numDataPoints+\" data points to \"+outDir+\"/\"+POINTS_FILE);\n\t\tSystem.out.println(\"Wrote \"+k+\" cluster centers to \"+outDir+\"/\"+CENTERS_FILE);\n\t}",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 +\n  92  \n  93  \n  94 +\n  95  \n  96  \n  97  \n  98 +\n  99  \n 100 +\n 101  \n 102  \n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 +\n 120  \n 121  \n 122  \n 123 +\n 124  \n 125 +\n 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137 +\n 138 +\n 139 +\n 140  ",
            "\t/**\n\t * Main method to generate data for the {@link KMeans} example program.\n\t *\n\t * <p>The generator creates to files:\n\t * <ul>\n\t * <li><code>&lt; output-path &gt;/points</code> for the data points\n\t * <li><code>&lt; output-path &gt;/centers</code> for the cluster centers\n\t * </ul>\n\t *\n\t * @param args\n\t * <ol>\n\t * <li>Int: Number of data points\n\t * <li>Int: Number of cluster centers\n\t * <li><b>Optional</b> String: Output path, default value is {tmp.dir}\n\t * <li><b>Optional</b> Double: Standard deviation of data points\n\t * <li><b>Optional</b> Double: Value range of cluster centers\n\t * <li><b>Optional</b> Long: Random seed\n\t * </ol>\n\t *\n\t * @throws IOException\n\t */\n\tpublic static void main(String[] args) throws IOException {\n\n\t\t// check parameter count\n\t\tif (args.length < 2) {\n\t\t\tSystem.out.println(\"KMeansDataGenerator -points <num> -k <num clusters> [-output <output-path>] [-stddev <relative stddev>] [-range <centroid range>] [-seed <seed>]\");\n\t\t\tSystem.exit(1);\n\t\t}\n\n\t\t// parse parameters\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\tfinal int numDataPoints = params.getInt(\"points\");\n\t\tfinal int k = params.getInt(\"k\");\n\t\tfinal String outDir = params.get(\"output\", System.getProperty(\"java.io.tmpdir\"));\n\t\tfinal double stddev = params.getDouble(\"stddev\", RELATIVE_STDDEV);\n\t\tfinal double range = params.getDouble(\"range\", DEFAULT_VALUE_RANGE);\n\t\tfinal long firstSeed = params.getLong(\"seed\", DEFAULT_SEED);\n\n\t\tfinal double absoluteStdDev = stddev * range;\n\t\tfinal Random random = new Random(firstSeed);\n\n\t\t// the means around which data points are distributed\n\t\tfinal double[][] means = uniformRandomCenters(random, k, DIMENSIONALITY, range);\n\n\t\t// write the points out\n\t\tBufferedWriter pointsOut = null;\n\t\ttry {\n\t\t\tpointsOut = new BufferedWriter(new FileWriter(new File(outDir + \"/\" + POINTS_FILE)));\n\t\t\tStringBuilder buffer = new StringBuilder();\n\n\t\t\tdouble[] point = new double[DIMENSIONALITY];\n\t\t\tint nextCentroid = 0;\n\n\t\t\tfor (int i = 1; i <= numDataPoints; i++) {\n\t\t\t\t// generate a point for the current centroid\n\t\t\t\tdouble[] centroid = means[nextCentroid];\n\t\t\t\tfor (int d = 0; d < DIMENSIONALITY; d++) {\n\t\t\t\t\tpoint[d] = (random.nextGaussian() * absoluteStdDev) + centroid[d];\n\t\t\t\t}\n\t\t\t\twritePoint(point, buffer, pointsOut);\n\t\t\t\tnextCentroid = (nextCentroid + 1) % k;\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tif (pointsOut != null) {\n\t\t\t\tpointsOut.close();\n\t\t\t}\n\t\t}\n\n\t\t// write the uniformly distributed centers to a file\n\t\tBufferedWriter centersOut = null;\n\t\ttry {\n\t\t\tcentersOut = new BufferedWriter(new FileWriter(new File(outDir + \"/\" + CENTERS_FILE)));\n\t\t\tStringBuilder buffer = new StringBuilder();\n\n\t\t\tdouble[][] centers = uniformRandomCenters(random, k, DIMENSIONALITY, range);\n\n\t\t\tfor (int i = 0; i < k; i++) {\n\t\t\t\twriteCenter(i + 1, centers[i], buffer, centersOut);\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tif (centersOut != null) {\n\t\t\t\tcentersOut.close();\n\t\t\t}\n\t\t}\n\n\t\tSystem.out.println(\"Wrote \" + numDataPoints + \" data points to \" + outDir + \"/\" + POINTS_FILE);\n\t\tSystem.out.println(\"Wrote \" + k + \" cluster centers to \" + outDir + \"/\" + CENTERS_FILE);\n\t}"
        ],
        [
            "TPCHQuery3::ShippingPriorityItem::setOrderkey(Long)",
            " 224 -",
            "\t\tpublic void setOrderkey(Long orderkey) { this.f0 = orderkey; }",
            " 248 +\n 249 +\n 250 +",
            "\t\tpublic void setOrderkey(Long orderkey) {\n\t\t\tthis.f0 = orderkey;\n\t\t}"
        ],
        [
            "LinearRegression::SubUpdate::map(Data)",
            " 215  \n 216  \n 217  \n 218 -\n 219 -\n 220  \n 221  \n 222 -\n 223 -\n 224  \n 225 -\n 226  ",
            "\t\t@Override\n\t\tpublic Tuple2<Params, Integer> map(Data in) throws Exception {\n\n\t\t\tfor(Params p : parameters){\n\t\t\t\tthis.parameter = p; \n\t\t\t}\n\n\t\t\tdouble theta_0 = parameter.theta0 - 0.01*((parameter.theta0 + (parameter.theta1*in.x)) - in.y);\n\t\t\tdouble theta_1 = parameter.theta1 - 0.01*(((parameter.theta0 + (parameter.theta1*in.x)) - in.y) * in.x);\n\n\t\t\treturn new Tuple2<Params,Integer>(new Params(theta_0,theta_1),count);\n\t\t}",
            " 211  \n 212  \n 213  \n 214 +\n 215 +\n 216  \n 217  \n 218 +\n 219 +\n 220  \n 221 +\n 222  ",
            "\t\t@Override\n\t\tpublic Tuple2<Params, Integer> map(Data in) throws Exception {\n\n\t\t\tfor (Params p : parameters){\n\t\t\t\tthis.parameter = p;\n\t\t\t}\n\n\t\t\tdouble theta0 = parameter.theta0 - 0.01 * ((parameter.theta0 + (parameter.theta1 * in.x)) - in.y);\n\t\t\tdouble theta1 = parameter.theta1 - 0.01 * (((parameter.theta0 + (parameter.theta1 * in.x)) - in.y) * in.x);\n\n\t\t\treturn new Tuple2<Params, Integer>(new Params(theta0, theta1), count);\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::EdgeWithDegrees::setFirstVertex(Integer)",
            "  99 -",
            "\t\tpublic void setFirstVertex(final Integer vertex1) { this.setField(vertex1, V1); }",
            " 133 +\n 134 +\n 135 +",
            "\t\tpublic void setFirstVertex(final Integer vertex1) {\n\t\t\tthis.setField(vertex1, V1);\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::EdgeWithDegrees::setFirstDegree(Integer)",
            " 103 -",
            "\t\tpublic void setFirstDegree(final Integer degree1) { this.setField(degree1, D1); }",
            " 141 +\n 142 +\n 143 +",
            "\t\tpublic void setFirstDegree(final Integer degree1) {\n\t\t\tthis.setField(degree1, D1);\n\t\t}"
        ],
        [
            "TPCHQuery3::ShippingPriorityItem::ShippingPriorityItem(Long,Double,String,Long)",
            " 215 -\n 216 -\n 217 -\n 218  \n 219 -\n 220 -\n 221  ",
            "\t\tpublic ShippingPriorityItem(Long o_orderkey, Double revenue,\n\t\t\t\tString o_orderdate, Long o_shippriority) {\n\t\t\tthis.f0 = o_orderkey;\n\t\t\tthis.f1 = revenue;\n\t\t\tthis.f2 = o_orderdate;\n\t\t\tthis.f3 = o_shippriority;\n\t\t}",
            " 236 +\n 237 +\n 238 +\n 239 +\n 240 +\n 241 +\n 242 +",
            "\t\tpublic ShippingPriorityItem(Long orderkey, Double revenue,\n\t\t\t\tString orderdate, Long shippriority) {\n\t\t\tthis.f0 = orderkey;\n\t\t\tthis.f1 = revenue;\n\t\t\tthis.f2 = orderdate;\n\t\t\tthis.f3 = shippriority;\n\t\t}"
        ],
        [
            "SocketWindowWordCountITCase::ServerThread::run()",
            " 132  \n 133  \n 134  \n 135 -\n 136  \n 137 -\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  ",
            "\t\t@Override\n\t\tpublic void run() {\n\t\t\ttry {\n\t\t\t\ttry (Socket socket = serverSocket.accept(); \n\t\t\t\t\t\tPrintWriter writer = new PrintWriter(socket.getOutputStream(), true)) {\n\t\t\t\t\t\n\t\t\t\t\twriter.println(WordCountData.TEXT);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tthis.error = t;\n\t\t\t}\n\t\t}",
            " 134  \n 135  \n 136  \n 137 +\n 138  \n 139 +\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  ",
            "\t\t@Override\n\t\tpublic void run() {\n\t\t\ttry {\n\t\t\t\ttry (Socket socket = serverSocket.accept();\n\t\t\t\t\t\tPrintWriter writer = new PrintWriter(socket.getOutputStream(), true)) {\n\n\t\t\t\t\twriter.println(WordCountData.TEXT);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tthis.error = t;\n\t\t\t}\n\t\t}"
        ],
        [
            "PageRank::BuildOutgoingEdgeList::reduce(Iterable,Collector)",
            " 168  \n 169  \n 170  \n 171  \n 172 -\n 173  \n 174  \n 175  \n 176  \n 177  \n 178  ",
            "\t\t@Override\n\t\tpublic void reduce(Iterable<Tuple2<Long, Long>> values, Collector<Tuple2<Long, Long[]>> out) {\n\t\t\tneighbors.clear();\n\t\t\tLong id = 0L;\n\t\t\t\n\t\t\tfor (Tuple2<Long, Long> n : values) {\n\t\t\t\tid = n.f0;\n\t\t\t\tneighbors.add(n.f1);\n\t\t\t}\n\t\t\tout.collect(new Tuple2<Long, Long[]>(id, neighbors.toArray(new Long[neighbors.size()])));\n\t\t}",
            " 162  \n 163  \n 164  \n 165  \n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "\t\t@Override\n\t\tpublic void reduce(Iterable<Tuple2<Long, Long>> values, Collector<Tuple2<Long, Long[]>> out) {\n\t\t\tneighbors.clear();\n\t\t\tLong id = 0L;\n\n\t\t\tfor (Tuple2<Long, Long> n : values) {\n\t\t\t\tid = n.f0;\n\t\t\t\tneighbors.add(n.f1);\n\t\t\t}\n\t\t\tout.collect(new Tuple2<Long, Long[]>(id, neighbors.toArray(new Long[neighbors.size()])));\n\t\t}"
        ],
        [
            "WindowJoin::runWindowJoin(DataStream,DataStream,long)",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91 -\n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "\tpublic static DataStream<Tuple3<String, Integer, Integer>> runWindowJoin(\n\t\t\tDataStream<Tuple2<String, Integer>> grades,\n\t\t\tDataStream<Tuple2<String, Integer>> salaries,\n\t\t\tlong windowSize) {\n\n\t\treturn grades.join(salaries)\n\t\t\t\t.where(new NameKeySelector())\n\t\t\t\t.equalTo(new NameKeySelector())\n\t\t\t\t\n\t\t\t\t.window(TumblingEventTimeWindows.of(Time.milliseconds(windowSize)))\n\t\t\t\t\n\t\t\t\t.apply(new JoinFunction<Tuple2<String, Integer>, Tuple2<String, Integer>, Tuple3<String, Integer, Integer>>() {\n\t\t\t\t\t\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Tuple3<String, Integer, Integer> join(\n\t\t\t\t\t\t\t\t\tTuple2<String, Integer> first,\n\t\t\t\t\t\t\t\t\tTuple2<String, Integer> second) {\n\t\t\t\t\t\treturn new Tuple3<String, Integer, Integer>(first.f0, first.f1, second.f1);\n\t\t\t\t\t}\n\t\t\t\t});\n\t}",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 +\n  89  \n  90 +\n  91  \n  92 +\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  ",
            "\tpublic static DataStream<Tuple3<String, Integer, Integer>> runWindowJoin(\n\t\t\tDataStream<Tuple2<String, Integer>> grades,\n\t\t\tDataStream<Tuple2<String, Integer>> salaries,\n\t\t\tlong windowSize) {\n\n\t\treturn grades.join(salaries)\n\t\t\t\t.where(new NameKeySelector())\n\t\t\t\t.equalTo(new NameKeySelector())\n\n\t\t\t\t.window(TumblingEventTimeWindows.of(Time.milliseconds(windowSize)))\n\n\t\t\t\t.apply(new JoinFunction<Tuple2<String, Integer>, Tuple2<String, Integer>, Tuple3<String, Integer, Integer>>() {\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Tuple3<String, Integer, Integer> join(\n\t\t\t\t\t\t\t\t\tTuple2<String, Integer> first,\n\t\t\t\t\t\t\t\t\tTuple2<String, Integer> second) {\n\t\t\t\t\t\treturn new Tuple3<String, Integer, Integer>(first.f0, first.f1, second.f1);\n\t\t\t\t\t}\n\t\t\t\t});\n\t}"
        ],
        [
            "WordCount::main(String)",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78 -\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up the execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input data\n\t\tDataSet<String> text;\n\t\tif (params.has(\"input\")) {\n\t\t\t// read the text file from given input path\n\t\t\ttext = env.readTextFile(params.get(\"input\"));\n\t\t} else {\n\t\t\t// get default test text data\n\t\t\tSystem.out.println(\"Executing WordCount example with default input data set.\");\n\t\t\tSystem.out.println(\"Use --input to specify file input.\");\n\t\t\ttext = WordCountData.getDefaultTextLineDataSet(env);\n\t\t}\n\n\t\tDataSet<Tuple2<String, Integer>> counts = \n\t\t\t\t// split up the lines in pairs (2-tuples) containing: (word,1)\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t// group by the tuple field \"0\" and sum up tuple field \"1\"\n\t\t\t\t.groupBy(0)\n\t\t\t\t.sum(1);\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tcounts.writeAsCsv(params.get(\"output\"), \"\\n\", \" \");\n\t\t\t// execute program\n\t\t\tenv.execute(\"WordCount Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tcounts.print();\n\t\t}\n\n\t}",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up the execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input data\n\t\tDataSet<String> text;\n\t\tif (params.has(\"input\")) {\n\t\t\t// read the text file from given input path\n\t\t\ttext = env.readTextFile(params.get(\"input\"));\n\t\t} else {\n\t\t\t// get default test text data\n\t\t\tSystem.out.println(\"Executing WordCount example with default input data set.\");\n\t\t\tSystem.out.println(\"Use --input to specify file input.\");\n\t\t\ttext = WordCountData.getDefaultTextLineDataSet(env);\n\t\t}\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t// split up the lines in pairs (2-tuples) containing: (word,1)\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t// group by the tuple field \"0\" and sum up tuple field \"1\"\n\t\t\t\t.groupBy(0)\n\t\t\t\t.sum(1);\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tcounts.writeAsCsv(params.get(\"output\"), \"\\n\", \" \");\n\t\t\t// execute program\n\t\t\tenv.execute(\"WordCount Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tcounts.print();\n\t\t}\n\n\t}"
        ],
        [
            "WebLogAnalysis::getVisitsDataSet(ExecutionEnvironment,ParameterTool)",
            " 290  \n 291  \n 292 -\n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  ",
            "\tprivate static DataSet<Tuple2<String, String>> getVisitsDataSet(ExecutionEnvironment env, ParameterTool params) {\n\t\t// Create DataSet for visits relation (URL, Date)\n\t\tif(params.has(\"visits\")) {\n\t\t\treturn env.readCsvFile(params.get(\"visits\"))\n\t\t\t\t\t\t.fieldDelimiter(\"|\")\n\t\t\t\t\t\t.includeFields(\"011000000\")\n\t\t\t\t\t\t.types(String.class, String.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing WebLogAnalysis example with default visits data set.\");\n\t\t\tSystem.out.println(\"Use --visits to specify file input.\");\n\t\t\treturn WebLogData.getVisitDataSet(env);\n\t\t}\n\t}",
            " 286  \n 287  \n 288 +\n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  ",
            "\tprivate static DataSet<Tuple2<String, String>> getVisitsDataSet(ExecutionEnvironment env, ParameterTool params) {\n\t\t// Create DataSet for visits relation (URL, Date)\n\t\tif (params.has(\"visits\")) {\n\t\t\treturn env.readCsvFile(params.get(\"visits\"))\n\t\t\t\t\t\t.fieldDelimiter(\"|\")\n\t\t\t\t\t\t.includeFields(\"011000000\")\n\t\t\t\t\t\t.types(String.class, String.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing WebLogAnalysis example with default visits data set.\");\n\t\t\tSystem.out.println(\"Use --visits to specify file input.\");\n\t\t\treturn WebLogData.getVisitDataSet(env);\n\t\t}\n\t}"
        ],
        [
            "SessionWindowing::main(String)",
            "  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 -\n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\t@SuppressWarnings(\"serial\")\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\t\tenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\tenv.setParallelism(1);\n\n\t\tfinal boolean fileOutput = params.has(\"output\");\n\n\t\tfinal List<Tuple3<String, Long, Integer>> input = new ArrayList<>();\n\n\t\tinput.add(new Tuple3<>(\"a\", 1L, 1));\n\t\tinput.add(new Tuple3<>(\"b\", 1L, 1));\n\t\tinput.add(new Tuple3<>(\"b\", 3L, 1));\n\t\tinput.add(new Tuple3<>(\"b\", 5L, 1));\n\t\tinput.add(new Tuple3<>(\"c\", 6L, 1));\n\t\t// We expect to detect the session \"a\" earlier than this point (the old\n\t\t// functionality can only detect here when the next starts)\n\t\tinput.add(new Tuple3<>(\"a\", 10L, 1));\n\t\t// We expect to detect session \"b\" and \"c\" at this point as well\n\t\tinput.add(new Tuple3<>(\"c\", 11L, 1));\n\n\t\tDataStream<Tuple3<String, Long, Integer>> source = env\n\t\t\t\t.addSource(new SourceFunction<Tuple3<String,Long,Integer>>() {\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void run(SourceContext<Tuple3<String, Long, Integer>> ctx) throws Exception {\n\t\t\t\t\t\tfor (Tuple3<String, Long, Integer> value : input) {\n\t\t\t\t\t\t\tctx.collectWithTimestamp(value, value.f1);\n\t\t\t\t\t\t\tctx.emitWatermark(new Watermark(value.f1 - 1));\n\t\t\t\t\t\t\tif (!fileOutput) {\n\t\t\t\t\t\t\t\tSystem.out.println(\"Collected: \" + value);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tctx.emitWatermark(new Watermark(Long.MAX_VALUE));\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void cancel() {\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\t// We create sessions for each id with max timeout of 3 time units\n\t\tDataStream<Tuple3<String, Long, Integer>> aggregated = source\n\t\t\t\t.keyBy(0)\n\t\t\t\t.window(EventTimeSessionWindows.withGap(Time.milliseconds(3L)))\n\t\t\t\t.sum(2);\n\n\t\tif (fileOutput) {\n\t\t\taggregated.writeAsText(params.get(\"output\"));\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\taggregated.print();\n\t\t}\n\n\t\tenv.execute();\n\t}",
            "  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  ",
            "\t@SuppressWarnings(\"serial\")\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\t\tenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\tenv.setParallelism(1);\n\n\t\tfinal boolean fileOutput = params.has(\"output\");\n\n\t\tfinal List<Tuple3<String, Long, Integer>> input = new ArrayList<>();\n\n\t\tinput.add(new Tuple3<>(\"a\", 1L, 1));\n\t\tinput.add(new Tuple3<>(\"b\", 1L, 1));\n\t\tinput.add(new Tuple3<>(\"b\", 3L, 1));\n\t\tinput.add(new Tuple3<>(\"b\", 5L, 1));\n\t\tinput.add(new Tuple3<>(\"c\", 6L, 1));\n\t\t// We expect to detect the session \"a\" earlier than this point (the old\n\t\t// functionality can only detect here when the next starts)\n\t\tinput.add(new Tuple3<>(\"a\", 10L, 1));\n\t\t// We expect to detect session \"b\" and \"c\" at this point as well\n\t\tinput.add(new Tuple3<>(\"c\", 11L, 1));\n\n\t\tDataStream<Tuple3<String, Long, Integer>> source = env\n\t\t\t\t.addSource(new SourceFunction<Tuple3<String, Long, Integer>>() {\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void run(SourceContext<Tuple3<String, Long, Integer>> ctx) throws Exception {\n\t\t\t\t\t\tfor (Tuple3<String, Long, Integer> value : input) {\n\t\t\t\t\t\t\tctx.collectWithTimestamp(value, value.f1);\n\t\t\t\t\t\t\tctx.emitWatermark(new Watermark(value.f1 - 1));\n\t\t\t\t\t\t\tif (!fileOutput) {\n\t\t\t\t\t\t\t\tSystem.out.println(\"Collected: \" + value);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tctx.emitWatermark(new Watermark(Long.MAX_VALUE));\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void cancel() {\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\t// We create sessions for each id with max timeout of 3 time units\n\t\tDataStream<Tuple3<String, Long, Integer>> aggregated = source\n\t\t\t\t.keyBy(0)\n\t\t\t\t.window(EventTimeSessionWindows.withGap(Time.milliseconds(3L)))\n\t\t\t\t.sum(2);\n\n\t\tif (fileOutput) {\n\t\t\taggregated.writeAsText(params.get(\"output\"));\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\taggregated.print();\n\t\t}\n\n\t\tenv.execute();\n\t}"
        ],
        [
            "WindowJoinITCase::testProgram()",
            "  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44 -\n  45  \n  46  \n  47  \n  48 -\n  49  \n  50  \n  51  \n  52 -\n  53  \n  54  \n  55  \n  56 -\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  ",
            "\t@Test\n\tpublic void testProgram() throws Exception {\n\t\tfinal String resultPath = File.createTempFile(\"result-path\", \"dir\").toURI().toString();\n\t\ttry {\n\t\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\t\t\t\n\t\t\tDataStream<Tuple2<String, Integer>> grades = env\n\t\t\t\t\t.fromElements(WindowJoinData.GRADES_INPUT.split(\"\\n\"))\n\t\t\t\t\t.map(new Parser());\n\t\n\t\t\tDataStream<Tuple2<String, Integer>> salaries = env\n\t\t\t\t\t.fromElements(WindowJoinData.SALARIES_INPUT.split(\"\\n\"))\n\t\t\t\t\t.map(new Parser());\n\t\t\t\n\t\t\tWindowJoin\n\t\t\t\t\t.runWindowJoin(grades, salaries, 100)\n\t\t\t\t\t.writeAsText(resultPath, WriteMode.OVERWRITE);\n\t\t\t\n\t\t\tenv.execute();\n\n\t\t\t// since the two sides of the join might have different speed\n\t\t\t// the exact output can not be checked just whether it is well-formed\n\t\t\t// checks that the result lines look like e.g. (bob, 2, 2015)\n\t\t\tcheckLinesAgainstRegexp(resultPath, \"^\\\\([a-z]+,(\\\\d),(\\\\d)+\\\\)\");\n\t\t}\n\t\tfinally {\n\t\t\ttry {\n\t\t\t\tFileUtils.deleteDirectory(new File(resultPath));\n\t\t\t} catch (Throwable ignored) {}\n\t\t}\n\t}",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47 +\n  48  \n  49  \n  50  \n  51 +\n  52  \n  53  \n  54  \n  55 +\n  56  \n  57  \n  58  \n  59 +\n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "\t@Test\n\tpublic void testProgram() throws Exception {\n\t\tfinal String resultPath = File.createTempFile(\"result-path\", \"dir\").toURI().toString();\n\t\ttry {\n\t\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\n\t\t\tDataStream<Tuple2<String, Integer>> grades = env\n\t\t\t\t\t.fromElements(WindowJoinData.GRADES_INPUT.split(\"\\n\"))\n\t\t\t\t\t.map(new Parser());\n\n\t\t\tDataStream<Tuple2<String, Integer>> salaries = env\n\t\t\t\t\t.fromElements(WindowJoinData.SALARIES_INPUT.split(\"\\n\"))\n\t\t\t\t\t.map(new Parser());\n\n\t\t\tWindowJoin\n\t\t\t\t\t.runWindowJoin(grades, salaries, 100)\n\t\t\t\t\t.writeAsText(resultPath, WriteMode.OVERWRITE);\n\n\t\t\tenv.execute();\n\n\t\t\t// since the two sides of the join might have different speed\n\t\t\t// the exact output can not be checked just whether it is well-formed\n\t\t\t// checks that the result lines look like e.g. (bob, 2, 2015)\n\t\t\tcheckLinesAgainstRegexp(resultPath, \"^\\\\([a-z]+,(\\\\d),(\\\\d)+\\\\)\");\n\t\t}\n\t\tfinally {\n\t\t\ttry {\n\t\t\t\tFileUtils.deleteDirectory(new File(resultPath));\n\t\t\t} catch (Throwable ignored) {}\n\t\t}\n\t}"
        ],
        [
            "TPCHQuery10::main(String)",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137  \n 138  \n 139 -\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 -\n 158  \n 159  \n 160  \n 161 -\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168 -\n 169  \n 170  \n 171 -\n 172  \n 173  \n 174 -\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185 -\n 186  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tif (!params.has(\"customer\") && !params.has(\"orders\") && !params.has(\"lineitem\") && !params.has(\"nation\")) {\n\t\t\tSystem.err.println(\"  This program expects data from the TPC-H benchmark as input data.\");\n\t\t\tSystem.err.println(\"  Due to legal restrictions, we can not ship generated data.\");\n\t\t\tSystem.err.println(\"  You can find the TPC-H data generator at http://www.tpc.org/tpch/.\");\n\t\t\tSystem.err.println(\"  Usage: TPCHQuery10 --customer <path> --orders <path> --lineitem <path> --nation <path> [--output <path>]\");\n\t\t\treturn;\n\t\t}\n\n\t\t// get customer data set: (custkey, name, address, nationkey, acctbal) \n\t\tDataSet<Tuple5<Integer, String, String, Integer, Double>> customers =\n\t\t\tgetCustomerDataSet(env, params.get(\"customer\"));\n\t\t// get orders data set: (orderkey, custkey, orderdate)\n\t\tDataSet<Tuple3<Integer, Integer, String>> orders =\n\t\t\tgetOrdersDataSet(env, params.get(\"orders\"));\n\t\t// get lineitem data set: (orderkey, extendedprice, discount, returnflag)\n\t\tDataSet<Tuple4<Integer, Double, Double, String>> lineitems =\n\t\t\tgetLineitemDataSet(env, params.get(\"lineitem\"));\n\t\t// get nation data set: (nationkey, name)\n\t\tDataSet<Tuple2<Integer, String>> nations =\n\t\t\tgetNationsDataSet(env, params.get(\"nation\"));\n\n\t\t// orders filtered by year: (orderkey, custkey)\n\t\tDataSet<Tuple2<Integer, Integer>> ordersFilteredByYear =\n\t\t\t\t// filter by year\n\t\t\t\torders.filter(\n\t\t\t\t\t\t\t\tnew FilterFunction<Tuple3<Integer,Integer, String>>() {\n\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\tpublic boolean filter(Tuple3<Integer, Integer, String> o) {\n\t\t\t\t\t\t\t\t\t\treturn Integer.parseInt(o.f2.substring(0, 4)) > 1990;\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t})\n\t\t\t\t// project fields out that are no longer required\n\t\t\t\t.project(0,1);\n\n\t\t// lineitems filtered by flag: (orderkey, revenue)\n\t\tDataSet<Tuple2<Integer, Double>> lineitemsFilteredByFlag = \n\t\t\t\t// filter by flag\n\t\t\t\tlineitems.filter(new FilterFunction<Tuple4<Integer, Double, Double, String>>() {\n\t\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\t\tpublic boolean filter(Tuple4<Integer, Double, Double, String> l) {\n\t\t\t\t\t\t\t\t\t\t\treturn l.f3.equals(\"R\");\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t})\n\t\t\t\t// compute revenue and project out return flag\n\t\t\t\t.map(new MapFunction<Tuple4<Integer, Double, Double, String>, Tuple2<Integer, Double>>() {\n\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\tpublic Tuple2<Integer, Double> map(Tuple4<Integer, Double, Double, String> l) {\n\t\t\t\t\t\t\t\t// revenue per item = l_extendedprice * (1 - l_discount)\n\t\t\t\t\t\t\t\treturn new Tuple2<Integer, Double>(l.f0, l.f1 * (1 - l.f2));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t});\n\n\t\t// join orders with lineitems: (custkey, revenue)\n\t\tDataSet<Tuple2<Integer, Double>> revenueByCustomer = \n\t\t\t\tordersFilteredByYear.joinWithHuge(lineitemsFilteredByFlag)\n\t\t\t\t\t\t\t\t\t.where(0).equalTo(0)\n\t\t\t\t\t\t\t\t\t.projectFirst(1).projectSecond(1);\n\t\t\n\t\trevenueByCustomer = revenueByCustomer.groupBy(0).aggregate(Aggregations.SUM, 1);\n\n\t\t// join customer with nation (custkey, name, address, nationname, acctbal)\n\t\tDataSet<Tuple5<Integer, String, String, String, Double>> customerWithNation = customers\n\t\t\t\t\t\t.joinWithTiny(nations)\n\t\t\t\t\t\t.where(3).equalTo(0)\n\t\t\t\t\t\t.projectFirst(0,1,2).projectSecond(1).projectFirst(4);\n\n\t\t// join customer (with nation) with revenue (custkey, name, address, nationname, acctbal, revenue)\n\t\tDataSet<Tuple6<Integer, String, String, String, Double, Double>> result = \n\t\t\t\tcustomerWithNation.join(revenueByCustomer)\n\t\t\t\t.where(0).equalTo(0)\n\t\t\t\t.projectFirst(0,1,2,3,4).projectSecond(1);\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tresult.writeAsCsv(params.get(\"output\"), \"\\n\", \"|\");\n\t\t\t// execute program\n\t\t\tenv.execute(\"TPCH Query 10 Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\t\t\n\t}",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 +\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129 +\n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 +\n 151  \n 152  \n 153  \n 154 +\n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 +\n 162  \n 163  \n 164 +\n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tif (!params.has(\"customer\") && !params.has(\"orders\") && !params.has(\"lineitem\") && !params.has(\"nation\")) {\n\t\t\tSystem.err.println(\"  This program expects data from the TPC-H benchmark as input data.\");\n\t\t\tSystem.err.println(\"  Due to legal restrictions, we can not ship generated data.\");\n\t\t\tSystem.err.println(\"  You can find the TPC-H data generator at http://www.tpc.org/tpch/.\");\n\t\t\tSystem.err.println(\"  Usage: TPCHQuery10 --customer <path> --orders <path> --lineitem <path> --nation <path> [--output <path>]\");\n\t\t\treturn;\n\t\t}\n\n\t\t// get customer data set: (custkey, name, address, nationkey, acctbal)\n\t\tDataSet<Tuple5<Integer, String, String, Integer, Double>> customers =\n\t\t\tgetCustomerDataSet(env, params.get(\"customer\"));\n\t\t// get orders data set: (orderkey, custkey, orderdate)\n\t\tDataSet<Tuple3<Integer, Integer, String>> orders =\n\t\t\tgetOrdersDataSet(env, params.get(\"orders\"));\n\t\t// get lineitem data set: (orderkey, extendedprice, discount, returnflag)\n\t\tDataSet<Tuple4<Integer, Double, Double, String>> lineitems =\n\t\t\tgetLineitemDataSet(env, params.get(\"lineitem\"));\n\t\t// get nation data set: (nationkey, name)\n\t\tDataSet<Tuple2<Integer, String>> nations =\n\t\t\tgetNationsDataSet(env, params.get(\"nation\"));\n\n\t\t// orders filtered by year: (orderkey, custkey)\n\t\tDataSet<Tuple2<Integer, Integer>> ordersFilteredByYear =\n\t\t\t\t// filter by year\n\t\t\t\torders.filter(\n\t\t\t\t\t\t\t\tnew FilterFunction<Tuple3<Integer, Integer, String>>() {\n\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\tpublic boolean filter(Tuple3<Integer, Integer, String> o) {\n\t\t\t\t\t\t\t\t\t\treturn Integer.parseInt(o.f2.substring(0, 4)) > 1990;\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t})\n\t\t\t\t// project fields out that are no longer required\n\t\t\t\t.project(0, 1);\n\n\t\t// lineitems filtered by flag: (orderkey, revenue)\n\t\tDataSet<Tuple2<Integer, Double>> lineitemsFilteredByFlag =\n\t\t\t\t// filter by flag\n\t\t\t\tlineitems.filter(new FilterFunction<Tuple4<Integer, Double, Double, String>>() {\n\t\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\t\tpublic boolean filter(Tuple4<Integer, Double, Double, String> l) {\n\t\t\t\t\t\t\t\t\t\t\treturn l.f3.equals(\"R\");\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t})\n\t\t\t\t// compute revenue and project out return flag\n\t\t\t\t.map(new MapFunction<Tuple4<Integer, Double, Double, String>, Tuple2<Integer, Double>>() {\n\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\tpublic Tuple2<Integer, Double> map(Tuple4<Integer, Double, Double, String> l) {\n\t\t\t\t\t\t\t\t// revenue per item = l_extendedprice * (1 - l_discount)\n\t\t\t\t\t\t\t\treturn new Tuple2<Integer, Double>(l.f0, l.f1 * (1 - l.f2));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t});\n\n\t\t// join orders with lineitems: (custkey, revenue)\n\t\tDataSet<Tuple2<Integer, Double>> revenueByCustomer =\n\t\t\t\tordersFilteredByYear.joinWithHuge(lineitemsFilteredByFlag)\n\t\t\t\t\t\t\t\t\t.where(0).equalTo(0)\n\t\t\t\t\t\t\t\t\t.projectFirst(1).projectSecond(1);\n\n\t\trevenueByCustomer = revenueByCustomer.groupBy(0).aggregate(Aggregations.SUM, 1);\n\n\t\t// join customer with nation (custkey, name, address, nationname, acctbal)\n\t\tDataSet<Tuple5<Integer, String, String, String, Double>> customerWithNation = customers\n\t\t\t\t\t\t.joinWithTiny(nations)\n\t\t\t\t\t\t.where(3).equalTo(0)\n\t\t\t\t\t\t.projectFirst(0, 1, 2).projectSecond(1).projectFirst(4);\n\n\t\t// join customer (with nation) with revenue (custkey, name, address, nationname, acctbal, revenue)\n\t\tDataSet<Tuple6<Integer, String, String, String, Double, Double>> result =\n\t\t\t\tcustomerWithNation.join(revenueByCustomer)\n\t\t\t\t.where(0).equalTo(0)\n\t\t\t\t.projectFirst(0, 1, 2, 3, 4).projectSecond(1);\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tresult.writeAsCsv(params.get(\"output\"), \"\\n\", \"|\");\n\t\t\t// execute program\n\t\t\tenv.execute(\"TPCH Query 10 Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\n\t}"
        ],
        [
            "WebLogAnalysis::FilterVisitsByDate::filter(Tuple2)",
            " 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 -\n 226  \n 227  ",
            "\t\t/**\n\t\t * Filters for records of the visits relation where the year of visit is equal to a\n\t\t * specified value. The URL of all visit records passing the filter is emitted.\n\t\t *\n\t\t * Output Format:\n\t\t * 0: URL\n\t\t * 1: DATE\n\t\t */\n\t\t@Override\n\t\tpublic boolean filter(Tuple2<String, String> value) throws Exception {\n\t\t\t// Parse date string with the format YYYY-MM-DD and extract the year\n\t\t\tString dateString = value.f1;\n\t\t\tint year = Integer.parseInt(dateString.substring(0,4));\n\t\t\treturn (year == YEARFILTER);\n\t\t}",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 +\n 222  \n 223  ",
            "\t\t/**\n\t\t * Filters for records of the visits relation where the year of visit is equal to a\n\t\t * specified value. The URL of all visit records passing the filter is emitted.\n\t\t *\n\t\t * <p>Output Format:\n\t\t * 0: URL\n\t\t * 1: DATE\n\t\t */\n\t\t@Override\n\t\tpublic boolean filter(Tuple2<String, String> value) throws Exception {\n\t\t\t// Parse date string with the format YYYY-MM-DD and extract the year\n\t\t\tString dateString = value.f1;\n\t\t\tint year = Integer.parseInt(dateString.substring(0, 4));\n\t\t\treturn (year == YEARFILTER);\n\t\t}"
        ],
        [
            "LinearRegressionDataGenerator::writePoint(double,StringBuilder,BufferedWriter)",
            "  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "\tprivate static void writePoint(double[] data, StringBuilder buffer, BufferedWriter out) throws IOException {\n\t\tbuffer.setLength(0);\n\n\t\t// write coordinates\n\t\tfor (int j = 0; j < data.length; j++) {\n\t\t\tbuffer.append(FORMAT.format(data[j]));\n\t\t\tif(j < data.length - 1) {\n\t\t\t\tbuffer.append(DELIMITER);\n\t\t\t}\n\t\t}\n\n\t\tout.write(buffer.toString());\n\t\tout.newLine();\n\t}",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "\tprivate static void writePoint(double[] data, StringBuilder buffer, BufferedWriter out) throws IOException {\n\t\tbuffer.setLength(0);\n\n\t\t// write coordinates\n\t\tfor (int j = 0; j < data.length; j++) {\n\t\t\tbuffer.append(FORMAT.format(data[j]));\n\t\t\tif (j < data.length - 1) {\n\t\t\t\tbuffer.append(DELIMITER);\n\t\t\t}\n\t\t}\n\n\t\tout.write(buffer.toString());\n\t\tout.newLine();\n\t}"
        ],
        [
            "EnumTriangles::TriadBuilder::reduce(Iterable,Collector)",
            " 168  \n 169  \n 170 -\n 171  \n 172 -\n 173  \n 174  \n 175 -\n 176  \n 177  \n 178  \n 179  \n 180 -\n 181  \n 182  \n 183  \n 184 -\n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "\t\t@Override\n\t\tpublic void reduce(Iterable<Edge> edgesIter, Collector<Triad> out) throws Exception {\n\t\t\t\n\t\t\tfinal Iterator<Edge> edges = edgesIter.iterator();\n\t\t\t\n\t\t\t// clear vertex list\n\t\t\tvertices.clear();\n\t\t\t\n\t\t\t// read first edge\n\t\t\tEdge firstEdge = edges.next();\n\t\t\toutTriad.setFirstVertex(firstEdge.getFirstVertex());\n\t\t\tvertices.add(firstEdge.getSecondVertex());\n\t\t\t\n\t\t\t// build and emit triads\n\t\t\twhile (edges.hasNext()) {\n\t\t\t\tInteger higherVertexId = edges.next().getSecondVertex();\n\t\t\t\t\n\t\t\t\t// combine vertex with all previously read vertices\n\t\t\t\tfor (Integer lowerVertexId : vertices) {\n\t\t\t\t\toutTriad.setSecondVertex(lowerVertexId);\n\t\t\t\t\toutTriad.setThirdVertex(higherVertexId);\n\t\t\t\t\tout.collect(outTriad);\n\t\t\t\t}\n\t\t\t\tvertices.add(higherVertexId);\n\t\t\t}\n\t\t}",
            " 164  \n 165  \n 166 +\n 167  \n 168 +\n 169  \n 170  \n 171 +\n 172  \n 173  \n 174  \n 175  \n 176 +\n 177  \n 178  \n 179  \n 180 +\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  ",
            "\t\t@Override\n\t\tpublic void reduce(Iterable<Edge> edgesIter, Collector<Triad> out) throws Exception {\n\n\t\t\tfinal Iterator<Edge> edges = edgesIter.iterator();\n\n\t\t\t// clear vertex list\n\t\t\tvertices.clear();\n\n\t\t\t// read first edge\n\t\t\tEdge firstEdge = edges.next();\n\t\t\toutTriad.setFirstVertex(firstEdge.getFirstVertex());\n\t\t\tvertices.add(firstEdge.getSecondVertex());\n\n\t\t\t// build and emit triads\n\t\t\twhile (edges.hasNext()) {\n\t\t\t\tInteger higherVertexId = edges.next().getSecondVertex();\n\n\t\t\t\t// combine vertex with all previously read vertices\n\t\t\t\tfor (Integer lowerVertexId : vertices) {\n\t\t\t\t\toutTriad.setSecondVertex(lowerVertexId);\n\t\t\t\t\toutTriad.setThirdVertex(higherVertexId);\n\t\t\t\t\tout.collect(outTriad);\n\t\t\t\t}\n\t\t\t\tvertices.add(higherVertexId);\n\t\t\t}\n\t\t}"
        ],
        [
            "WordCountPojo::Word::toString()",
            "  73  \n  74  \n  75 -\n  76  ",
            "\t\t@Override\n\t\tpublic String toString() {\n\t\t\treturn \"Word=\"+word+\" freq=\"+frequency;\n\t\t}",
            "  73  \n  74  \n  75 +\n  76  ",
            "\t\t@Override\n\t\tpublic String toString() {\n\t\t\treturn \"Word=\" + word + \" freq=\" + frequency;\n\t\t}"
        ],
        [
            "TPCHQuery3::Order::getShippriority()",
            " 208 -",
            "\t\tpublic Long getShippriority() { return this.f3; }",
            " 227 +\n 228 +\n 229 +",
            "\t\tpublic Long getShippriority() {\n\t\t\treturn this.f3;\n\t\t}"
        ],
        [
            "SocketWindowWordCountITCase::testJavaProgram()",
            "  40  \n  41  \n  42  \n  43 -\n  44  \n  45  \n  46  \n  47 -\n  48  \n  49 -\n  50  \n  51  \n  52 -\n  53  \n  54  \n  55 -\n  56  \n  57  \n  58  \n  59 -\n  60  \n  61  \n  62 -\n  63 -\n  64  \n  65  \n  66  \n  67  \n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  ",
            "\t@Test\n\tpublic void testJavaProgram() throws Exception {\n\t\tInetAddress localhost = InetAddress.getByName(\"localhost\");\n\t\t\n\t\t// suppress sysout messages from this example\n\t\tfinal PrintStream originalSysout = System.out;\n\t\tfinal PrintStream originalSyserr = System.err;\n\t\t\n\t\tfinal ByteArrayOutputStream errorMessages = new ByteArrayOutputStream();\n\t\t\n\t\tSystem.setOut(new PrintStream(new NullStream()));\n\t\tSystem.setErr(new PrintStream(errorMessages));\n\t\t\n\t\ttry {\n\t\t\ttry (ServerSocket server = new ServerSocket(0, 10, localhost)) {\n\t\t\t\t\n\t\t\t\tfinal ServerThread serverThread = new ServerThread(server);\n\t\t\t\tserverThread.setDaemon(true);\n\t\t\t\tserverThread.start();\n\t\t\t\t\n\t\t\t\tfinal int serverPort = server.getLocalPort();\n\n\t\t\t\torg.apache.flink.streaming.examples.socket.SocketWindowWordCount.main(\n\t\t\t\t\t\tnew String[] { \"--port\", String.valueOf(serverPort) });\n\n\t\t\t\tif (errorMessages.size() != 0) {\n\t\t\t\t\tfail(\"Found error message: \" + new String(errorMessages.toByteArray(), ConfigConstants.DEFAULT_CHARSET));\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tserverThread.join();\n\t\t\t\tserverThread.checkError();\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tSystem.setOut(originalSysout);\n\t\t\tSystem.setErr(originalSyserr);\n\t\t}\n\t}",
            "  43  \n  44  \n  45  \n  46 +\n  47  \n  48  \n  49  \n  50 +\n  51  \n  52 +\n  53  \n  54  \n  55 +\n  56  \n  57  \n  58 +\n  59  \n  60  \n  61  \n  62 +\n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70 +\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\t@Test\n\tpublic void testJavaProgram() throws Exception {\n\t\tInetAddress localhost = InetAddress.getByName(\"localhost\");\n\n\t\t// suppress sysout messages from this example\n\t\tfinal PrintStream originalSysout = System.out;\n\t\tfinal PrintStream originalSyserr = System.err;\n\n\t\tfinal ByteArrayOutputStream errorMessages = new ByteArrayOutputStream();\n\n\t\tSystem.setOut(new PrintStream(new NullStream()));\n\t\tSystem.setErr(new PrintStream(errorMessages));\n\n\t\ttry {\n\t\t\ttry (ServerSocket server = new ServerSocket(0, 10, localhost)) {\n\n\t\t\t\tfinal ServerThread serverThread = new ServerThread(server);\n\t\t\t\tserverThread.setDaemon(true);\n\t\t\t\tserverThread.start();\n\n\t\t\t\tfinal int serverPort = server.getLocalPort();\n\n\t\t\t\tSocketWindowWordCount.main(new String[] { \"--port\", String.valueOf(serverPort) });\n\n\t\t\t\tif (errorMessages.size() != 0) {\n\t\t\t\t\tfail(\"Found error message: \" + new String(errorMessages.toByteArray(), ConfigConstants.DEFAULT_CHARSET));\n\t\t\t\t}\n\n\t\t\t\tserverThread.join();\n\t\t\t\tserverThread.checkError();\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tSystem.setOut(originalSysout);\n\t\t\tSystem.setErr(originalSyserr);\n\t\t}\n\t}"
        ],
        [
            "EnumTrianglesDataTypes::EdgeWithDegrees::getSecondVertex()",
            "  93 -",
            "\t\tpublic Integer getSecondVertex() { return this.getField(V2); }",
            " 121 +\n 122 +\n 123 +",
            "\t\tpublic Integer getSecondVertex() {\n\t\t\treturn this.getField(V2);\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::Triad::setFirstVertex(Integer)",
            "  74 -",
            "\t\tpublic void setFirstVertex(final Integer vertex1) { this.setField(vertex1, V1); }",
            "  91 +\n  92 +\n  93 +",
            "\t\tpublic void setFirstVertex(final Integer vertex1) {\n\t\t\tthis.setField(vertex1, V1);\n\t\t}"
        ],
        [
            "ConnectedComponents::main(String)",
            "  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98 -\n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106 -\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 -\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "\tpublic static void main(String... args) throws Exception {\n\n\t\t// Checking input parameters\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up execution environment\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tfinal int maxIterations = params.getInt(\"iterations\", 10);\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\t\t\n\t\t// read vertex and edge data\n\t\tDataSet<Long> vertices = getVertexDataSet(env, params);\n\t\tDataSet<Tuple2<Long, Long>> edges = getEdgeDataSet(env, params).flatMap(new UndirectEdge());\n\t\t\n\t\t// assign the initial components (equal to the vertex id)\n\t\tDataSet<Tuple2<Long, Long>> verticesWithInitialId =\n\t\t\tvertices.map(new DuplicateValue<Long>());\n\t\t\t\t\n\t\t// open a delta iteration\n\t\tDeltaIteration<Tuple2<Long, Long>, Tuple2<Long, Long>> iteration =\n\t\t\t\tverticesWithInitialId.iterateDelta(verticesWithInitialId, maxIterations, 0);\n\t\t\n\t\t// apply the step logic: join with the edges, select the minimum neighbor, update if the component of the candidate is smaller\n\t\tDataSet<Tuple2<Long, Long>> changes = iteration.getWorkset().join(edges).where(0).equalTo(0).with(new NeighborWithComponentIDJoin())\n\t\t\t\t.groupBy(0).aggregate(Aggregations.MIN, 1)\n\t\t\t\t.join(iteration.getSolutionSet()).where(0).equalTo(0)\n\t\t\t\t.with(new ComponentIdFilter());\n\n\t\t// close the delta iteration (delta and new workset are identical)\n\t\tDataSet<Tuple2<Long, Long>> result = iteration.closeWith(changes, changes);\n\t\t\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tresult.writeAsCsv(params.get(\"output\"), \"\\n\", \" \");\n\t\t\t// execute program\n\t\t\tenv.execute(\"Connected Components Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\t}",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 +\n  89  \n  90  \n  91  \n  92 +\n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  ",
            "\tpublic static void main(String... args) throws Exception {\n\n\t\t// Checking input parameters\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up execution environment\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tfinal int maxIterations = params.getInt(\"iterations\", 10);\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// read vertex and edge data\n\t\tDataSet<Long> vertices = getVertexDataSet(env, params);\n\t\tDataSet<Tuple2<Long, Long>> edges = getEdgeDataSet(env, params).flatMap(new UndirectEdge());\n\n\t\t// assign the initial components (equal to the vertex id)\n\t\tDataSet<Tuple2<Long, Long>> verticesWithInitialId =\n\t\t\tvertices.map(new DuplicateValue<Long>());\n\n\t\t// open a delta iteration\n\t\tDeltaIteration<Tuple2<Long, Long>, Tuple2<Long, Long>> iteration =\n\t\t\t\tverticesWithInitialId.iterateDelta(verticesWithInitialId, maxIterations, 0);\n\n\t\t// apply the step logic: join with the edges, select the minimum neighbor, update if the component of the candidate is smaller\n\t\tDataSet<Tuple2<Long, Long>> changes = iteration.getWorkset().join(edges).where(0).equalTo(0).with(new NeighborWithComponentIDJoin())\n\t\t\t\t.groupBy(0).aggregate(Aggregations.MIN, 1)\n\t\t\t\t.join(iteration.getSolutionSet()).where(0).equalTo(0)\n\t\t\t\t.with(new ComponentIdFilter());\n\n\t\t// close the delta iteration (delta and new workset are identical)\n\t\tDataSet<Tuple2<Long, Long>> result = iteration.closeWith(changes, changes);\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tresult.writeAsCsv(params.get(\"output\"), \"\\n\", \" \");\n\t\t\t// execute program\n\t\t\tenv.execute(\"Connected Components Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\t}"
        ],
        [
            "EnumTrianglesDataTypes::EdgeWithDegrees::setSecondDegree(Integer)",
            " 105 -",
            "\t\tpublic void setSecondDegree(final Integer degree2) { this.setField(degree2, D2); }",
            " 145 +\n 146 +\n 147 +",
            "\t\tpublic void setSecondDegree(final Integer degree2) {\n\t\t\tthis.setField(degree2, D2);\n\t\t}"
        ],
        [
            "AsyncIOExample::SimpleSource::restoreState(List)",
            "  74  \n  75  \n  76 -\n  77  \n  78  ",
            "\t\t@Override\n\t\tpublic void restoreState(List<Integer> state) throws Exception {\n\t\t\tfor (Integer i : state)\n\t\t\t\tthis.start = i;\n\t\t}",
            "  75  \n  76  \n  77 +\n  78  \n  79 +\n  80  ",
            "\t\t@Override\n\t\tpublic void restoreState(List<Integer> state) throws Exception {\n\t\t\tfor (Integer i : state) {\n\t\t\t\tthis.start = i;\n\t\t\t}\n\t\t}"
        ],
        [
            "TPCHQuery3::ShippingPriorityItem::getShippriority()",
            " 229 -",
            "\t\tpublic Long getShippriority() { return this.f3; }",
            " 264 +\n 265 +\n 266 +",
            "\t\tpublic Long getShippriority() {\n\t\t\treturn this.f3;\n\t\t}"
        ],
        [
            "TPCHQuery3::Lineitem::getDiscount()",
            " 192 -",
            "\t\tpublic Double getDiscount() { return this.f2; }",
            " 189 +\n 190 +\n 191 +",
            "\t\tpublic Double getDiscount() {\n\t\t\treturn this.f2;\n\t\t}"
        ],
        [
            "TPCHQuery3::ShippingPriorityItem::getRevenue()",
            " 225 -",
            "\t\tpublic Double getRevenue() { return this.f1; }",
            " 252 +\n 253 +\n 254 +",
            "\t\tpublic Double getRevenue() {\n\t\t\treturn this.f1;\n\t\t}"
        ],
        [
            "EnumTriangles::main(String)",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 -\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\t// Checking input parameters\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\t\n\t\t// read input data\n\t\tDataSet<Edge> edges;\n\t\tif (params.has(\"edges\")) {\n\t\t\tedges = env.readCsvFile(params.get(\"edges\"))\n\t\t\t\t\t.fieldDelimiter(\" \")\n\t\t\t\t\t.includeFields(true, true)\n\t\t\t\t\t.types(Integer.class, Integer.class)\n\t\t\t\t\t.map(new TupleEdgeConverter());\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing EnumTriangles example with default edges data set.\");\n\t\t\tSystem.out.println(\"Use --edges to specify file input.\");\n\t\t\tedges = EnumTrianglesData.getDefaultEdgeDataSet(env);\n\t\t}\n\n\t\t// project edges by vertex id\n\t\tDataSet<Edge> edgesById = edges\n\t\t\t\t.map(new EdgeByIdProjector());\n\t\t\n\t\tDataSet<Triad> triangles = edgesById\n\t\t\t\t// build triads\n\t\t\t\t.groupBy(Edge.V1).sortGroup(Edge.V2, Order.ASCENDING).reduceGroup(new TriadBuilder())\n\t\t\t\t// filter triads\n\t\t\t\t.join(edgesById).where(Triad.V2, Triad.V3).equalTo(Edge.V1, Edge.V2).with(new TriadFilter());\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\ttriangles.writeAsCsv(params.get(\"output\"), \"\\n\", \",\");\n\t\t\t// execute program\n\t\t\tenv.execute(\"Basic Triangle Enumeration Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\ttriangles.print();\n\t\t}\n\t}",
            "  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 +\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\t// Checking input parameters\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// read input data\n\t\tDataSet<Edge> edges;\n\t\tif (params.has(\"edges\")) {\n\t\t\tedges = env.readCsvFile(params.get(\"edges\"))\n\t\t\t\t\t.fieldDelimiter(\" \")\n\t\t\t\t\t.includeFields(true, true)\n\t\t\t\t\t.types(Integer.class, Integer.class)\n\t\t\t\t\t.map(new TupleEdgeConverter());\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing EnumTriangles example with default edges data set.\");\n\t\t\tSystem.out.println(\"Use --edges to specify file input.\");\n\t\t\tedges = EnumTrianglesData.getDefaultEdgeDataSet(env);\n\t\t}\n\n\t\t// project edges by vertex id\n\t\tDataSet<Edge> edgesById = edges\n\t\t\t\t.map(new EdgeByIdProjector());\n\n\t\tDataSet<Triad> triangles = edgesById\n\t\t\t\t// build triads\n\t\t\t\t.groupBy(Edge.V1).sortGroup(Edge.V2, Order.ASCENDING).reduceGroup(new TriadBuilder())\n\t\t\t\t// filter triads\n\t\t\t\t.join(edgesById).where(Triad.V2, Triad.V3).equalTo(Edge.V1, Edge.V2).with(new TriadFilter());\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\ttriangles.writeAsCsv(params.get(\"output\"), \"\\n\", \",\");\n\t\t\t// execute program\n\t\t\tenv.execute(\"Basic Triangle Enumeration Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\ttriangles.print();\n\t\t}\n\t}"
        ],
        [
            "TPCHQuery3::Lineitem::getShipdate()",
            " 194 -",
            "\t\tpublic String getShipdate() { return this.f3; }",
            " 197 +\n 198 +\n 199 +",
            "\t\tpublic String getShipdate() {\n\t\t\treturn this.f3;\n\t\t}"
        ],
        [
            "TPCHQuery3::ShippingPriorityItem::setRevenue(Double)",
            " 226 -",
            "\t\tpublic void setRevenue(Double revenue) { this.f1 = revenue; }",
            " 256 +\n 257  \n 258  ",
            "\t\tpublic void setRevenue(Double revenue) {\n\t\t\tthis.f1 = revenue;\n\t\t}"
        ],
        [
            "SocketWindowWordCountITCase::ServerThread::ServerThread(ServerSocket)",
            " 126  \n 127  \n 128 -\n 129  \n 130  ",
            "\t\tpublic ServerThread(ServerSocket serverSocket) {\n\t\t\tsuper(\"Socket Server Thread\");\n\t\t\t\n\t\t\tthis.serverSocket = serverSocket;\n\t\t}",
            " 128  \n 129  \n 130 +\n 131  \n 132  ",
            "\t\tpublic ServerThread(ServerSocket serverSocket) {\n\t\t\tsuper(\"Socket Server Thread\");\n\n\t\t\tthis.serverSocket = serverSocket;\n\t\t}"
        ],
        [
            "WordCountPojo::main(String)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 -\n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up the execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\t\t\n\t\t// get input data\n\t\tDataSet<String> text;\n\t\tif (params.has(\"input\")) {\n\t\t\t// read the text file from given input path\n\t\t\ttext = env.readTextFile(params.get(\"input\"));\n\t\t} else {\n\t\t\t// get default test text data\n\t\t\tSystem.out.println(\"Executing WordCount example with default input data set.\");\n\t\t\tSystem.out.println(\"Use --input to specify file input.\");\n\t\t\ttext = WordCountData.getDefaultTextLineDataSet(env);\n\t\t}\n\n\t\tDataSet<Word> counts = \n\t\t\t// split up the lines into Word objects (with frequency = 1)\n\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t// group by the field word and sum up the frequency\n\t\t\t.groupBy(\"word\")\n\t\t\t.reduce(new ReduceFunction<Word>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Word reduce(Word value1, Word value2) throws Exception {\n\t\t\t\t\treturn new Word(value1.word,value1.frequency + value2.frequency);\n\t\t\t\t}\n\t\t\t});\n\t\t\n\t\tif (params.has(\"output\")) {\n\t\t\tcounts.writeAsText(params.get(\"output\"), WriteMode.OVERWRITE);\n\t\t\t// execute program\n\t\t\tenv.execute(\"WordCount-Pojo Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tcounts.print();\n\t\t}\n\n\t}",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 +\n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112 +\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up the execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input data\n\t\tDataSet<String> text;\n\t\tif (params.has(\"input\")) {\n\t\t\t// read the text file from given input path\n\t\t\ttext = env.readTextFile(params.get(\"input\"));\n\t\t} else {\n\t\t\t// get default test text data\n\t\t\tSystem.out.println(\"Executing WordCount example with default input data set.\");\n\t\t\tSystem.out.println(\"Use --input to specify file input.\");\n\t\t\ttext = WordCountData.getDefaultTextLineDataSet(env);\n\t\t}\n\n\t\tDataSet<Word> counts =\n\t\t\t// split up the lines into Word objects (with frequency = 1)\n\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t// group by the field word and sum up the frequency\n\t\t\t.groupBy(\"word\")\n\t\t\t.reduce(new ReduceFunction<Word>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Word reduce(Word value1, Word value2) throws Exception {\n\t\t\t\t\treturn new Word(value1.word, value1.frequency + value2.frequency);\n\t\t\t\t}\n\t\t\t});\n\n\t\tif (params.has(\"output\")) {\n\t\t\tcounts.writeAsText(params.get(\"output\"), WriteMode.OVERWRITE);\n\t\t\t// execute program\n\t\t\tenv.execute(\"WordCount-Pojo Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tcounts.print();\n\t\t}\n\n\t}"
        ],
        [
            "GroupedProcessingTimeWindowExample::main(String)",
            "  39  \n  40 -\n  41  \n  42  \n  43 -\n  44  \n  45  \n  46 -\n  47  \n  48 -\n  49  \n  50  \n  51 -\n  52  \n  53 -\n  54  \n  55  \n  56  \n  57  \n  58 -\n  59 -\n  60  \n  61  \n  62  \n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 -\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78 -\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 -\n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\t\n\t\tDataStream<Tuple2<Long, Long>> stream = env\n\t\t\t\t.addSource(new RichParallelSourceFunction<Tuple2<Long, Long>>() {\n\t\t\t\t\t\n\t\t\t\t\tprivate volatile boolean running = true;\n\t\t\t\t\t\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void run(SourceContext<Tuple2<Long, Long>> ctx) throws Exception {\n\t\t\t\t\t\t\n\t\t\t\t\t\tfinal long startTime = System.currentTimeMillis();\n\t\t\t\t\t\t\n\t\t\t\t\t\tfinal long numElements = 20000000;\n\t\t\t\t\t\tfinal long numKeys = 10000;\n\t\t\t\t\t\tlong val = 1L;\n\t\t\t\t\t\tlong count = 0L;\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\twhile (running && count < numElements) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t\tctx.collect(new Tuple2<>(val++, 1L));\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif (val > numKeys) {\n\t\t\t\t\t\t\t\tval = 1L;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tfinal long endTime = System.currentTimeMillis();\n\t\t\t\t\t\tSystem.out.println(\"Took \" + (endTime-startTime) + \" msecs for \" + numElements + \" values\");\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void cancel() {\n\t\t\t\t\t\trunning = false;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\n\t\tstream\n\t\t\t.keyBy(0)\n\t\t\t.timeWindow(Time.of(2500, MILLISECONDS), Time.of(500, MILLISECONDS))\n\t\t\t.reduce(new SummingReducer())\n\n\t\t\t// alternative: use a apply function which does not pre-aggregate\n//\t\t\t.keyBy(new FirstFieldKeyExtractor<Tuple2<Long, Long>, Long>())\n//\t\t\t.window(Time.of(2500, MILLISECONDS), Time.of(500, MILLISECONDS))\n//\t\t\t.apply(new SummingWindowFunction())\n\t\t\t\t\n\t\t\t.addSink(new SinkFunction<Tuple2<Long, Long>>() {\n\t\t\t\t@Override\n\t\t\t\tpublic void invoke(Tuple2<Long, Long> value) {\n\t\t\t\t}\n\t\t\t});\n\t\t\n\t\tenv.execute();\n\t}",
            "  42  \n  43 +\n  44  \n  45  \n  46 +\n  47  \n  48  \n  49 +\n  50  \n  51 +\n  52  \n  53  \n  54 +\n  55  \n  56 +\n  57  \n  58  \n  59  \n  60  \n  61 +\n  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72 +\n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80 +\n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\n\t\tDataStream<Tuple2<Long, Long>> stream = env\n\t\t\t\t.addSource(new RichParallelSourceFunction<Tuple2<Long, Long>>() {\n\n\t\t\t\t\tprivate volatile boolean running = true;\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void run(SourceContext<Tuple2<Long, Long>> ctx) throws Exception {\n\n\t\t\t\t\t\tfinal long startTime = System.currentTimeMillis();\n\n\t\t\t\t\t\tfinal long numElements = 20000000;\n\t\t\t\t\t\tfinal long numKeys = 10000;\n\t\t\t\t\t\tlong val = 1L;\n\t\t\t\t\t\tlong count = 0L;\n\n\t\t\t\t\t\twhile (running && count < numElements) {\n\t\t\t\t\t\t\tcount++;\n\t\t\t\t\t\t\tctx.collect(new Tuple2<>(val++, 1L));\n\n\t\t\t\t\t\t\tif (val > numKeys) {\n\t\t\t\t\t\t\t\tval = 1L;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tfinal long endTime = System.currentTimeMillis();\n\t\t\t\t\t\tSystem.out.println(\"Took \" + (endTime - startTime) + \" msecs for \" + numElements + \" values\");\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void cancel() {\n\t\t\t\t\t\trunning = false;\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\tstream\n\t\t\t.keyBy(0)\n\t\t\t.timeWindow(Time.of(2500, MILLISECONDS), Time.of(500, MILLISECONDS))\n\t\t\t.reduce(new SummingReducer())\n\n\t\t\t// alternative: use a apply function which does not pre-aggregate\n//\t\t\t.keyBy(new FirstFieldKeyExtractor<Tuple2<Long, Long>, Long>())\n//\t\t\t.window(Time.of(2500, MILLISECONDS), Time.of(500, MILLISECONDS))\n//\t\t\t.apply(new SummingWindowFunction())\n\n\t\t\t.addSink(new SinkFunction<Tuple2<Long, Long>>() {\n\t\t\t\t@Override\n\t\t\t\tpublic void invoke(Tuple2<Long, Long> value) {\n\t\t\t\t}\n\t\t\t});\n\n\t\tenv.execute();\n\t}"
        ],
        [
            "EnumTrianglesDataTypes::Triad::setSecondVertex(Integer)",
            "  76 -",
            "\t\tpublic void setSecondVertex(final Integer vertex2) { this.setField(vertex2, V2); }",
            "  95 +\n  96 +\n  97 +",
            "\t\tpublic void setSecondVertex(final Integer vertex2) {\n\t\t\tthis.setField(vertex2, V2);\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::EdgeWithDegrees::setSecondVertex(Integer)",
            " 101 -",
            "\t\tpublic void setSecondVertex(final Integer vertex2) { this.setField(vertex2, V2); }",
            " 137 +\n 138 +\n 139 +",
            "\t\tpublic void setSecondVertex(final Integer vertex2) {\n\t\t\tthis.setField(vertex2, V2);\n\t\t}"
        ],
        [
            "TwitterExample::SelectEnglishAndTokenizeFlatMap::flatMap(String,Collector)",
            " 126  \n 127  \n 128  \n 129  \n 130  \n 131 -\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "\t\t/**\n\t\t * Select the language from the incoming JSON text\n\t\t */\n\t\t@Override\n\t\tpublic void flatMap(String value, Collector<Tuple2<String, Integer>> out) throws Exception {\n\t\t\tif(jsonParser == null) {\n\t\t\t\tjsonParser = new ObjectMapper();\n\t\t\t}\n\t\t\tJsonNode jsonNode = jsonParser.readValue(value, JsonNode.class);\n\t\t\tboolean isEnglish = jsonNode.has(\"user\") && jsonNode.get(\"user\").has(\"lang\") && jsonNode.get(\"user\").get(\"lang\").asText().equals(\"en\");\n\t\t\tboolean hasText = jsonNode.has(\"text\");\n\t\t\tif (isEnglish && hasText) {\n\t\t\t\t// message of tweet\n\t\t\t\tStringTokenizer tokenizer = new StringTokenizer(jsonNode.get(\"text\").asText());\n\n\t\t\t\t// split the message\n\t\t\t\twhile (tokenizer.hasMoreTokens()) {\n\t\t\t\t\tString result = tokenizer.nextToken().replaceAll(\"\\\\s*\", \"\").toLowerCase();\n\n\t\t\t\t\tif (!result.equals(\"\")) {\n\t\t\t\t\t\tout.collect(new Tuple2<>(result, 1));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130 +\n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "\t\t/**\n\t\t * Select the language from the incoming JSON text.\n\t\t */\n\t\t@Override\n\t\tpublic void flatMap(String value, Collector<Tuple2<String, Integer>> out) throws Exception {\n\t\t\tif (jsonParser == null) {\n\t\t\t\tjsonParser = new ObjectMapper();\n\t\t\t}\n\t\t\tJsonNode jsonNode = jsonParser.readValue(value, JsonNode.class);\n\t\t\tboolean isEnglish = jsonNode.has(\"user\") && jsonNode.get(\"user\").has(\"lang\") && jsonNode.get(\"user\").get(\"lang\").asText().equals(\"en\");\n\t\t\tboolean hasText = jsonNode.has(\"text\");\n\t\t\tif (isEnglish && hasText) {\n\t\t\t\t// message of tweet\n\t\t\t\tStringTokenizer tokenizer = new StringTokenizer(jsonNode.get(\"text\").asText());\n\n\t\t\t\t// split the message\n\t\t\t\twhile (tokenizer.hasMoreTokens()) {\n\t\t\t\t\tString result = tokenizer.nextToken().replaceAll(\"\\\\s*\", \"\").toLowerCase();\n\n\t\t\t\t\tif (!result.equals(\"\")) {\n\t\t\t\t\t\tout.collect(new Tuple2<>(result, 1));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}"
        ],
        [
            "TPCHQuery3::Order::getCustKey()",
            " 206 -",
            "\t\tpublic Long getCustKey() { return this.f1; }",
            " 219 +\n 220 +\n 221 +",
            "\t\tpublic Long getCustKey() {\n\t\t\treturn this.f1;\n\t\t}"
        ],
        [
            "LinearRegression::Params::div(Integer)",
            " 186 -\n 187 -\n 188 -\n 189  \n 190  ",
            "\t\tpublic Params div(Integer a){\n\t\t\tthis.theta0 = theta0 / a ;\n\t\t\tthis.theta1 = theta1 / a ;\n\t\t\treturn this;\n\t\t}",
            " 182 +\n 183 +\n 184 +\n 185  \n 186  ",
            "\t\tpublic Params div(Integer a) {\n\t\t\tthis.theta0 = theta0 / a;\n\t\t\tthis.theta1 = theta1 / a;\n\t\t\treturn this;\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::Edge::setSecondVertex(Integer)",
            "  46 -",
            "\t\tpublic void setSecondVertex(final Integer vertex2) { this.setField(vertex2, V2); }",
            "  58 +\n  59 +\n  60 +",
            "\t\tpublic void setSecondVertex(final Integer vertex2) {\n\t\t\tthis.setField(vertex2, V2);\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::EdgeWithDegrees::getFirstVertex()",
            "  91 -",
            "\t\tpublic Integer getFirstVertex() { return this.getField(V1); }",
            " 117 +\n 118 +\n 119 +",
            "\t\tpublic Integer getFirstVertex() {\n\t\t\treturn this.getField(V1);\n\t\t}"
        ],
        [
            "LinearRegression::Params::Params(double,double)",
            " 160 -\n 161  \n 162  \n 163  ",
            "\t\tpublic Params(double x0, double x1){\n\t\t\tthis.theta0 = x0;\n\t\t\tthis.theta1 = x1;\n\t\t}",
            " 156 +\n 157  \n 158  \n 159  ",
            "\t\tpublic Params(double x0, double x1) {\n\t\t\tthis.theta0 = x0;\n\t\t\tthis.theta1 = x1;\n\t\t}"
        ],
        [
            "LinearRegressionDataGenerator::main(String)",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  \n  79 -\n  80  \n  81  \n  82  \n  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  ",
            "\t/**\n\t * Main method to generate data for the {@link org.apache.flink.examples.java.ml.LinearRegression} example program.\n\t * <p>\n\t * The generator creates to files:\n\t * <ul>\n\t * <li><code>{tmp.dir}/data</code> for the data points\n\t * </ul> \n\t * \n\t * @param args \n\t * <ol>\n\t * <li>Int: Number of data points\n\t * <li><b>Optional</b> Long: Random seed\n\t * </ol>\n\t */\n\tpublic static void main(String[] args) throws IOException {\n\n\t\t// check parameter count\n\t\tif (args.length < 1) {\n\t\t\tSystem.out.println(\"LinearRegressionDataGenerator <numberOfDataPoints> [<seed>]\");\n\t\t\tSystem.exit(1);\n\t\t}\n\n\t\t// parse parameters\n\t\tfinal int numDataPoints = Integer.parseInt(args[0]);\n\t\tfinal long firstSeed = args.length > 1 ? Long.parseLong(args[4]) : DEFAULT_SEED;\n\t\tfinal Random random = new Random(firstSeed);\n\t\tfinal String tmpDir = System.getProperty(\"java.io.tmpdir\");\n\n\t\t// write the points out\n\t\tBufferedWriter pointsOut = null;\n\t\ttry {\n\t\t\tpointsOut = new BufferedWriter(new FileWriter(new File(tmpDir+\"/\"+POINTS_FILE)));\n\t\t\tStringBuilder buffer = new StringBuilder();\n\n\t\t\t// DIMENSIONALITY + 1 means that the number of x(dimensionality) and target y\n\t\t\tdouble[] point = new double[DIMENSIONALITY+1];\n\n\t\t\tfor (int i = 1; i <= numDataPoints; i++) {\n\t\t\t\tpoint[0] = random.nextGaussian();\n\t\t\t\tpoint[1] = 2 * point[0] + 0.01*random.nextGaussian();\n\t\t\t\twritePoint(point, buffer, pointsOut);\n\t\t\t}\n\n\t\t}\n\t\tfinally {\n\t\t\tif (pointsOut != null) {\n\t\t\t\tpointsOut.close();\n\t\t\t}\n\t\t}\n\n\t\tSystem.out.println(\"Wrote \"+numDataPoints+\" data points to \"+tmpDir+\"/\"+POINTS_FILE);\n\t}",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 +\n  76  \n  77  \n  78  \n  79 +\n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95  ",
            "\t/**\n\t * Main method to generate data for the {@link org.apache.flink.examples.java.ml.LinearRegression} example program.\n\t *\n\t * <p>The generator creates to files:\n\t * <ul>\n\t * <li><code>{tmp.dir}/data</code> for the data points\n\t * </ul>\n\t *\n\t * @param args\n\t * <ol>\n\t * <li>Int: Number of data points\n\t * <li><b>Optional</b> Long: Random seed\n\t * </ol>\n\t */\n\tpublic static void main(String[] args) throws IOException {\n\n\t\t// check parameter count\n\t\tif (args.length < 1) {\n\t\t\tSystem.out.println(\"LinearRegressionDataGenerator <numberOfDataPoints> [<seed>]\");\n\t\t\tSystem.exit(1);\n\t\t}\n\n\t\t// parse parameters\n\t\tfinal int numDataPoints = Integer.parseInt(args[0]);\n\t\tfinal long firstSeed = args.length > 1 ? Long.parseLong(args[4]) : DEFAULT_SEED;\n\t\tfinal Random random = new Random(firstSeed);\n\t\tfinal String tmpDir = System.getProperty(\"java.io.tmpdir\");\n\n\t\t// write the points out\n\t\tBufferedWriter pointsOut = null;\n\t\ttry {\n\t\t\tpointsOut = new BufferedWriter(new FileWriter(new File(tmpDir + \"/\" + POINTS_FILE)));\n\t\t\tStringBuilder buffer = new StringBuilder();\n\n\t\t\t// DIMENSIONALITY + 1 means that the number of x(dimensionality) and target y\n\t\t\tdouble[] point = new double[DIMENSIONALITY + 1];\n\n\t\t\tfor (int i = 1; i <= numDataPoints; i++) {\n\t\t\t\tpoint[0] = random.nextGaussian();\n\t\t\t\tpoint[1] = 2 * point[0] + 0.01 * random.nextGaussian();\n\t\t\t\twritePoint(point, buffer, pointsOut);\n\t\t\t}\n\n\t\t}\n\t\tfinally {\n\t\t\tif (pointsOut != null) {\n\t\t\t\tpointsOut.close();\n\t\t\t}\n\t\t}\n\n\t\tSystem.out.println(\"Wrote \" + numDataPoints + \" data points to \" + tmpDir + \"/\" + POINTS_FILE);\n\t}"
        ],
        [
            "TransitiveClosureNaive::main(String)",
            "  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60 -\n  61  \n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 -\n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93 -\n  94  \n  95  \n  96 -\n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 -\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "\tpublic static void main (String... args) throws Exception {\n\n\t\t// Checking input parameters\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up execution environment\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\tfinal int maxIterations = params.getInt(\"iterations\", 10);\n\n\t\tDataSet<Tuple2<Long, Long>> edges;\n\t\tif (params.has(\"edges\")) {\n\t\t\tedges = env.readCsvFile(params.get(\"edges\")).fieldDelimiter(\" \").types(Long.class, Long.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing TransitiveClosureNaive example with default edges data set.\");\n\t\t\tSystem.out.println(\"Use --edges to specify file input.\");\n\t\t\tedges = ConnectedComponentsData.getDefaultEdgeDataSet(env);\n\t\t}\n\n\t\tIterativeDataSet<Tuple2<Long,Long>> paths = edges.iterate(maxIterations);\n\n\t\tDataSet<Tuple2<Long,Long>> nextPaths = paths\n\t\t\t\t.join(edges)\n\t\t\t\t.where(1)\n\t\t\t\t.equalTo(0)\n\t\t\t\t.with(new JoinFunction<Tuple2<Long, Long>, Tuple2<Long, Long>, Tuple2<Long, Long>>() {\n\t\t\t\t\t@Override\n\t\t\t\t\t/**\n\t\t\t\t\t\tleft: Path (z,x) - x is reachable by z\n\t\t\t\t\t\tright: Edge (x,y) - edge x-->y exists\n\t\t\t\t\t\tout: Path (z,y) - y is reachable by z\n\t\t\t\t\t */\n\t\t\t\t\tpublic Tuple2<Long, Long> join(Tuple2<Long, Long> left, Tuple2<Long, Long> right) throws Exception {\n\t\t\t\t\t\treturn new Tuple2<Long, Long>(left.f0, right.f1);\n\t\t\t\t\t}\n\t\t\t\t}).withForwardedFieldsFirst(\"0\").withForwardedFieldsSecond(\"1\")\n\t\t\t\t.union(paths)\n\t\t\t\t.groupBy(0, 1)\n\t\t\t\t.reduceGroup(new GroupReduceFunction<Tuple2<Long, Long>, Tuple2<Long, Long>>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void reduce(Iterable<Tuple2<Long, Long>> values, Collector<Tuple2<Long, Long>> out) throws Exception {\n\t\t\t\t\t\tout.collect(values.iterator().next());\n\t\t\t\t\t}\n\t\t\t\t}).withForwardedFields(\"0;1\");\n\n\t\tDataSet<Tuple2<Long,Long>> newPaths = paths\n\t\t\t\t.coGroup(nextPaths)\n\t\t\t\t.where(0).equalTo(0)\n\t\t\t\t.with(new CoGroupFunction<Tuple2<Long, Long>, Tuple2<Long, Long>, Tuple2<Long, Long>>() {\n\t\t\t\t\tSet<Tuple2<Long,Long>> prevSet = new HashSet<Tuple2<Long,Long>>();\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void coGroup(Iterable<Tuple2<Long, Long>> prevPaths, Iterable<Tuple2<Long, Long>> nextPaths, Collector<Tuple2<Long, Long>> out) throws Exception {\n\t\t\t\t\t\tfor (Tuple2<Long,Long> prev : prevPaths) {\n\t\t\t\t\t\t\tprevSet.add(prev);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfor (Tuple2<Long,Long> next: nextPaths) {\n\t\t\t\t\t\t\tif (!prevSet.contains(next)) {\n\t\t\t\t\t\t\t\tout.collect(next);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}).withForwardedFieldsFirst(\"0\").withForwardedFieldsSecond(\"0\");\n\n\t\tDataSet<Tuple2<Long, Long>> transitiveClosure = paths.closeWith(nextPaths, newPaths);\n\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\ttransitiveClosure.writeAsCsv(params.get(\"output\"), \"\\n\", \" \");\n\n\t\t\t// execute program explicitly, because file sinks are lazy\n\t\t\tenv.execute(\"Transitive Closure Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\ttransitiveClosure.print();\n\t\t}\n\t}",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 +\n  96  \n  97  \n  98  \n  99 +\n 100  \n 101  \n 102 +\n 103  \n 104  \n 105 +\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "\tpublic static void main (String... args) throws Exception {\n\n\t\t// Checking input parameters\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up execution environment\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\tfinal int maxIterations = params.getInt(\"iterations\", 10);\n\n\t\tDataSet<Tuple2<Long, Long>> edges;\n\t\tif (params.has(\"edges\")) {\n\t\t\tedges = env.readCsvFile(params.get(\"edges\")).fieldDelimiter(\" \").types(Long.class, Long.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing TransitiveClosureNaive example with default edges data set.\");\n\t\t\tSystem.out.println(\"Use --edges to specify file input.\");\n\t\t\tedges = ConnectedComponentsData.getDefaultEdgeDataSet(env);\n\t\t}\n\n\t\tIterativeDataSet<Tuple2<Long, Long>> paths = edges.iterate(maxIterations);\n\n\t\tDataSet<Tuple2<Long, Long>> nextPaths = paths\n\t\t\t\t.join(edges)\n\t\t\t\t.where(1)\n\t\t\t\t.equalTo(0)\n\t\t\t\t.with(new JoinFunction<Tuple2<Long, Long>, Tuple2<Long, Long>, Tuple2<Long, Long>>() {\n\t\t\t\t\t@Override\n\t\t\t\t\t/**\n\t\t\t\t\t\tleft: Path (z,x) - x is reachable by z\n\t\t\t\t\t\tright: Edge (x,y) - edge x-->y exists\n\t\t\t\t\t\tout: Path (z,y) - y is reachable by z\n\t\t\t\t\t */\n\t\t\t\t\tpublic Tuple2<Long, Long> join(Tuple2<Long, Long> left, Tuple2<Long, Long> right) throws Exception {\n\t\t\t\t\t\treturn new Tuple2<Long, Long>(left.f0, right.f1);\n\t\t\t\t\t}\n\t\t\t\t}).withForwardedFieldsFirst(\"0\").withForwardedFieldsSecond(\"1\")\n\t\t\t\t.union(paths)\n\t\t\t\t.groupBy(0, 1)\n\t\t\t\t.reduceGroup(new GroupReduceFunction<Tuple2<Long, Long>, Tuple2<Long, Long>>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void reduce(Iterable<Tuple2<Long, Long>> values, Collector<Tuple2<Long, Long>> out) throws Exception {\n\t\t\t\t\t\tout.collect(values.iterator().next());\n\t\t\t\t\t}\n\t\t\t\t}).withForwardedFields(\"0;1\");\n\n\t\tDataSet<Tuple2<Long, Long>> newPaths = paths\n\t\t\t\t.coGroup(nextPaths)\n\t\t\t\t.where(0).equalTo(0)\n\t\t\t\t.with(new CoGroupFunction<Tuple2<Long, Long>, Tuple2<Long, Long>, Tuple2<Long, Long>>() {\n\t\t\t\t\tSet<Tuple2<Long, Long>> prevSet = new HashSet<Tuple2<Long, Long>>();\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void coGroup(Iterable<Tuple2<Long, Long>> prevPaths, Iterable<Tuple2<Long, Long>> nextPaths, Collector<Tuple2<Long, Long>> out) throws Exception {\n\t\t\t\t\t\tfor (Tuple2<Long, Long> prev : prevPaths) {\n\t\t\t\t\t\t\tprevSet.add(prev);\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfor (Tuple2<Long, Long> next: nextPaths) {\n\t\t\t\t\t\t\tif (!prevSet.contains(next)) {\n\t\t\t\t\t\t\t\tout.collect(next);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}).withForwardedFieldsFirst(\"0\").withForwardedFieldsSecond(\"0\");\n\n\t\tDataSet<Tuple2<Long, Long>> transitiveClosure = paths.closeWith(nextPaths, newPaths);\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\ttransitiveClosure.writeAsCsv(params.get(\"output\"), \"\\n\", \" \");\n\n\t\t\t// execute program explicitly, because file sinks are lazy\n\t\t\tenv.execute(\"Transitive Closure Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\ttransitiveClosure.print();\n\t\t}\n\t}"
        ],
        [
            "WebLogAnalysis::getRanksDataSet(ExecutionEnvironment,ParameterTool)",
            " 277  \n 278  \n 279 -\n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  ",
            "\tprivate static DataSet<Tuple3<Integer, String, Integer>> getRanksDataSet(ExecutionEnvironment env, ParameterTool params) {\n\t\t// Create DataSet for ranks relation (Rank, URL, Avg-Visit-Duration)\n\t\tif(params.has(\"ranks\")) {\n\t\t\treturn env.readCsvFile(params.get(\"ranks\"))\n\t\t\t\t\t\t.fieldDelimiter(\"|\")\n\t\t\t\t\t\t.types(Integer.class, String.class, Integer.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing WebLogAnalysis example with default ranks data set.\");\n\t\t\tSystem.out.println(\"Use --ranks to specify file input.\");\n\t\t\treturn WebLogData.getRankDataSet(env);\n\t\t}\n\t}",
            " 273  \n 274  \n 275 +\n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  ",
            "\tprivate static DataSet<Tuple3<Integer, String, Integer>> getRanksDataSet(ExecutionEnvironment env, ParameterTool params) {\n\t\t// Create DataSet for ranks relation (Rank, URL, Avg-Visit-Duration)\n\t\tif (params.has(\"ranks\")) {\n\t\t\treturn env.readCsvFile(params.get(\"ranks\"))\n\t\t\t\t\t\t.fieldDelimiter(\"|\")\n\t\t\t\t\t\t.types(Integer.class, String.class, Integer.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing WebLogAnalysis example with default ranks data set.\");\n\t\t\tSystem.out.println(\"Use --ranks to specify file input.\");\n\t\t\treturn WebLogData.getRankDataSet(env);\n\t\t}\n\t}"
        ],
        [
            "TPCHQuery3::main(String)",
            "  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127 -\n 128  \n 129  \n 130  \n 131  \n 132  \n 133 -\n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 -\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 -\n 158  \n 159 -\n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172 -\n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (!params.has(\"lineitem\") && !params.has(\"customer\") && !params.has(\"orders\")) {\n\t\t\tSystem.err.println(\"  This program expects data from the TPC-H benchmark as input data.\");\n\t\t\tSystem.err.println(\"  Due to legal restrictions, we can not ship generated data.\");\n\t\t\tSystem.out.println(\"  You can find the TPC-H data generator at http://www.tpc.org/tpch/.\");\n\t\t\tSystem.out.println(\"  Usage: TPCHQuery3 --lineitem <path> --customer <path> --orders <path> [--output <path>]\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input data\n\t\tDataSet<Lineitem> lineitems = getLineitemDataSet(env, params.get(\"lineitem\"));\n\t\tDataSet<Order> orders = getOrdersDataSet(env, params.get(\"customer\"));\n\t\tDataSet<Customer> customers = getCustomerDataSet(env, params.get(\"orders\"));\n\t\t\n\t\t// Filter market segment \"AUTOMOBILE\"\n\t\tcustomers = customers.filter(\n\t\t\t\t\t\t\t\tnew FilterFunction<Customer>() {\n\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\tpublic boolean filter(Customer c) {\n\t\t\t\t\t\t\t\t\t\treturn c.getMktsegment().equals(\"AUTOMOBILE\");\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\n\t\t// Filter all Orders with o_orderdate < 12.03.1995\n\t\torders = orders.filter(\n\t\t\t\t\t\t\tnew FilterFunction<Order>() {\n\t\t\t\t\t\t\t\tprivate final DateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\");\n\t\t\t\t\t\t\t\tprivate final Date date = format.parse(\"1995-03-12\");\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\tpublic boolean filter(Order o) throws ParseException {\n\t\t\t\t\t\t\t\t\treturn format.parse(o.getOrderdate()).before(date);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t});\n\t\t\n\t\t// Filter all Lineitems with l_shipdate > 12.03.1995\n\t\tlineitems = lineitems.filter(\n\t\t\t\t\t\t\t\tnew FilterFunction<Lineitem>() {\n\t\t\t\t\t\t\t\t\tprivate final DateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\");\n\t\t\t\t\t\t\t\t\tprivate final Date date = format.parse(\"1995-03-12\");\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\tpublic boolean filter(Lineitem l) throws ParseException {\n\t\t\t\t\t\t\t\t\t\treturn format.parse(l.getShipdate()).after(date);\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\n\t\t// Join customers with orders and package them into a ShippingPriorityItem\n\t\tDataSet<ShippingPriorityItem> customerWithOrders = \n\t\t\t\tcustomers.join(orders).where(0).equalTo(1)\n\t\t\t\t\t\t\t.with(\n\t\t\t\t\t\t\t\tnew JoinFunction<Customer, Order, ShippingPriorityItem>() {\n\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\tpublic ShippingPriorityItem join(Customer c, Order o) {\n\t\t\t\t\t\t\t\t\t\treturn new ShippingPriorityItem(o.getOrderKey(), 0.0, o.getOrderdate(),\n\t\t\t\t\t\t\t\t\t\t\t\to.getShippriority());\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\t\t\n\t\t// Join the last join result with Lineitems\n\t\tDataSet<ShippingPriorityItem> result = \n\t\t\t\tcustomerWithOrders.join(lineitems).where(0).equalTo(0)\n\t\t\t\t\t\t\t\t\t.with(\n\t\t\t\t\t\t\t\t\t\t\tnew JoinFunction<ShippingPriorityItem, Lineitem, ShippingPriorityItem>() {\n\t\t\t\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\t\t\t\tpublic ShippingPriorityItem join(ShippingPriorityItem i, Lineitem l) {\n\t\t\t\t\t\t\t\t\t\t\t\t\ti.setRevenue(l.getExtendedprice() * (1 - l.getDiscount()));\n\t\t\t\t\t\t\t\t\t\t\t\t\treturn i;\n\t\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\t})\n\t\t\t\t\t\t\t\t// Group by l_orderkey, o_orderdate and o_shippriority and compute revenue sum\n\t\t\t\t\t\t\t\t.groupBy(0, 2, 3)\n\t\t\t\t\t\t\t\t.aggregate(Aggregations.SUM, 1);\n\t\t\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tresult.writeAsCsv(params.get(\"output\"), \"\\n\", \"|\");\n\t\t\t// execute program\n\t\t\tenv.execute(\"TPCH Query 3 Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\n\t}",
            "  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 +\n 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133 +\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 +\n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (!params.has(\"lineitem\") && !params.has(\"customer\") && !params.has(\"orders\")) {\n\t\t\tSystem.err.println(\"  This program expects data from the TPC-H benchmark as input data.\");\n\t\t\tSystem.err.println(\"  Due to legal restrictions, we can not ship generated data.\");\n\t\t\tSystem.out.println(\"  You can find the TPC-H data generator at http://www.tpc.org/tpch/.\");\n\t\t\tSystem.out.println(\"  Usage: TPCHQuery3 --lineitem <path> --customer <path> --orders <path> [--output <path>]\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input data\n\t\tDataSet<Lineitem> lineitems = getLineitemDataSet(env, params.get(\"lineitem\"));\n\t\tDataSet<Order> orders = getOrdersDataSet(env, params.get(\"customer\"));\n\t\tDataSet<Customer> customers = getCustomerDataSet(env, params.get(\"orders\"));\n\n\t\t// Filter market segment \"AUTOMOBILE\"\n\t\tcustomers = customers.filter(\n\t\t\t\t\t\t\t\tnew FilterFunction<Customer>() {\n\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\tpublic boolean filter(Customer c) {\n\t\t\t\t\t\t\t\t\t\treturn c.getMktsegment().equals(\"AUTOMOBILE\");\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\n\t\t// Filter all Orders with o_orderdate < 12.03.1995\n\t\torders = orders.filter(\n\t\t\t\t\t\t\tnew FilterFunction<Order>() {\n\t\t\t\t\t\t\t\tprivate final DateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\");\n\t\t\t\t\t\t\t\tprivate final Date date = format.parse(\"1995-03-12\");\n\n\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\tpublic boolean filter(Order o) throws ParseException {\n\t\t\t\t\t\t\t\t\treturn format.parse(o.getOrderdate()).before(date);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t});\n\n\t\t// Filter all Lineitems with l_shipdate > 12.03.1995\n\t\tlineitems = lineitems.filter(\n\t\t\t\t\t\t\t\tnew FilterFunction<Lineitem>() {\n\t\t\t\t\t\t\t\t\tprivate final DateFormat format = new SimpleDateFormat(\"yyyy-MM-dd\");\n\t\t\t\t\t\t\t\t\tprivate final Date date = format.parse(\"1995-03-12\");\n\n\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\tpublic boolean filter(Lineitem l) throws ParseException {\n\t\t\t\t\t\t\t\t\t\treturn format.parse(l.getShipdate()).after(date);\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\n\t\t// Join customers with orders and package them into a ShippingPriorityItem\n\t\tDataSet<ShippingPriorityItem> customerWithOrders =\n\t\t\t\tcustomers.join(orders).where(0).equalTo(1)\n\t\t\t\t\t\t\t.with(\n\t\t\t\t\t\t\t\tnew JoinFunction<Customer, Order, ShippingPriorityItem>() {\n\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\tpublic ShippingPriorityItem join(Customer c, Order o) {\n\t\t\t\t\t\t\t\t\t\treturn new ShippingPriorityItem(o.getOrderKey(), 0.0, o.getOrderdate(),\n\t\t\t\t\t\t\t\t\t\t\t\to.getShippriority());\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t});\n\n\t\t// Join the last join result with Lineitems\n\t\tDataSet<ShippingPriorityItem> result =\n\t\t\t\tcustomerWithOrders.join(lineitems).where(0).equalTo(0)\n\t\t\t\t\t\t\t\t\t.with(\n\t\t\t\t\t\t\t\t\t\t\tnew JoinFunction<ShippingPriorityItem, Lineitem, ShippingPriorityItem>() {\n\t\t\t\t\t\t\t\t\t\t\t\t@Override\n\t\t\t\t\t\t\t\t\t\t\t\tpublic ShippingPriorityItem join(ShippingPriorityItem i, Lineitem l) {\n\t\t\t\t\t\t\t\t\t\t\t\t\ti.setRevenue(l.getExtendedprice() * (1 - l.getDiscount()));\n\t\t\t\t\t\t\t\t\t\t\t\t\treturn i;\n\t\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\t})\n\t\t\t\t\t\t\t\t// Group by l_orderkey, o_orderdate and o_shippriority and compute revenue sum\n\t\t\t\t\t\t\t\t.groupBy(0, 2, 3)\n\t\t\t\t\t\t\t\t.aggregate(Aggregations.SUM, 1);\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tresult.writeAsCsv(params.get(\"output\"), \"\\n\", \"|\");\n\t\t\t// execute program\n\t\t\tenv.execute(\"TPCH Query 3 Example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\n\t}"
        ],
        [
            "WriteIntoKafka::main(String)",
            "  38  \n  39  \n  40 -\n  41  \n  42  \n  43  \n  44  \n  45  \n  46 -\n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 -\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tParameterTool parameterTool = ParameterTool.fromArgs(args);\n\t\tif(parameterTool.getNumberOfParameters() < 2) {\n\t\t\tSystem.out.println(\"Missing parameters!\");\n\t\t\tSystem.out.println(\"Usage: Kafka --topic <topic> --bootstrap.servers <kafka brokers>\");\n\t\t\treturn;\n\t\t}\n\n\t\tStreamExecutionEnvironment env =StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(4, 10000));\n\n\t\t// very simple data generator\n\t\tDataStream<String> messageStream = env.addSource(new SourceFunction<String>() {\n\t\t\tprivate static final long serialVersionUID = 6369260445318862378L;\n\t\t\tpublic boolean running = true;\n\n\t\t\t@Override\n\t\t\tpublic void run(SourceContext<String> ctx) throws Exception {\n\t\t\t\tlong i = 0;\n\t\t\t\twhile(this.running) {\n\t\t\t\t\tctx.collect(\"Element - \" + i++);\n\t\t\t\t\tThread.sleep(500);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void cancel() {\n\t\t\t\trunning = false;\n\t\t\t}\n\t\t});\n\n\t\t// write data into Kafka\n\t\tmessageStream.addSink(new FlinkKafkaProducer08<>(parameterTool.getRequired(\"topic\"), new SimpleStringSchema(), parameterTool.getProperties()));\n\n\t\tenv.execute(\"Write into Kafka example\");\n\t}",
            "  36  \n  37  \n  38 +\n  39  \n  40  \n  41  \n  42  \n  43  \n  44 +\n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56 +\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tParameterTool parameterTool = ParameterTool.fromArgs(args);\n\t\tif (parameterTool.getNumberOfParameters() < 2) {\n\t\t\tSystem.out.println(\"Missing parameters!\");\n\t\t\tSystem.out.println(\"Usage: Kafka --topic <topic> --bootstrap.servers <kafka brokers>\");\n\t\t\treturn;\n\t\t}\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(4, 10000));\n\n\t\t// very simple data generator\n\t\tDataStream<String> messageStream = env.addSource(new SourceFunction<String>() {\n\t\t\tprivate static final long serialVersionUID = 6369260445318862378L;\n\t\t\tpublic boolean running = true;\n\n\t\t\t@Override\n\t\t\tpublic void run(SourceContext<String> ctx) throws Exception {\n\t\t\t\tlong i = 0;\n\t\t\t\twhile (this.running) {\n\t\t\t\t\tctx.collect(\"Element - \" + i++);\n\t\t\t\t\tThread.sleep(500);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void cancel() {\n\t\t\t\trunning = false;\n\t\t\t}\n\t\t});\n\n\t\t// write data into Kafka\n\t\tmessageStream.addSink(new FlinkKafkaProducer08<>(parameterTool.getRequired(\"topic\"), new SimpleStringSchema(), parameterTool.getProperties()));\n\n\t\tenv.execute(\"Write into Kafka example\");\n\t}"
        ],
        [
            "TPCHQuery3::Order::getOrderdate()",
            " 207 -",
            "\t\tpublic String getOrderdate() { return this.f2; }",
            " 223 +\n 224 +\n 225 +",
            "\t\tpublic String getOrderdate() {\n\t\t\treturn this.f2;\n\t\t}"
        ],
        [
            "WordCount::Tokenizer::flatMap(String,Collector)",
            " 108  \n 109  \n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  ",
            "\t\t@Override\n\t\tpublic void flatMap(String value, Collector<Tuple2<String, Integer>> out) {\n\t\t\t// normalize and split the line\n\t\t\tString[] tokens = value.toLowerCase().split(\"\\\\W+\");\n\t\t\t\n\t\t\t// emit the pairs\n\t\t\tfor (String token : tokens) {\n\t\t\t\tif (token.length() > 0) {\n\t\t\t\t\tout.collect(new Tuple2<String, Integer>(token, 1));\n\t\t\t\t}\n\t\t\t}\n\t\t}",
            " 105  \n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "\t\t@Override\n\t\tpublic void flatMap(String value, Collector<Tuple2<String, Integer>> out) {\n\t\t\t// normalize and split the line\n\t\t\tString[] tokens = value.toLowerCase().split(\"\\\\W+\");\n\n\t\t\t// emit the pairs\n\t\t\tfor (String token : tokens) {\n\t\t\t\tif (token.length() > 0) {\n\t\t\t\t\tout.collect(new Tuple2<String, Integer>(token, 1));\n\t\t\t\t}\n\t\t\t}\n\t\t}"
        ],
        [
            "DistCp::main(String)",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103 -\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142 -\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\t// set up the execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tParameterTool params = ParameterTool.fromArgs(args);\n\t\tif (!params.has(\"input\") || !params.has(\"output\")) {\n\t\t\tSystem.err.println(\"Usage: --input <path> --output <path> [--parallelism <n>]\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal Path sourcePath = new Path(params.get(\"input\"));\n\t\tfinal Path targetPath = new Path(params.get(\"output\"));\n\t\tif (!isLocal(env) && !(isOnDistributedFS(sourcePath) && isOnDistributedFS(targetPath))) {\n\t\t\tSystem.out.println(\"In a distributed mode only HDFS input/output paths are supported\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal int parallelism = params.getInt(\"parallelism\", 10);\n\t\tif (parallelism <= 0) {\n\t\t\tSystem.err.println(\"Parallelism should be greater than 0\");\n\t\t\treturn;\n\t\t}\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\tenv.setParallelism(parallelism);\n\n\t\tlong startTime = System.currentTimeMillis();\n\t\tLOGGER.info(\"Initializing copy tasks\");\n\t\tList<FileCopyTask> tasks = getCopyTasks(sourcePath);\n\t\tLOGGER.info(\"Copy task initialization took \" + (System.currentTimeMillis() - startTime) + \"ms\");\n\n\t\tDataSet<FileCopyTask> inputTasks = new DataSource<>(env,\n\t\t\t\tnew FileCopyTaskInputFormat(tasks),\n\t\t\t\tnew GenericTypeInfo<>(FileCopyTask.class), \"fileCopyTasks\");\n\n\n\t\tFlatMapOperator<FileCopyTask, Object> res = inputTasks.flatMap(new RichFlatMapFunction<FileCopyTask, Object>() {\n\n\t\t\tprivate static final long serialVersionUID = 1109254230243989929L;\n\t\t\tprivate LongCounter fileCounter;\n\t\t\tprivate LongCounter bytesCounter;\n\n\t\t\t@Override\n\t\t\tpublic void open(Configuration parameters) throws Exception {\n\t\t\t\tbytesCounter = getRuntimeContext().getLongCounter(BYTES_COPIED_CNT_NAME);\n\t\t\t\tfileCounter = getRuntimeContext().getLongCounter(FILES_COPIED_CNT_NAME);\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void flatMap(FileCopyTask task, Collector<Object> out) throws Exception {\n\t\t\t\tLOGGER.info(\"Processing task: \" + task);\n\t\t\t\tPath outPath = new Path(targetPath, task.getRelativePath());\n\n\t\t\t\tFileSystem targetFs = targetPath.getFileSystem();\n\t\t\t\t// creating parent folders in case of a local FS\n\t\t\t\tif (!targetFs.isDistributedFS()) {\n\t\t\t\t\t//dealing with cases like file:///tmp or just /tmp\n\t\t\t\t\tFile outFile = outPath.toUri().isAbsolute() ? new File(outPath.toUri()) : new File(outPath.toString());\n\t\t\t\t\tFile parentFile = outFile.getParentFile();\n\t\t\t\t\tif (!parentFile.mkdirs() && !parentFile.exists()) {\n\t\t\t\t\t\tthrow new RuntimeException(\"Cannot create local file system directories: \" + parentFile);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tFSDataOutputStream outputStream = null;\n\t\t\t\tFSDataInputStream inputStream = null;\n\t\t\t\ttry {\n\t\t\t\t\toutputStream = targetFs.create(outPath, true);\n\t\t\t\t\tinputStream = task.getPath().getFileSystem().open(task.getPath());\n\t\t\t\t\tint bytes = IOUtils.copy(inputStream, outputStream);\n\t\t\t\t\tbytesCounter.add(bytes);\n\t\t\t\t} finally {\n\t\t\t\t\tIOUtils.closeQuietly(inputStream);\n\t\t\t\t\tIOUtils.closeQuietly(outputStream);\n\t\t\t\t}\n\t\t\t\tfileCounter.add(1l);\n\t\t\t}\n\t\t});\n\n\t\t// no data sinks are needed, therefore just printing an empty result\n\t\tres.print();\n\n\t\tMap<String, Object> accumulators = env.getLastJobExecutionResult().getAllAccumulatorResults();\n\t\tLOGGER.info(\"== COUNTERS ==\");\n\t\tfor (Map.Entry<String, Object> e : accumulators.entrySet()) {\n\t\t\tLOGGER.info(e.getKey() + \": \" + e.getValue());\n\t\t}\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 +\n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\t// set up the execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tParameterTool params = ParameterTool.fromArgs(args);\n\t\tif (!params.has(\"input\") || !params.has(\"output\")) {\n\t\t\tSystem.err.println(\"Usage: --input <path> --output <path> [--parallelism <n>]\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal Path sourcePath = new Path(params.get(\"input\"));\n\t\tfinal Path targetPath = new Path(params.get(\"output\"));\n\t\tif (!isLocal(env) && !(isOnDistributedFS(sourcePath) && isOnDistributedFS(targetPath))) {\n\t\t\tSystem.out.println(\"In a distributed mode only HDFS input/output paths are supported\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal int parallelism = params.getInt(\"parallelism\", 10);\n\t\tif (parallelism <= 0) {\n\t\t\tSystem.err.println(\"Parallelism should be greater than 0\");\n\t\t\treturn;\n\t\t}\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\tenv.setParallelism(parallelism);\n\n\t\tlong startTime = System.currentTimeMillis();\n\t\tLOGGER.info(\"Initializing copy tasks\");\n\t\tList<FileCopyTask> tasks = getCopyTasks(sourcePath);\n\t\tLOGGER.info(\"Copy task initialization took \" + (System.currentTimeMillis() - startTime) + \"ms\");\n\n\t\tDataSet<FileCopyTask> inputTasks = new DataSource<>(env,\n\t\t\t\tnew FileCopyTaskInputFormat(tasks),\n\t\t\t\tnew GenericTypeInfo<>(FileCopyTask.class), \"fileCopyTasks\");\n\n\t\tFlatMapOperator<FileCopyTask, Object> res = inputTasks.flatMap(new RichFlatMapFunction<FileCopyTask, Object>() {\n\n\t\t\tprivate static final long serialVersionUID = 1109254230243989929L;\n\t\t\tprivate LongCounter fileCounter;\n\t\t\tprivate LongCounter bytesCounter;\n\n\t\t\t@Override\n\t\t\tpublic void open(Configuration parameters) throws Exception {\n\t\t\t\tbytesCounter = getRuntimeContext().getLongCounter(BYTES_COPIED_CNT_NAME);\n\t\t\t\tfileCounter = getRuntimeContext().getLongCounter(FILES_COPIED_CNT_NAME);\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void flatMap(FileCopyTask task, Collector<Object> out) throws Exception {\n\t\t\t\tLOGGER.info(\"Processing task: \" + task);\n\t\t\t\tPath outPath = new Path(targetPath, task.getRelativePath());\n\n\t\t\t\tFileSystem targetFs = targetPath.getFileSystem();\n\t\t\t\t// creating parent folders in case of a local FS\n\t\t\t\tif (!targetFs.isDistributedFS()) {\n\t\t\t\t\t//dealing with cases like file:///tmp or just /tmp\n\t\t\t\t\tFile outFile = outPath.toUri().isAbsolute() ? new File(outPath.toUri()) : new File(outPath.toString());\n\t\t\t\t\tFile parentFile = outFile.getParentFile();\n\t\t\t\t\tif (!parentFile.mkdirs() && !parentFile.exists()) {\n\t\t\t\t\t\tthrow new RuntimeException(\"Cannot create local file system directories: \" + parentFile);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tFSDataOutputStream outputStream = null;\n\t\t\t\tFSDataInputStream inputStream = null;\n\t\t\t\ttry {\n\t\t\t\t\toutputStream = targetFs.create(outPath, true);\n\t\t\t\t\tinputStream = task.getPath().getFileSystem().open(task.getPath());\n\t\t\t\t\tint bytes = IOUtils.copy(inputStream, outputStream);\n\t\t\t\t\tbytesCounter.add(bytes);\n\t\t\t\t} finally {\n\t\t\t\t\tIOUtils.closeQuietly(inputStream);\n\t\t\t\t\tIOUtils.closeQuietly(outputStream);\n\t\t\t\t}\n\t\t\t\tfileCounter.add(1L);\n\t\t\t}\n\t\t});\n\n\t\t// no data sinks are needed, therefore just printing an empty result\n\t\tres.print();\n\n\t\tMap<String, Object> accumulators = env.getLastJobExecutionResult().getAllAccumulatorResults();\n\t\tLOGGER.info(\"== COUNTERS ==\");\n\t\tfor (Map.Entry<String, Object> e : accumulators.entrySet()) {\n\t\t\tLOGGER.info(e.getKey() + \": \" + e.getValue());\n\t\t}\n\t}"
        ],
        [
            "WindowJoin::main(String)",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55 -\n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t// parse the parameters\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\tfinal long windowSize = params.getLong(\"windowSize\", 2000);\n\t\tfinal long rate = params.getLong(\"rate\", 3L);\n\t\t\n\t\tSystem.out.println(\"Using windowSize=\" + windowSize + \", data rate=\" + rate);\n\t\tSystem.out.println(\"To customize example, use: WindowJoin [--windowSize <window-size-in-millis>] [--rate <elements-per-second>]\");\n\n\t\t// obtain execution environment, run this example in \"ingestion time\"\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// create the data sources for both grades and salaries\n\t\tDataStream<Tuple2<String, Integer>> grades = GradeSource.getSource(env, rate);\n\t\tDataStream<Tuple2<String, Integer>> salaries = SalarySource.getSource(env, rate);\n\t\t\n\t\t// run the actual window join program\n\t\t// for testability, this functionality is in a separate method.\n\t\tDataStream<Tuple3<String, Integer, Integer>> joinedStream = runWindowJoin(grades, salaries, windowSize);\n\n\t\t// print the results with a single thread, rather than in parallel\n\t\tjoinedStream.print().setParallelism(1);\n\n\t\t// execute program\n\t\tenv.execute(\"Windowed Join Example\");\n\t}",
            "  49  \n  50  \n  51  \n  52  \n  53  \n  54 +\n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t// parse the parameters\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\tfinal long windowSize = params.getLong(\"windowSize\", 2000);\n\t\tfinal long rate = params.getLong(\"rate\", 3L);\n\n\t\tSystem.out.println(\"Using windowSize=\" + windowSize + \", data rate=\" + rate);\n\t\tSystem.out.println(\"To customize example, use: WindowJoin [--windowSize <window-size-in-millis>] [--rate <elements-per-second>]\");\n\n\t\t// obtain execution environment, run this example in \"ingestion time\"\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// create the data sources for both grades and salaries\n\t\tDataStream<Tuple2<String, Integer>> grades = GradeSource.getSource(env, rate);\n\t\tDataStream<Tuple2<String, Integer>> salaries = SalarySource.getSource(env, rate);\n\n\t\t// run the actual window join program\n\t\t// for testability, this functionality is in a separate method.\n\t\tDataStream<Tuple3<String, Integer, Integer>> joinedStream = runWindowJoin(grades, salaries, windowSize);\n\n\t\t// print the results with a single thread, rather than in parallel\n\t\tjoinedStream.print().setParallelism(1);\n\n\t\t// execute program\n\t\tenv.execute(\"Windowed Join Example\");\n\t}"
        ],
        [
            "CollectionExecutionExample::EMail::toString()",
            "  65  \n  66 -\n  67  ",
            "\t\tpublic String toString() {\n\t\t\treturn \"eMail{userId=\"+userId+\" subject=\"+subject+\" body=\"+body+\"}\";\n\t\t}",
            "  70  \n  71 +\n  72  ",
            "\t\tpublic String toString() {\n\t\t\treturn \"eMail{userId=\" + userId + \" subject=\" + subject + \" body=\" + body + \"}\";\n\t\t}"
        ],
        [
            "SocketWindowWordCountITCase::testScalaProgram()",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "\t@Test\n\tpublic void testScalaProgram() throws Exception {\n\t\tInetAddress localhost = InetAddress.getByName(\"localhost\");\n\n\t\t// suppress sysout messages from this example\n\t\tfinal PrintStream originalSysout = System.out;\n\t\tfinal PrintStream originalSyserr = System.err;\n\n\t\tfinal ByteArrayOutputStream errorMessages = new ByteArrayOutputStream();\n\n\t\tSystem.setOut(new PrintStream(new NullStream()));\n\t\tSystem.setErr(new PrintStream(errorMessages));\n\n\t\ttry {\n\t\t\ttry (ServerSocket server = new ServerSocket(0, 10, localhost)) {\n\n\t\t\t\tfinal ServerThread serverThread = new ServerThread(server);\n\t\t\t\tserverThread.setDaemon(true);\n\t\t\t\tserverThread.start();\n\n\t\t\t\tfinal int serverPort = server.getLocalPort();\n\n\t\t\t\torg.apache.flink.streaming.scala.examples.socket.SocketWindowWordCount.main(\n\t\t\t\t\t\tnew String[] { \"--port\", String.valueOf(serverPort) });\n\n\t\t\t\tif (errorMessages.size() != 0) {\n\t\t\t\t\tfail(\"Found error message: \" + new String(errorMessages.toByteArray(), ConfigConstants.DEFAULT_CHARSET));\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tserverThread.join();\n\t\t\t\tserverThread.checkError();\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tSystem.setOut(originalSysout);\n\t\t\tSystem.setErr(originalSyserr);\n\t\t}\n\t}",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  ",
            "\t@Test\n\tpublic void testScalaProgram() throws Exception {\n\t\tInetAddress localhost = InetAddress.getByName(\"localhost\");\n\n\t\t// suppress sysout messages from this example\n\t\tfinal PrintStream originalSysout = System.out;\n\t\tfinal PrintStream originalSyserr = System.err;\n\n\t\tfinal ByteArrayOutputStream errorMessages = new ByteArrayOutputStream();\n\n\t\tSystem.setOut(new PrintStream(new NullStream()));\n\t\tSystem.setErr(new PrintStream(errorMessages));\n\n\t\ttry {\n\t\t\ttry (ServerSocket server = new ServerSocket(0, 10, localhost)) {\n\n\t\t\t\tfinal ServerThread serverThread = new ServerThread(server);\n\t\t\t\tserverThread.setDaemon(true);\n\t\t\t\tserverThread.start();\n\n\t\t\t\tfinal int serverPort = server.getLocalPort();\n\n\t\t\t\torg.apache.flink.streaming.scala.examples.socket.SocketWindowWordCount.main(\n\t\t\t\t\t\tnew String[] { \"--port\", String.valueOf(serverPort) });\n\n\t\t\t\tif (errorMessages.size() != 0) {\n\t\t\t\t\tfail(\"Found error message: \" + new String(errorMessages.toByteArray(), ConfigConstants.DEFAULT_CHARSET));\n\t\t\t\t}\n\n\t\t\t\tserverThread.join();\n\t\t\t\tserverThread.checkError();\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tSystem.setOut(originalSysout);\n\t\t\tSystem.setErr(originalSyserr);\n\t\t}\n\t}"
        ],
        [
            "ReadFromKafka::main(String)",
            "  38  \n  39  \n  40  \n  41  \n  42 -\n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t// parse input arguments\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif(parameterTool.getNumberOfParameters() < 4) {\n\t\t\tSystem.out.println(\"Missing parameters!\\nUsage: Kafka --topic <topic> \" +\n\t\t\t\t\t\"--bootstrap.servers <kafka brokers> --zookeeper.connect <zk quorum> --group.id <some id>\");\n\t\t\treturn;\n\t\t}\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(4, 10000));\n\t\tenv.enableCheckpointing(5000); // create a checkpoint every 5 seconds\n\t\tenv.getConfig().setGlobalJobParameters(parameterTool); // make parameters available in the web interface\n\n\t\tDataStream<String> messageStream = env\n\t\t\t\t.addSource(new FlinkKafkaConsumer08<>(\n\t\t\t\t\t\tparameterTool.getRequired(\"topic\"),\n\t\t\t\t\t\tnew SimpleStringSchema(),\n\t\t\t\t\t\tparameterTool.getProperties()));\n\n\t\t// write kafka stream to standard out.\n\t\tmessageStream.print();\n\n\t\tenv.execute(\"Read from Kafka example\");\n\t}",
            "  37  \n  38  \n  39  \n  40  \n  41 +\n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t// parse input arguments\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 4) {\n\t\t\tSystem.out.println(\"Missing parameters!\\nUsage: Kafka --topic <topic> \" +\n\t\t\t\t\t\"--bootstrap.servers <kafka brokers> --zookeeper.connect <zk quorum> --group.id <some id>\");\n\t\t\treturn;\n\t\t}\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.getConfig().setRestartStrategy(RestartStrategies.fixedDelayRestart(4, 10000));\n\t\tenv.enableCheckpointing(5000); // create a checkpoint every 5 seconds\n\t\tenv.getConfig().setGlobalJobParameters(parameterTool); // make parameters available in the web interface\n\n\t\tDataStream<String> messageStream = env\n\t\t\t\t.addSource(new FlinkKafkaConsumer08<>(\n\t\t\t\t\t\tparameterTool.getRequired(\"topic\"),\n\t\t\t\t\t\tnew SimpleStringSchema(),\n\t\t\t\t\t\tparameterTool.getProperties()));\n\n\t\t// write kafka stream to standard out.\n\t\tmessageStream.print();\n\n\t\tenv.execute(\"Read from Kafka example\");\n\t}"
        ],
        [
            "LinearRegression::main(String)",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 -\n 115  \n 116  \n 117 -\n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tfinal int iterations = params.getInt(\"iterations\", 10);\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input x data from elements\n\t\tDataSet<Data> data;\n\t\tif (params.has(\"input\")) {\n\t\t\t// read data from CSV file\n\t\t\tdata = env.readCsvFile(params.get(\"input\"))\n\t\t\t\t\t.fieldDelimiter(\" \")\n\t\t\t\t\t.includeFields(true, true)\n\t\t\t\t\t.pojoType(Data.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing LinearRegression example with default input data set.\");\n\t\t\tSystem.out.println(\"Use --input to specify file input.\");\n\t\t\tdata = LinearRegressionData.getDefaultDataDataSet(env);\n\t\t}\n\n\t\t// get the parameters from elements\n\t\tDataSet<Params> parameters = LinearRegressionData.getDefaultParamsDataSet(env);\n\n\t\t// set number of bulk iterations for SGD linear Regression\n\t\tIterativeDataSet<Params> loop = parameters.iterate(iterations);\n\n\t\tDataSet<Params> new_parameters = data\n\t\t\t\t// compute a single step using every sample\n\t\t\t\t.map(new SubUpdate()).withBroadcastSet(loop, \"parameters\")\n\t\t\t\t// sum up all the steps\n\t\t\t\t.reduce(new UpdateAccumulator())\n\t\t\t\t// average the steps and update all parameters\n\t\t\t\t.map(new Update());\n\n\t\t// feed new parameters back into next iteration\n\t\tDataSet<Params> result = loop.closeWith(new_parameters);\n\n\t\t// emit result\n\t\tif(params.has(\"output\")) {\n\t\t\tresult.writeAsText(params.get(\"output\"));\n\t\t\t// execute program\n\t\t\tenv.execute(\"Linear Regression example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\t}",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111  \n 112  \n 113 +\n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\t// set up execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\tfinal int iterations = params.getInt(\"iterations\", 10);\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\t// get input x data from elements\n\t\tDataSet<Data> data;\n\t\tif (params.has(\"input\")) {\n\t\t\t// read data from CSV file\n\t\t\tdata = env.readCsvFile(params.get(\"input\"))\n\t\t\t\t\t.fieldDelimiter(\" \")\n\t\t\t\t\t.includeFields(true, true)\n\t\t\t\t\t.pojoType(Data.class);\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing LinearRegression example with default input data set.\");\n\t\t\tSystem.out.println(\"Use --input to specify file input.\");\n\t\t\tdata = LinearRegressionData.getDefaultDataDataSet(env);\n\t\t}\n\n\t\t// get the parameters from elements\n\t\tDataSet<Params> parameters = LinearRegressionData.getDefaultParamsDataSet(env);\n\n\t\t// set number of bulk iterations for SGD linear Regression\n\t\tIterativeDataSet<Params> loop = parameters.iterate(iterations);\n\n\t\tDataSet<Params> newParameters = data\n\t\t\t\t// compute a single step using every sample\n\t\t\t\t.map(new SubUpdate()).withBroadcastSet(loop, \"parameters\")\n\t\t\t\t// sum up all the steps\n\t\t\t\t.reduce(new UpdateAccumulator())\n\t\t\t\t// average the steps and update all parameters\n\t\t\t\t.map(new Update());\n\n\t\t// feed new parameters back into next iteration\n\t\tDataSet<Params> result = loop.closeWith(newParameters);\n\n\t\t// emit result\n\t\tif (params.has(\"output\")) {\n\t\t\tresult.writeAsText(params.get(\"output\"));\n\t\t\t// execute program\n\t\t\tenv.execute(\"Linear Regression example\");\n\t\t} else {\n\t\t\tSystem.out.println(\"Printing result to stdout. Use --output to specify output path.\");\n\t\t\tresult.print();\n\t\t}\n\t}"
        ],
        [
            "KMeans::Point::euclideanDistance(Point)",
            " 189  \n 190 -\n 191  ",
            "\t\tpublic double euclideanDistance(Point other) {\n\t\t\treturn Math.sqrt((x-other.x)*(x-other.x) + (y-other.y)*(y-other.y));\n\t\t}",
            " 184  \n 185 +\n 186  ",
            "\t\tpublic double euclideanDistance(Point other) {\n\t\t\treturn Math.sqrt((x - other.x) * (x - other.x) + (y - other.y) * (y - other.y));\n\t\t}"
        ],
        [
            "PageRankData::getDefaultEdgeDataSet(ExecutionEnvironment)",
            "  69  \n  70 -\n  71  \n  72 -\n  73 -\n  74  \n  75  \n  76  ",
            "\tpublic static DataSet<Tuple2<Long, Long>> getDefaultEdgeDataSet(ExecutionEnvironment env) {\n\t\t\n\t\tList<Tuple2<Long, Long>> edges = new ArrayList<Tuple2<Long, Long>>();\n\t\tfor(Object[] e : EDGES) {\n\t\t\tedges.add(new Tuple2<Long, Long>((Long)e[0], (Long)e[1]));\n\t\t}\n\t\treturn env.fromCollection(edges);\n\t}",
            "  69  \n  70 +\n  71  \n  72 +\n  73 +\n  74  \n  75  \n  76  ",
            "\tpublic static DataSet<Tuple2<Long, Long>> getDefaultEdgeDataSet(ExecutionEnvironment env) {\n\n\t\tList<Tuple2<Long, Long>> edges = new ArrayList<Tuple2<Long, Long>>();\n\t\tfor (Object[] e : EDGES) {\n\t\t\tedges.add(new Tuple2<Long, Long>((Long) e[0], (Long) e[1]));\n\t\t}\n\t\treturn env.fromCollection(edges);\n\t}"
        ],
        [
            "EnumTrianglesDataTypes::EdgeWithDegrees::getFirstDegree()",
            "  95 -",
            "\t\tpublic Integer getFirstDegree() { return this.getField(D1); }",
            " 125 +\n 126 +\n 127 +",
            "\t\tpublic Integer getFirstDegree() {\n\t\t\treturn this.getField(D1);\n\t\t}"
        ],
        [
            "EnumTrianglesDataTypes::Edge::getFirstVertex()",
            "  40 -",
            "\t\tpublic Integer getFirstVertex() { return this.getField(V1); }",
            "  46 +\n  47 +\n  48 +",
            "\t\tpublic Integer getFirstVertex() {\n\t\t\treturn this.getField(V1);\n\t\t}"
        ],
        [
            "EnumTriangles::EdgeByIdProjector::map(Edge)",
            " 146  \n 147  \n 148 -\n 149  \n 150 -\n 151  \n 152  \n 153 -\n 154  \n 155  ",
            "\t\t@Override\n\t\tpublic Edge map(Edge inEdge) throws Exception {\n\t\t\t\n\t\t\t// flip vertices if necessary\n\t\t\tif(inEdge.getFirstVertex() > inEdge.getSecondVertex()) {\n\t\t\t\tinEdge.flipVertices();\n\t\t\t}\n\t\t\t\n\t\t\treturn inEdge;\n\t\t}",
            " 142  \n 143  \n 144 +\n 145  \n 146 +\n 147  \n 148  \n 149 +\n 150  \n 151  ",
            "\t\t@Override\n\t\tpublic Edge map(Edge inEdge) throws Exception {\n\n\t\t\t// flip vertices if necessary\n\t\t\tif (inEdge.getFirstVertex() > inEdge.getSecondVertex()) {\n\t\t\t\tinEdge.flipVertices();\n\t\t\t}\n\n\t\t\treturn inEdge;\n\t\t}"
        ],
        [
            "WebLogDataGenerator::main(String)",
            "  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58  \n  59  \n  60 -\n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 -\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "\t/**\n\t * Main method to generate data for the {@link WebLogAnalysis} example program.\n\t * <p>\n\t * The generator creates to files:\n\t * <ul>\n\t * <li><code>{tmp.dir}/documents</code> for the web documents\n\t * <li><code>{tmp.dir}/ranks</code> for the ranks of the web documents\n\t * <li><code>{tmp.dir}/visits</code> for the logged visits of web documents\n\t * </ul> \n\t * \n\t * @param args \n\t * <ol>\n\t * <li>Int: Number of web documents\n\t * <li>Int: Number of visits\n\t * </ol>\n\t */\n\tpublic static void main(String[] args) {\n\n\t\t// parse parameters\n\t\tif (args.length < 2) {\n\t\t\tSystem.out.println(\"WebLogDataGenerator <numberOfDocuments> <numberOfVisits>\");\n\t\t\tSystem.exit(1);\n\t\t}\n\t\t\n\t\tint noDocs = Integer.parseInt(args[0]);\n\t\tint noVisits = Integer.parseInt(args[1]);\n\t\t\n\t\tString[] filterKWs = { \"editors\", \"oscillations\", \"convection\" };\n\n\t\tString[] words = { \"Lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\",\n\t\t\t\t\"consectetuer\", \"adipiscing\", \"elit\", \"sed\", \"diam\", \"nonummy\",\n\t\t\t\t\"nibh\", \"euismod\", \"tincidunt\", \"ut\", \"laoreet\", \"dolore\",\n\t\t\t\t\"magna\", \"aliquam\", \"erat\", \"volutpat\", \"Ut\", \"wisi\", \"enim\",\n\t\t\t\t\"ad\", \"minim\", \"veniam\", \"quis\", \"nostrud\", \"exerci\", \"tation\",\n\t\t\t\t\"ullamcorper\", \"suscipit\", \"lobortis\", \"nisl\", \"ut\", \"aliquip\",\n\t\t\t\t\"ex\", \"ea\", \"commodo\" };\n\n\t\t\n\t\tfinal String outPath = System.getProperty(\"java.io.tmpdir\");\n\n\t\tSystem.out.println(\"Generating documents files...\");\n\t\tgenDocs(noDocs, filterKWs, words, outPath + \"/documents\");\n\t\tSystem.out.println(\"Generating ranks files...\");\n\t\tgenRanks(noDocs, outPath + \"/ranks\");\n\t\tSystem.out.println(\"Generating visits files...\");\n\t\tgenVisits(noVisits, noDocs, outPath + \"/visits\");\n\n\t\tSystem.out.println(\"Done!\");\n\t}",
            "  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58  \n  59  \n  60 +\n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\t/**\n\t * Main method to generate data for the {@link WebLogAnalysis} example program.\n\t *\n\t * <p>The generator creates to files:\n\t * <ul>\n\t * <li><code>{tmp.dir}/documents</code> for the web documents\n\t * <li><code>{tmp.dir}/ranks</code> for the ranks of the web documents\n\t * <li><code>{tmp.dir}/visits</code> for the logged visits of web documents\n\t * </ul>\n\t *\n\t * @param args\n\t * <ol>\n\t * <li>Int: Number of web documents\n\t * <li>Int: Number of visits\n\t * </ol>\n\t */\n\tpublic static void main(String[] args) {\n\n\t\t// parse parameters\n\t\tif (args.length < 2) {\n\t\t\tSystem.out.println(\"WebLogDataGenerator <numberOfDocuments> <numberOfVisits>\");\n\t\t\tSystem.exit(1);\n\t\t}\n\n\t\tint noDocs = Integer.parseInt(args[0]);\n\t\tint noVisits = Integer.parseInt(args[1]);\n\n\t\tString[] filterKWs = { \"editors\", \"oscillations\", \"convection\" };\n\n\t\tString[] words = { \"Lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\",\n\t\t\t\t\"consectetuer\", \"adipiscing\", \"elit\", \"sed\", \"diam\", \"nonummy\",\n\t\t\t\t\"nibh\", \"euismod\", \"tincidunt\", \"ut\", \"laoreet\", \"dolore\",\n\t\t\t\t\"magna\", \"aliquam\", \"erat\", \"volutpat\", \"Ut\", \"wisi\", \"enim\",\n\t\t\t\t\"ad\", \"minim\", \"veniam\", \"quis\", \"nostrud\", \"exerci\", \"tation\",\n\t\t\t\t\"ullamcorper\", \"suscipit\", \"lobortis\", \"nisl\", \"ut\", \"aliquip\",\n\t\t\t\t\"ex\", \"ea\", \"commodo\" };\n\n\t\tfinal String outPath = System.getProperty(\"java.io.tmpdir\");\n\n\t\tSystem.out.println(\"Generating documents files...\");\n\t\tgenDocs(noDocs, filterKWs, words, outPath + \"/documents\");\n\t\tSystem.out.println(\"Generating ranks files...\");\n\t\tgenRanks(noDocs, outPath + \"/ranks\");\n\t\tSystem.out.println(\"Generating visits files...\");\n\t\tgenVisits(noVisits, noDocs, outPath + \"/visits\");\n\n\t\tSystem.out.println(\"Done!\");\n\t}"
        ],
        [
            "EnumTrianglesDataTypes::EdgeWithDegrees::getSecondDegree()",
            "  97 -",
            "\t\tpublic Integer getSecondDegree() { return this.getField(D2); }",
            " 129 +\n 130 +\n 131 +",
            "\t\tpublic Integer getSecondDegree() {\n\t\t\treturn this.getField(D2);\n\t\t}"
        ],
        [
            "KMeans::Centroid::Centroid(int,double,double)",
            " 212  \n 213 -\n 214  \n 215  ",
            "\t\tpublic Centroid(int id, double x, double y) {\n\t\t\tsuper(x,y);\n\t\t\tthis.id = id;\n\t\t}",
            " 207  \n 208 +\n 209  \n 210  ",
            "\t\tpublic Centroid(int id, double x, double y) {\n\t\t\tsuper(x, y);\n\t\t\tthis.id = id;\n\t\t}"
        ]
    ],
    "0e69dd5cc603443355a722fbbac5d96264eae1d2": [
        [
            "YARNHighAvailabilityITCase::teardown()",
            "  83  \n  84  \n  85 -\n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "\t@AfterClass\n\tpublic static void teardown() throws Exception {\n\t\tif(zkServer != null) {\n\t\t\tzkServer.stop();\n\t\t}\n\n\t\tJavaTestKit.shutdownActorSystem(actorSystem);\n\t\tactorSystem = null;\n\t}",
            "  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "\t@AfterClass\n\tpublic static void teardown() throws Exception {\n\t\tif (zkServer != null) {\n\t\t\tzkServer.stop();\n\t\t}\n\n\t\tJavaTestKit.shutdownActorSystem(actorSystem);\n\t\tactorSystem = null;\n\t}"
        ],
        [
            "FlinkYarnSessionCliTest::testDynamicProperties()",
            "  54  \n  55 -\n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70 -\n  71 -\n  72  \n  73 -\n  74 -\n  75 -\n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  ",
            "\t@Test\n\tpublic void testDynamicProperties() throws IOException {\n\n\t\tMap<String, String> map = new HashMap<String, String>(System.getenv());\n\t\tFile tmpFolder = tmp.newFolder();\n\t\tFile fakeConf = new File(tmpFolder, \"flink-conf.yaml\");\n\t\tfakeConf.createNewFile();\n\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, tmpFolder.getAbsolutePath());\n\t\tTestBaseUtils.setEnv(map);\n\t\tFlinkYarnSessionCli cli = new FlinkYarnSessionCli(\"\", \"\", false);\n\t\tOptions options = new Options();\n\t\tcli.addGeneralOptions(options);\n\t\tcli.addRunOptions(options);\n\n\t\tCommandLineParser parser = new DefaultParser();\n\t\tCommandLine cmd = null;\n\t\ttry {\n\t\t\tcmd = parser.parse(options, new String[]{\"run\", \"-j\", \"fake.jar\", \"-n\", \"15\",\n\t\t\t\t\"-D\", \"akka.ask.timeout=5 min\", \"-D\", \"env.java.opts=-DappName=foobar\"});\n\t\t} catch(Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Parsing failed with \" + e.getMessage());\n\t\t}\n\n\t\tAbstractYarnClusterDescriptor flinkYarnDescriptor = cli.createDescriptor(null, cmd);\n\n\t\tAssert.assertNotNull(flinkYarnDescriptor);\n\n\t\tMap<String, String> dynProperties =\n\t\t\tFlinkYarnSessionCli.getDynamicProperties(flinkYarnDescriptor.getDynamicPropertiesEncoded());\n\t\tAssert.assertEquals(2, dynProperties.size());\n\t\tAssert.assertEquals(\"5 min\", dynProperties.get(\"akka.ask.timeout\"));\n\t\tAssert.assertEquals(\"-DappName=foobar\", dynProperties.get(\"env.java.opts\"));\n\t}",
            "  56  \n  57 +\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\t@Test\n\tpublic void testDynamicProperties() throws Exception {\n\n\t\tMap<String, String> map = new HashMap<String, String>(System.getenv());\n\t\tFile tmpFolder = tmp.newFolder();\n\t\tFile fakeConf = new File(tmpFolder, \"flink-conf.yaml\");\n\t\tfakeConf.createNewFile();\n\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, tmpFolder.getAbsolutePath());\n\t\tTestBaseUtils.setEnv(map);\n\t\tFlinkYarnSessionCli cli = new FlinkYarnSessionCli(\"\", \"\", false);\n\t\tOptions options = new Options();\n\t\tcli.addGeneralOptions(options);\n\t\tcli.addRunOptions(options);\n\n\t\tCommandLineParser parser = new DefaultParser();\n\t\tCommandLine cmd = parser.parse(options, new String[]{\"run\", \"-j\", \"fake.jar\", \"-n\", \"15\",\n\t\t\t\t\"-D\", \"akka.ask.timeout=5 min\", \"-D\", \"env.java.opts=-DappName=foobar\"});\n\n\t\tAbstractYarnClusterDescriptor flinkYarnDescriptor = cli.createDescriptor(null, cmd);\n\n\t\tAssert.assertNotNull(flinkYarnDescriptor);\n\n\t\tMap<String, String> dynProperties =\n\t\t\tFlinkYarnSessionCli.getDynamicProperties(flinkYarnDescriptor.getDynamicPropertiesEncoded());\n\t\tAssert.assertEquals(2, dynProperties.size());\n\t\tAssert.assertEquals(\"5 min\", dynProperties.get(\"akka.ask.timeout\"));\n\t\tAssert.assertEquals(\"-DappName=foobar\", dynProperties.get(\"env.java.opts\"));\n\t}"
        ],
        [
            "YarnTestBase::ensureNoProhibitedStringInLogFiles(String,String)",
            " 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 -\n 295  \n 296  \n 297 -\n 298  \n 299  \n 300  \n 301  \n 302 -\n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309 -\n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 -\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329 -\n 330  \n 331  \n 332  \n 333  \n 334  \n 335 -\n 336  \n 337  \n 338  \n 339  \n 340 -\n 341  \n 342  \n 343  \n 344 -\n 345  \n 346 -\n 347  \n 348  ",
            "\t/**\n\t * This method checks the written TaskManager and JobManager log files\n\t * for exceptions.\n\t *\n\t * WARN: Please make sure the tool doesn't find old logfiles from previous test runs.\n\t * So always run \"mvn clean\" before running the tests here.\n\t *\n\t */\n\tpublic static void ensureNoProhibitedStringInLogFiles(final String[] prohibited, final String[] whitelisted) {\n\t\tFile cwd = new File(\"target/\" + yarnConfiguration.get(TEST_CLUSTER_NAME_KEY));\n\t\tAssert.assertTrue(\"Expecting directory \" + cwd.getAbsolutePath() + \" to exist\", cwd.exists());\n\t\tAssert.assertTrue(\"Expecting directory \" + cwd.getAbsolutePath() + \" to be a directory\", cwd.isDirectory());\n\t\t\n\t\tFile foundFile = findFile(cwd.getAbsolutePath(), new FilenameFilter() {\n\t\t\t@Override\n\t\t\tpublic boolean accept(File dir, String name) {\n\t\t\t// scan each file for prohibited strings.\n\t\t\tFile f = new File(dir.getAbsolutePath()+ \"/\" + name);\n\t\t\ttry {\n\t\t\t\tScanner scanner = new Scanner(f);\n\t\t\t\twhile (scanner.hasNextLine()) {\n\t\t\t\t\tfinal String lineFromFile = scanner.nextLine();\n\t\t\t\t\tfor (String aProhibited : prohibited) {\n\t\t\t\t\t\tif (lineFromFile.contains(aProhibited)) {\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tboolean whitelistedFound = false;\n\t\t\t\t\t\t\tfor (String white : whitelisted) {\n\t\t\t\t\t\t\t\tif (lineFromFile.contains(white)) {\n\t\t\t\t\t\t\t\t\twhitelistedFound = true;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tif (!whitelistedFound) {\n\t\t\t\t\t\t\t\t// logging in FATAL to see the actual message in TRAVIS tests.\n\t\t\t\t\t\t\t\tMarker fatal = MarkerFactory.getMarker(\"FATAL\");\n\t\t\t\t\t\t\t\tLOG.error(fatal, \"Prohibited String '{}' in line '{}'\", aProhibited, lineFromFile);\n\t\t\t\t\t\t\t\treturn true;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\t} catch (FileNotFoundException e) {\n\t\t\t\tLOG.warn(\"Unable to locate file: \"+e.getMessage()+\" file: \"+f.getAbsolutePath());\n\t\t\t}\n\n\t\t\treturn false;\n\t\t\t}\n\t\t});\n\t\tif(foundFile != null) {\n\t\t\tScanner scanner =  null;\n\t\t\ttry {\n\t\t\t\tscanner = new Scanner(foundFile);\n\t\t\t} catch (FileNotFoundException e) {\n\t\t\t\tAssert.fail(\"Unable to locate file: \"+e.getMessage()+\" file: \"+foundFile.getAbsolutePath());\n\t\t\t}\n\t\t\tLOG.warn(\"Found a file with a prohibited string. Printing contents:\");\n\t\t\twhile (scanner.hasNextLine()) {\n\t\t\t\tLOG.warn(\"LINE: \"+scanner.nextLine());\n\t\t\t}\n\t\t\tAssert.fail(\"Found a file \"+foundFile+\" with a prohibited string: \"+Arrays.toString(prohibited));\n\t\t}\n\t}",
            " 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298 +\n 299  \n 300  \n 301 +\n 302  \n 303  \n 304  \n 305  \n 306 +\n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313 +\n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321 +\n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333 +\n 334  \n 335  \n 336  \n 337  \n 338  \n 339 +\n 340  \n 341  \n 342  \n 343  \n 344 +\n 345  \n 346  \n 347  \n 348 +\n 349  \n 350 +\n 351  \n 352  ",
            "\t/**\n\t * This method checks the written TaskManager and JobManager log files\n\t * for exceptions.\n\t *\n\t * <p>WARN: Please make sure the tool doesn't find old logfiles from previous test runs.\n\t * So always run \"mvn clean\" before running the tests here.\n\t *\n\t */\n\tpublic static void ensureNoProhibitedStringInLogFiles(final String[] prohibited, final String[] whitelisted) {\n\t\tFile cwd = new File(\"target/\" + YARN_CONFIGURATION.get(TEST_CLUSTER_NAME_KEY));\n\t\tAssert.assertTrue(\"Expecting directory \" + cwd.getAbsolutePath() + \" to exist\", cwd.exists());\n\t\tAssert.assertTrue(\"Expecting directory \" + cwd.getAbsolutePath() + \" to be a directory\", cwd.isDirectory());\n\n\t\tFile foundFile = findFile(cwd.getAbsolutePath(), new FilenameFilter() {\n\t\t\t@Override\n\t\t\tpublic boolean accept(File dir, String name) {\n\t\t\t// scan each file for prohibited strings.\n\t\t\tFile f = new File(dir.getAbsolutePath() + \"/\" + name);\n\t\t\ttry {\n\t\t\t\tScanner scanner = new Scanner(f);\n\t\t\t\twhile (scanner.hasNextLine()) {\n\t\t\t\t\tfinal String lineFromFile = scanner.nextLine();\n\t\t\t\t\tfor (String aProhibited : prohibited) {\n\t\t\t\t\t\tif (lineFromFile.contains(aProhibited)) {\n\n\t\t\t\t\t\t\tboolean whitelistedFound = false;\n\t\t\t\t\t\t\tfor (String white : whitelisted) {\n\t\t\t\t\t\t\t\tif (lineFromFile.contains(white)) {\n\t\t\t\t\t\t\t\t\twhitelistedFound = true;\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (!whitelistedFound) {\n\t\t\t\t\t\t\t\t// logging in FATAL to see the actual message in TRAVIS tests.\n\t\t\t\t\t\t\t\tMarker fatal = MarkerFactory.getMarker(\"FATAL\");\n\t\t\t\t\t\t\t\tLOG.error(fatal, \"Prohibited String '{}' in line '{}'\", aProhibited, lineFromFile);\n\t\t\t\t\t\t\t\treturn true;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t}\n\t\t\t} catch (FileNotFoundException e) {\n\t\t\t\tLOG.warn(\"Unable to locate file: \" + e.getMessage() + \" file: \" + f.getAbsolutePath());\n\t\t\t}\n\n\t\t\treturn false;\n\t\t\t}\n\t\t});\n\t\tif (foundFile != null) {\n\t\t\tScanner scanner =  null;\n\t\t\ttry {\n\t\t\t\tscanner = new Scanner(foundFile);\n\t\t\t} catch (FileNotFoundException e) {\n\t\t\t\tAssert.fail(\"Unable to locate file: \" + e.getMessage() + \" file: \" + foundFile.getAbsolutePath());\n\t\t\t}\n\t\t\tLOG.warn(\"Found a file with a prohibited string. Printing contents:\");\n\t\t\twhile (scanner.hasNextLine()) {\n\t\t\t\tLOG.warn(\"LINE: \" + scanner.nextLine());\n\t\t\t}\n\t\t\tAssert.fail(\"Found a file \" + foundFile + \" with a prohibited string: \" + Arrays.toString(prohibited));\n\t\t}\n\t}"
        ],
        [
            "YARNSessionFIFOSecuredITCase::setup()",
            "  40  \n  41  \n  42  \n  43  \n  44  \n  45 -\n  46 -\n  47 -\n  48 -\n  49  \n  50  \n  51  \n  52 -\n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  \n  79  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\n\t\tLOG.info(\"starting secure cluster environment for testing\");\n\n\t\tyarnConfiguration.setClass(YarnConfiguration.RM_SCHEDULER, FifoScheduler.class, ResourceScheduler.class);\n\t\tyarnConfiguration.setInt(YarnConfiguration.NM_PMEM_MB, 768);\n\t\tyarnConfiguration.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 512);\n\t\tyarnConfiguration.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, \"flink-yarn-tests-fifo-secured\");\n\n\t\tSecureTestEnvironment.prepare(tmp);\n\n\t\tpopulateYarnSecureConfigurations(yarnConfiguration, SecureTestEnvironment.getHadoopServicePrincipal(),\n\t\t\t\tSecureTestEnvironment.getTestKeytab());\n\n\t\tConfiguration flinkConfig = new Configuration();\n\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB,\n\t\t\t\tSecureTestEnvironment.getTestKeytab());\n\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL,\n\t\t\t\tSecureTestEnvironment.getHadoopServicePrincipal());\n\n\t\tSecurityUtils.SecurityConfiguration ctx = new SecurityUtils.SecurityConfiguration(flinkConfig,\n\t\t\t\tyarnConfiguration);\n\t\ttry {\n\t\t\tTestingSecurityContext.install(ctx, SecureTestEnvironment.getClientSecurityConfigurationMap());\n\n\t\t\tSecurityUtils.getInstalledContext().runSecured(new Callable<Object>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() {\n\t\t\t\t\tstartYARNSecureMode(yarnConfiguration, SecureTestEnvironment.getHadoopServicePrincipal(),\n\t\t\t\t\t\t\tSecureTestEnvironment.getTestKeytab());\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t});\n\n\t\t} catch(Exception e) {\n\t\t\tthrow new RuntimeException(\"Exception occurred while setting up secure test context. Reason: {}\", e);\n\t\t}\n\n\t}",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49 +\n  50 +\n  51 +\n  52 +\n  53  \n  54  \n  55  \n  56 +\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73 +\n  74  \n  75  \n  76  \n  77  \n  78  \n  79 +\n  80  \n  81  \n  82  \n  83  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\n\t\tLOG.info(\"starting secure cluster environment for testing\");\n\n\t\tYARN_CONFIGURATION.setClass(YarnConfiguration.RM_SCHEDULER, FifoScheduler.class, ResourceScheduler.class);\n\t\tYARN_CONFIGURATION.setInt(YarnConfiguration.NM_PMEM_MB, 768);\n\t\tYARN_CONFIGURATION.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 512);\n\t\tYARN_CONFIGURATION.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, \"flink-yarn-tests-fifo-secured\");\n\n\t\tSecureTestEnvironment.prepare(tmp);\n\n\t\tpopulateYarnSecureConfigurations(YARN_CONFIGURATION, SecureTestEnvironment.getHadoopServicePrincipal(),\n\t\t\t\tSecureTestEnvironment.getTestKeytab());\n\n\t\tConfiguration flinkConfig = new Configuration();\n\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB,\n\t\t\t\tSecureTestEnvironment.getTestKeytab());\n\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL,\n\t\t\t\tSecureTestEnvironment.getHadoopServicePrincipal());\n\n\t\tSecurityUtils.SecurityConfiguration ctx = new SecurityUtils.SecurityConfiguration(flinkConfig,\n\t\t\tYARN_CONFIGURATION);\n\t\ttry {\n\t\t\tTestingSecurityContext.install(ctx, SecureTestEnvironment.getClientSecurityConfigurationMap());\n\n\t\t\tSecurityUtils.getInstalledContext().runSecured(new Callable<Object>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() {\n\t\t\t\t\tstartYARNSecureMode(YARN_CONFIGURATION, SecureTestEnvironment.getHadoopServicePrincipal(),\n\t\t\t\t\t\t\tSecureTestEnvironment.getTestKeytab());\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t});\n\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Exception occurred while setting up secure test context. Reason: {}\", e);\n\t\t}\n\n\t}"
        ],
        [
            "YARNSessionCapacitySchedulerITCase::setup()",
            "  75  \n  76  \n  77 -\n  78 -\n  79 -\n  80 -\n  81 -\n  82 -\n  83  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\tyarnConfiguration.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class, ResourceScheduler.class);\n\t\tyarnConfiguration.set(\"yarn.scheduler.capacity.root.queues\", \"default,qa-team\");\n\t\tyarnConfiguration.setInt(\"yarn.scheduler.capacity.root.default.capacity\", 40);\n\t\tyarnConfiguration.setInt(\"yarn.scheduler.capacity.root.qa-team.capacity\", 60);\n\t\tyarnConfiguration.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, \"flink-yarn-tests-capacityscheduler\");\n\t\tstartYARNWithConfig(yarnConfiguration);\n\t}",
            "  83  \n  84  \n  85 +\n  86 +\n  87 +\n  88 +\n  89 +\n  90 +\n  91  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\tYARN_CONFIGURATION.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class, ResourceScheduler.class);\n\t\tYARN_CONFIGURATION.set(\"yarn.scheduler.capacity.root.queues\", \"default,qa-team\");\n\t\tYARN_CONFIGURATION.setInt(\"yarn.scheduler.capacity.root.default.capacity\", 40);\n\t\tYARN_CONFIGURATION.setInt(\"yarn.scheduler.capacity.root.qa-team.capacity\", 60);\n\t\tYARN_CONFIGURATION.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, \"flink-yarn-tests-capacityscheduler\");\n\t\tstartYARNWithConfig(YARN_CONFIGURATION);\n\t}"
        ],
        [
            "YARNSessionFIFOITCase::testJavaAPI()",
            " 223  \n 224  \n 225  \n 226  \n 227  \n 228 -\n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249 -\n 250  \n 251  \n 252 -\n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 -\n 260  \n 261  \n 262  \n 263 -\n 264  \n 265 -\n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  ",
            "\t/**\n\t * Test the YARN Java API\n\t */\n\t@Test\n\tpublic void testJavaAPI() throws Exception {\n\t\tfinal int WAIT_TIME = 15;\n\t\tLOG.info(\"Starting testJavaAPI()\");\n\n\t\tAbstractYarnClusterDescriptor flinkYarnClient = new YarnClusterDescriptor();\n\t\tAssert.assertNotNull(\"unable to get yarn client\", flinkYarnClient);\n\t\tflinkYarnClient.setTaskManagerCount(1);\n\t\tflinkYarnClient.setJobManagerMemory(768);\n\t\tflinkYarnClient.setTaskManagerMemory(1024);\n\t\tflinkYarnClient.setLocalJarPath(new Path(flinkUberjar.getAbsolutePath()));\n\t\tflinkYarnClient.addShipFiles(Arrays.asList(flinkLibFolder.listFiles()));\n\t\tString confDirPath = System.getenv(ConfigConstants.ENV_FLINK_CONF_DIR);\n\t\tflinkYarnClient.setConfigurationDirectory(confDirPath);\n\t\tflinkYarnClient.setFlinkConfiguration(GlobalConfiguration.loadConfiguration());\n\t\tflinkYarnClient.setConfigurationFilePath(new Path(confDirPath + File.separator + \"flink-conf.yaml\"));\n\n\t\t// deploy\n\t\tClusterClient yarnCluster = null;\n\t\ttry {\n\t\t\tyarnCluster = flinkYarnClient.deploy();\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Failing test\", e);\n\t\t\tAssert.fail(\"Error while deploying YARN cluster: \"+e.getMessage());\n\t\t}\n\t\tGetClusterStatusResponse expectedStatus = new GetClusterStatusResponse(1, 1);\n\t\tfor(int second = 0; second < WAIT_TIME * 2; second++) { // run \"forever\"\n\t\t\ttry {\n\t\t\t\tThread.sleep(1000);\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tLOG.warn(\"Interrupted\", e);\n\t\t\t}\n\t\t\tGetClusterStatusResponse status = yarnCluster.getClusterStatus();\n\t\t\tif(status != null && status.equals(expectedStatus)) {\n\t\t\t\tLOG.info(\"ClusterClient reached status \" + status);\n\t\t\t\tbreak; // all good, cluster started\n\t\t\t}\n\t\t\tif(second > WAIT_TIME) {\n\t\t\t\t// we waited for 15 seconds. cluster didn't come up correctly\n\t\t\t\tAssert.fail(\"The custer didn't start after \" + WAIT_TIME + \" seconds\");\n\t\t\t}\n\t\t}\n\n\t\t// use the cluster\n\t\tAssert.assertNotNull(yarnCluster.getJobManagerAddress());\n\t\tAssert.assertNotNull(yarnCluster.getWebInterfaceURL());\n\n\t\tLOG.info(\"Shutting down cluster. All tests passed\");\n\t\t// shutdown cluster\n\t\tyarnCluster.shutdown();\n\t\tLOG.info(\"Finished testJavaAPI()\");\n\t}",
            " 218  \n 219  \n 220  \n 221  \n 222  \n 223 +\n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244 +\n 245  \n 246  \n 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 +\n 255  \n 256  \n 257  \n 258 +\n 259  \n 260 +\n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  ",
            "\t/**\n\t * Test the YARN Java API.\n\t */\n\t@Test\n\tpublic void testJavaAPI() throws Exception {\n\t\tfinal int waitTime = 15;\n\t\tLOG.info(\"Starting testJavaAPI()\");\n\n\t\tAbstractYarnClusterDescriptor flinkYarnClient = new YarnClusterDescriptor();\n\t\tAssert.assertNotNull(\"unable to get yarn client\", flinkYarnClient);\n\t\tflinkYarnClient.setTaskManagerCount(1);\n\t\tflinkYarnClient.setJobManagerMemory(768);\n\t\tflinkYarnClient.setTaskManagerMemory(1024);\n\t\tflinkYarnClient.setLocalJarPath(new Path(flinkUberjar.getAbsolutePath()));\n\t\tflinkYarnClient.addShipFiles(Arrays.asList(flinkLibFolder.listFiles()));\n\t\tString confDirPath = System.getenv(ConfigConstants.ENV_FLINK_CONF_DIR);\n\t\tflinkYarnClient.setConfigurationDirectory(confDirPath);\n\t\tflinkYarnClient.setFlinkConfiguration(GlobalConfiguration.loadConfiguration());\n\t\tflinkYarnClient.setConfigurationFilePath(new Path(confDirPath + File.separator + \"flink-conf.yaml\"));\n\n\t\t// deploy\n\t\tClusterClient yarnCluster = null;\n\t\ttry {\n\t\t\tyarnCluster = flinkYarnClient.deploy();\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Failing test\", e);\n\t\t\tAssert.fail(\"Error while deploying YARN cluster: \" + e.getMessage());\n\t\t}\n\t\tGetClusterStatusResponse expectedStatus = new GetClusterStatusResponse(1, 1);\n\t\tfor (int second = 0; second < waitTime * 2; second++) { // run \"forever\"\n\t\t\ttry {\n\t\t\t\tThread.sleep(1000);\n\t\t\t} catch (InterruptedException e) {\n\t\t\t\tLOG.warn(\"Interrupted\", e);\n\t\t\t}\n\t\t\tGetClusterStatusResponse status = yarnCluster.getClusterStatus();\n\t\t\tif (status != null && status.equals(expectedStatus)) {\n\t\t\t\tLOG.info(\"ClusterClient reached status \" + status);\n\t\t\t\tbreak; // all good, cluster started\n\t\t\t}\n\t\t\tif (second > waitTime) {\n\t\t\t\t// we waited for 15 seconds. cluster didn't come up correctly\n\t\t\t\tAssert.fail(\"The custer didn't start after \" + waitTime + \" seconds\");\n\t\t\t}\n\t\t}\n\n\t\t// use the cluster\n\t\tAssert.assertNotNull(yarnCluster.getJobManagerAddress());\n\t\tAssert.assertNotNull(yarnCluster.getWebInterfaceURL());\n\n\t\tLOG.info(\"Shutting down cluster. All tests passed\");\n\t\t// shutdown cluster\n\t\tyarnCluster.shutdown();\n\t\tLOG.info(\"Finished testJavaAPI()\");\n\t}"
        ],
        [
            "YarnTestBase::populateYarnSecureConfigurations(Configuration,String,String)",
            " 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 -\n 167  \n 168 -\n 169  \n 170 -\n 171  ",
            "\tpublic static void populateYarnSecureConfigurations(Configuration conf, String principal, String keytab) {\n\n\t\tconf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\tconf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\n\t\tconf.set(YarnConfiguration.RM_KEYTAB, keytab);\n\t\tconf.set(YarnConfiguration.RM_PRINCIPAL, principal);\n\t\tconf.set(YarnConfiguration.NM_KEYTAB, keytab);\n\t\tconf.set(YarnConfiguration.NM_PRINCIPAL, principal);\n\n\t\tconf.set(YarnConfiguration.RM_WEBAPP_SPNEGO_USER_NAME_KEY, principal);\n\t\tconf.set(YarnConfiguration.RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY,keytab);\n\t\tconf.set(YarnConfiguration.NM_WEBAPP_SPNEGO_USER_NAME_KEY, principal);\n\t\tconf.set(YarnConfiguration.NM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY,keytab);\n\n\t\tconf.set(\"hadoop.security.auth_to_local\",\"RULE:[1:$1] RULE:[2:$1]\");\n\t}",
            " 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167  \n 168 +\n 169  \n 170 +\n 171  ",
            "\tpublic static void populateYarnSecureConfigurations(Configuration conf, String principal, String keytab) {\n\n\t\tconf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\tconf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\n\t\tconf.set(YarnConfiguration.RM_KEYTAB, keytab);\n\t\tconf.set(YarnConfiguration.RM_PRINCIPAL, principal);\n\t\tconf.set(YarnConfiguration.NM_KEYTAB, keytab);\n\t\tconf.set(YarnConfiguration.NM_PRINCIPAL, principal);\n\n\t\tconf.set(YarnConfiguration.RM_WEBAPP_SPNEGO_USER_NAME_KEY, principal);\n\t\tconf.set(YarnConfiguration.RM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY, keytab);\n\t\tconf.set(YarnConfiguration.NM_WEBAPP_SPNEGO_USER_NAME_KEY, principal);\n\t\tconf.set(YarnConfiguration.NM_WEBAPP_SPNEGO_KEYTAB_FILE_KEY, keytab);\n\n\t\tconf.set(\"hadoop.security.auth_to_local\", \"RULE:[1:$1] RULE:[2:$1]\");\n\t}"
        ],
        [
            "YarnTestBase::startWithArgs(String,String,RunTypes)",
            " 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493 -\n 494  \n 495  \n 496  \n 497  \n 498  \n 499 -\n 500  \n 501  \n 502 -\n 503 -\n 504  \n 505  \n 506  \n 507  \n 508 -\n 509  \n 510 -\n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518 -\n 519  \n 520  \n 521  ",
            "\t/**\n\t * This method returns once the \"startedAfterString\" has been seen.\n\t */\n\tprotected Runner startWithArgs(String[] args, String startedAfterString, RunTypes type) {\n\t\tLOG.info(\"Running with args {}\", Arrays.toString(args));\n\n\t\toutContent = new ByteArrayOutputStream();\n\t\terrContent = new ByteArrayOutputStream();\n\t\tSystem.setOut(new PrintStream(outContent));\n\t\tSystem.setErr(new PrintStream(errContent));\n\n\t\tfinal int START_TIMEOUT_SECONDS = 60;\n\n\t\tRunner runner = new Runner(args, type, 0);\n\t\trunner.setName(\"Frontend (CLI/YARN Client) runner thread (startWithArgs()).\");\n\t\trunner.start();\n\n\t\tfor(int second = 0; second <  START_TIMEOUT_SECONDS; second++) {\n\t\t\tsleep(1000);\n\t\t\t// check output for correct TaskManager startup.\n\t\t\tif(outContent.toString().contains(startedAfterString)\n\t\t\t\t\t|| errContent.toString().contains(startedAfterString) ) {\n\t\t\t\tLOG.info(\"Found expected output in redirected streams\");\n\t\t\t\treturn runner;\n\t\t\t}\n\t\t\t// check if thread died\n\t\t\tif(!runner.isAlive()) {\n\t\t\t\tsendOutput();\n\t\t\t\tif(runner.getRunnerError() != null) {\n\t\t\t\t\tthrow new RuntimeException(\"Runner failed with exception.\", runner.getRunnerError());\n\t\t\t\t}\n\t\t\t\tAssert.fail(\"Runner thread died before the test was finished.\");\n\t\t\t}\n\t\t}\n\n\t\tsendOutput();\n\t\tAssert.fail(\"During the timeout period of \" + START_TIMEOUT_SECONDS + \" seconds the \" +\n\t\t\t\t\"expected string did not show up\");\n\t\treturn null;\n\t}",
            " 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497 +\n 498  \n 499  \n 500  \n 501  \n 502  \n 503 +\n 504  \n 505  \n 506 +\n 507 +\n 508  \n 509  \n 510  \n 511  \n 512 +\n 513  \n 514 +\n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522 +\n 523  \n 524  \n 525  ",
            "\t/**\n\t * This method returns once the \"startedAfterString\" has been seen.\n\t */\n\tprotected Runner startWithArgs(String[] args, String startedAfterString, RunTypes type) {\n\t\tLOG.info(\"Running with args {}\", Arrays.toString(args));\n\n\t\toutContent = new ByteArrayOutputStream();\n\t\terrContent = new ByteArrayOutputStream();\n\t\tSystem.setOut(new PrintStream(outContent));\n\t\tSystem.setErr(new PrintStream(errContent));\n\n\t\tfinal int startTimeoutSeconds = 60;\n\n\t\tRunner runner = new Runner(args, type, 0);\n\t\trunner.setName(\"Frontend (CLI/YARN Client) runner thread (startWithArgs()).\");\n\t\trunner.start();\n\n\t\tfor (int second = 0; second <  startTimeoutSeconds; second++) {\n\t\t\tsleep(1000);\n\t\t\t// check output for correct TaskManager startup.\n\t\t\tif (outContent.toString().contains(startedAfterString)\n\t\t\t\t\t|| errContent.toString().contains(startedAfterString)) {\n\t\t\t\tLOG.info(\"Found expected output in redirected streams\");\n\t\t\t\treturn runner;\n\t\t\t}\n\t\t\t// check if thread died\n\t\t\tif (!runner.isAlive()) {\n\t\t\t\tsendOutput();\n\t\t\t\tif (runner.getRunnerError() != null) {\n\t\t\t\t\tthrow new RuntimeException(\"Runner failed with exception.\", runner.getRunnerError());\n\t\t\t\t}\n\t\t\t\tAssert.fail(\"Runner thread died before the test was finished.\");\n\t\t\t}\n\t\t}\n\n\t\tsendOutput();\n\t\tAssert.fail(\"During the timeout period of \" + startTimeoutSeconds + \" seconds the \" +\n\t\t\t\t\"expected string did not show up\");\n\t\treturn null;\n\t}"
        ],
        [
            "UtilsTest::testHeapCutoff()",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63 -\n  64 -\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85 -\n  86  ",
            "\t/**\n\t * Remove 15% of the heap, at least 384MB.\n\t *\n\t */\n\t@Test\n\tpublic void testHeapCutoff() {\n\t\tConfiguration conf = new Configuration();\n\t\tconf.setDouble(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO, 0.15);\n\t\tconf.setInteger(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN, 384);\n\n\t\tAssert.assertEquals(616, Utils.calculateHeapSize(1000, conf) );\n\t\tAssert.assertEquals(8500, Utils.calculateHeapSize(10000, conf) );\n\n\t\t// test different configuration\n\t\tAssert.assertEquals(3400, Utils.calculateHeapSize(4000, conf));\n\n\t\tconf.setString(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN, \"1000\");\n\t\tconf.setString(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO, \"0.1\");\n\t\tAssert.assertEquals(3000, Utils.calculateHeapSize(4000, conf));\n\n\t\tconf.setString(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO, \"0.5\");\n\t\tAssert.assertEquals(2000, Utils.calculateHeapSize(4000, conf));\n\n\t\tconf.setString(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO, \"1\");\n\t\tAssert.assertEquals(0, Utils.calculateHeapSize(4000, conf));\n\n\t\t// test also deprecated keys\n\t\tconf = new Configuration();\n\t\tconf.setDouble(ConfigConstants.YARN_HEAP_CUTOFF_RATIO, 0.15);\n\t\tconf.setInteger(ConfigConstants.YARN_HEAP_CUTOFF_MIN, 384);\n\n\t\tAssert.assertEquals(616, Utils.calculateHeapSize(1000, conf) );\n\t\tAssert.assertEquals(8500, Utils.calculateHeapSize(10000, conf) );\n\t}",
            "  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68 +\n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91  ",
            "\t/**\n\t * Remove 15% of the heap, at least 384MB.\n\t *\n\t */\n\t@Test\n\tpublic void testHeapCutoff() {\n\t\tConfiguration conf = new Configuration();\n\t\tconf.setDouble(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO, 0.15);\n\t\tconf.setInteger(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN, 384);\n\n\t\tAssert.assertEquals(616, Utils.calculateHeapSize(1000, conf));\n\t\tAssert.assertEquals(8500, Utils.calculateHeapSize(10000, conf));\n\n\t\t// test different configuration\n\t\tAssert.assertEquals(3400, Utils.calculateHeapSize(4000, conf));\n\n\t\tconf.setString(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN, \"1000\");\n\t\tconf.setString(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO, \"0.1\");\n\t\tAssert.assertEquals(3000, Utils.calculateHeapSize(4000, conf));\n\n\t\tconf.setString(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO, \"0.5\");\n\t\tAssert.assertEquals(2000, Utils.calculateHeapSize(4000, conf));\n\n\t\tconf.setString(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO, \"1\");\n\t\tAssert.assertEquals(0, Utils.calculateHeapSize(4000, conf));\n\n\t\t// test also deprecated keys\n\t\tconf = new Configuration();\n\t\tconf.setDouble(ConfigConstants.YARN_HEAP_CUTOFF_RATIO, 0.15);\n\t\tconf.setInteger(ConfigConstants.YARN_HEAP_CUTOFF_MIN, 384);\n\n\t\tAssert.assertEquals(616, Utils.calculateHeapSize(1000, conf));\n\t\tAssert.assertEquals(8500, Utils.calculateHeapSize(10000, conf));\n\t}"
        ],
        [
            "YARNHighAvailabilityITCase::setup()",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 -\n  78 -\n  79  \n  80 -\n  81  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\tactorSystem = AkkaUtils.createDefaultActorSystem();\n\n\t\ttry {\n\t\t\tzkServer = new TestingServer();\n\t\t\tzkServer.start();\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Could not start ZooKeeper testing cluster.\");\n\t\t}\n\n\t\tyarnConfiguration.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, \"flink-yarn-tests-ha\");\n\t\tyarnConfiguration.set(YarnConfiguration.RM_AM_MAX_ATTEMPTS, \"\" + numberApplicationAttempts);\n\n\t\tstartYARNWithConfig(yarnConfiguration);\n\t}",
            "  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83 +\n  84  \n  85 +\n  86  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\tactorSystem = AkkaUtils.createDefaultActorSystem();\n\n\t\ttry {\n\t\t\tzkServer = new TestingServer();\n\t\t\tzkServer.start();\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Could not start ZooKeeper testing cluster.\");\n\t\t}\n\n\t\tYARN_CONFIGURATION.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, \"flink-yarn-tests-ha\");\n\t\tYARN_CONFIGURATION.set(YarnConfiguration.RM_AM_MAX_ATTEMPTS, \"\" + numberApplicationAttempts);\n\n\t\tstartYARNWithConfig(YARN_CONFIGURATION);\n\t}"
        ],
        [
            "YARNSessionCapacitySchedulerITCase::testTaskManagerFailure()",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 -\n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 -\n 161  \n 162  \n 163 -\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208 -\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 -\n 226  \n 227  \n 228 -\n 229  \n 230 -\n 231  \n 232  \n 233 -\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255 -\n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 -\n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285 -\n 286 -\n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "\t/**\n\t * Test TaskManager failure and also if the vcores are set correctly (see issue FLINK-2213).\n\t */\n\t@Test(timeout=100000) // timeout after 100 seconds\n\tpublic void testTaskManagerFailure() {\n\t\tLOG.info(\"Starting testTaskManagerFailure()\");\n\t\tRunner runner = startWithArgs(new String[]{\"-j\", flinkUberjar.getAbsolutePath(), \"-t\", flinkLibFolder.getAbsolutePath(),\n\t\t\t\t\"-n\", \"1\",\n\t\t\t\t\"-jm\", \"768\",\n\t\t\t\t\"-tm\", \"1024\",\n\t\t\t\t\"-s\", \"3\", // set the slots 3 to check if the vCores are set properly!\n\t\t\t\t\"-nm\", \"customName\",\n\t\t\t\t\"-Dfancy-configuration-value=veryFancy\",\n\t\t\t\t\"-Dyarn.maximum-failed-containers=3\",\n\t\t\t\t\"-D\" + ConfigConstants.YARN_VCORES + \"=2\"},\n\t\t\t\"Number of connected TaskManagers changed to 1. Slots available: 3\",\n\t\t\tRunTypes.YARN_SESSION);\n\n\t\tAssert.assertEquals(2, getRunningContainers());\n\n\t\t// ------------------------ Test if JobManager web interface is accessible -------\n\n\t\tYarnClient yc = null;\n\t\ttry {\n\t\t\tyc = YarnClient.createYarnClient();\n\t\t\tyc.init(yarnConfiguration);\n\t\t\tyc.start();\n\n\t\t\tList<ApplicationReport> apps = yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING));\n\t\t\tAssert.assertEquals(1, apps.size()); // Only one running\n\t\t\tApplicationReport app = apps.get(0);\n\t\t\tAssert.assertEquals(\"customName\", app.getName());\n\t\t\tString url = app.getTrackingUrl();\n\t\t\tif(!url.endsWith(\"/\")) {\n\t\t\t\turl += \"/\";\n\t\t\t}\n\t\t\tif(!url.startsWith(\"http://\")) {\n\t\t\t\turl = \"http://\" + url;\n\t\t\t}\n\t\t\tLOG.info(\"Got application URL from YARN {}\", url);\n\n\t\t\tString response = TestBaseUtils.getFromHTTP(url + \"taskmanagers/\");\n\n\t\t\tJsonNode parsedTMs = new ObjectMapper().readTree(response);\n\t\t\tArrayNode taskManagers = (ArrayNode) parsedTMs.get(\"taskmanagers\");\n\t\t\tAssert.assertNotNull(taskManagers);\n\t\t\tAssert.assertEquals(1, taskManagers.size());\n\t\t\tAssert.assertEquals(3, taskManagers.get(0).get(\"slotsNumber\").asInt());\n\n\t\t\t// get the configuration from webinterface & check if the dynamic properties from YARN show up there.\n\t\t\tString jsonConfig = TestBaseUtils.getFromHTTP(url + \"jobmanager/config\");\n\t\t\tMap<String, String> parsedConfig = WebMonitorUtils.fromKeyValueJsonArray(jsonConfig);\n\n\t\t\tAssert.assertEquals(\"veryFancy\", parsedConfig.get(\"fancy-configuration-value\"));\n\t\t\tAssert.assertEquals(\"3\", parsedConfig.get(\"yarn.maximum-failed-containers\"));\n\t\t\tAssert.assertEquals(\"2\", parsedConfig.get(ConfigConstants.YARN_VCORES));\n\n\t\t\t// -------------- FLINK-1902: check if jobmanager hostname/port are shown in web interface\n\t\t\t// first, get the hostname/port\n\t\t\tString oC = outContent.toString();\n\t\t\tPattern p = Pattern.compile(\"Flink JobManager is now running on ([a-zA-Z0-9.-]+):([0-9]+)\");\n\t\t\tMatcher matches = p.matcher(oC);\n\t\t\tString hostname = null;\n\t\t\tString port = null;\n\t\t\twhile(matches.find()) {\n\t\t\t\thostname = matches.group(1).toLowerCase();\n\t\t\t\tport = matches.group(2);\n\t\t\t}\n\t\t\tLOG.info(\"Extracted hostname:port: {} {}\", hostname, port);\n\n\t\t\tAssert.assertEquals(\"unable to find hostname in \" + jsonConfig, hostname,\n\t\t\t\tparsedConfig.get(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY));\n\t\t\tAssert.assertEquals(\"unable to find port in \" + jsonConfig, port,\n\t\t\t\tparsedConfig.get(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY));\n\n\t\t\t// test logfile access\n\t\t\tString logs = TestBaseUtils.getFromHTTP(url + \"jobmanager/log\");\n\t\t\tAssert.assertTrue(logs.contains(\"Starting YARN ApplicationMaster\"));\n\t\t\tAssert.assertTrue(logs.contains(\"Starting JobManager\"));\n\t\t\tAssert.assertTrue(logs.contains(\"Starting JobManager Web Frontend\"));\n\t\t} catch(Throwable e) {\n\t\t\tLOG.warn(\"Error while running test\",e);\n\t\t\tAssert.fail(e.getMessage());\n\t\t}\n\n\t\t// ------------------------ Kill container with TaskManager and check if vcores are set correctly -------\n\n\t\t// find container id of taskManager:\n\t\tContainerId taskManagerContainer = null;\n\t\tNodeManager nodeManager = null;\n\t\tUserGroupInformation remoteUgi = null;\n\t\tNMTokenIdentifier nmIdent = null;\n\t\ttry {\n\t\t\tremoteUgi = UserGroupInformation.getCurrentUser();\n\t\t} catch (IOException e) {\n\t\t\tLOG.warn(\"Unable to get curr user\", e);\n\t\t\tAssert.fail();\n\t\t}\n\t\tfor(int nmId = 0; nmId < NUM_NODEMANAGERS; nmId++) {\n\t\t\tNodeManager nm = yarnCluster.getNodeManager(nmId);\n\t\t\tConcurrentMap<ContainerId, Container> containers = nm.getNMContext().getContainers();\n\t\t\tfor(Map.Entry<ContainerId, Container> entry : containers.entrySet()) {\n\t\t\t\tString command = Joiner.on(\" \").join(entry.getValue().getLaunchContext().getCommands());\n\t\t\t\tif(command.contains(YarnTaskManager.class.getSimpleName())) {\n\t\t\t\t\ttaskManagerContainer = entry.getKey();\n\t\t\t\t\tnodeManager = nm;\n\t\t\t\t\tnmIdent = new NMTokenIdentifier(taskManagerContainer.getApplicationAttemptId(), null, \"\",0);\n\t\t\t\t\t// allow myself to do stuff with the container\n\t\t\t\t\t// remoteUgi.addCredentials(entry.getValue().getCredentials());\n\t\t\t\t\tremoteUgi.addTokenIdentifier(nmIdent);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsleep(500);\n\t\t}\n\n\t\tAssert.assertNotNull(\"Unable to find container with TaskManager\", taskManagerContainer);\n\t\tAssert.assertNotNull(\"Illegal state\", nodeManager);\n\n\t\tyc.stop();\n\n\t\tList<ContainerId> toStop = new LinkedList<ContainerId>();\n\t\ttoStop.add(taskManagerContainer);\n\t\tStopContainersRequest scr = StopContainersRequest.newInstance(toStop);\n\n\t\ttry {\n\t\t\tnodeManager.getNMContext().getContainerManager().stopContainers(scr);\n\t\t} catch (Throwable e) {\n\t\t\tLOG.warn(\"Error stopping container\", e);\n\t\t\tAssert.fail(\"Error stopping container: \"+e.getMessage());\n\t\t}\n\n\t\t// stateful termination check:\n\t\t// wait until we saw a container being killed and AFTERWARDS a new one launched\n\t\tboolean ok = false;\n\t\tdo {\n\t\t\tLOG.debug(\"Waiting for correct order of events. Output: {}\", errContent.toString());\n\n\t\t\tString o = errContent.toString();\n\t\t\tint killedOff = o.indexOf(\"Container killed by the ApplicationMaster\");\n\t\t\tif (killedOff != -1) {\n\t\t\t\to = o.substring(killedOff);\n\t\t\t\tok = o.indexOf(\"Launching TaskManager\") > 0;\n\t\t\t}\n\t\t\tsleep(1000);\n\t\t} while(!ok);\n\n\n\t\t// send \"stop\" command to command line interface\n\t\trunner.sendStop();\n\t\t// wait for the thread to stop\n\t\ttry {\n\t\t\trunner.join(1000);\n\t\t} catch (InterruptedException e) {\n\t\t\tLOG.warn(\"Interrupted while stopping runner\", e);\n\t\t}\n\t\tLOG.warn(\"stopped\");\n\n\t\t// ----------- Send output to logger\n\t\tSystem.setOut(originalStdout);\n\t\tSystem.setErr(originalStderr);\n\t\tString oC = outContent.toString();\n\t\tString eC = errContent.toString();\n\t\tLOG.info(\"Sending stdout content through logger: \\n\\n{}\\n\\n\", oC);\n\t\tLOG.info(\"Sending stderr content through logger: \\n\\n{}\\n\\n\", eC);\n\n\t\t// ------ Check if everything happened correctly\n\t\tAssert.assertTrue(\"Expect to see failed container\",\n\t\t\teC.contains(\"New messages from the YARN cluster\"));\n\n\t\tAssert.assertTrue(\"Expect to see failed container\",\n\t\t\teC.contains(\"Container killed by the ApplicationMaster\"));\n\n\t\tAssert.assertTrue(\"Expect to see new container started\",\n\t\t\teC.contains(\"Launching TaskManager\") && eC.contains(\"on host\"));\n\n\t\t// cleanup auth for the subsequent tests.\n\t\tremoteUgi.getTokenIdentifiers().remove(nmIdent);\n\n\t\tLOG.info(\"Finished testTaskManagerFailure()\");\n\t}",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159 +\n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170 +\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214 +\n 215 +\n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232 +\n 233  \n 234  \n 235 +\n 236  \n 237 +\n 238  \n 239  \n 240 +\n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262 +\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291 +\n 292 +\n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  ",
            "\t/**\n\t * Test TaskManager failure and also if the vcores are set correctly (see issue FLINK-2213).\n\t */\n\t@Test(timeout = 100000) // timeout after 100 seconds\n\tpublic void testTaskManagerFailure() {\n\t\tLOG.info(\"Starting testTaskManagerFailure()\");\n\t\tRunner runner = startWithArgs(new String[]{\"-j\", flinkUberjar.getAbsolutePath(), \"-t\", flinkLibFolder.getAbsolutePath(),\n\t\t\t\t\"-n\", \"1\",\n\t\t\t\t\"-jm\", \"768\",\n\t\t\t\t\"-tm\", \"1024\",\n\t\t\t\t\"-s\", \"3\", // set the slots 3 to check if the vCores are set properly!\n\t\t\t\t\"-nm\", \"customName\",\n\t\t\t\t\"-Dfancy-configuration-value=veryFancy\",\n\t\t\t\t\"-Dyarn.maximum-failed-containers=3\",\n\t\t\t\t\"-D\" + ConfigConstants.YARN_VCORES + \"=2\"},\n\t\t\t\"Number of connected TaskManagers changed to 1. Slots available: 3\",\n\t\t\tRunTypes.YARN_SESSION);\n\n\t\tAssert.assertEquals(2, getRunningContainers());\n\n\t\t// ------------------------ Test if JobManager web interface is accessible -------\n\n\t\tYarnClient yc = null;\n\t\ttry {\n\t\t\tyc = YarnClient.createYarnClient();\n\t\t\tyc.init(YARN_CONFIGURATION);\n\t\t\tyc.start();\n\n\t\t\tList<ApplicationReport> apps = yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING));\n\t\t\tAssert.assertEquals(1, apps.size()); // Only one running\n\t\t\tApplicationReport app = apps.get(0);\n\t\t\tAssert.assertEquals(\"customName\", app.getName());\n\t\t\tString url = app.getTrackingUrl();\n\t\t\tif (!url.endsWith(\"/\")) {\n\t\t\t\turl += \"/\";\n\t\t\t}\n\t\t\tif (!url.startsWith(\"http://\")) {\n\t\t\t\turl = \"http://\" + url;\n\t\t\t}\n\t\t\tLOG.info(\"Got application URL from YARN {}\", url);\n\n\t\t\tString response = TestBaseUtils.getFromHTTP(url + \"taskmanagers/\");\n\n\t\t\tJsonNode parsedTMs = new ObjectMapper().readTree(response);\n\t\t\tArrayNode taskManagers = (ArrayNode) parsedTMs.get(\"taskmanagers\");\n\t\t\tAssert.assertNotNull(taskManagers);\n\t\t\tAssert.assertEquals(1, taskManagers.size());\n\t\t\tAssert.assertEquals(3, taskManagers.get(0).get(\"slotsNumber\").asInt());\n\n\t\t\t// get the configuration from webinterface & check if the dynamic properties from YARN show up there.\n\t\t\tString jsonConfig = TestBaseUtils.getFromHTTP(url + \"jobmanager/config\");\n\t\t\tMap<String, String> parsedConfig = WebMonitorUtils.fromKeyValueJsonArray(jsonConfig);\n\n\t\t\tAssert.assertEquals(\"veryFancy\", parsedConfig.get(\"fancy-configuration-value\"));\n\t\t\tAssert.assertEquals(\"3\", parsedConfig.get(\"yarn.maximum-failed-containers\"));\n\t\t\tAssert.assertEquals(\"2\", parsedConfig.get(ConfigConstants.YARN_VCORES));\n\n\t\t\t// -------------- FLINK-1902: check if jobmanager hostname/port are shown in web interface\n\t\t\t// first, get the hostname/port\n\t\t\tString oC = outContent.toString();\n\t\t\tPattern p = Pattern.compile(\"Flink JobManager is now running on ([a-zA-Z0-9.-]+):([0-9]+)\");\n\t\t\tMatcher matches = p.matcher(oC);\n\t\t\tString hostname = null;\n\t\t\tString port = null;\n\t\t\twhile (matches.find()) {\n\t\t\t\thostname = matches.group(1).toLowerCase();\n\t\t\t\tport = matches.group(2);\n\t\t\t}\n\t\t\tLOG.info(\"Extracted hostname:port: {} {}\", hostname, port);\n\n\t\t\tAssert.assertEquals(\"unable to find hostname in \" + jsonConfig, hostname,\n\t\t\t\tparsedConfig.get(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY));\n\t\t\tAssert.assertEquals(\"unable to find port in \" + jsonConfig, port,\n\t\t\t\tparsedConfig.get(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY));\n\n\t\t\t// test logfile access\n\t\t\tString logs = TestBaseUtils.getFromHTTP(url + \"jobmanager/log\");\n\t\t\tAssert.assertTrue(logs.contains(\"Starting YARN ApplicationMaster\"));\n\t\t\tAssert.assertTrue(logs.contains(\"Starting JobManager\"));\n\t\t\tAssert.assertTrue(logs.contains(\"Starting JobManager Web Frontend\"));\n\t\t} catch (Throwable e) {\n\t\t\tLOG.warn(\"Error while running test\", e);\n\t\t\tAssert.fail(e.getMessage());\n\t\t}\n\n\t\t// ------------------------ Kill container with TaskManager and check if vcores are set correctly -------\n\n\t\t// find container id of taskManager:\n\t\tContainerId taskManagerContainer = null;\n\t\tNodeManager nodeManager = null;\n\t\tUserGroupInformation remoteUgi = null;\n\t\tNMTokenIdentifier nmIdent = null;\n\t\ttry {\n\t\t\tremoteUgi = UserGroupInformation.getCurrentUser();\n\t\t} catch (IOException e) {\n\t\t\tLOG.warn(\"Unable to get curr user\", e);\n\t\t\tAssert.fail();\n\t\t}\n\t\tfor (int nmId = 0; nmId < NUM_NODEMANAGERS; nmId++) {\n\t\t\tNodeManager nm = yarnCluster.getNodeManager(nmId);\n\t\t\tConcurrentMap<ContainerId, Container> containers = nm.getNMContext().getContainers();\n\t\t\tfor (Map.Entry<ContainerId, Container> entry : containers.entrySet()) {\n\t\t\t\tString command = Joiner.on(\" \").join(entry.getValue().getLaunchContext().getCommands());\n\t\t\t\tif (command.contains(YarnTaskManager.class.getSimpleName())) {\n\t\t\t\t\ttaskManagerContainer = entry.getKey();\n\t\t\t\t\tnodeManager = nm;\n\t\t\t\t\tnmIdent = new NMTokenIdentifier(taskManagerContainer.getApplicationAttemptId(), null, \"\", 0);\n\t\t\t\t\t// allow myself to do stuff with the container\n\t\t\t\t\t// remoteUgi.addCredentials(entry.getValue().getCredentials());\n\t\t\t\t\tremoteUgi.addTokenIdentifier(nmIdent);\n\t\t\t\t}\n\t\t\t}\n\t\t\tsleep(500);\n\t\t}\n\n\t\tAssert.assertNotNull(\"Unable to find container with TaskManager\", taskManagerContainer);\n\t\tAssert.assertNotNull(\"Illegal state\", nodeManager);\n\n\t\tyc.stop();\n\n\t\tList<ContainerId> toStop = new LinkedList<ContainerId>();\n\t\ttoStop.add(taskManagerContainer);\n\t\tStopContainersRequest scr = StopContainersRequest.newInstance(toStop);\n\n\t\ttry {\n\t\t\tnodeManager.getNMContext().getContainerManager().stopContainers(scr);\n\t\t} catch (Throwable e) {\n\t\t\tLOG.warn(\"Error stopping container\", e);\n\t\t\tAssert.fail(\"Error stopping container: \" + e.getMessage());\n\t\t}\n\n\t\t// stateful termination check:\n\t\t// wait until we saw a container being killed and AFTERWARDS a new one launched\n\t\tboolean ok = false;\n\t\tdo {\n\t\t\tLOG.debug(\"Waiting for correct order of events. Output: {}\", errContent.toString());\n\n\t\t\tString o = errContent.toString();\n\t\t\tint killedOff = o.indexOf(\"Container killed by the ApplicationMaster\");\n\t\t\tif (killedOff != -1) {\n\t\t\t\to = o.substring(killedOff);\n\t\t\t\tok = o.indexOf(\"Launching TaskManager\") > 0;\n\t\t\t}\n\t\t\tsleep(1000);\n\t\t} while(!ok);\n\n\t\t// send \"stop\" command to command line interface\n\t\trunner.sendStop();\n\t\t// wait for the thread to stop\n\t\ttry {\n\t\t\trunner.join(1000);\n\t\t} catch (InterruptedException e) {\n\t\t\tLOG.warn(\"Interrupted while stopping runner\", e);\n\t\t}\n\t\tLOG.warn(\"stopped\");\n\n\t\t// ----------- Send output to logger\n\t\tSystem.setOut(ORIGINAL_STDOUT);\n\t\tSystem.setErr(ORIGINAL_STDERR);\n\t\tString oC = outContent.toString();\n\t\tString eC = errContent.toString();\n\t\tLOG.info(\"Sending stdout content through logger: \\n\\n{}\\n\\n\", oC);\n\t\tLOG.info(\"Sending stderr content through logger: \\n\\n{}\\n\\n\", eC);\n\n\t\t// ------ Check if everything happened correctly\n\t\tAssert.assertTrue(\"Expect to see failed container\",\n\t\t\teC.contains(\"New messages from the YARN cluster\"));\n\n\t\tAssert.assertTrue(\"Expect to see failed container\",\n\t\t\teC.contains(\"Container killed by the ApplicationMaster\"));\n\n\t\tAssert.assertTrue(\"Expect to see new container started\",\n\t\t\teC.contains(\"Launching TaskManager\") && eC.contains(\"on host\"));\n\n\t\t// cleanup auth for the subsequent tests.\n\t\tremoteUgi.getTokenIdentifiers().remove(nmIdent);\n\n\t\tLOG.info(\"Finished testTaskManagerFailure()\");\n\t}"
        ],
        [
            "YARNSessionFIFOITCase::testDetachedMode()",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103 -\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114 -\n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 -\n 125  \n 126  \n 127 -\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143 -\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "\t/**\n\t * Test regular operation, including command line parameter parsing.\n\t */\n\t@Test(timeout=60000) // timeout after a minute.\n\tpublic void testDetachedMode() throws InterruptedException {\n\t\tLOG.info(\"Starting testDetachedMode()\");\n\t\taddTestAppender(FlinkYarnSessionCli.class, Level.INFO);\n\t\tRunner runner =\n\t\t\tstartWithArgs(new String[]{\"-j\", flinkUberjar.getAbsolutePath(),\n\t\t\t\t\t\t\"-t\", flinkLibFolder.getAbsolutePath(),\n\t\t\t\t\t\t\"-n\", \"1\",\n\t\t\t\t\t\t\"-jm\", \"768\",\n\t\t\t\t\t\t\"-tm\", \"1024\",\n\t\t\t\t\t\t\"--name\", \"MyCustomName\", // test setting a custom name\n\t\t\t\t\t\t\"--detached\"},\n\t\t\t\t\"Flink JobManager is now running on\", RunTypes.YARN_SESSION);\n\n\t\t// before checking any strings outputted by the CLI, first give it time to return\n\t\trunner.join();\n\t\tcheckForLogString(\"The Flink YARN client has been started in detached mode\");\n\n\t\tLOG.info(\"Waiting until two containers are running\");\n\t\t// wait until two containers are running\n\t\twhile(getRunningContainers() < 2) {\n\t\t\tsleep(500);\n\t\t}\n\n\t\t//additional sleep for the JM/TM to start and establish connection\n\t\tsleep(2000);\n\t\tLOG.info(\"Two containers are running. Killing the application\");\n\n\t\t// kill application \"externally\".\n\t\ttry {\n\t\t\tYarnClient yc = YarnClient.createYarnClient();\n\t\t\tyc.init(yarnConfiguration);\n\t\t\tyc.start();\n\t\t\tList<ApplicationReport> apps = yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING));\n\t\t\tAssert.assertEquals(1, apps.size()); // Only one running\n\t\t\tApplicationReport app = apps.get(0);\n\n\t\t\tAssert.assertEquals(\"MyCustomName\", app.getName());\n\t\t\tApplicationId id = app.getApplicationId();\n\t\t\tyc.killApplication(id);\n\n\t\t\twhile(yc.getApplications(EnumSet.of(YarnApplicationState.KILLED)).size() == 0) {\n\t\t\t\tsleep(500);\n\t\t\t}\n\t\t} catch(Throwable t) {\n\t\t\tLOG.warn(\"Killing failed\", t);\n\t\t\tAssert.fail();\n\t\t} finally {\n\n\t\t\t//cleanup the yarn-properties file\n\t\t\tString confDirPath = System.getenv(\"FLINK_CONF_DIR\");\n\t\t\tFile configDirectory = new File(confDirPath);\n\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Using configuration directory \" + configDirectory.getAbsolutePath());\n\n\t\t\t// load the configuration\n\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Trying to load configuration file\");\n\t\t\tGlobalConfiguration.loadConfiguration(configDirectory.getAbsolutePath());\n\n\t\t\ttry {\n\t\t\t\tFile yarnPropertiesFile = FlinkYarnSessionCli.getYarnPropertiesLocation(GlobalConfiguration.loadConfiguration());\n\t\t\t\tif(yarnPropertiesFile.exists()) {\n\t\t\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Cleaning up temporary Yarn address reference: {}\", yarnPropertiesFile.getAbsolutePath());\n\t\t\t\t\tyarnPropertiesFile.delete();\n\t\t\t\t}\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"testDetachedPerJobYarnClusterInternal: Exception while deleting the JobManager address file\", e);\n\t\t\t}\n\n\t\t}\n\n\t\tLOG.info(\"Finished testDetachedMode()\");\n\t}",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 +\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 +\n 121  \n 122  \n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "\t/**\n\t * Test regular operation, including command line parameter parsing.\n\t */\n\t@Test(timeout = 60000) // timeout after a minute.\n\tpublic void testDetachedMode() throws InterruptedException {\n\t\tLOG.info(\"Starting testDetachedMode()\");\n\t\taddTestAppender(FlinkYarnSessionCli.class, Level.INFO);\n\t\tRunner runner =\n\t\t\tstartWithArgs(new String[]{\"-j\", flinkUberjar.getAbsolutePath(),\n\t\t\t\t\t\t\"-t\", flinkLibFolder.getAbsolutePath(),\n\t\t\t\t\t\t\"-n\", \"1\",\n\t\t\t\t\t\t\"-jm\", \"768\",\n\t\t\t\t\t\t\"-tm\", \"1024\",\n\t\t\t\t\t\t\"--name\", \"MyCustomName\", // test setting a custom name\n\t\t\t\t\t\t\"--detached\"},\n\t\t\t\t\"Flink JobManager is now running on\", RunTypes.YARN_SESSION);\n\n\t\t// before checking any strings outputted by the CLI, first give it time to return\n\t\trunner.join();\n\t\tcheckForLogString(\"The Flink YARN client has been started in detached mode\");\n\n\t\tLOG.info(\"Waiting until two containers are running\");\n\t\t// wait until two containers are running\n\t\twhile (getRunningContainers() < 2) {\n\t\t\tsleep(500);\n\t\t}\n\n\t\t//additional sleep for the JM/TM to start and establish connection\n\t\tsleep(2000);\n\t\tLOG.info(\"Two containers are running. Killing the application\");\n\n\t\t// kill application \"externally\".\n\t\ttry {\n\t\t\tYarnClient yc = YarnClient.createYarnClient();\n\t\t\tyc.init(YARN_CONFIGURATION);\n\t\t\tyc.start();\n\t\t\tList<ApplicationReport> apps = yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING));\n\t\t\tAssert.assertEquals(1, apps.size()); // Only one running\n\t\t\tApplicationReport app = apps.get(0);\n\n\t\t\tAssert.assertEquals(\"MyCustomName\", app.getName());\n\t\t\tApplicationId id = app.getApplicationId();\n\t\t\tyc.killApplication(id);\n\n\t\t\twhile (yc.getApplications(EnumSet.of(YarnApplicationState.KILLED)).size() == 0) {\n\t\t\t\tsleep(500);\n\t\t\t}\n\t\t} catch (Throwable t) {\n\t\t\tLOG.warn(\"Killing failed\", t);\n\t\t\tAssert.fail();\n\t\t} finally {\n\n\t\t\t//cleanup the yarn-properties file\n\t\t\tString confDirPath = System.getenv(\"FLINK_CONF_DIR\");\n\t\t\tFile configDirectory = new File(confDirPath);\n\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Using configuration directory \" + configDirectory.getAbsolutePath());\n\n\t\t\t// load the configuration\n\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Trying to load configuration file\");\n\t\t\tGlobalConfiguration.loadConfiguration(configDirectory.getAbsolutePath());\n\n\t\t\ttry {\n\t\t\t\tFile yarnPropertiesFile = FlinkYarnSessionCli.getYarnPropertiesLocation(GlobalConfiguration.loadConfiguration());\n\t\t\t\tif (yarnPropertiesFile.exists()) {\n\t\t\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Cleaning up temporary Yarn address reference: {}\", yarnPropertiesFile.getAbsolutePath());\n\t\t\t\t\tyarnPropertiesFile.delete();\n\t\t\t\t}\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"testDetachedPerJobYarnClusterInternal: Exception while deleting the JobManager address file\", e);\n\t\t\t}\n\n\t\t}\n\n\t\tLOG.info(\"Finished testDetachedMode()\");\n\t}"
        ],
        [
            "YarnTestBase::startYARNWithConfig(Configuration)",
            " 372  \n 373 -\n 374  ",
            "\tpublic static void startYARNWithConfig(Configuration conf) {\n\t\tstart(conf,null,null);\n\t}",
            " 376  \n 377 +\n 378  ",
            "\tpublic static void startYARNWithConfig(Configuration conf) {\n\t\tstart(conf, null, null);\n\t}"
        ],
        [
            "CliFrontendYarnAddressConfigurationTest::writeYarnPropertiesFile(String)",
            " 305  \n 306  \n 307  \n 308  \n 309  \n 310 -\n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  ",
            "\tprivate File writeYarnPropertiesFile(String contents) throws IOException {\n\t\tFile tmpFolder = temporaryFolder.newFolder();\n\t\tString currentUser = System.getProperty(\"user.name\");\n\n\t\t// copy .yarn-properties-<username>\n\t\tFile testPropertiesFile = new File(tmpFolder, \".yarn-properties-\"+currentUser);\n\t\tFiles.write(testPropertiesFile.toPath(), contents.getBytes(), StandardOpenOption.CREATE);\n\n\t\t// copy reference flink-conf.yaml to temporary test directory and append custom configuration path.\n\t\tString confFile = flinkConf + \"\\nyarn.properties-file.location: \" + tmpFolder;\n\t\tFile testConfFile = new File(tmpFolder.getAbsolutePath(), \"flink-conf.yaml\");\n\t\tFiles.write(testConfFile.toPath(), confFile.getBytes(), StandardOpenOption.CREATE);\n\n\t\treturn tmpFolder.getAbsoluteFile();\n\t}",
            " 297  \n 298  \n 299  \n 300  \n 301  \n 302 +\n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "\tprivate File writeYarnPropertiesFile(String contents) throws IOException {\n\t\tFile tmpFolder = temporaryFolder.newFolder();\n\t\tString currentUser = System.getProperty(\"user.name\");\n\n\t\t// copy .yarn-properties-<username>\n\t\tFile testPropertiesFile = new File(tmpFolder, \".yarn-properties-\" + currentUser);\n\t\tFiles.write(testPropertiesFile.toPath(), contents.getBytes(), StandardOpenOption.CREATE);\n\n\t\t// copy reference flink-conf.yaml to temporary test directory and append custom configuration path.\n\t\tString confFile = flinkConf + \"\\nyarn.properties-file.location: \" + tmpFolder;\n\t\tFile testConfFile = new File(tmpFolder.getAbsolutePath(), \"flink-conf.yaml\");\n\t\tFiles.write(testConfFile.toPath(), confFile.getBytes(), StandardOpenOption.CREATE);\n\n\t\treturn tmpFolder.getAbsoluteFile();\n\t}"
        ],
        [
            "YarnTestBase::getRunningContainers()",
            " 358  \n 359  \n 360 -\n 361  \n 362  \n 363  \n 364  \n 365  \n 366  ",
            "\tpublic static int getRunningContainers() {\n\t\tint count = 0;\n\t\tfor(int nmId = 0; nmId < NUM_NODEMANAGERS; nmId++) {\n\t\t\tNodeManager nm = yarnCluster.getNodeManager(nmId);\n\t\t\tConcurrentMap<ContainerId, Container> containers = nm.getNMContext().getContainers();\n\t\t\tcount += containers.size();\n\t\t}\n\t\treturn count;\n\t}",
            " 362  \n 363  \n 364 +\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  ",
            "\tpublic static int getRunningContainers() {\n\t\tint count = 0;\n\t\tfor (int nmId = 0; nmId < NUM_NODEMANAGERS; nmId++) {\n\t\t\tNodeManager nm = yarnCluster.getNodeManager(nmId);\n\t\t\tConcurrentMap<ContainerId, Container> containers = nm.getNMContext().getContainers();\n\t\t\tcount += containers.size();\n\t\t}\n\t\treturn count;\n\t}"
        ],
        [
            "YarnTestBase::setup()",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471 -\n 472  ",
            "\t/**\n\t * Default @BeforeClass impl. Overwrite this for passing a different configuration\n\t */\n\t@BeforeClass\n\tpublic static void setup() {\n\t\tstartYARNWithConfig(yarnConfiguration);\n\t}",
            " 470  \n 471  \n 472  \n 473  \n 474  \n 475 +\n 476  ",
            "\t/**\n\t * Default @BeforeClass impl. Overwrite this for passing a different configuration\n\t */\n\t@BeforeClass\n\tpublic static void setup() {\n\t\tstartYARNWithConfig(YARN_CONFIGURATION);\n\t}"
        ],
        [
            "UtilsTest::checkForLogString(String)",
            " 146  \n 147  \n 148 -\n 149 -\n 150  \n 151  \n 152  \n 153  ",
            "\tpublic static void checkForLogString(String expected) {\n\t\tLoggingEvent found = getEventContainingString(expected);\n\t\tif(found != null) {\n\t\t\tLOG.info(\"Found expected string '\"+expected+\"' in log message \"+found);\n\t\t\treturn;\n\t\t}\n\t\tAssert.fail(\"Unable to find expected string '\" + expected + \"' in log messages\");\n\t}",
            " 151  \n 152  \n 153 +\n 154 +\n 155  \n 156  \n 157  \n 158  ",
            "\tpublic static void checkForLogString(String expected) {\n\t\tLoggingEvent found = getEventContainingString(expected);\n\t\tif (found != null) {\n\t\t\tLOG.info(\"Found expected string '\" + expected + \"' in log message \" + found);\n\t\t\treturn;\n\t\t}\n\t\tAssert.fail(\"Unable to find expected string '\" + expected + \"' in log messages\");\n\t}"
        ],
        [
            "YarnTestBase::Runner::sendStop()",
            " 685  \n 686  \n 687 -\n 688  \n 689  \n 690  ",
            "\t\t/** Stops the Yarn session */\n\t\tpublic void sendStop() {\n\t\t\tif(yCli != null) {\n\t\t\t\tyCli.stop();\n\t\t\t}\n\t\t}",
            " 691  \n 692  \n 693 +\n 694  \n 695  \n 696  ",
            "\t\t/** Stops the Yarn session. */\n\t\tpublic void sendStop() {\n\t\t\tif (yCli != null) {\n\t\t\t\tyCli.stop();\n\t\t\t}\n\t\t}"
        ],
        [
            "YarnTestBase::sleep(int)",
            " 350  \n 351  \n 352  \n 353  \n 354 -\n 355  \n 356  ",
            "\tpublic static void sleep(int time) {\n\t\ttry {\n\t\t\tThread.sleep(time);\n\t\t} catch (InterruptedException e) {\n\t\t\tLOG.warn(\"Interruped\",e);\n\t\t}\n\t}",
            " 354  \n 355  \n 356  \n 357  \n 358 +\n 359  \n 360  ",
            "\tpublic static void sleep(int time) {\n\t\ttry {\n\t\t\tThread.sleep(time);\n\t\t} catch (InterruptedException e) {\n\t\t\tLOG.warn(\"Interruped\", e);\n\t\t}\n\t}"
        ],
        [
            "UtilsTest::TestAppender::requiresLayout()",
            " 175 -",
            "\t\tpublic boolean requiresLayout() {return false;}",
            " 183 +\n 184 +\n 185 +",
            "\t\tpublic boolean requiresLayout() {\n\t\t\treturn false;\n\t\t}"
        ],
        [
            "YarnTestBase::findFile(String,FilenameFilter)",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 -\n 212  \n 213  \n 214 -\n 215  \n 216 -\n 217  \n 218 -\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "\t/**\n\t * Locate a file or directory\n\t */\n\tpublic static File findFile(String startAt, FilenameFilter fnf) {\n\t\tFile root = new File(startAt);\n\t\tString[] files = root.list();\n\t\tif(files == null) {\n\t\t\treturn null;\n\t\t}\n\t\tfor(String file : files) {\n\t\t\tFile f = new File(startAt + File.separator + file);\n\t\t\tif(f.isDirectory()) {\n\t\t\t\tFile r = findFile(f.getAbsolutePath(), fnf);\n\t\t\t\tif(r != null) {\n\t\t\t\t\treturn r;\n\t\t\t\t}\n\t\t\t} else if (fnf.accept(f.getParentFile(), f.getName())) {\n\t\t\t\treturn f;\n\t\t\t}\n\t\t}\n\t\treturn null;\n\t}",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 +\n 212  \n 213  \n 214 +\n 215  \n 216 +\n 217  \n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "\t/**\n\t * Locate a file or directory.\n\t */\n\tpublic static File findFile(String startAt, FilenameFilter fnf) {\n\t\tFile root = new File(startAt);\n\t\tString[] files = root.list();\n\t\tif (files == null) {\n\t\t\treturn null;\n\t\t}\n\t\tfor (String file : files) {\n\t\t\tFile f = new File(startAt + File.separator + file);\n\t\t\tif (f.isDirectory()) {\n\t\t\t\tFile r = findFile(f.getAbsolutePath(), fnf);\n\t\t\t\tif (r != null) {\n\t\t\t\t\treturn r;\n\t\t\t\t}\n\t\t\t} else if (fnf.accept(f.getParentFile(), f.getName())) {\n\t\t\t\treturn f;\n\t\t\t}\n\t\t}\n\t\treturn null;\n\t}"
        ],
        [
            "YarnTestBase::checkClusterEmpty()",
            " 186  \n 187  \n 188 -\n 189  \n 190 -\n 191  \n 192  \n 193  \n 194  \n 195 -\n 196 -\n 197  \n 198  \n 199  \n 200 -\n 201  \n 202  \n 203  ",
            "\t@Before\n\tpublic void checkClusterEmpty() throws IOException, YarnException {\n\t\tif(yarnClient == null) {\n\t\t\tyarnClient = YarnClient.createYarnClient();\n\t\t\tyarnClient.init(yarnConfiguration);\n\t\t\tyarnClient.start();\n\t\t}\n\n\t\tList<ApplicationReport> apps = yarnClient.getApplications();\n\t\tfor(ApplicationReport app : apps) {\n\t\t\tif(app.getYarnApplicationState() != YarnApplicationState.FINISHED\n\t\t\t\t\t&& app.getYarnApplicationState() != YarnApplicationState.KILLED\n\t\t\t\t\t&& app.getYarnApplicationState() != YarnApplicationState.FAILED) {\n\t\t\t\tAssert.fail(\"There is at least one application on the cluster is not finished.\" +\n\t\t\t\t\t\t\"App \"+app.getApplicationId()+\" is in state \"+app.getYarnApplicationState());\n\t\t\t}\n\t\t}\n\t}",
            " 186  \n 187  \n 188 +\n 189  \n 190 +\n 191  \n 192  \n 193  \n 194  \n 195 +\n 196 +\n 197  \n 198  \n 199  \n 200 +\n 201  \n 202  \n 203  ",
            "\t@Before\n\tpublic void checkClusterEmpty() throws IOException, YarnException {\n\t\tif (yarnClient == null) {\n\t\t\tyarnClient = YarnClient.createYarnClient();\n\t\t\tyarnClient.init(YARN_CONFIGURATION);\n\t\t\tyarnClient.start();\n\t\t}\n\n\t\tList<ApplicationReport> apps = yarnClient.getApplications();\n\t\tfor (ApplicationReport app : apps) {\n\t\t\tif (app.getYarnApplicationState() != YarnApplicationState.FINISHED\n\t\t\t\t\t&& app.getYarnApplicationState() != YarnApplicationState.KILLED\n\t\t\t\t\t&& app.getYarnApplicationState() != YarnApplicationState.FAILED) {\n\t\t\t\tAssert.fail(\"There is at least one application on the cluster is not finished.\" +\n\t\t\t\t\t\t\"App \" + app.getApplicationId() + \" is in state \" + app.getYarnApplicationState());\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "YARNSessionFIFOITCase::testQueryCluster()",
            " 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  ",
            "\t/**\n\t * Test querying the YARN cluster.\n\t *\n\t * This test validates through 666*2 cores in the \"cluster\".\n\t */\n\t@Test\n\tpublic void testQueryCluster() {\n\t\tLOG.info(\"Starting testQueryCluster()\");\n\t\trunWithArgs(new String[] {\"-q\"}, \"Summary: totalMemory 8192 totalCores 1332\",null, RunTypes.YARN_SESSION, 0); // we have 666*2 cores.\n\t\tLOG.info(\"Finished testQueryCluster()\");\n\t}",
            " 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161  \n 162  ",
            "\t/**\n\t * Test querying the YARN cluster.\n\t *\n\t * <p>This test validates through 666*2 cores in the \"cluster\".\n\t */\n\t@Test\n\tpublic void testQueryCluster() {\n\t\tLOG.info(\"Starting testQueryCluster()\");\n\t\trunWithArgs(new String[] {\"-q\"}, \"Summary: totalMemory 8192 totalCores 1332\", null, RunTypes.YARN_SESSION, 0); // we have 666*2 cores.\n\t\tLOG.info(\"Finished testQueryCluster()\");\n\t}"
        ],
        [
            "YarnTestBase::teardown()",
            " 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712 -\n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721 -\n 722 -\n 723 -\n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  ",
            "\t@AfterClass\n\tpublic static void teardown() throws Exception {\n\n\t\tLOG.info(\"Stopping MiniYarn Cluster\");\n\t\tyarnCluster.stop();\n\n\t\t// Unset FLINK_CONF_DIR, as it might change the behavior of other tests\n\t\tMap<String, String> map = new HashMap<>(System.getenv());\n\t\tmap.remove(ConfigConstants.ENV_FLINK_CONF_DIR);\n\t\tmap.remove(\"YARN_CONF_DIR\");\n\t\tmap.remove(\"IN_TESTS\");\n\t\tTestBaseUtils.setEnv(map);\n\n\t\tif(tempConfPathForSecureRun != null) {\n\t\t\tFileUtil.fullyDelete(tempConfPathForSecureRun);\n\t\t\ttempConfPathForSecureRun = null;\n\t\t}\n\n\t\t// When we are on travis, we copy the temp files of JUnit (containing the MiniYARNCluster log files)\n\t\t// to <flinkRoot>/target/flink-yarn-tests-*.\n\t\t// The files from there are picked up by the ./tools/travis_watchdog.sh script\n\t\t// to upload them to Amazon S3.\n\t\tif(isOnTravis()) {\n\t\t\tFile target = new File(\"../target\" + yarnConfiguration.get(TEST_CLUSTER_NAME_KEY));\n\t\t\tif(!target.mkdirs()) {\n\t\t\t\tLOG.warn(\"Error creating dirs to {}\", target);\n\t\t\t}\n\t\t\tFile src = tmp.getRoot();\n\t\t\tLOG.info(\"copying the final files from {} to {}\", src.getAbsolutePath(), target.getAbsolutePath());\n\t\t\ttry {\n\t\t\t\tFileUtils.copyDirectoryToDirectory(src, target);\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.warn(\"Error copying the final files from {} to {}: msg: {}\", src.getAbsolutePath(), target.getAbsolutePath(), e.getMessage(), e);\n\t\t\t}\n\t\t}\n\n\t}",
            " 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718 +\n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727 +\n 728 +\n 729 +\n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  ",
            "\t@AfterClass\n\tpublic static void teardown() throws Exception {\n\n\t\tLOG.info(\"Stopping MiniYarn Cluster\");\n\t\tyarnCluster.stop();\n\n\t\t// Unset FLINK_CONF_DIR, as it might change the behavior of other tests\n\t\tMap<String, String> map = new HashMap<>(System.getenv());\n\t\tmap.remove(ConfigConstants.ENV_FLINK_CONF_DIR);\n\t\tmap.remove(\"YARN_CONF_DIR\");\n\t\tmap.remove(\"IN_TESTS\");\n\t\tTestBaseUtils.setEnv(map);\n\n\t\tif (tempConfPathForSecureRun != null) {\n\t\t\tFileUtil.fullyDelete(tempConfPathForSecureRun);\n\t\t\ttempConfPathForSecureRun = null;\n\t\t}\n\n\t\t// When we are on travis, we copy the temp files of JUnit (containing the MiniYARNCluster log files)\n\t\t// to <flinkRoot>/target/flink-yarn-tests-*.\n\t\t// The files from there are picked up by the ./tools/travis_watchdog.sh script\n\t\t// to upload them to Amazon S3.\n\t\tif (isOnTravis()) {\n\t\t\tFile target = new File(\"../target\" + YARN_CONFIGURATION.get(TEST_CLUSTER_NAME_KEY));\n\t\t\tif (!target.mkdirs()) {\n\t\t\t\tLOG.warn(\"Error creating dirs to {}\", target);\n\t\t\t}\n\t\t\tFile src = tmp.getRoot();\n\t\t\tLOG.info(\"copying the final files from {} to {}\", src.getAbsolutePath(), target.getAbsolutePath());\n\t\t\ttry {\n\t\t\t\tFileUtils.copyDirectoryToDirectory(src, target);\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.warn(\"Error copying the final files from {} to {}: msg: {}\", src.getAbsolutePath(), target.getAbsolutePath(), e.getMessage(), e);\n\t\t\t}\n\t\t}\n\n\t}"
        ],
        [
            "YarnTestBase::sendOutput()",
            " 616  \n 617 -\n 618 -\n 619  \n 620  \n 621  \n 622  ",
            "\tprotected static void sendOutput() {\n\t\tSystem.setOut(originalStdout);\n\t\tSystem.setErr(originalStderr);\n\n\t\tLOG.info(\"Sending stdout content through logger: \\n\\n{}\\n\\n\", outContent.toString());\n\t\tLOG.info(\"Sending stderr content through logger: \\n\\n{}\\n\\n\", errContent.toString());\n\t}",
            " 620  \n 621 +\n 622 +\n 623  \n 624  \n 625  \n 626  ",
            "\tprotected static void sendOutput() {\n\t\tSystem.setOut(ORIGINAL_STDOUT);\n\t\tSystem.setErr(ORIGINAL_STDERR);\n\n\t\tLOG.info(\"Sending stdout content through logger: \\n\\n{}\\n\\n\", outContent.toString());\n\t\tLOG.info(\"Sending stderr content through logger: \\n\\n{}\\n\\n\", errContent.toString());\n\t}"
        ],
        [
            "YarnTestBase::start(Configuration,String,String)",
            " 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414 -\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421 -\n 422  \n 423 -\n 424 -\n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455 -\n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  ",
            "\tprivate static void start(Configuration conf, String principal, String keytab) {\n\t\t// set the home directory to a temp directory. Flink on YARN is using the home dir to distribute the file\n\t\tFile homeDir = null;\n\t\ttry {\n\t\t\thomeDir = tmp.newFolder();\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(e.getMessage());\n\t\t}\n\t\tSystem.setProperty(\"user.home\", homeDir.getAbsolutePath());\n\t\tString uberjarStartLoc = \"..\";\n\t\tLOG.info(\"Trying to locate uberjar in {}\", new File(uberjarStartLoc));\n\t\tflinkUberjar = findFile(uberjarStartLoc, new RootDirFilenameFilter());\n\t\tAssert.assertNotNull(\"Flink uberjar not found\", flinkUberjar);\n\t\tString flinkDistRootDir = flinkUberjar.getParentFile().getParent();\n\t\tflinkLibFolder = flinkUberjar.getParentFile(); // the uberjar is located in lib/\n\t\tAssert.assertNotNull(\"Flink flinkLibFolder not found\", flinkLibFolder);\n\t\tAssert.assertTrue(\"lib folder not found\", flinkLibFolder.exists());\n\t\tAssert.assertTrue(\"lib folder not found\", flinkLibFolder.isDirectory());\n\n\t\tif (!flinkUberjar.exists()) {\n\t\t\tAssert.fail(\"Unable to locate yarn-uberjar.jar\");\n\t\t}\n\n\t\ttry {\n\t\t\tLOG.info(\"Starting up MiniYARNCluster\");\n\t\t\tif (yarnCluster == null) {\n\t\t\t\tyarnCluster = new MiniYARNCluster(conf.get(YarnTestBase.TEST_CLUSTER_NAME_KEY), NUM_NODEMANAGERS, 1, 1);\n\n\t\t\t\tyarnCluster.init(conf);\n\t\t\t\tyarnCluster.start();\n\t\t\t}\n\n\t\t\tMap<String, String> map = new HashMap<String, String>(System.getenv());\n\n\t\t\tFile flinkConfDirPath = findFile(flinkDistRootDir, new ContainsName(new String[]{\"flink-conf.yaml\"}));\n\t\t\tAssert.assertNotNull(flinkConfDirPath);\n\n\t\t\tif(!StringUtils.isBlank(principal) && !StringUtils.isBlank(keytab)) {\n\t\t\t\t//copy conf dir to test temporary workspace location\n\t\t\t\ttempConfPathForSecureRun = tmp.newFolder(\"conf\");\n\n\t\t\t\tString confDirPath = flinkConfDirPath.getParentFile().getAbsolutePath();\n\t\t\t\tFileUtils.copyDirectory(new File(confDirPath), tempConfPathForSecureRun);\n\n\t\t\t\ttry(FileWriter fw = new FileWriter(new File(tempConfPathForSecureRun,\"flink-conf.yaml\"), true);\n\t\t\t\t\tBufferedWriter bw = new BufferedWriter(fw);\n\t\t\t\t\tPrintWriter out = new PrintWriter(bw))\n\t\t\t\t{\n\t\t\t\t\tLOG.info(\"writing keytab: \" + keytab + \" and principal: \" + principal + \" to config file\");\n\t\t\t\t\tout.println(\"\");\n\t\t\t\t\tout.println(\"#Security Configurations Auto Populated \");\n\t\t\t\t\tout.println(SecurityOptions.KERBEROS_LOGIN_KEYTAB.key() + \": \" + keytab);\n\t\t\t\t\tout.println(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL.key() + \": \" + principal);\n\t\t\t\t\tout.println(\"\");\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tthrow new RuntimeException(\"Exception occured while trying to append the security configurations.\", e);\n\t\t\t\t}\n\n\t\t\t\tString configDir = tempConfPathForSecureRun.getAbsolutePath();\n\n\t\t\t\tLOG.info(\"Temporary Flink configuration directory to be used for secure test: {}\", configDir);\n\n\t\t\t\tAssert.assertNotNull(configDir);\n\n\t\t\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, configDir);\n\n\t\t\t} else {\n\t\t\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, flinkConfDirPath.getParent());\n\t\t\t}\n\n\t\t\tFile yarnConfFile = writeYarnSiteConfigXML(conf);\n\t\t\tmap.put(\"YARN_CONF_DIR\", yarnConfFile.getParentFile().getAbsolutePath());\n\t\t\tmap.put(\"IN_TESTS\", \"yes we are in tests\"); // see YarnClusterDescriptor() for more infos\n\t\t\tTestBaseUtils.setEnv(map);\n\n\t\t\tAssert.assertTrue(yarnCluster.getServiceState() == Service.STATE.STARTED);\n\n\t\t\t// wait for the nodeManagers to connect\n\t\t\twhile(!yarnCluster.waitForNodeManagersToConnect(500)) {\n\t\t\t\tLOG.info(\"Waiting for Nodemanagers to connect\");\n\t\t\t}\n\t\t} catch (Exception ex) {\n\t\t\tex.printStackTrace();\n\t\t\tLOG.error(\"setup failure\", ex);\n\t\t\tAssert.fail();\n\t\t}\n\n\t}",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418 +\n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425 +\n 426  \n 427 +\n 428 +\n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459 +\n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "\tprivate static void start(Configuration conf, String principal, String keytab) {\n\t\t// set the home directory to a temp directory. Flink on YARN is using the home dir to distribute the file\n\t\tFile homeDir = null;\n\t\ttry {\n\t\t\thomeDir = tmp.newFolder();\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(e.getMessage());\n\t\t}\n\t\tSystem.setProperty(\"user.home\", homeDir.getAbsolutePath());\n\t\tString uberjarStartLoc = \"..\";\n\t\tLOG.info(\"Trying to locate uberjar in {}\", new File(uberjarStartLoc));\n\t\tflinkUberjar = findFile(uberjarStartLoc, new RootDirFilenameFilter());\n\t\tAssert.assertNotNull(\"Flink uberjar not found\", flinkUberjar);\n\t\tString flinkDistRootDir = flinkUberjar.getParentFile().getParent();\n\t\tflinkLibFolder = flinkUberjar.getParentFile(); // the uberjar is located in lib/\n\t\tAssert.assertNotNull(\"Flink flinkLibFolder not found\", flinkLibFolder);\n\t\tAssert.assertTrue(\"lib folder not found\", flinkLibFolder.exists());\n\t\tAssert.assertTrue(\"lib folder not found\", flinkLibFolder.isDirectory());\n\n\t\tif (!flinkUberjar.exists()) {\n\t\t\tAssert.fail(\"Unable to locate yarn-uberjar.jar\");\n\t\t}\n\n\t\ttry {\n\t\t\tLOG.info(\"Starting up MiniYARNCluster\");\n\t\t\tif (yarnCluster == null) {\n\t\t\t\tyarnCluster = new MiniYARNCluster(conf.get(YarnTestBase.TEST_CLUSTER_NAME_KEY), NUM_NODEMANAGERS, 1, 1);\n\n\t\t\t\tyarnCluster.init(conf);\n\t\t\t\tyarnCluster.start();\n\t\t\t}\n\n\t\t\tMap<String, String> map = new HashMap<String, String>(System.getenv());\n\n\t\t\tFile flinkConfDirPath = findFile(flinkDistRootDir, new ContainsName(new String[]{\"flink-conf.yaml\"}));\n\t\t\tAssert.assertNotNull(flinkConfDirPath);\n\n\t\t\tif (!StringUtils.isBlank(principal) && !StringUtils.isBlank(keytab)) {\n\t\t\t\t//copy conf dir to test temporary workspace location\n\t\t\t\ttempConfPathForSecureRun = tmp.newFolder(\"conf\");\n\n\t\t\t\tString confDirPath = flinkConfDirPath.getParentFile().getAbsolutePath();\n\t\t\t\tFileUtils.copyDirectory(new File(confDirPath), tempConfPathForSecureRun);\n\n\t\t\t\ttry (FileWriter fw = new FileWriter(new File(tempConfPathForSecureRun, \"flink-conf.yaml\"), true);\n\t\t\t\t\tBufferedWriter bw = new BufferedWriter(fw);\n\t\t\t\t\tPrintWriter out = new PrintWriter(bw)) {\n\n\t\t\t\t\tLOG.info(\"writing keytab: \" + keytab + \" and principal: \" + principal + \" to config file\");\n\t\t\t\t\tout.println(\"\");\n\t\t\t\t\tout.println(\"#Security Configurations Auto Populated \");\n\t\t\t\t\tout.println(SecurityOptions.KERBEROS_LOGIN_KEYTAB.key() + \": \" + keytab);\n\t\t\t\t\tout.println(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL.key() + \": \" + principal);\n\t\t\t\t\tout.println(\"\");\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tthrow new RuntimeException(\"Exception occured while trying to append the security configurations.\", e);\n\t\t\t\t}\n\n\t\t\t\tString configDir = tempConfPathForSecureRun.getAbsolutePath();\n\n\t\t\t\tLOG.info(\"Temporary Flink configuration directory to be used for secure test: {}\", configDir);\n\n\t\t\t\tAssert.assertNotNull(configDir);\n\n\t\t\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, configDir);\n\n\t\t\t} else {\n\t\t\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, flinkConfDirPath.getParent());\n\t\t\t}\n\n\t\t\tFile yarnConfFile = writeYarnSiteConfigXML(conf);\n\t\t\tmap.put(\"YARN_CONF_DIR\", yarnConfFile.getParentFile().getAbsolutePath());\n\t\t\tmap.put(\"IN_TESTS\", \"yes we are in tests\"); // see YarnClusterDescriptor() for more infos\n\t\t\tTestBaseUtils.setEnv(map);\n\n\t\t\tAssert.assertTrue(yarnCluster.getServiceState() == Service.STATE.STARTED);\n\n\t\t\t// wait for the nodeManagers to connect\n\t\t\twhile (!yarnCluster.waitForNodeManagersToConnect(500)) {\n\t\t\t\tLOG.info(\"Waiting for Nodemanagers to connect\");\n\t\t\t}\n\t\t} catch (Exception ex) {\n\t\t\tex.printStackTrace();\n\t\t\tLOG.error(\"setup failure\", ex);\n\t\t\tAssert.fail();\n\t\t}\n\n\t}"
        ],
        [
            "YARNSessionCapacitySchedulerITCase::testDetachedPerJobYarnClusterInternal(String)",
            " 389  \n 390  \n 391 -\n 392  \n 393  \n 394  \n 395  \n 396 -\n 397  \n 398  \n 399 -\n 400  \n 401  \n 402  \n 403  \n 404  \n 405 -\n 406  \n 407  \n 408  \n 409 -\n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453 -\n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462 -\n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474 -\n 475  \n 476 -\n 477  \n 478  \n 479  \n 480 -\n 481 -\n 482 -\n 483 -\n 484  \n 485  \n 486  \n 487  \n 488  \n 489 -\n 490 -\n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503 -\n 504  \n 505  \n 506  \n 507 -\n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517 -\n 518  \n 519  \n 520 -\n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536 -\n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  ",
            "\tprivate void testDetachedPerJobYarnClusterInternal(String job) {\n\t\tYarnClient yc = YarnClient.createYarnClient();\n\t\tyc.init(yarnConfiguration);\n\t\tyc.start();\n\n\t\t// get temporary folder for writing output of wordcount example\n\t\tFile tmpOutFolder = null;\n\t\ttry{\n\t\t\ttmpOutFolder = tmp.newFolder();\n\t\t}\n\t\tcatch(IOException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\n\t\t// get temporary file for reading input data for wordcount example\n\t\tFile tmpInFile;\n\t\ttry{\n\t\t\ttmpInFile = tmp.newFile();\n\t\t\tFileUtils.writeStringToFile(tmpInFile, WordCountData.TEXT);\n\t\t}\n\t\tcatch(IOException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\n\t\tRunner runner = startWithArgs(new String[]{\n\t\t\t\t\"run\", \"-m\", \"yarn-cluster\",\n\t\t\t\t\"-yj\", flinkUberjar.getAbsolutePath(),\n\t\t\t\t\"-yt\", flinkLibFolder.getAbsolutePath(),\n\t\t\t\t\"-yn\", \"1\",\n\t\t\t\t\"-yjm\", \"768\",\n\t\t\t\t\"-yD\", \"yarn.heap-cutoff-ratio=0.5\", // test if the cutoff is passed correctly\n\t\t\t\t\"-yD\", \"yarn.tags=test-tag\",\n\t\t\t\t\"-ytm\", \"1024\",\n\t\t\t\t\"-ys\", \"2\", // test requesting slots from YARN.\n\t\t\t\t\"--yarndetached\", job,\n\t\t\t\t\"--input\", tmpInFile.getAbsoluteFile().toString(),\n\t\t\t\t\"--output\", tmpOutFolder.getAbsoluteFile().toString()},\n\t\t\t\"Job has been submitted with JobID\",\n\t\t\tRunTypes.CLI_FRONTEND);\n\n\t\t// it should usually be 2, but on slow machines, the number varies\n\t\tAssert.assertTrue(\"There should be at most 2 containers running\", getRunningContainers() <= 2);\n\t\t// give the runner some time to detach\n\t\tfor (int attempt = 0; runner.isAlive() && attempt < 5; attempt++) {\n\t\t\ttry {\n\t\t\t\tThread.sleep(500);\n\t\t\t} catch (InterruptedException e) {\n\t\t\t}\n\t\t}\n\t\tAssert.assertFalse(\"The runner should detach.\", runner.isAlive());\n\t\tLOG.info(\"CLI Frontend has returned, so the job is running\");\n\n\t\t// find out the application id and wait until it has finished.\n\t\ttry {\n\t\t\tList<ApplicationReport> apps = yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING));\n\n\t\t\tApplicationId tmpAppId;\n\t\t\tif (apps.size() == 1) {\n\t\t\t\t// Better method to find the right appId. But sometimes the app is shutting down very fast\n\t\t\t\t// Only one running\n\t\t\t\ttmpAppId = apps.get(0).getApplicationId();\n\n\t\t\t\tLOG.info(\"waiting for the job with appId {} to finish\", tmpAppId);\n\t\t\t\t// wait until the app has finished\n\t\t\t\twhile(yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING)).size() > 0) {\n\t\t\t\t\tsleep(500);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// get appId by finding the latest finished appid\n\t\t\t\tapps = yc.getApplications();\n\t\t\t\tCollections.sort(apps, new Comparator<ApplicationReport>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic int compare(ApplicationReport o1, ApplicationReport o2) {\n\t\t\t\t\t\treturn o1.getApplicationId().compareTo(o2.getApplicationId())*-1;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\ttmpAppId = apps.get(0).getApplicationId();\n\t\t\t\tLOG.info(\"Selected {} as the last appId from {}\", tmpAppId, Arrays.toString(apps.toArray()));\n\t\t\t}\n\t\t\tfinal ApplicationId id = tmpAppId;\n\n\t\t\t// now it has finished.\n\t\t\t// check the output files.\n\t\t\tFile[] listOfOutputFiles = tmpOutFolder.listFiles();\n\n\n\t\t\tAssert.assertNotNull(\"Taskmanager output not found\", listOfOutputFiles);\n\t\t\tLOG.info(\"The job has finished. TaskManager output files found in {}\", tmpOutFolder );\n\n\t\t\t// read all output files in output folder to one output string\n\t\t\tString content = \"\";\n\t\t\tfor(File f:listOfOutputFiles)\n\t\t\t{\n\t\t\t\tif(f.isFile())\n\t\t\t\t{\n\t\t\t\t\tcontent += FileUtils.readFileToString(f) + \"\\n\";\n\t\t\t\t}\n\t\t\t}\n\t\t\t//String content = FileUtils.readFileToString(taskmanagerOut);\n\t\t\t// check for some of the wordcount outputs.\n\t\t\tAssert.assertTrue(\"Expected string 'da 5' or '(all,2)' not found in string '\"+content+\"'\", content.contains(\"da 5\") || content.contains(\"(da,5)\") || content.contains(\"(all,2)\"));\n\t\t\tAssert.assertTrue(\"Expected string 'der 29' or '(mind,1)' not found in string'\"+content+\"'\",content.contains(\"der 29\") || content.contains(\"(der,29)\") || content.contains(\"(mind,1)\"));\n\n\t\t\t// check if the heap size for the TaskManager was set correctly\n\t\t\tFile jobmanagerLog = YarnTestBase.findFile(\"..\", new FilenameFilter() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean accept(File dir, String name) {\n\t\t\t\t\treturn name.contains(\"jobmanager.log\") && dir.getAbsolutePath().contains(id.toString());\n\t\t\t\t}\n\t\t\t});\n\t\t\tAssert.assertNotNull(\"Unable to locate JobManager log\", jobmanagerLog);\n\t\t\tcontent = FileUtils.readFileToString(jobmanagerLog);\n\t\t\t// TM was started with 1024 but we cut off 50% (NOT THE DEFAULT VALUE)\n\t\t\tString expected = \"Starting TaskManagers with command: $JAVA_HOME/bin/java -Xms424m -Xmx424m\";\n\t\t\tAssert.assertTrue(\"Expected string '\" + expected + \"' not found in JobManager log: '\"+jobmanagerLog+\"'\",\n\t\t\t\tcontent.contains(expected));\n\t\t\texpected = \" (2/2) (attempt #0) to \";\n\t\t\tAssert.assertTrue(\"Expected string '\" + expected + \"' not found in JobManager log.\" +\n\t\t\t\t\t\"This string checks that the job has been started with a parallelism of 2. Log contents: '\"+jobmanagerLog+\"'\",\n\t\t\t\tcontent.contains(expected));\n\n\t\t\t// make sure the detached app is really finished.\n\t\t\tLOG.info(\"Checking again that app has finished\");\n\t\t\tApplicationReport rep;\n\t\t\tdo {\n\t\t\t\tsleep(500);\n\t\t\t\trep = yc.getApplicationReport(id);\n\t\t\t\tLOG.info(\"Got report {}\", rep);\n\t\t\t} while(rep.getYarnApplicationState() == YarnApplicationState.RUNNING);\n\n\t\t\tverifyApplicationTags(rep);\n\t\t} catch(Throwable t) {\n\t\t\tLOG.warn(\"Error while detached yarn session was running\", t);\n\t\t\tAssert.fail(t.getMessage());\n\t\t} finally {\n\n\t\t\t//cleanup the yarn-properties file\n\t\t\tString confDirPath = System.getenv(\"FLINK_CONF_DIR\");\n\t\t\tFile configDirectory = new File(confDirPath);\n\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Using configuration directory \" + configDirectory.getAbsolutePath());\n\n\t\t\t// load the configuration\n\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Trying to load configuration file\");\n\t\t\tGlobalConfiguration.loadConfiguration(configDirectory.getAbsolutePath());\n\n\t\t\ttry {\n\t\t\t\tFile yarnPropertiesFile = FlinkYarnSessionCli.getYarnPropertiesLocation(GlobalConfiguration.loadConfiguration());\n\t\t\t\tif(yarnPropertiesFile.exists()) {\n\t\t\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Cleaning up temporary Yarn address reference: {}\", yarnPropertiesFile.getAbsolutePath());\n\t\t\t\t\tyarnPropertiesFile.delete();\n\t\t\t\t}\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"testDetachedPerJobYarnClusterInternal: Exception while deleting the JobManager address file\", e);\n\t\t\t}\n\n\t\t}\n\t}",
            " 395  \n 396  \n 397 +\n 398  \n 399  \n 400  \n 401  \n 402 +\n 403  \n 404  \n 405 +\n 406  \n 407  \n 408  \n 409  \n 410  \n 411 +\n 412  \n 413  \n 414  \n 415 +\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459 +\n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468 +\n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481 +\n 482  \n 483  \n 484  \n 485 +\n 486 +\n 487  \n 488  \n 489  \n 490  \n 491  \n 492 +\n 493 +\n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506 +\n 507  \n 508  \n 509  \n 510 +\n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520 +\n 521  \n 522  \n 523 +\n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539 +\n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  ",
            "\tprivate void testDetachedPerJobYarnClusterInternal(String job) {\n\t\tYarnClient yc = YarnClient.createYarnClient();\n\t\tyc.init(YARN_CONFIGURATION);\n\t\tyc.start();\n\n\t\t// get temporary folder for writing output of wordcount example\n\t\tFile tmpOutFolder = null;\n\t\ttry {\n\t\t\ttmpOutFolder = tmp.newFolder();\n\t\t}\n\t\tcatch (IOException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\n\t\t// get temporary file for reading input data for wordcount example\n\t\tFile tmpInFile;\n\t\ttry {\n\t\t\ttmpInFile = tmp.newFile();\n\t\t\tFileUtils.writeStringToFile(tmpInFile, WordCountData.TEXT);\n\t\t}\n\t\tcatch (IOException e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\n\t\tRunner runner = startWithArgs(new String[]{\n\t\t\t\t\"run\", \"-m\", \"yarn-cluster\",\n\t\t\t\t\"-yj\", flinkUberjar.getAbsolutePath(),\n\t\t\t\t\"-yt\", flinkLibFolder.getAbsolutePath(),\n\t\t\t\t\"-yn\", \"1\",\n\t\t\t\t\"-yjm\", \"768\",\n\t\t\t\t\"-yD\", \"yarn.heap-cutoff-ratio=0.5\", // test if the cutoff is passed correctly\n\t\t\t\t\"-yD\", \"yarn.tags=test-tag\",\n\t\t\t\t\"-ytm\", \"1024\",\n\t\t\t\t\"-ys\", \"2\", // test requesting slots from YARN.\n\t\t\t\t\"--yarndetached\", job,\n\t\t\t\t\"--input\", tmpInFile.getAbsoluteFile().toString(),\n\t\t\t\t\"--output\", tmpOutFolder.getAbsoluteFile().toString()},\n\t\t\t\"Job has been submitted with JobID\",\n\t\t\tRunTypes.CLI_FRONTEND);\n\n\t\t// it should usually be 2, but on slow machines, the number varies\n\t\tAssert.assertTrue(\"There should be at most 2 containers running\", getRunningContainers() <= 2);\n\t\t// give the runner some time to detach\n\t\tfor (int attempt = 0; runner.isAlive() && attempt < 5; attempt++) {\n\t\t\ttry {\n\t\t\t\tThread.sleep(500);\n\t\t\t} catch (InterruptedException e) {\n\t\t\t}\n\t\t}\n\t\tAssert.assertFalse(\"The runner should detach.\", runner.isAlive());\n\t\tLOG.info(\"CLI Frontend has returned, so the job is running\");\n\n\t\t// find out the application id and wait until it has finished.\n\t\ttry {\n\t\t\tList<ApplicationReport> apps = yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING));\n\n\t\t\tApplicationId tmpAppId;\n\t\t\tif (apps.size() == 1) {\n\t\t\t\t// Better method to find the right appId. But sometimes the app is shutting down very fast\n\t\t\t\t// Only one running\n\t\t\t\ttmpAppId = apps.get(0).getApplicationId();\n\n\t\t\t\tLOG.info(\"waiting for the job with appId {} to finish\", tmpAppId);\n\t\t\t\t// wait until the app has finished\n\t\t\t\twhile (yc.getApplications(EnumSet.of(YarnApplicationState.RUNNING)).size() > 0) {\n\t\t\t\t\tsleep(500);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\t// get appId by finding the latest finished appid\n\t\t\t\tapps = yc.getApplications();\n\t\t\t\tCollections.sort(apps, new Comparator<ApplicationReport>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic int compare(ApplicationReport o1, ApplicationReport o2) {\n\t\t\t\t\t\treturn o1.getApplicationId().compareTo(o2.getApplicationId()) * -1;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t\ttmpAppId = apps.get(0).getApplicationId();\n\t\t\t\tLOG.info(\"Selected {} as the last appId from {}\", tmpAppId, Arrays.toString(apps.toArray()));\n\t\t\t}\n\t\t\tfinal ApplicationId id = tmpAppId;\n\n\t\t\t// now it has finished.\n\t\t\t// check the output files.\n\t\t\tFile[] listOfOutputFiles = tmpOutFolder.listFiles();\n\n\t\t\tAssert.assertNotNull(\"Taskmanager output not found\", listOfOutputFiles);\n\t\t\tLOG.info(\"The job has finished. TaskManager output files found in {}\", tmpOutFolder);\n\n\t\t\t// read all output files in output folder to one output string\n\t\t\tString content = \"\";\n\t\t\tfor (File f:listOfOutputFiles) {\n\t\t\t\tif (f.isFile()) {\n\t\t\t\t\tcontent += FileUtils.readFileToString(f) + \"\\n\";\n\t\t\t\t}\n\t\t\t}\n\t\t\t//String content = FileUtils.readFileToString(taskmanagerOut);\n\t\t\t// check for some of the wordcount outputs.\n\t\t\tAssert.assertTrue(\"Expected string 'da 5' or '(all,2)' not found in string '\" + content + \"'\", content.contains(\"da 5\") || content.contains(\"(da,5)\") || content.contains(\"(all,2)\"));\n\t\t\tAssert.assertTrue(\"Expected string 'der 29' or '(mind,1)' not found in string'\" + content + \"'\", content.contains(\"der 29\") || content.contains(\"(der,29)\") || content.contains(\"(mind,1)\"));\n\n\t\t\t// check if the heap size for the TaskManager was set correctly\n\t\t\tFile jobmanagerLog = YarnTestBase.findFile(\"..\", new FilenameFilter() {\n\t\t\t\t@Override\n\t\t\t\tpublic boolean accept(File dir, String name) {\n\t\t\t\t\treturn name.contains(\"jobmanager.log\") && dir.getAbsolutePath().contains(id.toString());\n\t\t\t\t}\n\t\t\t});\n\t\t\tAssert.assertNotNull(\"Unable to locate JobManager log\", jobmanagerLog);\n\t\t\tcontent = FileUtils.readFileToString(jobmanagerLog);\n\t\t\t// TM was started with 1024 but we cut off 50% (NOT THE DEFAULT VALUE)\n\t\t\tString expected = \"Starting TaskManagers with command: $JAVA_HOME/bin/java -Xms424m -Xmx424m\";\n\t\t\tAssert.assertTrue(\"Expected string '\" + expected + \"' not found in JobManager log: '\" + jobmanagerLog + \"'\",\n\t\t\t\tcontent.contains(expected));\n\t\t\texpected = \" (2/2) (attempt #0) to \";\n\t\t\tAssert.assertTrue(\"Expected string '\" + expected + \"' not found in JobManager log.\" +\n\t\t\t\t\t\"This string checks that the job has been started with a parallelism of 2. Log contents: '\" + jobmanagerLog + \"'\",\n\t\t\t\tcontent.contains(expected));\n\n\t\t\t// make sure the detached app is really finished.\n\t\t\tLOG.info(\"Checking again that app has finished\");\n\t\t\tApplicationReport rep;\n\t\t\tdo {\n\t\t\t\tsleep(500);\n\t\t\t\trep = yc.getApplicationReport(id);\n\t\t\t\tLOG.info(\"Got report {}\", rep);\n\t\t\t} while (rep.getYarnApplicationState() == YarnApplicationState.RUNNING);\n\n\t\t\tverifyApplicationTags(rep);\n\t\t} catch (Throwable t) {\n\t\t\tLOG.warn(\"Error while detached yarn session was running\", t);\n\t\t\tAssert.fail(t.getMessage());\n\t\t} finally {\n\n\t\t\t//cleanup the yarn-properties file\n\t\t\tString confDirPath = System.getenv(\"FLINK_CONF_DIR\");\n\t\t\tFile configDirectory = new File(confDirPath);\n\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Using configuration directory \" + configDirectory.getAbsolutePath());\n\n\t\t\t// load the configuration\n\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Trying to load configuration file\");\n\t\t\tGlobalConfiguration.loadConfiguration(configDirectory.getAbsolutePath());\n\n\t\t\ttry {\n\t\t\t\tFile yarnPropertiesFile = FlinkYarnSessionCli.getYarnPropertiesLocation(GlobalConfiguration.loadConfiguration());\n\t\t\t\tif (yarnPropertiesFile.exists()) {\n\t\t\t\t\tLOG.info(\"testDetachedPerJobYarnClusterInternal: Cleaning up temporary Yarn address reference: {}\", yarnPropertiesFile.getAbsolutePath());\n\t\t\t\t\tyarnPropertiesFile.delete();\n\t\t\t\t}\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"testDetachedPerJobYarnClusterInternal: Exception while deleting the JobManager address file\", e);\n\t\t\t}\n\n\t\t}\n\t}"
        ],
        [
            "YarnTestBase::ContainsName::accept(File,String)",
            " 254  \n 255  \n 256 -\n 257 -\n 258 -\n 259  \n 260  \n 261  \n 262  \n 263  \n 264 -\n 265 -\n 266  \n 267  \n 268  \n 269  \n 270  \n 271  ",
            "\t\t@Override\n\t\tpublic boolean accept(File dir, String name) {\n\t\t\tif(excludeInPath == null) {\n\t\t\t\tfor(String n: names) {\n\t\t\t\t\tif(!name.contains(n)) {\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn true;\n\t\t\t} else {\n\t\t\t\tfor(String n: names) {\n\t\t\t\t\tif(!name.contains(n)) {\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn !dir.toString().contains(excludeInPath);\n\t\t\t}\n\t\t}",
            " 258  \n 259  \n 260 +\n 261 +\n 262 +\n 263  \n 264  \n 265  \n 266  \n 267  \n 268 +\n 269 +\n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "\t\t@Override\n\t\tpublic boolean accept(File dir, String name) {\n\t\t\tif (excludeInPath == null) {\n\t\t\t\tfor (String n: names) {\n\t\t\t\t\tif (!name.contains(n)) {\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn true;\n\t\t\t} else {\n\t\t\t\tfor (String n: names) {\n\t\t\t\t\tif (!name.contains(n)) {\n\t\t\t\t\t\treturn false;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn !dir.toString().contains(excludeInPath);\n\t\t\t}\n\t\t}"
        ],
        [
            "YARNSessionFIFOITCase::setup()",
            "  66  \n  67  \n  68 -\n  69 -\n  70 -\n  71 -\n  72 -\n  73  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\tyarnConfiguration.setClass(YarnConfiguration.RM_SCHEDULER, FifoScheduler.class, ResourceScheduler.class);\n\t\tyarnConfiguration.setInt(YarnConfiguration.NM_PMEM_MB, 768);\n\t\tyarnConfiguration.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 512);\n\t\tyarnConfiguration.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, \"flink-yarn-tests-fifo\");\n\t\tstartYARNWithConfig(yarnConfiguration);\n\t}",
            "  62  \n  63  \n  64 +\n  65 +\n  66 +\n  67 +\n  68 +\n  69  ",
            "\t@BeforeClass\n\tpublic static void setup() {\n\t\tYARN_CONFIGURATION.setClass(YarnConfiguration.RM_SCHEDULER, FifoScheduler.class, ResourceScheduler.class);\n\t\tYARN_CONFIGURATION.setInt(YarnConfiguration.NM_PMEM_MB, 768);\n\t\tYARN_CONFIGURATION.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB, 512);\n\t\tYARN_CONFIGURATION.set(YarnTestBase.TEST_CLUSTER_NAME_KEY, \"flink-yarn-tests-fifo\");\n\t\tstartYARNWithConfig(YARN_CONFIGURATION);\n\t}"
        ],
        [
            "YarnTestBase::runWithArgs(String,String,String,RunTypes,int)",
            " 523  \n 524 -\n 525  ",
            "\tprotected void runWithArgs(String[] args, String terminateAfterString, String[] failOnStrings, RunTypes type, int returnCode) {\n\t\trunWithArgs(args,terminateAfterString, failOnStrings, type, returnCode, false);\n\t}",
            " 527  \n 528 +\n 529  ",
            "\tprotected void runWithArgs(String[] args, String terminateAfterString, String[] failOnStrings, RunTypes type, int returnCode) {\n\t\trunWithArgs(args, terminateAfterString, failOnStrings, type, returnCode, false);\n\t}"
        ],
        [
            "UtilsTest::getEventContainingString(String)",
            " 155  \n 156 -\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "\tpublic static LoggingEvent getEventContainingString(String expected) {\n\t\tif(testAppender == null) {\n\t\t\tthrow new NullPointerException(\"Initialize test appender first\");\n\t\t}\n\t\tLoggingEvent found = null;\n\t\t// make sure that different threads are not logging while the logs are checked\n\t\tsynchronized (testAppender.events) {\n\t\t\tfor (LoggingEvent event : testAppender.events) {\n\t\t\t\tif (event.getMessage().toString().contains(expected)) {\n\t\t\t\t\tfound = event;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn found;\n\t}",
            " 160  \n 161 +\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "\tpublic static LoggingEvent getEventContainingString(String expected) {\n\t\tif (testAppender == null) {\n\t\t\tthrow new NullPointerException(\"Initialize test appender first\");\n\t\t}\n\t\tLoggingEvent found = null;\n\t\t// make sure that different threads are not logging while the logs are checked\n\t\tsynchronized (testAppender.events) {\n\t\t\tfor (LoggingEvent event : testAppender.events) {\n\t\t\t\tif (event.getMessage().toString().contains(expected)) {\n\t\t\t\t\tfound = event;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn found;\n\t}"
        ],
        [
            "YarnTestBase::runWithArgs(String,String,String,RunTypes,int,boolean)",
            " 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543 -\n 544  \n 545 -\n 546 -\n 547 -\n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557 -\n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570 -\n 571  \n 572 -\n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579 -\n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603 -\n 604  \n 605  \n 606 -\n 607  \n 608  \n 609  \n 610 -\n 611  \n 612  \n 613  \n 614  ",
            "\t/**\n\t * The test has been passed once the \"terminateAfterString\" has been seen.\n\t * @param args Command line arguments for the runner\n\t * @param terminateAfterString the runner is searching the stdout and stderr for this string. as soon as it appears, the test has passed\n\t * @param failOnPatterns The runner is searching stdout and stderr for the pattern (regexp) specified here. If one appears, the test has failed\n\t * @param type Set the type of the runner\n\t * @param expectedReturnValue Expected return code from the runner.\n\t * @param checkLogForTerminateString  If true, the runner checks also the log4j logger for the terminate string\n\t */\n\tprotected void runWithArgs(String[] args, String terminateAfterString, String[] failOnPatterns, RunTypes type, int expectedReturnValue, boolean checkLogForTerminateString) {\n\t\tLOG.info(\"Running with args {}\", Arrays.toString(args));\n\n\t\toutContent = new ByteArrayOutputStream();\n\t\terrContent = new ByteArrayOutputStream();\n\t\tSystem.setOut(new PrintStream(outContent));\n\t\tSystem.setErr(new PrintStream(errContent));\n\n\n\t\t// we wait for at most three minutes\n\t\tfinal int START_TIMEOUT_SECONDS = 180;\n\t\tfinal long deadline = System.currentTimeMillis() + (START_TIMEOUT_SECONDS * 1000);\n\t\t\n\t\tRunner runner = new Runner(args, type, expectedReturnValue);\n\t\trunner.start();\n\n\t\tboolean expectedStringSeen = false;\n\t\tboolean testPassedFromLog4j = false;\n\t\tdo {\n\t\t\tsleep(1000);\n\t\t\tString outContentString = outContent.toString();\n\t\t\tString errContentString = errContent.toString();\n\t\t\tif(failOnPatterns != null) {\n\t\t\t\tfor (String failOnString : failOnPatterns) {\n\t\t\t\t\tPattern pattern = Pattern.compile(failOnString);\n\t\t\t\t\tif (pattern.matcher(outContentString).find() || pattern.matcher(errContentString).find()) {\n\t\t\t\t\t\tLOG.warn(\"Failing test. Output contained illegal string '\" + failOnString + \"'\");\n\t\t\t\t\t\tsendOutput();\n\t\t\t\t\t\t// stopping runner.\n\t\t\t\t\t\trunner.sendStop();\n\t\t\t\t\t\tAssert.fail(\"Output contained illegal string '\" + failOnString + \"'\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// check output for the expected terminateAfterString.\n\t\t\tif(checkLogForTerminateString) {\n\t\t\t\tLoggingEvent matchedEvent = UtilsTest.getEventContainingString(terminateAfterString);\n\t\t\t\tif(matchedEvent != null) {\n\t\t\t\t\ttestPassedFromLog4j = true;\n\t\t\t\t\tLOG.info(\"Found expected output in logging event {}\", matchedEvent);\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\tif (outContentString.contains(terminateAfterString) || errContentString.contains(terminateAfterString) || testPassedFromLog4j ) {\n\t\t\t\texpectedStringSeen = true;\n\t\t\t\tLOG.info(\"Found expected output in redirected streams\");\n\t\t\t\t// send \"stop\" command to command line interface\n\t\t\t\tLOG.info(\"RunWithArgs: request runner to stop\");\n\t\t\t\trunner.sendStop();\n\t\t\t\t// wait for the thread to stop\n\t\t\t\ttry {\n\t\t\t\t\trunner.join(30000);\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\tLOG.warn(\"Interrupted while stopping runner\", e);\n\t\t\t\t}\n\t\t\t\tLOG.warn(\"RunWithArgs runner stopped.\");\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// check if thread died\n\t\t\t\tif (!runner.isAlive()) {\n\t\t\t\t\t// leave loop: the runner died, so we can not expect new strings to show up.\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\twhile (runner.getRunnerError() == null && !expectedStringSeen && System.currentTimeMillis() < deadline);\n\t\t\n\t\tsendOutput();\n\n\t\tif(runner.getRunnerError() != null) {\n\t\t\t// this lets the test fail.\n\t\t\tthrow new RuntimeException(\"Runner failed\", runner.getRunnerError());\n\t\t}\n\t\tAssert.assertTrue(\"During the timeout period of \" + START_TIMEOUT_SECONDS + \" seconds the \" +\n\t\t\t\t\"expected string did not show up\", expectedStringSeen);\n\n\t\tLOG.info(\"Test was successful\");\n\t}",
            " 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549 +\n 550 +\n 551 +\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561 +\n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574 +\n 575  \n 576 +\n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583 +\n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607 +\n 608  \n 609  \n 610 +\n 611  \n 612  \n 613  \n 614 +\n 615  \n 616  \n 617  \n 618  ",
            "\t/**\n\t * The test has been passed once the \"terminateAfterString\" has been seen.\n\t * @param args Command line arguments for the runner\n\t * @param terminateAfterString the runner is searching the stdout and stderr for this string. as soon as it appears, the test has passed\n\t * @param failOnPatterns The runner is searching stdout and stderr for the pattern (regexp) specified here. If one appears, the test has failed\n\t * @param type Set the type of the runner\n\t * @param expectedReturnValue Expected return code from the runner.\n\t * @param checkLogForTerminateString  If true, the runner checks also the log4j logger for the terminate string\n\t */\n\tprotected void runWithArgs(String[] args, String terminateAfterString, String[] failOnPatterns, RunTypes type, int expectedReturnValue, boolean checkLogForTerminateString) {\n\t\tLOG.info(\"Running with args {}\", Arrays.toString(args));\n\n\t\toutContent = new ByteArrayOutputStream();\n\t\terrContent = new ByteArrayOutputStream();\n\t\tSystem.setOut(new PrintStream(outContent));\n\t\tSystem.setErr(new PrintStream(errContent));\n\n\t\t// we wait for at most three minutes\n\t\tfinal int startTimeoutSeconds = 180;\n\t\tfinal long deadline = System.currentTimeMillis() + (startTimeoutSeconds * 1000);\n\n\t\tRunner runner = new Runner(args, type, expectedReturnValue);\n\t\trunner.start();\n\n\t\tboolean expectedStringSeen = false;\n\t\tboolean testPassedFromLog4j = false;\n\t\tdo {\n\t\t\tsleep(1000);\n\t\t\tString outContentString = outContent.toString();\n\t\t\tString errContentString = errContent.toString();\n\t\t\tif (failOnPatterns != null) {\n\t\t\t\tfor (String failOnString : failOnPatterns) {\n\t\t\t\t\tPattern pattern = Pattern.compile(failOnString);\n\t\t\t\t\tif (pattern.matcher(outContentString).find() || pattern.matcher(errContentString).find()) {\n\t\t\t\t\t\tLOG.warn(\"Failing test. Output contained illegal string '\" + failOnString + \"'\");\n\t\t\t\t\t\tsendOutput();\n\t\t\t\t\t\t// stopping runner.\n\t\t\t\t\t\trunner.sendStop();\n\t\t\t\t\t\tAssert.fail(\"Output contained illegal string '\" + failOnString + \"'\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\t// check output for the expected terminateAfterString.\n\t\t\tif (checkLogForTerminateString) {\n\t\t\t\tLoggingEvent matchedEvent = UtilsTest.getEventContainingString(terminateAfterString);\n\t\t\t\tif (matchedEvent != null) {\n\t\t\t\t\ttestPassedFromLog4j = true;\n\t\t\t\t\tLOG.info(\"Found expected output in logging event {}\", matchedEvent);\n\t\t\t\t}\n\n\t\t\t}\n\n\t\t\tif (outContentString.contains(terminateAfterString) || errContentString.contains(terminateAfterString) || testPassedFromLog4j) {\n\t\t\t\texpectedStringSeen = true;\n\t\t\t\tLOG.info(\"Found expected output in redirected streams\");\n\t\t\t\t// send \"stop\" command to command line interface\n\t\t\t\tLOG.info(\"RunWithArgs: request runner to stop\");\n\t\t\t\trunner.sendStop();\n\t\t\t\t// wait for the thread to stop\n\t\t\t\ttry {\n\t\t\t\t\trunner.join(30000);\n\t\t\t\t}\n\t\t\t\tcatch (InterruptedException e) {\n\t\t\t\t\tLOG.warn(\"Interrupted while stopping runner\", e);\n\t\t\t\t}\n\t\t\t\tLOG.warn(\"RunWithArgs runner stopped.\");\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// check if thread died\n\t\t\t\tif (!runner.isAlive()) {\n\t\t\t\t\t// leave loop: the runner died, so we can not expect new strings to show up.\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\twhile (runner.getRunnerError() == null && !expectedStringSeen && System.currentTimeMillis() < deadline);\n\n\t\tsendOutput();\n\n\t\tif (runner.getRunnerError() != null) {\n\t\t\t// this lets the test fail.\n\t\t\tthrow new RuntimeException(\"Runner failed\", runner.getRunnerError());\n\t\t}\n\t\tAssert.assertTrue(\"During the timeout period of \" + startTimeoutSeconds + \" seconds the \" +\n\t\t\t\t\"expected string did not show up\", expectedStringSeen);\n\n\t\tLOG.info(\"Test was successful\");\n\t}"
        ]
    ],
    "0bca76ede8b7447014a7d7ed17633d77ecfafe18": [
        [
            "MesosFlinkResourceManager::registered(Registered)",
            " 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552 -\n 553 -\n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  ",
            "\t/**\n\t * Called when connected to Mesos as a new framework.\n\t */\n\tprivate void registered(Registered message) {\n\t\tconnectionMonitor.tell(message, self());\n\n\t\ttry {\n\t\t\tworkerStore.setFrameworkID(Option.apply(message.frameworkId()));\n\t\t}\n\t\tcatch(Exception ex) {\n\t\t\tfatalError(\"unable to store the assigned framework ID\", ex);\n\t\t\treturn;\n\t\t}\n\n\t\tlaunchCoordinator.tell(message, self());\n\t\treconciliationCoordinator.tell(message, self());\n\t\ttaskRouter.tell(message, self());\n\t}",
            " 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549 +\n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  ",
            "\t/**\n\t * Called when connected to Mesos as a new framework.\n\t */\n\tprivate void registered(Registered message) {\n\t\tconnectionMonitor.tell(message, self());\n\n\t\ttry {\n\t\t\tworkerStore.setFrameworkID(Option.apply(message.frameworkId()));\n\t\t} catch (Exception ex) {\n\t\t\tfatalError(\"unable to store the assigned framework ID\", ex);\n\t\t\treturn;\n\t\t}\n\n\t\tlaunchCoordinator.tell(message, self());\n\t\treconciliationCoordinator.tell(message, self());\n\t\ttaskRouter.tell(message, self());\n\t}"
        ],
        [
            "MesosFlinkResourceManager::acceptOffers(AcceptOffers)",
            " 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424 -\n 425 -\n 426  \n 427  \n 428  ",
            "\t/**\n\t * Accept offers as advised by the launch coordinator.\n\t *\n\t * Acceptance is routed through the RM to update the persistent state before\n\t * forwarding the message to Mesos.\n\t */\n\tprivate void acceptOffers(AcceptOffers msg) {\n\n\t\ttry {\n\t\t\tList<TaskMonitor.TaskGoalStateUpdated> toMonitor = new ArrayList<>(msg.operations().size());\n\n\t\t\t// transition the persistent state of some tasks to Launched\n\t\t\tfor (Protos.Offer.Operation op : msg.operations()) {\n\t\t\t\tif (op.getType() != Protos.Offer.Operation.Type.LAUNCH) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tfor (Protos.TaskInfo info : op.getLaunch().getTaskInfosList()) {\n\t\t\t\t\tMesosWorkerStore.Worker worker = workersInNew.remove(extractResourceID(info.getTaskId()));\n\t\t\t\t\tassert (worker != null);\n\n\t\t\t\t\tworker = worker.launchWorker(info.getSlaveId(), msg.hostname());\n\t\t\t\t\tworkerStore.putWorker(worker);\n\t\t\t\t\tworkersInLaunch.put(extractResourceID(worker.taskID()), worker);\n\n\t\t\t\t\tLOG.info(\"Launching Mesos task {} on host {}.\",\n\t\t\t\t\t\tworker.taskID().getValue(), worker.hostname().get());\n\n\t\t\t\t\ttoMonitor.add(new TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker)));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// tell the task router about the new plans\n\t\t\tfor (TaskMonitor.TaskGoalStateUpdated update : toMonitor) {\n\t\t\t\ttaskRouter.tell(update, self());\n\t\t\t}\n\n\t\t\t// send the acceptance message to Mesos\n\t\t\tschedulerDriver.acceptOffers(msg.offerIds(), msg.operations(), msg.filters());\n\t\t}\n\t\tcatch(Exception ex) {\n\t\t\tfatalError(\"unable to accept offers\", ex);\n\t\t}\n\t}",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423 +\n 424  \n 425  \n 426  ",
            "\t/**\n\t * Accept offers as advised by the launch coordinator.\n\t *\n\t * <p>Acceptance is routed through the RM to update the persistent state before\n\t * forwarding the message to Mesos.\n\t */\n\tprivate void acceptOffers(AcceptOffers msg) {\n\n\t\ttry {\n\t\t\tList<TaskMonitor.TaskGoalStateUpdated> toMonitor = new ArrayList<>(msg.operations().size());\n\n\t\t\t// transition the persistent state of some tasks to Launched\n\t\t\tfor (Protos.Offer.Operation op : msg.operations()) {\n\t\t\t\tif (op.getType() != Protos.Offer.Operation.Type.LAUNCH) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tfor (Protos.TaskInfo info : op.getLaunch().getTaskInfosList()) {\n\t\t\t\t\tMesosWorkerStore.Worker worker = workersInNew.remove(extractResourceID(info.getTaskId()));\n\t\t\t\t\tassert (worker != null);\n\n\t\t\t\t\tworker = worker.launchWorker(info.getSlaveId(), msg.hostname());\n\t\t\t\t\tworkerStore.putWorker(worker);\n\t\t\t\t\tworkersInLaunch.put(extractResourceID(worker.taskID()), worker);\n\n\t\t\t\t\tLOG.info(\"Launching Mesos task {} on host {}.\",\n\t\t\t\t\t\tworker.taskID().getValue(), worker.hostname().get());\n\n\t\t\t\t\ttoMonitor.add(new TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker)));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// tell the task router about the new plans\n\t\t\tfor (TaskMonitor.TaskGoalStateUpdated update : toMonitor) {\n\t\t\t\ttaskRouter.tell(update, self());\n\t\t\t}\n\n\t\t\t// send the acceptance message to Mesos\n\t\t\tschedulerDriver.acceptOffers(msg.offerIds(), msg.operations(), msg.filters());\n\t\t} catch (Exception ex) {\n\t\t\tfatalError(\"unable to accept offers\", ex);\n\t\t}\n\t}"
        ],
        [
            "MesosTaskManagerParametersTest::givenOneConstraintInConfigShouldBeParsed()",
            " 103  \n 104 -\n 105 -\n 106 -\n 107 -\n 108 -\n 109 -\n 110 -\n 111 -\n 112 -\n 113 -\n 114 -\n 115 -",
            "    @Test\n    public void givenOneConstraintInConfigShouldBeParsed() throws Exception {\n\n        MesosTaskManagerParameters mesosTaskManagerParameters = MesosTaskManagerParameters.create(withHardHostAttrConstraintConfiguration(\"cluster:foo\"));\n        assertThat(mesosTaskManagerParameters.constraints().size(), is(1));\n        ConstraintEvaluator firstConstraintEvaluator = new HostAttrValueConstraint(\"cluster\", new Func1<String, String>() {\n            @Override\n            public String call(String s) {\n                return \"foo\";\n            }\n        });\n        assertThat(mesosTaskManagerParameters.constraints().get(0).getName(), is(firstConstraintEvaluator.getName()));\n    }",
            " 109  \n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121 +",
            "\t@Test\n\tpublic void givenOneConstraintInConfigShouldBeParsed() throws Exception {\n\n\t\tMesosTaskManagerParameters mesosTaskManagerParameters = MesosTaskManagerParameters.create(withHardHostAttrConstraintConfiguration(\"cluster:foo\"));\n\t\tassertThat(mesosTaskManagerParameters.constraints().size(), is(1));\n\t\tConstraintEvaluator firstConstraintEvaluator = new HostAttrValueConstraint(\"cluster\", new Func1<String, String>() {\n\t\t\t@Override\n\t\t\tpublic String call(String s) {\n\t\t\t\treturn \"foo\";\n\t\t\t}\n\t\t});\n\t\tassertThat(mesosTaskManagerParameters.constraints().get(0).getName(), is(firstConstraintEvaluator.getName()));\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testRequestNewWorkers()",
            " 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 -\n 446 -\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  ",
            "\t/**\n\t * Test request for new workers.\n\t */\n\t@Test\n\tpublic void testRequestNewWorkers() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\t// set the target pool size\n\t\t\t\t\t\twhen(workerStore.newTaskID()).thenReturn(task1).thenThrow(new AssertionFailedError());\n\t\t\t\t\t\tresourceManager.tell(new SetWorkerPoolSize(1), jobManager);\n\n\t\t\t\t\t\t// verify that a new worker was persisted, the internal state was updated, the task router was notified,\n\t\t\t\t\t\t// and the launch coordinator was asked to launch a task\n\t\t\t\t\t\tMesosWorkerStore.Worker expected = MesosWorkerStore.Worker.newWorker(task1);\n\t\t\t\t\t\tverify(workerStore).putWorker(expected);\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInNew, hasEntry(extractResourceID(task1), expected));\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(TaskMonitor.TaskGoalStateUpdated.class);\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(LaunchCoordinator.Launch.class);\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472 +\n 473  \n 474  \n 475  \n 476  \n 477  \n 478  ",
            "\t/**\n\t * Test request for new workers.\n\t */\n\t@Test\n\tpublic void testRequestNewWorkers() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\t// set the target pool size\n\t\t\t\t\t\twhen(workerStore.newTaskID()).thenReturn(task1).thenThrow(new AssertionFailedError());\n\t\t\t\t\t\tresourceManager.tell(new SetWorkerPoolSize(1), jobManager);\n\n\t\t\t\t\t\t// verify that a new worker was persisted, the internal state was updated, the task router was notified,\n\t\t\t\t\t\t// and the launch coordinator was asked to launch a task\n\t\t\t\t\t\tMesosWorkerStore.Worker expected = MesosWorkerStore.Worker.newWorker(task1);\n\t\t\t\t\t\tverify(workerStore).putWorker(expected);\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInNew, hasEntry(extractResourceID(task1), expected));\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(TaskMonitor.TaskGoalStateUpdated.class);\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(LaunchCoordinator.Launch.class);\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosTaskManagerParameters::bootstrapCommand()",
            " 182  \n 183  \n 184  \n 185 -",
            "\t/**\n \t * Get the bootstrap command.\n \t */\n\tpublic Option<String> bootstrapCommand() { return bootstrapCommand;\t}\t",
            " 185  \n 186  \n 187  \n 188 +\n 189 +\n 190 +",
            "\t/**\n\t * Get the bootstrap command.\n\t */\n\tpublic Option<String> bootstrapCommand() {\n\t\treturn bootstrapCommand;\n\t}"
        ],
        [
            "ZooKeeperMesosWorkerStore::removeWorker(Protos)",
            " 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 -\n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  ",
            "\t@Override\n\tpublic boolean removeWorker(Protos.TaskID taskID) throws Exception {\n\t\tcheckNotNull(taskID, \"taskID\");\n\t\tString path = getPathForWorker(taskID);\n\t\tsynchronized (startStopLock) {\n\t\t\tverifyIsRunning();\n\n\t\t\tif(workersInZooKeeper.exists(path) == -1) {\n\t\t\t\tLOG.debug(\"No such worker {} in ZooKeeper.\", taskID);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tworkersInZooKeeper.releaseAndTryRemove(path);\n\t\t\tLOG.debug(\"Removed worker {} from ZooKeeper.\", taskID);\n\t\t\treturn true;\n\t\t}\n\t}",
            " 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242 +\n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  ",
            "\t@Override\n\tpublic boolean removeWorker(Protos.TaskID taskID) throws Exception {\n\t\tcheckNotNull(taskID, \"taskID\");\n\t\tString path = getPathForWorker(taskID);\n\t\tsynchronized (startStopLock) {\n\t\t\tverifyIsRunning();\n\n\t\t\tif (workersInZooKeeper.exists(path) == -1) {\n\t\t\t\tLOG.debug(\"No such worker {} in ZooKeeper.\", taskID);\n\t\t\t\treturn false;\n\t\t\t}\n\n\t\t\tworkersInZooKeeper.releaseAndTryRemove(path);\n\t\t\tLOG.debug(\"Removed worker {} from ZooKeeper.\", taskID);\n\t\t\treturn true;\n\t\t}\n\t}"
        ],
        [
            "LaunchableMesosWorker::launch(Protos,TaskAssignmentResult)",
            " 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 -\n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274 -\n 275  \n 276 -\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288 -\n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 -\n 304  \n 305  ",
            "\t/**\n\t * Construct the TaskInfo needed to launch the worker.\n\t * @param slaveId the assigned slave.\n\t * @param assignment the assignment details.\n\t * @return a fully-baked TaskInfo.\n\t */\n\t@Override\n\tpublic Protos.TaskInfo launch(Protos.SlaveID slaveId, TaskAssignmentResult assignment) {\n\n\t\tContaineredTaskManagerParameters tmParams = params.containeredParameters();\n\n\t\tfinal Configuration dynamicProperties = new Configuration();\n\n\t\t// incorporate the dynamic properties set by the template\n\t\tdynamicProperties.addAll(containerSpec.getDynamicConfiguration());\n\n\t\t// build a TaskInfo with assigned resources, environment variables, etc\n\t\tfinal Protos.TaskInfo.Builder taskInfo = Protos.TaskInfo.newBuilder()\n\t\t\t.setSlaveId(slaveId)\n\t\t\t.setTaskId(taskID)\n\t\t\t.setName(taskID.getValue())\n\t\t\t.addResources(scalar(\"cpus\", assignment.getRequest().getCPUs()))\n\t\t\t.addResources(scalar(\"mem\", assignment.getRequest().getMemory()));\n\n\t\tfinal Protos.CommandInfo.Builder cmd = taskInfo.getCommandBuilder();\n\t\tfinal Protos.Environment.Builder env = cmd.getEnvironmentBuilder();\n\t\tfinal StringBuilder jvmArgs = new StringBuilder();\n\n\t\t//configure task manager hostname property if hostname override property is supplied\n\t\tOption<String> taskManagerHostnameOption = params.getTaskManagerHostname();\n\n\t\tif(taskManagerHostnameOption.isDefined()) {\n\t\t\t// replace the TASK_ID pattern by the actual task id value of the Mesos task\n\t\t\tfinal String taskManagerHostname = MesosTaskManagerParameters.TASK_ID_PATTERN\n\t\t\t\t.matcher(taskManagerHostnameOption.get())\n\t\t\t\t.replaceAll(Matcher.quoteReplacement(taskID.getValue()));\n\n\t\t\tdynamicProperties.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, taskManagerHostname);\n\t\t}\n\n\t\t// use the assigned ports for the TM\n\t\tif (assignment.getAssignedPorts().size() < TM_PORT_KEYS.length) {\n\t\t\tthrow new IllegalArgumentException(\"unsufficient # of ports assigned\");\n\t\t}\n\t\tfor (int i = 0; i < TM_PORT_KEYS.length; i++) {\n\t\t\tint port = assignment.getAssignedPorts().get(i);\n\t\t\tString key = TM_PORT_KEYS[i];\n\t\t\ttaskInfo.addResources(ranges(\"ports\", range(port, port)));\n\t\t\tdynamicProperties.setInteger(key, port);\n\t\t}\n\n\t\t// ship additional files\n\t\tfor(ContainerSpecification.Artifact artifact : containerSpec.getArtifacts()) {\n\t\t\tcmd.addUris(Utils.uri(resolver, artifact));\n\t\t}\n\n\t\t// propagate environment variables\n\t\tfor (Map.Entry<String, String> entry : params.containeredParameters().taskManagerEnv().entrySet()) {\n\t\t\tenv.addVariables(variable(entry.getKey(), entry.getValue()));\n\t\t}\n\t\tfor (Map.Entry<String, String> entry : containerSpec.getEnvironmentVariables().entrySet()) {\n\t\t\tenv.addVariables(variable(entry.getKey(), entry.getValue()));\n\t\t}\n\n\t\t// propagate the Mesos task ID to the TM\n\t\tenv.addVariables(variable(MesosConfigKeys.ENV_FLINK_CONTAINER_ID, taskInfo.getTaskId().getValue()));\n\n\t\t// finalize the memory parameters\n\t\tjvmArgs.append(\" -Xms\").append(tmParams.taskManagerHeapSizeMB()).append(\"m\");\n\t\tjvmArgs.append(\" -Xmx\").append(tmParams.taskManagerHeapSizeMB()).append(\"m\");\n\t\tif (tmParams.taskManagerDirectMemoryLimitMB() >= 0) {\n\t\t\tjvmArgs.append(\" -XX:MaxDirectMemorySize=\").append(tmParams.taskManagerDirectMemoryLimitMB()).append(\"m\");\n\t\t}\n\n\t\t// pass dynamic system properties\n\t\tjvmArgs.append(' ').append(\n\t\t\tContainerSpecification.formatSystemProperties(containerSpec.getSystemProperties()));\n\n\t\t// finalize JVM args\n\t\tenv.addVariables(variable(MesosConfigKeys.ENV_JVM_ARGS, jvmArgs.toString()));\n\n\t\t// populate TASK_NAME and FRAMEWORK_NAME environment variables to the TM container\n\t\tenv.addVariables(variable(MesosConfigKeys.ENV_TASK_NAME, taskInfo.getTaskId().getValue()));\n\t\tenv.addVariables(variable(MesosConfigKeys.ENV_FRAMEWORK_NAME, mesosConfiguration.frameworkInfo().getName()));\n\n\t\t// build the launch command w/ dynamic application properties\n\t\tOption<String> bootstrapCmdOption = params.bootstrapCommand();\n\n\t\tfinal String bootstrapCommand = bootstrapCmdOption.isDefined() ? bootstrapCmdOption.get() + \" && \" : \"\";\n\t\tfinal String launchCommand = bootstrapCommand + \"$FLINK_HOME/bin/mesos-taskmanager.sh \" + ContainerSpecification.formatSystemProperties(dynamicProperties);\n\n\t\tcmd.setValue(launchCommand);\n\n\t\t// build the container info\n\t\tProtos.ContainerInfo.Builder containerInfo = Protos.ContainerInfo.newBuilder();\n\t\t// in event that no docker image or mesos image name is specified, we must still\n\t\t// set type to MESOS\n\t\tcontainerInfo.setType(Protos.ContainerInfo.Type.MESOS);\n\t\tswitch(params.containerType()) {\n\t\t\tcase MESOS:\n\t\t\t\tif(params.containerImageName().isDefined()) {\n\t\t\t\t\tcontainerInfo\n\t\t\t\t\t\t.setMesos(Protos.ContainerInfo.MesosInfo.newBuilder()\n\t\t\t\t\t\t\t.setImage(Protos.Image.newBuilder()\n\t\t\t\t\t\t\t\t.setType(Protos.Image.Type.DOCKER)\n\t\t\t\t\t\t\t\t.setDocker(Protos.Image.Docker.newBuilder()\n\t\t\t\t\t\t\t\t\t.setName(params.containerImageName().get()))));\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\tcase DOCKER:\n\t\t\t\tassert(params.containerImageName().isDefined());\n\t\t\t\t\tcontainerInfo\n\t\t\t\t\t.setType(Protos.ContainerInfo.Type.DOCKER)\n\t\t\t\t\t.setDocker(Protos.ContainerInfo.DockerInfo.newBuilder()\n\t\t\t\t\t\t.setNetwork(Protos.ContainerInfo.DockerInfo.Network.HOST)\n\t\t\t\t\t\t.setImage(params.containerImageName().get()));\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalStateException(\"unsupported container type\");\n\t\t}\n\n\t\t// add any volumes to the containerInfo\n\t\tcontainerInfo.addAllVolumes(params.containerVolumes());\n\t\ttaskInfo.setContainer(containerInfo);\n\n\n\t\treturn taskInfo.build();\n\t}",
            " 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 +\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276 +\n 277  \n 278 +\n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290 +\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "\t/**\n\t * Construct the TaskInfo needed to launch the worker.\n\t * @param slaveId the assigned slave.\n\t * @param assignment the assignment details.\n\t * @return a fully-baked TaskInfo.\n\t */\n\t@Override\n\tpublic Protos.TaskInfo launch(Protos.SlaveID slaveId, TaskAssignmentResult assignment) {\n\n\t\tContaineredTaskManagerParameters tmParams = params.containeredParameters();\n\n\t\tfinal Configuration dynamicProperties = new Configuration();\n\n\t\t// incorporate the dynamic properties set by the template\n\t\tdynamicProperties.addAll(containerSpec.getDynamicConfiguration());\n\n\t\t// build a TaskInfo with assigned resources, environment variables, etc\n\t\tfinal Protos.TaskInfo.Builder taskInfo = Protos.TaskInfo.newBuilder()\n\t\t\t.setSlaveId(slaveId)\n\t\t\t.setTaskId(taskID)\n\t\t\t.setName(taskID.getValue())\n\t\t\t.addResources(scalar(\"cpus\", assignment.getRequest().getCPUs()))\n\t\t\t.addResources(scalar(\"mem\", assignment.getRequest().getMemory()));\n\n\t\tfinal Protos.CommandInfo.Builder cmd = taskInfo.getCommandBuilder();\n\t\tfinal Protos.Environment.Builder env = cmd.getEnvironmentBuilder();\n\t\tfinal StringBuilder jvmArgs = new StringBuilder();\n\n\t\t//configure task manager hostname property if hostname override property is supplied\n\t\tOption<String> taskManagerHostnameOption = params.getTaskManagerHostname();\n\n\t\tif (taskManagerHostnameOption.isDefined()) {\n\t\t\t// replace the TASK_ID pattern by the actual task id value of the Mesos task\n\t\t\tfinal String taskManagerHostname = MesosTaskManagerParameters.TASK_ID_PATTERN\n\t\t\t\t.matcher(taskManagerHostnameOption.get())\n\t\t\t\t.replaceAll(Matcher.quoteReplacement(taskID.getValue()));\n\n\t\t\tdynamicProperties.setString(ConfigConstants.TASK_MANAGER_HOSTNAME_KEY, taskManagerHostname);\n\t\t}\n\n\t\t// use the assigned ports for the TM\n\t\tif (assignment.getAssignedPorts().size() < TM_PORT_KEYS.length) {\n\t\t\tthrow new IllegalArgumentException(\"unsufficient # of ports assigned\");\n\t\t}\n\t\tfor (int i = 0; i < TM_PORT_KEYS.length; i++) {\n\t\t\tint port = assignment.getAssignedPorts().get(i);\n\t\t\tString key = TM_PORT_KEYS[i];\n\t\t\ttaskInfo.addResources(ranges(\"ports\", range(port, port)));\n\t\t\tdynamicProperties.setInteger(key, port);\n\t\t}\n\n\t\t// ship additional files\n\t\tfor (ContainerSpecification.Artifact artifact : containerSpec.getArtifacts()) {\n\t\t\tcmd.addUris(Utils.uri(resolver, artifact));\n\t\t}\n\n\t\t// propagate environment variables\n\t\tfor (Map.Entry<String, String> entry : params.containeredParameters().taskManagerEnv().entrySet()) {\n\t\t\tenv.addVariables(variable(entry.getKey(), entry.getValue()));\n\t\t}\n\t\tfor (Map.Entry<String, String> entry : containerSpec.getEnvironmentVariables().entrySet()) {\n\t\t\tenv.addVariables(variable(entry.getKey(), entry.getValue()));\n\t\t}\n\n\t\t// propagate the Mesos task ID to the TM\n\t\tenv.addVariables(variable(MesosConfigKeys.ENV_FLINK_CONTAINER_ID, taskInfo.getTaskId().getValue()));\n\n\t\t// finalize the memory parameters\n\t\tjvmArgs.append(\" -Xms\").append(tmParams.taskManagerHeapSizeMB()).append(\"m\");\n\t\tjvmArgs.append(\" -Xmx\").append(tmParams.taskManagerHeapSizeMB()).append(\"m\");\n\t\tif (tmParams.taskManagerDirectMemoryLimitMB() >= 0) {\n\t\t\tjvmArgs.append(\" -XX:MaxDirectMemorySize=\").append(tmParams.taskManagerDirectMemoryLimitMB()).append(\"m\");\n\t\t}\n\n\t\t// pass dynamic system properties\n\t\tjvmArgs.append(' ').append(\n\t\t\tContainerSpecification.formatSystemProperties(containerSpec.getSystemProperties()));\n\n\t\t// finalize JVM args\n\t\tenv.addVariables(variable(MesosConfigKeys.ENV_JVM_ARGS, jvmArgs.toString()));\n\n\t\t// populate TASK_NAME and FRAMEWORK_NAME environment variables to the TM container\n\t\tenv.addVariables(variable(MesosConfigKeys.ENV_TASK_NAME, taskInfo.getTaskId().getValue()));\n\t\tenv.addVariables(variable(MesosConfigKeys.ENV_FRAMEWORK_NAME, mesosConfiguration.frameworkInfo().getName()));\n\n\t\t// build the launch command w/ dynamic application properties\n\t\tOption<String> bootstrapCmdOption = params.bootstrapCommand();\n\n\t\tfinal String bootstrapCommand = bootstrapCmdOption.isDefined() ? bootstrapCmdOption.get() + \" && \" : \"\";\n\t\tfinal String launchCommand = bootstrapCommand + \"$FLINK_HOME/bin/mesos-taskmanager.sh \" + ContainerSpecification.formatSystemProperties(dynamicProperties);\n\n\t\tcmd.setValue(launchCommand);\n\n\t\t// build the container info\n\t\tProtos.ContainerInfo.Builder containerInfo = Protos.ContainerInfo.newBuilder();\n\t\t// in event that no docker image or mesos image name is specified, we must still\n\t\t// set type to MESOS\n\t\tcontainerInfo.setType(Protos.ContainerInfo.Type.MESOS);\n\t\tswitch (params.containerType()) {\n\t\t\tcase MESOS:\n\t\t\t\tif (params.containerImageName().isDefined()) {\n\t\t\t\t\tcontainerInfo\n\t\t\t\t\t\t.setMesos(Protos.ContainerInfo.MesosInfo.newBuilder()\n\t\t\t\t\t\t\t.setImage(Protos.Image.newBuilder()\n\t\t\t\t\t\t\t\t.setType(Protos.Image.Type.DOCKER)\n\t\t\t\t\t\t\t\t.setDocker(Protos.Image.Docker.newBuilder()\n\t\t\t\t\t\t\t\t\t.setName(params.containerImageName().get()))));\n\t\t\t\t}\n\t\t\t\tbreak;\n\n\t\t\tcase DOCKER:\n\t\t\t\tassert(params.containerImageName().isDefined());\n\t\t\t\tcontainerInfo\n\t\t\t\t\t.setType(Protos.ContainerInfo.Type.DOCKER)\n\t\t\t\t\t.setDocker(Protos.ContainerInfo.DockerInfo.newBuilder()\n\t\t\t\t\t\t.setNetwork(Protos.ContainerInfo.DockerInfo.Network.HOST)\n\t\t\t\t\t\t.setImage(params.containerImageName().get()));\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalStateException(\"unsupported container type\");\n\t\t}\n\n\t\t// add any volumes to the containerInfo\n\t\tcontainerInfo.addAllVolumes(params.containerVolumes());\n\t\ttaskInfo.setContainer(containerInfo);\n\n\t\treturn taskInfo.build();\n\t}"
        ],
        [
            "ZooKeeperMesosWorkerStore::stop(boolean)",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 -\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  ",
            "\tpublic void stop(boolean cleanup) throws Exception {\n\t\tsynchronized (startStopLock) {\n\t\t\tif (isRunning) {\n\t\t\t\tframeworkIdInZooKeeper.close();\n\t\t\t\ttotalTaskCountInZooKeeper.close();\n\n\t\t\t\tif(cleanup) {\n\t\t\t\t\tworkersInZooKeeper.releaseAndTryRemoveAll();\n\t\t\t\t}\n\n\t\t\t\tisRunning = false;\n\t\t\t}\n\t\t}\n\t}",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "\tpublic void stop(boolean cleanup) throws Exception {\n\t\tsynchronized (startStopLock) {\n\t\t\tif (isRunning) {\n\t\t\t\tframeworkIdInZooKeeper.close();\n\t\t\t\ttotalTaskCountInZooKeeper.close();\n\n\t\t\t\tif (cleanup) {\n\t\t\t\t\tworkersInZooKeeper.releaseAndTryRemoveAll();\n\t\t\t\t}\n\n\t\t\t\tisRunning = false;\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "MesosApplicationMasterRunner::runPrivileged(Configuration,Configuration)",
            " 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321 -\n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331 -\n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383 -\n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399 -\n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407 -\n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "\t/**\n\t * The main work method, must run as a privileged action.\n\t *\n\t * @return The return code for the Java process.\n\t */\n\tprotected int runPrivileged(Configuration config, Configuration dynamicProperties) {\n\n\t\tActorSystem actorSystem = null;\n\t\tWebMonitor webMonitor = null;\n\t\tMesosArtifactServer artifactServer = null;\n\t\tScheduledExecutorService futureExecutor = null;\n\t\tExecutorService ioExecutor = null;\n\t\tMesosServices mesosServices = null;\n\t\tHighAvailabilityServices highAvailabilityServices = null;\n\n\t\ttry {\n\t\t\t// ------- (1) load and parse / validate all configurations -------\n\n\t\t\tfinal String appMasterHostname = config.getString(\n\t\t\t\tJobManagerOptions.ADDRESS,\n\t\t\t\tInetAddress.getLocalHost().getHostName());\n\n\t\t\tLOG.info(\"App Master Hostname to use: {}\", appMasterHostname);\n\n\t\t\t// Mesos configuration\n\t\t\tfinal MesosConfiguration mesosConfig = createMesosConfig(config, appMasterHostname);\n\n\t\t\t// JM configuration\n\t\t\tint numberProcessors = Hardware.getNumberCPUCores();\n\n\t\t\tfutureExecutor = Executors.newScheduledThreadPool(\n\t\t\t\tnumberProcessors,\n\t\t\t\tnew ExecutorThreadFactory(\"mesos-jobmanager-future\"));\n\n\t\t\tioExecutor = Executors.newFixedThreadPool(\n\t\t\t\tnumberProcessors,\n\t\t\t\tnew ExecutorThreadFactory(\"mesos-jobmanager-io\"));\n\n\t\t\tmesosServices = MesosServicesUtils.createMesosServices(config);\n\n\t\t\t// TM configuration\n\t\t\tfinal MesosTaskManagerParameters taskManagerParameters = MesosTaskManagerParameters.create(config);\n\n\t\t\tLOG.info(\"TaskManagers will be created with {} task slots\",\n\t\t\t\ttaskManagerParameters.containeredParameters().numSlots());\n\t\t\tLOG.info(\"TaskManagers will be started with container size {} MB, JVM heap size {} MB, \" +\n\t\t\t\t\t\"JVM direct memory limit {} MB, {} cpus\",\n\t\t\t\ttaskManagerParameters.containeredParameters().taskManagerTotalMemoryMB(),\n\t\t\t\ttaskManagerParameters.containeredParameters().taskManagerHeapSizeMB(),\n\t\t\t\ttaskManagerParameters.containeredParameters().taskManagerDirectMemoryLimitMB(),\n\t\t\t\ttaskManagerParameters.cpus());\n\n\t\t\t// JM endpoint, which should be explicitly configured based on acquired net resources\n\t\t\tfinal int listeningPort = config.getInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_IPC_PORT);\n\t\t\tcheckState(listeningPort >= 0 && listeningPort <= 65536, \"Config parameter \\\"\" +\n\t\t\t\tConfigConstants.JOB_MANAGER_IPC_PORT_KEY + \"\\\" is invalid, it must be between 0 and 65536\");\n\n\t\t\t// ----------------- (2) start the actor system -------------------\n\n\t\t\t// try to start the actor system, JobManager and JobManager actor system\n\t\t\t// using the configured address and ports\n\t\t\tactorSystem = BootstrapTools.startActorSystem(config, appMasterHostname, listeningPort, LOG);\n\n\t\t\tAddress address = AkkaUtils.getAddress(actorSystem);\n\t\t\tfinal String akkaHostname = address.host().get();\n\t\t\tfinal int akkaPort = (Integer) address.port().get();\n\n\t\t\tLOG.info(\"Actor system bound to hostname {}.\", akkaHostname);\n\n\t\t\t// try to start the artifact server\n\t\t\tLOG.debug(\"Starting Artifact Server\");\n\t\t\tfinal int artifactServerPort = config.getInteger(ConfigConstants.MESOS_ARTIFACT_SERVER_PORT_KEY,\n\t\t\t\tConfigConstants.DEFAULT_MESOS_ARTIFACT_SERVER_PORT);\n\t\t\tfinal String artifactServerPrefix = UUID.randomUUID().toString();\n\t\t\tartifactServer = new MesosArtifactServer(artifactServerPrefix, akkaHostname, artifactServerPort, config);\n\n\t\t\t// ----------------- (3) Generate the configuration for the TaskManagers -------------------\n\n\t\t\t// generate a container spec which conveys the artifacts/vars needed to launch a TM\n\t\t\tContainerSpecification taskManagerContainerSpec = new ContainerSpecification();\n\n\t\t\t// propagate the AM dynamic configuration to the TM\n\t\t\ttaskManagerContainerSpec.getDynamicConfiguration().addAll(dynamicProperties);\n\n\t\t\t// propagate newly-generated configuration elements\n\t\t\tfinal Configuration taskManagerConfig = BootstrapTools.generateTaskManagerConfiguration(\n\t\t\t\tnew Configuration(), akkaHostname, akkaPort, taskManagerParameters.containeredParameters().numSlots(),\n\t\t\t\tTASKMANAGER_REGISTRATION_TIMEOUT);\n\t\t\ttaskManagerContainerSpec.getDynamicConfiguration().addAll(taskManagerConfig);\n\n\t\t\t// apply the overlays\n\t\t\tapplyOverlays(config, taskManagerContainerSpec);\n\n\t\t\t// configure the artifact server to serve the specified artifacts\n\t\t\tconfigureArtifactServer(artifactServer, taskManagerContainerSpec);\n\n\t\t\t// ----------------- (4) start the actors -------------------\n\n\t\t\t// 1) JobManager & Archive (in non-HA case, the leader service takes this)\n\t\t\t// 2) Web Monitor (we need its port to register)\n\t\t\t// 3) Resource Master for Mesos\n\t\t\t// 4) Process reapers for the JobManager and Resource Master\n\n\t\t\t// 0: Start the JobManager services\n\t\t\thighAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\t\tconfig,\n\t\t\t\tioExecutor,\n\t\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\t\t// 1: the JobManager\n\t\t\tLOG.debug(\"Starting JobManager actor\");\n\n\t\t\t// we start the JobManager with its standard name\n\t\t\tActorRef jobManager = JobManager.startJobManagerActors(\n\t\t\t\tconfig,\n\t\t\t\tactorSystem,\n\t\t\t\tfutureExecutor,\n\t\t\t\tioExecutor,\n\t\t\t\thighAvailabilityServices,\n\t\t\t\tOption.apply(JobMaster.JOB_MANAGER_NAME),\n\t\t\t\tOption.apply(JobMaster.ARCHIVE_NAME),\n\t\t\t\tgetJobManagerClass(),\n\t\t\t\tgetArchivistClass())._1();\n\n\n\t\t\t// 2: the web monitor\n\t\t\tLOG.debug(\"Starting Web Frontend\");\n\n\t\t\twebMonitor = BootstrapTools.startWebMonitorIfConfigured(\n\t\t\t\tconfig,\n\t\t\t\thighAvailabilityServices,\n\t\t\t\tactorSystem,\n\t\t\t\tjobManager,\n\t\t\t\tLOG);\n\t\t\tif(webMonitor != null) {\n\t\t\t\tfinal URL webMonitorURL = new URL(\"http\", appMasterHostname, webMonitor.getServerPort(), \"/\");\n\t\t\t\tmesosConfig.frameworkInfo().setWebuiUrl(webMonitorURL.toExternalForm());\n\t\t\t}\n\n\t\t\t// 3: Flink's Mesos ResourceManager\n\t\t\tLOG.debug(\"Starting Mesos Flink Resource Manager\");\n\n\t\t\t// create the worker store to persist task information across restarts\n\t\t\tMesosWorkerStore workerStore = mesosServices.createMesosWorkerStore(\n\t\t\t\tconfig,\n\t\t\t\tioExecutor);\n\n\t\t\tProps resourceMasterProps = MesosFlinkResourceManager.createActorProps(\n\t\t\t\tgetResourceManagerClass(),\n\t\t\t\tconfig,\n\t\t\t\tmesosConfig,\n\t\t\t\tworkerStore,\n\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\ttaskManagerParameters,\n\t\t\t\ttaskManagerContainerSpec,\n\t\t\t\tartifactServer,\n\t\t\t\tLOG);\n\n\t\t\tActorRef resourceMaster = actorSystem.actorOf(resourceMasterProps, \"Mesos_Resource_Master\");\n\n\t\t\t// 4: Process reapers\n\t\t\t// The process reapers ensure that upon unexpected actor death, the process exits\n\t\t\t// and does not stay lingering around unresponsive\n\n\t\t\tLOG.debug(\"Starting process reapers for JobManager\");\n\n\t\t\tactorSystem.actorOf(\n\t\t\t\tProps.create(ProcessReaper.class, resourceMaster, LOG, ACTOR_DIED_EXIT_CODE),\n\t\t\t\t\"Mesos_Resource_Master_Process_Reaper\");\n\n\t\t\tactorSystem.actorOf(\n\t\t\t\tProps.create(ProcessReaper.class, jobManager, LOG, ACTOR_DIED_EXIT_CODE),\n\t\t\t\t\"JobManager_Process_Reaper\");\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"Mesos JobManager initialization failed\", t);\n\n\t\t\tif (webMonitor != null) {\n\t\t\t\ttry {\n\t\t\t\t\twebMonitor.stop();\n\t\t\t\t} catch (Throwable ignored) {\n\t\t\t\t\tLOG.warn(\"Failed to stop the web frontend\", ignored);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif(artifactServer != null) {\n\t\t\t\ttry {\n\t\t\t\t\tartifactServer.stop();\n\t\t\t\t} catch (Throwable ignored) {\n\t\t\t\t\tLOG.error(\"Failed to stop the artifact server\", ignored);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (actorSystem != null) {\n\t\t\t\ttry {\n\t\t\t\t\tactorSystem.shutdown();\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error shutting down actor system\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif(futureExecutor != null) {\n\t\t\t\ttry {\n\t\t\t\t\tfutureExecutor.shutdownNow();\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error shutting down future executor\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif(ioExecutor != null) {\n\t\t\t\ttry {\n\t\t\t\t\tioExecutor.shutdownNow();\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error shutting down io executor\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (mesosServices != null) {\n\t\t\t\ttry {\n\t\t\t\t\tmesosServices.close(false);\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error closing the mesos services.\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\n\t\t// everything started, we can wait until all is done or the process is killed\n\t\tLOG.info(\"Mesos JobManager started\");\n\n\t\t// wait until everything is done\n\t\tactorSystem.awaitTermination();\n\n\t\t// if we get here, everything work out jolly all right, and we even exited smoothly\n\t\tif (webMonitor != null) {\n\t\t\ttry {\n\t\t\t\twebMonitor.stop();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to stop the web frontend\", t);\n\t\t\t}\n\t\t}\n\n\t\ttry {\n\t\t\tartifactServer.stop();\n\t\t} catch (Throwable t) {\n\t\t\tLOG.error(\"Failed to stop the artifact server\", t);\n\t\t}\n\n\t\tif (highAvailabilityServices != null) {\n\t\t\ttry {\n\t\t\t\thighAvailabilityServices.close();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tLOG.error(\"Could not properly stop the high availability services.\");\n\t\t\t}\n\t\t}\n\n\t\torg.apache.flink.runtime.concurrent.Executors.gracefulShutdown(\n\t\t\tAkkaUtils.getTimeout(config).toMillis(),\n\t\t\tTimeUnit.MILLISECONDS,\n\t\t\tfutureExecutor,\n\t\t\tioExecutor);\n\n\t\ttry {\n\t\t\tmesosServices.close(true);\n\t\t} catch (Throwable t) {\n\t\t\tLOG.error(\"Failed to clean up and close MesosServices.\", t);\n\t\t}\n\n\t\treturn 0;\n\t}",
            " 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330 +\n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382 +\n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398 +\n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406 +\n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  ",
            "\t/**\n\t * The main work method, must run as a privileged action.\n\t *\n\t * @return The return code for the Java process.\n\t */\n\tprotected int runPrivileged(Configuration config, Configuration dynamicProperties) {\n\n\t\tActorSystem actorSystem = null;\n\t\tWebMonitor webMonitor = null;\n\t\tMesosArtifactServer artifactServer = null;\n\t\tScheduledExecutorService futureExecutor = null;\n\t\tExecutorService ioExecutor = null;\n\t\tMesosServices mesosServices = null;\n\t\tHighAvailabilityServices highAvailabilityServices = null;\n\n\t\ttry {\n\t\t\t// ------- (1) load and parse / validate all configurations -------\n\n\t\t\tfinal String appMasterHostname = config.getString(\n\t\t\t\tJobManagerOptions.ADDRESS,\n\t\t\t\tInetAddress.getLocalHost().getHostName());\n\n\t\t\tLOG.info(\"App Master Hostname to use: {}\", appMasterHostname);\n\n\t\t\t// Mesos configuration\n\t\t\tfinal MesosConfiguration mesosConfig = createMesosConfig(config, appMasterHostname);\n\n\t\t\t// JM configuration\n\t\t\tint numberProcessors = Hardware.getNumberCPUCores();\n\n\t\t\tfutureExecutor = Executors.newScheduledThreadPool(\n\t\t\t\tnumberProcessors,\n\t\t\t\tnew ExecutorThreadFactory(\"mesos-jobmanager-future\"));\n\n\t\t\tioExecutor = Executors.newFixedThreadPool(\n\t\t\t\tnumberProcessors,\n\t\t\t\tnew ExecutorThreadFactory(\"mesos-jobmanager-io\"));\n\n\t\t\tmesosServices = MesosServicesUtils.createMesosServices(config);\n\n\t\t\t// TM configuration\n\t\t\tfinal MesosTaskManagerParameters taskManagerParameters = MesosTaskManagerParameters.create(config);\n\n\t\t\tLOG.info(\"TaskManagers will be created with {} task slots\",\n\t\t\t\ttaskManagerParameters.containeredParameters().numSlots());\n\t\t\tLOG.info(\"TaskManagers will be started with container size {} MB, JVM heap size {} MB, \" +\n\t\t\t\t\t\"JVM direct memory limit {} MB, {} cpus\",\n\t\t\t\ttaskManagerParameters.containeredParameters().taskManagerTotalMemoryMB(),\n\t\t\t\ttaskManagerParameters.containeredParameters().taskManagerHeapSizeMB(),\n\t\t\t\ttaskManagerParameters.containeredParameters().taskManagerDirectMemoryLimitMB(),\n\t\t\t\ttaskManagerParameters.cpus());\n\n\t\t\t// JM endpoint, which should be explicitly configured based on acquired net resources\n\t\t\tfinal int listeningPort = config.getInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_IPC_PORT);\n\t\t\tcheckState(listeningPort >= 0 && listeningPort <= 65536, \"Config parameter \\\"\" +\n\t\t\t\tConfigConstants.JOB_MANAGER_IPC_PORT_KEY + \"\\\" is invalid, it must be between 0 and 65536\");\n\n\t\t\t// ----------------- (2) start the actor system -------------------\n\n\t\t\t// try to start the actor system, JobManager and JobManager actor system\n\t\t\t// using the configured address and ports\n\t\t\tactorSystem = BootstrapTools.startActorSystem(config, appMasterHostname, listeningPort, LOG);\n\n\t\t\tAddress address = AkkaUtils.getAddress(actorSystem);\n\t\t\tfinal String akkaHostname = address.host().get();\n\t\t\tfinal int akkaPort = (Integer) address.port().get();\n\n\t\t\tLOG.info(\"Actor system bound to hostname {}.\", akkaHostname);\n\n\t\t\t// try to start the artifact server\n\t\t\tLOG.debug(\"Starting Artifact Server\");\n\t\t\tfinal int artifactServerPort = config.getInteger(ConfigConstants.MESOS_ARTIFACT_SERVER_PORT_KEY,\n\t\t\t\tConfigConstants.DEFAULT_MESOS_ARTIFACT_SERVER_PORT);\n\t\t\tfinal String artifactServerPrefix = UUID.randomUUID().toString();\n\t\t\tartifactServer = new MesosArtifactServer(artifactServerPrefix, akkaHostname, artifactServerPort, config);\n\n\t\t\t// ----------------- (3) Generate the configuration for the TaskManagers -------------------\n\n\t\t\t// generate a container spec which conveys the artifacts/vars needed to launch a TM\n\t\t\tContainerSpecification taskManagerContainerSpec = new ContainerSpecification();\n\n\t\t\t// propagate the AM dynamic configuration to the TM\n\t\t\ttaskManagerContainerSpec.getDynamicConfiguration().addAll(dynamicProperties);\n\n\t\t\t// propagate newly-generated configuration elements\n\t\t\tfinal Configuration taskManagerConfig = BootstrapTools.generateTaskManagerConfiguration(\n\t\t\t\tnew Configuration(), akkaHostname, akkaPort, taskManagerParameters.containeredParameters().numSlots(),\n\t\t\t\tTASKMANAGER_REGISTRATION_TIMEOUT);\n\t\t\ttaskManagerContainerSpec.getDynamicConfiguration().addAll(taskManagerConfig);\n\n\t\t\t// apply the overlays\n\t\t\tapplyOverlays(config, taskManagerContainerSpec);\n\n\t\t\t// configure the artifact server to serve the specified artifacts\n\t\t\tconfigureArtifactServer(artifactServer, taskManagerContainerSpec);\n\n\t\t\t// ----------------- (4) start the actors -------------------\n\n\t\t\t// 1) JobManager & Archive (in non-HA case, the leader service takes this)\n\t\t\t// 2) Web Monitor (we need its port to register)\n\t\t\t// 3) Resource Master for Mesos\n\t\t\t// 4) Process reapers for the JobManager and Resource Master\n\n\t\t\t// 0: Start the JobManager services\n\t\t\thighAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\t\tconfig,\n\t\t\t\tioExecutor,\n\t\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\t\t// 1: the JobManager\n\t\t\tLOG.debug(\"Starting JobManager actor\");\n\n\t\t\t// we start the JobManager with its standard name\n\t\t\tActorRef jobManager = JobManager.startJobManagerActors(\n\t\t\t\tconfig,\n\t\t\t\tactorSystem,\n\t\t\t\tfutureExecutor,\n\t\t\t\tioExecutor,\n\t\t\t\thighAvailabilityServices,\n\t\t\t\tOption.apply(JobMaster.JOB_MANAGER_NAME),\n\t\t\t\tOption.apply(JobMaster.ARCHIVE_NAME),\n\t\t\t\tgetJobManagerClass(),\n\t\t\t\tgetArchivistClass())._1();\n\n\t\t\t// 2: the web monitor\n\t\t\tLOG.debug(\"Starting Web Frontend\");\n\n\t\t\twebMonitor = BootstrapTools.startWebMonitorIfConfigured(\n\t\t\t\tconfig,\n\t\t\t\thighAvailabilityServices,\n\t\t\t\tactorSystem,\n\t\t\t\tjobManager,\n\t\t\t\tLOG);\n\t\t\tif (webMonitor != null) {\n\t\t\t\tfinal URL webMonitorURL = new URL(\"http\", appMasterHostname, webMonitor.getServerPort(), \"/\");\n\t\t\t\tmesosConfig.frameworkInfo().setWebuiUrl(webMonitorURL.toExternalForm());\n\t\t\t}\n\n\t\t\t// 3: Flink's Mesos ResourceManager\n\t\t\tLOG.debug(\"Starting Mesos Flink Resource Manager\");\n\n\t\t\t// create the worker store to persist task information across restarts\n\t\t\tMesosWorkerStore workerStore = mesosServices.createMesosWorkerStore(\n\t\t\t\tconfig,\n\t\t\t\tioExecutor);\n\n\t\t\tProps resourceMasterProps = MesosFlinkResourceManager.createActorProps(\n\t\t\t\tgetResourceManagerClass(),\n\t\t\t\tconfig,\n\t\t\t\tmesosConfig,\n\t\t\t\tworkerStore,\n\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\ttaskManagerParameters,\n\t\t\t\ttaskManagerContainerSpec,\n\t\t\t\tartifactServer,\n\t\t\t\tLOG);\n\n\t\t\tActorRef resourceMaster = actorSystem.actorOf(resourceMasterProps, \"Mesos_Resource_Master\");\n\n\t\t\t// 4: Process reapers\n\t\t\t// The process reapers ensure that upon unexpected actor death, the process exits\n\t\t\t// and does not stay lingering around unresponsive\n\n\t\t\tLOG.debug(\"Starting process reapers for JobManager\");\n\n\t\t\tactorSystem.actorOf(\n\t\t\t\tProps.create(ProcessReaper.class, resourceMaster, LOG, ACTOR_DIED_EXIT_CODE),\n\t\t\t\t\"Mesos_Resource_Master_Process_Reaper\");\n\n\t\t\tactorSystem.actorOf(\n\t\t\t\tProps.create(ProcessReaper.class, jobManager, LOG, ACTOR_DIED_EXIT_CODE),\n\t\t\t\t\"JobManager_Process_Reaper\");\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"Mesos JobManager initialization failed\", t);\n\n\t\t\tif (webMonitor != null) {\n\t\t\t\ttry {\n\t\t\t\t\twebMonitor.stop();\n\t\t\t\t} catch (Throwable ignored) {\n\t\t\t\t\tLOG.warn(\"Failed to stop the web frontend\", ignored);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (artifactServer != null) {\n\t\t\t\ttry {\n\t\t\t\t\tartifactServer.stop();\n\t\t\t\t} catch (Throwable ignored) {\n\t\t\t\t\tLOG.error(\"Failed to stop the artifact server\", ignored);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (actorSystem != null) {\n\t\t\t\ttry {\n\t\t\t\t\tactorSystem.shutdown();\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error shutting down actor system\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (futureExecutor != null) {\n\t\t\t\ttry {\n\t\t\t\t\tfutureExecutor.shutdownNow();\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error shutting down future executor\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (ioExecutor != null) {\n\t\t\t\ttry {\n\t\t\t\t\tioExecutor.shutdownNow();\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error shutting down io executor\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (mesosServices != null) {\n\t\t\t\ttry {\n\t\t\t\t\tmesosServices.close(false);\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error closing the mesos services.\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\n\t\t// everything started, we can wait until all is done or the process is killed\n\t\tLOG.info(\"Mesos JobManager started\");\n\n\t\t// wait until everything is done\n\t\tactorSystem.awaitTermination();\n\n\t\t// if we get here, everything work out jolly all right, and we even exited smoothly\n\t\tif (webMonitor != null) {\n\t\t\ttry {\n\t\t\t\twebMonitor.stop();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to stop the web frontend\", t);\n\t\t\t}\n\t\t}\n\n\t\ttry {\n\t\t\tartifactServer.stop();\n\t\t} catch (Throwable t) {\n\t\t\tLOG.error(\"Failed to stop the artifact server\", t);\n\t\t}\n\n\t\tif (highAvailabilityServices != null) {\n\t\t\ttry {\n\t\t\t\thighAvailabilityServices.close();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tLOG.error(\"Could not properly stop the high availability services.\");\n\t\t\t}\n\t\t}\n\n\t\torg.apache.flink.runtime.concurrent.Executors.gracefulShutdown(\n\t\t\tAkkaUtils.getTimeout(config).toMillis(),\n\t\t\tTimeUnit.MILLISECONDS,\n\t\t\tfutureExecutor,\n\t\t\tioExecutor);\n\n\t\ttry {\n\t\t\tmesosServices.close(true);\n\t\t} catch (Throwable t) {\n\t\t\tLOG.error(\"Failed to clean up and close MesosServices.\", t);\n\t\t}\n\n\t\treturn 0;\n\t}"
        ],
        [
            "MesosFlinkResourceManager::initialize()",
            " 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 -\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "\t@Override\n\tprotected void initialize() throws Exception {\n\t\tLOG.info(\"Initializing Mesos resource master\");\n\n\t\tworkerStore.start();\n\n\t\t// create the scheduler driver to communicate with Mesos\n\t\tschedulerCallbackHandler = new SchedulerProxy(self());\n\n\t\t// register with Mesos\n\t\tFrameworkInfo.Builder frameworkInfo = mesosConfig.frameworkInfo()\n\t\t\t.clone()\n\t\t\t.setCheckpoint(true);\n\n\t\tOption<Protos.FrameworkID> frameworkID = workerStore.getFrameworkID();\n\t\tif(frameworkID.isEmpty()) {\n\t\t\tLOG.info(\"Registering as new framework.\");\n\t\t}\n\t\telse {\n\t\t\tLOG.info(\"Recovery scenario: re-registering using framework ID {}.\", frameworkID.get().getValue());\n\t\t\tframeworkInfo.setId(frameworkID.get());\n\t\t}\n\n\t\tMesosConfiguration initializedMesosConfig = mesosConfig.withFrameworkInfo(frameworkInfo);\n\t\tMesosConfiguration.logMesosConfig(LOG, initializedMesosConfig);\n\t\tschedulerDriver = initializedMesosConfig.createDriver(schedulerCallbackHandler, false);\n\n\t\t// create supporting actors\n\t\tconnectionMonitor = createConnectionMonitor();\n\t\tlaunchCoordinator = createLaunchCoordinator();\n\t\treconciliationCoordinator = createReconciliationCoordinator();\n\t\ttaskRouter = createTaskRouter();\n\n\t\trecoverWorkers();\n\n\t\tconnectionMonitor.tell(new ConnectionMonitor.Start(), self());\n\t\tschedulerDriver.start();\n\t}",
            " 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163 +\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  ",
            "\t@Override\n\tprotected void initialize() throws Exception {\n\t\tLOG.info(\"Initializing Mesos resource master\");\n\n\t\tworkerStore.start();\n\n\t\t// create the scheduler driver to communicate with Mesos\n\t\tschedulerCallbackHandler = new SchedulerProxy(self());\n\n\t\t// register with Mesos\n\t\tFrameworkInfo.Builder frameworkInfo = mesosConfig.frameworkInfo()\n\t\t\t.clone()\n\t\t\t.setCheckpoint(true);\n\n\t\tOption<Protos.FrameworkID> frameworkID = workerStore.getFrameworkID();\n\t\tif (frameworkID.isEmpty()) {\n\t\t\tLOG.info(\"Registering as new framework.\");\n\t\t}\n\t\telse {\n\t\t\tLOG.info(\"Recovery scenario: re-registering using framework ID {}.\", frameworkID.get().getValue());\n\t\t\tframeworkInfo.setId(frameworkID.get());\n\t\t}\n\n\t\tMesosConfiguration initializedMesosConfig = mesosConfig.withFrameworkInfo(frameworkInfo);\n\t\tMesosConfiguration.logMesosConfig(LOG, initializedMesosConfig);\n\t\tschedulerDriver = initializedMesosConfig.createDriver(schedulerCallbackHandler, false);\n\n\t\t// create supporting actors\n\t\tconnectionMonitor = createConnectionMonitor();\n\t\tlaunchCoordinator = createLaunchCoordinator();\n\t\treconciliationCoordinator = createReconciliationCoordinator();\n\t\ttaskRouter = createTaskRouter();\n\n\t\trecoverWorkers();\n\n\t\tconnectionMonitor.tell(new ConnectionMonitor.Start(), self());\n\t\tschedulerDriver.start();\n\t}"
        ],
        [
            "MesosArtifactServer::MesosArtifactServer(String,String,int,Configuration)",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 -\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172 -\n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\tpublic MesosArtifactServer(String prefix, String serverHostname, int configuredPort, Configuration config)\n\t\tthrows Exception {\n\t\tif (configuredPort < 0 || configuredPort > 0xFFFF) {\n\t\t\tthrow new IllegalArgumentException(\"File server port is invalid: \" + configuredPort);\n\t\t}\n\n\t\t// Config to enable https access to the artifact server\n\t\tboolean enableSSL = config.getBoolean(\n\t\t\t\tConfigConstants.MESOS_ARTIFACT_SERVER_SSL_ENABLED,\n\t\t\t\tConfigConstants.DEFAULT_MESOS_ARTIFACT_SERVER_SSL_ENABLED) &&\n\t\t\t\tSSLUtils.getSSLEnabled(config);\n\n\t\tif (enableSSL) {\n\t\t\tLOG.info(\"Enabling ssl for the artifact server\");\n\t\t\ttry {\n\t\t\t\tserverSSLContext = SSLUtils.createSSLServerContext(config);\n\t\t\t} catch (Exception e) {\n\t\t\t\tthrow new IOException(\"Failed to initialize SSLContext for the artifact server\", e);\n\t\t\t}\n\t\t} else {\n\t\t\tserverSSLContext = null;\n\t\t}\n\n\t\trouter = new Router();\n\n\t\tfinal Configuration sslConfig = config;\n\t\tChannelInitializer<SocketChannel> initializer = new ChannelInitializer<SocketChannel>() {\n\n\t\t\t@Override\n\t\t\tprotected void initChannel(SocketChannel ch) {\n\t\t\t\tHandler handler = new Handler(router);\n\n\t\t\t\t// SSL should be the first handler in the pipeline\n\t\t\t\tif (serverSSLContext != null) {\n\t\t\t\t\tSSLEngine sslEngine = serverSSLContext.createSSLEngine();\n\t\t\t\t\tSSLUtils.setSSLVerAndCipherSuites(sslEngine, sslConfig);\n\t\t\t\t\tsslEngine.setUseClientMode(false);\n\t\t\t\t\tch.pipeline().addLast(\"ssl\", new SslHandler(sslEngine));\n\t\t\t\t}\n\n\t\t\t\tch.pipeline()\n\t\t\t\t\t.addLast(new HttpServerCodec())\n\t\t\t\t\t.addLast(new ChunkedWriteHandler())\n\t\t\t\t\t.addLast(handler.name(), handler)\n\t\t\t\t\t.addLast(new UnknownFileHandler());\n\t\t\t}\n\t\t};\n\n\t\tNioEventLoopGroup bossGroup   = new NioEventLoopGroup(1);\n\t\tNioEventLoopGroup workerGroup = new NioEventLoopGroup();\n\n\t\tthis.bootstrap = new ServerBootstrap();\n\t\tthis.bootstrap\n\t\t\t.group(bossGroup, workerGroup)\n\t\t\t.channel(NioServerSocketChannel.class)\n\t\t\t.childHandler(initializer);\n\n\t\tChannel ch = this.bootstrap.bind(serverHostname, configuredPort).sync().channel();\n\t\tthis.serverChannel = ch;\n\n\t\tInetSocketAddress bindAddress = (InetSocketAddress) ch.localAddress();\n\t\tString address = bindAddress.getAddress().getHostAddress();\n\t\tint port = bindAddress.getPort();\n\n\t\tString httpProtocol = (serverSSLContext != null) ? \"https\": \"http\";\n\n\t\tbaseURL = new URL(httpProtocol, serverHostname, port, \"/\" + prefix + \"/\");\n\n\t\tLOG.info(\"Mesos Artifact Server Base URL: {}, listening at {}:{}\", baseURL, address, port);\n\t}",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157 +\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  ",
            "\tpublic MesosArtifactServer(String prefix, String serverHostname, int configuredPort, Configuration config)\n\t\tthrows Exception {\n\t\tif (configuredPort < 0 || configuredPort > 0xFFFF) {\n\t\t\tthrow new IllegalArgumentException(\"File server port is invalid: \" + configuredPort);\n\t\t}\n\n\t\t// Config to enable https access to the artifact server\n\t\tboolean enableSSL = config.getBoolean(\n\t\t\t\tConfigConstants.MESOS_ARTIFACT_SERVER_SSL_ENABLED,\n\t\t\t\tConfigConstants.DEFAULT_MESOS_ARTIFACT_SERVER_SSL_ENABLED) &&\n\t\t\t\tSSLUtils.getSSLEnabled(config);\n\n\t\tif (enableSSL) {\n\t\t\tLOG.info(\"Enabling ssl for the artifact server\");\n\t\t\ttry {\n\t\t\t\tserverSSLContext = SSLUtils.createSSLServerContext(config);\n\t\t\t} catch (Exception e) {\n\t\t\t\tthrow new IOException(\"Failed to initialize SSLContext for the artifact server\", e);\n\t\t\t}\n\t\t} else {\n\t\t\tserverSSLContext = null;\n\t\t}\n\n\t\trouter = new Router();\n\n\t\tfinal Configuration sslConfig = config;\n\t\tChannelInitializer<SocketChannel> initializer = new ChannelInitializer<SocketChannel>() {\n\n\t\t\t@Override\n\t\t\tprotected void initChannel(SocketChannel ch) {\n\t\t\t\tHandler handler = new Handler(router);\n\n\t\t\t\t// SSL should be the first handler in the pipeline\n\t\t\t\tif (serverSSLContext != null) {\n\t\t\t\t\tSSLEngine sslEngine = serverSSLContext.createSSLEngine();\n\t\t\t\t\tSSLUtils.setSSLVerAndCipherSuites(sslEngine, sslConfig);\n\t\t\t\t\tsslEngine.setUseClientMode(false);\n\t\t\t\t\tch.pipeline().addLast(\"ssl\", new SslHandler(sslEngine));\n\t\t\t\t}\n\n\t\t\t\tch.pipeline()\n\t\t\t\t\t.addLast(new HttpServerCodec())\n\t\t\t\t\t.addLast(new ChunkedWriteHandler())\n\t\t\t\t\t.addLast(handler.name(), handler)\n\t\t\t\t\t.addLast(new UnknownFileHandler());\n\t\t\t}\n\t\t};\n\n\t\tNioEventLoopGroup bossGroup = new NioEventLoopGroup(1);\n\t\tNioEventLoopGroup workerGroup = new NioEventLoopGroup();\n\n\t\tthis.bootstrap = new ServerBootstrap();\n\t\tthis.bootstrap\n\t\t\t.group(bossGroup, workerGroup)\n\t\t\t.channel(NioServerSocketChannel.class)\n\t\t\t.childHandler(initializer);\n\n\t\tChannel ch = this.bootstrap.bind(serverHostname, configuredPort).sync().channel();\n\t\tthis.serverChannel = ch;\n\n\t\tInetSocketAddress bindAddress = (InetSocketAddress) ch.localAddress();\n\t\tString address = bindAddress.getAddress().getHostAddress();\n\t\tint port = bindAddress.getPort();\n\n\t\tString httpProtocol = (serverSSLContext != null) ? \"https\" : \"http\";\n\n\t\tbaseURL = new URL(httpProtocol, serverHostname, port, \"/\" + prefix + \"/\");\n\n\t\tLOG.info(\"Mesos Artifact Server Base URL: {}, listening at {}:{}\", baseURL, address, port);\n\t}"
        ],
        [
            "MesosTaskManagerParameters::getTaskManagerHostname()",
            " 177  \n 178  \n 179  \n 180 -",
            "\t/**\n \t * Get the taskManager hostname.\n \t */\n\tpublic Option<String> getTaskManagerHostname() { return taskManagerHostname; }",
            " 178  \n 179  \n 180  \n 181 +\n 182 +\n 183 +",
            "\t/**\n\t * Get the taskManager hostname.\n\t */\n\tpublic Option<String> getTaskManagerHostname() {\n\t\treturn taskManagerHostname;\n\t}"
        ],
        [
            "LaunchableMesosWorker::LaunchableMesosWorker(MesosArtifactResolver,MesosTaskManagerParameters,ContainerSpecification,Protos,MesosConfiguration)",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 -\n  92  \n  93  ",
            "\t/**\n\t * Construct a launchable Mesos worker.\n\t * @param resolver The resolver for retrieving artifacts (e.g. jars, configuration)\n\t * @param params the TM parameters such as memory, cpu to acquire.\n\t * @param containerSpec an abstract container specification for launch time.\n\t * @param taskID the taskID for this worker.\n\t */\n\tpublic LaunchableMesosWorker(\n\t\t\tMesosArtifactResolver resolver,\n\t\t\tMesosTaskManagerParameters params,\n\t\t\tContainerSpecification containerSpec,\n\t\t\tProtos.TaskID taskID,\n\t\t\tMesosConfiguration mesosConfiguration) {\n\t\tthis.resolver = Preconditions.checkNotNull(resolver);\n\t\tthis.containerSpec = Preconditions.checkNotNull(containerSpec);\n\t\tthis.params = Preconditions.checkNotNull(params);\n\t\tthis.taskID = Preconditions.checkNotNull(taskID);\n\t\tthis.mesosConfiguration = Preconditions.checkNotNull(mesosConfiguration);\n\t\t\n\t\tthis.taskRequest = new Request();\n\t}",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95  ",
            "\t/**\n\t * Construct a launchable Mesos worker.\n\t * @param resolver The resolver for retrieving artifacts (e.g. jars, configuration)\n\t * @param params the TM parameters such as memory, cpu to acquire.\n\t * @param containerSpec an abstract container specification for launch time.\n\t * @param taskID the taskID for this worker.\n\t */\n\tpublic LaunchableMesosWorker(\n\t\t\tMesosArtifactResolver resolver,\n\t\t\tMesosTaskManagerParameters params,\n\t\t\tContainerSpecification containerSpec,\n\t\t\tProtos.TaskID taskID,\n\t\t\tMesosConfiguration mesosConfiguration) {\n\t\tthis.resolver = Preconditions.checkNotNull(resolver);\n\t\tthis.containerSpec = Preconditions.checkNotNull(containerSpec);\n\t\tthis.params = Preconditions.checkNotNull(params);\n\t\tthis.taskID = Preconditions.checkNotNull(taskID);\n\t\tthis.mesosConfiguration = Preconditions.checkNotNull(mesosConfiguration);\n\n\t\tthis.taskRequest = new Request();\n\t}"
        ],
        [
            "MesosTaskManagerParametersTest::givenInvalidConstraintInConfigShouldBeParsed()",
            " 124  \n 125 -\n 126 -\n 127 -\n 128 -\n 129 -",
            "    @Test\n    public void givenInvalidConstraintInConfigShouldBeParsed() throws Exception {\n\n        MesosTaskManagerParameters mesosTaskManagerParameters = MesosTaskManagerParameters.create(withHardHostAttrConstraintConfiguration(\",:,\"));\n        assertThat(mesosTaskManagerParameters.constraints().size(), is(0));\n    }",
            " 130  \n 131 +\n 132 +\n 133 +\n 134 +\n 135 +",
            "\t@Test\n\tpublic void givenInvalidConstraintInConfigShouldBeParsed() throws Exception {\n\n\t\tMesosTaskManagerParameters mesosTaskManagerParameters = MesosTaskManagerParameters.create(withHardHostAttrConstraintConfiguration(\",:,\"));\n\t\tassertThat(mesosTaskManagerParameters.constraints().size(), is(0));\n\t}"
        ],
        [
            "MesosArtifactServer::removePath(Path)",
            " 231  \n 232 -\n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "\tpublic synchronized void removePath(Path remoteFile) {\n\t\tif(paths.containsKey(remoteFile)) {\n\t\t\tURL fileURL = null;\n\t\t\ttry {\n\t\t\t\tfileURL = new URL(baseURL, remoteFile.toString());\n\t\t\t} catch (MalformedURLException e) {\n\t\t\t\tthrow new RuntimeException(e);\n\t\t\t}\n\t\t\trouter.removePath(fileURL.getPath());\n\t\t}\n\t}",
            " 232  \n 233 +\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "\tpublic synchronized void removePath(Path remoteFile) {\n\t\tif (paths.containsKey(remoteFile)) {\n\t\t\tURL fileURL = null;\n\t\t\ttry {\n\t\t\t\tfileURL = new URL(baseURL, remoteFile.toString());\n\t\t\t} catch (MalformedURLException e) {\n\t\t\t\tthrow new RuntimeException(e);\n\t\t\t}\n\t\t\trouter.removePath(fileURL.getPath());\n\t\t}\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::TestingMesosFlinkResourceManager::createTaskRouter()",
            " 129  \n 130 -",
            "\t\t@Override\n\t\tprotected ActorRef createTaskRouter() { return taskRouter.ref(); }",
            " 152  \n 153 +\n 154 +\n 155 +",
            "\t\t@Override\n\t\tprotected ActorRef createTaskRouter() {\n\t\t\treturn taskRouter.ref();\n\t\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::TestingMesosFlinkResourceManager::createReconciliationCoordinator()",
            " 133  \n 134 -",
            "\t\t@Override\n\t\tprotected ActorRef createReconciliationCoordinator() { return reconciliationCoordinator.ref(); }",
            " 162  \n 163 +\n 164 +\n 165 +",
            "\t\t@Override\n\t\tprotected ActorRef createReconciliationCoordinator() {\n\t\t\treturn reconciliationCoordinator.ref();\n\t\t}"
        ],
        [
            "MesosFlinkResourceManager::recoverWorkers()",
            " 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311 -\n 312  \n 313  \n 314  \n 315  \n 316  \n 317 -\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 -\n 335  \n 336  \n 337  \n 338 -\n 339  \n 340  \n 341  \n 342  ",
            "\t/**\n\t * Recover framework/worker information persisted by a prior incarnation of the RM.\n\t */\n\tprivate void recoverWorkers() throws Exception {\n\t\t// if this application master starts as part of an ApplicationMaster/JobManager recovery,\n\t\t// then some worker tasks are most likely still alive and we can re-obtain them\n\t\tfinal List<MesosWorkerStore.Worker> tasksFromPreviousAttempts = workerStore.recoverWorkers();\n\n\t\tif (!tasksFromPreviousAttempts.isEmpty()) {\n\t\t\tLOG.info(\"Retrieved {} TaskManagers from previous attempt\", tasksFromPreviousAttempts.size());\n\n\t\t\tList<Tuple2<TaskRequest,String>> toAssign = new ArrayList<>(tasksFromPreviousAttempts.size());\n\t\t\tList<LaunchableTask> toLaunch = new ArrayList<>(tasksFromPreviousAttempts.size());\n\n\t\t\tfor (final MesosWorkerStore.Worker worker : tasksFromPreviousAttempts) {\n\t\t\t\tLaunchableMesosWorker launchable = createLaunchableMesosWorker(worker.taskID());\n\n\t\t\t\tswitch(worker.state()) {\n\t\t\t\t\tcase New:\n\t\t\t\t\t\tworkersInNew.put(extractResourceID(worker.taskID()), worker);\n\t\t\t\t\t\ttoLaunch.add(launchable);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase Launched:\n\t\t\t\t\t\tworkersInLaunch.put(extractResourceID(worker.taskID()), worker);\n\t\t\t\t\t\ttoAssign.add(new Tuple2<>(launchable.taskRequest(), worker.hostname().get()));\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase Released:\n\t\t\t\t\t\tworkersBeingReturned.put(extractResourceID(worker.taskID()), worker);\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\ttaskRouter.tell(new TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker)), self());\n\t\t\t}\n\n\t\t\t// tell the launch coordinator about prior assignments\n\t\t\tif(toAssign.size() >= 1) {\n\t\t\t\tlaunchCoordinator.tell(new LaunchCoordinator.Assign(toAssign), self());\n\t\t\t}\n\t\t\t// tell the launch coordinator to launch any new tasks\n\t\t\tif(toLaunch.size() >= 1) {\n\t\t\t\tlaunchCoordinator.tell(new LaunchCoordinator.Launch(toLaunch), self());\n\t\t\t}\n\t\t}\n\t}",
            " 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311 +\n 312  \n 313  \n 314  \n 315  \n 316  \n 317 +\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 +\n 335  \n 336  \n 337  \n 338 +\n 339  \n 340  \n 341  \n 342  ",
            "\t/**\n\t * Recover framework/worker information persisted by a prior incarnation of the RM.\n\t */\n\tprivate void recoverWorkers() throws Exception {\n\t\t// if this application master starts as part of an ApplicationMaster/JobManager recovery,\n\t\t// then some worker tasks are most likely still alive and we can re-obtain them\n\t\tfinal List<MesosWorkerStore.Worker> tasksFromPreviousAttempts = workerStore.recoverWorkers();\n\n\t\tif (!tasksFromPreviousAttempts.isEmpty()) {\n\t\t\tLOG.info(\"Retrieved {} TaskManagers from previous attempt\", tasksFromPreviousAttempts.size());\n\n\t\t\tList<Tuple2<TaskRequest, String>> toAssign = new ArrayList<>(tasksFromPreviousAttempts.size());\n\t\t\tList<LaunchableTask> toLaunch = new ArrayList<>(tasksFromPreviousAttempts.size());\n\n\t\t\tfor (final MesosWorkerStore.Worker worker : tasksFromPreviousAttempts) {\n\t\t\t\tLaunchableMesosWorker launchable = createLaunchableMesosWorker(worker.taskID());\n\n\t\t\t\tswitch (worker.state()) {\n\t\t\t\t\tcase New:\n\t\t\t\t\t\tworkersInNew.put(extractResourceID(worker.taskID()), worker);\n\t\t\t\t\t\ttoLaunch.add(launchable);\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase Launched:\n\t\t\t\t\t\tworkersInLaunch.put(extractResourceID(worker.taskID()), worker);\n\t\t\t\t\t\ttoAssign.add(new Tuple2<>(launchable.taskRequest(), worker.hostname().get()));\n\t\t\t\t\t\tbreak;\n\t\t\t\t\tcase Released:\n\t\t\t\t\t\tworkersBeingReturned.put(extractResourceID(worker.taskID()), worker);\n\t\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\ttaskRouter.tell(new TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker)), self());\n\t\t\t}\n\n\t\t\t// tell the launch coordinator about prior assignments\n\t\t\tif (toAssign.size() >= 1) {\n\t\t\t\tlaunchCoordinator.tell(new LaunchCoordinator.Assign(toAssign), self());\n\t\t\t}\n\t\t\t// tell the launch coordinator to launch any new tasks\n\t\t\tif (toLaunch.size() >= 1) {\n\t\t\t\tlaunchCoordinator.tell(new LaunchCoordinator.Launch(toLaunch), self());\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "ZooKeeperMesosWorkerStore::ZooKeeperMesosWorkerStore(ZooKeeperStateHandleStore,ZooKeeperSharedValue,ZooKeeperSharedCount)",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72 -\n  73  ",
            "\t@SuppressWarnings(\"unchecked\")\n\tpublic ZooKeeperMesosWorkerStore(\n\t\tZooKeeperStateHandleStore<MesosWorkerStore.Worker> workersInZooKeeper,\n\t\tZooKeeperSharedValue frameworkIdInZooKeeper,\n\t\tZooKeeperSharedCount totalTaskCountInZooKeeper) throws Exception {\n\t\tthis.workersInZooKeeper = checkNotNull(workersInZooKeeper, \"workersInZooKeeper\");\n\t\tthis.frameworkIdInZooKeeper = checkNotNull(frameworkIdInZooKeeper, \"frameworkIdInZooKeeper\");\n\t\tthis.totalTaskCountInZooKeeper= checkNotNull(totalTaskCountInZooKeeper, \"totalTaskCountInZooKeeper\");\n\t}",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 +\n  75  ",
            "\t@SuppressWarnings(\"unchecked\")\n\tpublic ZooKeeperMesosWorkerStore(\n\t\tZooKeeperStateHandleStore<MesosWorkerStore.Worker> workersInZooKeeper,\n\t\tZooKeeperSharedValue frameworkIdInZooKeeper,\n\t\tZooKeeperSharedCount totalTaskCountInZooKeeper) throws Exception {\n\t\tthis.workersInZooKeeper = checkNotNull(workersInZooKeeper, \"workersInZooKeeper\");\n\t\tthis.frameworkIdInZooKeeper = checkNotNull(frameworkIdInZooKeeper, \"frameworkIdInZooKeeper\");\n\t\tthis.totalTaskCountInZooKeeper = checkNotNull(totalTaskCountInZooKeeper, \"totalTaskCountInZooKeeper\");\n\t}"
        ],
        [
            "MesosTaskManagerParametersTest::withHardHostAttrConstraintConfiguration(String)",
            " 132 -\n 133 -\n 134 -\n 135 -\n 136 -\n 137 -\n 138 -\n 139 -\n 140 -",
            "    private static Configuration withHardHostAttrConstraintConfiguration(final String configuration) {\n        return new Configuration() {\n            private static final long serialVersionUID = -3249384117909445760L;\n\n            {\n                setString(MesosTaskManagerParameters.MESOS_CONSTRAINTS_HARD_HOSTATTR, configuration);\n            }\n        };\n    }",
            " 137 +\n 138 +\n 139 +\n 140 +\n 141 +\n 142 +\n 143 +\n 144 +\n 145 +",
            "\tprivate static Configuration withHardHostAttrConstraintConfiguration(final String configuration) {\n\t\treturn new Configuration() {\n\t\t\tprivate static final long serialVersionUID = -3249384117909445760L;\n\n\t\t\t{\n\t\t\t\tsetString(MesosTaskManagerParameters.MESOS_CONSTRAINTS_HARD_HOSTATTR, configuration);\n\t\t\t}\n\t\t};\n\t}"
        ],
        [
            "MesosArtifactServer::addPath(Path,Path)",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 -\n 218  \n 219  \n 220 -\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "\t/**\n\t * Adds a path to the artifact server.\n\t * @param path the qualified FS path to serve (local, hdfs, etc).\n\t * @param remoteFile the remote path with which to locate the file.\n\t * @return the fully-qualified remote path to the file.\n\t * @throws MalformedURLException if the remote path is invalid.\n\t */\n\tpublic synchronized URL addPath(Path path, Path remoteFile) throws IOException, MalformedURLException {\n\t\tif(paths.containsKey(remoteFile)) {\n\t\t\tthrow new IllegalArgumentException(\"duplicate path registered\");\n\t\t}\n\t\tif(remoteFile.isAbsolute()) {\n\t\t\tthrow new IllegalArgumentException(\"not expecting an absolute path\");\n\t\t}\n\t\tURL fileURL = new URL(baseURL, remoteFile.toString());\n\t\trouter.ANY(fileURL.getPath(), new VirtualFileServerHandler(path));\n\n\t\tpaths.put(remoteFile, fileURL);\n\n\t\treturn fileURL;\n\t}",
            " 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218 +\n 219  \n 220  \n 221 +\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "\t/**\n\t * Adds a path to the artifact server.\n\t * @param path the qualified FS path to serve (local, hdfs, etc).\n\t * @param remoteFile the remote path with which to locate the file.\n\t * @return the fully-qualified remote path to the file.\n\t * @throws MalformedURLException if the remote path is invalid.\n\t */\n\tpublic synchronized URL addPath(Path path, Path remoteFile) throws IOException, MalformedURLException {\n\t\tif (paths.containsKey(remoteFile)) {\n\t\t\tthrow new IllegalArgumentException(\"duplicate path registered\");\n\t\t}\n\t\tif (remoteFile.isAbsolute()) {\n\t\t\tthrow new IllegalArgumentException(\"not expecting an absolute path\");\n\t\t}\n\t\tURL fileURL = new URL(baseURL, remoteFile.toString());\n\t\trouter.ANY(fileURL.getPath(), new VirtualFileServerHandler(path));\n\n\t\tpaths.put(remoteFile, fileURL);\n\n\t\treturn fileURL;\n\t}"
        ],
        [
            "MesosFlinkResourceManager::handleMessage(Object)",
            " 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 -\n 252  \n 253  \n 254  \n 255  ",
            "\t@Override\n\tprotected void handleMessage(Object message) {\n\n\t\t// check for Mesos-specific actor messages first\n\n\t\t// --- messages about Mesos connection\n\t\tif (message instanceof Registered) {\n\t\t\tregistered((Registered) message);\n\t\t} else if (message instanceof ReRegistered) {\n\t\t\treregistered((ReRegistered) message);\n\t\t} else if (message instanceof Disconnected) {\n\t\t\tdisconnected((Disconnected) message);\n\t\t} else if (message instanceof Error) {\n\t\t\terror(((Error) message).message());\n\n\t\t// --- messages about offers\n\t\t} else if (message instanceof ResourceOffers || message instanceof OfferRescinded) {\n\t\t\tlaunchCoordinator.tell(message, self());\n\t\t} else if (message instanceof AcceptOffers) {\n\t\t\tacceptOffers((AcceptOffers) message);\n\n\t\t// --- messages about tasks\n\t\t} else if (message instanceof StatusUpdate) {\n\t\t\ttaskStatusUpdated((StatusUpdate) message);\n\t\t} else if (message instanceof ReconciliationCoordinator.Reconcile) {\n\t\t\t// a reconciliation request from a task\n\t\t\treconciliationCoordinator.tell(message, self());\n\t\t} else if (message instanceof TaskMonitor.TaskTerminated) {\n\t\t\t// a termination message from a task\n\t\t\tTaskMonitor.TaskTerminated msg = (TaskMonitor.TaskTerminated) message;\n\t\t\ttaskTerminated(msg.taskID(), msg.status());\n\n\t\t} else  {\n\t\t\t// message handled by the generic resource master code\n\t\t\tsuper.handleMessage(message);\n\t\t}\n\t}",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253 +\n 254  \n 255  \n 256  \n 257  ",
            "\t@Override\n\tprotected void handleMessage(Object message) {\n\n\t\t// check for Mesos-specific actor messages first\n\n\t\t// --- messages about Mesos connection\n\t\tif (message instanceof Registered) {\n\t\t\tregistered((Registered) message);\n\t\t} else if (message instanceof ReRegistered) {\n\t\t\treregistered((ReRegistered) message);\n\t\t} else if (message instanceof Disconnected) {\n\t\t\tdisconnected((Disconnected) message);\n\t\t} else if (message instanceof Error) {\n\t\t\terror(((Error) message).message());\n\n\t\t// --- messages about offers\n\t\t} else if (message instanceof ResourceOffers || message instanceof OfferRescinded) {\n\t\t\tlaunchCoordinator.tell(message, self());\n\t\t} else if (message instanceof AcceptOffers) {\n\t\t\tacceptOffers((AcceptOffers) message);\n\n\t\t// --- messages about tasks\n\t\t} else if (message instanceof StatusUpdate) {\n\t\t\ttaskStatusUpdated((StatusUpdate) message);\n\t\t} else if (message instanceof ReconciliationCoordinator.Reconcile) {\n\t\t\t// a reconciliation request from a task\n\t\t\treconciliationCoordinator.tell(message, self());\n\t\t} else if (message instanceof TaskMonitor.TaskTerminated) {\n\t\t\t// a termination message from a task\n\t\t\tTaskMonitor.TaskTerminated msg = (TaskMonitor.TaskTerminated) message;\n\t\t\ttaskTerminated(msg.taskID(), msg.status());\n\n\t\t} else {\n\t\t\t// message handled by the generic resource master code\n\t\t\tsuper.handleMessage(message);\n\t\t}\n\t}"
        ],
        [
            "MesosTaskManagerParametersTest::givenTwoConstraintsInConfigShouldBeParsed()",
            "  81  \n  82 -\n  83 -\n  84 -\n  85 -\n  86 -\n  87 -\n  88 -\n  89 -\n  90 -\n  91 -\n  92 -\n  93 -\n  94 -\n  95 -\n  96 -\n  97 -\n  98 -\n  99 -\n 100 -\n 101 -",
            "\t@Test\n    public void givenTwoConstraintsInConfigShouldBeParsed() throws Exception {\n\n        MesosTaskManagerParameters mesosTaskManagerParameters = MesosTaskManagerParameters.create(withHardHostAttrConstraintConfiguration(\"cluster:foo,az:eu-west-1\"));\n        assertThat(mesosTaskManagerParameters.constraints().size(), is(2));\n        ConstraintEvaluator firstConstraintEvaluator = new HostAttrValueConstraint(\"cluster\", new Func1<String, String>() {\n            @Override\n            public String call(String s) {\n                return \"foo\";\n            }\n        });\n        ConstraintEvaluator secondConstraintEvaluator = new HostAttrValueConstraint(\"az\", new Func1<String, String>() {\n            @Override\n            public String call(String s) {\n                return \"foo\";\n            }\n        });\n        assertThat(mesosTaskManagerParameters.constraints().get(0).getName(), is(firstConstraintEvaluator.getName()));\n        assertThat(mesosTaskManagerParameters.constraints().get(1).getName(), is(secondConstraintEvaluator.getName()));\n\n    }",
            "  87  \n  88 +\n  89 +\n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95 +\n  96 +\n  97 +\n  98 +\n  99 +\n 100 +\n 101 +\n 102 +\n 103 +\n 104 +\n 105 +\n 106 +\n 107 +",
            "\t@Test\n\tpublic void givenTwoConstraintsInConfigShouldBeParsed() throws Exception {\n\n\t\tMesosTaskManagerParameters mesosTaskManagerParameters = MesosTaskManagerParameters.create(withHardHostAttrConstraintConfiguration(\"cluster:foo,az:eu-west-1\"));\n\t\tassertThat(mesosTaskManagerParameters.constraints().size(), is(2));\n\t\tConstraintEvaluator firstConstraintEvaluator = new HostAttrValueConstraint(\"cluster\", new Func1<String, String>() {\n\t\t\t@Override\n\t\t\tpublic String call(String s) {\n\t\t\t\treturn \"foo\";\n\t\t\t}\n\t\t});\n\t\tConstraintEvaluator secondConstraintEvaluator = new HostAttrValueConstraint(\"az\", new Func1<String, String>() {\n\t\t\t@Override\n\t\t\tpublic String call(String s) {\n\t\t\t\treturn \"foo\";\n\t\t\t}\n\t\t});\n\t\tassertThat(mesosTaskManagerParameters.constraints().get(0).getName(), is(firstConstraintEvaluator.getName()));\n\t\tassertThat(mesosTaskManagerParameters.constraints().get(1).getName(), is(secondConstraintEvaluator.getName()));\n\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testRegistered()",
            " 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668 -\n 669 -\n 670  \n 671  \n 672  \n 673  \n 674  \n 675  ",
            "\t/**\n\t * Test Mesos registration handling.\n\t */\n\t@Test\n\tpublic void testRegistered() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\tProtos.MasterInfo masterInfo = Protos.MasterInfo.newBuilder()\n\t\t\t\t\t\t\t.setId(\"master1\").setIp(0).setPort(5050).build();\n\t\t\t\t\t\tresourceManager.tell(new Registered(framework1, masterInfo), resourceManager);\n\n\t\t\t\t\t\tverify(workerStore).setFrameworkID(Option.apply(framework1));\n\t\t\t\t\t\tresourceManagerInstance.connectionMonitor.expectMsgClass(Registered.class);\n\t\t\t\t\t\tresourceManagerInstance.reconciliationCoordinator.expectMsgClass(Registered.class);\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(Registered.class);\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(Registered.class);\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690 +\n 691  \n 692  \n 693  \n 694  \n 695  \n 696  ",
            "\t/**\n\t * Test Mesos registration handling.\n\t */\n\t@Test\n\tpublic void testRegistered() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\tProtos.MasterInfo masterInfo = Protos.MasterInfo.newBuilder()\n\t\t\t\t\t\t\t.setId(\"master1\").setIp(0).setPort(5050).build();\n\t\t\t\t\t\tresourceManager.tell(new Registered(framework1, masterInfo), resourceManager);\n\n\t\t\t\t\t\tverify(workerStore).setFrameworkID(Option.apply(framework1));\n\t\t\t\t\t\tresourceManagerInstance.connectionMonitor.expectMsgClass(Registered.class);\n\t\t\t\t\t\tresourceManagerInstance.reconciliationCoordinator.expectMsgClass(Registered.class);\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(Registered.class);\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(Registered.class);\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testWorkerRegistered()",
            " 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367 -\n 368 -\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  ",
            "\t/**\n\t * Test normal worker registration.\n\t */\n\t@Test\n\tpublic void testWorkerRegistered() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial state with a (recovered) launched worker\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1launched));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch, hasEntry(extractResourceID(task1), worker1launched));\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\t// send registration message\n\t\t\t\t\t\tNotifyResourceStarted msg = new NotifyResourceStarted(extractResourceID(task1));\n\t\t\t\t\t\tresourceManager.tell(msg);\n\n\t\t\t\t\t\t// verify that the internal state was updated\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch.entrySet(), empty());\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396 +\n 397  \n 398  \n 399  \n 400  \n 401  \n 402  ",
            "\t/**\n\t * Test normal worker registration.\n\t */\n\t@Test\n\tpublic void testWorkerRegistered() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial state with a (recovered) launched worker\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1launched));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch, hasEntry(extractResourceID(task1), worker1launched));\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\t// send registration message\n\t\t\t\t\t\tNotifyResourceStarted msg = new NotifyResourceStarted(extractResourceID(task1));\n\t\t\t\t\t\tresourceManager.tell(msg);\n\n\t\t\t\t\t\t// verify that the internal state was updated\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch.entrySet(), empty());\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManager::taskTerminated(Protos,Protos)",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601 -\n 602 -\n 603  \n 604  \n 605  \n 606  \n 607 -\n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  ",
            "\t/**\n\t * Invoked when a Mesos task reaches a terminal status.\n\t */\n\tprivate void taskTerminated(Protos.TaskID taskID, Protos.TaskStatus status) {\n\t\t// this callback occurs for failed containers and for released containers alike\n\n\t\tfinal ResourceID id = extractResourceID(taskID);\n\n\t\tboolean existed;\n\t\ttry {\n\t\t\texisted = workerStore.removeWorker(taskID);\n\t\t}\n\t\tcatch(Exception ex) {\n\t\t\tfatalError(\"unable to remove worker\", ex);\n\t\t\treturn;\n\t\t}\n\n\t\tif(!existed) {\n\t\t\tLOG.info(\"Received a termination notice for an unrecognized worker: {}\", id);\n\t\t\treturn;\n\t\t}\n\n\t\t// check if this is a failed task or a released task\n\t\tif (workersBeingReturned.remove(id) != null) {\n\t\t\t// regular finished worker that we released\n\t\t\tLOG.info(\"Worker {} finished successfully with diagnostics: {}\",\n\t\t\t\tid, status.getMessage());\n\t\t} else {\n\t\t\t// failed worker, either at startup, or running\n\t\t\tfinal MesosWorkerStore.Worker launched = workersInLaunch.remove(id);\n\t\t\tif (launched != null) {\n\t\t\t\tLOG.info(\"Mesos task {} failed, with a TaskManager in launch or registration. \" +\n\t\t\t\t\t\"State: {} Reason: {} ({})\", id, status.getState(), status.getReason(), status.getMessage());\n\t\t\t\t// we will trigger re-acquiring new workers at the end\n\t\t\t} else {\n\t\t\t\t// failed registered worker\n\t\t\t\tLOG.info(\"Mesos task {} failed, with a registered TaskManager. \" +\n\t\t\t\t\t\"State: {} Reason: {} ({})\", id, status.getState(), status.getReason(), status.getMessage());\n\n\t\t\t\t// notify the generic logic, which notifies the JobManager, etc.\n\t\t\t\tnotifyWorkerFailed(id, \"Mesos task \" + id + \" failed.  State: \" + status.getState());\n\t\t\t}\n\n\t\t\t// general failure logging\n\t\t\tfailedTasksSoFar++;\n\n\t\t\tString diagMessage = String.format(\"Diagnostics for task %s in state %s : \" +\n\t\t\t\t\t\"reason=%s message=%s\",\n\t\t\t\tid, status.getState(), status.getReason(), status.getMessage());\n\t\t\tsendInfoMessage(diagMessage);\n\n\t\t\tLOG.info(diagMessage);\n\t\t\tLOG.info(\"Total number of failed tasks so far: {}\", failedTasksSoFar);\n\n\t\t\t// maxFailedTasks == -1 is infinite number of retries.\n\t\t\tif (maxFailedTasks >= 0 && failedTasksSoFar > maxFailedTasks) {\n\t\t\t\tString msg = \"Stopping Mesos session because the number of failed tasks (\"\n\t\t\t\t\t+ failedTasksSoFar + \") exceeded the maximum failed tasks (\"\n\t\t\t\t\t+ maxFailedTasks + \"). This number is controlled by the '\"\n\t\t\t\t\t+ ConfigConstants.MESOS_MAX_FAILED_TASKS + \"' configuration setting. \"\n\t\t\t\t\t+ \"By default its the number of requested tasks.\";\n\n\t\t\t\tLOG.error(msg);\n\t\t\t\tself().tell(decorateMessage(new StopCluster(ApplicationStatus.FAILED, msg)),\n\t\t\t\t\tActorRef.noSender());\n\n\t\t\t\t// no need to do anything else\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\t// in case failed containers were among the finished containers, make\n\t\t// sure we re-examine and request new ones\n\t\ttriggerCheckWorkers();\n\t}",
            " 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597 +\n 598  \n 599  \n 600  \n 601  \n 602 +\n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  ",
            "\t/**\n\t * Invoked when a Mesos task reaches a terminal status.\n\t */\n\tprivate void taskTerminated(Protos.TaskID taskID, Protos.TaskStatus status) {\n\t\t// this callback occurs for failed containers and for released containers alike\n\n\t\tfinal ResourceID id = extractResourceID(taskID);\n\n\t\tboolean existed;\n\t\ttry {\n\t\t\texisted = workerStore.removeWorker(taskID);\n\t\t} catch (Exception ex) {\n\t\t\tfatalError(\"unable to remove worker\", ex);\n\t\t\treturn;\n\t\t}\n\n\t\tif (!existed) {\n\t\t\tLOG.info(\"Received a termination notice for an unrecognized worker: {}\", id);\n\t\t\treturn;\n\t\t}\n\n\t\t// check if this is a failed task or a released task\n\t\tif (workersBeingReturned.remove(id) != null) {\n\t\t\t// regular finished worker that we released\n\t\t\tLOG.info(\"Worker {} finished successfully with diagnostics: {}\",\n\t\t\t\tid, status.getMessage());\n\t\t} else {\n\t\t\t// failed worker, either at startup, or running\n\t\t\tfinal MesosWorkerStore.Worker launched = workersInLaunch.remove(id);\n\t\t\tif (launched != null) {\n\t\t\t\tLOG.info(\"Mesos task {} failed, with a TaskManager in launch or registration. \" +\n\t\t\t\t\t\"State: {} Reason: {} ({})\", id, status.getState(), status.getReason(), status.getMessage());\n\t\t\t\t// we will trigger re-acquiring new workers at the end\n\t\t\t} else {\n\t\t\t\t// failed registered worker\n\t\t\t\tLOG.info(\"Mesos task {} failed, with a registered TaskManager. \" +\n\t\t\t\t\t\"State: {} Reason: {} ({})\", id, status.getState(), status.getReason(), status.getMessage());\n\n\t\t\t\t// notify the generic logic, which notifies the JobManager, etc.\n\t\t\t\tnotifyWorkerFailed(id, \"Mesos task \" + id + \" failed.  State: \" + status.getState());\n\t\t\t}\n\n\t\t\t// general failure logging\n\t\t\tfailedTasksSoFar++;\n\n\t\t\tString diagMessage = String.format(\"Diagnostics for task %s in state %s : \" +\n\t\t\t\t\t\"reason=%s message=%s\",\n\t\t\t\tid, status.getState(), status.getReason(), status.getMessage());\n\t\t\tsendInfoMessage(diagMessage);\n\n\t\t\tLOG.info(diagMessage);\n\t\t\tLOG.info(\"Total number of failed tasks so far: {}\", failedTasksSoFar);\n\n\t\t\t// maxFailedTasks == -1 is infinite number of retries.\n\t\t\tif (maxFailedTasks >= 0 && failedTasksSoFar > maxFailedTasks) {\n\t\t\t\tString msg = \"Stopping Mesos session because the number of failed tasks (\"\n\t\t\t\t\t+ failedTasksSoFar + \") exceeded the maximum failed tasks (\"\n\t\t\t\t\t+ maxFailedTasks + \"). This number is controlled by the '\"\n\t\t\t\t\t+ ConfigConstants.MESOS_MAX_FAILED_TASKS + \"' configuration setting. \"\n\t\t\t\t\t+ \"By default its the number of requested tasks.\";\n\n\t\t\t\tLOG.error(msg);\n\t\t\t\tself().tell(decorateMessage(new StopCluster(ApplicationStatus.FAILED, msg)),\n\t\t\t\t\tActorRef.noSender());\n\n\t\t\t\t// no need to do anything else\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\n\t\t// in case failed containers were among the finished containers, make\n\t\t// sure we re-examine and request new ones\n\t\ttriggerCheckWorkers();\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::TestingMesosFlinkResourceManager::createConnectionMonitor()",
            " 127  \n 128 -",
            "\t\t@Override\n\t\tprotected ActorRef createConnectionMonitor() { return connectionMonitor.ref(); }",
            " 147  \n 148 +\n 149 +\n 150 +",
            "\t\t@Override\n\t\tprotected ActorRef createConnectionMonitor() {\n\t\t\treturn connectionMonitor.ref();\n\t\t}"
        ],
        [
            "Utils::uri(MesosArtifactResolver,ContainerSpecification)",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56 -\n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  ",
            "\t/**\n\t * Construct a Mesos URI.\n\t */\n\tpublic static Protos.CommandInfo.URI uri(MesosArtifactResolver resolver, ContainerSpecification.Artifact artifact) {\n\t\tOption<URL> url = resolver.resolve(artifact.dest);\n\t\tif(url.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\"Unresolvable artifact: \" + artifact.dest);\n\t\t}\n\n\t\treturn Protos.CommandInfo.URI.newBuilder()\n\t\t\t.setValue(url.get().toExternalForm())\n\t\t\t.setOutputFile(artifact.dest.toString())\n\t\t\t.setExtract(artifact.extract)\n\t\t\t.setCache(artifact.cachable)\n\t\t\t.setExecutable(artifact.executable)\n\t\t\t.build();\n\t}",
            "  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  ",
            "\t/**\n\t * Construct a Mesos URI.\n\t */\n\tpublic static Protos.CommandInfo.URI uri(MesosArtifactResolver resolver, ContainerSpecification.Artifact artifact) {\n\t\tOption<URL> url = resolver.resolve(artifact.dest);\n\t\tif (url.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\"Unresolvable artifact: \" + artifact.dest);\n\t\t}\n\n\t\treturn Protos.CommandInfo.URI.newBuilder()\n\t\t\t.setValue(url.get().toExternalForm())\n\t\t\t.setOutputFile(artifact.dest.toString())\n\t\t\t.setExtract(artifact.extract)\n\t\t\t.setCache(artifact.cachable)\n\t\t\t.setExecutable(artifact.executable)\n\t\t\t.build();\n\t}"
        ],
        [
            "MesosArtifactServer::VirtualFileServerHandler::channelRead0(ChannelHandlerContext,Routed)",
            " 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295 -\n 296  \n 297  \n 298  \n 299  \n 300 -\n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 -\n 326 -\n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  ",
            "\t\t@Override\n\t\tprotected void channelRead0(ChannelHandlerContext ctx, Routed routed) throws Exception {\n\n\t\t\tHttpRequest request = routed.request();\n\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"{} request for file '{}'\", request.getMethod(), path);\n\t\t\t}\n\n\t\t\tif(!(request.getMethod() == GET || request.getMethod() == HEAD)) {\n\t\t\t\tsendMethodNotAllowed(ctx);\n\t\t\t\treturn;\n\t\t\t}\n\n\n\t\t\tfinal FileStatus status;\n\t\t\ttry {\n\t\t\t\tstatus = fs.getFileStatus(path);\n\t\t\t}\n\t\t\tcatch (IOException e) {\n\t\t\t\tLOG.error(\"unable to stat file\", e);\n\t\t\t\tsendError(ctx, GONE);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// compose the response\n\t\t\tHttpResponse response = new DefaultHttpResponse(HTTP_1_1, OK);\n\t\t\tHttpHeaders.setHeader(response, CONNECTION, HttpHeaders.Values.CLOSE);\n\t\t\tHttpHeaders.setHeader(response, CACHE_CONTROL, \"private\");\n\t\t\tHttpHeaders.setHeader(response, CONTENT_TYPE, Mimetypes.MIMETYPE_OCTET_STREAM);\n\t\t\tHttpHeaders.setContentLength(response, status.getLen());\n\n\t\t\tctx.write(response);\n\n\t\t\tif (request.getMethod() == GET) {\n\t\t\t\t// write the content.  Netty will close the stream.\n\t\t\t\tfinal FSDataInputStream stream = fs.open(path);\n\t\t\t\ttry {\n\t\t\t\t\tctx.write(new ChunkedStream(stream));\n\t\t\t\t}\n\t\t\t\tcatch(Exception e) {\n\t\t\t\t\tstream.close();\n\t\t\t\t\tthrow e;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tChannelFuture lastContentFuture = ctx.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT);\n\t\t\tlastContentFuture.addListener(ChannelFutureListener.CLOSE);\n\t\t}",
            " 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296 +\n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 +\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  ",
            "\t\t@Override\n\t\tprotected void channelRead0(ChannelHandlerContext ctx, Routed routed) throws Exception {\n\n\t\t\tHttpRequest request = routed.request();\n\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"{} request for file '{}'\", request.getMethod(), path);\n\t\t\t}\n\n\t\t\tif (!(request.getMethod() == GET || request.getMethod() == HEAD)) {\n\t\t\t\tsendMethodNotAllowed(ctx);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tfinal FileStatus status;\n\t\t\ttry {\n\t\t\t\tstatus = fs.getFileStatus(path);\n\t\t\t}\n\t\t\tcatch (IOException e) {\n\t\t\t\tLOG.error(\"unable to stat file\", e);\n\t\t\t\tsendError(ctx, GONE);\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t// compose the response\n\t\t\tHttpResponse response = new DefaultHttpResponse(HTTP_1_1, OK);\n\t\t\tHttpHeaders.setHeader(response, CONNECTION, HttpHeaders.Values.CLOSE);\n\t\t\tHttpHeaders.setHeader(response, CACHE_CONTROL, \"private\");\n\t\t\tHttpHeaders.setHeader(response, CONTENT_TYPE, Mimetypes.MIMETYPE_OCTET_STREAM);\n\t\t\tHttpHeaders.setContentLength(response, status.getLen());\n\n\t\t\tctx.write(response);\n\n\t\t\tif (request.getMethod() == GET) {\n\t\t\t\t// write the content.  Netty will close the stream.\n\t\t\t\tfinal FSDataInputStream stream = fs.open(path);\n\t\t\t\ttry {\n\t\t\t\t\tctx.write(new ChunkedStream(stream));\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tstream.close();\n\t\t\t\t\tthrow e;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tChannelFuture lastContentFuture = ctx.writeAndFlush(LastHttpContent.EMPTY_LAST_CONTENT);\n\t\t\tlastContentFuture.addListener(ChannelFutureListener.CLOSE);\n\t\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testRegisteredWorkerFailed()",
            " 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607 -\n 608 -\n 609  \n 610  \n 611  \n 612  \n 613  \n 614  ",
            "\t/**\n\t * Test unplanned task failure of a registered worker.\n\t */\n\t@Test\n\tpublic void testRegisteredWorkerFailed() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state with a launched & registered worker\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1launched));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(singletonList(extractResourceID(task1)));\n\n\t\t\t\t\t\t// tell the RM that a task failed (and prepare a replacement task)\n\t\t\t\t\t\twhen(workerStore.newTaskID()).thenReturn(task2);\n\t\t\t\t\t\twhen(workerStore.removeWorker(task1)).thenReturn(true);\n\t\t\t\t\t\tresourceManager.tell(new SetWorkerPoolSize(1), jobManager);\n\t\t\t\t\t\tresourceManager.tell(new TaskMonitor.TaskTerminated(task1, Protos.TaskStatus.newBuilder()\n\t\t\t\t\t\t\t.setTaskId(task1).setSlaveId(slave1).setState(Protos.TaskState.TASK_FAILED).build()));\n\n\t\t\t\t\t\t// verify that the instance state was updated and a replacement was created\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch.entrySet(), empty());\n\t\t\t\t\t\texpectMsgClass(ResourceRemoved.class);\n\t\t\t\t\t\tverify(workerStore).newTaskID();\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631 +\n 632  \n 633  \n 634  \n 635  \n 636  \n 637  ",
            "\t/**\n\t * Test unplanned task failure of a registered worker.\n\t */\n\t@Test\n\tpublic void testRegisteredWorkerFailed() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state with a launched & registered worker\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1launched));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(singletonList(extractResourceID(task1)));\n\n\t\t\t\t\t\t// tell the RM that a task failed (and prepare a replacement task)\n\t\t\t\t\t\twhen(workerStore.newTaskID()).thenReturn(task2);\n\t\t\t\t\t\twhen(workerStore.removeWorker(task1)).thenReturn(true);\n\t\t\t\t\t\tresourceManager.tell(new SetWorkerPoolSize(1), jobManager);\n\t\t\t\t\t\tresourceManager.tell(new TaskMonitor.TaskTerminated(task1, Protos.TaskStatus.newBuilder()\n\t\t\t\t\t\t\t.setTaskId(task1).setSlaveId(slave1).setState(Protos.TaskState.TASK_FAILED).build()));\n\n\t\t\t\t\t\t// verify that the instance state was updated and a replacement was created\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch.entrySet(), empty());\n\t\t\t\t\t\texpectMsgClass(ResourceRemoved.class);\n\t\t\t\t\t\tverify(workerStore).newTaskID();\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManager::reacceptRegisteredWorkers(Collection)",
            " 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473 -\n 474 -\n 475 -\n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  ",
            "\t/**\n\t * Accept the given registered workers into the internal state.\n\t *\n\t * @param toConsolidate The worker IDs known previously to the JobManager.\n\t * @return A collection of registered worker node records.\n\t */\n\t@Override\n\tprotected Collection<RegisteredMesosWorkerNode> reacceptRegisteredWorkers(Collection<ResourceID> toConsolidate) {\n\n\t\t// we check for each task manager if we recognize its Mesos task ID\n\t\tList<RegisteredMesosWorkerNode> accepted = new ArrayList<>(toConsolidate.size());\n\t\tfor (ResourceID resourceID : toConsolidate) {\n\t\t\tMesosWorkerStore.Worker worker = workersInLaunch.remove(resourceID);\n\t\t\tif (worker != null) {\n\t\t\t\tLOG.info(\"Mesos worker consolidation recognizes TaskManager {}.\", resourceID);\n\t\t\t\taccepted.add(new RegisteredMesosWorkerNode(worker));\n\t\t\t}\n\t\t\telse {\n\t\t\t\tif(isStarted(resourceID)) {\n\t\t\t\t\tLOG.info(\"TaskManager {} has already been registered at the resource manager.\", resourceID);\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tLOG.info(\"Mesos worker consolidation does not recognize TaskManager {}.\", resourceID);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn accepted;\n\t}",
            " 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471 +\n 472 +\n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  ",
            "\t/**\n\t * Accept the given registered workers into the internal state.\n\t *\n\t * @param toConsolidate The worker IDs known previously to the JobManager.\n\t * @return A collection of registered worker node records.\n\t */\n\t@Override\n\tprotected Collection<RegisteredMesosWorkerNode> reacceptRegisteredWorkers(Collection<ResourceID> toConsolidate) {\n\n\t\t// we check for each task manager if we recognize its Mesos task ID\n\t\tList<RegisteredMesosWorkerNode> accepted = new ArrayList<>(toConsolidate.size());\n\t\tfor (ResourceID resourceID : toConsolidate) {\n\t\t\tMesosWorkerStore.Worker worker = workersInLaunch.remove(resourceID);\n\t\t\tif (worker != null) {\n\t\t\t\tLOG.info(\"Mesos worker consolidation recognizes TaskManager {}.\", resourceID);\n\t\t\t\taccepted.add(new RegisteredMesosWorkerNode(worker));\n\t\t\t} else {\n\t\t\t\tif (isStarted(resourceID)) {\n\t\t\t\t\tLOG.info(\"TaskManager {} has already been registered at the resource manager.\", resourceID);\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\tLOG.info(\"Mesos worker consolidation does not recognize TaskManager {}.\", resourceID);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn accepted;\n\t}"
        ],
        [
            "FlinkMesosSessionCli::decodeDynamicProperties(String)",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44 -\n  45 -\n  46 -\n  47  \n  48  \n  49  \n  50  \n  51  \n  52 -\n  53 -\n  54  \n  55  \n  56  ",
            "\t/**\n\t * Decode encoded dynamic properties.\n\t * @param dynamicPropertiesEncoded encoded properties produced by the encoding method.\n\t * @return a configuration instance to be merged with the static configuration.\n\t */\n\tpublic static Configuration decodeDynamicProperties(String dynamicPropertiesEncoded) {\n\t\ttry {\n\t\t\tConfiguration configuration = new Configuration();\n\t\t\tif(dynamicPropertiesEncoded != null) {\n\t\t\t\tTypeReference<Map<String, String>> typeRef = new TypeReference<Map<String, String>>() {};\n\t\t\t\tMap<String,String> props = mapper.readValue(dynamicPropertiesEncoded, typeRef);\n\t\t\t\tfor (Map.Entry<String, String> property : props.entrySet()) {\n\t\t\t\t\tconfiguration.setString(property.getKey(), property.getValue());\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn configuration;\n\t\t}\n\t\tcatch(IOException ex) {\n\t\t\tthrow new IllegalArgumentException(\"unreadable encoded properties\", ex);\n\t\t}\n\t}",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46 +\n  47 +\n  48 +\n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55 +\n  56  \n  57  \n  58  ",
            "\t/**\n\t * Decode encoded dynamic properties.\n\t *\n\t * @param dynamicPropertiesEncoded encoded properties produced by the encoding method.\n\t * @return a configuration instance to be merged with the static configuration.\n\t */\n\tpublic static Configuration decodeDynamicProperties(String dynamicPropertiesEncoded) {\n\t\ttry {\n\t\t\tConfiguration configuration = new Configuration();\n\t\t\tif (dynamicPropertiesEncoded != null) {\n\t\t\t\tTypeReference<Map<String, String>> typeRef = new TypeReference<Map<String, String>>() {\n\t\t\t\t};\n\t\t\t\tMap<String, String> props = mapper.readValue(dynamicPropertiesEncoded, typeRef);\n\t\t\t\tfor (Map.Entry<String, String> property : props.entrySet()) {\n\t\t\t\t\tconfiguration.setString(property.getKey(), property.getValue());\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn configuration;\n\t\t} catch (IOException ex) {\n\t\t\tthrow new IllegalArgumentException(\"unreadable encoded properties\", ex);\n\t\t}\n\t}"
        ],
        [
            "MesosConfiguration::logMesosConfig(Logger,MesosConfiguration)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 -\n 141  \n 142  \n 143 -\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "\t/**\n\t * A utility method to log relevant Mesos connection info\n     */\n\tpublic static void logMesosConfig(Logger log, MesosConfiguration config) {\n\n\t\tMap<String,String> env = System.getenv();\n\t\tProtos.FrameworkInfo.Builder info = config.frameworkInfo();\n\n\t\tlog.info(\"--------------------------------------------------------------------------------\");\n\t\tlog.info(\" Mesos Info:\");\n\t\tlog.info(\"    Master URL: {}\", config.masterUrl());\n\n\t\tlog.info(\" Framework Info:\");\n\t\tlog.info(\"    ID: {}\", info.hasId() ? info.getId().getValue() : \"(none)\");\n\t\tlog.info(\"    Name: {}\", info.hasName() ? info.getName() : \"(none)\");\n\t\tlog.info(\"    Failover Timeout (secs): {}\", info.getFailoverTimeout());\n\t\tlog.info(\"    Role: {}\", info.hasRole() ? info.getRole() : \"(none)\");\n\t\tlog.info(\"    Principal: {}\", info.hasPrincipal() ? info.getPrincipal() : \"(none)\");\n\t\tlog.info(\"    Host: {}\", info.hasHostname() ? info.getHostname() : \"(none)\");\n\t\tif(env.containsKey(\"LIBPROCESS_IP\")) {\n\t\t\tlog.info(\"    LIBPROCESS_IP: {}\", env.get(\"LIBPROCESS_IP\"));\n\t\t}\n\t\tif(env.containsKey(\"LIBPROCESS_PORT\")) {\n\t\t\tlog.info(\"    LIBPROCESS_PORT: {}\", env.get(\"LIBPROCESS_PORT\"));\n\t\t}\n\t\tlog.info(\"    Web UI: {}\", info.hasWebuiUrl() ? info.getWebuiUrl() : \"(none)\");\n\n\t\tlog.info(\"--------------------------------------------------------------------------------\");\n\n\t}",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  \n 143  \n 144 +\n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "\t/**\n\t * A utility method to log relevant Mesos connection info.\n\t */\n\tpublic static void logMesosConfig(Logger log, MesosConfiguration config) {\n\n\t\tMap<String, String> env = System.getenv();\n\t\tProtos.FrameworkInfo.Builder info = config.frameworkInfo();\n\n\t\tlog.info(\"--------------------------------------------------------------------------------\");\n\t\tlog.info(\" Mesos Info:\");\n\t\tlog.info(\"    Master URL: {}\", config.masterUrl());\n\n\t\tlog.info(\" Framework Info:\");\n\t\tlog.info(\"    ID: {}\", info.hasId() ? info.getId().getValue() : \"(none)\");\n\t\tlog.info(\"    Name: {}\", info.hasName() ? info.getName() : \"(none)\");\n\t\tlog.info(\"    Failover Timeout (secs): {}\", info.getFailoverTimeout());\n\t\tlog.info(\"    Role: {}\", info.hasRole() ? info.getRole() : \"(none)\");\n\t\tlog.info(\"    Principal: {}\", info.hasPrincipal() ? info.getPrincipal() : \"(none)\");\n\t\tlog.info(\"    Host: {}\", info.hasHostname() ? info.getHostname() : \"(none)\");\n\t\tif (env.containsKey(\"LIBPROCESS_IP\")) {\n\t\t\tlog.info(\"    LIBPROCESS_IP: {}\", env.get(\"LIBPROCESS_IP\"));\n\t\t}\n\t\tif (env.containsKey(\"LIBPROCESS_PORT\")) {\n\t\t\tlog.info(\"    LIBPROCESS_PORT: {}\", env.get(\"LIBPROCESS_PORT\"));\n\t\t}\n\t\tlog.info(\"    Web UI: {}\", info.hasWebuiUrl() ? info.getWebuiUrl() : \"(none)\");\n\n\t\tlog.info(\"--------------------------------------------------------------------------------\");\n\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testRecoverWorkers()",
            " 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304 -\n 305 -\n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "\t/**\n\t * Test recovery of persistent workers.\n\t */\n\t@Test\n\tpublic void testRecoverWorkers() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state then initialize the RM\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1 = MesosWorkerStore.Worker.newWorker(task1);\n\t\t\t\t\t\tMesosWorkerStore.Worker worker2 = MesosWorkerStore.Worker.newWorker(task2).launchWorker(slave1, slave1host);\n\t\t\t\t\t\tMesosWorkerStore.Worker worker3 = MesosWorkerStore.Worker.newWorker(task3).launchWorker(slave1, slave1host).releaseWorker();\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(Arrays.asList(worker1, worker2, worker3));\n\t\t\t\t\t\tinitialize();\n\n\t\t\t\t\t\t// verify that the internal state was updated, the task router was notified,\n\t\t\t\t\t\t// and the launch coordinator was asked to launch a task\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInNew, hasEntry(extractResourceID(task1), worker1));\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch, hasEntry(extractResourceID(task2), worker2));\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersBeingReturned, hasEntry(extractResourceID(task3), worker3));\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(TaskMonitor.TaskGoalStateUpdated.class);\n\t\t\t\t\t\tLaunchCoordinator.Assign actualAssign =\n\t\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(LaunchCoordinator.Assign.class);\n\t\t\t\t\t\tassertThat(actualAssign.tasks(), hasSize(1));\n\t\t\t\t\t\tassertThat(actualAssign.tasks().get(0).f0.getId(), equalTo(task2.getValue()));\n\t\t\t\t\t\tassertThat(actualAssign.tasks().get(0).f1, equalTo(slave1host));\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(LaunchCoordinator.Launch.class);\n\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 +\n 336  \n 337  \n 338  \n 339  \n 340  \n 341  ",
            "\t/**\n\t * Test recovery of persistent workers.\n\t */\n\t@Test\n\tpublic void testRecoverWorkers() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state then initialize the RM\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1 = MesosWorkerStore.Worker.newWorker(task1);\n\t\t\t\t\t\tMesosWorkerStore.Worker worker2 = MesosWorkerStore.Worker.newWorker(task2).launchWorker(slave1, slave1host);\n\t\t\t\t\t\tMesosWorkerStore.Worker worker3 = MesosWorkerStore.Worker.newWorker(task3).launchWorker(slave1, slave1host).releaseWorker();\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(Arrays.asList(worker1, worker2, worker3));\n\t\t\t\t\t\tinitialize();\n\n\t\t\t\t\t\t// verify that the internal state was updated, the task router was notified,\n\t\t\t\t\t\t// and the launch coordinator was asked to launch a task\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInNew, hasEntry(extractResourceID(task1), worker1));\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch, hasEntry(extractResourceID(task2), worker2));\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersBeingReturned, hasEntry(extractResourceID(task3), worker3));\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(TaskMonitor.TaskGoalStateUpdated.class);\n\t\t\t\t\t\tLaunchCoordinator.Assign actualAssign =\n\t\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(LaunchCoordinator.Assign.class);\n\t\t\t\t\t\tassertThat(actualAssign.tasks(), hasSize(1));\n\t\t\t\t\t\tassertThat(actualAssign.tasks().get(0).f0.getId(), equalTo(task2.getValue()));\n\t\t\t\t\t\tassertThat(actualAssign.tasks().get(0).f1, equalTo(slave1host));\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(LaunchCoordinator.Launch.class);\n\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosTaskManagerParameters::create(Configuration)",
            " 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 -\n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 -\n 225  \n 226  \n 227  \n 228  \n 229  \n 230 -\n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253 -\n 254  \n 255  \n 256  \n 257  \n 258  ",
            "\t/**\n\t * Create the Mesos TaskManager parameters.\n\t * @param flinkConfig the TM configuration.\n     */\n\tpublic static MesosTaskManagerParameters create(Configuration flinkConfig) {\n\n\t\tList<ConstraintEvaluator> constraints = parseConstraints(flinkConfig.getString(MESOS_CONSTRAINTS_HARD_HOSTATTR));\n\t\t// parse the common parameters\n\t\tContaineredTaskManagerParameters containeredParameters = ContaineredTaskManagerParameters.create(\n\t\t\tflinkConfig,\n\t\t\tflinkConfig.getInteger(MESOS_RM_TASKS_MEMORY_MB),\n\t\t\tflinkConfig.getInteger(MESOS_RM_TASKS_SLOTS));\n\n\t\tdouble cpus = flinkConfig.getDouble(MESOS_RM_TASKS_CPUS);\n\t\tif(cpus <= 0.0) {\n\t\t\tcpus = Math.max(containeredParameters.numSlots(), 1.0);\n\t\t}\n\n\t\t// parse the containerization parameters\n\t\tString imageName = flinkConfig.getString(MESOS_RM_CONTAINER_IMAGE_NAME);\n\n\t\tContainerType containerType;\n\t\tString containerTypeString = flinkConfig.getString(MESOS_RM_CONTAINER_TYPE);\n\t\tswitch(containerTypeString) {\n\t\t\tcase MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE_MESOS:\n\t\t\t\tcontainerType = ContainerType.MESOS;\n\t\t\t\tbreak;\n\t\t\tcase MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE_DOCKER:\n\t\t\t\tcontainerType = ContainerType.DOCKER;\n\t\t\t\tif(imageName == null || imageName.length() == 0) {\n\t\t\t\t\tthrow new IllegalConfigurationException(MESOS_RM_CONTAINER_IMAGE_NAME.key() +\n\t\t\t\t\t\t\" must be specified for docker container type\");\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalConfigurationException(\"invalid container type: \" + containerTypeString);\n\t\t}\n\n\t\tOption<String> containerVolOpt = Option.<String>apply(flinkConfig.getString(MESOS_RM_CONTAINER_VOLUMES));\n\n\t\tList<Protos.Volume> containerVolumes = buildVolumes(containerVolOpt);\n\n\t\t//obtain Task Manager Host Name from the configuration\n\t\tOption<String> taskManagerHostname = Option.apply(flinkConfig.getString(MESOS_TM_HOSTNAME));\n\n\t\t//obtain bootstrap command from the configuration\n\t\tOption<String> tmBootstrapCommand = Option.apply(flinkConfig.getString(MESOS_TM_BOOTSTRAP_CMD));\n\n\t\treturn new MesosTaskManagerParameters(\n\t\t\tcpus,\n\t\t\tcontainerType,\n\t\t\tOption.apply(imageName),\n\t\t\tcontaineredParameters,\t\t\t\n\t\t\tcontainerVolumes,\n\t\t\tconstraints,\n\t\t\ttmBootstrapCommand,\n\t\t\ttaskManagerHostname);\n\t}",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 +\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231  \n 232  \n 233  \n 234  \n 235  \n 236 +\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260  \n 261  \n 262  \n 263  \n 264  ",
            "\t/**\n\t * Create the Mesos TaskManager parameters.\n\t *\n\t * @param flinkConfig the TM configuration.\n\t */\n\tpublic static MesosTaskManagerParameters create(Configuration flinkConfig) {\n\n\t\tList<ConstraintEvaluator> constraints = parseConstraints(flinkConfig.getString(MESOS_CONSTRAINTS_HARD_HOSTATTR));\n\t\t// parse the common parameters\n\t\tContaineredTaskManagerParameters containeredParameters = ContaineredTaskManagerParameters.create(\n\t\t\tflinkConfig,\n\t\t\tflinkConfig.getInteger(MESOS_RM_TASKS_MEMORY_MB),\n\t\t\tflinkConfig.getInteger(MESOS_RM_TASKS_SLOTS));\n\n\t\tdouble cpus = flinkConfig.getDouble(MESOS_RM_TASKS_CPUS);\n\t\tif (cpus <= 0.0) {\n\t\t\tcpus = Math.max(containeredParameters.numSlots(), 1.0);\n\t\t}\n\n\t\t// parse the containerization parameters\n\t\tString imageName = flinkConfig.getString(MESOS_RM_CONTAINER_IMAGE_NAME);\n\n\t\tContainerType containerType;\n\t\tString containerTypeString = flinkConfig.getString(MESOS_RM_CONTAINER_TYPE);\n\t\tswitch (containerTypeString) {\n\t\t\tcase MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE_MESOS:\n\t\t\t\tcontainerType = ContainerType.MESOS;\n\t\t\t\tbreak;\n\t\t\tcase MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE_DOCKER:\n\t\t\t\tcontainerType = ContainerType.DOCKER;\n\t\t\t\tif (imageName == null || imageName.length() == 0) {\n\t\t\t\t\tthrow new IllegalConfigurationException(MESOS_RM_CONTAINER_IMAGE_NAME.key() +\n\t\t\t\t\t\t\" must be specified for docker container type\");\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalConfigurationException(\"invalid container type: \" + containerTypeString);\n\t\t}\n\n\t\tOption<String> containerVolOpt = Option.<String>apply(flinkConfig.getString(MESOS_RM_CONTAINER_VOLUMES));\n\n\t\tList<Protos.Volume> containerVolumes = buildVolumes(containerVolOpt);\n\n\t\t//obtain Task Manager Host Name from the configuration\n\t\tOption<String> taskManagerHostname = Option.apply(flinkConfig.getString(MESOS_TM_HOSTNAME));\n\n\t\t//obtain bootstrap command from the configuration\n\t\tOption<String> tmBootstrapCommand = Option.apply(flinkConfig.getString(MESOS_TM_BOOTSTRAP_CMD));\n\n\t\treturn new MesosTaskManagerParameters(\n\t\t\tcpus,\n\t\t\tcontainerType,\n\t\t\tOption.apply(imageName),\n\t\t\tcontaineredParameters,\n\t\t\tcontainerVolumes,\n\t\t\tconstraints,\n\t\t\ttmBootstrapCommand,\n\t\t\ttaskManagerHostname);\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testStopApplication()",
            " 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635 -\n 636 -\n 637  \n 638  \n 639  \n 640  \n 641  \n 642  ",
            "\t/**\n\t * Test cluster stop handling.\n\t */\n\t@Test\n\tpublic void testStopApplication() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\t\t\t\t\t\twatch(resourceManager.actor());\n\t\t\t\t\t\tresourceManager.tell(new StopCluster(ApplicationStatus.SUCCEEDED, \"\"), resourceManager);\n\n\t\t\t\t\t\t// verify that the Mesos framework is shutdown\n\t\t\t\t\t\tverify(schedulerDriver).stop(false);\n\t\t\t\t\t\tverify(workerStore).stop(true);\n\t\t\t\t\t\texpectTerminated(resourceManager.actor());\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658 +\n 659  \n 660  \n 661  \n 662  \n 663  \n 664  ",
            "\t/**\n\t * Test cluster stop handling.\n\t */\n\t@Test\n\tpublic void testStopApplication() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\t\t\t\t\t\twatch(resourceManager.actor());\n\t\t\t\t\t\tresourceManager.tell(new StopCluster(ApplicationStatus.SUCCEEDED, \"\"), resourceManager);\n\n\t\t\t\t\t\t// verify that the Mesos framework is shutdown\n\t\t\t\t\t\tverify(schedulerDriver).stop(false);\n\t\t\t\t\t\tverify(workerStore).stop(true);\n\t\t\t\t\t\texpectTerminated(resourceManager.actor());\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testAcceptOffers()",
            " 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511 -\n 512 -\n 513  \n 514  \n 515  \n 516  \n 517  \n 518  ",
            "\t/**\n\t * Test offer acceptance.\n\t */\n\t@Test\n\tpublic void testAcceptOffers() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state with a new task then initialize the RM\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1 = MesosWorkerStore.Worker.newWorker(task1);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInNew, hasEntry(extractResourceID(task1), worker1));\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(TaskMonitor.TaskGoalStateUpdated.class);\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\t// send an AcceptOffers message as the LaunchCoordinator would\n\t\t\t\t\t\t// to launch task1 onto slave1 with offer1\n\t\t\t\t\t\tProtos.TaskInfo task1info = Protos.TaskInfo.newBuilder()\n\t\t\t\t\t\t\t.setTaskId(task1).setName(\"\").setSlaveId(slave1).build();\n\t\t\t\t\t\tAcceptOffers msg = new AcceptOffers(slave1host, singletonList(offer1), singletonList(launch(task1info)));\n\t\t\t\t\t\tresourceManager.tell(msg);\n\n\t\t\t\t\t\t// verify that the worker was persisted, the internal state was updated,\n\t\t\t\t\t\t// Mesos was asked to launch task1, and the task router was notified\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = worker1.launchWorker(slave1, slave1host);\n\t\t\t\t\t\tverify(workerStore).putWorker(worker1launched);\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInNew.entrySet(), empty());\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch, hasEntry(extractResourceID(task1), worker1launched));\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsg(\n\t\t\t\t\t\t\tnew TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker1launched)));\n\t\t\t\t\t\tverify(schedulerDriver).acceptOffers(msg.offerIds(), msg.operations(), msg.filters());\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537 +\n 538  \n 539  \n 540  \n 541  \n 542  \n 543  ",
            "\t/**\n\t * Test offer acceptance.\n\t */\n\t@Test\n\tpublic void testAcceptOffers() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state with a new task then initialize the RM\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1 = MesosWorkerStore.Worker.newWorker(task1);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInNew, hasEntry(extractResourceID(task1), worker1));\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(TaskMonitor.TaskGoalStateUpdated.class);\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\t// send an AcceptOffers message as the LaunchCoordinator would\n\t\t\t\t\t\t// to launch task1 onto slave1 with offer1\n\t\t\t\t\t\tProtos.TaskInfo task1info = Protos.TaskInfo.newBuilder()\n\t\t\t\t\t\t\t.setTaskId(task1).setName(\"\").setSlaveId(slave1).build();\n\t\t\t\t\t\tAcceptOffers msg = new AcceptOffers(slave1host, singletonList(offer1), singletonList(launch(task1info)));\n\t\t\t\t\t\tresourceManager.tell(msg);\n\n\t\t\t\t\t\t// verify that the worker was persisted, the internal state was updated,\n\t\t\t\t\t\t// Mesos was asked to launch task1, and the task router was notified\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = worker1.launchWorker(slave1, slave1host);\n\t\t\t\t\t\tverify(workerStore).putWorker(worker1launched);\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInNew.entrySet(), empty());\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch, hasEntry(extractResourceID(task1), worker1launched));\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsg(\n\t\t\t\t\t\t\tnew TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker1launched)));\n\t\t\t\t\t\tverify(schedulerDriver).acceptOffers(msg.offerIds(), msg.operations(), msg.filters());\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManager::extractGoalState(MesosWorkerStore)",
            " 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698 -\n 699 -\n 700 -\n 701 -\n 702 -\n 703  \n 704  ",
            "\t/**\n\t * Extracts the Mesos task goal state from the worker information.\n\t * @param worker the persistent worker information.\n\t * @return goal state information for the {@Link TaskMonitor}.\n\t */\n\tstatic TaskMonitor.TaskGoalState extractGoalState(MesosWorkerStore.Worker worker) {\n\t\tswitch(worker.state()) {\n\t\t\tcase New: return new TaskMonitor.New(worker.taskID());\n\t\t\tcase Launched: return new TaskMonitor.Launched(worker.taskID(), worker.slaveID().get());\n\t\t\tcase Released: return new TaskMonitor.Released(worker.taskID(), worker.slaveID().get());\n\t\t\tdefault: throw new IllegalArgumentException(\"unsupported worker state\");\n\t\t}\n\t}",
            " 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693 +\n 694 +\n 695 +\n 696 +\n 697 +\n 698 +\n 699 +\n 700 +\n 701 +\n 702  \n 703  ",
            "\t/**\n\t * Extracts the Mesos task goal state from the worker information.\n\t * @param worker the persistent worker information.\n\t * @return goal state information for the {@Link TaskMonitor}.\n\t */\n\tstatic TaskMonitor.TaskGoalState extractGoalState(MesosWorkerStore.Worker worker) {\n\t\tswitch (worker.state()) {\n\t\t\tcase New:\n\t\t\t\treturn new TaskMonitor.New(worker.taskID());\n\t\t\tcase Launched:\n\t\t\t\treturn new TaskMonitor.Launched(worker.taskID(), worker.slaveID().get());\n\t\t\tcase Released:\n\t\t\t\treturn new TaskMonitor.Released(worker.taskID(), worker.slaveID().get());\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalArgumentException(\"unsupported worker state\");\n\t\t}\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testReacceptRegisteredWorkers()",
            " 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 -\n 335 -\n 336  \n 337  \n 338  \n 339  \n 340  \n 341  ",
            "\t/**\n\t * Test re-acceptance of registered workers upon JM registration.\n\t */\n\t@Test\n\tpublic void testReacceptRegisteredWorkers() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state then initialize the RM\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1launched));\n\t\t\t\t\t\tinitialize();\n\n\t\t\t\t\t\t// send RegisterResourceManagerSuccessful to the RM with some 'known' workers.\n\t\t\t\t\t\t// This will cause the RM to reaccept the workers.\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch, hasEntry(extractResourceID(task1), worker1launched));\n\t\t\t\t\t\tregister(singletonList(extractResourceID(task1)));\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch.entrySet(), empty());\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364 +\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  ",
            "\t/**\n\t * Test re-acceptance of registered workers upon JM registration.\n\t */\n\t@Test\n\tpublic void testReacceptRegisteredWorkers() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state then initialize the RM\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1launched));\n\t\t\t\t\t\tinitialize();\n\n\t\t\t\t\t\t// send RegisterResourceManagerSuccessful to the RM with some 'known' workers.\n\t\t\t\t\t\t// This will cause the RM to reaccept the workers.\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch, hasEntry(extractResourceID(task1), worker1launched));\n\t\t\t\t\t\tregister(singletonList(extractResourceID(task1)));\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch.entrySet(), empty());\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosTaskManagerParametersTest::givenEmptyConstraintInConfigShouldBeParsed()",
            " 117  \n 118 -\n 119 -\n 120 -\n 121 -\n 122 -",
            "    @Test\n    public void givenEmptyConstraintInConfigShouldBeParsed() throws Exception {\n\n        MesosTaskManagerParameters mesosTaskManagerParameters = MesosTaskManagerParameters.create(withHardHostAttrConstraintConfiguration(\"\"));\n        assertThat(mesosTaskManagerParameters.constraints().size(), is(0));\n    }",
            " 123  \n 124 +\n 125 +\n 126 +\n 127 +\n 128 +",
            "\t@Test\n\tpublic void givenEmptyConstraintInConfigShouldBeParsed() throws Exception {\n\n\t\tMesosTaskManagerParameters mesosTaskManagerParameters = MesosTaskManagerParameters.create(withHardHostAttrConstraintConfiguration(\"\"));\n\t\tassertThat(mesosTaskManagerParameters.constraints().size(), is(0));\n\t}"
        ],
        [
            "MesosFlinkResourceManager::shutdownApplication(ApplicationStatus,String)",
            " 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270 -\n 271 -\n 272  \n 273  \n 274  \n 275  \n 276  \n 277 -\n 278 -\n 279  \n 280  \n 281  \n 282  \n 283  ",
            "\t/**\n\t * Called to shut down the cluster (not a failover situation).\n\t *\n\t * @param finalStatus The application status to report.\n\t * @param optionalDiagnostics An optional diagnostics message.\n\t */\n\t@Override\n\tprotected void shutdownApplication(ApplicationStatus finalStatus, String optionalDiagnostics) {\n\n\t\tLOG.info(\"Shutting down and unregistering as a Mesos framework.\");\n\t\ttry {\n\t\t\t// unregister the framework, which implicitly removes all tasks.\n\t\t\tschedulerDriver.stop(false);\n\t\t}\n\t\tcatch(Exception ex) {\n\t\t\tLOG.warn(\"unable to unregister the framework\", ex);\n\t\t}\n\n\t\ttry {\n\t\t\tworkerStore.stop(true);\n\t\t}\n\t\tcatch(Exception ex) {\n\t\t\tLOG.warn(\"unable to stop the worker state store\", ex);\n\t\t}\n\n\t\tcontext().stop(self());\n\t}",
            " 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272 +\n 273  \n 274  \n 275  \n 276  \n 277  \n 278 +\n 279  \n 280  \n 281  \n 282  \n 283  ",
            "\t/**\n\t * Called to shut down the cluster (not a failover situation).\n\t *\n\t * @param finalStatus The application status to report.\n\t * @param optionalDiagnostics An optional diagnostics message.\n\t */\n\t@Override\n\tprotected void shutdownApplication(ApplicationStatus finalStatus, String optionalDiagnostics) {\n\n\t\tLOG.info(\"Shutting down and unregistering as a Mesos framework.\");\n\t\ttry {\n\t\t\t// unregister the framework, which implicitly removes all tasks.\n\t\t\tschedulerDriver.stop(false);\n\t\t} catch (Exception ex) {\n\t\t\tLOG.warn(\"unable to unregister the framework\", ex);\n\t\t}\n\n\t\ttry {\n\t\t\tworkerStore.stop(true);\n\t\t} catch (Exception ex) {\n\t\t\tLOG.warn(\"unable to stop the worker state store\", ex);\n\t\t}\n\n\t\tcontext().stop(self());\n\t}"
        ],
        [
            "MesosApplicationMasterRunner::createMesosConfig(Configuration,String)",
            " 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496 -\n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520 -\n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529 -\n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  ",
            "\t/**\n\t * Loads and validates the ResourceManager Mesos configuration from the given Flink configuration.\n\t */\n\tpublic static MesosConfiguration createMesosConfig(Configuration flinkConfig, String hostname) {\n\n\t\tProtos.FrameworkInfo.Builder frameworkInfo = Protos.FrameworkInfo.newBuilder()\n\t\t\t.setHostname(hostname);\n\t\tProtos.Credential.Builder credential = null;\n\n\t\tif(!flinkConfig.containsKey(ConfigConstants.MESOS_MASTER_URL)) {\n\t\t\tthrow new IllegalConfigurationException(ConfigConstants.MESOS_MASTER_URL + \" must be configured.\");\n\t\t}\n\t\tString masterUrl = flinkConfig.getString(ConfigConstants.MESOS_MASTER_URL, null);\n\n\t\tDuration failoverTimeout = FiniteDuration.apply(\n\t\t\tflinkConfig.getInteger(\n\t\t\t\tConfigConstants.MESOS_FAILOVER_TIMEOUT_SECONDS,\n\t\t\t\tConfigConstants.DEFAULT_MESOS_FAILOVER_TIMEOUT_SECS),\n\t\t\tTimeUnit.SECONDS);\n\t\tframeworkInfo.setFailoverTimeout(failoverTimeout.toSeconds());\n\n\t\tframeworkInfo.setName(flinkConfig.getString(\n\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_NAME,\n\t\t\tConfigConstants.DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_NAME));\n\n\t\tframeworkInfo.setRole(flinkConfig.getString(\n\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_ROLE,\n\t\t\tConfigConstants.DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_ROLE));\n\n\t\tframeworkInfo.setUser(flinkConfig.getString(\n\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_USER,\n\t\t\tConfigConstants.DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_USER));\n\n\t\tif(flinkConfig.containsKey(ConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_PRINCIPAL)) {\n\t\t\tframeworkInfo.setPrincipal(flinkConfig.getString(\n\t\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_PRINCIPAL, null));\n\n\t\t\tcredential = Protos.Credential.newBuilder();\n\t\t\tcredential.setPrincipal(frameworkInfo.getPrincipal());\n\n\t\t\t// some environments use a side-channel to communicate the secret to Mesos,\n\t\t\t// and thus don't set the 'secret' configuration setting\n\t\t\tif(flinkConfig.containsKey(ConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_SECRET)) {\n\t\t\t\tcredential.setSecret(flinkConfig.getString(\n\t\t\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_SECRET, null));\n\t\t\t}\n\t\t}\n\n\t\tMesosConfiguration mesos =\n\t\t\tnew MesosConfiguration(masterUrl, frameworkInfo, scala.Option.apply(credential));\n\n\t\treturn mesos;\n\t}",
            " 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495 +\n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519 +\n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528 +\n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  ",
            "\t/**\n\t * Loads and validates the ResourceManager Mesos configuration from the given Flink configuration.\n\t */\n\tpublic static MesosConfiguration createMesosConfig(Configuration flinkConfig, String hostname) {\n\n\t\tProtos.FrameworkInfo.Builder frameworkInfo = Protos.FrameworkInfo.newBuilder()\n\t\t\t.setHostname(hostname);\n\t\tProtos.Credential.Builder credential = null;\n\n\t\tif (!flinkConfig.containsKey(ConfigConstants.MESOS_MASTER_URL)) {\n\t\t\tthrow new IllegalConfigurationException(ConfigConstants.MESOS_MASTER_URL + \" must be configured.\");\n\t\t}\n\t\tString masterUrl = flinkConfig.getString(ConfigConstants.MESOS_MASTER_URL, null);\n\n\t\tDuration failoverTimeout = FiniteDuration.apply(\n\t\t\tflinkConfig.getInteger(\n\t\t\t\tConfigConstants.MESOS_FAILOVER_TIMEOUT_SECONDS,\n\t\t\t\tConfigConstants.DEFAULT_MESOS_FAILOVER_TIMEOUT_SECS),\n\t\t\tTimeUnit.SECONDS);\n\t\tframeworkInfo.setFailoverTimeout(failoverTimeout.toSeconds());\n\n\t\tframeworkInfo.setName(flinkConfig.getString(\n\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_NAME,\n\t\t\tConfigConstants.DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_NAME));\n\n\t\tframeworkInfo.setRole(flinkConfig.getString(\n\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_ROLE,\n\t\t\tConfigConstants.DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_ROLE));\n\n\t\tframeworkInfo.setUser(flinkConfig.getString(\n\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_USER,\n\t\t\tConfigConstants.DEFAULT_MESOS_RESOURCEMANAGER_FRAMEWORK_USER));\n\n\t\tif (flinkConfig.containsKey(ConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_PRINCIPAL)) {\n\t\t\tframeworkInfo.setPrincipal(flinkConfig.getString(\n\t\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_PRINCIPAL, null));\n\n\t\t\tcredential = Protos.Credential.newBuilder();\n\t\t\tcredential.setPrincipal(frameworkInfo.getPrincipal());\n\n\t\t\t// some environments use a side-channel to communicate the secret to Mesos,\n\t\t\t// and thus don't set the 'secret' configuration setting\n\t\t\tif (flinkConfig.containsKey(ConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_SECRET)) {\n\t\t\t\tcredential.setSecret(flinkConfig.getString(\n\t\t\t\t\tConfigConstants.MESOS_RESOURCEMANAGER_FRAMEWORK_SECRET, null));\n\t\t\t}\n\t\t}\n\n\t\tMesosConfiguration mesos =\n\t\t\tnew MesosConfiguration(masterUrl, frameworkInfo, scala.Option.apply(credential));\n\n\t\treturn mesos;\n\t}"
        ],
        [
            "MesosArtifactServer::VirtualFileServerHandler::VirtualFileServerHandler(Path)",
            " 275  \n 276  \n 277 -\n 278  \n 279  \n 280  \n 281 -\n 282  \n 283  \n 284  ",
            "\t\tpublic VirtualFileServerHandler(Path path) throws IOException {\n\t\t\tthis.path = path;\n\t\t\tif(!path.isAbsolute()) {\n\t\t\t\tthrow new IllegalArgumentException(\"path must be absolute: \" + path.toString());\n\t\t\t}\n\t\t\tthis.fs = path.getFileSystem();\n\t\t\tif(!fs.exists(path) || fs.getFileStatus(path).isDir()) {\n\t\t\t\tthrow new IllegalArgumentException(\"no such file: \" + path.toString());\n\t\t\t}\n\t\t}",
            " 276  \n 277  \n 278 +\n 279  \n 280  \n 281  \n 282 +\n 283  \n 284  \n 285  ",
            "\t\tpublic VirtualFileServerHandler(Path path) throws IOException {\n\t\t\tthis.path = path;\n\t\t\tif (!path.isAbsolute()) {\n\t\t\t\tthrow new IllegalArgumentException(\"path must be absolute: \" + path.toString());\n\t\t\t}\n\t\t\tthis.fs = path.getFileSystem();\n\t\t\tif (!fs.exists(path) || fs.getFileStatus(path).isDir()) {\n\t\t\t\tthrow new IllegalArgumentException(\"no such file: \" + path.toString());\n\t\t\t}\n\t\t}"
        ],
        [
            "MesosApplicationMasterRunner::configureArtifactServer(MesosArtifactServer,ContainerSpecification)",
            " 566  \n 567  \n 568 -\n 569  \n 570  \n 571  ",
            "\tprivate static void configureArtifactServer(MesosArtifactServer server, ContainerSpecification container) throws IOException {\n\t\t// serve the artifacts associated with the container environment\n\t\tfor(ContainerSpecification.Artifact artifact : container.getArtifacts()) {\n\t\t\tserver.addPath(artifact.source, artifact.dest);\n\t\t}\n\t}",
            " 565  \n 566  \n 567 +\n 568  \n 569  \n 570  ",
            "\tprivate static void configureArtifactServer(MesosArtifactServer server, ContainerSpecification container) throws IOException {\n\t\t// serve the artifacts associated with the container environment\n\t\tfor (ContainerSpecification.Artifact artifact : container.getArtifacts()) {\n\t\t\tserver.addPath(artifact.source, artifact.dest);\n\t\t}\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testDisconnected()",
            " 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729 -\n 730 -\n 731  \n 732  \n 733  \n 734  \n 735  \n 736  ",
            "\t/**\n\t * Test Mesos re-registration handling.\n\t */\n\t@Test\n\tpublic void testDisconnected() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\tresourceManager.tell(new Disconnected(), resourceManager);\n\n\t\t\t\t\t\tresourceManagerInstance.connectionMonitor.expectMsgClass(Disconnected.class);\n\t\t\t\t\t\tresourceManagerInstance.reconciliationCoordinator.expectMsgClass(Disconnected.class);\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(Disconnected.class);\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(Disconnected.class);\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749 +\n 750  \n 751  \n 752  \n 753  \n 754  \n 755  ",
            "\t/**\n\t * Test Mesos re-registration handling.\n\t */\n\t@Test\n\tpublic void testDisconnected() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\tresourceManager.tell(new Disconnected(), resourceManager);\n\n\t\t\t\t\t\tresourceManagerInstance.connectionMonitor.expectMsgClass(Disconnected.class);\n\t\t\t\t\t\tresourceManagerInstance.reconciliationCoordinator.expectMsgClass(Disconnected.class);\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(Disconnected.class);\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(Disconnected.class);\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManager::requestNewWorkers(int)",
            " 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377 -\n 378  \n 379  \n 380 -\n 381 -\n 382  \n 383  \n 384  ",
            "\t/**\n\t * Plan for some additional workers to be launched.\n\t *\n\t * @param numWorkers The number of workers to allocate.\n\t */\n\t@Override\n\tprotected void requestNewWorkers(int numWorkers) {\n\n\t\ttry {\n\t\t\tList<TaskMonitor.TaskGoalStateUpdated> toMonitor = new ArrayList<>(numWorkers);\n\t\t\tList<LaunchableTask> toLaunch = new ArrayList<>(numWorkers);\n\n\t\t\t// generate new workers into persistent state and launch associated actors\n\t\t\tfor (int i = 0; i < numWorkers; i++) {\n\t\t\t\tMesosWorkerStore.Worker worker = MesosWorkerStore.Worker.newWorker(workerStore.newTaskID());\n\t\t\t\tworkerStore.putWorker(worker);\n\t\t\t\tworkersInNew.put(extractResourceID(worker.taskID()), worker);\n\n\t\t\t\tLaunchableMesosWorker launchable = createLaunchableMesosWorker(worker.taskID());\n\n\t\t\t\tLOG.info(\"Scheduling Mesos task {} with ({} MB, {} cpus).\",\n\t\t\t\t\tlaunchable.taskID().getValue(), launchable.taskRequest().getMemory(), launchable.taskRequest().getCPUs());\n\n\t\t\t\ttoMonitor.add(new TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker)));\n\t\t\t\ttoLaunch.add(launchable);\n\t\t\t}\n\n\t\t\t// tell the task router about the new plans\n\t\t\tfor (TaskMonitor.TaskGoalStateUpdated update : toMonitor) {\n\t\t\t\ttaskRouter.tell(update, self());\n\t\t\t}\n\n\t\t\t// tell the launch coordinator to launch the new tasks\n\t\t\tif(toLaunch.size() >= 1) {\n\t\t\t\tlaunchCoordinator.tell(new LaunchCoordinator.Launch(toLaunch), self());\n\t\t\t}\n\t\t}\n\t\tcatch(Exception ex) {\n\t\t\tfatalError(\"unable to request new workers\", ex);\n\t\t}\n\t}",
            " 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377 +\n 378  \n 379  \n 380 +\n 381  \n 382  \n 383  ",
            "\t/**\n\t * Plan for some additional workers to be launched.\n\t *\n\t * @param numWorkers The number of workers to allocate.\n\t */\n\t@Override\n\tprotected void requestNewWorkers(int numWorkers) {\n\n\t\ttry {\n\t\t\tList<TaskMonitor.TaskGoalStateUpdated> toMonitor = new ArrayList<>(numWorkers);\n\t\t\tList<LaunchableTask> toLaunch = new ArrayList<>(numWorkers);\n\n\t\t\t// generate new workers into persistent state and launch associated actors\n\t\t\tfor (int i = 0; i < numWorkers; i++) {\n\t\t\t\tMesosWorkerStore.Worker worker = MesosWorkerStore.Worker.newWorker(workerStore.newTaskID());\n\t\t\t\tworkerStore.putWorker(worker);\n\t\t\t\tworkersInNew.put(extractResourceID(worker.taskID()), worker);\n\n\t\t\t\tLaunchableMesosWorker launchable = createLaunchableMesosWorker(worker.taskID());\n\n\t\t\t\tLOG.info(\"Scheduling Mesos task {} with ({} MB, {} cpus).\",\n\t\t\t\t\tlaunchable.taskID().getValue(), launchable.taskRequest().getMemory(), launchable.taskRequest().getCPUs());\n\n\t\t\t\ttoMonitor.add(new TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker)));\n\t\t\t\ttoLaunch.add(launchable);\n\t\t\t}\n\n\t\t\t// tell the task router about the new plans\n\t\t\tfor (TaskMonitor.TaskGoalStateUpdated update : toMonitor) {\n\t\t\t\ttaskRouter.tell(update, self());\n\t\t\t}\n\n\t\t\t// tell the launch coordinator to launch the new tasks\n\t\t\tif (toLaunch.size() >= 1) {\n\t\t\t\tlaunchCoordinator.tell(new LaunchCoordinator.Launch(toLaunch), self());\n\t\t\t}\n\t\t} catch (Exception ex) {\n\t\t\tfatalError(\"unable to request new workers\", ex);\n\t\t}\n\t}"
        ],
        [
            "MesosFlinkResourceManager::createActorProps(Class,Configuration,MesosConfiguration,MesosWorkerStore,LeaderRetrievalService,MesosTaskManagerParameters,ContainerSpecification,MesosArtifactResolver,Logger)",
            " 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749 -\n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757 -\n 758 -\n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  ",
            "\t/**\n\t * Creates the props needed to instantiate this actor.\n\t *\n\t * Rather than extracting and validating parameters in the constructor, this factory method takes\n\t * care of that. That way, errors occur synchronously, and are not swallowed simply in a\n\t * failed asynchronous attempt to start the actor.\n\n\t * @param actorClass\n\t *             The actor class, to allow overriding this actor with subclasses for testing.\n\t * @param flinkConfig\n\t *             The Flink configuration object.\n\t * @param taskManagerParameters\n\t *             The parameters for launching TaskManager containers.\n\t * @param taskManagerContainerSpec\n\t *             The container specification.\n\t * @param artifactResolver\n\t *             The artifact resolver to locate artifacts\n\t * @param log\n\t *             The logger to log to.\n\t *\n\t * @return The Props object to instantiate the MesosFlinkResourceManager actor.\n\t */\n\tpublic static Props createActorProps(Class<? extends MesosFlinkResourceManager> actorClass,\n\t\t\tConfiguration flinkConfig,\n\t\t\tMesosConfiguration mesosConfig,\n\t\t\tMesosWorkerStore workerStore,\n\t\t\tLeaderRetrievalService leaderRetrievalService,\n\t\t\tMesosTaskManagerParameters taskManagerParameters,\n\t\t\tContainerSpecification taskManagerContainerSpec,\n\t\t\tMesosArtifactResolver artifactResolver,\n\t\t\tLogger log)\n\t{\n\n\t\tfinal int numInitialTaskManagers = flinkConfig.getInteger(\n\t\t\tConfigConstants.MESOS_INITIAL_TASKS, 0);\n\t\tif (numInitialTaskManagers >= 0) {\n\t\t\tlog.info(\"Mesos framework to allocate {} initial tasks\",\n\t\t\t\tnumInitialTaskManagers);\n\t\t}\n\t\telse {\n\t\t\tthrow new IllegalConfigurationException(\"Invalid value for \" +\n\t\t\t\tConfigConstants.MESOS_INITIAL_TASKS + \", which must be at least zero.\");\n\t\t}\n\n\t\tfinal int maxFailedTasks = flinkConfig.getInteger(\n\t\t\tConfigConstants.MESOS_MAX_FAILED_TASKS, numInitialTaskManagers);\n\t\tif (maxFailedTasks >= 0) {\n\t\t\tlog.info(\"Mesos framework tolerates {} failed tasks before giving up\",\n\t\t\t\tmaxFailedTasks);\n\t\t}\n\n\t\treturn Props.create(actorClass,\n\t\t\tflinkConfig,\n\t\t\tmesosConfig,\n\t\t\tworkerStore,\n\t\t\tleaderRetrievalService,\n\t\t\ttaskManagerParameters,\n\t\t\ttaskManagerContainerSpec,\n\t\t\tartifactResolver,\n\t\t\tmaxFailedTasks,\n\t\t\tnumInitialTaskManagers);\n\t}",
            " 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748 +\n 749 +\n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757 +\n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  ",
            "\t/**\n\t * Creates the props needed to instantiate this actor.\n\t *\n\t * <p>Rather than extracting and validating parameters in the constructor, this factory method takes\n\t * care of that. That way, errors occur synchronously, and are not swallowed simply in a\n\t * failed asynchronous attempt to start the actor.\n\n\t * @param actorClass\n\t *             The actor class, to allow overriding this actor with subclasses for testing.\n\t * @param flinkConfig\n\t *             The Flink configuration object.\n\t * @param taskManagerParameters\n\t *             The parameters for launching TaskManager containers.\n\t * @param taskManagerContainerSpec\n\t *             The container specification.\n\t * @param artifactResolver\n\t *             The artifact resolver to locate artifacts\n\t * @param log\n\t *             The logger to log to.\n\t *\n\t * @return The Props object to instantiate the MesosFlinkResourceManager actor.\n\t */\n\tpublic static Props createActorProps(\n\t\t\tClass<? extends MesosFlinkResourceManager> actorClass,\n\t\t\tConfiguration flinkConfig,\n\t\t\tMesosConfiguration mesosConfig,\n\t\t\tMesosWorkerStore workerStore,\n\t\t\tLeaderRetrievalService leaderRetrievalService,\n\t\t\tMesosTaskManagerParameters taskManagerParameters,\n\t\t\tContainerSpecification taskManagerContainerSpec,\n\t\t\tMesosArtifactResolver artifactResolver,\n\t\t\tLogger log) {\n\n\t\tfinal int numInitialTaskManagers = flinkConfig.getInteger(\n\t\t\tConfigConstants.MESOS_INITIAL_TASKS, 0);\n\t\tif (numInitialTaskManagers >= 0) {\n\t\t\tlog.info(\"Mesos framework to allocate {} initial tasks\",\n\t\t\t\tnumInitialTaskManagers);\n\t\t}\n\t\telse {\n\t\t\tthrow new IllegalConfigurationException(\"Invalid value for \" +\n\t\t\t\tConfigConstants.MESOS_INITIAL_TASKS + \", which must be at least zero.\");\n\t\t}\n\n\t\tfinal int maxFailedTasks = flinkConfig.getInteger(\n\t\t\tConfigConstants.MESOS_MAX_FAILED_TASKS, numInitialTaskManagers);\n\t\tif (maxFailedTasks >= 0) {\n\t\t\tlog.info(\"Mesos framework tolerates {} failed tasks before giving up\",\n\t\t\t\tmaxFailedTasks);\n\t\t}\n\n\t\treturn Props.create(actorClass,\n\t\t\tflinkConfig,\n\t\t\tmesosConfig,\n\t\t\tworkerStore,\n\t\t\tleaderRetrievalService,\n\t\t\ttaskManagerParameters,\n\t\t\ttaskManagerContainerSpec,\n\t\t\tartifactResolver,\n\t\t\tmaxFailedTasks,\n\t\t\tnumInitialTaskManagers);\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testError()",
            " 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755 -\n 756 -\n 757  \n 758  \n 759  \n 760  \n 761  \n 762  ",
            "\t/**\n\t * Test Mesos scheduler error.\n\t */\n\t@Test\n\tpublic void testError() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\twatch(resourceManager.actor());\n\t\t\t\t\t\tresourceManager.tell(new Error(\"test\"), resourceManager);\n\t\t\t\t\t\texpectTerminated(resourceManager.actor());\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774 +\n 775  \n 776  \n 777  \n 778  \n 779  \n 780  ",
            "\t/**\n\t * Test Mesos scheduler error.\n\t */\n\t@Test\n\tpublic void testError() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\twatch(resourceManager.actor());\n\t\t\t\t\t\tresourceManager.tell(new Error(\"test\"), resourceManager);\n\t\t\t\t\t\texpectTerminated(resourceManager.actor());\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testReleaseRegisteredWorker()",
            " 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412 -\n 413 -\n 414  \n 415  \n 416  \n 417  \n 418  \n 419  ",
            "\t/**\n\t * Test release of registered workers.\n\t */\n\t@Test\n\tpublic void testReleaseRegisteredWorker() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state, initialize the RM, then register with task1 as a registered worker\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1 = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(LaunchCoordinator.Assign.class);\n\t\t\t\t\t\tregister(singletonList(extractResourceID(task1)));\n\n\t\t\t\t\t\t// release the registered worker\n\t\t\t\t\t\tresourceManager.tell(new RemoveResource(extractResourceID(task1)));\n\n\t\t\t\t\t\t// verify that the worker was persisted, the internal state was updated, the task router was notified,\n\t\t\t\t\t\t// and the launch coordinator was notified about the host assignment change\n\t\t\t\t\t\tMesosWorkerStore.Worker worker2Released = worker1.releaseWorker();\n\t\t\t\t\t\tverify(workerStore).putWorker(worker2Released);\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersBeingReturned, hasEntry(extractResourceID(task1), worker2Released));\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsg(new LaunchCoordinator.Unassign(task1, slave1host));\n\n\t\t\t\t\t\t// send the subsequent terminated message\n\t\t\t\t\t\twhen(workerStore.removeWorker(task1)).thenReturn(true);\n\t\t\t\t\t\tresourceManager.tell(new TaskMonitor.TaskTerminated(task1, Protos.TaskStatus.newBuilder()\n\t\t\t\t\t\t\t.setTaskId(task1).setSlaveId(slave1).setState(Protos.TaskState.TASK_FINISHED).build()));\n\n\t\t\t\t\t\t// verify that the instance state was updated\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersBeingReturned.entrySet(), empty());\n\t\t\t\t\t\tverify(workerStore).removeWorker(task1);\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440 +\n 441  \n 442  \n 443  \n 444  \n 445  \n 446  ",
            "\t/**\n\t * Test release of registered workers.\n\t */\n\t@Test\n\tpublic void testReleaseRegisteredWorker() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state, initialize the RM, then register with task1 as a registered worker\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1 = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(LaunchCoordinator.Assign.class);\n\t\t\t\t\t\tregister(singletonList(extractResourceID(task1)));\n\n\t\t\t\t\t\t// release the registered worker\n\t\t\t\t\t\tresourceManager.tell(new RemoveResource(extractResourceID(task1)));\n\n\t\t\t\t\t\t// verify that the worker was persisted, the internal state was updated, the task router was notified,\n\t\t\t\t\t\t// and the launch coordinator was notified about the host assignment change\n\t\t\t\t\t\tMesosWorkerStore.Worker worker2Released = worker1.releaseWorker();\n\t\t\t\t\t\tverify(workerStore).putWorker(worker2Released);\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersBeingReturned, hasEntry(extractResourceID(task1), worker2Released));\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsg(new LaunchCoordinator.Unassign(task1, slave1host));\n\n\t\t\t\t\t\t// send the subsequent terminated message\n\t\t\t\t\t\twhen(workerStore.removeWorker(task1)).thenReturn(true);\n\t\t\t\t\t\tresourceManager.tell(new TaskMonitor.TaskTerminated(task1, Protos.TaskStatus.newBuilder()\n\t\t\t\t\t\t\t.setTaskId(task1).setSlaveId(slave1).setState(Protos.TaskState.TASK_FINISHED).build()));\n\n\t\t\t\t\t\t// verify that the instance state was updated\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersBeingReturned.entrySet(), empty());\n\t\t\t\t\t\tverify(workerStore).removeWorker(task1);\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosConfiguration::createDriver(Scheduler,boolean)",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "\t/**\n\t * Create the Mesos scheduler driver based on this configuration.\n\t * @param scheduler the scheduler to use.\n\t * @param implicitAcknowledgements whether to configure the driver for implicit acknowledgements.\n     * @return a scheduler driver.\n     */\n\tpublic SchedulerDriver createDriver(Scheduler scheduler, boolean implicitAcknowledgements) {\n\t\tMesosSchedulerDriver schedulerDriver;\n\t\tif(this.credential().isDefined()) {\n\t\t\tschedulerDriver =\n\t\t\t\tnew MesosSchedulerDriver(scheduler, frameworkInfo.build(), this.masterUrl(),\n\t\t\t\t\timplicitAcknowledgements, this.credential().get().build());\n\t\t}\n\t\telse {\n\t\t\tschedulerDriver =\n\t\t\t\tnew MesosSchedulerDriver(scheduler, frameworkInfo.build(), this.masterUrl(),\n\t\t\t\t\timplicitAcknowledgements);\n\t\t}\n\t\treturn schedulerDriver;\n\t}",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "\t/**\n\t * Create the Mesos scheduler driver based on this configuration.\n\t * @param scheduler the scheduler to use.\n\t * @param implicitAcknowledgements whether to configure the driver for implicit acknowledgements.\n\t * @return a scheduler driver.\n\t */\n\tpublic SchedulerDriver createDriver(Scheduler scheduler, boolean implicitAcknowledgements) {\n\t\tMesosSchedulerDriver schedulerDriver;\n\t\tif (this.credential().isDefined()) {\n\t\t\tschedulerDriver =\n\t\t\t\tnew MesosSchedulerDriver(scheduler, frameworkInfo.build(), this.masterUrl(),\n\t\t\t\t\timplicitAcknowledgements, this.credential().get().build());\n\t\t}\n\t\telse {\n\t\t\tschedulerDriver =\n\t\t\t\tnew MesosSchedulerDriver(scheduler, frameworkInfo.build(), this.masterUrl(),\n\t\t\t\t\timplicitAcknowledgements);\n\t\t}\n\t\treturn schedulerDriver;\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testPendingWorkerFailed()",
            " 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570 -\n 571 -\n 572  \n 573  \n 574  \n 575  \n 576  \n 577  ",
            "\t/**\n\t * Test unplanned task failure of a pending worker.\n\t */\n\t@Test\n\tpublic void testPendingWorkerFailed() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state with a launched worker that hasn't yet registered\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1launched));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\t// tell the RM that a task failed (and prepare a replacement task)\n\t\t\t\t\t\twhen(workerStore.newTaskID()).thenReturn(task2);\n\t\t\t\t\t\twhen(workerStore.removeWorker(task1)).thenReturn(true);\n\t\t\t\t\t\tresourceManager.tell(new SetWorkerPoolSize(1), jobManager);\n\t\t\t\t\t\tresourceManager.tell(new TaskMonitor.TaskTerminated(task1, Protos.TaskStatus.newBuilder()\n\t\t\t\t\t\t\t.setTaskId(task1).setSlaveId(slave1).setState(Protos.TaskState.TASK_FAILED).build()));\n\n\t\t\t\t\t\t// verify that the instance state was updated\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch.entrySet(), empty());\n\t\t\t\t\t\tverify(workerStore).newTaskID();\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595 +\n 596  \n 597  \n 598  \n 599  \n 600  \n 601  ",
            "\t/**\n\t * Test unplanned task failure of a pending worker.\n\t */\n\t@Test\n\tpublic void testPendingWorkerFailed() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// set the initial persistent state with a launched worker that hasn't yet registered\n\t\t\t\t\t\tMesosWorkerStore.Worker worker1launched = MesosWorkerStore.Worker.newWorker(task1).launchWorker(slave1, slave1host);\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\twhen(workerStore.recoverWorkers()).thenReturn(singletonList(worker1launched));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\t// tell the RM that a task failed (and prepare a replacement task)\n\t\t\t\t\t\twhen(workerStore.newTaskID()).thenReturn(task2);\n\t\t\t\t\t\twhen(workerStore.removeWorker(task1)).thenReturn(true);\n\t\t\t\t\t\tresourceManager.tell(new SetWorkerPoolSize(1), jobManager);\n\t\t\t\t\t\tresourceManager.tell(new TaskMonitor.TaskTerminated(task1, Protos.TaskStatus.newBuilder()\n\t\t\t\t\t\t\t.setTaskId(task1).setSlaveId(slave1).setState(Protos.TaskState.TASK_FAILED).build()));\n\n\t\t\t\t\t\t// verify that the instance state was updated\n\t\t\t\t\t\tassertThat(resourceManagerInstance.workersInLaunch.entrySet(), empty());\n\t\t\t\t\t\tverify(workerStore).newTaskID();\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::testReRegistered()",
            " 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700 -\n 701 -\n 702  \n 703  \n 704  \n 705  \n 706  \n 707  ",
            "\t/**\n\t * Test Mesos re-registration handling.\n\t */\n\t@Test\n\tpublic void testReRegistered() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\tProtos.MasterInfo masterInfo = Protos.MasterInfo.newBuilder()\n\t\t\t\t\t\t\t.setId(\"master1\").setIp(0).setPort(5050).build();\n\t\t\t\t\t\tresourceManager.tell(new ReRegistered(masterInfo), resourceManager);\n\n\t\t\t\t\t\tresourceManagerInstance.connectionMonitor.expectMsgClass(ReRegistered.class);\n\t\t\t\t\t\tresourceManagerInstance.reconciliationCoordinator.expectMsgClass(ReRegistered.class);\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(ReRegistered.class);\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(ReRegistered.class);\n\t\t\t\t\t}\n\t\t\t\t\tcatch(Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}",
            " 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721 +\n 722  \n 723  \n 724  \n 725  \n 726  \n 727  ",
            "\t/**\n\t * Test Mesos re-registration handling.\n\t */\n\t@Test\n\tpublic void testReRegistered() {\n\t\tnew Context() {{\n\t\t\tnew Within(duration(\"10 seconds\")) {\n\t\t\t\t@Override\n\t\t\t\tprotected void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\twhen(workerStore.getFrameworkID()).thenReturn(Option.apply(framework1));\n\t\t\t\t\t\tinitialize();\n\t\t\t\t\t\tregister(Collections.<ResourceID>emptyList());\n\n\t\t\t\t\t\tProtos.MasterInfo masterInfo = Protos.MasterInfo.newBuilder()\n\t\t\t\t\t\t\t.setId(\"master1\").setIp(0).setPort(5050).build();\n\t\t\t\t\t\tresourceManager.tell(new ReRegistered(masterInfo), resourceManager);\n\n\t\t\t\t\t\tresourceManagerInstance.connectionMonitor.expectMsgClass(ReRegistered.class);\n\t\t\t\t\t\tresourceManagerInstance.reconciliationCoordinator.expectMsgClass(ReRegistered.class);\n\t\t\t\t\t\tresourceManagerInstance.launchCoordinator.expectMsgClass(ReRegistered.class);\n\t\t\t\t\t\tresourceManagerInstance.taskRouter.expectMsgClass(ReRegistered.class);\n\t\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\t\tthrow new RuntimeException(ex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t}};\n\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::Context::initialize()",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 -\n 218 -\n 219 -\n 220 -\n 221 -\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  ",
            "\t\t/**\n\t\t * Initialize the resource manager.\n\t\t */\n\t\tpublic void initialize() {\n\t\t\tContainerSpecification containerSpecification = new ContainerSpecification();\n\t\t\tContaineredTaskManagerParameters containeredParams =\n\t\t\t\tnew ContaineredTaskManagerParameters(1024, 768, 256, 4, new HashMap<String, String>());\n\t\t\tMesosTaskManagerParameters tmParams = new MesosTaskManagerParameters(\n\t\t\t\t1.0, \n\t\t\t\tMesosTaskManagerParameters.ContainerType.MESOS, \n\t\t\t\tOption.<String>empty(), \n\t\t\t\tcontaineredParams, \n\t\t\t\tCollections.<Protos.Volume>emptyList(), \n\t\t\t\tCollections.<ConstraintEvaluator>emptyList(),\n\t\t\t\tOption.<String>empty(),\n\t\t\t\tOption.<String>empty());\n\n\t\t\tTestActorRef<TestingMesosFlinkResourceManager> resourceManagerRef =\n\t\t\t\tTestActorRef.create(system, MesosFlinkResourceManager.createActorProps(\n\t\t\t\t\tTestingMesosFlinkResourceManager.class,\n\t\t\t\t\tconfig,\n\t\t\t\t\tmesosConfig,\n\t\t\t\t\tworkerStore,\n\t\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\t\ttmParams,\n\t\t\t\t\tcontainerSpecification,\n\t\t\t\t\tartifactResolver,\n\t\t\t\t\tLOG));\n\t\t\tresourceManagerInstance = resourceManagerRef.underlyingActor();\n\t\t\tresourceManager = new AkkaActorGateway(resourceManagerRef, HighAvailabilityServices.DEFAULT_LEADER_ID);\n\n\t\t\tverify(schedulerDriver).start();\n\t\t\tresourceManagerInstance.connectionMonitor.expectMsgClass(ConnectionMonitor.Start.class);\n\t\t}",
            " 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248 +\n 249 +\n 250 +\n 251 +\n 252 +\n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "\t\t/**\n\t\t * Initialize the resource manager.\n\t\t */\n\t\tpublic void initialize() {\n\t\t\tContainerSpecification containerSpecification = new ContainerSpecification();\n\t\t\tContaineredTaskManagerParameters containeredParams =\n\t\t\t\tnew ContaineredTaskManagerParameters(1024, 768, 256, 4, new HashMap<String, String>());\n\t\t\tMesosTaskManagerParameters tmParams = new MesosTaskManagerParameters(\n\t\t\t\t1.0,\n\t\t\t\tMesosTaskManagerParameters.ContainerType.MESOS,\n\t\t\t\tOption.<String>empty(),\n\t\t\t\tcontaineredParams,\n\t\t\t\tCollections.<Protos.Volume>emptyList(),\n\t\t\t\tCollections.<ConstraintEvaluator>emptyList(),\n\t\t\t\tOption.<String>empty(),\n\t\t\t\tOption.<String>empty());\n\n\t\t\tTestActorRef<TestingMesosFlinkResourceManager> resourceManagerRef =\n\t\t\t\tTestActorRef.create(system, MesosFlinkResourceManager.createActorProps(\n\t\t\t\t\tTestingMesosFlinkResourceManager.class,\n\t\t\t\t\tconfig,\n\t\t\t\t\tmesosConfig,\n\t\t\t\t\tworkerStore,\n\t\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\t\ttmParams,\n\t\t\t\t\tcontainerSpecification,\n\t\t\t\t\tartifactResolver,\n\t\t\t\t\tLOG));\n\t\t\tresourceManagerInstance = resourceManagerRef.underlyingActor();\n\t\t\tresourceManager = new AkkaActorGateway(resourceManagerRef, HighAvailabilityServices.DEFAULT_LEADER_ID);\n\n\t\t\tverify(schedulerDriver).start();\n\t\t\tresourceManagerInstance.connectionMonitor.expectMsgClass(ConnectionMonitor.Start.class);\n\t\t}"
        ],
        [
            "MesosFlinkResourceManagerTest::TestingMesosFlinkResourceManager::createLaunchCoordinator()",
            " 131  \n 132 -",
            "\t\t@Override\n\t\tprotected ActorRef createLaunchCoordinator() { return launchCoordinator.ref(); }",
            " 157  \n 158 +\n 159 +\n 160 +",
            "\t\t@Override\n\t\tprotected ActorRef createLaunchCoordinator() {\n\t\t\treturn launchCoordinator.ref();\n\t\t}"
        ]
    ],
    "c793ea41d88fe84fa97d825728ad95f35e27ef82": [
        [
            "OptimizerPlanEnvironment::getOptimizedPlan(PackagedProgram)",
            "  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102  \n 103  \n 104 -\n 105  \n 106  ",
            "\tpublic FlinkPlan getOptimizedPlan(PackagedProgram prog) throws ProgramInvocationException {\n\t\t\n\t\t// temporarily write syserr and sysout to a byte array.\n\t\tPrintStream originalOut = System.out;\n\t\tPrintStream originalErr = System.err;\n\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n\t\tSystem.setOut(new PrintStream(baos));\n\t\tByteArrayOutputStream baes = new ByteArrayOutputStream();\n\t\tSystem.setErr(new PrintStream(baes));\n\n\t\tsetAsContext();\n\t\ttry {\n\t\t\tprog.invokeInteractiveModeForExecution();\n\t\t}\n\t\tcatch (ProgramInvocationException e) {\n\t\t\tthrow e;\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// the invocation gets aborted with the preview plan\n\t\t\tif (optimizerPlan != null) {\n\t\t\t\treturn optimizerPlan;\n\t\t\t} else {\n\t\t\t\tthrow new ProgramInvocationException(\"The program caused an error: \", t);\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tunsetAsContext();\n\t\t\tSystem.setOut(originalOut);\n\t\t\tSystem.setErr(originalErr);\n\t\t}\n\n\t\tString stdout = baos.toString();\n\t\tString stderr = baes.toString();\n\t\t\n\t\tthrow new ProgramInvocationException(\n\t\t\t\t\"The program plan could not be fetched - the program aborted pre-maturely.\"\n\t\t\t\t\t\t+ \"\\n\\nSystem.err: \" + (stdout.length() == 0 ? \"(none)\" : stdout) \n\t\t\t\t\t\t+ \"\\n\\nSystem.out: \" + (stderr.length() == 0 ? \"(none)\" : stderr));\n\t}",
            "  71  \n  72 +\n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105  \n 106  \n 107 +\n 108  \n 109  ",
            "\tpublic FlinkPlan getOptimizedPlan(PackagedProgram prog) throws ProgramInvocationException {\n\n\t\t// temporarily write syserr and sysout to a byte array.\n\t\tPrintStream originalOut = System.out;\n\t\tPrintStream originalErr = System.err;\n\t\tByteArrayOutputStream baos = new ByteArrayOutputStream();\n\t\tSystem.setOut(new PrintStream(baos));\n\t\tByteArrayOutputStream baes = new ByteArrayOutputStream();\n\t\tSystem.setErr(new PrintStream(baes));\n\n\t\tsetAsContext();\n\t\ttry {\n\t\t\tprog.invokeInteractiveModeForExecution();\n\t\t}\n\t\tcatch (ProgramInvocationException e) {\n\t\t\tthrow e;\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// the invocation gets aborted with the preview plan\n\t\t\tif (optimizerPlan != null) {\n\t\t\t\treturn optimizerPlan;\n\t\t\t} else {\n\t\t\t\tthrow new ProgramInvocationException(\"The program caused an error: \", t);\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\tunsetAsContext();\n\t\t\tSystem.setOut(originalOut);\n\t\t\tSystem.setErr(originalErr);\n\t\t}\n\n\t\tString stdout = baos.toString();\n\t\tString stderr = baes.toString();\n\n\t\tthrow new ProgramInvocationException(\n\t\t\t\t\"The program plan could not be fetched - the program aborted pre-maturely.\"\n\t\t\t\t\t\t+ \"\\n\\nSystem.err: \" + (stdout.length() == 0 ? \"(none)\" : stdout)\n\t\t\t\t\t\t+ \"\\n\\nSystem.out: \" + (stderr.length() == 0 ? \"(none)\" : stderr));\n\t}"
        ],
        [
            "PackagedProgram::getPlan()",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486  \n 487  ",
            "\t/**\n\t * Returns the plan as generated from the Pact Assembler.\n\t * \n\t * @return The program's plan.\n\t * @throws ProgramInvocationException Thrown, if an error occurred in the program while\n\t *         creating the program's {@link Plan}.\n\t */\n\tprivate Plan getPlan() throws ProgramInvocationException {\n\t\tif (this.plan == null) {\n\t\t\tThread.currentThread().setContextClassLoader(this.userCodeClassLoader);\n\t\t\tthis.plan = createPlanFromProgram(this.program, this.args);\n\t\t}\n\t\t\n\t\treturn this.plan;\n\t}",
            " 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482 +\n 483  \n 484  ",
            "\t/**\n\t * Returns the plan as generated from the Pact Assembler.\n\t *\n\t * @return The program's plan.\n\t * @throws ProgramInvocationException Thrown, if an error occurred in the program while\n\t *         creating the program's {@link Plan}.\n\t */\n\tprivate Plan getPlan() throws ProgramInvocationException {\n\t\tif (this.plan == null) {\n\t\t\tThread.currentThread().setContextClassLoader(this.userCodeClassLoader);\n\t\t\tthis.plan = createPlanFromProgram(this.program, this.args);\n\t\t}\n\n\t\treturn this.plan;\n\t}"
        ],
        [
            "ClientTest::TestGetAccumulator::main(String)",
            " 424 -\n 425  \n 426  \n 427  \n 428  ",
            "\t\tpublic static void main(String args[]) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute().getAccumulatorResult(ACCUMULATOR_NAME);\n\t\t}",
            " 443 +\n 444  \n 445  \n 446  \n 447  ",
            "\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute().getAccumulatorResult(ACCUMULATOR_NAME);\n\t\t}"
        ],
        [
            "ExecutionPlanAfterExecutionTest::testCreatePlanAfterGetExecutionPlan()",
            "  60  \n  61  \n  62  \n  63 -\n  64  \n  65  \n  66  \n  67 -\n  68 -\n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77  \n  78  ",
            "\t@Test\n\tpublic void testCreatePlanAfterGetExecutionPlan() {\n\t\tExecutionEnvironment env = new LocalEnvironment();\n\t\t\n\t\tDataSet<Integer> baseSet = env.fromElements(1, 2);\n\n\t\tDataSet<Integer> result = baseSet.map(new MapFunction<Integer, Integer>() {\n\t\t\t@Override public Integer map(Integer value) throws Exception { return value * 2; }\n\t\t});\n\t\tresult.output(new DiscardingOutputFormat<Integer>());\n\n\t\ttry {\n\t\t\tenv.getExecutionPlan();\n\t\t\tenv.createProgramPlan();\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Cannot run both #getExecutionPlan and #execute. Message: \"+e.getMessage());\n\t\t}\n\t}",
            "  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  \n  73 +\n  74 +\n  75 +\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  ",
            "\t@Test\n\tpublic void testCreatePlanAfterGetExecutionPlan() {\n\t\tExecutionEnvironment env = new LocalEnvironment();\n\n\t\tDataSet<Integer> baseSet = env.fromElements(1, 2);\n\n\t\tDataSet<Integer> result = baseSet.map(new MapFunction<Integer, Integer>() {\n\t\t\t@Override public Integer map(Integer value) throws Exception {\n\t\t\t\treturn value * 2;\n\t\t\t}});\n\t\tresult.output(new DiscardingOutputFormat<Integer>());\n\n\t\ttry {\n\t\t\tenv.getExecutionPlan();\n\t\t\tenv.createProgramPlan();\n\t\t} catch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Cannot run both #getExecutionPlan and #execute. Message: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "CliFrontendListCancelTest::testCancel()",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75 -\n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 -\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "\t@Test\n\tpublic void testCancel() {\n\t\ttry {\n\t\t\t// test unrecognized option\n\t\t\t{\n\t\t\t\tString[] parameters = {\"-v\", \"-l\"};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\t\t\t\n\t\t\t// test missing job id\n\t\t\t{\n\t\t\t\tString[] parameters = {};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\t\t\t\n\t\t\t// test cancel properly\n\t\t\t{\n\t\t\t\tJobID jid = new JobID();\n\t\t\t\tString jidString = jid.toString();\n\n\t\t\t\tfinal UUID leaderSessionID = UUID.randomUUID();\n\n\t\t\t\tfinal ActorRef jm = actorSystem.actorOf(Props.create(\n\t\t\t\t\t\t\t\tCliJobManager.class,\n\t\t\t\t\t\t\t\tjid,\n\t\t\t\t\t\t\t\tleaderSessionID\n\t\t\t\t\t\t)\n\t\t\t\t);\n\n\t\t\t\tfinal ActorGateway gateway = new AkkaActorGateway(jm, leaderSessionID);\n\t\t\t\t\n\t\t\t\tString[] parameters = { jidString };\n\t\t\t\tInfoListTestCliFrontend testFrontend = new InfoListTestCliFrontend(gateway);\n\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode == 0);\n\t\t\t}\n\n\t\t\t// test cancel properly\n\t\t\t{\n\t\t\t\tJobID jid1 = new JobID();\n\t\t\t\tJobID jid2 = new JobID();\n\n\t\t\t\tfinal UUID leaderSessionID = UUID.randomUUID();\n\n\t\t\t\tfinal ActorRef jm = actorSystem.actorOf(\n\t\t\t\t\t\tProps.create(\n\t\t\t\t\t\t\t\tCliJobManager.class,\n\t\t\t\t\t\t\t\tjid1,\n\t\t\t\t\t\t\t\tleaderSessionID\n\t\t\t\t\t\t)\n\t\t\t\t);\n\n\t\t\t\tfinal ActorGateway gateway = new AkkaActorGateway(jm, leaderSessionID);\n\n\t\t\t\tString[] parameters = { jid2.toString() };\n\t\t\t\tInfoListTestCliFrontend testFrontend = new InfoListTestCliFrontend(gateway);\n\n\t\t\t\tassertTrue(testFrontend.cancel(parameters) != 0);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Program caused an exception: \" + e.getMessage());\n\t\t}\n\t}",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 +\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  ",
            "\t@Test\n\tpublic void testCancel() {\n\t\ttry {\n\t\t\t// test unrecognized option\n\t\t\t{\n\t\t\t\tString[] parameters = {\"-v\", \"-l\"};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\n\t\t\t// test missing job id\n\t\t\t{\n\t\t\t\tString[] parameters = {};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\n\t\t\t// test cancel properly\n\t\t\t{\n\t\t\t\tJobID jid = new JobID();\n\t\t\t\tString jidString = jid.toString();\n\n\t\t\t\tfinal UUID leaderSessionID = UUID.randomUUID();\n\n\t\t\t\tfinal ActorRef jm = actorSystem.actorOf(Props.create(\n\t\t\t\t\t\t\t\tCliJobManager.class,\n\t\t\t\t\t\t\t\tjid,\n\t\t\t\t\t\t\t\tleaderSessionID\n\t\t\t\t\t\t)\n\t\t\t\t);\n\n\t\t\t\tfinal ActorGateway gateway = new AkkaActorGateway(jm, leaderSessionID);\n\n\t\t\t\tString[] parameters = { jidString };\n\t\t\t\tInfoListTestCliFrontend testFrontend = new InfoListTestCliFrontend(gateway);\n\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode == 0);\n\t\t\t}\n\n\t\t\t// test cancel properly\n\t\t\t{\n\t\t\t\tJobID jid1 = new JobID();\n\t\t\t\tJobID jid2 = new JobID();\n\n\t\t\t\tfinal UUID leaderSessionID = UUID.randomUUID();\n\n\t\t\t\tfinal ActorRef jm = actorSystem.actorOf(\n\t\t\t\t\t\tProps.create(\n\t\t\t\t\t\t\t\tCliJobManager.class,\n\t\t\t\t\t\t\t\tjid1,\n\t\t\t\t\t\t\t\tleaderSessionID\n\t\t\t\t\t\t)\n\t\t\t\t);\n\n\t\t\t\tfinal ActorGateway gateway = new AkkaActorGateway(jm, leaderSessionID);\n\n\t\t\t\tString[] parameters = { jid2.toString() };\n\t\t\t\tInfoListTestCliFrontend testFrontend = new InfoListTestCliFrontend(gateway);\n\n\t\t\t\tassertTrue(testFrontend.cancel(parameters) != 0);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Program caused an exception: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "ClusterClient::logAndSysout(String)",
            " 794  \n 795  \n 796  \n 797  \n 798  \n 799 -\n 800  \n 801  \n 802  \n 803  ",
            "\t/**\n\t * Logs and prints to sysout if printing to stdout is enabled.\n\t * @param message The message to log/print\n\t */\n\tprotected void logAndSysout(String message) {\n\t\tLOG.info(message);\n\t\tif (printStatusDuringExecution) {\n\t\t\tSystem.out.println(message);\n\t\t}\n\t}",
            " 794  \n 795  \n 796  \n 797  \n 798  \n 799 +\n 800  \n 801  \n 802  \n 803  ",
            "\t/**\n\t * Logs and prints to sysout if printing to stdout is enabled.\n\t * @param message The message to log/print\n\t */\n\tprotected void logAndSysout(String message) {\n\t\tlog.info(message);\n\t\tif (printStatusDuringExecution) {\n\t\t\tSystem.out.println(message);\n\t\t}\n\t}"
        ],
        [
            "ExecutionPlanCreationTest::TestOptimizerPlan::main(String)",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86 -\n  87  \n  88  \n  89 -\n  90  \n  91 -\n  92  \n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  ",
            "\t\t@SuppressWarnings(\"serial\")\n\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tif (args.length < 2) {\n\t\t\t\tSystem.err.println(\"Usage: TestOptimizerPlan <input-file-path> <output-file-path>\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t\t\n\t\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\t\n\t\t\tDataSet<Tuple2<Long, Long>> input = env.readCsvFile(args[0])\n\t\t\t\t\t.fieldDelimiter(\"\\t\").types(Long.class, Long.class);\n\t\t\t\n\t\t\tDataSet<Tuple2<Long, Long>> result = input.map(\n\t\t\t\t\tnew MapFunction<Tuple2<Long,Long>, Tuple2<Long,Long>>() {\n\t\t\t\t\t\tpublic Tuple2<Long, Long> map(Tuple2<Long, Long> value){\n\t\t\t\t\t\t\treturn new Tuple2<Long, Long>(value.f0, value.f1+1);\n\t\t\t\t\t\t}\n\t\t\t});\n\t\t\tresult.writeAsCsv(args[1], \"\\n\", \"\\t\");\n\t\t\tenv.execute();\n\t\t}",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95 +\n  96  \n  97  \n  98 +\n  99  \n 100 +\n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  ",
            "\t\t@SuppressWarnings(\"serial\")\n\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tif (args.length < 2) {\n\t\t\t\tSystem.err.println(\"Usage: TestOptimizerPlan <input-file-path> <output-file-path>\");\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t\tDataSet<Tuple2<Long, Long>> input = env.readCsvFile(args[0])\n\t\t\t\t\t.fieldDelimiter(\"\\t\").types(Long.class, Long.class);\n\n\t\t\tDataSet<Tuple2<Long, Long>> result = input.map(\n\t\t\t\t\tnew MapFunction<Tuple2<Long, Long>, Tuple2<Long, Long>>() {\n\t\t\t\t\t\tpublic Tuple2<Long, Long> map(Tuple2<Long, Long> value){\n\t\t\t\t\t\t\treturn new Tuple2<Long, Long>(value.f0, value.f1 + 1);\n\t\t\t\t\t\t}\n\t\t\t});\n\t\t\tresult.writeAsCsv(args[1], \"\\n\", \"\\t\");\n\t\t\tenv.execute();\n\t\t}"
        ],
        [
            "CliFrontend::buildProgram(ProgramOptions)",
            " 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880 -\n 881 -\n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  ",
            "\t/**\n\t * Creates a Packaged program from the given command line options.\n\t *\n\t * @return A PackagedProgram (upon success)\n\t * @throws java.io.FileNotFoundException\n\t * @throws org.apache.flink.client.program.ProgramInvocationException\n\t */\n\tprotected PackagedProgram buildProgram(ProgramOptions options)\n\t\t\tthrows FileNotFoundException, ProgramInvocationException\n\t{\n\t\tString[] programArgs = options.getProgramArgs();\n\t\tString jarFilePath = options.getJarFilePath();\n\t\tList<URL> classpaths = options.getClasspaths();\n\n\t\tif (jarFilePath == null) {\n\t\t\tthrow new IllegalArgumentException(\"The program JAR file was not specified.\");\n\t\t}\n\n\t\tFile jarFile = new File(jarFilePath);\n\n\t\t// Check if JAR file exists\n\t\tif (!jarFile.exists()) {\n\t\t\tthrow new FileNotFoundException(\"JAR file does not exist: \" + jarFile);\n\t\t}\n\t\telse if (!jarFile.isFile()) {\n\t\t\tthrow new FileNotFoundException(\"JAR file is not a file: \" + jarFile);\n\t\t}\n\n\t\t// Get assembler class\n\t\tString entryPointClass = options.getEntryPointClassName();\n\n\t\tPackagedProgram program = entryPointClass == null ?\n\t\t\t\tnew PackagedProgram(jarFile, classpaths, programArgs) :\n\t\t\t\tnew PackagedProgram(jarFile, classpaths, entryPointClass, programArgs);\n\n\t\tprogram.setSavepointRestoreSettings(options.getSavepointRestoreSettings());\n\n\t\treturn program;\n\t}",
            " 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878 +\n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  ",
            "\t/**\n\t * Creates a Packaged program from the given command line options.\n\t *\n\t * @return A PackagedProgram (upon success)\n\t * @throws java.io.FileNotFoundException\n\t * @throws org.apache.flink.client.program.ProgramInvocationException\n\t */\n\tprotected PackagedProgram buildProgram(ProgramOptions options)\n\t\t\tthrows FileNotFoundException, ProgramInvocationException {\n\t\tString[] programArgs = options.getProgramArgs();\n\t\tString jarFilePath = options.getJarFilePath();\n\t\tList<URL> classpaths = options.getClasspaths();\n\n\t\tif (jarFilePath == null) {\n\t\t\tthrow new IllegalArgumentException(\"The program JAR file was not specified.\");\n\t\t}\n\n\t\tFile jarFile = new File(jarFilePath);\n\n\t\t// Check if JAR file exists\n\t\tif (!jarFile.exists()) {\n\t\t\tthrow new FileNotFoundException(\"JAR file does not exist: \" + jarFile);\n\t\t}\n\t\telse if (!jarFile.isFile()) {\n\t\t\tthrow new FileNotFoundException(\"JAR file is not a file: \" + jarFile);\n\t\t}\n\n\t\t// Get assembler class\n\t\tString entryPointClass = options.getEntryPointClassName();\n\n\t\tPackagedProgram program = entryPointClass == null ?\n\t\t\t\tnew PackagedProgram(jarFile, classpaths, programArgs) :\n\t\t\t\tnew PackagedProgram(jarFile, classpaths, entryPointClass, programArgs);\n\n\t\tprogram.setSavepointRestoreSettings(options.getSavepointRestoreSettings());\n\n\t\treturn program;\n\t}"
        ],
        [
            "ClusterClient::endSessions(List)",
            " 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716 -\n 717  \n 718  \n 719 -\n 720  \n 721  \n 722  \n 723  ",
            "\t/**\n\t * Tells the JobManager to finish the sessions (jobs) defined by the given IDs.\n\t *\n\t * @param jobIds The IDs that identify the sessions.\n\t */\n\tpublic void endSessions(List<JobID> jobIds) throws Exception {\n\t\tif (jobIds == null) {\n\t\t\tthrow new IllegalArgumentException(\"The JobIDs must not be null\");\n\t\t}\n\n\t\tActorGateway jobManagerGateway = getJobManagerGateway();\n\t\t\n\t\tfor (JobID jid : jobIds) {\n\t\t\tif (jid != null) {\n\t\t\t\tLOG.info(\"Telling job manager to end the session {}.\", jid);\n\t\t\t\tjobManagerGateway.tell(new JobManagerMessages.RemoveCachedJob(jid));\n\t\t\t}\n\t\t}\n\t}",
            " 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716 +\n 717  \n 718  \n 719 +\n 720  \n 721  \n 722  \n 723  ",
            "\t/**\n\t * Tells the JobManager to finish the sessions (jobs) defined by the given IDs.\n\t *\n\t * @param jobIds The IDs that identify the sessions.\n\t */\n\tpublic void endSessions(List<JobID> jobIds) throws Exception {\n\t\tif (jobIds == null) {\n\t\t\tthrow new IllegalArgumentException(\"The JobIDs must not be null\");\n\t\t}\n\n\t\tActorGateway jobManagerGateway = getJobManagerGateway();\n\n\t\tfor (JobID jid : jobIds) {\n\t\t\tif (jid != null) {\n\t\t\t\tlog.info(\"Telling job manager to end the session {}.\", jid);\n\t\t\t\tjobManagerGateway.tell(new JobManagerMessages.RemoveCachedJob(jid));\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "PackagedProgram::hasMainMethod(Class)",
            " 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497 -\n 498  \n 499  \n 500 -\n 501  \n 502  ",
            "\tprivate static boolean hasMainMethod(Class<?> entryClass) {\n\t\tMethod mainMethod;\n\t\ttry {\n\t\t\tmainMethod = entryClass.getMethod(\"main\", String[].class);\n\t\t} catch (NoSuchMethodException e) {\n\t\t\treturn false;\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new RuntimeException(\"Could not look up the main(String[]) method from the class \" + \n\t\t\t\t\tentryClass.getName() + \": \" + t.getMessage(), t);\n\t\t}\n\t\t\n\t\treturn Modifier.isStatic(mainMethod.getModifiers()) && Modifier.isPublic(mainMethod.getModifiers());\n\t}",
            " 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494 +\n 495  \n 496  \n 497 +\n 498  \n 499  ",
            "\tprivate static boolean hasMainMethod(Class<?> entryClass) {\n\t\tMethod mainMethod;\n\t\ttry {\n\t\t\tmainMethod = entryClass.getMethod(\"main\", String[].class);\n\t\t} catch (NoSuchMethodException e) {\n\t\t\treturn false;\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new RuntimeException(\"Could not look up the main(String[]) method from the class \" +\n\t\t\t\t\tentryClass.getName() + \": \" + t.getMessage(), t);\n\t\t}\n\n\t\treturn Modifier.isStatic(mainMethod.getModifiers()) && Modifier.isPublic(mainMethod.getModifiers());\n\t}"
        ],
        [
            "ContextEnvironmentFactory::createExecutionEnvironment()",
            "  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 -\n  72 -\n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\t@Override\n\tpublic ExecutionEnvironment createExecutionEnvironment() {\n\t\tif (isDetached && lastEnvCreated != null) {\n\t\t\tthrow new InvalidProgramException(\"Multiple enviornments cannot be created in detached mode\");\n\t\t}\n\n\t\tlastEnvCreated = isDetached ?\n\t\t\t\tnew DetachedEnvironment(client, jarFilesToAttach, classpathsToAttach, userCodeClassLoader, savepointSettings):\n\t\t\t\tnew ContextEnvironment(client, jarFilesToAttach, classpathsToAttach, userCodeClassLoader, savepointSettings);\n\t\tif (defaultParallelism > 0) {\n\t\t\tlastEnvCreated.setParallelism(defaultParallelism);\n\t\t}\n\t\treturn lastEnvCreated;\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 +\n  71 +\n  72 +\n  73  \n  74  \n  75  \n  76  \n  77  ",
            "\t@Override\n\tpublic ExecutionEnvironment createExecutionEnvironment() {\n\t\tif (isDetached && lastEnvCreated != null) {\n\t\t\tthrow new InvalidProgramException(\"Multiple enviornments cannot be created in detached mode\");\n\t\t}\n\n\t\tlastEnvCreated = isDetached\n\t\t\t? new DetachedEnvironment(client, jarFilesToAttach, classpathsToAttach, userCodeClassLoader, savepointSettings)\n\t\t\t: new ContextEnvironment(client, jarFilesToAttach, classpathsToAttach, userCodeClassLoader, savepointSettings);\n\t\tif (defaultParallelism > 0) {\n\t\t\tlastEnvCreated.setParallelism(defaultParallelism);\n\t\t}\n\t\treturn lastEnvCreated;\n\t}"
        ],
        [
            "ClusterClient::LazyActorSystemLoader::LazyActorSystemLoader(HighAvailabilityServices,Time,Configuration,Logger)",
            " 177  \n 178  \n 179  \n 180  \n 181 -\n 182  \n 183  \n 184  \n 185 -\n 186  ",
            "\t\tprivate LazyActorSystemLoader(\n\t\t\t\tHighAvailabilityServices highAvailabilityServices,\n\t\t\t\tTime timeout,\n\t\t\t\tConfiguration configuration,\n\t\t\t\tLogger LOG) {\n\t\t\tthis.highAvailabilityServices = Preconditions.checkNotNull(highAvailabilityServices);\n\t\t\tthis.timeout = Preconditions.checkNotNull(timeout);\n\t\t\tthis.configuration = Preconditions.checkNotNull(configuration);\n\t\t\tthis.LOG = Preconditions.checkNotNull(LOG);\n\t\t}",
            " 182  \n 183  \n 184  \n 185  \n 186 +\n 187  \n 188  \n 189  \n 190 +\n 191  ",
            "\t\tprivate LazyActorSystemLoader(\n\t\t\t\tHighAvailabilityServices highAvailabilityServices,\n\t\t\t\tTime timeout,\n\t\t\t\tConfiguration configuration,\n\t\t\t\tLogger log) {\n\t\t\tthis.highAvailabilityServices = Preconditions.checkNotNull(highAvailabilityServices);\n\t\t\tthis.timeout = Preconditions.checkNotNull(timeout);\n\t\t\tthis.configuration = Preconditions.checkNotNull(configuration);\n\t\t\tthis.log = Preconditions.checkNotNull(log);\n\t\t}"
        ],
        [
            "CliFrontendInfoTest::testErrorCases()",
            "  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44 -\n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  ",
            "\t@Test\n\tpublic void testErrorCases() {\n\t\ttry {\n\t\t\t// test unrecognized option\n\t\t\t{\n\t\t\t\tString[] parameters = {\"-v\", \"-l\"};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\t\t\t\n\t\t\t// test missing options\n\t\t\t{\n\t\t\t\tString[] parameters = {};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Program caused an exception: \" + e.getMessage());\n\t\t}\n\t}",
            "  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48 +\n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  ",
            "\t@Test\n\tpublic void testErrorCases() {\n\t\ttry {\n\t\t\t// test unrecognized option\n\t\t\t{\n\t\t\t\tString[] parameters = {\"-v\", \"-l\"};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\n\t\t\t// test missing options\n\t\t\t{\n\t\t\t\tString[] parameters = {};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.cancel(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Program caused an exception: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "ExecutionPlanAfterExecutionTest::testExecuteAfterGetExecutionPlan()",
            "  38  \n  39  \n  40 -\n  41  \n  42 -\n  43  \n  44  \n  45  \n  46 -\n  47 -\n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  ",
            "\t@Test\n\tpublic void testExecuteAfterGetExecutionPlan() {\n\t\tExecutionEnvironment env = new LocalEnvironment(); \n\t\tenv.getConfig().disableSysoutLogging();\n\t\t\n\t\tDataSet<Integer> baseSet = env.fromElements(1, 2);\n\n\t\tDataSet<Integer> result = baseSet.map(new MapFunction<Integer, Integer>() {\n\t\t\t@Override public Integer map(Integer value) throws Exception { return value * 2; }\n\t\t});\n\t\tresult.output(new DiscardingOutputFormat<Integer>());\n\n\t\ttry {\n\t\t\tenv.getExecutionPlan();\n\t\t\tenv.execute();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Cannot run both #getExecutionPlan and #execute.\");\n\t\t}\n\t}",
            "  43  \n  44  \n  45 +\n  46  \n  47 +\n  48  \n  49  \n  50  \n  51 +\n  52 +\n  53 +\n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "\t@Test\n\tpublic void testExecuteAfterGetExecutionPlan() {\n\t\tExecutionEnvironment env = new LocalEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\n\t\tDataSet<Integer> baseSet = env.fromElements(1, 2);\n\n\t\tDataSet<Integer> result = baseSet.map(new MapFunction<Integer, Integer>() {\n\t\t\t@Override public Integer map(Integer value) throws Exception {\n\t\t\t\treturn value * 2;\n\t\t\t}});\n\t\tresult.output(new DiscardingOutputFormat<Integer>());\n\n\t\ttry {\n\t\t\tenv.getExecutionPlan();\n\t\t\tenv.execute();\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Cannot run both #getExecutionPlan and #execute.\");\n\t\t}\n\t}"
        ],
        [
            "PackagedProgram::callMainMethod(Class,String)",
            " 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516 -\n 517  \n 518  \n 519 -\n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526 -\n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  ",
            "\tprivate static void callMainMethod(Class<?> entryClass, String[] args) throws ProgramInvocationException {\n\t\tMethod mainMethod;\n\t\tif (!Modifier.isPublic(entryClass.getModifiers())) {\n\t\t\tthrow new ProgramInvocationException(\"The class \" + entryClass.getName() + \" must be public.\");\n\t\t}\n\n\t\ttry {\n\t\t\tmainMethod = entryClass.getMethod(\"main\", String[].class);\n\t\t} catch (NoSuchMethodException e) {\n\t\t\tthrow new ProgramInvocationException(\"The class \" + entryClass.getName() + \" has no main(String[]) method.\");\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new ProgramInvocationException(\"Could not look up the main(String[]) method from the class \" + \n\t\t\t\t\tentryClass.getName() + \": \" + t.getMessage(), t);\n\t\t}\n\t\t\n\t\tif (!Modifier.isStatic(mainMethod.getModifiers())) {\n\t\t\tthrow new ProgramInvocationException(\"The class \" + entryClass.getName() + \" declares a non-static main method.\");\n\t\t}\n\t\tif (!Modifier.isPublic(mainMethod.getModifiers())) {\n\t\t\tthrow new ProgramInvocationException(\"The class \" + entryClass.getName() + \" declares a non-public main method.\");\n\t\t}\n\t\t\n\t\ttry {\n\t\t\tmainMethod.invoke(null, (Object) args);\n\t\t}\n\t\tcatch (IllegalArgumentException e) {\n\t\t\tthrow new ProgramInvocationException(\"Could not invoke the main method, arguments are not matching.\", e);\n\t\t}\n\t\tcatch (IllegalAccessException e) {\n\t\t\tthrow new ProgramInvocationException(\"Access to the main method was denied: \" + e.getMessage(), e);\n\t\t}\n\t\tcatch (InvocationTargetException e) {\n\t\t\tThrowable exceptionInMethod = e.getTargetException();\n\t\t\tif (exceptionInMethod instanceof Error) {\n\t\t\t\tthrow (Error) exceptionInMethod;\n\t\t\t} else if (exceptionInMethod instanceof ProgramParametrizationException) {\n\t\t\t\tthrow (ProgramParametrizationException) exceptionInMethod;\n\t\t\t} else if (exceptionInMethod instanceof ProgramInvocationException) {\n\t\t\t\tthrow (ProgramInvocationException) exceptionInMethod;\n\t\t\t} else {\n\t\t\t\tthrow new ProgramInvocationException(\"The main method caused an error.\", exceptionInMethod);\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new ProgramInvocationException(\"An error occurred while invoking the program's main method: \" + t.getMessage(), t);\n\t\t}\n\t}",
            " 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513 +\n 514  \n 515  \n 516 +\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523 +\n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  ",
            "\tprivate static void callMainMethod(Class<?> entryClass, String[] args) throws ProgramInvocationException {\n\t\tMethod mainMethod;\n\t\tif (!Modifier.isPublic(entryClass.getModifiers())) {\n\t\t\tthrow new ProgramInvocationException(\"The class \" + entryClass.getName() + \" must be public.\");\n\t\t}\n\n\t\ttry {\n\t\t\tmainMethod = entryClass.getMethod(\"main\", String[].class);\n\t\t} catch (NoSuchMethodException e) {\n\t\t\tthrow new ProgramInvocationException(\"The class \" + entryClass.getName() + \" has no main(String[]) method.\");\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new ProgramInvocationException(\"Could not look up the main(String[]) method from the class \" +\n\t\t\t\t\tentryClass.getName() + \": \" + t.getMessage(), t);\n\t\t}\n\n\t\tif (!Modifier.isStatic(mainMethod.getModifiers())) {\n\t\t\tthrow new ProgramInvocationException(\"The class \" + entryClass.getName() + \" declares a non-static main method.\");\n\t\t}\n\t\tif (!Modifier.isPublic(mainMethod.getModifiers())) {\n\t\t\tthrow new ProgramInvocationException(\"The class \" + entryClass.getName() + \" declares a non-public main method.\");\n\t\t}\n\n\t\ttry {\n\t\t\tmainMethod.invoke(null, (Object) args);\n\t\t}\n\t\tcatch (IllegalArgumentException e) {\n\t\t\tthrow new ProgramInvocationException(\"Could not invoke the main method, arguments are not matching.\", e);\n\t\t}\n\t\tcatch (IllegalAccessException e) {\n\t\t\tthrow new ProgramInvocationException(\"Access to the main method was denied: \" + e.getMessage(), e);\n\t\t}\n\t\tcatch (InvocationTargetException e) {\n\t\t\tThrowable exceptionInMethod = e.getTargetException();\n\t\t\tif (exceptionInMethod instanceof Error) {\n\t\t\t\tthrow (Error) exceptionInMethod;\n\t\t\t} else if (exceptionInMethod instanceof ProgramParametrizationException) {\n\t\t\t\tthrow (ProgramParametrizationException) exceptionInMethod;\n\t\t\t} else if (exceptionInMethod instanceof ProgramInvocationException) {\n\t\t\t\tthrow (ProgramInvocationException) exceptionInMethod;\n\t\t\t} else {\n\t\t\t\tthrow new ProgramInvocationException(\"The main method caused an error.\", exceptionInMethod);\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new ProgramInvocationException(\"An error occurred while invoking the program's main method: \" + t.getMessage(), t);\n\t\t}\n\t}"
        ],
        [
            "ClusterClient::getOptimizedPlan(Optimizer,PackagedProgram,int)",
            " 305  \n 306 -\n 307 -\n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  ",
            "\tpublic static FlinkPlan getOptimizedPlan(Optimizer compiler, PackagedProgram prog, int parallelism)\n\t\t\tthrows CompilerException, ProgramInvocationException\n\t{\n\t\tThread.currentThread().setContextClassLoader(prog.getUserCodeClassLoader());\n\t\tif (prog.isUsingProgramEntryPoint()) {\n\t\t\treturn getOptimizedPlan(compiler, prog.getPlanWithJars(), parallelism);\n\t\t} else if (prog.isUsingInteractiveMode()) {\n\t\t\t// temporary hack to support the optimizer plan preview\n\t\t\tOptimizerPlanEnvironment env = new OptimizerPlanEnvironment(compiler);\n\t\t\tif (parallelism > 0) {\n\t\t\t\tenv.setParallelism(parallelism);\n\t\t\t}\n\n\t\t\treturn env.getOptimizedPlan(prog);\n\t\t} else {\n\t\t\tthrow new RuntimeException(\"Couldn't determine program mode.\");\n\t\t}\n\t}",
            " 309  \n 310 +\n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  ",
            "\tpublic static FlinkPlan getOptimizedPlan(Optimizer compiler, PackagedProgram prog, int parallelism)\n\t\t\tthrows CompilerException, ProgramInvocationException {\n\t\tThread.currentThread().setContextClassLoader(prog.getUserCodeClassLoader());\n\t\tif (prog.isUsingProgramEntryPoint()) {\n\t\t\treturn getOptimizedPlan(compiler, prog.getPlanWithJars(), parallelism);\n\t\t} else if (prog.isUsingInteractiveMode()) {\n\t\t\t// temporary hack to support the optimizer plan preview\n\t\t\tOptimizerPlanEnvironment env = new OptimizerPlanEnvironment(compiler);\n\t\t\tif (parallelism > 0) {\n\t\t\t\tenv.setParallelism(parallelism);\n\t\t\t}\n\n\t\t\treturn env.getOptimizedPlan(prog);\n\t\t} else {\n\t\t\tthrow new RuntimeException(\"Couldn't determine program mode.\");\n\t\t}\n\t}"
        ],
        [
            "PackagedProgram::PackagedProgram(Class,String)",
            " 225  \n 226  \n 227  \n 228 -\n 229  \n 230  \n 231  \n 232 -\n 233  \n 234  \n 235 -\n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245 -\n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256 -\n 257  \n 258  \n 259  ",
            "\tPackagedProgram(Class<?> entryPointClass, String... args) throws ProgramInvocationException {\n\t\tthis.jarFile = null;\n\t\tthis.args = args == null ? new String[0] : args;\n\t\t\n\t\tthis.extractedTempLibraries = Collections.emptyList();\n\t\tthis.classpaths = Collections.emptyList();\n\t\tthis.userCodeClassLoader = entryPointClass.getClassLoader();\n\t\t\n\t\t// load the entry point class\n\t\tthis.mainClass = entryPointClass;\n\t\t\n\t\t// if the entry point is a program, instantiate the class and get the plan\n\t\tif (Program.class.isAssignableFrom(this.mainClass)) {\n\t\t\tProgram prg = null;\n\t\t\ttry {\n\t\t\t\tprg = InstantiationUtil.instantiate(this.mainClass.asSubclass(Program.class), Program.class);\n\t\t\t} catch (Exception e) {\n\t\t\t\t// validate that the class has a main method at least.\n\t\t\t\t// the main method possibly instantiates the program properly\n\t\t\t\tif (!hasMainMethod(mainClass)) {\n\t\t\t\t\tthrow new ProgramInvocationException(\"The given program class implements the \" + \n\t\t\t\t\t\t\tProgram.class.getName() + \" interface, but cannot be instantiated. \" +\n\t\t\t\t\t\t\t\"It also declares no main(String[]) method as alternative entry point\", e);\n\t\t\t\t}\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new ProgramInvocationException(\"Error while trying to instantiate program class.\", t);\n\t\t\t}\n\t\t\tthis.program = prg;\n\t\t} else if (hasMainMethod(mainClass)) {\n\t\t\tthis.program = null;\n\t\t} else {\n\t\t\tthrow new ProgramInvocationException(\"The given program class neither has a main(String[]) method, nor does it implement the \" + \n\t\t\t\t\tProgram.class.getName() + \" interface.\");\n\t\t}\n\t}",
            " 224  \n 225  \n 226  \n 227 +\n 228  \n 229  \n 230  \n 231 +\n 232  \n 233  \n 234 +\n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244 +\n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255 +\n 256  \n 257  \n 258  ",
            "\tPackagedProgram(Class<?> entryPointClass, String... args) throws ProgramInvocationException {\n\t\tthis.jarFile = null;\n\t\tthis.args = args == null ? new String[0] : args;\n\n\t\tthis.extractedTempLibraries = Collections.emptyList();\n\t\tthis.classpaths = Collections.emptyList();\n\t\tthis.userCodeClassLoader = entryPointClass.getClassLoader();\n\n\t\t// load the entry point class\n\t\tthis.mainClass = entryPointClass;\n\n\t\t// if the entry point is a program, instantiate the class and get the plan\n\t\tif (Program.class.isAssignableFrom(this.mainClass)) {\n\t\t\tProgram prg = null;\n\t\t\ttry {\n\t\t\t\tprg = InstantiationUtil.instantiate(this.mainClass.asSubclass(Program.class), Program.class);\n\t\t\t} catch (Exception e) {\n\t\t\t\t// validate that the class has a main method at least.\n\t\t\t\t// the main method possibly instantiates the program properly\n\t\t\t\tif (!hasMainMethod(mainClass)) {\n\t\t\t\t\tthrow new ProgramInvocationException(\"The given program class implements the \" +\n\t\t\t\t\t\t\tProgram.class.getName() + \" interface, but cannot be instantiated. \" +\n\t\t\t\t\t\t\t\"It also declares no main(String[]) method as alternative entry point\", e);\n\t\t\t\t}\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new ProgramInvocationException(\"Error while trying to instantiate program class.\", t);\n\t\t\t}\n\t\t\tthis.program = prg;\n\t\t} else if (hasMainMethod(mainClass)) {\n\t\t\tthis.program = null;\n\t\t} else {\n\t\t\tthrow new ProgramInvocationException(\"The given program class neither has a main(String[]) method, nor does it implement the \" +\n\t\t\t\t\tProgram.class.getName() + \" interface.\");\n\t\t}\n\t}"
        ],
        [
            "ExecutionPlanCreationTest::testGetExecutionPlan()",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47 -\n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 -\n  60  \n  61  \n  62 -\n  63  \n  64  \n  65  \n  66  \n  67 -\n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  ",
            "\t@Test\n\tpublic void testGetExecutionPlan() {\n\t\ttry {\n\t\t\tPackagedProgram prg = new PackagedProgram(TestOptimizerPlan.class, \"/dev/random\", \"/tmp\");\n\t\t\tassertNotNull(prg.getPreviewPlan());\n\t\t\t\n\t\t\tInetAddress mockAddress = InetAddress.getLocalHost();\n\t\t\tInetSocketAddress mockJmAddress = new InetSocketAddress(mockAddress, 12345);\n\n\t\t\tConfiguration config = new Configuration();\n\n\t\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, mockJmAddress.getHostName());\n\t\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, mockJmAddress.getPort());\n\n\t\t\tOptimizer optimizer = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), config);\n\t\t\tOptimizedPlan op = (OptimizedPlan) ClusterClient.getOptimizedPlan(optimizer, prg, -1);\n\t\t\tassertNotNull(op);\n\t\t\t\n\t\t\tPlanJSONDumpGenerator dumper = new PlanJSONDumpGenerator();\n\t\t\tassertNotNull(dumper.getOptimizerPlanAsJSON(op));\n\t\t\t\n\t\t\t// test HTML escaping\n\t\t\tPlanJSONDumpGenerator dumper2 = new PlanJSONDumpGenerator();\n\t\t\tdumper2.setEncodeForHTML(true);\n\t\t\tString htmlEscaped = dumper2.getOptimizerPlanAsJSON(op);\n\t\t\t\n\t\t\tassertEquals(-1, htmlEscaped.indexOf('\\\\'));\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t}",
            "  48  \n  49  \n  50  \n  51  \n  52  \n  53 +\n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68 +\n  69  \n  70  \n  71  \n  72  \n  73 +\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "\t@Test\n\tpublic void testGetExecutionPlan() {\n\t\ttry {\n\t\t\tPackagedProgram prg = new PackagedProgram(TestOptimizerPlan.class, \"/dev/random\", \"/tmp\");\n\t\t\tassertNotNull(prg.getPreviewPlan());\n\n\t\t\tInetAddress mockAddress = InetAddress.getLocalHost();\n\t\t\tInetSocketAddress mockJmAddress = new InetSocketAddress(mockAddress, 12345);\n\n\t\t\tConfiguration config = new Configuration();\n\n\t\t\tconfig.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, mockJmAddress.getHostName());\n\t\t\tconfig.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, mockJmAddress.getPort());\n\n\t\t\tOptimizer optimizer = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), config);\n\t\t\tOptimizedPlan op = (OptimizedPlan) ClusterClient.getOptimizedPlan(optimizer, prg, -1);\n\t\t\tassertNotNull(op);\n\n\t\t\tPlanJSONDumpGenerator dumper = new PlanJSONDumpGenerator();\n\t\t\tassertNotNull(dumper.getOptimizerPlanAsJSON(op));\n\n\t\t\t// test HTML escaping\n\t\t\tPlanJSONDumpGenerator dumper2 = new PlanJSONDumpGenerator();\n\t\t\tdumper2.setEncodeForHTML(true);\n\t\t\tString htmlEscaped = dumper2.getOptimizerPlanAsJSON(op);\n\n\t\t\tassertEquals(-1, htmlEscaped.indexOf('\\\\'));\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "RemoteExecutor::RemoteExecutor(InetSocketAddress,Configuration,List,List)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112 -\n 113  \n 114  \n 115  ",
            "\tpublic RemoteExecutor(InetSocketAddress inet, Configuration clientConfiguration,\n\t\t\tList<URL> jarFiles, List<URL> globalClasspaths) {\n\t\tthis.clientConfiguration = clientConfiguration;\n\t\tthis.jarFiles = jarFiles;\n\t\tthis.globalClasspaths = globalClasspaths;\n\n\n\t\tclientConfiguration.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, inet.getHostName());\n\t\tclientConfiguration.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, inet.getPort());\n\t}",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  ",
            "\tpublic RemoteExecutor(InetSocketAddress inet, Configuration clientConfiguration,\n\t\t\tList<URL> jarFiles, List<URL> globalClasspaths) {\n\t\tthis.clientConfiguration = clientConfiguration;\n\t\tthis.jarFiles = jarFiles;\n\t\tthis.globalClasspaths = globalClasspaths;\n\n\t\tclientConfiguration.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, inet.getHostName());\n\t\tclientConfiguration.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, inet.getPort());\n\t}"
        ],
        [
            "ClientTest::TestGetAllAccumulator::main(String)",
            " 433 -\n 434  \n 435  \n 436  \n 437  ",
            "\t\tpublic static void main(String args[]) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute().getAllAccumulatorResults();\n\t\t}",
            " 455 +\n 456  \n 457  \n 458  \n 459  ",
            "\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute().getAllAccumulatorResults();\n\t\t}"
        ],
        [
            "PackagedProgram::loadMainClass(String,ClassLoader)",
            " 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630 -\n 631  \n 632  \n 633  \n 634  \n 635  \n 636  ",
            "\tprivate static Class<?> loadMainClass(String className, ClassLoader cl) throws ProgramInvocationException {\n\t\tClassLoader contextCl = null;\n\t\ttry {\n\t\t\tcontextCl = Thread.currentThread().getContextClassLoader();\n\t\t\tThread.currentThread().setContextClassLoader(cl);\n\t\t\treturn Class.forName(className, false, cl);\n\t\t}\n\t\tcatch (ClassNotFoundException e) {\n\t\t\tthrow new ProgramInvocationException(\"The program's entry point class '\" + className\n\t\t\t\t+ \"' was not found in the jar file.\", e);\n\t\t}\n\t\tcatch (ExceptionInInitializerError e) {\n\t\t\tthrow new ProgramInvocationException(\"The program's entry point class '\" + className\n\t\t\t\t+ \"' threw an error during initialization.\", e);\n\t\t}\n\t\tcatch (LinkageError e) {\n\t\t\tthrow new ProgramInvocationException(\"The program's entry point class '\" + className\n\t\t\t\t+ \"' could not be loaded due to a linkage failure.\", e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new ProgramInvocationException(\"The program's entry point class '\" + className\n\t\t\t\t+ \"' caused an exception during initialization: \"+ t.getMessage(), t);\n\t\t} finally {\n\t\t\tif (contextCl != null) {\n\t\t\t\tThread.currentThread().setContextClassLoader(contextCl);\n\t\t\t}\n\t\t}\n\t}",
            " 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626 +\n 627  \n 628  \n 629  \n 630  \n 631  \n 632  ",
            "\tprivate static Class<?> loadMainClass(String className, ClassLoader cl) throws ProgramInvocationException {\n\t\tClassLoader contextCl = null;\n\t\ttry {\n\t\t\tcontextCl = Thread.currentThread().getContextClassLoader();\n\t\t\tThread.currentThread().setContextClassLoader(cl);\n\t\t\treturn Class.forName(className, false, cl);\n\t\t}\n\t\tcatch (ClassNotFoundException e) {\n\t\t\tthrow new ProgramInvocationException(\"The program's entry point class '\" + className\n\t\t\t\t+ \"' was not found in the jar file.\", e);\n\t\t}\n\t\tcatch (ExceptionInInitializerError e) {\n\t\t\tthrow new ProgramInvocationException(\"The program's entry point class '\" + className\n\t\t\t\t+ \"' threw an error during initialization.\", e);\n\t\t}\n\t\tcatch (LinkageError e) {\n\t\t\tthrow new ProgramInvocationException(\"The program's entry point class '\" + className\n\t\t\t\t+ \"' could not be loaded due to a linkage failure.\", e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new ProgramInvocationException(\"The program's entry point class '\" + className\n\t\t\t\t+ \"' caused an exception during initialization: \" + t.getMessage(), t);\n\t\t} finally {\n\t\t\tif (contextCl != null) {\n\t\t\t\tThread.currentThread().setContextClassLoader(contextCl);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "WordCount::Tokenizer::flatMap(String,Collector)",
            "  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "\t\t@Override\n\t\tpublic void flatMap(String value, Collector<Tuple2<String, Integer>> out) {\n\t\t\t// normalize and split the line\n\t\t\tString[] tokens = value.toLowerCase().split(\"\\\\W+\");\n\t\t\t\n\t\t\t// emit the pairs\n\t\t\tfor (String token : tokens) {\n\t\t\t\tif (token.length() > 0) {\n\t\t\t\t\tout.collect(new Tuple2<String, Integer>(token, 1));\n\t\t\t\t}\n\t\t\t}\n\t\t}",
            "  79  \n  80  \n  81  \n  82  \n  83 +\n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "\t\t@Override\n\t\tpublic void flatMap(String value, Collector<Tuple2<String, Integer>> out) {\n\t\t\t// normalize and split the line\n\t\t\tString[] tokens = value.toLowerCase().split(\"\\\\W+\");\n\n\t\t\t// emit the pairs\n\t\t\tfor (String token : tokens) {\n\t\t\t\tif (token.length() > 0) {\n\t\t\t\t\tout.collect(new Tuple2<String, Integer>(token, 1));\n\t\t\t\t}\n\t\t\t}\n\t\t}"
        ],
        [
            "ClusterClient::stop(JobID)",
            " 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635 -\n 636  \n 637  \n 638 -\n 639  \n 640  \n 641  \n 642  \n 643  ",
            "\t/**\n\t * Stops a program on Flink cluster whose job-manager is configured in this client's configuration.\n\t * Stopping works only for streaming programs. Be aware, that the program might continue to run for\n\t * a while after sending the stop command, because after sources stopped to emit data all operators\n\t * need to finish processing.\n\t * \n\t * @param jobId\n\t *            the job ID of the streaming program to stop\n\t * @throws Exception\n\t *             If the job ID is invalid (ie, is unknown or refers to a batch job) or if sending the stop signal\n\t *             failed. That might be due to an I/O problem, ie, the job-manager is unreachable.\n\t */\n\tpublic void stop(final JobID jobId) throws Exception {\n\t\tfinal ActorGateway jobManagerGateway = getJobManagerGateway();\n\n\t\tfinal Future<Object> response;\n\t\ttry {\n\t\t\tresponse = jobManagerGateway.ask(new JobManagerMessages.StopJob(jobId), timeout);\n\t\t} catch (final Exception e) {\n\t\t\tthrow new ProgramInvocationException(\"Failed to query the job manager gateway.\", e);\n\t\t}\n\n\t\tfinal Object result = Await.result(response, timeout);\n\n\t\tif (result instanceof JobManagerMessages.StoppingSuccess) {\n\t\t\tLOG.info(\"Job stopping with ID \" + jobId + \" succeeded.\");\n\t\t} else if (result instanceof JobManagerMessages.StoppingFailure) {\n\t\t\tfinal Throwable t = ((JobManagerMessages.StoppingFailure) result).cause();\n\t\t\tLOG.info(\"Job stopping with ID \" + jobId + \" failed.\", t);\n\t\t\tthrow new Exception(\"Failed to stop the job because of \\n\" + t.getMessage());\n\t\t} else {\n\t\t\tthrow new Exception(\"Unknown message received while stopping: \" + result.getClass().getName());\n\t\t}\n\t}",
            " 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636 +\n 637  \n 638  \n 639 +\n 640  \n 641  \n 642  \n 643  \n 644  ",
            "\t/**\n\t * Stops a program on Flink cluster whose job-manager is configured in this client's configuration.\n\t * Stopping works only for streaming programs. Be aware, that the program might continue to run for\n\t * a while after sending the stop command, because after sources stopped to emit data all operators\n\t * need to finish processing.\n\t *\n\t * @param jobId\n\t *            the job ID of the streaming program to stop\n\t * @throws Exception\n\t *             If the job ID is invalid (ie, is unknown or refers to a batch job) or if sending the stop signal\n\t *             failed. That might be due to an I/O problem, ie, the job-manager is unreachable.\n\t */\n\tpublic void stop(final JobID jobId) throws Exception {\n\t\tfinal ActorGateway jobManagerGateway = getJobManagerGateway();\n\n\t\tfinal Future<Object> response;\n\t\ttry {\n\t\t\tresponse = jobManagerGateway.ask(new JobManagerMessages.StopJob(jobId), timeout);\n\t\t} catch (final Exception e) {\n\t\t\tthrow new ProgramInvocationException(\"Failed to query the job manager gateway.\", e);\n\t\t}\n\n\t\tfinal Object result = Await.result(response, timeout);\n\n\t\tif (result instanceof JobManagerMessages.StoppingSuccess) {\n\t\t\tlog.info(\"Job stopping with ID \" + jobId + \" succeeded.\");\n\t\t} else if (result instanceof JobManagerMessages.StoppingFailure) {\n\t\t\tfinal Throwable t = ((JobManagerMessages.StoppingFailure) result).cause();\n\t\t\tlog.info(\"Job stopping with ID \" + jobId + \" failed.\", t);\n\t\t\tthrow new Exception(\"Failed to stop the job because of \\n\" + t.getMessage());\n\t\t} else {\n\t\t\tthrow new Exception(\"Unknown message received while stopping: \" + result.getClass().getName());\n\t\t}\n\t}"
        ],
        [
            "ClusterClient::getJobManagerGateway()",
            " 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781 -\n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  ",
            "\t/**\n\t * Returns the {@link ActorGateway} of the current job manager leader using\n\t * the {@link LeaderRetrievalService}.\n\t *\n\t * @return ActorGateway of the current job manager leader\n\t * @throws Exception\n\t */\n\tpublic ActorGateway getJobManagerGateway() throws Exception {\n\t\tLOG.debug(\"Looking up JobManager\");\n\n\t\ttry {\n\t\t\treturn LeaderRetrievalUtils.retrieveLeaderGateway(\n\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\tactorSystemLoader.get(),\n\t\t\t\tlookupTimeout);\n\t\t} catch (LeaderRetrievalException lre) {\n\t\t\tthrow new FlinkException(\"Could not connect to the leading JobManager. Please check that the \" +\n\t\t\t\t\"JobManager is running.\", lre);\n\t\t}\n\t}",
            " 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781 +\n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  ",
            "\t/**\n\t * Returns the {@link ActorGateway} of the current job manager leader using\n\t * the {@link LeaderRetrievalService}.\n\t *\n\t * @return ActorGateway of the current job manager leader\n\t * @throws Exception\n\t */\n\tpublic ActorGateway getJobManagerGateway() throws Exception {\n\t\tlog.debug(\"Looking up JobManager\");\n\n\t\ttry {\n\t\t\treturn LeaderRetrievalUtils.retrieveLeaderGateway(\n\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\tactorSystemLoader.get(),\n\t\t\t\tlookupTimeout);\n\t\t} catch (LeaderRetrievalException lre) {\n\t\t\tthrow new FlinkException(\"Could not connect to the leading JobManager. Please check that the \" +\n\t\t\t\t\"JobManager is running.\", lre);\n\t\t}\n\t}"
        ],
        [
            "PackagedProgramTest::testGetPreviewPlan()",
            "  31  \n  32  \n  33  \n  34  \n  35 -\n  36  \n  37  \n  38  \n  39  \n  40  \n  41 -\n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  ",
            "\t@Test\n\tpublic void testGetPreviewPlan() {\n\t\ttry {\n\t\t\tPackagedProgram prog = new PackagedProgram(new File(CliFrontendTestUtils.getTestJarPath()));\n\t\t\t\n\t\t\tfinal PrintStream out = System.out;\n\t\t\tfinal PrintStream err = System.err;\n\t\t\ttry {\n\t\t\t\tSystem.setOut(new PrintStream(new NullOutputStream()));\n\t\t\t\tSystem.setErr(new PrintStream(new NullOutputStream()));\n\t\t\t\t\n\t\t\t\tAssert.assertNotNull(prog.getPreviewPlan());\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\tSystem.setOut(out);\n\t\t\t\tSystem.setErr(err);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tSystem.err.println(e.getMessage());\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test is erroneous: \" + e.getMessage());\n\t\t}\n\t}",
            "  34  \n  35  \n  36  \n  37  \n  38 +\n  39  \n  40  \n  41  \n  42  \n  43  \n  44 +\n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  ",
            "\t@Test\n\tpublic void testGetPreviewPlan() {\n\t\ttry {\n\t\t\tPackagedProgram prog = new PackagedProgram(new File(CliFrontendTestUtils.getTestJarPath()));\n\n\t\t\tfinal PrintStream out = System.out;\n\t\t\tfinal PrintStream err = System.err;\n\t\t\ttry {\n\t\t\t\tSystem.setOut(new PrintStream(new NullOutputStream()));\n\t\t\t\tSystem.setErr(new PrintStream(new NullOutputStream()));\n\n\t\t\t\tAssert.assertNotNull(prog.getPreviewPlan());\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\tSystem.setOut(out);\n\t\t\t\tSystem.setErr(err);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tSystem.err.println(e.getMessage());\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(\"Test is erroneous: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "ClusterClient::LazyActorSystemLoader::get()",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 -\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  ",
            "\t\t/**\n\t\t * Creates a new ActorSystem or returns an existing one.\n\t\t * @return ActorSystem\n\t\t * @throws Exception if the ActorSystem could not be created\n\t\t */\n\t\tpublic ActorSystem get() throws FlinkException {\n\n\t\t\tif (!isLoaded()) {\n\t\t\t\t// start actor system\n\t\t\t\tLOG.info(\"Starting client actor system.\");\n\n\t\t\t\tfinal InetAddress ownHostname;\n\t\t\t\ttry {\n\t\t\t\t\townHostname = LeaderRetrievalUtils.findConnectingAddress(\n\t\t\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\t\t\ttimeout);\n\t\t\t\t} catch (LeaderRetrievalException lre) {\n\t\t\t\t\tthrow new FlinkException(\"Could not find out our own hostname by connecting to the \" +\n\t\t\t\t\t\t\"leading JobManager. Please make sure that the Flink cluster has been started.\", lre);\n\t\t\t\t}\n\n\t\t\t\ttry {\n\t\t\t\t\tactorSystem = AkkaUtils.createActorSystem(\n\t\t\t\t\t\tconfiguration,\n\t\t\t\t\t\tOption.apply(new Tuple2<String, Object>(ownHostname.getCanonicalHostName(), 0)));\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tthrow new FlinkException(\"Could not start the ActorSystem lazily.\", e);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn actorSystem;\n\t\t}",
            " 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  ",
            "\t\t/**\n\t\t * Creates a new ActorSystem or returns an existing one.\n\t\t * @return ActorSystem\n\t\t * @throws Exception if the ActorSystem could not be created\n\t\t */\n\t\tpublic ActorSystem get() throws FlinkException {\n\n\t\t\tif (!isLoaded()) {\n\t\t\t\t// start actor system\n\t\t\t\tlog.info(\"Starting client actor system.\");\n\n\t\t\t\tfinal InetAddress ownHostname;\n\t\t\t\ttry {\n\t\t\t\t\townHostname = LeaderRetrievalUtils.findConnectingAddress(\n\t\t\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\t\t\ttimeout);\n\t\t\t\t} catch (LeaderRetrievalException lre) {\n\t\t\t\t\tthrow new FlinkException(\"Could not find out our own hostname by connecting to the \" +\n\t\t\t\t\t\t\"leading JobManager. Please make sure that the Flink cluster has been started.\", lre);\n\t\t\t\t}\n\n\t\t\t\ttry {\n\t\t\t\t\tactorSystem = AkkaUtils.createActorSystem(\n\t\t\t\t\t\tconfiguration,\n\t\t\t\t\t\tOption.apply(new Tuple2<String, Object>(ownHostname.getCanonicalHostName(), 0)));\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tthrow new FlinkException(\"Could not start the ActorSystem lazily.\", e);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn actorSystem;\n\t\t}"
        ],
        [
            "ClusterClient::run(FlinkPlan,List,List,ClassLoader,SavepointRestoreSettings)",
            " 437  \n 438  \n 439 -\n 440 -\n 441  \n 442  \n 443  ",
            "\tpublic JobSubmissionResult run(FlinkPlan compiledPlan,\n\t\t\tList<URL> libraries, List<URL> classpaths, ClassLoader classLoader, SavepointRestoreSettings savepointSettings)\n\t\tthrows ProgramInvocationException\n\t{\n\t\tJobGraph job = getJobGraph(compiledPlan, libraries, classpaths, savepointSettings);\n\t\treturn submitJob(job, classLoader);\n\t}",
            " 439  \n 440  \n 441 +\n 442  \n 443  \n 444  ",
            "\tpublic JobSubmissionResult run(FlinkPlan compiledPlan,\n\t\t\tList<URL> libraries, List<URL> classpaths, ClassLoader classLoader, SavepointRestoreSettings savepointSettings)\n\t\t\tthrows ProgramInvocationException {\n\t\tJobGraph job = getJobGraph(compiledPlan, libraries, classpaths, savepointSettings);\n\t\treturn submitJob(job, classLoader);\n\t}"
        ],
        [
            "ClientTest::TestOptimizerPlan::main(String)",
            " 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 -\n 373  \n 374 -\n 375  \n 376  \n 377  \n 378  \n 379  ",
            "\t\t@SuppressWarnings(\"serial\")\n\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tif (args.length < 2) {\n\t\t\t\tSystem.err.println(\"Usage: TestOptimizerPlan <input-file-path> <output-file-path>\");\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t\tDataSet<Tuple2<Long, Long>> input = env.readCsvFile(args[0])\n\t\t\t\t\t.fieldDelimiter(\"\\t\").types(Long.class, Long.class);\n\n\t\t\tDataSet<Tuple2<Long, Long>> result = input.map(\n\t\t\t\t\tnew MapFunction<Tuple2<Long,Long>, Tuple2<Long,Long>>() {\n\t\t\t\t\t\tpublic Tuple2<Long, Long> map(Tuple2<Long, Long> value){\n\t\t\t\t\t\t\treturn new Tuple2<Long, Long>(value.f0, value.f1+1);\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t\tresult.writeAsCsv(args[1], \"\\n\", \"\\t\");\n\t\t\tenv.execute();\n\t\t}",
            " 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375 +\n 376  \n 377 +\n 378  \n 379  \n 380  \n 381  \n 382  ",
            "\t\t@SuppressWarnings(\"serial\")\n\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tif (args.length < 2) {\n\t\t\t\tSystem.err.println(\"Usage: TestOptimizerPlan <input-file-path> <output-file-path>\");\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t\tDataSet<Tuple2<Long, Long>> input = env.readCsvFile(args[0])\n\t\t\t\t\t.fieldDelimiter(\"\\t\").types(Long.class, Long.class);\n\n\t\t\tDataSet<Tuple2<Long, Long>> result = input.map(\n\t\t\t\t\tnew MapFunction<Tuple2<Long, Long>, Tuple2<Long, Long>>() {\n\t\t\t\t\t\tpublic Tuple2<Long, Long> map(Tuple2<Long, Long> value){\n\t\t\t\t\t\t\treturn new Tuple2<Long, Long>(value.f0, value.f1 + 1);\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t\tresult.writeAsCsv(args[1], \"\\n\", \"\\t\");\n\t\t\tenv.execute();\n\t\t}"
        ],
        [
            "ClusterClient::getOptimizedPlanAsJson(Optimizer,PackagedProgram,int)",
            " 298  \n 299 -\n 300 -\n 301  \n 302  \n 303  ",
            "\tpublic static String getOptimizedPlanAsJson(Optimizer compiler, PackagedProgram prog, int parallelism)\n\t\t\tthrows CompilerException, ProgramInvocationException\n\t{\n\t\tPlanJSONDumpGenerator jsonGen = new PlanJSONDumpGenerator();\n\t\treturn jsonGen.getOptimizerPlanAsJSON((OptimizedPlan) getOptimizedPlan(compiler, prog, parallelism));\n\t}",
            " 303  \n 304 +\n 305  \n 306  \n 307  ",
            "\tpublic static String getOptimizedPlanAsJson(Optimizer compiler, PackagedProgram prog, int parallelism)\n\t\t\tthrows CompilerException, ProgramInvocationException {\n\t\tPlanJSONDumpGenerator jsonGen = new PlanJSONDumpGenerator();\n\t\treturn jsonGen.getOptimizerPlanAsJSON((OptimizedPlan) getOptimizedPlan(compiler, prog, parallelism));\n\t}"
        ],
        [
            "CliFrontendPackageProgramTest::testPlanWithExternalClass()",
            " 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306 -\n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  ",
            "\t/**\n\t * Ensure that we will never have the following error.\n\t *\n\t * <pre>\n\t * \torg.apache.flink.client.program.ProgramInvocationException: The main method caused an error.\n\t *\t\tat org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:398)\n\t *\t\tat org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:301)\n\t *\t\tat org.apache.flink.client.program.Client.getOptimizedPlan(Client.java:140)\n\t *\t\tat org.apache.flink.client.program.Client.getOptimizedPlanAsJson(Client.java:125)\n\t *\t\tat org.apache.flink.client.CliFrontend.info(CliFrontend.java:439)\n\t *\t\tat org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:931)\n\t *\t\tat org.apache.flink.client.CliFrontend.main(CliFrontend.java:951)\n\t *\tCaused by: java.io.IOException: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.RCFileInputFormat\n\t *\t\tat org.apache.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:102)\n\t *\t\tat org.apache.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:54)\n\t *\t\tat tlabs.CDR_In_Report.createHCatInputFormat(CDR_In_Report.java:322)\n\t *\t\tat tlabs.CDR_Out_Report.main(CDR_Out_Report.java:380)\n\t *\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t *\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\t *\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t *\t\tat java.lang.reflect.Method.invoke(Method.java:622)\n\t *\t\tat org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:383)\n\t * </pre>\n\t *\n\t * The test works as follows:\n\t *\n\t * <ul>\n\t *   <li> Use the CliFrontend to invoke a jar file that loads a class which is only available\n\t * \t      in the jarfile itself (via a custom classloader)\n\t *   <li> Change the Usercode classloader of the PackagedProgram to a special classloader for this test\n\t *   <li> the classloader will accept the special class (and return a String.class)\n\t * </ul>\n\t */\n\t@Test\n\tpublic void testPlanWithExternalClass() throws CompilerException, ProgramInvocationException {\n\t\tfinal boolean[] callme = { false }; // create a final object reference, to be able to change its val later\n\n\t\ttry {\n\t\t\tString[] arguments = {\n\t\t\t\t\t\"--classpath\", \"file:///tmp/foo\",\n\t\t\t\t\t\"--classpath\", \"file:///tmp/bar\",\n\t\t\t\t\t\"-c\", TEST_JAR_CLASSLOADERTEST_CLASS, getTestJarPath(),\n\t\t\t\t\t\"true\", \"arg1\", \"arg2\" };\n\t\t\tURL[] classpath = new URL[] { new URL(\"file:///tmp/foo\"), new URL(\"file:///tmp/bar\") };\n\t\t\tString[] reducedArguments = { \"true\", \"arg1\", \"arg2\" };\n\n\t\t\tRunOptions options = CliFrontendParser.parseRunCommand(arguments);\n\t\t\tassertEquals(getTestJarPath(), options.getJarFilePath());\n\t\t\tassertArrayEquals(classpath, options.getClasspaths().toArray());\n\t\t\tassertEquals(TEST_JAR_CLASSLOADERTEST_CLASS, options.getEntryPointClassName());\n\t\t\tassertArrayEquals(reducedArguments, options.getProgramArgs());\n\t\t\t\n\t\t\tCliFrontend frontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\tPackagedProgram prog = spy(frontend.buildProgram(options));\n\n\t\t\tClassLoader testClassLoader = new ClassLoader(prog.getUserCodeClassLoader()) {\n\t\t\t\t@Override\n\t\t\t\tpublic Class<?> loadClass(String name) throws ClassNotFoundException {\n\t\t\t\t\tif (\"org.apache.hadoop.hive.ql.io.RCFileInputFormat\".equals(name)) {\n\t\t\t\t\t\tcallme[0] = true;\n\t\t\t\t\t\treturn String.class; // Intentionally return the wrong class.\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn super.loadClass(name);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\twhen(prog.getUserCodeClassLoader()).thenReturn(testClassLoader);\n\n\t\t\tassertEquals(TEST_JAR_CLASSLOADERTEST_CLASS, prog.getMainClassName());\n\t\t\tassertArrayEquals(reducedArguments, prog.getArguments());\n\n\t\t\tConfiguration c = new Configuration();\n\t\t\tOptimizer compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), c);\n\n\t\t\t// we expect this to fail with a \"ClassNotFoundException\"\n\t\t\tClusterClient.getOptimizedPlanAsJson(compiler, prog, 666);\n\t\t\tfail(\"Should have failed with a ClassNotFoundException\");\n\t\t}\n\t\tcatch (ProgramInvocationException e) {\n\t\t\tif (!(e.getCause() instanceof ClassNotFoundException)) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail(\"Program didn't throw ClassNotFoundException\");\n\t\t\t}\n\t\t\tassertTrue(\"Classloader was not called\", callme[0]);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Program failed with the wrong exception: \" + e.getClass().getName());\n\t\t}\n\t}",
            " 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 +\n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  ",
            "\t/**\n\t * Ensure that we will never have the following error.\n\t *\n\t * <pre>\n\t * \torg.apache.flink.client.program.ProgramInvocationException: The main method caused an error.\n\t *\t\tat org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:398)\n\t *\t\tat org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:301)\n\t *\t\tat org.apache.flink.client.program.Client.getOptimizedPlan(Client.java:140)\n\t *\t\tat org.apache.flink.client.program.Client.getOptimizedPlanAsJson(Client.java:125)\n\t *\t\tat org.apache.flink.client.CliFrontend.info(CliFrontend.java:439)\n\t *\t\tat org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:931)\n\t *\t\tat org.apache.flink.client.CliFrontend.main(CliFrontend.java:951)\n\t *\tCaused by: java.io.IOException: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.ql.io.RCFileInputFormat\n\t *\t\tat org.apache.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:102)\n\t *\t\tat org.apache.hcatalog.mapreduce.HCatInputFormat.setInput(HCatInputFormat.java:54)\n\t *\t\tat tlabs.CDR_In_Report.createHCatInputFormat(CDR_In_Report.java:322)\n\t *\t\tat tlabs.CDR_Out_Report.main(CDR_Out_Report.java:380)\n\t *\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t *\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\t *\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t *\t\tat java.lang.reflect.Method.invoke(Method.java:622)\n\t *\t\tat org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:383)\n\t * </pre>\n\t *\n\t * <p>The test works as follows:\n\t *\n\t * <ul>\n\t *   <li> Use the CliFrontend to invoke a jar file that loads a class which is only available\n\t * \t      in the jarfile itself (via a custom classloader)\n\t *   <li> Change the Usercode classloader of the PackagedProgram to a special classloader for this test\n\t *   <li> the classloader will accept the special class (and return a String.class)\n\t * </ul>\n\t */\n\t@Test\n\tpublic void testPlanWithExternalClass() throws CompilerException, ProgramInvocationException {\n\t\tfinal boolean[] callme = { false }; // create a final object reference, to be able to change its val later\n\n\t\ttry {\n\t\t\tString[] arguments = {\n\t\t\t\t\t\"--classpath\", \"file:///tmp/foo\",\n\t\t\t\t\t\"--classpath\", \"file:///tmp/bar\",\n\t\t\t\t\t\"-c\", TEST_JAR_CLASSLOADERTEST_CLASS, getTestJarPath(),\n\t\t\t\t\t\"true\", \"arg1\", \"arg2\" };\n\t\t\tURL[] classpath = new URL[] { new URL(\"file:///tmp/foo\"), new URL(\"file:///tmp/bar\") };\n\t\t\tString[] reducedArguments = { \"true\", \"arg1\", \"arg2\" };\n\n\t\t\tRunOptions options = CliFrontendParser.parseRunCommand(arguments);\n\t\t\tassertEquals(getTestJarPath(), options.getJarFilePath());\n\t\t\tassertArrayEquals(classpath, options.getClasspaths().toArray());\n\t\t\tassertEquals(TEST_JAR_CLASSLOADERTEST_CLASS, options.getEntryPointClassName());\n\t\t\tassertArrayEquals(reducedArguments, options.getProgramArgs());\n\n\t\t\tCliFrontend frontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\tPackagedProgram prog = spy(frontend.buildProgram(options));\n\n\t\t\tClassLoader testClassLoader = new ClassLoader(prog.getUserCodeClassLoader()) {\n\t\t\t\t@Override\n\t\t\t\tpublic Class<?> loadClass(String name) throws ClassNotFoundException {\n\t\t\t\t\tif (\"org.apache.hadoop.hive.ql.io.RCFileInputFormat\".equals(name)) {\n\t\t\t\t\t\tcallme[0] = true;\n\t\t\t\t\t\treturn String.class; // Intentionally return the wrong class.\n\t\t\t\t\t} else {\n\t\t\t\t\t\treturn super.loadClass(name);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\twhen(prog.getUserCodeClassLoader()).thenReturn(testClassLoader);\n\n\t\t\tassertEquals(TEST_JAR_CLASSLOADERTEST_CLASS, prog.getMainClassName());\n\t\t\tassertArrayEquals(reducedArguments, prog.getArguments());\n\n\t\t\tConfiguration c = new Configuration();\n\t\t\tOptimizer compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), c);\n\n\t\t\t// we expect this to fail with a \"ClassNotFoundException\"\n\t\t\tClusterClient.getOptimizedPlanAsJson(compiler, prog, 666);\n\t\t\tfail(\"Should have failed with a ClassNotFoundException\");\n\t\t}\n\t\tcatch (ProgramInvocationException e) {\n\t\t\tif (!(e.getCause() instanceof ClassNotFoundException)) {\n\t\t\t\te.printStackTrace();\n\t\t\t\tfail(\"Program didn't throw ClassNotFoundException\");\n\t\t\t}\n\t\t\tassertTrue(\"Classloader was not called\", callme[0]);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Program failed with the wrong exception: \" + e.getClass().getName());\n\t\t}\n\t}"
        ],
        [
            "ContextEnvironmentFactory::ContextEnvironmentFactory(ClusterClient,List,List,ClassLoader,int,boolean,SavepointRestoreSettings)",
            "  52  \n  53  \n  54 -\n  55 -\n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  ",
            "\tpublic ContextEnvironmentFactory(ClusterClient client, List<URL> jarFilesToAttach,\n\t\t\tList<URL> classpathsToAttach, ClassLoader userCodeClassLoader, int defaultParallelism,\n\t\t\tboolean isDetached, SavepointRestoreSettings savepointSettings)\n\t{\n\t\tthis.client = client;\n\t\tthis.jarFilesToAttach = jarFilesToAttach;\n\t\tthis.classpathsToAttach = classpathsToAttach;\n\t\tthis.userCodeClassLoader = userCodeClassLoader;\n\t\tthis.defaultParallelism = defaultParallelism;\n\t\tthis.isDetached = isDetached;\n\t\tthis.savepointSettings = savepointSettings;\n\t}",
            "  52  \n  53  \n  54 +\n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  ",
            "\tpublic ContextEnvironmentFactory(ClusterClient client, List<URL> jarFilesToAttach,\n\t\t\tList<URL> classpathsToAttach, ClassLoader userCodeClassLoader, int defaultParallelism,\n\t\t\tboolean isDetached, SavepointRestoreSettings savepointSettings) {\n\t\tthis.client = client;\n\t\tthis.jarFilesToAttach = jarFilesToAttach;\n\t\tthis.classpathsToAttach = classpathsToAttach;\n\t\tthis.userCodeClassLoader = userCodeClassLoader;\n\t\tthis.defaultParallelism = defaultParallelism;\n\t\tthis.isDetached = isDetached;\n\t\tthis.savepointSettings = savepointSettings;\n\t}"
        ],
        [
            "ClientTest::TestExecuteTwice::main(String)",
            " 388 -\n 389  \n 390  \n 391  \n 392  \n 393  ",
            "\t\tpublic static void main(String args[]) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute();\n\t\t\tenv.fromElements(1, 2).collect();\n\t\t}",
            " 395 +\n 396  \n 397  \n 398  \n 399  \n 400  ",
            "\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute();\n\t\t\tenv.fromElements(1, 2).collect();\n\t\t}"
        ],
        [
            "ClientTest::TestGetRuntime::main(String)",
            " 406 -\n 407  \n 408  \n 409  \n 410  ",
            "\t\tpublic static void main(String args[]) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute().getNetRuntime();\n\t\t}",
            " 419 +\n 420  \n 421  \n 422  \n 423  ",
            "\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute().getNetRuntime();\n\t\t}"
        ],
        [
            "WordCount::getTextDataSet(ExecutionEnvironment)",
            " 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  ",
            "\tprivate static DataSet<String> getTextDataSet(ExecutionEnvironment env) {\n\t\tif(fileOutput) {\n\t\t\t// read the text file from given input path\n\t\t\treturn env.readTextFile(textPath);\n\t\t} else {\n\t\t\t// get default test text data\n\t\t\treturn env.fromElements(\n\t\t\t\t\t\"To be, or not to be,--that is the question:--\",\n\t\t\t\t\t\"Whether 'tis nobler in the mind to suffer\",\n\t\t\t\t\t\"The slings and arrows of outrageous fortune\",\n\t\t\t\t\t\"Or to take arms against a sea of troubles,\",\n\t\t\t\t\t\"And by opposing end them?--To die,--to sleep,--\",\n\t\t\t\t\t\"No more; and by a sleep to say we end\",\n\t\t\t\t\t\"The heartache, and the thousand natural shocks\",\n\t\t\t\t\t\"That flesh is heir to,--'tis a consummation\",\n\t\t\t\t\t\"Devoutly to be wish'd. To die,--to sleep;--\",\n\t\t\t\t\t\"To sleep! perchance to dream:--ay, there's the rub;\",\n\t\t\t\t\t\"For in that sleep of death what dreams may come,\",\n\t\t\t\t\t\"When we have shuffled off this mortal coil,\",\n\t\t\t\t\t\"Must give us pause: there's the respect\",\n\t\t\t\t\t\"That makes calamity of so long life;\",\n\t\t\t\t\t\"For who would bear the whips and scorns of time,\",\n\t\t\t\t\t\"The oppressor's wrong, the proud man's contumely,\",\n\t\t\t\t\t\"The pangs of despis'd love, the law's delay,\",\n\t\t\t\t\t\"The insolence of office, and the spurns\",\n\t\t\t\t\t\"That patient merit of the unworthy takes,\",\n\t\t\t\t\t\"When he himself might his quietus make\",\n\t\t\t\t\t\"With a bare bodkin? who would these fardels bear,\",\n\t\t\t\t\t\"To grunt and sweat under a weary life,\",\n\t\t\t\t\t\"But that the dread of something after death,--\",\n\t\t\t\t\t\"The undiscover'd country, from whose bourn\",\n\t\t\t\t\t\"No traveller returns,--puzzles the will,\",\n\t\t\t\t\t\"And makes us rather bear those ills we have\",\n\t\t\t\t\t\"Than fly to others that we know not of?\",\n\t\t\t\t\t\"Thus conscience does make cowards of us all;\",\n\t\t\t\t\t\"And thus the native hue of resolution\",\n\t\t\t\t\t\"Is sicklied o'er with the pale cast of thought;\",\n\t\t\t\t\t\"And enterprises of great pith and moment,\",\n\t\t\t\t\t\"With this regard, their currents turn awry,\",\n\t\t\t\t\t\"And lose the name of action.--Soft you now!\",\n\t\t\t\t\t\"The fair Ophelia!--Nymph, in thy orisons\",\n\t\t\t\t\t\"Be all my sins remember'd.\"",
            " 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "\tprivate static DataSet<String> getTextDataSet(ExecutionEnvironment env) {\n\t\tif (fileOutput) {\n\t\t\t// read the text file from given input path\n\t\t\treturn env.readTextFile(textPath);\n\t\t} else {\n\t\t\t// get default test text data\n\t\t\treturn env.fromElements(\n\t\t\t\t\t\"To be, or not to be,--that is the question:--\",\n\t\t\t\t\t\"Whether 'tis nobler in the mind to suffer\",\n\t\t\t\t\t\"The slings and arrows of outrageous fortune\",\n\t\t\t\t\t\"Or to take arms against a sea of troubles,\",\n\t\t\t\t\t\"And by opposing end them?--To die,--to sleep,--\",\n\t\t\t\t\t\"No more; and by a sleep to say we end\",\n\t\t\t\t\t\"The heartache, and the thousand natural shocks\",\n\t\t\t\t\t\"That flesh is heir to,--'tis a consummation\",\n\t\t\t\t\t\"Devoutly to be wish'd. To die,--to sleep;--\",\n\t\t\t\t\t\"To sleep! perchance to dream:--ay, there's the rub;\",\n\t\t\t\t\t\"For in that sleep of death what dreams may come,\",\n\t\t\t\t\t\"When we have shuffled off this mortal coil,\",\n\t\t\t\t\t\"Must give us pause: there's the respect\",\n\t\t\t\t\t\"That makes calamity of so long life;\",\n\t\t\t\t\t\"For who would bear the whips and scorns of time,\",\n\t\t\t\t\t\"The oppressor's wrong, the proud man's contumely,\",\n\t\t\t\t\t\"The pangs of despis'd love, the law's delay,\",\n\t\t\t\t\t\"The insolence of office, and the spurns\",\n\t\t\t\t\t\"That patient merit of the unworthy takes,\",\n\t\t\t\t\t\"When he himself might his quietus make\",\n\t\t\t\t\t\"With a bare bodkin? who would these fardels bear,\",\n\t\t\t\t\t\"To grunt and sweat under a weary life,\",\n\t\t\t\t\t\"But that the dread of something after death,--\",\n\t\t\t\t\t\"The undiscover'd country, from whose bourn\",\n\t\t\t\t\t\"No traveller returns,--puzzles the will,\",\n\t\t\t\t\t\"And makes us rather bear those ills we have\",\n\t\t\t\t\t\"Than fly to others that we know not of?\",\n\t\t\t\t\t\"Thus conscience does make cowards of us all;\",\n\t\t\t\t\t\"And thus the native hue of resolution\",\n\t\t\t\t\t\"Is sicklied o'er with the pale cast of thought;\",\n\t\t\t\t\t\"And enterprises of great pith and moment,\",\n\t\t\t\t\t\"With this regard, their currents turn awry,\",\n\t\t\t\t\t\"And lose the name of action.--Soft you now!\",\n\t\t\t\t\t\"The fair Ophelia!--Nymph, in thy orisons\",\n\t\t\t\t\t\"Be all my sins remember'd.\""
        ],
        [
            "CliFrontend::list(String)",
            " 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440 -\n 441  \n 442  \n 443  \n 444  \n 445 -\n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467 -\n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  ",
            "\t/**\n\t * Executes the list action.\n\t * \n\t * @param args Command line arguments for the list action.\n\t */\n\tprotected int list(String[] args) {\n\t\tLOG.info(\"Running 'list' command.\");\n\n\t\tListOptions options;\n\t\ttry {\n\t\t\toptions = CliFrontendParser.parseListCommand(args);\n\t\t}\n\t\tcatch (CliArgsException e) {\n\t\t\treturn handleArgException(e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\t// evaluate help flag\n\t\tif (options.isPrintHelp()) {\n\t\t\tCliFrontendParser.printHelpForList();\n\t\t\treturn 0;\n\t\t}\n\n\t\tboolean running = options.getRunning();\n\t\tboolean scheduled = options.getScheduled();\n\n\t\t// print running and scheduled jobs if not option supplied\n\t\tif (!running && !scheduled) {\n\t\t\trunning = true;\n\t\t\tscheduled = true;\n\t\t}\n\n\t\ttry {\n\t\t\tActorGateway jobManagerGateway = getJobManagerGateway(options);\n\n\t\t\tLOG.info(\"Connecting to JobManager to retrieve list of jobs\");\n\t\t\tFuture<Object> response = jobManagerGateway.ask(\n\t\t\t\tJobManagerMessages.getRequestRunningJobsStatus(),\n\t\t\t\tclientTimeout);\n\n\t\t\tObject result;\n\t\t\ttry {\n\t\t\t\tresult = Await.result(response, clientTimeout);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\tthrow new Exception(\"Could not retrieve running jobs from the JobManager.\", e);\n\t\t\t}\n\n\t\t\tif (result instanceof RunningJobsStatus) {\n\t\t\t\tLOG.info(\"Successfully retrieved list of jobs\");\n\n\t\t\t\tList<JobStatusMessage> jobs = ((RunningJobsStatus) result).getStatusMessages();\n\n\t\t\t\tArrayList<JobStatusMessage> runningJobs = null;\n\t\t\t\tArrayList<JobStatusMessage> scheduledJobs = null;\n\t\t\t\tif (running) {\n\t\t\t\t\trunningJobs = new ArrayList<JobStatusMessage>();\n\t\t\t\t}\n\t\t\t\tif (scheduled) {\n\t\t\t\t\tscheduledJobs = new ArrayList<JobStatusMessage>();\n\t\t\t\t}\n\n\t\t\t\tfor (JobStatusMessage rj : jobs) {\n\t\t\t\t\tif (running && (rj.getJobState().equals(JobStatus.RUNNING)\n\t\t\t\t\t\t\t|| rj.getJobState().equals(JobStatus.RESTARTING))) {\n\t\t\t\t\t\trunningJobs.add(rj);\n\t\t\t\t\t}\n\t\t\t\t\tif (scheduled && rj.getJobState().equals(JobStatus.CREATED)) {\n\t\t\t\t\t\tscheduledJobs.add(rj);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tSimpleDateFormat df = new SimpleDateFormat(\"dd.MM.yyyy HH:mm:ss\");\n\t\t\t\tComparator<JobStatusMessage> njec = new Comparator<JobStatusMessage>(){\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic int compare(JobStatusMessage o1, JobStatusMessage o2) {\n\t\t\t\t\t\treturn (int)(o1.getStartTime()-o2.getStartTime());\n\t\t\t\t\t}\n\t\t\t\t};\n\n\t\t\t\tif (running) {\n\t\t\t\t\tif(runningJobs.size() == 0) {\n\t\t\t\t\t\tSystem.out.println(\"No running jobs.\");\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tCollections.sort(runningJobs, njec);\n\n\t\t\t\t\t\tSystem.out.println(\"------------------ Running/Restarting Jobs -------------------\");\n\t\t\t\t\t\tfor (JobStatusMessage rj : runningJobs) {\n\t\t\t\t\t\t\tSystem.out.println(df.format(new Date(rj.getStartTime()))\n\t\t\t\t\t\t\t\t\t+ \" : \" + rj.getJobId() + \" : \" + rj.getJobName() + \" (\" + rj.getJobState() + \")\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\tSystem.out.println(\"--------------------------------------------------------------\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (scheduled) {\n\t\t\t\t\tif (scheduledJobs.size() == 0) {\n\t\t\t\t\t\tSystem.out.println(\"No scheduled jobs.\");\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tCollections.sort(scheduledJobs, njec);\n\n\t\t\t\t\t\tSystem.out.println(\"----------------------- Scheduled Jobs -----------------------\");\n\t\t\t\t\t\tfor(JobStatusMessage rj : scheduledJobs) {\n\t\t\t\t\t\t\tSystem.out.println(df.format(new Date(rj.getStartTime()))\n\t\t\t\t\t\t\t\t\t+ \" : \" + rj.getJobId() + \" : \" + rj.getJobName());\n\t\t\t\t\t\t}\n\t\t\t\t\t\tSystem.out.println(\"--------------------------------------------------------------\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new Exception(\"ReqeustRunningJobs requires a response of type \" +\n\t\t\t\t\t\t\"RunningJobs. Instead the response is of type \" + result.getClass() + \".\");\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\t}",
            " 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438 +\n 439  \n 440  \n 441  \n 442  \n 443 +\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465 +\n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  ",
            "\t/**\n\t * Executes the list action.\n\t *\n\t * @param args Command line arguments for the list action.\n\t */\n\tprotected int list(String[] args) {\n\t\tLOG.info(\"Running 'list' command.\");\n\n\t\tListOptions options;\n\t\ttry {\n\t\t\toptions = CliFrontendParser.parseListCommand(args);\n\t\t}\n\t\tcatch (CliArgsException e) {\n\t\t\treturn handleArgException(e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\t// evaluate help flag\n\t\tif (options.isPrintHelp()) {\n\t\t\tCliFrontendParser.printHelpForList();\n\t\t\treturn 0;\n\t\t}\n\n\t\tboolean running = options.getRunning();\n\t\tboolean scheduled = options.getScheduled();\n\n\t\t// print running and scheduled jobs if not option supplied\n\t\tif (!running && !scheduled) {\n\t\t\trunning = true;\n\t\t\tscheduled = true;\n\t\t}\n\n\t\ttry {\n\t\t\tActorGateway jobManagerGateway = getJobManagerGateway(options);\n\n\t\t\tLOG.info(\"Connecting to JobManager to retrieve list of jobs\");\n\t\t\tFuture<Object> response = jobManagerGateway.ask(\n\t\t\t\tJobManagerMessages.getRequestRunningJobsStatus(),\n\t\t\t\tclientTimeout);\n\n\t\t\tObject result;\n\t\t\ttry {\n\t\t\t\tresult = Await.result(response, clientTimeout);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\tthrow new Exception(\"Could not retrieve running jobs from the JobManager.\", e);\n\t\t\t}\n\n\t\t\tif (result instanceof RunningJobsStatus) {\n\t\t\t\tLOG.info(\"Successfully retrieved list of jobs\");\n\n\t\t\t\tList<JobStatusMessage> jobs = ((RunningJobsStatus) result).getStatusMessages();\n\n\t\t\t\tArrayList<JobStatusMessage> runningJobs = null;\n\t\t\t\tArrayList<JobStatusMessage> scheduledJobs = null;\n\t\t\t\tif (running) {\n\t\t\t\t\trunningJobs = new ArrayList<JobStatusMessage>();\n\t\t\t\t}\n\t\t\t\tif (scheduled) {\n\t\t\t\t\tscheduledJobs = new ArrayList<JobStatusMessage>();\n\t\t\t\t}\n\n\t\t\t\tfor (JobStatusMessage rj : jobs) {\n\t\t\t\t\tif (running && (rj.getJobState().equals(JobStatus.RUNNING)\n\t\t\t\t\t\t\t|| rj.getJobState().equals(JobStatus.RESTARTING))) {\n\t\t\t\t\t\trunningJobs.add(rj);\n\t\t\t\t\t}\n\t\t\t\t\tif (scheduled && rj.getJobState().equals(JobStatus.CREATED)) {\n\t\t\t\t\t\tscheduledJobs.add(rj);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tSimpleDateFormat df = new SimpleDateFormat(\"dd.MM.yyyy HH:mm:ss\");\n\t\t\t\tComparator<JobStatusMessage> njec = new Comparator<JobStatusMessage>(){\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic int compare(JobStatusMessage o1, JobStatusMessage o2) {\n\t\t\t\t\t\treturn (int) (o1.getStartTime() - o2.getStartTime());\n\t\t\t\t\t}\n\t\t\t\t};\n\n\t\t\t\tif (running) {\n\t\t\t\t\tif (runningJobs.size() == 0) {\n\t\t\t\t\t\tSystem.out.println(\"No running jobs.\");\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tCollections.sort(runningJobs, njec);\n\n\t\t\t\t\t\tSystem.out.println(\"------------------ Running/Restarting Jobs -------------------\");\n\t\t\t\t\t\tfor (JobStatusMessage rj : runningJobs) {\n\t\t\t\t\t\t\tSystem.out.println(df.format(new Date(rj.getStartTime()))\n\t\t\t\t\t\t\t\t\t+ \" : \" + rj.getJobId() + \" : \" + rj.getJobName() + \" (\" + rj.getJobState() + \")\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\tSystem.out.println(\"--------------------------------------------------------------\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (scheduled) {\n\t\t\t\t\tif (scheduledJobs.size() == 0) {\n\t\t\t\t\t\tSystem.out.println(\"No scheduled jobs.\");\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tCollections.sort(scheduledJobs, njec);\n\n\t\t\t\t\t\tSystem.out.println(\"----------------------- Scheduled Jobs -----------------------\");\n\t\t\t\t\t\tfor (JobStatusMessage rj : scheduledJobs) {\n\t\t\t\t\t\t\tSystem.out.println(df.format(new Date(rj.getStartTime()))\n\t\t\t\t\t\t\t\t\t+ \" : \" + rj.getJobId() + \" : \" + rj.getJobName());\n\t\t\t\t\t\t}\n\t\t\t\t\t\tSystem.out.println(\"--------------------------------------------------------------\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn 0;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new Exception(\"ReqeustRunningJobs requires a response of type \" +\n\t\t\t\t\t\t\"RunningJobs. Instead the response is of type \" + result.getClass() + \".\");\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\t}"
        ],
        [
            "ClusterClient::ClusterClient(Configuration,HighAvailabilityServices)",
            " 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 -\n 157  \n 158  \n 159  ",
            "\t/**\n\t * Creates a instance that submits the programs to the JobManager defined in the\n\t * configuration. This method will try to resolve the JobManager hostname and throw an exception\n\t * if that is not possible.\n\t *\n\t * @param flinkConfig The config used to obtain the job-manager's address, and used to configure the optimizer.\n\t * @param highAvailabilityServices HighAvailabilityServices to use for leader retrieval\n\t */\n\tpublic ClusterClient(Configuration flinkConfig, HighAvailabilityServices highAvailabilityServices) {\n\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n\t\tthis.compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), flinkConfig);\n\n\t\tthis.timeout = AkkaUtils.getClientTimeout(flinkConfig);\n\t\tthis.lookupTimeout = AkkaUtils.getLookupTimeout(flinkConfig);\n\n\t\tthis.actorSystemLoader = new LazyActorSystemLoader(\n\t\t\thighAvailabilityServices,\n\t\t\tTime.milliseconds(lookupTimeout.toMillis()),\n\t\t\tflinkConfig,\n\t\t\tLOG);\n\n\t\tthis.highAvailabilityServices = Preconditions.checkNotNull(highAvailabilityServices);\n\t}",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  ",
            "\t/**\n\t * Creates a instance that submits the programs to the JobManager defined in the\n\t * configuration. This method will try to resolve the JobManager hostname and throw an exception\n\t * if that is not possible.\n\t *\n\t * @param flinkConfig The config used to obtain the job-manager's address, and used to configure the optimizer.\n\t * @param highAvailabilityServices HighAvailabilityServices to use for leader retrieval\n\t */\n\tpublic ClusterClient(Configuration flinkConfig, HighAvailabilityServices highAvailabilityServices) {\n\t\tthis.flinkConfig = Preconditions.checkNotNull(flinkConfig);\n\t\tthis.compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), flinkConfig);\n\n\t\tthis.timeout = AkkaUtils.getClientTimeout(flinkConfig);\n\t\tthis.lookupTimeout = AkkaUtils.getLookupTimeout(flinkConfig);\n\n\t\tthis.actorSystemLoader = new LazyActorSystemLoader(\n\t\t\thighAvailabilityServices,\n\t\t\tTime.milliseconds(lookupTimeout.toMillis()),\n\t\t\tflinkConfig,\n\t\t\tlog);\n\n\t\tthis.highAvailabilityServices = Preconditions.checkNotNull(highAvailabilityServices);\n\t}"
        ],
        [
            "ClientTest::TestGetJobID::main(String)",
            " 415 -\n 416  \n 417  \n 418  \n 419  ",
            "\t\tpublic static void main(String args[]) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute().getJobID();\n\t\t}",
            " 431 +\n 432  \n 433  \n 434  \n 435  ",
            "\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).output(new DiscardingOutputFormat<Integer>());\n\t\t\tenv.execute().getJobID();\n\t\t}"
        ],
        [
            "PackagedProgram::PackagedProgram(File,List,String,String)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 -\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 -\n 182  \n 183 -\n 184  \n 185  \n 186 -\n 187  \n 188  \n 189  \n 190  \n 191 -\n 192  \n 193  \n 194  \n 195  \n 196 -\n 197  \n 198  \n 199 -\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209 -\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 -\n 221  \n 222  \n 223  ",
            "\t/**\n\t * Creates an instance that wraps the plan defined in the jar file using the given\n\t * arguments. For generating the plan the class defined in the className parameter\n\t * is used.\n\t * \n\t * @param jarFile\n\t *        The jar file which contains the plan.\n\t * @param classpaths\n\t *        Additional classpath URLs needed by the Program.\n\t * @param entryPointClassName\n\t *        Name of the class which generates the plan. Overrides the class defined\n\t *        in the jar file manifest\n\t * @param args\n\t *        Optional. The arguments used to create the pact plan, depend on\n\t *        implementation of the pact plan. See getDescription().\n\t * @throws ProgramInvocationException\n\t *         This invocation is thrown if the Program can't be properly loaded. Causes\n\t *         may be a missing / wrong class or manifest files.\n\t */\n\tpublic PackagedProgram(File jarFile, List<URL> classpaths, String entryPointClassName, String... args) throws ProgramInvocationException {\n\t\tif (jarFile == null) {\n\t\t\tthrow new IllegalArgumentException(\"The jar file must not be null.\");\n\t\t}\n\t\t\n\t\tURL jarFileUrl;\n\t\ttry {\n\t\t\tjarFileUrl = jarFile.getAbsoluteFile().toURI().toURL();\n\t\t} catch (MalformedURLException e1) {\n\t\t\tthrow new IllegalArgumentException(\"The jar file path is invalid.\");\n\t\t}\n\t\t\n\t\tcheckJarFile(jarFileUrl);\n\t\t\n\t\tthis.jarFile = jarFileUrl;\n\t\tthis.args = args == null ? new String[0] : args;\n\t\t\n\t\t// if no entryPointClassName name was given, we try and look one up through the manifest\n\t\tif (entryPointClassName == null) {\n\t\t\tentryPointClassName = getEntryPointClassNameFromJar(jarFileUrl);\n\t\t}\n\t\t\n\t\t// now that we have an entry point, we can extract the nested jar files (if any)\n\t\tthis.extractedTempLibraries = extractContainedLibraries(jarFileUrl);\n\t\tthis.classpaths = classpaths;\n\t\tthis.userCodeClassLoader = JobWithJars.buildUserCodeClassLoader(getAllLibraries(), classpaths, getClass().getClassLoader());\n\t\t\n\t\t// load the entry point class\n\t\tthis.mainClass = loadMainClass(entryPointClassName, userCodeClassLoader);\n\t\t\n\t\t// if the entry point is a program, instantiate the class and get the plan\n\t\tif (Program.class.isAssignableFrom(this.mainClass)) {\n\t\t\tProgram prg = null;\n\t\t\ttry {\n\t\t\t\tprg = InstantiationUtil.instantiate(this.mainClass.asSubclass(Program.class), Program.class);\n\t\t\t} catch (Exception e) {\n\t\t\t\t// validate that the class has a main method at least.\n\t\t\t\t// the main method possibly instantiates the program properly\n\t\t\t\tif (!hasMainMethod(mainClass)) {\n\t\t\t\t\tthrow new ProgramInvocationException(\"The given program class implements the \" + \n\t\t\t\t\t\t\tProgram.class.getName() + \" interface, but cannot be instantiated. \" +\n\t\t\t\t\t\t\t\"It also declares no main(String[]) method as alternative entry point\", e);\n\t\t\t\t}\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new ProgramInvocationException(\"Error while trying to instantiate program class.\", t);\n\t\t\t}\n\t\t\tthis.program = prg;\n\t\t} else if (hasMainMethod(mainClass)) {\n\t\t\tthis.program = null;\n\t\t} else {\n\t\t\tthrow new ProgramInvocationException(\"The given program class neither has a main(String[]) method, nor does it implement the \" + \n\t\t\t\t\tProgram.class.getName() + \" interface.\");\n\t\t}\n\t}",
            " 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 +\n 181  \n 182 +\n 183  \n 184  \n 185 +\n 186  \n 187  \n 188  \n 189  \n 190 +\n 191  \n 192  \n 193  \n 194  \n 195 +\n 196  \n 197  \n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 +\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220  \n 221  \n 222  ",
            "\t/**\n\t * Creates an instance that wraps the plan defined in the jar file using the given\n\t * arguments. For generating the plan the class defined in the className parameter\n\t * is used.\n\t *\n\t * @param jarFile\n\t *        The jar file which contains the plan.\n\t * @param classpaths\n\t *        Additional classpath URLs needed by the Program.\n\t * @param entryPointClassName\n\t *        Name of the class which generates the plan. Overrides the class defined\n\t *        in the jar file manifest\n\t * @param args\n\t *        Optional. The arguments used to create the pact plan, depend on\n\t *        implementation of the pact plan. See getDescription().\n\t * @throws ProgramInvocationException\n\t *         This invocation is thrown if the Program can't be properly loaded. Causes\n\t *         may be a missing / wrong class or manifest files.\n\t */\n\tpublic PackagedProgram(File jarFile, List<URL> classpaths, String entryPointClassName, String... args) throws ProgramInvocationException {\n\t\tif (jarFile == null) {\n\t\t\tthrow new IllegalArgumentException(\"The jar file must not be null.\");\n\t\t}\n\n\t\tURL jarFileUrl;\n\t\ttry {\n\t\t\tjarFileUrl = jarFile.getAbsoluteFile().toURI().toURL();\n\t\t} catch (MalformedURLException e1) {\n\t\t\tthrow new IllegalArgumentException(\"The jar file path is invalid.\");\n\t\t}\n\n\t\tcheckJarFile(jarFileUrl);\n\n\t\tthis.jarFile = jarFileUrl;\n\t\tthis.args = args == null ? new String[0] : args;\n\n\t\t// if no entryPointClassName name was given, we try and look one up through the manifest\n\t\tif (entryPointClassName == null) {\n\t\t\tentryPointClassName = getEntryPointClassNameFromJar(jarFileUrl);\n\t\t}\n\n\t\t// now that we have an entry point, we can extract the nested jar files (if any)\n\t\tthis.extractedTempLibraries = extractContainedLibraries(jarFileUrl);\n\t\tthis.classpaths = classpaths;\n\t\tthis.userCodeClassLoader = JobWithJars.buildUserCodeClassLoader(getAllLibraries(), classpaths, getClass().getClassLoader());\n\n\t\t// load the entry point class\n\t\tthis.mainClass = loadMainClass(entryPointClassName, userCodeClassLoader);\n\n\t\t// if the entry point is a program, instantiate the class and get the plan\n\t\tif (Program.class.isAssignableFrom(this.mainClass)) {\n\t\t\tProgram prg = null;\n\t\t\ttry {\n\t\t\t\tprg = InstantiationUtil.instantiate(this.mainClass.asSubclass(Program.class), Program.class);\n\t\t\t} catch (Exception e) {\n\t\t\t\t// validate that the class has a main method at least.\n\t\t\t\t// the main method possibly instantiates the program properly\n\t\t\t\tif (!hasMainMethod(mainClass)) {\n\t\t\t\t\tthrow new ProgramInvocationException(\"The given program class implements the \" +\n\t\t\t\t\t\t\tProgram.class.getName() + \" interface, but cannot be instantiated. \" +\n\t\t\t\t\t\t\t\"It also declares no main(String[]) method as alternative entry point\", e);\n\t\t\t\t}\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new ProgramInvocationException(\"Error while trying to instantiate program class.\", t);\n\t\t\t}\n\t\t\tthis.program = prg;\n\t\t} else if (hasMainMethod(mainClass)) {\n\t\t\tthis.program = null;\n\t\t} else {\n\t\t\tthrow new ProgramInvocationException(\"The given program class neither has a main(String[]) method, nor does it implement the \" +\n\t\t\t\t\tProgram.class.getName() + \" interface.\");\n\t\t}\n\t}"
        ],
        [
            "ClusterClient::run(PackagedProgram,int)",
            " 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350 -\n 351 -\n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365 -\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  ",
            "\t/**\n\t * General purpose method to run a user jar from the CliFrontend in either blocking or detached mode, depending\n\t * on whether {@code setDetached(true)} or {@code setDetached(false)}.\n\t * @param prog the packaged program\n\t * @param parallelism the parallelism to execute the contained Flink job\n\t * @return The result of the execution\n\t * @throws ProgramMissingJobException\n\t * @throws ProgramInvocationException\n\t */\n\tpublic JobSubmissionResult run(PackagedProgram prog, int parallelism)\n\t\t\tthrows ProgramInvocationException, ProgramMissingJobException\n\t{\n\t\tThread.currentThread().setContextClassLoader(prog.getUserCodeClassLoader());\n\t\tif (prog.isUsingProgramEntryPoint()) {\n\n\t\t\tfinal JobWithJars jobWithJars;\n\t\t\tif (hasUserJarsInClassPath(prog.getAllLibraries())) {\n\t\t\t\tjobWithJars = prog.getPlanWithoutJars();\n\t\t\t} else {\n\t\t\t\tjobWithJars = prog.getPlanWithJars();\n\t\t\t}\n\n\t\t\treturn run(jobWithJars, parallelism, prog.getSavepointSettings());\n\t\t}\n\t\telse if (prog.isUsingInteractiveMode()) {\n\t\t\tLOG.info(\"Starting program in interactive mode\");\n\n\t\t\tfinal List<URL> libraries;\n\t\t\tif (hasUserJarsInClassPath(prog.getAllLibraries())) {\n\t\t\t\tlibraries = Collections.emptyList();\n\t\t\t} else {\n\t\t\t\tlibraries = prog.getAllLibraries();\n\t\t\t}\n\n\t\t\tContextEnvironmentFactory factory = new ContextEnvironmentFactory(this, libraries,\n\t\t\t\t\tprog.getClasspaths(), prog.getUserCodeClassLoader(), parallelism, isDetached(),\n\t\t\t\t\tprog.getSavepointSettings());\n\t\t\tContextEnvironment.setAsContext(factory);\n\n\t\t\ttry {\n\t\t\t\t// invoke main method\n\t\t\t\tprog.invokeInteractiveModeForExecution();\n\t\t\t\tif (lastJobExecutionResult == null && factory.getLastEnvCreated() == null) {\n\t\t\t\t\tthrow new ProgramMissingJobException();\n\t\t\t\t}\n\t\t\t\tif (isDetached()) {\n\t\t\t\t\t// in detached mode, we execute the whole user code to extract the Flink job, afterwards we run it here\n\t\t\t\t\treturn ((DetachedEnvironment) factory.getLastEnvCreated()).finalizeExecute();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// in blocking mode, we execute all Flink jobs contained in the user code and then return here\n\t\t\t\t\treturn this.lastJobExecutionResult;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\tContextEnvironment.unsetContext();\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tthrow new ProgramInvocationException(\"PackagedProgram does not have a valid invocation mode.\");\n\t\t}\n\t}",
            " 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353 +\n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367 +\n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  ",
            "\t/**\n\t * General purpose method to run a user jar from the CliFrontend in either blocking or detached mode, depending\n\t * on whether {@code setDetached(true)} or {@code setDetached(false)}.\n\t * @param prog the packaged program\n\t * @param parallelism the parallelism to execute the contained Flink job\n\t * @return The result of the execution\n\t * @throws ProgramMissingJobException\n\t * @throws ProgramInvocationException\n\t */\n\tpublic JobSubmissionResult run(PackagedProgram prog, int parallelism)\n\t\t\tthrows ProgramInvocationException, ProgramMissingJobException {\n\t\tThread.currentThread().setContextClassLoader(prog.getUserCodeClassLoader());\n\t\tif (prog.isUsingProgramEntryPoint()) {\n\n\t\t\tfinal JobWithJars jobWithJars;\n\t\t\tif (hasUserJarsInClassPath(prog.getAllLibraries())) {\n\t\t\t\tjobWithJars = prog.getPlanWithoutJars();\n\t\t\t} else {\n\t\t\t\tjobWithJars = prog.getPlanWithJars();\n\t\t\t}\n\n\t\t\treturn run(jobWithJars, parallelism, prog.getSavepointSettings());\n\t\t}\n\t\telse if (prog.isUsingInteractiveMode()) {\n\t\t\tlog.info(\"Starting program in interactive mode\");\n\n\t\t\tfinal List<URL> libraries;\n\t\t\tif (hasUserJarsInClassPath(prog.getAllLibraries())) {\n\t\t\t\tlibraries = Collections.emptyList();\n\t\t\t} else {\n\t\t\t\tlibraries = prog.getAllLibraries();\n\t\t\t}\n\n\t\t\tContextEnvironmentFactory factory = new ContextEnvironmentFactory(this, libraries,\n\t\t\t\t\tprog.getClasspaths(), prog.getUserCodeClassLoader(), parallelism, isDetached(),\n\t\t\t\t\tprog.getSavepointSettings());\n\t\t\tContextEnvironment.setAsContext(factory);\n\n\t\t\ttry {\n\t\t\t\t// invoke main method\n\t\t\t\tprog.invokeInteractiveModeForExecution();\n\t\t\t\tif (lastJobExecutionResult == null && factory.getLastEnvCreated() == null) {\n\t\t\t\t\tthrow new ProgramMissingJobException();\n\t\t\t\t}\n\t\t\t\tif (isDetached()) {\n\t\t\t\t\t// in detached mode, we execute the whole user code to extract the Flink job, afterwards we run it here\n\t\t\t\t\treturn ((DetachedEnvironment) factory.getLastEnvCreated()).finalizeExecute();\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// in blocking mode, we execute all Flink jobs contained in the user code and then return here\n\t\t\t\t\treturn this.lastJobExecutionResult;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\tContextEnvironment.unsetContext();\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tthrow new ProgramInvocationException(\"PackagedProgram does not have a valid invocation mode.\");\n\t\t}\n\t}"
        ],
        [
            "CliFrontend::info(String)",
            " 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326 -\n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "\t/**\n\t * Executes the info action.\n\t * \n\t * @param args Command line arguments for the info action.\n\t */\n\tprotected int info(String[] args) {\n\t\tLOG.info(\"Running 'info' command.\");\n\n\t\t// Parse command line options\n\t\tInfoOptions options;\n\t\ttry {\n\t\t\toptions = CliFrontendParser.parseInfoCommand(args);\n\t\t}\n\t\tcatch (CliArgsException e) {\n\t\t\treturn handleArgException(e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\t// evaluate help flag\n\t\tif (options.isPrintHelp()) {\n\t\t\tCliFrontendParser.printHelpForInfo();\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (options.getJarFilePath() == null) {\n\t\t\treturn handleArgException(new CliArgsException(\"The program JAR file was not specified.\"));\n\t\t}\n\n\t\t// -------- build the packaged program -------------\n\n\t\tPackagedProgram program;\n\t\ttry {\n\t\t\tLOG.info(\"Building program from JAR file\");\n\t\t\tprogram = buildProgram(options);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\ttry {\n\t\t\tint parallelism = options.getParallelism();\n\n\t\t\tLOG.info(\"Creating program plan dump\");\n\n\t\t\tOptimizer compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), config);\n\t\t\tFlinkPlan flinkPlan = ClusterClient.getOptimizedPlan(compiler, program, parallelism);\n\t\t\t\n\t\t\tString jsonPlan = null;\n\t\t\tif (flinkPlan instanceof OptimizedPlan) {\n\t\t\t\tjsonPlan = new PlanJSONDumpGenerator().getOptimizerPlanAsJSON((OptimizedPlan) flinkPlan);\n\t\t\t} else if (flinkPlan instanceof StreamingPlan) {\n\t\t\t\tjsonPlan = ((StreamingPlan) flinkPlan).getStreamingPlanAsJSON();\n\t\t\t}\n\n\t\t\tif (jsonPlan != null) {\n\t\t\t\tSystem.out.println(\"----------------------- Execution Plan -----------------------\");\n\t\t\t\tSystem.out.println(jsonPlan);\n\t\t\t\tSystem.out.println(\"--------------------------------------------------------------\");\n\t\t\t}\n\t\t\telse {\n\t\t\t\tSystem.out.println(\"JSON plan could not be generated.\");\n\t\t\t}\n\n\t\t\tString description = program.getDescription();\n\t\t\tif (description != null) {\n\t\t\t\tSystem.out.println();\n\t\t\t\tSystem.out.println(description);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tSystem.out.println();\n\t\t\t\tSystem.out.println(\"No description provided.\");\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\t\tfinally {\n\t\t\tprogram.deleteExtractedLibraries();\n\t\t}\n\t}",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324 +\n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  ",
            "\t/**\n\t * Executes the info action.\n\t *\n\t * @param args Command line arguments for the info action.\n\t */\n\tprotected int info(String[] args) {\n\t\tLOG.info(\"Running 'info' command.\");\n\n\t\t// Parse command line options\n\t\tInfoOptions options;\n\t\ttry {\n\t\t\toptions = CliFrontendParser.parseInfoCommand(args);\n\t\t}\n\t\tcatch (CliArgsException e) {\n\t\t\treturn handleArgException(e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\t// evaluate help flag\n\t\tif (options.isPrintHelp()) {\n\t\t\tCliFrontendParser.printHelpForInfo();\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (options.getJarFilePath() == null) {\n\t\t\treturn handleArgException(new CliArgsException(\"The program JAR file was not specified.\"));\n\t\t}\n\n\t\t// -------- build the packaged program -------------\n\n\t\tPackagedProgram program;\n\t\ttry {\n\t\t\tLOG.info(\"Building program from JAR file\");\n\t\t\tprogram = buildProgram(options);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\ttry {\n\t\t\tint parallelism = options.getParallelism();\n\n\t\t\tLOG.info(\"Creating program plan dump\");\n\n\t\t\tOptimizer compiler = new Optimizer(new DataStatistics(), new DefaultCostEstimator(), config);\n\t\t\tFlinkPlan flinkPlan = ClusterClient.getOptimizedPlan(compiler, program, parallelism);\n\n\t\t\tString jsonPlan = null;\n\t\t\tif (flinkPlan instanceof OptimizedPlan) {\n\t\t\t\tjsonPlan = new PlanJSONDumpGenerator().getOptimizerPlanAsJSON((OptimizedPlan) flinkPlan);\n\t\t\t} else if (flinkPlan instanceof StreamingPlan) {\n\t\t\t\tjsonPlan = ((StreamingPlan) flinkPlan).getStreamingPlanAsJSON();\n\t\t\t}\n\n\t\t\tif (jsonPlan != null) {\n\t\t\t\tSystem.out.println(\"----------------------- Execution Plan -----------------------\");\n\t\t\t\tSystem.out.println(jsonPlan);\n\t\t\t\tSystem.out.println(\"--------------------------------------------------------------\");\n\t\t\t}\n\t\t\telse {\n\t\t\t\tSystem.out.println(\"JSON plan could not be generated.\");\n\t\t\t}\n\n\t\t\tString description = program.getDescription();\n\t\t\tif (description != null) {\n\t\t\t\tSystem.out.println();\n\t\t\t\tSystem.out.println(description);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tSystem.out.println();\n\t\t\t\tSystem.out.println(\"No description provided.\");\n\t\t\t}\n\t\t\treturn 0;\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\t\tfinally {\n\t\t\tprogram.deleteExtractedLibraries();\n\t\t}\n\t}"
        ],
        [
            "CliFrontend::run(String)",
            " 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 -\n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  ",
            "\t/**\n\t * Executions the run action.\n\t * \n\t * @param args Command line arguments for the run action.\n\t */\n\tprotected int run(String[] args) {\n\t\tLOG.info(\"Running 'run' command.\");\n\n\t\tRunOptions options;\n\t\ttry {\n\t\t\toptions = CliFrontendParser.parseRunCommand(args);\n\t\t}\n\t\tcatch (CliArgsException e) {\n\t\t\treturn handleArgException(e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\t// evaluate help flag\n\t\tif (options.isPrintHelp()) {\n\t\t\tCliFrontendParser.printHelpForRun();\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (options.getJarFilePath() == null) {\n\t\t\treturn handleArgException(new CliArgsException(\"The program JAR file was not specified.\"));\n\t\t}\n\n\t\tPackagedProgram program;\n\t\ttry {\n\t\t\tLOG.info(\"Building program from JAR file\");\n\t\t\tprogram = buildProgram(options);\n\t\t}\n\t\tcatch (FileNotFoundException e) {\n\t\t\treturn handleArgException(e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\tClusterClient client = null;\n\t\ttry {\n\n\t\t\tclient = createClient(options, program);\n\t\t\tclient.setPrintStatusDuringExecution(options.getStdoutLogging());\n\t\t\tclient.setDetached(options.getDetachedMode());\n\t\t\tLOG.debug(\"Client slots is set to {}\", client.getMaxSlots());\n\n\t\t\tLOG.debug(options.getSavepointRestoreSettings().toString());\n\n\t\t\tint userParallelism = options.getParallelism();\n\t\t\tLOG.debug(\"User parallelism is set to {}\", userParallelism);\n\t\t\tif (client.getMaxSlots() != -1 && userParallelism == -1) {\n\t\t\t\tlogAndSysout(\"Using the parallelism provided by the remote cluster (\"\n\t\t\t\t\t+ client.getMaxSlots()+\"). \"\n\t\t\t\t\t+ \"To use another parallelism, set it at the ./bin/flink client.\");\n\t\t\t\tuserParallelism = client.getMaxSlots();\n\t\t\t}\n\n\t\t\treturn executeProgram(program, client, userParallelism);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\t\tfinally {\n\t\t\tif (client != null) {\n\t\t\t\ttry {\n\t\t\t\t\tclient.shutdown();\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tLOG.warn(\"Could not properly shut down the cluster client.\", e);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (program != null) {\n\t\t\t\tprogram.deleteExtractedLibraries();\n\t\t\t}\n\t\t}\n\t}",
            " 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252 +\n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "\t/**\n\t * Executions the run action.\n\t *\n\t * @param args Command line arguments for the run action.\n\t */\n\tprotected int run(String[] args) {\n\t\tLOG.info(\"Running 'run' command.\");\n\n\t\tRunOptions options;\n\t\ttry {\n\t\t\toptions = CliFrontendParser.parseRunCommand(args);\n\t\t}\n\t\tcatch (CliArgsException e) {\n\t\t\treturn handleArgException(e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\t// evaluate help flag\n\t\tif (options.isPrintHelp()) {\n\t\t\tCliFrontendParser.printHelpForRun();\n\t\t\treturn 0;\n\t\t}\n\n\t\tif (options.getJarFilePath() == null) {\n\t\t\treturn handleArgException(new CliArgsException(\"The program JAR file was not specified.\"));\n\t\t}\n\n\t\tPackagedProgram program;\n\t\ttry {\n\t\t\tLOG.info(\"Building program from JAR file\");\n\t\t\tprogram = buildProgram(options);\n\t\t}\n\t\tcatch (FileNotFoundException e) {\n\t\t\treturn handleArgException(e);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\n\t\tClusterClient client = null;\n\t\ttry {\n\n\t\t\tclient = createClient(options, program);\n\t\t\tclient.setPrintStatusDuringExecution(options.getStdoutLogging());\n\t\t\tclient.setDetached(options.getDetachedMode());\n\t\t\tLOG.debug(\"Client slots is set to {}\", client.getMaxSlots());\n\n\t\t\tLOG.debug(options.getSavepointRestoreSettings().toString());\n\n\t\t\tint userParallelism = options.getParallelism();\n\t\t\tLOG.debug(\"User parallelism is set to {}\", userParallelism);\n\t\t\tif (client.getMaxSlots() != -1 && userParallelism == -1) {\n\t\t\t\tlogAndSysout(\"Using the parallelism provided by the remote cluster (\"\n\t\t\t\t\t+ client.getMaxSlots() + \"). \"\n\t\t\t\t\t+ \"To use another parallelism, set it at the ./bin/flink client.\");\n\t\t\t\tuserParallelism = client.getMaxSlots();\n\t\t\t}\n\n\t\t\treturn executeProgram(program, client, userParallelism);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\treturn handleError(t);\n\t\t}\n\t\tfinally {\n\t\t\tif (client != null) {\n\t\t\t\ttry {\n\t\t\t\t\tclient.shutdown();\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tLOG.warn(\"Could not properly shut down the cluster client.\", e);\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (program != null) {\n\t\t\t\tprogram.deleteExtractedLibraries();\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "PackagedProgram::getEntryPointClassNameFromJar(URL)",
            " 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577 -\n 578  \n 579  \n 580  \n 581 -\n 582  \n 583 -\n 584  \n 585  \n 586  \n 587  \n 588  \n 589 -\n 590 -\n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  ",
            "\tprivate static String getEntryPointClassNameFromJar(URL jarFile) throws ProgramInvocationException {\n\t\tJarFile jar;\n\t\tManifest manifest;\n\t\tString className;\n\n\t\t// Open jar file\n\t\ttry {\n\t\t\tjar = new JarFile(new File(jarFile.toURI()));\n\t\t} catch (URISyntaxException use) {\n\t\t\tthrow new ProgramInvocationException(\"Invalid file path '\" + jarFile.getPath() + \"'\", use);\n\t\t} catch (IOException ioex) {\n\t\t\tthrow new ProgramInvocationException(\"Error while opening jar file '\" + jarFile.getPath() + \"'. \"\n\t\t\t\t+ ioex.getMessage(), ioex);\n\t\t}\n\n\t\t// jar file must be closed at the end\n\t\ttry {\n\t\t\t// Read from jar manifest\n\t\t\ttry {\n\t\t\t\tmanifest = jar.getManifest();\n\t\t\t} catch (IOException ioex) {\n\t\t\t\tthrow new ProgramInvocationException(\"The Manifest in the jar file could not be accessed '\"\n\t\t\t\t\t+ jarFile.getPath() + \"'. \" + ioex.getMessage(), ioex);\n\t\t\t}\n\t\n\t\t\tif (manifest == null) {\n\t\t\t\tthrow new ProgramInvocationException(\"No manifest found in jar file '\" + jarFile.getPath() + \"'. The manifest is need to point to the program's main class.\");\n\t\t\t}\n\t\n\t\t\tAttributes attributes = manifest.getMainAttributes();\n\t\t\t\n\t\t\t// check for a \"program-class\" entry first\n\t\t\tclassName = attributes.getValue(PackagedProgram.MANIFEST_ATTRIBUTE_ASSEMBLER_CLASS);\n\t\t\tif (className != null) {\n\t\t\t\treturn className;\n\t\t\t}\n\t\t\t\n\t\t\t\n\t\t\t// check for a main class\n\t\t\tclassName = attributes.getValue(PackagedProgram.MANIFEST_ATTRIBUTE_MAIN_CLASS);\n\t\t\tif (className != null) {\n\t\t\t\treturn className;\n\t\t\t} else {\n\t\t\t\tthrow new ProgramInvocationException(\"Neither a '\" + MANIFEST_ATTRIBUTE_MAIN_CLASS + \"', nor a '\" +\n\t\t\t\t\t\tMANIFEST_ATTRIBUTE_ASSEMBLER_CLASS + \"' entry was found in the jar file.\");\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\ttry {\n\t\t\t\tjar.close();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new ProgramInvocationException(\"Could not close the JAR file: \" + t.getMessage(), t);\n\t\t\t}\n\t\t}\n\t}",
            " 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574 +\n 575  \n 576  \n 577  \n 578 +\n 579  \n 580 +\n 581  \n 582  \n 583  \n 584  \n 585  \n 586 +\n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  ",
            "\tprivate static String getEntryPointClassNameFromJar(URL jarFile) throws ProgramInvocationException {\n\t\tJarFile jar;\n\t\tManifest manifest;\n\t\tString className;\n\n\t\t// Open jar file\n\t\ttry {\n\t\t\tjar = new JarFile(new File(jarFile.toURI()));\n\t\t} catch (URISyntaxException use) {\n\t\t\tthrow new ProgramInvocationException(\"Invalid file path '\" + jarFile.getPath() + \"'\", use);\n\t\t} catch (IOException ioex) {\n\t\t\tthrow new ProgramInvocationException(\"Error while opening jar file '\" + jarFile.getPath() + \"'. \"\n\t\t\t\t+ ioex.getMessage(), ioex);\n\t\t}\n\n\t\t// jar file must be closed at the end\n\t\ttry {\n\t\t\t// Read from jar manifest\n\t\t\ttry {\n\t\t\t\tmanifest = jar.getManifest();\n\t\t\t} catch (IOException ioex) {\n\t\t\t\tthrow new ProgramInvocationException(\"The Manifest in the jar file could not be accessed '\"\n\t\t\t\t\t+ jarFile.getPath() + \"'. \" + ioex.getMessage(), ioex);\n\t\t\t}\n\n\t\t\tif (manifest == null) {\n\t\t\t\tthrow new ProgramInvocationException(\"No manifest found in jar file '\" + jarFile.getPath() + \"'. The manifest is need to point to the program's main class.\");\n\t\t\t}\n\n\t\t\tAttributes attributes = manifest.getMainAttributes();\n\n\t\t\t// check for a \"program-class\" entry first\n\t\t\tclassName = attributes.getValue(PackagedProgram.MANIFEST_ATTRIBUTE_ASSEMBLER_CLASS);\n\t\t\tif (className != null) {\n\t\t\t\treturn className;\n\t\t\t}\n\n\t\t\t// check for a main class\n\t\t\tclassName = attributes.getValue(PackagedProgram.MANIFEST_ATTRIBUTE_MAIN_CLASS);\n\t\t\tif (className != null) {\n\t\t\t\treturn className;\n\t\t\t} else {\n\t\t\t\tthrow new ProgramInvocationException(\"Neither a '\" + MANIFEST_ATTRIBUTE_MAIN_CLASS + \"', nor a '\" +\n\t\t\t\t\t\tMANIFEST_ATTRIBUTE_ASSEMBLER_CLASS + \"' entry was found in the jar file.\");\n\t\t\t}\n\t\t}\n\t\tfinally {\n\t\t\ttry {\n\t\t\t\tjar.close();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new ProgramInvocationException(\"Could not close the JAR file: \" + t.getMessage(), t);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "ClientTest::TestEager::main(String)",
            " 398 -\n 399  \n 400  \n 401  ",
            "\t\tpublic static void main(String args[]) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).collect();\n\t\t}",
            " 408 +\n 409  \n 410  \n 411  ",
            "\t\tpublic static void main(String[] args) throws Exception {\n\t\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\tenv.fromElements(1, 2).collect();\n\t\t}"
        ],
        [
            "WordCount::parseParameters(String)",
            " 103  \n 104 -\n 105 -\n 106  \n 107  \n 108 -\n 109  \n 110  \n 111 -\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "\tprivate static boolean parseParameters(String[] args) {\n\t\t\n\t\tif(args.length > 0) {\n\t\t\t// parse input arguments\n\t\t\tfileOutput = true;\n\t\t\tif(args.length == 2) { // cli line: program {textPath} {outputPath}\n\t\t\t\ttextPath = args[0];\n\t\t\t\toutputPath = args[1];\n\t\t\t} else if(args.length == 4 && (args[0].startsWith(\"-v\") || args[0].startsWith(\"--verbose\"))) { // cli line: program {optArg} {optVal} {textPath} {outputPath}\n\t\t\t\tBoolean.valueOf(args[1]); // parse verbosity flag\n\t\t\t\ttextPath = args[2];\n\t\t\t\toutputPath = args[3];\n\t\t\t} else {\n\t\t\t\tSystem.err.println(\"Usage: WordCount <text path> <result path>\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing WordCount example with built-in default data.\");\n\t\t\tSystem.out.println(\"  Provide parameters to read input data from a file.\");\n\t\t\tSystem.out.println(\"  Usage: WordCount <text path> <result path>\");\n\t\t}\n\t\treturn true;\n\t}",
            " 102  \n 103 +\n 104 +\n 105  \n 106  \n 107 +\n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  ",
            "\tprivate static boolean parseParameters(String[] args) {\n\n\t\tif (args.length > 0) {\n\t\t\t// parse input arguments\n\t\t\tfileOutput = true;\n\t\t\tif (args.length == 2) { // cli line: program {textPath} {outputPath}\n\t\t\t\ttextPath = args[0];\n\t\t\t\toutputPath = args[1];\n\t\t\t} else if (args.length == 4 && (args[0].startsWith(\"-v\") || args[0].startsWith(\"--verbose\"))) { // cli line: program {optArg} {optVal} {textPath} {outputPath}\n\t\t\t\tBoolean.valueOf(args[1]); // parse verbosity flag\n\t\t\t\ttextPath = args[2];\n\t\t\t\toutputPath = args[3];\n\t\t\t} else {\n\t\t\t\tSystem.err.println(\"Usage: WordCount <text path> <result path>\");\n\t\t\t\treturn false;\n\t\t\t}\n\t\t} else {\n\t\t\tSystem.out.println(\"Executing WordCount example with built-in default data.\");\n\t\t\tSystem.out.println(\"  Provide parameters to read input data from a file.\");\n\t\t\tSystem.out.println(\"  Usage: WordCount <text path> <result path>\");\n\t\t}\n\t\treturn true;\n\t}"
        ],
        [
            "CliFrontendTestUtils::getTestJarPath()",
            "  47  \n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  ",
            "\tpublic static String getTestJarPath() throws FileNotFoundException, MalformedURLException {\n\t\tFile f = new File(\"target/maven-test-jar.jar\");\n\t\tif(!f.exists()) {\n\t\t\tthrow new FileNotFoundException(\"Test jar not present. Invoke tests using maven \"\n\t\t\t\t\t+ \"or build the jar using 'mvn process-test-classes' in flink-clients\");\n\t\t}\n\t\treturn f.getAbsolutePath();\n\t}",
            "  44  \n  45  \n  46 +\n  47  \n  48  \n  49  \n  50  \n  51  ",
            "\tpublic static String getTestJarPath() throws FileNotFoundException, MalformedURLException {\n\t\tFile f = new File(\"target/maven-test-jar.jar\");\n\t\tif (!f.exists()) {\n\t\t\tthrow new FileNotFoundException(\"Test jar not present. Invoke tests using maven \"\n\t\t\t\t\t+ \"or build the jar using 'mvn process-test-classes' in flink-clients\");\n\t\t}\n\t\treturn f.getAbsolutePath();\n\t}"
        ],
        [
            "PackagedProgram::extractContainedLibraries(URL)",
            " 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667 -\n 668  \n 669 -\n 670  \n 671  \n 672  \n 673  \n 674 -\n 675  \n 676  \n 677  \n 678  \n 679 -\n 680  \n 681  \n 682  \n 683  \n 684 -\n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692 -\n 693  \n 694 -\n 695  \n 696  \n 697  \n 698  \n 699  \n 700 -\n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708 -\n 709  \n 710  \n 711 -\n 712  \n 713 -\n 714  \n 715  \n 716 -\n 717  \n 718 -\n 719 -\n 720  \n 721  \n 722 -\n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741 -\n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749 -\n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  ",
            "\t/**\n\t * Takes all JAR files that are contained in this program's JAR file and extracts them\n\t * to the system's temp directory.\n\t * \n\t * @return The file names of the extracted temporary files.\n\t * @throws ProgramInvocationException Thrown, if the extraction process failed.\n\t */\n\tpublic static List<File> extractContainedLibraries(URL jarFile) throws ProgramInvocationException {\n\t\t\n\t\tRandom rnd = new Random();\n\t\t\n\t\tJarFile jar = null;\n\t\ttry {\n\t\t\tjar = new JarFile(new File(jarFile.toURI()));\n\t\t\tfinal List<JarEntry> containedJarFileEntries = new ArrayList<JarEntry>();\n\t\t\t\n\t\t\tEnumeration<JarEntry> entries = jar.entries();\n\t\t\twhile (entries.hasMoreElements()) {\n\t\t\t\tJarEntry entry = entries.nextElement();\n\t\t\t\tString name = entry.getName();\n\t\t\t\t\n\t\t\t\tif (name.length() > 8 && name.startsWith(\"lib/\") && name.endsWith(\".jar\")) {\n\t\t\t\t\tcontainedJarFileEntries.add(entry);\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t\tif (containedJarFileEntries.isEmpty()) {\n\t\t\t\treturn Collections.emptyList();\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// go over all contained jar files\n\t\t\t\tfinal List<File> extractedTempLibraries = new ArrayList<File>(containedJarFileEntries.size());\n\t\t\t\tfinal byte[] buffer = new byte[4096];\n\t\t\t\t\n\t\t\t\tboolean incomplete = true;\n\t\t\t\t\n\t\t\t\ttry {\n\t\t\t\t\tfor (int i = 0; i < containedJarFileEntries.size(); i++) {\n\t\t\t\t\t\tfinal JarEntry entry = containedJarFileEntries.get(i);\n\t\t\t\t\t\tString name = entry.getName();\n\t\t\t\t\t\tname = name.replace(File.separatorChar, '_');\n\t\t\t\t\t\n\t\t\t\t\t\tFile tempFile;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\ttempFile = File.createTempFile(rnd.nextInt(Integer.MAX_VALUE) + \"_\", name);\n\t\t\t\t\t\t\ttempFile.deleteOnExit();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (IOException e) {\n\t\t\t\t\t\t\tthrow new ProgramInvocationException(\n\t\t\t\t\t\t\t\t\"An I/O error occurred while creating temporary file to extract nested library '\" + \n\t\t\t\t\t\t\t\t\t\tentry.getName() + \"'.\", e);\n\t\t\t\t\t\t}\n\t\t\t\t\t\t\n\t\t\t\t\t\textractedTempLibraries.add(tempFile);\n\t\t\t\t\t\t\n\t\t\t\t\t\t// copy the temp file contents to a temporary File\n\t\t\t\t\t\tOutputStream out = null;\n\t\t\t\t\t\tInputStream in = null; \n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tout = new FileOutputStream(tempFile);\n\t\t\t\t\t\t\tin = new BufferedInputStream(jar.getInputStream(entry));\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tint numRead = 0;\n\t\t\t\t\t\t\twhile ((numRead = in.read(buffer)) != -1) {\n\t\t\t\t\t\t\t\tout.write(buffer, 0, numRead);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (IOException e) {\n\t\t\t\t\t\t\tthrow new ProgramInvocationException(\"An I/O error occurred while extracting nested library '\"\n\t\t\t\t\t\t\t\t\t+ entry.getName() + \"' to temporary file '\" + tempFile.getAbsolutePath() + \"'.\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfinally {\n\t\t\t\t\t\t\tif (out != null) {\n\t\t\t\t\t\t\t\tout.close();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (in != null) {\n\t\t\t\t\t\t\t\tin.close();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\tincomplete = false;\n\t\t\t\t}\n\t\t\t\tfinally {\n\t\t\t\t\tif (incomplete) {\n\t\t\t\t\t\tdeleteExtractedLibraries(extractedTempLibraries);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\treturn extractedTempLibraries;\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new ProgramInvocationException(\"Unknown I/O error while extracting contained jar files.\", t);\n\t\t}\n\t\tfinally {\n\t\t\tif (jar != null) {\n\t\t\t\ttry {\n\t\t\t\t\tjar.close();\n\t\t\t\t} catch (Throwable t) {}\n\t\t\t}\n\t\t}\n\t}",
            " 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663 +\n 664  \n 665 +\n 666  \n 667  \n 668  \n 669  \n 670 +\n 671  \n 672  \n 673  \n 674  \n 675 +\n 676  \n 677  \n 678  \n 679  \n 680 +\n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688 +\n 689  \n 690 +\n 691  \n 692  \n 693  \n 694  \n 695  \n 696 +\n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704 +\n 705  \n 706  \n 707 +\n 708  \n 709 +\n 710  \n 711  \n 712 +\n 713  \n 714 +\n 715  \n 716  \n 717 +\n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736 +\n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744 +\n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  ",
            "\t/**\n\t * Takes all JAR files that are contained in this program's JAR file and extracts them\n\t * to the system's temp directory.\n\t *\n\t * @return The file names of the extracted temporary files.\n\t * @throws ProgramInvocationException Thrown, if the extraction process failed.\n\t */\n\tpublic static List<File> extractContainedLibraries(URL jarFile) throws ProgramInvocationException {\n\n\t\tRandom rnd = new Random();\n\n\t\tJarFile jar = null;\n\t\ttry {\n\t\t\tjar = new JarFile(new File(jarFile.toURI()));\n\t\t\tfinal List<JarEntry> containedJarFileEntries = new ArrayList<JarEntry>();\n\n\t\t\tEnumeration<JarEntry> entries = jar.entries();\n\t\t\twhile (entries.hasMoreElements()) {\n\t\t\t\tJarEntry entry = entries.nextElement();\n\t\t\t\tString name = entry.getName();\n\n\t\t\t\tif (name.length() > 8 && name.startsWith(\"lib/\") && name.endsWith(\".jar\")) {\n\t\t\t\t\tcontainedJarFileEntries.add(entry);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (containedJarFileEntries.isEmpty()) {\n\t\t\t\treturn Collections.emptyList();\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// go over all contained jar files\n\t\t\t\tfinal List<File> extractedTempLibraries = new ArrayList<File>(containedJarFileEntries.size());\n\t\t\t\tfinal byte[] buffer = new byte[4096];\n\n\t\t\t\tboolean incomplete = true;\n\n\t\t\t\ttry {\n\t\t\t\t\tfor (int i = 0; i < containedJarFileEntries.size(); i++) {\n\t\t\t\t\t\tfinal JarEntry entry = containedJarFileEntries.get(i);\n\t\t\t\t\t\tString name = entry.getName();\n\t\t\t\t\t\tname = name.replace(File.separatorChar, '_');\n\n\t\t\t\t\t\tFile tempFile;\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\ttempFile = File.createTempFile(rnd.nextInt(Integer.MAX_VALUE) + \"_\", name);\n\t\t\t\t\t\t\ttempFile.deleteOnExit();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (IOException e) {\n\t\t\t\t\t\t\tthrow new ProgramInvocationException(\n\t\t\t\t\t\t\t\t\"An I/O error occurred while creating temporary file to extract nested library '\" +\n\t\t\t\t\t\t\t\t\t\tentry.getName() + \"'.\", e);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\textractedTempLibraries.add(tempFile);\n\n\t\t\t\t\t\t// copy the temp file contents to a temporary File\n\t\t\t\t\t\tOutputStream out = null;\n\t\t\t\t\t\tInputStream in = null;\n\t\t\t\t\t\ttry {\n\n\t\t\t\t\t\t\tout = new FileOutputStream(tempFile);\n\t\t\t\t\t\t\tin = new BufferedInputStream(jar.getInputStream(entry));\n\n\t\t\t\t\t\t\tint numRead = 0;\n\t\t\t\t\t\t\twhile ((numRead = in.read(buffer)) != -1) {\n\t\t\t\t\t\t\t\tout.write(buffer, 0, numRead);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t\tcatch (IOException e) {\n\t\t\t\t\t\t\tthrow new ProgramInvocationException(\"An I/O error occurred while extracting nested library '\"\n\t\t\t\t\t\t\t\t\t+ entry.getName() + \"' to temporary file '\" + tempFile.getAbsolutePath() + \"'.\");\n\t\t\t\t\t\t}\n\t\t\t\t\t\tfinally {\n\t\t\t\t\t\t\tif (out != null) {\n\t\t\t\t\t\t\t\tout.close();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (in != null) {\n\t\t\t\t\t\t\t\tin.close();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tincomplete = false;\n\t\t\t\t}\n\t\t\t\tfinally {\n\t\t\t\t\tif (incomplete) {\n\t\t\t\t\t\tdeleteExtractedLibraries(extractedTempLibraries);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treturn extractedTempLibraries;\n\t\t\t}\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tthrow new ProgramInvocationException(\"Unknown I/O error while extracting contained jar files.\", t);\n\t\t}\n\t\tfinally {\n\t\t\tif (jar != null) {\n\t\t\t\ttry {\n\t\t\t\t\tjar.close();\n\t\t\t\t} catch (Throwable t) {}\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "CliFrontendListCancelTest::testList()",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208 -\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "\t@Test\n\tpublic void testList() {\n\t\ttry {\n\t\t\t// test unrecognized option\n\t\t\t{\n\t\t\t\tString[] parameters = {\"-v\", \"-k\"};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.list(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\t\t\t\n\t\t\t// test list properly\n\t\t\t{\n\t\t\t\tfinal UUID leaderSessionID = UUID.randomUUID();\n\t\t\t\tfinal ActorRef jm = actorSystem.actorOf(\n\t\t\t\t\t\tProps.create(\n\t\t\t\t\t\t\t\tCliJobManager.class,\n\t\t\t\t\t\t\t\tnull,\n\t\t\t\t\t\t\t\tleaderSessionID\n\t\t\t\t\t\t)\n\t\t\t\t);\n\t\t\t\tfinal ActorGateway gateway = new AkkaActorGateway(jm, leaderSessionID);\n\t\t\t\tString[] parameters = {\"-r\", \"-s\"};\n\t\t\t\tInfoListTestCliFrontend testFrontend = new InfoListTestCliFrontend(gateway);\n\t\t\t\tint retCode = testFrontend.list(parameters);\n\t\t\t\tassertTrue(retCode == 0);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tSystem.err.println(e.getMessage());\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Program caused an exception: \" + e.getMessage());\n\t\t}\n\t}",
            " 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 +\n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  ",
            "\t@Test\n\tpublic void testList() {\n\t\ttry {\n\t\t\t// test unrecognized option\n\t\t\t{\n\t\t\t\tString[] parameters = {\"-v\", \"-k\"};\n\t\t\t\tCliFrontend testFrontend = new CliFrontend(CliFrontendTestUtils.getConfigDir());\n\t\t\t\tint retCode = testFrontend.list(parameters);\n\t\t\t\tassertTrue(retCode != 0);\n\t\t\t}\n\n\t\t\t// test list properly\n\t\t\t{\n\t\t\t\tfinal UUID leaderSessionID = UUID.randomUUID();\n\t\t\t\tfinal ActorRef jm = actorSystem.actorOf(\n\t\t\t\t\t\tProps.create(\n\t\t\t\t\t\t\t\tCliJobManager.class,\n\t\t\t\t\t\t\t\tnull,\n\t\t\t\t\t\t\t\tleaderSessionID\n\t\t\t\t\t\t)\n\t\t\t\t);\n\t\t\t\tfinal ActorGateway gateway = new AkkaActorGateway(jm, leaderSessionID);\n\t\t\t\tString[] parameters = {\"-r\", \"-s\"};\n\t\t\t\tInfoListTestCliFrontend testFrontend = new InfoListTestCliFrontend(gateway);\n\t\t\t\tint retCode = testFrontend.list(parameters);\n\t\t\t\tassertTrue(retCode == 0);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tSystem.err.println(e.getMessage());\n\t\t\te.printStackTrace();\n\t\t\tfail(\"Program caused an exception: \" + e.getMessage());\n\t\t}\n\t}"
        ],
        [
            "WordCount::main(String)",
            "  38  \n  39 -\n  40 -\n  41  \n  42  \n  43 -\n  44  \n  45  \n  46 -\n  47  \n  48  \n  49 -\n  50 -\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 -\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\t\n\t\tif(!parseParameters(args)) {\n\t\t\treturn;\n\t\t}\n\t\t\n\t\t// set up the execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\t\n\t\t// get input data\n\t\tDataSet<String> text = getTextDataSet(env);\n\t\t\n\t\tDataSet<Tuple2<String, Integer>> counts = \n\t\t\t\t// split up the lines in pairs (2-tuples) containing: (word,1)\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t// group by the tuple field \"0\" and sum up tuple field \"1\"\n\t\t\t\t.groupBy(0)\n\t\t\t\t.aggregate(Aggregations.SUM, 1);\n\n\t\t// emit result\n\t\tif(fileOutput) {\n\t\t\tcounts.writeAsCsv(outputPath, \"\\n\", \" \");\n\t\t\t// execute program\n\t\t\tenv.execute(\"WordCount Example\");\n\t\t} else {\n\t\t\tcounts.print();\n\t\t}\n\t}",
            "  37  \n  38 +\n  39 +\n  40  \n  41  \n  42 +\n  43  \n  44  \n  45 +\n  46  \n  47  \n  48 +\n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tif (!parseParameters(args)) {\n\t\t\treturn;\n\t\t}\n\n\t\t// set up the execution environment\n\t\tfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// get input data\n\t\tDataSet<String> text = getTextDataSet(env);\n\n\t\tDataSet<Tuple2<String, Integer>> counts =\n\t\t\t\t// split up the lines in pairs (2-tuples) containing: (word,1)\n\t\t\t\ttext.flatMap(new Tokenizer())\n\t\t\t\t// group by the tuple field \"0\" and sum up tuple field \"1\"\n\t\t\t\t.groupBy(0)\n\t\t\t\t.aggregate(Aggregations.SUM, 1);\n\n\t\t// emit result\n\t\tif (fileOutput) {\n\t\t\tcounts.writeAsCsv(outputPath, \"\\n\", \" \");\n\t\t\t// execute program\n\t\t\tenv.execute(\"WordCount Example\");\n\t\t} else {\n\t\t\tcounts.print();\n\t\t}\n\t}"
        ]
    ],
    "15856af4393ddc1b1a679c1c717de456136afb16": [
        [
            "AbstractYarnClusterDescriptor::startAppMaster(JobGraph,YarnClient,YarnClientApplication)",
            " 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674 -\n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  ",
            "\tpublic ApplicationReport startAppMaster(JobGraph jobGraph, YarnClient yarnClient, YarnClientApplication yarnApplication) throws Exception {\n\n\t\t// ------------------ Set default file system scheme -------------------------\n\n\t\ttry {\n\t\t\torg.apache.flink.core.fs.FileSystem.setDefaultScheme(flinkConfiguration);\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error while setting the default \" +\n\t\t\t\t\t\"filesystem scheme from configuration.\", e);\n\t\t}\n\n\t\t// initialize file system\n\t\t// Copy the application master jar to the filesystem\n\t\t// Create a local resource to point to the destination jar path\n\t\tfinal FileSystem fs = FileSystem.get(conf);\n\n\t\t// hard coded check for the GoogleHDFS client because its not overriding the getScheme() method.\n\t\tif (!fs.getClass().getSimpleName().equals(\"GoogleHadoopFileSystem\") &&\n\t\t\t\tfs.getScheme().startsWith(\"file\")) {\n\t\t\tLOG.warn(\"The file system scheme is '\" + fs.getScheme() + \"'. This indicates that the \"\n\t\t\t\t\t+ \"specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.\"\n\t\t\t\t\t+ \"The Flink YARN client needs to store its files in a distributed file system\");\n\t\t}\n\n\t\tApplicationSubmissionContext appContext = yarnApplication.getApplicationSubmissionContext();\n\t\tSet<File> systemShipFiles = new HashSet<>(shipFiles.size());\n\t\tfor (File file : shipFiles) {\n\t\t\tsystemShipFiles.add(file.getAbsoluteFile());\n\t\t}\n\n\t\t//check if there is a logback or log4j file\n\t\tFile logbackFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOGBACK_NAME);\n\t\tfinal boolean hasLogback = logbackFile.exists();\n\t\tif (hasLogback) {\n\t\t\tsystemShipFiles.add(logbackFile);\n\t\t}\n\n\t\tFile log4jFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOG4J_NAME);\n\t\tfinal boolean hasLog4j = log4jFile.exists();\n\t\tif (hasLog4j) {\n\t\t\tsystemShipFiles.add(log4jFile);\n\t\t\tif (hasLogback) {\n\t\t\t\t// this means there is already a logback configuration file --> fail\n\t\t\t\tLOG.warn(\"The configuration directory ('\" + configurationDirectory + \"') contains both LOG4J and \" +\n\t\t\t\t\t\"Logback configuration files. Please delete or rename one of them.\");\n\t\t\t}\n\t\t}\n\n\t\taddLibFolderToShipFiles(systemShipFiles);\n\n\t\t// Set-up ApplicationSubmissionContext for the application\n\n\t\tfinal ApplicationId appId = appContext.getApplicationId();\n\n\t\t// ------------------ Add Zookeeper namespace to local flinkConfiguraton ------\n\t\tString zkNamespace = getZookeeperNamespace();\n\t\t// no user specified cli argument for namespace?\n\t\tif (zkNamespace == null || zkNamespace.isEmpty()) {\n\t\t\t// namespace defined in config? else use applicationId as default.\n\t\t\tzkNamespace = flinkConfiguration.getString(HighAvailabilityOptions.HA_CLUSTER_ID, String.valueOf(appId));\n\t\t\tsetZookeeperNamespace(zkNamespace);\n\t\t}\n\n\t\tflinkConfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\tif (HighAvailabilityMode.isHighAvailabilityModeActivated(flinkConfiguration)) {\n\t\t\t// activate re-execution of failed applications\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));\n\n\t\t\tactivateHighAvailabilitySupport(appContext);\n\t\t} else {\n\t\t\t// set number of application retries to 1 in the default case\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\t1));\n\t\t}\n\n\t\t// local resource map for Yarn\n\t\tfinal Map<String, LocalResource> localResources = new HashMap<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// list of remote paths (after upload)\n\t\tfinal List<Path> paths = new ArrayList<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// ship list that enables reuse of resources for task manager containers\n\t\tStringBuilder envShipFileList = new StringBuilder();\n\n\t\t// upload and register ship files\t\n\t\tList<String> systemClassPaths = uploadAndRegisterFiles(systemShipFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\t\tList<String> userClassPaths = uploadAndRegisterFiles(userJarFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.ORDER) {\n\t\t\tsystemClassPaths.addAll(userClassPaths);\n\t\t}\n\n\t\t// normalize classpath by sorting\n\t\tCollections.sort(systemClassPaths);\n\t\tCollections.sort(userClassPaths);\n\n\t\t// classpath assembler\n\t\tStringBuilder classPathBuilder = new StringBuilder();\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.FIRST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\t\tfor (String classPath : systemClassPaths) {\n\t\t\tclassPathBuilder.append(classPath).append(File.pathSeparator);\n\t\t}\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.LAST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\n\t\t// Setup jar for ApplicationMaster\n\t\tLocalResource appMasterJar = Records.newRecord(LocalResource.class);\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\tPath remotePathJar =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkJarPath, appMasterJar, fs.getHomeDirectory());\n\t\tPath remotePathConf =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkConfigurationPath, flinkConf, fs.getHomeDirectory());\n\t\tlocalResources.put(\"flink.jar\", appMasterJar);\n\t\tlocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\tpaths.add(remotePathJar);\n\t\tclassPathBuilder.append(\"flink.jar\").append(File.pathSeparator);\n\t\tpaths.add(remotePathConf);\n\t\tclassPathBuilder.append(\"flink-conf.yaml\").append(File.pathSeparator);\n\n\t\t// write job graph to tmp file and add it to local resource\n\t\t// TODO: server use user main method to generate job graph\n\t\tif (jobGraph != null) {\n\t\t\ttry {\n\t\t\t\tFile fp = File.createTempFile(appId.toString(), null);\n\t\t\t\tfp.deleteOnExit();\n\t\t\t\ttry (FileOutputStream output = new FileOutputStream(fp);\n\t\t\t\t\tObjectOutputStream obOutput = new ObjectOutputStream(output);){\n\t\t\t\t\tobOutput.writeObject(jobGraph);\n\t\t\t\t}\n\t\t\t\tLocalResource jobgraph = Records.newRecord(LocalResource.class);\n\t\t\t\tPath remoteJobGraph =\n\t\t\t\t\t\tUtils.setupLocalResource(fs, appId.toString(), new Path(fp.toURI()), jobgraph, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(\"job.graph\", jobgraph);\n\t\t\t\tpaths.add(remoteJobGraph);\n\t\t\t\tclassPathBuilder.append(\"job.graph\").append(File.pathSeparator);\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Add job graph to local resource fail\");\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\n\t\tPath yarnFilesDir = new Path(fs.getHomeDirectory(), \".flink/\" + appId + '/');\n\n\t\tFsPermission permission = new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n\t\tfs.setPermission(yarnFilesDir, permission); // set permission for path.\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\t//In Integration test setup, the Yarn containers created by YarnMiniCluster does not have the Yarn site XML\n\t\t//and KRB5 configuration files. We are adding these files as container local resources for the container\n\t\t//applications (JM/TMs) to have proper secure cluster setup\n\t\tPath remoteKrb5Path = null;\n\t\tPath remoteYarnSiteXmlPath = null;\n\t\tboolean hasKrb5 = false;\n\t\tif(System.getenv(\"IN_TESTS\") != null) {\n\t\t\tString krb5Config = System.getProperty(\"java.security.krb5.conf\");\n\t\t\tif(krb5Config != null && krb5Config.length() != 0) {\n\t\t\t\tFile krb5 = new File(krb5Config);\n\t\t\t\tLOG.info(\"Adding KRB5 configuration {} to the AM container local resource bucket\", krb5.getAbsolutePath());\n\t\t\t\tLocalResource krb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath krb5ConfPath = new Path(krb5.getAbsolutePath());\n\t\t\t\tremoteKrb5Path = Utils.setupLocalResource(fs, appId.toString(), krb5ConfPath, krb5ConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.KRB5_FILE_NAME, krb5ConfResource);\n\n\t\t\t\tFile f = new File(System.getenv(\"YARN_CONF_DIR\"),Utils.YARN_SITE_FILE_NAME);\n\t\t\t\tLOG.info(\"Adding Yarn configuration {} to the AM container local resource bucket\", f.getAbsolutePath());\n\t\t\t\tLocalResource yarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath yarnSitePath = new Path(f.getAbsolutePath());\n\t\t\t\tremoteYarnSiteXmlPath = Utils.setupLocalResource(fs, appId.toString(), yarnSitePath, yarnConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.YARN_SITE_FILE_NAME, yarnConfResource);\n\n\t\t\t\thasKrb5 = true;\n\t\t\t}\n\t\t}\n\n\t\t// setup security tokens\n\t\tLocalResource keytabResource = null;\n\t\tPath remotePathKeytab = null;\n\t\tString keytab = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);\n\t\tif(keytab != null) {\n\t\t\tLOG.info(\"Adding keytab {} to the AM container local resource bucket\", keytab);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(keytab);\n\t\t\tremotePathKeytab = Utils.setupLocalResource(fs, appId.toString(), keytabPath, keytabResource, fs.getHomeDirectory());\n\t\t\tlocalResources.put(Utils.KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\tfinal ContainerLaunchContext amContainer = setupApplicationMasterContainer(hasLogback, hasLog4j, hasKrb5);\n\n\t\tif ( UserGroupInformation.isSecurityEnabled() && keytab == null ) {\n\t\t\t//set tokens only when keytab is not provided\n\t\t\tLOG.info(\"Adding delegation token to the AM container..\");\n\t\t\tUtils.setTokensFor(amContainer, paths, conf);\n\t\t}\n\n\t\tamContainer.setLocalResources(localResources);\n\t\tfs.close();\n\n\t\t// Setup CLASSPATH and environment variables for ApplicationMaster\n\t\tfinal Map<String, String> appMasterEnv = new HashMap<>();\n\t\t// set user specified app master environment variables\n\t\tappMasterEnv.putAll(Utils.getEnvironmentVariables(ConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX, flinkConfiguration));\n\t\t// set Flink app class path\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_FLINK_CLASSPATH, classPathBuilder.toString());\n\n\t\t// set Flink on YARN internal configuration values\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_COUNT, String.valueOf(taskManagerCount));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_MEMORY, String.valueOf(taskManagerMemoryMb));\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_JAR_PATH, remotePathJar.toString() );\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_APP_ID, appId.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_HOME_DIR, fs.getHomeDirectory().toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_SHIP_FILES, envShipFileList.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_SLOTS, String.valueOf(slots));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_DETACHED, String.valueOf(detached));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE, getZookeeperNamespace());\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_YARN_FILES, yarnFilesDir.toUri().toString());\n\n\t\t// https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md#identity-on-an-insecure-cluster-hadoop_user_name\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif(keytabResource != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PATH, remotePathKeytab.toString() );\n\t\t\tString principal = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL);\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, principal );\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif(remoteYarnSiteXmlPath != null && remoteKrb5Path != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_YARN_SITE_XML_PATH, remoteYarnSiteXmlPath.toString());\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_KRB5_PATH, remoteKrb5Path.toString() );\n\t\t}\n\n\t\tif(dynamicPropertiesEncoded != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n\t\t}\n\n\t\t// set classpath from YARN configuration\n\t\tUtils.setupYarnClassPath(conf, appMasterEnv);\n\n\t\tamContainer.setEnvironment(appMasterEnv);\n\n\t\t// Set up resource type requirements for ApplicationMaster\n\t\tResource capability = Records.newRecord(Resource.class);\n\t\tcapability.setMemory(jobManagerMemoryMb);\n\t\tcapability.setVirtualCores(1);\n\n\t\tString name;\n\t\tif(customName == null) {\n\t\t\tname = \"Flink session with \" + taskManagerCount + \" TaskManagers\";\n\t\t\tif(detached) {\n\t\t\t\tname += \" (detached)\";\n\t\t\t}\n\t\t} else {\n\t\t\tname = customName;\n\t\t}\n\n\t\tappContext.setApplicationName(name);\n\t\tappContext.setApplicationType(\"Apache Flink\");\n\t\tappContext.setAMContainerSpec(amContainer);\n\t\tappContext.setResource(capability);\n\t\tif(yarnQueue != null) {\n\t\t\tappContext.setQueue(yarnQueue);\n\t\t}\n\n\t\tsetApplicationTags(appContext);\n\n\t\t// add a hook to clean up in case deployment fails\n\t\tThread deploymentFailureHook = new DeploymentFailureHook(yarnClient, yarnApplication, yarnFilesDir);\n\t\tRuntime.getRuntime().addShutdownHook(deploymentFailureHook);\n\t\tLOG.info(\"Submitting application master \" + appId);\n\t\tyarnClient.submitApplication(appContext);\n\n\t\tLOG.info(\"Waiting for the cluster to be allocated\");\n\t\tfinal long startTime = System.currentTimeMillis();\n\t\tApplicationReport report;\n\t\tYarnApplicationState lastAppState = YarnApplicationState.NEW;\n\t\tloop: while( true ) {\n\t\t\ttry {\n\t\t\t\treport = yarnClient.getApplicationReport(appId);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new YarnDeploymentException(\"Failed to deploy the cluster.\", e);\n\t\t\t}\n\t\t\tYarnApplicationState appState = report.getYarnApplicationState();\n\t\t\tLOG.debug(\"Application State: {}\", appState);\n\t\t\tswitch(appState) {\n\t\t\t\tcase FAILED:\n\t\t\t\tcase FINISHED: //TODO: the finished state may be valid in flip-6\n\t\t\t\tcase KILLED:\n\t\t\t\t\tthrow new YarnDeploymentException(\"The YARN application unexpectedly switched to state \"\n\t\t\t\t\t\t+ appState + \" during deployment. \\n\" +\n\t\t\t\t\t\t\"Diagnostics from YARN: \" + report.getDiagnostics() + \"\\n\" +\n\t\t\t\t\t\t\"If log aggregation is enabled on your cluster, use this command to further investigate the issue:\\n\" +\n\t\t\t\t\t\t\"yarn logs -applicationId \" + appId);\n\t\t\t\t\t//break ..\n\t\t\t\tcase RUNNING:\n\t\t\t\t\tLOG.info(\"YARN application has been deployed successfully.\");\n\t\t\t\t\tbreak loop;\n\t\t\t\tdefault:\n\t\t\t\t\tif (appState != lastAppState) {\n\t\t\t\t\t\tLOG.info(\"Deploying cluster, current state \" + appState);\n\t\t\t\t\t}\n\t\t\t\t\tif(System.currentTimeMillis() - startTime > 60000) {\n\t\t\t\t\t\tLOG.info(\"Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\");\n\t\t\t\t\t}\n\n\t\t\t}\n\t\t\tlastAppState = appState;\n\t\t\tThread.sleep(250);\n\t\t}\n\t\t// print the application id for user to cancel themselves.\n\t\tif (isDetachedMode()) {\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop \" +\n\t\t\t\t\t\"it:\\nyarn application -kill \" + appId + \"\\nPlease also note that the \" +\n\t\t\t\t\t\"temporary files of the YARN session in the home directoy will not be removed.\");\n\t\t}\n\t\t// since deployment was successful, remove the hook\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(deploymentFailureHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we're already in the shut down hook.\n\t\t}\n\t\treturn report;\n\t}",
            " 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674 +\n 675 +\n 676 +\n 677 +\n 678 +\n 679 +\n 680 +\n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  ",
            "\tpublic ApplicationReport startAppMaster(JobGraph jobGraph, YarnClient yarnClient, YarnClientApplication yarnApplication) throws Exception {\n\n\t\t// ------------------ Set default file system scheme -------------------------\n\n\t\ttry {\n\t\t\torg.apache.flink.core.fs.FileSystem.setDefaultScheme(flinkConfiguration);\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error while setting the default \" +\n\t\t\t\t\t\"filesystem scheme from configuration.\", e);\n\t\t}\n\n\t\t// initialize file system\n\t\t// Copy the application master jar to the filesystem\n\t\t// Create a local resource to point to the destination jar path\n\t\tfinal FileSystem fs = FileSystem.get(conf);\n\n\t\t// hard coded check for the GoogleHDFS client because its not overriding the getScheme() method.\n\t\tif (!fs.getClass().getSimpleName().equals(\"GoogleHadoopFileSystem\") &&\n\t\t\t\tfs.getScheme().startsWith(\"file\")) {\n\t\t\tLOG.warn(\"The file system scheme is '\" + fs.getScheme() + \"'. This indicates that the \"\n\t\t\t\t\t+ \"specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.\"\n\t\t\t\t\t+ \"The Flink YARN client needs to store its files in a distributed file system\");\n\t\t}\n\n\t\tApplicationSubmissionContext appContext = yarnApplication.getApplicationSubmissionContext();\n\t\tSet<File> systemShipFiles = new HashSet<>(shipFiles.size());\n\t\tfor (File file : shipFiles) {\n\t\t\tsystemShipFiles.add(file.getAbsoluteFile());\n\t\t}\n\n\t\t//check if there is a logback or log4j file\n\t\tFile logbackFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOGBACK_NAME);\n\t\tfinal boolean hasLogback = logbackFile.exists();\n\t\tif (hasLogback) {\n\t\t\tsystemShipFiles.add(logbackFile);\n\t\t}\n\n\t\tFile log4jFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOG4J_NAME);\n\t\tfinal boolean hasLog4j = log4jFile.exists();\n\t\tif (hasLog4j) {\n\t\t\tsystemShipFiles.add(log4jFile);\n\t\t\tif (hasLogback) {\n\t\t\t\t// this means there is already a logback configuration file --> fail\n\t\t\t\tLOG.warn(\"The configuration directory ('\" + configurationDirectory + \"') contains both LOG4J and \" +\n\t\t\t\t\t\"Logback configuration files. Please delete or rename one of them.\");\n\t\t\t}\n\t\t}\n\n\t\taddLibFolderToShipFiles(systemShipFiles);\n\n\t\t// Set-up ApplicationSubmissionContext for the application\n\n\t\tfinal ApplicationId appId = appContext.getApplicationId();\n\n\t\t// ------------------ Add Zookeeper namespace to local flinkConfiguraton ------\n\t\tString zkNamespace = getZookeeperNamespace();\n\t\t// no user specified cli argument for namespace?\n\t\tif (zkNamespace == null || zkNamespace.isEmpty()) {\n\t\t\t// namespace defined in config? else use applicationId as default.\n\t\t\tzkNamespace = flinkConfiguration.getString(HighAvailabilityOptions.HA_CLUSTER_ID, String.valueOf(appId));\n\t\t\tsetZookeeperNamespace(zkNamespace);\n\t\t}\n\n\t\tflinkConfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\tif (HighAvailabilityMode.isHighAvailabilityModeActivated(flinkConfiguration)) {\n\t\t\t// activate re-execution of failed applications\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));\n\n\t\t\tactivateHighAvailabilitySupport(appContext);\n\t\t} else {\n\t\t\t// set number of application retries to 1 in the default case\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\t1));\n\t\t}\n\n\t\t// local resource map for Yarn\n\t\tfinal Map<String, LocalResource> localResources = new HashMap<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// list of remote paths (after upload)\n\t\tfinal List<Path> paths = new ArrayList<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// ship list that enables reuse of resources for task manager containers\n\t\tStringBuilder envShipFileList = new StringBuilder();\n\n\t\t// upload and register ship files\t\n\t\tList<String> systemClassPaths = uploadAndRegisterFiles(systemShipFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\n\t\tList<String> userClassPaths;\n\t\tif (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\tuserClassPaths = uploadAndRegisterFiles(userJarFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\t\t} else {\n\t\t\tuserClassPaths = Collections.emptyList();\n\t\t}\n\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.ORDER) {\n\t\t\tsystemClassPaths.addAll(userClassPaths);\n\t\t}\n\n\t\t// normalize classpath by sorting\n\t\tCollections.sort(systemClassPaths);\n\t\tCollections.sort(userClassPaths);\n\n\t\t// classpath assembler\n\t\tStringBuilder classPathBuilder = new StringBuilder();\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.FIRST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\t\tfor (String classPath : systemClassPaths) {\n\t\t\tclassPathBuilder.append(classPath).append(File.pathSeparator);\n\t\t}\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.LAST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\n\t\t// Setup jar for ApplicationMaster\n\t\tLocalResource appMasterJar = Records.newRecord(LocalResource.class);\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\tPath remotePathJar =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkJarPath, appMasterJar, fs.getHomeDirectory());\n\t\tPath remotePathConf =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkConfigurationPath, flinkConf, fs.getHomeDirectory());\n\t\tlocalResources.put(\"flink.jar\", appMasterJar);\n\t\tlocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\tpaths.add(remotePathJar);\n\t\tclassPathBuilder.append(\"flink.jar\").append(File.pathSeparator);\n\t\tpaths.add(remotePathConf);\n\t\tclassPathBuilder.append(\"flink-conf.yaml\").append(File.pathSeparator);\n\n\t\t// write job graph to tmp file and add it to local resource\n\t\t// TODO: server use user main method to generate job graph\n\t\tif (jobGraph != null) {\n\t\t\ttry {\n\t\t\t\tFile fp = File.createTempFile(appId.toString(), null);\n\t\t\t\tfp.deleteOnExit();\n\t\t\t\ttry (FileOutputStream output = new FileOutputStream(fp);\n\t\t\t\t\tObjectOutputStream obOutput = new ObjectOutputStream(output);){\n\t\t\t\t\tobOutput.writeObject(jobGraph);\n\t\t\t\t}\n\t\t\t\tLocalResource jobgraph = Records.newRecord(LocalResource.class);\n\t\t\t\tPath remoteJobGraph =\n\t\t\t\t\t\tUtils.setupLocalResource(fs, appId.toString(), new Path(fp.toURI()), jobgraph, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(\"job.graph\", jobgraph);\n\t\t\t\tpaths.add(remoteJobGraph);\n\t\t\t\tclassPathBuilder.append(\"job.graph\").append(File.pathSeparator);\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Add job graph to local resource fail\");\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\n\t\tPath yarnFilesDir = new Path(fs.getHomeDirectory(), \".flink/\" + appId + '/');\n\n\t\tFsPermission permission = new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n\t\tfs.setPermission(yarnFilesDir, permission); // set permission for path.\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\t//In Integration test setup, the Yarn containers created by YarnMiniCluster does not have the Yarn site XML\n\t\t//and KRB5 configuration files. We are adding these files as container local resources for the container\n\t\t//applications (JM/TMs) to have proper secure cluster setup\n\t\tPath remoteKrb5Path = null;\n\t\tPath remoteYarnSiteXmlPath = null;\n\t\tboolean hasKrb5 = false;\n\t\tif(System.getenv(\"IN_TESTS\") != null) {\n\t\t\tString krb5Config = System.getProperty(\"java.security.krb5.conf\");\n\t\t\tif(krb5Config != null && krb5Config.length() != 0) {\n\t\t\t\tFile krb5 = new File(krb5Config);\n\t\t\t\tLOG.info(\"Adding KRB5 configuration {} to the AM container local resource bucket\", krb5.getAbsolutePath());\n\t\t\t\tLocalResource krb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath krb5ConfPath = new Path(krb5.getAbsolutePath());\n\t\t\t\tremoteKrb5Path = Utils.setupLocalResource(fs, appId.toString(), krb5ConfPath, krb5ConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.KRB5_FILE_NAME, krb5ConfResource);\n\n\t\t\t\tFile f = new File(System.getenv(\"YARN_CONF_DIR\"),Utils.YARN_SITE_FILE_NAME);\n\t\t\t\tLOG.info(\"Adding Yarn configuration {} to the AM container local resource bucket\", f.getAbsolutePath());\n\t\t\t\tLocalResource yarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath yarnSitePath = new Path(f.getAbsolutePath());\n\t\t\t\tremoteYarnSiteXmlPath = Utils.setupLocalResource(fs, appId.toString(), yarnSitePath, yarnConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.YARN_SITE_FILE_NAME, yarnConfResource);\n\n\t\t\t\thasKrb5 = true;\n\t\t\t}\n\t\t}\n\n\t\t// setup security tokens\n\t\tLocalResource keytabResource = null;\n\t\tPath remotePathKeytab = null;\n\t\tString keytab = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);\n\t\tif(keytab != null) {\n\t\t\tLOG.info(\"Adding keytab {} to the AM container local resource bucket\", keytab);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(keytab);\n\t\t\tremotePathKeytab = Utils.setupLocalResource(fs, appId.toString(), keytabPath, keytabResource, fs.getHomeDirectory());\n\t\t\tlocalResources.put(Utils.KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\tfinal ContainerLaunchContext amContainer = setupApplicationMasterContainer(hasLogback, hasLog4j, hasKrb5);\n\n\t\tif ( UserGroupInformation.isSecurityEnabled() && keytab == null ) {\n\t\t\t//set tokens only when keytab is not provided\n\t\t\tLOG.info(\"Adding delegation token to the AM container..\");\n\t\t\tUtils.setTokensFor(amContainer, paths, conf);\n\t\t}\n\n\t\tamContainer.setLocalResources(localResources);\n\t\tfs.close();\n\n\t\t// Setup CLASSPATH and environment variables for ApplicationMaster\n\t\tfinal Map<String, String> appMasterEnv = new HashMap<>();\n\t\t// set user specified app master environment variables\n\t\tappMasterEnv.putAll(Utils.getEnvironmentVariables(ConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX, flinkConfiguration));\n\t\t// set Flink app class path\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_FLINK_CLASSPATH, classPathBuilder.toString());\n\n\t\t// set Flink on YARN internal configuration values\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_COUNT, String.valueOf(taskManagerCount));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_MEMORY, String.valueOf(taskManagerMemoryMb));\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_JAR_PATH, remotePathJar.toString() );\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_APP_ID, appId.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_HOME_DIR, fs.getHomeDirectory().toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_SHIP_FILES, envShipFileList.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_SLOTS, String.valueOf(slots));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_DETACHED, String.valueOf(detached));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE, getZookeeperNamespace());\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_YARN_FILES, yarnFilesDir.toUri().toString());\n\n\t\t// https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md#identity-on-an-insecure-cluster-hadoop_user_name\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif(keytabResource != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PATH, remotePathKeytab.toString() );\n\t\t\tString principal = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL);\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, principal );\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif(remoteYarnSiteXmlPath != null && remoteKrb5Path != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_YARN_SITE_XML_PATH, remoteYarnSiteXmlPath.toString());\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_KRB5_PATH, remoteKrb5Path.toString() );\n\t\t}\n\n\t\tif(dynamicPropertiesEncoded != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n\t\t}\n\n\t\t// set classpath from YARN configuration\n\t\tUtils.setupYarnClassPath(conf, appMasterEnv);\n\n\t\tamContainer.setEnvironment(appMasterEnv);\n\n\t\t// Set up resource type requirements for ApplicationMaster\n\t\tResource capability = Records.newRecord(Resource.class);\n\t\tcapability.setMemory(jobManagerMemoryMb);\n\t\tcapability.setVirtualCores(1);\n\n\t\tString name;\n\t\tif(customName == null) {\n\t\t\tname = \"Flink session with \" + taskManagerCount + \" TaskManagers\";\n\t\t\tif(detached) {\n\t\t\t\tname += \" (detached)\";\n\t\t\t}\n\t\t} else {\n\t\t\tname = customName;\n\t\t}\n\n\t\tappContext.setApplicationName(name);\n\t\tappContext.setApplicationType(\"Apache Flink\");\n\t\tappContext.setAMContainerSpec(amContainer);\n\t\tappContext.setResource(capability);\n\t\tif(yarnQueue != null) {\n\t\t\tappContext.setQueue(yarnQueue);\n\t\t}\n\n\t\tsetApplicationTags(appContext);\n\n\t\t// add a hook to clean up in case deployment fails\n\t\tThread deploymentFailureHook = new DeploymentFailureHook(yarnClient, yarnApplication, yarnFilesDir);\n\t\tRuntime.getRuntime().addShutdownHook(deploymentFailureHook);\n\t\tLOG.info(\"Submitting application master \" + appId);\n\t\tyarnClient.submitApplication(appContext);\n\n\t\tLOG.info(\"Waiting for the cluster to be allocated\");\n\t\tfinal long startTime = System.currentTimeMillis();\n\t\tApplicationReport report;\n\t\tYarnApplicationState lastAppState = YarnApplicationState.NEW;\n\t\tloop: while( true ) {\n\t\t\ttry {\n\t\t\t\treport = yarnClient.getApplicationReport(appId);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new YarnDeploymentException(\"Failed to deploy the cluster.\", e);\n\t\t\t}\n\t\t\tYarnApplicationState appState = report.getYarnApplicationState();\n\t\t\tLOG.debug(\"Application State: {}\", appState);\n\t\t\tswitch(appState) {\n\t\t\t\tcase FAILED:\n\t\t\t\tcase FINISHED: //TODO: the finished state may be valid in flip-6\n\t\t\t\tcase KILLED:\n\t\t\t\t\tthrow new YarnDeploymentException(\"The YARN application unexpectedly switched to state \"\n\t\t\t\t\t\t+ appState + \" during deployment. \\n\" +\n\t\t\t\t\t\t\"Diagnostics from YARN: \" + report.getDiagnostics() + \"\\n\" +\n\t\t\t\t\t\t\"If log aggregation is enabled on your cluster, use this command to further investigate the issue:\\n\" +\n\t\t\t\t\t\t\"yarn logs -applicationId \" + appId);\n\t\t\t\t\t//break ..\n\t\t\t\tcase RUNNING:\n\t\t\t\t\tLOG.info(\"YARN application has been deployed successfully.\");\n\t\t\t\t\tbreak loop;\n\t\t\t\tdefault:\n\t\t\t\t\tif (appState != lastAppState) {\n\t\t\t\t\t\tLOG.info(\"Deploying cluster, current state \" + appState);\n\t\t\t\t\t}\n\t\t\t\t\tif(System.currentTimeMillis() - startTime > 60000) {\n\t\t\t\t\t\tLOG.info(\"Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\");\n\t\t\t\t\t}\n\n\t\t\t}\n\t\t\tlastAppState = appState;\n\t\t\tThread.sleep(250);\n\t\t}\n\t\t// print the application id for user to cancel themselves.\n\t\tif (isDetachedMode()) {\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop \" +\n\t\t\t\t\t\"it:\\nyarn application -kill \" + appId + \"\\nPlease also note that the \" +\n\t\t\t\t\t\"temporary files of the YARN session in the home directoy will not be removed.\");\n\t\t}\n\t\t// since deployment was successful, remove the hook\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(deploymentFailureHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we're already in the shut down hook.\n\t\t}\n\t\treturn report;\n\t}"
        ]
    ],
    "77b0fb9fe3656a5ae7e2ca3bbce28cfa5a0e247e": [
        [
            "YarnPreConfiguredMasterHaServicesTest::destroyHDFS()",
            "  76  \n  77  \n  78 -\n  79 -\n  80  \n  81 -\n  82 -\n  83  ",
            "\t@AfterClass\n\tpublic static void destroyHDFS() {\n\t\tif (HDFS_CLUSTER != null) {\n\t\t\tHDFS_CLUSTER.shutdown();\n\t\t}\n\t\tHDFS_CLUSTER = null;\n\t\tHDFS_ROOT_PATH = null;\n\t}",
            "  78  \n  79  \n  80 +\n  81 +\n  82  \n  83 +\n  84 +\n  85  ",
            "\t@AfterClass\n\tpublic static void destroyHDFS() {\n\t\tif (hdfsCluster != null) {\n\t\t\thdfsCluster.shutdown();\n\t\t}\n\t\thdfsCluster = null;\n\t\thdfsRootPath = null;\n\t}"
        ],
        [
            "YarnPreConfiguredMasterHaServicesTest::initConfig()",
            "  85  \n  86  \n  87  \n  88 -\n  89  ",
            "\t@Before\n\tpublic void initConfig() {\n\t\thadoopConfig = new org.apache.hadoop.conf.Configuration();\n\t\thadoopConfig.set(org.apache.hadoop.fs.FileSystem.FS_DEFAULT_NAME_KEY, HDFS_ROOT_PATH.toString());\n\t}",
            "  87  \n  88  \n  89  \n  90 +\n  91  ",
            "\t@Before\n\tpublic void initConfig() {\n\t\thadoopConfig = new org.apache.hadoop.conf.Configuration();\n\t\thadoopConfig.set(org.apache.hadoop.fs.FileSystem.FS_DEFAULT_NAME_KEY, hdfsRootPath.toString());\n\t}"
        ],
        [
            "YarnClusterClient::getClusterStatus()",
            " 224  \n 225  \n 226  \n 227  \n 228  \n 229 -\n 230  \n 231  \n 232 -\n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  ",
            "\t/**\n\t * This method is only available if the cluster hasn't been started in detached mode.\n\t */\n\t@Override\n\tpublic GetClusterStatusResponse getClusterStatus() {\n\t\tif(!isConnected) {\n\t\t\tthrow new IllegalStateException(\"The cluster is not connected to the cluster.\");\n\t\t}\n\t\tif(hasBeenShutdown()) {\n\t\t\tthrow new IllegalStateException(\"The cluster has already been shutdown.\");\n\t\t}\n\n\t\ttry {\n\t\t\tfinal Future<Object> clusterStatusOption =\n\t\t\t\tgetJobManagerGateway().ask(\n\t\t\t\t\tGetClusterStatus.getInstance(),\n\t\t\t\t\takkaDuration);\n\t\t\treturn (GetClusterStatusResponse) Await.result(clusterStatusOption, akkaDuration);\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Unable to get ClusterClient status from Application Client\", e);\n\t\t}\n\t}",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231  \n 232  \n 233 +\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "\t/**\n\t * This method is only available if the cluster hasn't been started in detached mode.\n\t */\n\t@Override\n\tpublic GetClusterStatusResponse getClusterStatus() {\n\t\tif (!isConnected) {\n\t\t\tthrow new IllegalStateException(\"The cluster is not connected to the cluster.\");\n\t\t}\n\t\tif (hasBeenShutdown()) {\n\t\t\tthrow new IllegalStateException(\"The cluster has already been shutdown.\");\n\t\t}\n\n\t\ttry {\n\t\t\tfinal Future<Object> clusterStatusOption =\n\t\t\t\tgetJobManagerGateway().ask(\n\t\t\t\t\tGetClusterStatus.getInstance(),\n\t\t\t\t\takkaDuration);\n\t\t\treturn (GetClusterStatusResponse) Await.result(clusterStatusOption, akkaDuration);\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Unable to get ClusterClient status from Application Client\", e);\n\t\t}\n\t}"
        ],
        [
            "YarnApplicationMasterRunnerTest::testCreateTaskExecutorContext()",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  ",
            "\t@Test\n\tpublic void testCreateTaskExecutorContext() throws Exception {\n\t\tFile root = folder.getRoot();\n\t\tFile home = new File(root, \"home\");\n\t\tboolean created = home.mkdir();\n\t\tassertTrue(created);\n\n\t\tAnswer<?> getDefault = new Answer<Object>() {\n\t\t\t@Override\n\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n\t\t\t\treturn invocationOnMock.getArguments()[1];\n\t\t\t}\n\t\t};\n\t\tConfiguration flinkConf = new Configuration();\n\t\tYarnConfiguration yarnConf = mock(YarnConfiguration.class);\n\t\tdoAnswer(getDefault).when(yarnConf).get(anyString(), anyString());\n\t\tdoAnswer(getDefault).when(yarnConf).getInt(anyString(), anyInt());\n\t\tdoAnswer(new Answer() {\n\t\t\t@Override\n\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n\t\t\t\treturn new String[] {(String) invocationOnMock.getArguments()[1]};\n\t\t\t}\n\t\t}).when(yarnConf).getStrings(anyString(), Mockito.<String> anyVararg());\n\n\t\tMap<String, String> env = ImmutableMap. <String, String> builder()\n\t\t\t.put(ENV_APP_ID, \"foo\")\n\t\t\t.put(ENV_CLIENT_HOME_DIR, home.getAbsolutePath())\n\t\t\t.put(ENV_CLIENT_SHIP_FILES, \"\")\n\t\t\t.put(ENV_FLINK_CLASSPATH, \"\")\n\t\t\t.put(ENV_HADOOP_USER_NAME, \"foo\")\n\t\t\t.put(FLINK_JAR_PATH, root.toURI().toString())\n\t\t\t.build();\n\t\tContaineredTaskManagerParameters tmParams = mock(ContaineredTaskManagerParameters.class);\n\t\tConfiguration taskManagerConf = new Configuration();\n\n\t\tString workingDirectory = root.getAbsolutePath();\n\t\tClass<?> taskManagerMainClass = YarnApplicationMasterRunnerTest.class;\n\t\tContainerLaunchContext ctx = Utils.createTaskExecutorContext(flinkConf, yarnConf, env, tmParams,\n\t\t\ttaskManagerConf, workingDirectory, taskManagerMainClass, LOG);\n\t\tassertEquals(\"file\", ctx.getLocalResources().get(\"flink.jar\").getResource().getScheme());\n\t}",
            "  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 +\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "\t@Test\n\tpublic void testCreateTaskExecutorContext() throws Exception {\n\t\tFile root = folder.getRoot();\n\t\tFile home = new File(root, \"home\");\n\t\tboolean created = home.mkdir();\n\t\tassertTrue(created);\n\n\t\tAnswer<?> getDefault = new Answer<Object>() {\n\t\t\t@Override\n\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n\t\t\t\treturn invocationOnMock.getArguments()[1];\n\t\t\t}\n\t\t};\n\t\tConfiguration flinkConf = new Configuration();\n\t\tYarnConfiguration yarnConf = mock(YarnConfiguration.class);\n\t\tdoAnswer(getDefault).when(yarnConf).get(anyString(), anyString());\n\t\tdoAnswer(getDefault).when(yarnConf).getInt(anyString(), anyInt());\n\t\tdoAnswer(new Answer() {\n\t\t\t@Override\n\t\t\tpublic Object answer(InvocationOnMock invocationOnMock) throws Throwable {\n\t\t\t\treturn new String[] {(String) invocationOnMock.getArguments()[1]};\n\t\t\t}\n\t\t}).when(yarnConf).getStrings(anyString(), Mockito.<String> anyVararg());\n\n\t\tMap<String, String> env = ImmutableMap.<String, String> builder()\n\t\t\t.put(ENV_APP_ID, \"foo\")\n\t\t\t.put(ENV_CLIENT_HOME_DIR, home.getAbsolutePath())\n\t\t\t.put(ENV_CLIENT_SHIP_FILES, \"\")\n\t\t\t.put(ENV_FLINK_CLASSPATH, \"\")\n\t\t\t.put(ENV_HADOOP_USER_NAME, \"foo\")\n\t\t\t.put(FLINK_JAR_PATH, root.toURI().toString())\n\t\t\t.build();\n\t\tContaineredTaskManagerParameters tmParams = mock(ContaineredTaskManagerParameters.class);\n\t\tConfiguration taskManagerConf = new Configuration();\n\n\t\tString workingDirectory = root.getAbsolutePath();\n\t\tClass<?> taskManagerMainClass = YarnApplicationMasterRunnerTest.class;\n\t\tContainerLaunchContext ctx = Utils.createTaskExecutorContext(flinkConf, yarnConf, env, tmParams,\n\t\t\ttaskManagerConf, workingDirectory, taskManagerMainClass, LOG);\n\t\tassertEquals(\"file\", ctx.getLocalResources().get(\"flink.jar\").getResource().getScheme());\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::hasUserJarFiles(List)",
            " 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277 -\n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "\t/**\n\t * Returns true if the descriptor has the job jars to include in the classpath.\n\t */\n\tpublic boolean hasUserJarFiles(List<URL> requiredJarFiles) {\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\treturn false;\n\t\t}\n\t\tif (userJarFiles.size() != requiredJarFiles.size()) {\n\t\t\treturn false;\n\t\t}\n\t\ttry {\n\t\t\tfor(URL jarFile : requiredJarFiles) {\n\t\t\t\tif (!userJarFiles.contains(new File(jarFile.toURI()))) {\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (URISyntaxException e) {\n\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t}",
            " 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285 +\n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  ",
            "\t/**\n\t * Returns true if the descriptor has the job jars to include in the classpath.\n\t */\n\tpublic boolean hasUserJarFiles(List<URL> requiredJarFiles) {\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\treturn false;\n\t\t}\n\t\tif (userJarFiles.size() != requiredJarFiles.size()) {\n\t\t\treturn false;\n\t\t}\n\t\ttry {\n\t\t\tfor (URL jarFile : requiredJarFiles) {\n\t\t\t\tif (!userJarFiles.contains(new File(jarFile.toURI()))) {\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (URISyntaxException e) {\n\t\t\treturn false;\n\t\t}\n\t\treturn true;\n\t}"
        ],
        [
            "Utils::createTaskExecutorContext(org,YarnConfiguration,Map,ContaineredTaskManagerParameters,org,String,Class,Logger)",
            " 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350 -\n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362 -\n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408 -\n 409  \n 410  \n 411  \n 412  \n 413 -\n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453 -\n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  ",
            "\t/**\n\t * Creates the launch context, which describes how to bring up a TaskExecutor / TaskManager process in\n\t * an allocated YARN container.\n\t *\n\t * <p>This code is extremely YARN specific and registers all the resources that the TaskExecutor\n\t * needs (such as JAR file, config file, ...) and all environment variables in a YARN\n\t * container launch context. The launch context then ensures that those resources will be\n\t * copied into the containers transient working directory.\n\t *\n\t * @param flinkConfig\n\t *\t\t The Flink configuration object.\n\t * @param yarnConfig\n\t *\t\t The YARN configuration object.\n\t * @param env\n\t *\t\t The environment variables.\n\t * @param tmParams\n\t *\t\t The TaskExecutor container memory parameters.\n\t * @param taskManagerConfig\n\t *\t\t The configuration for the TaskExecutors.\n\t * @param workingDirectory\n\t *\t\t The current application master container's working directory.\n\t * @param taskManagerMainClass\n\t *\t\t The class with the main method.\n\t * @param log\n\t *\t\t The logger.\n\t *\n\t * @return The launch context for the TaskManager processes.\n\t *\n\t * @throws Exception Thrown if teh launch context could not be created, for example if\n\t *\t\t\t\t   the resources could not be copied.\n\t */\n\tstatic ContainerLaunchContext createTaskExecutorContext(\n\t\torg.apache.flink.configuration.Configuration flinkConfig,\n\t\tYarnConfiguration yarnConfig,\n\t\tMap<String, String> env,\n\t\tContaineredTaskManagerParameters tmParams,\n\t\torg.apache.flink.configuration.Configuration taskManagerConfig,\n\t\tString workingDirectory,\n\t\tClass<?> taskManagerMainClass,\n\t\tLogger log) throws Exception {\n\n\t\t// get and validate all relevant variables\n\n\t\tString remoteFlinkJarPath = env.get(YarnConfigKeys.FLINK_JAR_PATH);\n\t\trequire(remoteFlinkJarPath != null, \"Environment variable %s not set\", YarnConfigKeys.FLINK_JAR_PATH);\n\n\t\tString appId = env.get(YarnConfigKeys.ENV_APP_ID);\n\t\trequire(appId != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_APP_ID);\n\n\t\tString clientHomeDir = env.get(YarnConfigKeys.ENV_CLIENT_HOME_DIR);\n\t\trequire(clientHomeDir != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_CLIENT_HOME_DIR);\n\n\t\tString shipListString = env.get(YarnConfigKeys.ENV_CLIENT_SHIP_FILES);\n\t\trequire(shipListString != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_CLIENT_SHIP_FILES);\n\n\t\tString yarnClientUsername = env.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\trequire(yarnClientUsername != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\n\t\tfinal String remoteKeytabPath = env.get(YarnConfigKeys.KEYTAB_PATH);\n\t\tlog.info(\"TM:remote keytab path obtained {}\", remoteKeytabPath);\n\n\t\tfinal String remoteKeytabPrincipal = env.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\tlog.info(\"TM:remote keytab principal obtained {}\", remoteKeytabPrincipal);\n\n\t\tfinal String remoteYarnConfPath = env.get(YarnConfigKeys.ENV_YARN_SITE_XML_PATH);\n\t\tlog.info(\"TM:remote yarn conf path obtained {}\", remoteYarnConfPath);\n\n\t\tfinal String remoteKrb5Path = env.get(YarnConfigKeys.ENV_KRB5_PATH);\n\t\tlog.info(\"TM:remote krb5 path obtained {}\", remoteKrb5Path);\n\n\t\tString classPathString = env.get(ENV_FLINK_CLASSPATH);\n\t\trequire(classPathString != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_FLINK_CLASSPATH);\n\n\t\t//register keytab\n\t\tLocalResource keytabResource = null;\n\t\tif(remoteKeytabPath != null) {\n\t\t\tlog.info(\"Adding keytab {} to the AM container local resource bucket\", remoteKeytabPath);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(remoteKeytabPath);\n\t\t\tFileSystem fs = keytabPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, keytabPath, keytabResource);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tLocalResource yarnConfResource = null;\n\t\tLocalResource krb5ConfResource = null;\n\t\tboolean hasKrb5 = false;\n\t\tif(remoteYarnConfPath != null && remoteKrb5Path != null) {\n\t\t\tlog.info(\"TM:Adding remoteYarnConfPath {} to the container local resource bucket\", remoteYarnConfPath);\n\t\t\tyarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\tPath yarnConfPath = new Path(remoteYarnConfPath);\n\t\t\tFileSystem fs = yarnConfPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, yarnConfPath, yarnConfResource);\n\n\t\t\tlog.info(\"TM:Adding remoteKrb5Path {} to the container local resource bucket\", remoteKrb5Path);\n\t\t\tkrb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\tPath krb5ConfPath = new Path(remoteKrb5Path);\n\t\t\tfs = krb5ConfPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, krb5ConfPath, krb5ConfResource);\n\n\t\t\thasKrb5 = true;\n\t\t}\n\n\t\t// register Flink Jar with remote HDFS\n\t\tLocalResource flinkJar = Records.newRecord(LocalResource.class);\n\t\t{\n\t\t\tPath remoteJarPath = new Path(remoteFlinkJarPath);\n\t\t\tFileSystem fs = remoteJarPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, remoteJarPath, flinkJar);\n\t\t}\n\n\t\t// register conf with local fs\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\t{\n\t\t\t// write the TaskManager configuration to a local file\n\t\t\tfinal File taskManagerConfigFile =\n\t\t\t\t\tnew File(workingDirectory, UUID.randomUUID() + \"-taskmanager-conf.yaml\");\n\t\t\tlog.debug(\"Writing TaskManager configuration to {}\", taskManagerConfigFile.getAbsolutePath());\n\t\t\tBootstrapTools.writeConfiguration(taskManagerConfig, taskManagerConfigFile);\n\n\t\t\tPath homeDirPath = new Path(clientHomeDir);\n\t\t\tFileSystem fs = homeDirPath.getFileSystem(yarnConfig);\n\t\t\tsetupLocalResource(fs, appId,\n\t\t\t\t\tnew Path(taskManagerConfigFile.toURI()), flinkConf, new Path(clientHomeDir));\n\n\t\t\tlog.info(\"Prepared local resource for modified yaml: {}\", flinkConf);\n\t\t}\n\n\t\tMap<String, LocalResource> taskManagerLocalResources = new HashMap<>();\n\t\ttaskManagerLocalResources.put(\"flink.jar\", flinkJar);\n\t\ttaskManagerLocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif(yarnConfResource != null && krb5ConfResource != null) {\n\t\t\ttaskManagerLocalResources.put(YARN_SITE_FILE_NAME, yarnConfResource);\n\t\t\ttaskManagerLocalResources.put(KRB5_FILE_NAME, krb5ConfResource);\n\t\t}\n\n\t\tif(keytabResource != null) {\n\t\t\ttaskManagerLocalResources.put(KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\t// prepare additional files to be shipped\n\t\tfor (String pathStr : shipListString.split(\",\")) {\n\t\t\tif (!pathStr.isEmpty()) {\n\t\t\t\tLocalResource resource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath path = new Path(pathStr);\n\t\t\t\tregisterLocalResource(path.getFileSystem(yarnConfig), path, resource);\n\t\t\t\ttaskManagerLocalResources.put(path.getName(), resource);\n\t\t\t}\n\t\t}\n\n\t\t// now that all resources are prepared, we can create the launch context\n\n\t\tlog.info(\"Creating container launch context for TaskManagers\");\n\n\t\tboolean hasLogback = new File(workingDirectory, \"logback.xml\").exists();\n\t\tboolean hasLog4j = new File(workingDirectory, \"log4j.properties\").exists();\n\n\t\tString launchCommand = BootstrapTools.getTaskManagerShellCommand(\n\t\t\t\tflinkConfig, tmParams, \".\", ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n\t\t\t\thasLogback, hasLog4j, hasKrb5, taskManagerMainClass);\n\n\t\tlog.info(\"Starting TaskManagers with command: \" + launchCommand);\n\n\t\tContainerLaunchContext ctx = Records.newRecord(ContainerLaunchContext.class);\n\t\tctx.setCommands(Collections.singletonList(launchCommand));\n\t\tctx.setLocalResources(taskManagerLocalResources);\n\n\t\tMap<String, String> containerEnv = new HashMap<>();\n\t\tcontainerEnv.putAll(tmParams.taskManagerEnv());\n\n\t\t// add YARN classpath, etc to the container environment\n\t\tcontainerEnv.put(ENV_FLINK_CLASSPATH, classPathString);\n\t\tsetupYarnClassPath(yarnConfig, containerEnv);\n\n\t\tcontainerEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif(remoteKeytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\tcontainerEnv.put(YarnConfigKeys.KEYTAB_PATH, remoteKeytabPath);\n\t\t\tcontainerEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, remoteKeytabPrincipal);\n\t\t}\n\n\t\tctx.setEnvironment(containerEnv);\n\n\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n\t\t\tlog.debug(\"Adding security tokens to Task Executor Container launch Context....\");\n\t\t\tUserGroupInformation user = UserGroupInformation.getCurrentUser();\n\t\t\tCredentials credentials = user.getCredentials();\n\t\t\tcredentials.writeTokenStorageToStream(dob);\n\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\t\t\tctx.setTokens(securityTokens);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tlog.error(\"Getting current user info failed when trying to launch the container\", t);\n\t\t}\n\n\t\treturn ctx;\n\t}",
            " 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346 +\n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358 +\n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404 +\n 405  \n 406  \n 407  \n 408  \n 409 +\n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 +\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  ",
            "\t/**\n\t * Creates the launch context, which describes how to bring up a TaskExecutor / TaskManager process in\n\t * an allocated YARN container.\n\t *\n\t * <p>This code is extremely YARN specific and registers all the resources that the TaskExecutor\n\t * needs (such as JAR file, config file, ...) and all environment variables in a YARN\n\t * container launch context. The launch context then ensures that those resources will be\n\t * copied into the containers transient working directory.\n\t *\n\t * @param flinkConfig\n\t *\t\t The Flink configuration object.\n\t * @param yarnConfig\n\t *\t\t The YARN configuration object.\n\t * @param env\n\t *\t\t The environment variables.\n\t * @param tmParams\n\t *\t\t The TaskExecutor container memory parameters.\n\t * @param taskManagerConfig\n\t *\t\t The configuration for the TaskExecutors.\n\t * @param workingDirectory\n\t *\t\t The current application master container's working directory.\n\t * @param taskManagerMainClass\n\t *\t\t The class with the main method.\n\t * @param log\n\t *\t\t The logger.\n\t *\n\t * @return The launch context for the TaskManager processes.\n\t *\n\t * @throws Exception Thrown if teh launch context could not be created, for example if\n\t *\t\t\t\t   the resources could not be copied.\n\t */\n\tstatic ContainerLaunchContext createTaskExecutorContext(\n\t\torg.apache.flink.configuration.Configuration flinkConfig,\n\t\tYarnConfiguration yarnConfig,\n\t\tMap<String, String> env,\n\t\tContaineredTaskManagerParameters tmParams,\n\t\torg.apache.flink.configuration.Configuration taskManagerConfig,\n\t\tString workingDirectory,\n\t\tClass<?> taskManagerMainClass,\n\t\tLogger log) throws Exception {\n\n\t\t// get and validate all relevant variables\n\n\t\tString remoteFlinkJarPath = env.get(YarnConfigKeys.FLINK_JAR_PATH);\n\t\trequire(remoteFlinkJarPath != null, \"Environment variable %s not set\", YarnConfigKeys.FLINK_JAR_PATH);\n\n\t\tString appId = env.get(YarnConfigKeys.ENV_APP_ID);\n\t\trequire(appId != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_APP_ID);\n\n\t\tString clientHomeDir = env.get(YarnConfigKeys.ENV_CLIENT_HOME_DIR);\n\t\trequire(clientHomeDir != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_CLIENT_HOME_DIR);\n\n\t\tString shipListString = env.get(YarnConfigKeys.ENV_CLIENT_SHIP_FILES);\n\t\trequire(shipListString != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_CLIENT_SHIP_FILES);\n\n\t\tString yarnClientUsername = env.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\trequire(yarnClientUsername != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\n\t\tfinal String remoteKeytabPath = env.get(YarnConfigKeys.KEYTAB_PATH);\n\t\tlog.info(\"TM:remote keytab path obtained {}\", remoteKeytabPath);\n\n\t\tfinal String remoteKeytabPrincipal = env.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\tlog.info(\"TM:remote keytab principal obtained {}\", remoteKeytabPrincipal);\n\n\t\tfinal String remoteYarnConfPath = env.get(YarnConfigKeys.ENV_YARN_SITE_XML_PATH);\n\t\tlog.info(\"TM:remote yarn conf path obtained {}\", remoteYarnConfPath);\n\n\t\tfinal String remoteKrb5Path = env.get(YarnConfigKeys.ENV_KRB5_PATH);\n\t\tlog.info(\"TM:remote krb5 path obtained {}\", remoteKrb5Path);\n\n\t\tString classPathString = env.get(ENV_FLINK_CLASSPATH);\n\t\trequire(classPathString != null, \"Environment variable %s not set\", YarnConfigKeys.ENV_FLINK_CLASSPATH);\n\n\t\t//register keytab\n\t\tLocalResource keytabResource = null;\n\t\tif (remoteKeytabPath != null) {\n\t\t\tlog.info(\"Adding keytab {} to the AM container local resource bucket\", remoteKeytabPath);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(remoteKeytabPath);\n\t\t\tFileSystem fs = keytabPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, keytabPath, keytabResource);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tLocalResource yarnConfResource = null;\n\t\tLocalResource krb5ConfResource = null;\n\t\tboolean hasKrb5 = false;\n\t\tif (remoteYarnConfPath != null && remoteKrb5Path != null) {\n\t\t\tlog.info(\"TM:Adding remoteYarnConfPath {} to the container local resource bucket\", remoteYarnConfPath);\n\t\t\tyarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\tPath yarnConfPath = new Path(remoteYarnConfPath);\n\t\t\tFileSystem fs = yarnConfPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, yarnConfPath, yarnConfResource);\n\n\t\t\tlog.info(\"TM:Adding remoteKrb5Path {} to the container local resource bucket\", remoteKrb5Path);\n\t\t\tkrb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\tPath krb5ConfPath = new Path(remoteKrb5Path);\n\t\t\tfs = krb5ConfPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, krb5ConfPath, krb5ConfResource);\n\n\t\t\thasKrb5 = true;\n\t\t}\n\n\t\t// register Flink Jar with remote HDFS\n\t\tLocalResource flinkJar = Records.newRecord(LocalResource.class);\n\t\t{\n\t\t\tPath remoteJarPath = new Path(remoteFlinkJarPath);\n\t\t\tFileSystem fs = remoteJarPath.getFileSystem(yarnConfig);\n\t\t\tregisterLocalResource(fs, remoteJarPath, flinkJar);\n\t\t}\n\n\t\t// register conf with local fs\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\t{\n\t\t\t// write the TaskManager configuration to a local file\n\t\t\tfinal File taskManagerConfigFile =\n\t\t\t\t\tnew File(workingDirectory, UUID.randomUUID() + \"-taskmanager-conf.yaml\");\n\t\t\tlog.debug(\"Writing TaskManager configuration to {}\", taskManagerConfigFile.getAbsolutePath());\n\t\t\tBootstrapTools.writeConfiguration(taskManagerConfig, taskManagerConfigFile);\n\n\t\t\tPath homeDirPath = new Path(clientHomeDir);\n\t\t\tFileSystem fs = homeDirPath.getFileSystem(yarnConfig);\n\t\t\tsetupLocalResource(fs, appId,\n\t\t\t\t\tnew Path(taskManagerConfigFile.toURI()), flinkConf, new Path(clientHomeDir));\n\n\t\t\tlog.info(\"Prepared local resource for modified yaml: {}\", flinkConf);\n\t\t}\n\n\t\tMap<String, LocalResource> taskManagerLocalResources = new HashMap<>();\n\t\ttaskManagerLocalResources.put(\"flink.jar\", flinkJar);\n\t\ttaskManagerLocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif (yarnConfResource != null && krb5ConfResource != null) {\n\t\t\ttaskManagerLocalResources.put(YARN_SITE_FILE_NAME, yarnConfResource);\n\t\t\ttaskManagerLocalResources.put(KRB5_FILE_NAME, krb5ConfResource);\n\t\t}\n\n\t\tif (keytabResource != null) {\n\t\t\ttaskManagerLocalResources.put(KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\t// prepare additional files to be shipped\n\t\tfor (String pathStr : shipListString.split(\",\")) {\n\t\t\tif (!pathStr.isEmpty()) {\n\t\t\t\tLocalResource resource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath path = new Path(pathStr);\n\t\t\t\tregisterLocalResource(path.getFileSystem(yarnConfig), path, resource);\n\t\t\t\ttaskManagerLocalResources.put(path.getName(), resource);\n\t\t\t}\n\t\t}\n\n\t\t// now that all resources are prepared, we can create the launch context\n\n\t\tlog.info(\"Creating container launch context for TaskManagers\");\n\n\t\tboolean hasLogback = new File(workingDirectory, \"logback.xml\").exists();\n\t\tboolean hasLog4j = new File(workingDirectory, \"log4j.properties\").exists();\n\n\t\tString launchCommand = BootstrapTools.getTaskManagerShellCommand(\n\t\t\t\tflinkConfig, tmParams, \".\", ApplicationConstants.LOG_DIR_EXPANSION_VAR,\n\t\t\t\thasLogback, hasLog4j, hasKrb5, taskManagerMainClass);\n\n\t\tlog.info(\"Starting TaskManagers with command: \" + launchCommand);\n\n\t\tContainerLaunchContext ctx = Records.newRecord(ContainerLaunchContext.class);\n\t\tctx.setCommands(Collections.singletonList(launchCommand));\n\t\tctx.setLocalResources(taskManagerLocalResources);\n\n\t\tMap<String, String> containerEnv = new HashMap<>();\n\t\tcontainerEnv.putAll(tmParams.taskManagerEnv());\n\n\t\t// add YARN classpath, etc to the container environment\n\t\tcontainerEnv.put(ENV_FLINK_CLASSPATH, classPathString);\n\t\tsetupYarnClassPath(yarnConfig, containerEnv);\n\n\t\tcontainerEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif (remoteKeytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\tcontainerEnv.put(YarnConfigKeys.KEYTAB_PATH, remoteKeytabPath);\n\t\t\tcontainerEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, remoteKeytabPrincipal);\n\t\t}\n\n\t\tctx.setEnvironment(containerEnv);\n\n\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n\t\t\tlog.debug(\"Adding security tokens to Task Executor Container launch Context....\");\n\t\t\tUserGroupInformation user = UserGroupInformation.getCurrentUser();\n\t\t\tCredentials credentials = user.getCredentials();\n\t\t\tcredentials.writeTokenStorageToStream(dob);\n\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\t\t\tctx.setTokens(securityTokens);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tlog.error(\"Getting current user info failed when trying to launch the container\", t);\n\t\t}\n\n\t\treturn ctx;\n\t}"
        ],
        [
            "YarnClusterClient::getNewMessages()",
            " 275  \n 276  \n 277  \n 278 -\n 279  \n 280  \n 281  \n 282 -\n 283  \n 284  \n 285  \n 286  \n 287  \n 288 -\n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297 -\n 298  \n 299  \n 300  \n 301  \n 302 -\n 303  \n 304  \n 305  \n 306  \n 307  \n 308 -\n 309  \n 310  \n 311  \n 312  \n 313 -\n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  ",
            "\t@Override\n\tpublic List<String> getNewMessages() {\n\n\t\tif(hasBeenShutdown()) {\n\t\t\tthrow new RuntimeException(\"The YarnClusterClient has already been stopped\");\n\t\t}\n\n\t\tif(!isConnected) {\n\t\t\tthrow new IllegalStateException(\"The cluster has been connected to the ApplicationMaster.\");\n\t\t}\n\n\t\tList<String> ret = new ArrayList<String>();\n\t\t// get messages from ApplicationClient (locally)\n\t\twhile(true) {\n\t\t\tObject result;\n\t\t\ttry {\n\t\t\t\tFuture<Object> response =\n\t\t\t\t\tPatterns.ask(\n\t\t\t\t\t\tapplicationClient.get(),\n\t\t\t\t\t\tYarnMessages.getLocalGetYarnMessage(),\n\t\t\t\t\t\tnew Timeout(akkaDuration));\n\t\t\t\tresult = Await.result(response, akkaDuration);\n\t\t\t} catch(Exception ioe) {\n\t\t\t\tLOG.warn(\"Error retrieving the YARN messages locally\", ioe);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif(!(result instanceof Option)) {\n\t\t\t\tthrow new RuntimeException(\"LocalGetYarnMessage requires a response of type \" +\n\t\t\t\t\t\t\"Option. Instead the response is of type \" + result.getClass() + \".\");\n\t\t\t} else {\n\t\t\t\tOption messageOption = (Option) result;\n\t\t\t\tLOG.debug(\"Received message option {}\", messageOption);\n\t\t\t\tif(messageOption.isEmpty()) {\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tObject obj = messageOption.get();\n\n\t\t\t\t\tif(obj instanceof InfoMessage) {\n\t\t\t\t\t\tInfoMessage msg = (InfoMessage) obj;\n\t\t\t\t\t\tret.add(\"[\" + msg.date() + \"] \" + msg.message());\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLOG.warn(\"LocalGetYarnMessage returned unexpected type: \" + messageOption);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn ret;\n\t}",
            " 276  \n 277  \n 278  \n 279 +\n 280  \n 281  \n 282  \n 283 +\n 284  \n 285  \n 286  \n 287  \n 288  \n 289 +\n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298 +\n 299  \n 300  \n 301  \n 302  \n 303 +\n 304  \n 305  \n 306  \n 307  \n 308  \n 309 +\n 310  \n 311  \n 312  \n 313  \n 314 +\n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  ",
            "\t@Override\n\tpublic List<String> getNewMessages() {\n\n\t\tif (hasBeenShutdown()) {\n\t\t\tthrow new RuntimeException(\"The YarnClusterClient has already been stopped\");\n\t\t}\n\n\t\tif (!isConnected) {\n\t\t\tthrow new IllegalStateException(\"The cluster has been connected to the ApplicationMaster.\");\n\t\t}\n\n\t\tList<String> ret = new ArrayList<String>();\n\t\t// get messages from ApplicationClient (locally)\n\t\twhile (true) {\n\t\t\tObject result;\n\t\t\ttry {\n\t\t\t\tFuture<Object> response =\n\t\t\t\t\tPatterns.ask(\n\t\t\t\t\t\tapplicationClient.get(),\n\t\t\t\t\t\tYarnMessages.getLocalGetYarnMessage(),\n\t\t\t\t\t\tnew Timeout(akkaDuration));\n\t\t\t\tresult = Await.result(response, akkaDuration);\n\t\t\t} catch (Exception ioe) {\n\t\t\t\tLOG.warn(\"Error retrieving the YARN messages locally\", ioe);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (!(result instanceof Option)) {\n\t\t\t\tthrow new RuntimeException(\"LocalGetYarnMessage requires a response of type \" +\n\t\t\t\t\t\t\"Option. Instead the response is of type \" + result.getClass() + \".\");\n\t\t\t} else {\n\t\t\t\tOption messageOption = (Option) result;\n\t\t\t\tLOG.debug(\"Received message option {}\", messageOption);\n\t\t\t\tif (messageOption.isEmpty()) {\n\t\t\t\t\tbreak;\n\t\t\t\t} else {\n\t\t\t\t\tObject obj = messageOption.get();\n\n\t\t\t\t\tif (obj instanceof InfoMessage) {\n\t\t\t\t\t\tInfoMessage msg = (InfoMessage) obj;\n\t\t\t\t\t\tret.add(\"[\" + msg.date() + \"] \" + msg.message());\n\t\t\t\t\t} else {\n\t\t\t\t\t\tLOG.warn(\"LocalGetYarnMessage returned unexpected type: \" + messageOption);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn ret;\n\t}"
        ],
        [
            "YarnFlinkResourceManager::createActorProps(Class,Configuration,YarnConfiguration,LeaderRetrievalService,String,String,ContaineredTaskManagerParameters,ContainerLaunchContext,int,Logger)",
            " 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709 -\n 710 -\n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  ",
            "\t/**\n\t * Creates the props needed to instantiate this actor.\n\t * \n\t * Rather than extracting and validating parameters in the constructor, this factory method takes\n\t * care of that. That way, errors occur synchronously, and are not swallowed simply in a\n\t * failed asynchronous attempt to start the actor.\n\t \n\t * @param actorClass \n\t *             The actor class, to allow overriding this actor with subclasses for testing.\n\t * @param flinkConfig\n\t *             The Flink configuration object.\n\t * @param yarnConfig\n\t *             The YARN configuration object.\n\t * @param applicationMasterHostName\n\t *             The hostname where this application master actor runs.\n\t * @param webFrontendURL\n\t *             The URL of the tracking web frontend.\n\t * @param taskManagerParameters\n\t *             The parameters for launching TaskManager containers.\n\t * @param taskManagerLaunchContext\n\t *             The parameters for launching the TaskManager processes in the TaskManager containers.\n\t * @param numInitialTaskManagers\n\t *             The initial number of TaskManagers to allocate.\n\t * @param log\n\t *             The logger to log to.\n\t * \n\t * @return The Props object to instantiate the YarnFlinkResourceManager actor.\n\t */\n\tpublic static Props createActorProps(Class<? extends YarnFlinkResourceManager> actorClass,\n\t\t\tConfiguration flinkConfig,\n\t\t\tYarnConfiguration yarnConfig,\n\t\t\tLeaderRetrievalService leaderRetrievalService,\n\t\t\tString applicationMasterHostName,\n\t\t\tString webFrontendURL,\n\t\t\tContaineredTaskManagerParameters taskManagerParameters,\n\t\t\tContainerLaunchContext taskManagerLaunchContext,\n\t\t\tint numInitialTaskManagers,\n\t\t\tLogger log)\n\t{\n\t\tfinal int yarnHeartbeatIntervalMS = flinkConfig.getInteger(\n\t\t\tConfigConstants.YARN_HEARTBEAT_DELAY_SECONDS, DEFAULT_YARN_HEARTBEAT_INTERVAL_MS / 1000) * 1000;\n\n\t\tfinal long yarnExpiryIntervalMS = yarnConfig.getLong(\n\t\t\tYarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS,\n\t\t\tYarnConfiguration.DEFAULT_RM_AM_EXPIRY_INTERVAL_MS);\n\n\t\tif (yarnHeartbeatIntervalMS >= yarnExpiryIntervalMS) {\n\t\t\tlog.warn(\"The heartbeat interval of the Flink Application master ({}) is greater \" +\n\t\t\t\t\t\"than YARN's expiry interval ({}). The application is likely to be killed by YARN.\",\n\t\t\t\tyarnHeartbeatIntervalMS, yarnExpiryIntervalMS);\n\t\t}\n\n\t\tfinal int maxFailedContainers = flinkConfig.getInteger(\n\t\t\tConfigConstants.YARN_MAX_FAILED_CONTAINERS, numInitialTaskManagers);\n\t\tif (maxFailedContainers >= 0) {\n\t\t\tlog.info(\"YARN application tolerates {} failed TaskManager containers before giving up\",\n\t\t\t\tmaxFailedContainers);\n\t\t}\n\n\t\treturn Props.create(actorClass,\n\t\t\tflinkConfig,\n\t\t\tyarnConfig,\n\t\t\tleaderRetrievalService,\n\t\t\tapplicationMasterHostName,\n\t\t\twebFrontendURL,\n\t\t\ttaskManagerParameters,\n\t\t\ttaskManagerLaunchContext,\n\t\t\tyarnHeartbeatIntervalMS,\n\t\t\tmaxFailedContainers,\n\t\t\tnumInitialTaskManagers);\n\t}",
            " 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707 +\n 708 +\n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  ",
            "\t/**\n\t * Creates the props needed to instantiate this actor.\n\t *\n\t * <p>Rather than extracting and validating parameters in the constructor, this factory method takes\n\t * care of that. That way, errors occur synchronously, and are not swallowed simply in a\n\t * failed asynchronous attempt to start the actor.\n\t *\n\t * @param actorClass\n\t *             The actor class, to allow overriding this actor with subclasses for testing.\n\t * @param flinkConfig\n\t *             The Flink configuration object.\n\t * @param yarnConfig\n\t *             The YARN configuration object.\n\t * @param applicationMasterHostName\n\t *             The hostname where this application master actor runs.\n\t * @param webFrontendURL\n\t *             The URL of the tracking web frontend.\n\t * @param taskManagerParameters\n\t *             The parameters for launching TaskManager containers.\n\t * @param taskManagerLaunchContext\n\t *             The parameters for launching the TaskManager processes in the TaskManager containers.\n\t * @param numInitialTaskManagers\n\t *             The initial number of TaskManagers to allocate.\n\t * @param log\n\t *             The logger to log to.\n\t *\n\t * @return The Props object to instantiate the YarnFlinkResourceManager actor.\n\t */\n\tpublic static Props createActorProps(Class<? extends YarnFlinkResourceManager> actorClass,\n\t\t\tConfiguration flinkConfig,\n\t\t\tYarnConfiguration yarnConfig,\n\t\t\tLeaderRetrievalService leaderRetrievalService,\n\t\t\tString applicationMasterHostName,\n\t\t\tString webFrontendURL,\n\t\t\tContaineredTaskManagerParameters taskManagerParameters,\n\t\t\tContainerLaunchContext taskManagerLaunchContext,\n\t\t\tint numInitialTaskManagers,\n\t\t\tLogger log) {\n\n\t\tfinal int yarnHeartbeatIntervalMS = flinkConfig.getInteger(\n\t\t\tConfigConstants.YARN_HEARTBEAT_DELAY_SECONDS, DEFAULT_YARN_HEARTBEAT_INTERVAL_MS / 1000) * 1000;\n\n\t\tfinal long yarnExpiryIntervalMS = yarnConfig.getLong(\n\t\t\tYarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS,\n\t\t\tYarnConfiguration.DEFAULT_RM_AM_EXPIRY_INTERVAL_MS);\n\n\t\tif (yarnHeartbeatIntervalMS >= yarnExpiryIntervalMS) {\n\t\t\tlog.warn(\"The heartbeat interval of the Flink Application master ({}) is greater \" +\n\t\t\t\t\t\"than YARN's expiry interval ({}). The application is likely to be killed by YARN.\",\n\t\t\t\tyarnHeartbeatIntervalMS, yarnExpiryIntervalMS);\n\t\t}\n\n\t\tfinal int maxFailedContainers = flinkConfig.getInteger(\n\t\t\tConfigConstants.YARN_MAX_FAILED_CONTAINERS, numInitialTaskManagers);\n\t\tif (maxFailedContainers >= 0) {\n\t\t\tlog.info(\"YARN application tolerates {} failed TaskManager containers before giving up\",\n\t\t\t\tmaxFailedContainers);\n\t\t}\n\n\t\treturn Props.create(actorClass,\n\t\t\tflinkConfig,\n\t\t\tyarnConfig,\n\t\t\tleaderRetrievalService,\n\t\t\tapplicationMasterHostName,\n\t\t\twebFrontendURL,\n\t\t\ttaskManagerParameters,\n\t\t\ttaskManagerLaunchContext,\n\t\t\tyarnHeartbeatIntervalMS,\n\t\t\tmaxFailedContainers,\n\t\t\tnumInitialTaskManagers);\n\t}"
        ],
        [
            "FlinkYarnSessionCli::runInteractiveCli(YarnClusterClient,boolean)",
            " 405  \n 406 -\n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446 -\n 447 -\n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461 -\n 462  \n 463  \n 464 -\n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474 -\n 475  \n 476  \n 477  ",
            "\tpublic static void runInteractiveCli(YarnClusterClient yarnCluster, boolean readConsoleInput) {\n\t\tfinal String HELP = \"Available commands:\\n\" +\n\t\t\t\t\"help - show these commands\\n\" +\n\t\t\t\t\"stop - stop the YARN session\";\n\t\tint numTaskmanagers = 0;\n\t\ttry {\n\t\t\tBufferedReader in = new BufferedReader(new InputStreamReader(System.in));\n\t\t\tlabel:\n\t\t\twhile (true) {\n\t\t\t\t// ------------------ check if there are updates by the cluster -----------\n\n\t\t\t\ttry {\n\t\t\t\t\tGetClusterStatusResponse status = yarnCluster.getClusterStatus();\n\t\t\t\t\tLOG.debug(\"Received status message: {}\", status);\n\n\t\t\t\t\tif (status != null && numTaskmanagers != status.numRegisteredTaskManagers()) {\n\t\t\t\t\t\tSystem.err.println(\"Number of connected TaskManagers changed to \" +\n\t\t\t\t\t\t\tstatus.numRegisteredTaskManagers() + \". \" +\n\t\t\t\t\t\t\t\"Slots available: \" + status.totalNumberOfSlots());\n\t\t\t\t\t\tnumTaskmanagers = status.numRegisteredTaskManagers();\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tLOG.warn(\"Could not retrieve the current cluster status. Skipping current retrieval attempt ...\", e);\n\t\t\t\t}\n\n\t\t\t\tList<String> messages = yarnCluster.getNewMessages();\n\t\t\t\tif (messages != null && messages.size() > 0) {\n\t\t\t\t\tSystem.err.println(\"New messages from the YARN cluster: \");\n\t\t\t\t\tfor (String msg : messages) {\n\t\t\t\t\t\tSystem.err.println(msg);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (yarnCluster.getApplicationStatus() != ApplicationStatus.SUCCEEDED) {\n\t\t\t\t\tSystem.err.println(\"The YARN cluster has failed\");\n\t\t\t\t\tyarnCluster.shutdown();\n\t\t\t\t}\n\n\t\t\t\t// wait until CLIENT_POLLING_INTERVAL is over or the user entered something.\n\t\t\t\tlong startTime = System.currentTimeMillis();\n\t\t\t\twhile ((System.currentTimeMillis() - startTime) < CLIENT_POLLING_INTERVALL * 1000\n\t\t\t\t\t\t&& (!readConsoleInput || !in.ready()))\n\t\t\t\t{\n\t\t\t\t\tThread.sleep(200);\n\t\t\t\t}\n\t\t\t\t//------------- handle interactive command by user. ----------------------\n\n\t\t\t\tif (readConsoleInput && in.ready()) {\n\t\t\t\t\tString command = in.readLine();\n\t\t\t\t\tswitch (command) {\n\t\t\t\t\t\tcase \"quit\":\n\t\t\t\t\t\tcase \"stop\":\n\t\t\t\t\t\t\tyarnCluster.shutdownCluster();\n\t\t\t\t\t\t\tbreak label;\n\n\t\t\t\t\t\tcase \"help\":\n\t\t\t\t\t\t\tSystem.err.println(HELP);\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tSystem.err.println(\"Unknown command '\" + command + \"'. Showing help: \\n\" + HELP);\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (yarnCluster.hasBeenShutdown()) {\n\t\t\t\t\tLOG.info(\"Stopping interactive command line interface, YARN cluster has been stopped.\");\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} catch(Exception e) {\n\t\t\tLOG.warn(\"Exception while running the interactive command line interface\", e);\n\t\t}\n\t}",
            " 406  \n 407 +\n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447 +\n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461 +\n 462  \n 463  \n 464 +\n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474 +\n 475  \n 476  \n 477  ",
            "\tpublic static void runInteractiveCli(YarnClusterClient yarnCluster, boolean readConsoleInput) {\n\t\tfinal String help = \"Available commands:\\n\" +\n\t\t\t\t\"help - show these commands\\n\" +\n\t\t\t\t\"stop - stop the YARN session\";\n\t\tint numTaskmanagers = 0;\n\t\ttry {\n\t\t\tBufferedReader in = new BufferedReader(new InputStreamReader(System.in));\n\t\t\tlabel:\n\t\t\twhile (true) {\n\t\t\t\t// ------------------ check if there are updates by the cluster -----------\n\n\t\t\t\ttry {\n\t\t\t\t\tGetClusterStatusResponse status = yarnCluster.getClusterStatus();\n\t\t\t\t\tLOG.debug(\"Received status message: {}\", status);\n\n\t\t\t\t\tif (status != null && numTaskmanagers != status.numRegisteredTaskManagers()) {\n\t\t\t\t\t\tSystem.err.println(\"Number of connected TaskManagers changed to \" +\n\t\t\t\t\t\t\tstatus.numRegisteredTaskManagers() + \". \" +\n\t\t\t\t\t\t\t\"Slots available: \" + status.totalNumberOfSlots());\n\t\t\t\t\t\tnumTaskmanagers = status.numRegisteredTaskManagers();\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tLOG.warn(\"Could not retrieve the current cluster status. Skipping current retrieval attempt ...\", e);\n\t\t\t\t}\n\n\t\t\t\tList<String> messages = yarnCluster.getNewMessages();\n\t\t\t\tif (messages != null && messages.size() > 0) {\n\t\t\t\t\tSystem.err.println(\"New messages from the YARN cluster: \");\n\t\t\t\t\tfor (String msg : messages) {\n\t\t\t\t\t\tSystem.err.println(msg);\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (yarnCluster.getApplicationStatus() != ApplicationStatus.SUCCEEDED) {\n\t\t\t\t\tSystem.err.println(\"The YARN cluster has failed\");\n\t\t\t\t\tyarnCluster.shutdown();\n\t\t\t\t}\n\n\t\t\t\t// wait until CLIENT_POLLING_INTERVAL is over or the user entered something.\n\t\t\t\tlong startTime = System.currentTimeMillis();\n\t\t\t\twhile ((System.currentTimeMillis() - startTime) < CLIENT_POLLING_INTERVALL * 1000\n\t\t\t\t\t\t&& (!readConsoleInput || !in.ready())) {\n\t\t\t\t\tThread.sleep(200);\n\t\t\t\t}\n\t\t\t\t//------------- handle interactive command by user. ----------------------\n\n\t\t\t\tif (readConsoleInput && in.ready()) {\n\t\t\t\t\tString command = in.readLine();\n\t\t\t\t\tswitch (command) {\n\t\t\t\t\t\tcase \"quit\":\n\t\t\t\t\t\tcase \"stop\":\n\t\t\t\t\t\t\tyarnCluster.shutdownCluster();\n\t\t\t\t\t\t\tbreak label;\n\n\t\t\t\t\t\tcase \"help\":\n\t\t\t\t\t\t\tSystem.err.println(help);\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\tdefault:\n\t\t\t\t\t\t\tSystem.err.println(\"Unknown command '\" + command + \"'. Showing help: \\n\" + help);\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (yarnCluster.hasBeenShutdown()) {\n\t\t\t\t\tLOG.info(\"Stopping interactive command line interface, YARN cluster has been stopped.\");\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Exception while running the interactive command line interface\", e);\n\t\t}\n\t}"
        ],
        [
            "FlinkYarnSessionCli::printUsage()",
            " 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386 -\n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  ",
            "\tprivate void printUsage() {\n\t\tSystem.out.println(\"Usage:\");\n\t\tHelpFormatter formatter = new HelpFormatter();\n\t\tformatter.setWidth(200);\n\t\tformatter.setLeftPadding(5);\n\t\tformatter.setSyntaxPrefix(\"   Required\");\n\t\tOptions req = new Options();\n\t\treq.addOption(CONTAINER);\n\t\tformatter.printHelp(\" \", req);\n\n\t\tformatter.setSyntaxPrefix(\"   Optional\");\n\t\tOptions options = new Options();\n\t\taddGeneralOptions(options);\n\t\taddRunOptions(options);\n\t\tformatter.printHelp(\" \", options);\n\t}",
            " 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387 +\n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  ",
            "\tprivate void printUsage() {\n\t\tSystem.out.println(\"Usage:\");\n\t\tHelpFormatter formatter = new HelpFormatter();\n\t\tformatter.setWidth(200);\n\t\tformatter.setLeftPadding(5);\n\t\tformatter.setSyntaxPrefix(\"   Required\");\n\t\tOptions req = new Options();\n\t\treq.addOption(container);\n\t\tformatter.printHelp(\" \", req);\n\n\t\tformatter.setSyntaxPrefix(\"   Optional\");\n\t\tOptions options = new Options();\n\t\taddGeneralOptions(options);\n\t\taddRunOptions(options);\n\t\tformatter.printHelp(\" \", options);\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::deploy()",
            " 420  \n 421  \n 422  \n 423 -\n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  ",
            "\t@Override\n\tpublic YarnClusterClient deploy() {\n\t\ttry {\n\t\t\tif(UserGroupInformation.isSecurityEnabled()) {\n\t\t\t\t// note: UGI::hasKerberosCredentials inaccurately reports false\n\t\t\t\t// for logins based on a keytab (fixed in Hadoop 2.6.1, see HADOOP-10786),\n\t\t\t\t// so we check only in ticket cache scenario.\n\t\t\t\tboolean useTicketCache = flinkConfiguration.getBoolean(SecurityOptions.KERBEROS_LOGIN_USETICKETCACHE);\n\n\t\t\t\tUserGroupInformation loginUser = UserGroupInformation.getCurrentUser();\n\t\t\t\tif (loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS\n\t\t\t\t\t\t&& useTicketCache && !loginUser.hasKerberosCredentials()) {\n\t\t\t\t\tLOG.error(\"Hadoop security with Kerberos is enabled but the login user does not have Kerberos credentials\");\n\t\t\t\t\tthrow new RuntimeException(\"Hadoop security with Kerberos is enabled but the login user \" +\n\t\t\t\t\t\t\t\"does not have Kerberos credentials\");\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn deployInternal();\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Couldn't deploy Yarn cluster\", e);\n\t\t}\n\t}",
            " 427  \n 428  \n 429  \n 430 +\n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  ",
            "\t@Override\n\tpublic YarnClusterClient deploy() {\n\t\ttry {\n\t\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\t\t// note: UGI::hasKerberosCredentials inaccurately reports false\n\t\t\t\t// for logins based on a keytab (fixed in Hadoop 2.6.1, see HADOOP-10786),\n\t\t\t\t// so we check only in ticket cache scenario.\n\t\t\t\tboolean useTicketCache = flinkConfiguration.getBoolean(SecurityOptions.KERBEROS_LOGIN_USETICKETCACHE);\n\n\t\t\t\tUserGroupInformation loginUser = UserGroupInformation.getCurrentUser();\n\t\t\t\tif (loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS\n\t\t\t\t\t\t&& useTicketCache && !loginUser.hasKerberosCredentials()) {\n\t\t\t\t\tLOG.error(\"Hadoop security with Kerberos is enabled but the login user does not have Kerberos credentials\");\n\t\t\t\t\tthrow new RuntimeException(\"Hadoop security with Kerberos is enabled but the login user \" +\n\t\t\t\t\t\t\t\"does not have Kerberos credentials\");\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn deployInternal();\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Couldn't deploy Yarn cluster\", e);\n\t\t}\n\t}"
        ],
        [
            "YarnPreConfiguredMasterHaServicesTest::testCallsOnClosedServices()",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "\t@Test\n\tpublic void testCallsOnClosedServices() throws Exception {\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setString(YarnConfigOptions.APP_MASTER_RPC_ADDRESS, \"localhost\");\n\t\tflinkConfig.setInteger(YarnConfigOptions.APP_MASTER_RPC_PORT, 1427);\n\n\t\tYarnHighAvailabilityServices services = new YarnPreConfiguredMasterNonHaServices(\n\t\t\tflinkConfig,\n\t\t\thadoopConfig,\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\t// this method is not supported\n\t\ttry {\n\t\t\tservices.getSubmittedJobGraphStore();\n\t\t\tfail();\n\t\t} catch (UnsupportedOperationException ignored) {}\n\n\n\t\tservices.close();\n\n\t\t// all these methods should fail now\n\n\t\ttry {\n\t\t\tservices.createBlobStore();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getCheckpointRecoveryFactory();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getJobManagerLeaderElectionService(new JobID());\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getJobManagerLeaderRetriever(new JobID());\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getRunningJobsRegistry();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getResourceManagerLeaderElectionService();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getResourceManagerLeaderRetriever();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\t}",
            " 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  ",
            "\t@Test\n\tpublic void testCallsOnClosedServices() throws Exception {\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setString(YarnConfigOptions.APP_MASTER_RPC_ADDRESS, \"localhost\");\n\t\tflinkConfig.setInteger(YarnConfigOptions.APP_MASTER_RPC_PORT, 1427);\n\n\t\tYarnHighAvailabilityServices services = new YarnPreConfiguredMasterNonHaServices(\n\t\t\tflinkConfig,\n\t\t\thadoopConfig,\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\t// this method is not supported\n\t\ttry {\n\t\t\tservices.getSubmittedJobGraphStore();\n\t\t\tfail();\n\t\t} catch (UnsupportedOperationException ignored) {}\n\n\t\tservices.close();\n\n\t\t// all these methods should fail now\n\n\t\ttry {\n\t\t\tservices.createBlobStore();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getCheckpointRecoveryFactory();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getJobManagerLeaderElectionService(new JobID());\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getJobManagerLeaderRetriever(new JobID());\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getRunningJobsRegistry();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getResourceManagerLeaderElectionService();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\n\t\ttry {\n\t\t\tservices.getResourceManagerLeaderRetriever();\n\t\t\tfail();\n\t\t} catch (IllegalStateException ignored) {}\n\t}"
        ],
        [
            "FlinkYarnSessionCli::FlinkYarnSessionCli(String,String,boolean)",
            " 136  \n 137  \n 138  \n 139 -\n 140 -\n 141 -\n 142 -\n 143 -\n 144 -\n 145 -\n 146 -\n 147 -\n 148 -\n 149 -\n 150 -\n 151 -\n 152 -\n 153 -\n 154 -\n 155 -\n 156 -\n 157 -\n 158 -\n 159 -\n 160 -\n 161 -\n 162 -\n 163 -\n 164 -\n 165 -\n 166 -\n 167 -\n 168 -\n 169  ",
            "\tpublic FlinkYarnSessionCli(String shortPrefix, String longPrefix, boolean acceptInteractiveInput) {\n\t\tthis.acceptInteractiveInput = acceptInteractiveInput;\n\n\t\tQUERY = new Option(shortPrefix + \"q\", longPrefix + \"query\", false, \"Display available YARN resources (memory, cores)\");\n\t\tAPPLICATION_ID = new Option(shortPrefix + \"id\", longPrefix + \"applicationId\", true, \"Attach to running YARN session\");\n\t\tQUEUE = new Option(shortPrefix + \"qu\", longPrefix + \"queue\", true, \"Specify YARN queue.\");\n\t\tSHIP_PATH = new Option(shortPrefix + \"t\", longPrefix + \"ship\", true, \"Ship files in the specified directory (t for transfer)\");\n\t\tFLINK_JAR = new Option(shortPrefix + \"j\", longPrefix + \"jar\", true, \"Path to Flink jar file\");\n\t\tJM_MEMORY = new Option(shortPrefix + \"jm\", longPrefix + \"jobManagerMemory\", true, \"Memory for JobManager Container [in MB]\");\n\t\tTM_MEMORY = new Option(shortPrefix + \"tm\", longPrefix + \"taskManagerMemory\", true, \"Memory per TaskManager Container [in MB]\");\n\t\tCONTAINER = new Option(shortPrefix + \"n\", longPrefix + \"container\", true, \"Number of YARN container to allocate (=Number of Task Managers)\");\n\t\tSLOTS = new Option(shortPrefix + \"s\", longPrefix + \"slots\", true, \"Number of slots per TaskManager\");\n\t\tDYNAMIC_PROPERTIES = new Option(shortPrefix + \"D\", true, \"Dynamic properties\");\n\t\tDETACHED = new Option(shortPrefix + \"d\", longPrefix + \"detached\", false, \"Start detached\");\n\t\tSTREAMING = new Option(shortPrefix + \"st\", longPrefix + \"streaming\", false, \"Start Flink in streaming mode\");\n\t\tNAME = new Option(shortPrefix + \"nm\", longPrefix + \"name\", true, \"Set a custom name for the application on YARN\");\n\t\tZOOKEEPER_NAMESPACE = new Option(shortPrefix + \"z\", longPrefix + \"zookeeperNamespace\", true, \"Namespace to create the Zookeeper sub-paths for high availability mode\");\n\n\t\tALL_OPTIONS = new Options();\n\t\tALL_OPTIONS.addOption(FLINK_JAR);\n\t\tALL_OPTIONS.addOption(JM_MEMORY);\n\t\tALL_OPTIONS.addOption(TM_MEMORY);\n\t\tALL_OPTIONS.addOption(CONTAINER);\n\t\tALL_OPTIONS.addOption(QUEUE);\n\t\tALL_OPTIONS.addOption(QUERY);\n\t\tALL_OPTIONS.addOption(SHIP_PATH);\n\t\tALL_OPTIONS.addOption(SLOTS);\n\t\tALL_OPTIONS.addOption(DYNAMIC_PROPERTIES);\n\t\tALL_OPTIONS.addOption(DETACHED);\n\t\tALL_OPTIONS.addOption(STREAMING);\n\t\tALL_OPTIONS.addOption(NAME);\n\t\tALL_OPTIONS.addOption(APPLICATION_ID);\n\t\tALL_OPTIONS.addOption(ZOOKEEPER_NAMESPACE);\n\t}",
            " 138  \n 139  \n 140  \n 141 +\n 142 +\n 143 +\n 144 +\n 145 +\n 146 +\n 147 +\n 148 +\n 149 +\n 150 +\n 151 +\n 152 +\n 153 +\n 154 +\n 155 +\n 156 +\n 157 +\n 158 +\n 159 +\n 160 +\n 161 +\n 162 +\n 163 +\n 164 +\n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171  ",
            "\tpublic FlinkYarnSessionCli(String shortPrefix, String longPrefix, boolean acceptInteractiveInput) {\n\t\tthis.acceptInteractiveInput = acceptInteractiveInput;\n\n\t\tquery = new Option(shortPrefix + \"q\", longPrefix + \"query\", false, \"Display available YARN resources (memory, cores)\");\n\t\tapplicationId = new Option(shortPrefix + \"id\", longPrefix + \"applicationId\", true, \"Attach to running YARN session\");\n\t\tqueue = new Option(shortPrefix + \"qu\", longPrefix + \"queue\", true, \"Specify YARN queue.\");\n\t\tshipPath = new Option(shortPrefix + \"t\", longPrefix + \"ship\", true, \"Ship files in the specified directory (t for transfer)\");\n\t\tflinkJar = new Option(shortPrefix + \"j\", longPrefix + \"jar\", true, \"Path to Flink jar file\");\n\t\tjmMemory = new Option(shortPrefix + \"jm\", longPrefix + \"jobManagerMemory\", true, \"Memory for JobManager Container [in MB]\");\n\t\ttmMemory = new Option(shortPrefix + \"tm\", longPrefix + \"taskManagerMemory\", true, \"Memory per TaskManager Container [in MB]\");\n\t\tcontainer = new Option(shortPrefix + \"n\", longPrefix + \"container\", true, \"Number of YARN container to allocate (=Number of Task Managers)\");\n\t\tslots = new Option(shortPrefix + \"s\", longPrefix + \"slots\", true, \"Number of slots per TaskManager\");\n\t\tdynamicproperties = new Option(shortPrefix + \"D\", true, \"Dynamic properties\");\n\t\tdetached = new Option(shortPrefix + \"d\", longPrefix + \"detached\", false, \"Start detached\");\n\t\tstreaming = new Option(shortPrefix + \"st\", longPrefix + \"streaming\", false, \"Start Flink in streaming mode\");\n\t\tname = new Option(shortPrefix + \"nm\", longPrefix + \"name\", true, \"Set a custom name for the application on YARN\");\n\t\tzookeeperNamespace = new Option(shortPrefix + \"z\", longPrefix + \"zookeeperNamespace\", true, \"Namespace to create the Zookeeper sub-paths for high availability mode\");\n\n\t\tallOptions = new Options();\n\t\tallOptions.addOption(flinkJar);\n\t\tallOptions.addOption(jmMemory);\n\t\tallOptions.addOption(tmMemory);\n\t\tallOptions.addOption(container);\n\t\tallOptions.addOption(queue);\n\t\tallOptions.addOption(query);\n\t\tallOptions.addOption(shipPath);\n\t\tallOptions.addOption(slots);\n\t\tallOptions.addOption(dynamicproperties);\n\t\tallOptions.addOption(detached);\n\t\tallOptions.addOption(streaming);\n\t\tallOptions.addOption(name);\n\t\tallOptions.addOption(applicationId);\n\t\tallOptions.addOption(zookeeperNamespace);\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::setTaskManagerMemory(int)",
            " 193  \n 194 -\n 195  \n 196 -\n 197  \n 198  \n 199  ",
            "\tpublic void setTaskManagerMemory(int memoryMb) {\n\t\tif(memoryMb < MIN_TM_MEMORY) {\n\t\t\tthrow new IllegalArgumentException(\"The TaskManager memory (\" + memoryMb + \") is below the minimum required memory amount \"\n\t\t\t\t+ \"of \" + MIN_TM_MEMORY+ \" MB\");\n\t\t}\n\t\tthis.taskManagerMemoryMb = memoryMb;\n\t}",
            " 201  \n 202 +\n 203  \n 204 +\n 205  \n 206  \n 207  ",
            "\tpublic void setTaskManagerMemory(int memoryMb) {\n\t\tif (memoryMb < MIN_TM_MEMORY) {\n\t\t\tthrow new IllegalArgumentException(\"The TaskManager memory (\" + memoryMb + \") is below the minimum required memory amount \"\n\t\t\t\t+ \"of \" + MIN_TM_MEMORY + \" MB\");\n\t\t}\n\t\tthis.taskManagerMemoryMb = memoryMb;\n\t}"
        ],
        [
            "Utils::calculateHeapSize(int,org)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 -\n 106  \n 107  \n 108  \n 109  \n 110  ",
            "\t/**\n\t * See documentation\n\t */\n\tpublic static int calculateHeapSize(int memory, org.apache.flink.configuration.Configuration conf) {\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(conf,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_RATIO, ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO);\n\t\tBootstrapTools.substituteDeprecatedConfigKey(conf,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_MIN, ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN);\n\n\t\tfloat memoryCutoffRatio = conf.getFloat(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO,\n\t\t\tConfigConstants.DEFAULT_YARN_HEAP_CUTOFF_RATIO);\n\t\tint minCutoff = conf.getInteger(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN,\n\t\t\tConfigConstants.DEFAULT_YARN_HEAP_CUTOFF);\n\n\t\tif (memoryCutoffRatio > 1 || memoryCutoffRatio < 0) {\n\t\t\tthrow new IllegalArgumentException(\"The configuration value '\"\n\t\t\t\t+ ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO\n\t\t\t\t+ \"' must be between 0 and 1. Value given=\" + memoryCutoffRatio);\n\t\t}\n\t\tif (minCutoff > memory) {\n\t\t\tthrow new IllegalArgumentException(\"The configuration value '\"\n\t\t\t\t+ ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN\n\t\t\t\t+ \"' is higher (\" + minCutoff + \") than the requested amount of memory \" + memory);\n\t\t}\n\n\t\tint heapLimit = (int)((float)memory * memoryCutoffRatio);\n\t\tif (heapLimit < minCutoff) {\n\t\t\theapLimit = minCutoff;\n\t\t}\n\t\treturn memory - heapLimit;\n\t}",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105  \n 106  \n 107  \n 108  \n 109  ",
            "\t/**\n\t * See documentation.\n\t */\n\tpublic static int calculateHeapSize(int memory, org.apache.flink.configuration.Configuration conf) {\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(conf,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_RATIO, ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO);\n\t\tBootstrapTools.substituteDeprecatedConfigKey(conf,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_MIN, ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN);\n\n\t\tfloat memoryCutoffRatio = conf.getFloat(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO,\n\t\t\tConfigConstants.DEFAULT_YARN_HEAP_CUTOFF_RATIO);\n\t\tint minCutoff = conf.getInteger(ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN,\n\t\t\tConfigConstants.DEFAULT_YARN_HEAP_CUTOFF);\n\n\t\tif (memoryCutoffRatio > 1 || memoryCutoffRatio < 0) {\n\t\t\tthrow new IllegalArgumentException(\"The configuration value '\"\n\t\t\t\t+ ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO\n\t\t\t\t+ \"' must be between 0 and 1. Value given=\" + memoryCutoffRatio);\n\t\t}\n\t\tif (minCutoff > memory) {\n\t\t\tthrow new IllegalArgumentException(\"The configuration value '\"\n\t\t\t\t+ ConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN\n\t\t\t\t+ \"' is higher (\" + minCutoff + \") than the requested amount of memory \" + memory);\n\t\t}\n\n\t\tint heapLimit = (int) ((float) memory * memoryCutoffRatio);\n\t\tif (heapLimit < minCutoff) {\n\t\t\theapLimit = minCutoff;\n\t\t}\n\t\treturn memory - heapLimit;\n\t}"
        ],
        [
            "YarnResourceManager::initialize()",
            " 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "\t@Override\n\tprotected void initialize() throws ResourceManagerException {\n\t\tresourceManagerClient = AMRMClientAsync.createAMRMClientAsync(yarnHeartbeatIntervalMillis, this);\n\t\tresourceManagerClient.init(yarnConfig);\n\t\tresourceManagerClient.start();\n\t\ttry {\n\t\t\t//TODO: change akka address to tcp host and port, the getAddress() interface should return a standard tcp address\n\t\t\tTuple2<String, Integer> hostPort = parseHostPort(getAddress());\n\t\t\t//TODO: the third paramter should be the webmonitor address\n\t\t\tresourceManagerClient.registerApplicationMaster(hostPort.f0, hostPort.f1, getAddress());\n\t\t} catch (Exception e) {\n\t\t\tLOG.info(\"registerApplicationMaster fail\", e);\n\t\t}\n\n\t\t// create the client to communicate with the node managers\n\t\tnodeManagerClient = NMClient.createNMClient();\n\t\tnodeManagerClient.init(yarnConfig);\n\t\tnodeManagerClient.start();\n\t\tnodeManagerClient.cleanupRunningContainersOnStop(true);\n\t}",
            " 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163 +\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  ",
            "\t@Override\n\tprotected void initialize() throws ResourceManagerException {\n\t\tresourceManagerClient = AMRMClientAsync.createAMRMClientAsync(yarnHeartbeatIntervalMillis, this);\n\t\tresourceManagerClient.init(yarnConfig);\n\t\tresourceManagerClient.start();\n\t\ttry {\n\t\t\t//TODO: change akka address to tcp host and port, the getAddress() interface should return a standard tcp address\n\t\t\tTuple2<String, Integer> hostPort = parseHostPort(getAddress());\n\t\t\t//TODO: the third paramter should be the webmonitor address\n\t\t\tresourceManagerClient.registerApplicationMaster(hostPort.f0, hostPort.f1, getAddress());\n\t\t} catch (Exception e) {\n\t\t\tlog.info(\"registerApplicationMaster fail\", e);\n\t\t}\n\n\t\t// create the client to communicate with the node managers\n\t\tnodeManagerClient = NMClient.createNMClient();\n\t\tnodeManagerClient.init(yarnConfig);\n\t\tnodeManagerClient.start();\n\t\tnodeManagerClient.cleanupRunningContainersOnStop(true);\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::deployInternal()",
            " 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456 -\n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480 -\n 481  \n 482 -\n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498 -\n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506 -\n 507  \n 508  \n 509 -\n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518 -\n 519 -\n 520  \n 521  \n 522 -\n 523  \n 524  \n 525 -\n 526  \n 527  \n 528 -\n 529  \n 530  \n 531 -\n 532  \n 533  \n 534  \n 535  \n 536  \n 537 -\n 538  \n 539 -\n 540  \n 541  \n 542 -\n 543  \n 544 -\n 545  \n 546 -\n 547  \n 548 -\n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555 -\n 556  \n 557  \n 558 -\n 559  \n 560  \n 561 -\n 562 -\n 563  \n 564  \n 565  \n 566  \n 567 -\n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  ",
            "\t/**\n\t * This method will block until the ApplicationMaster/JobManager have been\n\t * deployed on YARN.\n\t */\n\tprotected YarnClusterClient deployInternal() throws Exception {\n\t\tisReadyForDeployment();\n\t\tLOG.info(\"Using values:\");\n\t\tLOG.info(\"\\tTaskManager count = {}\", taskManagerCount);\n\t\tLOG.info(\"\\tJobManager memory = {}\", jobManagerMemoryMb);\n\t\tLOG.info(\"\\tTaskManager memory = {}\", taskManagerMemoryMb);\n\n\t\tfinal YarnClient yarnClient = getYarnClient();\n\n\n\t\t// ------------------ Check if the specified queue exists --------------------\n\n\t\ttry {\n\t\t\tList<QueueInfo> queues = yarnClient.getAllQueues();\n\t\t\tif (queues.size() > 0 && this.yarnQueue != null) { // check only if there are queues configured in yarn and for this session.\n\t\t\t\tboolean queueFound = false;\n\t\t\t\tfor (QueueInfo queue : queues) {\n\t\t\t\t\tif (queue.getQueueName().equals(this.yarnQueue)) {\n\t\t\t\t\t\tqueueFound = true;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!queueFound) {\n\t\t\t\t\tString queueNames = \"\";\n\t\t\t\t\tfor (QueueInfo queue : queues) {\n\t\t\t\t\t\tqueueNames += queue.getQueueName() + \", \";\n\t\t\t\t\t}\n\t\t\t\t\tLOG.warn(\"The specified queue '\" + this.yarnQueue + \"' does not exist. \" +\n\t\t\t\t\t\t\"Available queues: \" + queueNames);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tLOG.debug(\"The YARN cluster does not have any queues configured\");\n\t\t\t}\n\t\t} catch(Throwable e) {\n\t\t\tLOG.warn(\"Error while getting queue information from YARN: \" + e.getMessage());\n\t\t\tif(LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Error details\", e);\n\t\t\t}\n\t\t}\n\n\t\t// ------------------ Add dynamic properties to local flinkConfiguraton ------\n\t\tMap<String, String> dynProperties = getDynamicProperties(dynamicPropertiesEncoded);\n\t\tfor (Map.Entry<String, String> dynProperty : dynProperties.entrySet()) {\n\t\t\tflinkConfiguration.setString(dynProperty.getKey(), dynProperty.getValue());\n\t\t}\n\n\t\t// ------------------ Check if the YARN ClusterClient has the requested resources --------------\n\n\t\t// the yarnMinAllocationMB specifies the smallest possible container allocation size.\n\t\t// all allocations below this value are automatically set to this value.\n\t\tfinal int yarnMinAllocationMB = conf.getInt(\"yarn.scheduler.minimum-allocation-mb\", 0);\n\t\tif(jobManagerMemoryMb < yarnMinAllocationMB || taskManagerMemoryMb < yarnMinAllocationMB) {\n\t\t\tLOG.warn(\"The JobManager or TaskManager memory is below the smallest possible YARN Container size. \"\n\t\t\t\t+ \"The value of 'yarn.scheduler.minimum-allocation-mb' is '\" + yarnMinAllocationMB + \"'. Please increase the memory size.\" +\n\t\t\t\t\"YARN will allocate the smaller containers but the scheduler will account for the minimum-allocation-mb, maybe not all instances \" +\n\t\t\t\t\"you requested will start.\");\n\t\t}\n\n\t\t// set the memory to minAllocationMB to do the next checks correctly\n\t\tif(jobManagerMemoryMb < yarnMinAllocationMB) {\n\t\t\tjobManagerMemoryMb =  yarnMinAllocationMB;\n\t\t}\n\t\tif(taskManagerMemoryMb < yarnMinAllocationMB) {\n\t\t\ttaskManagerMemoryMb =  yarnMinAllocationMB;\n\t\t}\n\n\t\t// Create application via yarnClient\n\t\tfinal YarnClientApplication yarnApplication = yarnClient.createApplication();\n\t\tGetNewApplicationResponse appResponse = yarnApplication.getNewApplicationResponse();\n\n\t\tResource maxRes = appResponse.getMaximumResourceCapability();\n\t\tfinal String NOTE = \"Please check the 'yarn.scheduler.maximum-allocation-mb' and the 'yarn.nodemanager.resource.memory-mb' configuration values\\n\";\n\t\tif(jobManagerMemoryMb > maxRes.getMemory() ) {\n\t\t\tfailSessionDuringDeployment(yarnClient, yarnApplication);\n\t\t\tthrow new YarnDeploymentException(\"The cluster does not have the requested resources for the JobManager available!\\n\"\n\t\t\t\t+ \"Maximum Memory: \" + maxRes.getMemory() + \"MB Requested: \" + jobManagerMemoryMb + \"MB. \" + NOTE);\n\t\t}\n\n\t\tif(taskManagerMemoryMb > maxRes.getMemory() ) {\n\t\t\tfailSessionDuringDeployment(yarnClient, yarnApplication);\n\t\t\tthrow new YarnDeploymentException(\"The cluster does not have the requested resources for the TaskManagers available!\\n\"\n\t\t\t\t+ \"Maximum Memory: \" + maxRes.getMemory() + \" Requested: \" + taskManagerMemoryMb + \"MB. \" + NOTE);\n\t\t}\n\n\t\tfinal String NOTE_RSC = \"\\nThe Flink YARN client will try to allocate the YARN session, but maybe not all TaskManagers are \" +\n\t\t\t\"connecting from the beginning because the resources are currently not available in the cluster. \" +\n\t\t\t\"The allocation might take more time than usual because the Flink YARN client needs to wait until \" +\n\t\t\t\"the resources become available.\";\n\t\tint totalMemoryRequired = jobManagerMemoryMb + taskManagerMemoryMb * taskManagerCount;\n\t\tClusterResourceDescription freeClusterMem = getCurrentFreeClusterResources(yarnClient);\n\t\tif(freeClusterMem.totalFreeMemory < totalMemoryRequired) {\n\t\t\tLOG.warn(\"This YARN session requires \" + totalMemoryRequired + \"MB of memory in the cluster. \"\n\t\t\t\t+ \"There are currently only \" + freeClusterMem.totalFreeMemory + \"MB available.\" + NOTE_RSC);\n\n\t\t}\n\t\tif(taskManagerMemoryMb > freeClusterMem.containerLimit) {\n\t\t\tLOG.warn(\"The requested amount of memory for the TaskManagers (\" + taskManagerMemoryMb + \"MB) is more than \"\n\t\t\t\t+ \"the largest possible YARN container: \" + freeClusterMem.containerLimit + NOTE_RSC);\n\t\t}\n\t\tif(jobManagerMemoryMb > freeClusterMem.containerLimit) {\n\t\t\tLOG.warn(\"The requested amount of memory for the JobManager (\" + jobManagerMemoryMb + \"MB) is more than \"\n\t\t\t\t+ \"the largest possible YARN container: \" + freeClusterMem.containerLimit + NOTE_RSC);\n\t\t}\n\n\t\t// ----------------- check if the requested containers fit into the cluster.\n\n\t\tint[] nmFree = Arrays.copyOf(freeClusterMem.nodeManagersFree, freeClusterMem.nodeManagersFree.length);\n\t\t// first, allocate the jobManager somewhere.\n\t\tif(!allocateResource(nmFree, jobManagerMemoryMb)) {\n\t\t\tLOG.warn(\"Unable to find a NodeManager that can fit the JobManager/Application master. \" +\n\t\t\t\t\"The JobManager requires \" + jobManagerMemoryMb + \"MB. NodeManagers available: \" +\n\t\t\t\tArrays.toString(freeClusterMem.nodeManagersFree) + NOTE_RSC);\n\t\t}\n\t\t// allocate TaskManagers\n\t\tfor(int i = 0; i < taskManagerCount; i++) {\n\t\t\tif(!allocateResource(nmFree, taskManagerMemoryMb)) {\n\t\t\t\tLOG.warn(\"There is not enough memory available in the YARN cluster. \" +\n\t\t\t\t\t\"The TaskManager(s) require \" + taskManagerMemoryMb + \"MB each. \" +\n\t\t\t\t\t\"NodeManagers available: \" + Arrays.toString(freeClusterMem.nodeManagersFree) + \"\\n\" +\n\t\t\t\t\t\"After allocating the JobManager (\" + jobManagerMemoryMb + \"MB) and (\" + i + \"/\" + taskManagerCount + \") TaskManagers, \" +\n\t\t\t\t\t\"the following NodeManagers are available: \" + Arrays.toString(nmFree)  + NOTE_RSC );\n\t\t\t}\n\t\t}\n\n\t\tApplicationReport report = startAppMaster(null, yarnClient, yarnApplication);\n\n\t\tString host = report.getHost();\n\t\tint port = report.getRpcPort();\n\n\t\t// Correctly initialize the Flink config\n\t\tflinkConfiguration.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, host);\n\t\tflinkConfiguration.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, port);\n\n\t\t// the Flink cluster is deployed in YARN. Represent cluster\n\t\treturn createYarnClusterClient(this, yarnClient, report, flinkConfiguration, true);\n\t}",
            " 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486 +\n 487  \n 488 +\n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504 +\n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512 +\n 513  \n 514  \n 515 +\n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524 +\n 525 +\n 526  \n 527  \n 528 +\n 529  \n 530  \n 531 +\n 532  \n 533  \n 534 +\n 535  \n 536  \n 537 +\n 538  \n 539  \n 540  \n 541  \n 542  \n 543 +\n 544  \n 545 +\n 546  \n 547  \n 548 +\n 549  \n 550 +\n 551  \n 552 +\n 553  \n 554 +\n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561 +\n 562  \n 563  \n 564 +\n 565  \n 566  \n 567 +\n 568 +\n 569  \n 570  \n 571  \n 572  \n 573 +\n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  ",
            "\t/**\n\t * This method will block until the ApplicationMaster/JobManager have been\n\t * deployed on YARN.\n\t */\n\tprotected YarnClusterClient deployInternal() throws Exception {\n\t\tisReadyForDeployment();\n\t\tLOG.info(\"Using values:\");\n\t\tLOG.info(\"\\tTaskManager count = {}\", taskManagerCount);\n\t\tLOG.info(\"\\tJobManager memory = {}\", jobManagerMemoryMb);\n\t\tLOG.info(\"\\tTaskManager memory = {}\", taskManagerMemoryMb);\n\n\t\tfinal YarnClient yarnClient = getYarnClient();\n\n\t\t// ------------------ Check if the specified queue exists --------------------\n\n\t\ttry {\n\t\t\tList<QueueInfo> queues = yarnClient.getAllQueues();\n\t\t\tif (queues.size() > 0 && this.yarnQueue != null) { // check only if there are queues configured in yarn and for this session.\n\t\t\t\tboolean queueFound = false;\n\t\t\t\tfor (QueueInfo queue : queues) {\n\t\t\t\t\tif (queue.getQueueName().equals(this.yarnQueue)) {\n\t\t\t\t\t\tqueueFound = true;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!queueFound) {\n\t\t\t\t\tString queueNames = \"\";\n\t\t\t\t\tfor (QueueInfo queue : queues) {\n\t\t\t\t\t\tqueueNames += queue.getQueueName() + \", \";\n\t\t\t\t\t}\n\t\t\t\t\tLOG.warn(\"The specified queue '\" + this.yarnQueue + \"' does not exist. \" +\n\t\t\t\t\t\t\"Available queues: \" + queueNames);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tLOG.debug(\"The YARN cluster does not have any queues configured\");\n\t\t\t}\n\t\t} catch (Throwable e) {\n\t\t\tLOG.warn(\"Error while getting queue information from YARN: \" + e.getMessage());\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Error details\", e);\n\t\t\t}\n\t\t}\n\n\t\t// ------------------ Add dynamic properties to local flinkConfiguraton ------\n\t\tMap<String, String> dynProperties = getDynamicProperties(dynamicPropertiesEncoded);\n\t\tfor (Map.Entry<String, String> dynProperty : dynProperties.entrySet()) {\n\t\t\tflinkConfiguration.setString(dynProperty.getKey(), dynProperty.getValue());\n\t\t}\n\n\t\t// ------------------ Check if the YARN ClusterClient has the requested resources --------------\n\n\t\t// the yarnMinAllocationMB specifies the smallest possible container allocation size.\n\t\t// all allocations below this value are automatically set to this value.\n\t\tfinal int yarnMinAllocationMB = conf.getInt(\"yarn.scheduler.minimum-allocation-mb\", 0);\n\t\tif (jobManagerMemoryMb < yarnMinAllocationMB || taskManagerMemoryMb < yarnMinAllocationMB) {\n\t\t\tLOG.warn(\"The JobManager or TaskManager memory is below the smallest possible YARN Container size. \"\n\t\t\t\t+ \"The value of 'yarn.scheduler.minimum-allocation-mb' is '\" + yarnMinAllocationMB + \"'. Please increase the memory size.\" +\n\t\t\t\t\"YARN will allocate the smaller containers but the scheduler will account for the minimum-allocation-mb, maybe not all instances \" +\n\t\t\t\t\"you requested will start.\");\n\t\t}\n\n\t\t// set the memory to minAllocationMB to do the next checks correctly\n\t\tif (jobManagerMemoryMb < yarnMinAllocationMB) {\n\t\t\tjobManagerMemoryMb =  yarnMinAllocationMB;\n\t\t}\n\t\tif (taskManagerMemoryMb < yarnMinAllocationMB) {\n\t\t\ttaskManagerMemoryMb =  yarnMinAllocationMB;\n\t\t}\n\n\t\t// Create application via yarnClient\n\t\tfinal YarnClientApplication yarnApplication = yarnClient.createApplication();\n\t\tGetNewApplicationResponse appResponse = yarnApplication.getNewApplicationResponse();\n\n\t\tResource maxRes = appResponse.getMaximumResourceCapability();\n\t\tfinal String note = \"Please check the 'yarn.scheduler.maximum-allocation-mb' and the 'yarn.nodemanager.resource.memory-mb' configuration values\\n\";\n\t\tif (jobManagerMemoryMb > maxRes.getMemory()) {\n\t\t\tfailSessionDuringDeployment(yarnClient, yarnApplication);\n\t\t\tthrow new YarnDeploymentException(\"The cluster does not have the requested resources for the JobManager available!\\n\"\n\t\t\t\t+ \"Maximum Memory: \" + maxRes.getMemory() + \"MB Requested: \" + jobManagerMemoryMb + \"MB. \" + note);\n\t\t}\n\n\t\tif (taskManagerMemoryMb > maxRes.getMemory()) {\n\t\t\tfailSessionDuringDeployment(yarnClient, yarnApplication);\n\t\t\tthrow new YarnDeploymentException(\"The cluster does not have the requested resources for the TaskManagers available!\\n\"\n\t\t\t\t+ \"Maximum Memory: \" + maxRes.getMemory() + \" Requested: \" + taskManagerMemoryMb + \"MB. \" + note);\n\t\t}\n\n\t\tfinal String noteRsc = \"\\nThe Flink YARN client will try to allocate the YARN session, but maybe not all TaskManagers are \" +\n\t\t\t\"connecting from the beginning because the resources are currently not available in the cluster. \" +\n\t\t\t\"The allocation might take more time than usual because the Flink YARN client needs to wait until \" +\n\t\t\t\"the resources become available.\";\n\t\tint totalMemoryRequired = jobManagerMemoryMb + taskManagerMemoryMb * taskManagerCount;\n\t\tClusterResourceDescription freeClusterMem = getCurrentFreeClusterResources(yarnClient);\n\t\tif (freeClusterMem.totalFreeMemory < totalMemoryRequired) {\n\t\t\tLOG.warn(\"This YARN session requires \" + totalMemoryRequired + \"MB of memory in the cluster. \"\n\t\t\t\t+ \"There are currently only \" + freeClusterMem.totalFreeMemory + \"MB available.\" + noteRsc);\n\n\t\t}\n\t\tif (taskManagerMemoryMb > freeClusterMem.containerLimit) {\n\t\t\tLOG.warn(\"The requested amount of memory for the TaskManagers (\" + taskManagerMemoryMb + \"MB) is more than \"\n\t\t\t\t+ \"the largest possible YARN container: \" + freeClusterMem.containerLimit + noteRsc);\n\t\t}\n\t\tif (jobManagerMemoryMb > freeClusterMem.containerLimit) {\n\t\t\tLOG.warn(\"The requested amount of memory for the JobManager (\" + jobManagerMemoryMb + \"MB) is more than \"\n\t\t\t\t+ \"the largest possible YARN container: \" + freeClusterMem.containerLimit + noteRsc);\n\t\t}\n\n\t\t// ----------------- check if the requested containers fit into the cluster.\n\n\t\tint[] nmFree = Arrays.copyOf(freeClusterMem.nodeManagersFree, freeClusterMem.nodeManagersFree.length);\n\t\t// first, allocate the jobManager somewhere.\n\t\tif (!allocateResource(nmFree, jobManagerMemoryMb)) {\n\t\t\tLOG.warn(\"Unable to find a NodeManager that can fit the JobManager/Application master. \" +\n\t\t\t\t\"The JobManager requires \" + jobManagerMemoryMb + \"MB. NodeManagers available: \" +\n\t\t\t\tArrays.toString(freeClusterMem.nodeManagersFree) + noteRsc);\n\t\t}\n\t\t// allocate TaskManagers\n\t\tfor (int i = 0; i < taskManagerCount; i++) {\n\t\t\tif (!allocateResource(nmFree, taskManagerMemoryMb)) {\n\t\t\t\tLOG.warn(\"There is not enough memory available in the YARN cluster. \" +\n\t\t\t\t\t\"The TaskManager(s) require \" + taskManagerMemoryMb + \"MB each. \" +\n\t\t\t\t\t\"NodeManagers available: \" + Arrays.toString(freeClusterMem.nodeManagersFree) + \"\\n\" +\n\t\t\t\t\t\"After allocating the JobManager (\" + jobManagerMemoryMb + \"MB) and (\" + i + \"/\" + taskManagerCount + \") TaskManagers, \" +\n\t\t\t\t\t\"the following NodeManagers are available: \" + Arrays.toString(nmFree)  + noteRsc);\n\t\t\t}\n\t\t}\n\n\t\tApplicationReport report = startAppMaster(null, yarnClient, yarnApplication);\n\n\t\tString host = report.getHost();\n\t\tint port = report.getRpcPort();\n\n\t\t// Correctly initialize the Flink config\n\t\tflinkConfiguration.setString(ConfigConstants.JOB_MANAGER_IPC_ADDRESS_KEY, host);\n\t\tflinkConfiguration.setInteger(ConfigConstants.JOB_MANAGER_IPC_PORT_KEY, port);\n\n\t\t// the Flink cluster is deployed in YARN. Represent cluster\n\t\treturn createYarnClusterClient(this, yarnClient, report, flinkConfiguration, true);\n\t}"
        ],
        [
            "FlinkYarnSessionCli::run(String)",
            " 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573 -\n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 -\n 581  \n 582  \n 583  \n 584  \n 585  \n 586 -\n 587  \n 588  \n 589  \n 590  \n 591  \n 592 -\n 593  \n 594  \n 595  \n 596  \n 597 -\n 598 -\n 599 -\n 600 -\n 601  \n 602  \n 603  \n 604  \n 605 -\n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613 -\n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632 -\n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  ",
            "\tpublic int run(String[] args) {\n\t\t//\n\t\t//\tCommand Line Options\n\t\t//\n\t\tOptions options = new Options();\n\t\taddGeneralOptions(options);\n\t\taddRunOptions(options);\n\n\t\tCommandLineParser parser = new PosixParser();\n\t\tCommandLine cmd;\n\t\ttry {\n\t\t\tcmd = parser.parse(options, args);\n\t\t} catch(Exception e) {\n\t\t\tSystem.out.println(e.getMessage());\n\t\t\tprintUsage();\n\t\t\treturn 1;\n\t\t}\n\n\t\t// Query cluster for metrics\n\t\tif (cmd.hasOption(QUERY.getOpt())) {\n\t\t\tAbstractYarnClusterDescriptor yarnDescriptor = getClusterDescriptor();\n\t\t\tString description;\n\t\t\ttry {\n\t\t\t\tdescription = yarnDescriptor.getClusterDescription();\n\t\t\t} catch (Exception e) {\n\t\t\t\tSystem.err.println(\"Error while querying the YARN cluster for available resources: \"+e.getMessage());\n\t\t\t\te.printStackTrace(System.err);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tSystem.out.println(description);\n\t\t\treturn 0;\n\t\t} else if (cmd.hasOption(APPLICATION_ID.getOpt())) {\n\n\t\t\tAbstractYarnClusterDescriptor yarnDescriptor = getClusterDescriptor();\n\n\t\t\t//configure ZK namespace depending on the value passed\n\t\t\tString zkNamespace = cmd.hasOption(ZOOKEEPER_NAMESPACE.getOpt()) ?\n\t\t\t\t\t\t\t\t\tcmd.getOptionValue(ZOOKEEPER_NAMESPACE.getOpt())\n\t\t\t\t\t\t\t\t\t:yarnDescriptor.getFlinkConfiguration()\n\t\t\t\t\t\t\t\t\t.getString(HA_ZOOKEEPER_NAMESPACE_KEY, cmd.getOptionValue(APPLICATION_ID.getOpt()));\n\t\t\tLOG.info(\"Going to use the ZK namespace: {}\", zkNamespace);\n\t\t\tyarnDescriptor.getFlinkConfiguration().setString(HA_ZOOKEEPER_NAMESPACE_KEY, zkNamespace);\n\n\t\t\ttry {\n\t\t\t\tyarnCluster = yarnDescriptor.retrieve(cmd.getOptionValue(APPLICATION_ID.getOpt()));\n\t\t\t} catch (Exception e) {\n\t\t\t\tthrow new RuntimeException(\"Could not retrieve existing Yarn application\", e);\n\t\t\t}\n\n\t\t\tif (detachedMode) {\n\t\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop it:\\n\" +\n\t\t\t\t\t\"yarn application -kill \" + APPLICATION_ID.getOpt());\n\t\t\t\tyarnCluster.disconnect();\n\t\t\t} else {\n\t\t\t\trunInteractiveCli(yarnCluster, true);\n\t\t\t}\n\t\t} else {\n\n\t\t\tAbstractYarnClusterDescriptor yarnDescriptor;\n\t\t\ttry {\n\t\t\t\tyarnDescriptor = createDescriptor(null, cmd);\n\t\t\t} catch (Exception e) {\n\t\t\t\tSystem.err.println(\"Error while starting the YARN Client: \" + e.getMessage());\n\t\t\t\te.printStackTrace(System.err);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tyarnCluster = yarnDescriptor.deploy();\n\t\t\t} catch (Exception e) {\n\t\t\t\tSystem.err.println(\"Error while deploying YARN cluster: \"+e.getMessage());\n\t\t\t\te.printStackTrace(System.err);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\t//------------------ ClusterClient deployed, handle connection details\n\t\t\tString jobManagerAddress =\n\t\t\t\tyarnCluster.getJobManagerAddress().getAddress().getHostName() +\n\t\t\t\t\t\":\" + yarnCluster.getJobManagerAddress().getPort();\n\n\t\t\tSystem.out.println(\"Flink JobManager is now running on \" + jobManagerAddress);\n\t\t\tSystem.out.println(\"JobManager Web Interface: \" + yarnCluster.getWebInterfaceURL());\n\n\t\t\t// file that we write into the conf/ dir containing the jobManager address and the dop.\n\t\t\tFile yarnPropertiesFile = getYarnPropertiesLocation(yarnCluster.getFlinkConfiguration());\n\n\t\t\tProperties yarnProps = new Properties();\n\t\t\tyarnProps.setProperty(YARN_APPLICATION_ID_KEY, yarnCluster.getApplicationId().toString());\n\t\t\tif (yarnDescriptor.getTaskManagerSlots() != -1) {\n\t\t\t\tString parallelism =\n\t\t\t\t\t\tInteger.toString(yarnDescriptor.getTaskManagerSlots() * yarnDescriptor.getTaskManagerCount());\n\t\t\t\tyarnProps.setProperty(YARN_PROPERTIES_PARALLELISM, parallelism);\n\t\t\t}\n\t\t\t// add dynamic properties\n\t\t\tif (yarnDescriptor.getDynamicPropertiesEncoded() != null) {\n\t\t\t\tyarnProps.setProperty(YARN_PROPERTIES_DYNAMIC_PROPERTIES_STRING,\n\t\t\t\t\t\tyarnDescriptor.getDynamicPropertiesEncoded());\n\t\t\t}\n\t\t\twriteYarnProperties(yarnProps, yarnPropertiesFile);\n\n\t\t\t//------------------ ClusterClient running, let user control it ------------\n\n\t\t\tif (detachedMode) {\n\t\t\t\t// print info and quit:\n\t\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop it:\\n\" +\n\t\t\t\t\t\t\"yarn application -kill \" + yarnCluster.getApplicationId());\n\t\t\t\tyarnCluster.waitForClusterToBeReady();\n\t\t\t\tyarnCluster.disconnect();\n\t\t\t} else {\n\t\t\t\trunInteractiveCli(yarnCluster, acceptInteractiveInput);\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}",
            " 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573 +\n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 +\n 581  \n 582  \n 583  \n 584  \n 585  \n 586 +\n 587  \n 588  \n 589  \n 590  \n 591  \n 592 +\n 593  \n 594  \n 595  \n 596  \n 597 +\n 598 +\n 599 +\n 600 +\n 601  \n 602  \n 603  \n 604  \n 605 +\n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613 +\n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632 +\n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  ",
            "\tpublic int run(String[] args) {\n\t\t//\n\t\t//\tCommand Line Options\n\t\t//\n\t\tOptions options = new Options();\n\t\taddGeneralOptions(options);\n\t\taddRunOptions(options);\n\n\t\tCommandLineParser parser = new PosixParser();\n\t\tCommandLine cmd;\n\t\ttry {\n\t\t\tcmd = parser.parse(options, args);\n\t\t} catch (Exception e) {\n\t\t\tSystem.out.println(e.getMessage());\n\t\t\tprintUsage();\n\t\t\treturn 1;\n\t\t}\n\n\t\t// Query cluster for metrics\n\t\tif (cmd.hasOption(query.getOpt())) {\n\t\t\tAbstractYarnClusterDescriptor yarnDescriptor = getClusterDescriptor();\n\t\t\tString description;\n\t\t\ttry {\n\t\t\t\tdescription = yarnDescriptor.getClusterDescription();\n\t\t\t} catch (Exception e) {\n\t\t\t\tSystem.err.println(\"Error while querying the YARN cluster for available resources: \" + e.getMessage());\n\t\t\t\te.printStackTrace(System.err);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\tSystem.out.println(description);\n\t\t\treturn 0;\n\t\t} else if (cmd.hasOption(applicationId.getOpt())) {\n\n\t\t\tAbstractYarnClusterDescriptor yarnDescriptor = getClusterDescriptor();\n\n\t\t\t//configure ZK namespace depending on the value passed\n\t\t\tString zkNamespace = cmd.hasOption(zookeeperNamespace.getOpt()) ?\n\t\t\t\t\t\t\t\t\tcmd.getOptionValue(zookeeperNamespace.getOpt())\n\t\t\t\t\t\t\t\t\t: yarnDescriptor.getFlinkConfiguration()\n\t\t\t\t\t\t\t\t\t.getString(HA_ZOOKEEPER_NAMESPACE_KEY, cmd.getOptionValue(applicationId.getOpt()));\n\t\t\tLOG.info(\"Going to use the ZK namespace: {}\", zkNamespace);\n\t\t\tyarnDescriptor.getFlinkConfiguration().setString(HA_ZOOKEEPER_NAMESPACE_KEY, zkNamespace);\n\n\t\t\ttry {\n\t\t\t\tyarnCluster = yarnDescriptor.retrieve(cmd.getOptionValue(applicationId.getOpt()));\n\t\t\t} catch (Exception e) {\n\t\t\t\tthrow new RuntimeException(\"Could not retrieve existing Yarn application\", e);\n\t\t\t}\n\n\t\t\tif (detachedMode) {\n\t\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop it:\\n\" +\n\t\t\t\t\t\"yarn application -kill \" + applicationId.getOpt());\n\t\t\t\tyarnCluster.disconnect();\n\t\t\t} else {\n\t\t\t\trunInteractiveCli(yarnCluster, true);\n\t\t\t}\n\t\t} else {\n\n\t\t\tAbstractYarnClusterDescriptor yarnDescriptor;\n\t\t\ttry {\n\t\t\t\tyarnDescriptor = createDescriptor(null, cmd);\n\t\t\t} catch (Exception e) {\n\t\t\t\tSystem.err.println(\"Error while starting the YARN Client: \" + e.getMessage());\n\t\t\t\te.printStackTrace(System.err);\n\t\t\t\treturn 1;\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tyarnCluster = yarnDescriptor.deploy();\n\t\t\t} catch (Exception e) {\n\t\t\t\tSystem.err.println(\"Error while deploying YARN cluster: \" + e.getMessage());\n\t\t\t\te.printStackTrace(System.err);\n\t\t\t\treturn 1;\n\t\t\t}\n\t\t\t//------------------ ClusterClient deployed, handle connection details\n\t\t\tString jobManagerAddress =\n\t\t\t\tyarnCluster.getJobManagerAddress().getAddress().getHostName() +\n\t\t\t\t\t\":\" + yarnCluster.getJobManagerAddress().getPort();\n\n\t\t\tSystem.out.println(\"Flink JobManager is now running on \" + jobManagerAddress);\n\t\t\tSystem.out.println(\"JobManager Web Interface: \" + yarnCluster.getWebInterfaceURL());\n\n\t\t\t// file that we write into the conf/ dir containing the jobManager address and the dop.\n\t\t\tFile yarnPropertiesFile = getYarnPropertiesLocation(yarnCluster.getFlinkConfiguration());\n\n\t\t\tProperties yarnProps = new Properties();\n\t\t\tyarnProps.setProperty(YARN_APPLICATION_ID_KEY, yarnCluster.getApplicationId().toString());\n\t\t\tif (yarnDescriptor.getTaskManagerSlots() != -1) {\n\t\t\t\tString parallelism =\n\t\t\t\t\t\tInteger.toString(yarnDescriptor.getTaskManagerSlots() * yarnDescriptor.getTaskManagerCount());\n\t\t\t\tyarnProps.setProperty(YARN_PROPERTIES_PARALLELISM, parallelism);\n\t\t\t}\n\t\t\t// add dynamic properties\n\t\t\tif (yarnDescriptor.getDynamicPropertiesEncoded() != null) {\n\t\t\t\tyarnProps.setProperty(YARN_PROPERTIES_DYNAMIC_PROPERTIES_STRING,\n\t\t\t\t\t\tyarnDescriptor.getDynamicPropertiesEncoded());\n\t\t\t}\n\t\t\twriteYarnProperties(yarnProps, yarnPropertiesFile);\n\n\t\t\t//------------------ ClusterClient running, let user control it ------------\n\n\t\t\tif (detachedMode) {\n\t\t\t\t// print info and quit:\n\t\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop it:\\n\" +\n\t\t\t\t\t\t\"yarn application -kill \" + yarnCluster.getApplicationId());\n\t\t\t\tyarnCluster.waitForClusterToBeReady();\n\t\t\t\tyarnCluster.disconnect();\n\t\t\t} else {\n\t\t\t\trunInteractiveCli(yarnCluster, acceptInteractiveInput);\n\t\t\t}\n\t\t}\n\t\treturn 0;\n\t}"
        ],
        [
            "YarnClusterClient::PollingThread::run()",
            " 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487 -\n 488  \n 489  \n 490  \n 491  ",
            "\t\t@Override\n\t\tpublic void run() {\n\t\t\twhile (running.get() && yarnClient.isInState(Service.STATE.STARTED)) {\n\t\t\t\ttry {\n\t\t\t\t\tApplicationReport report = yarnClient.getApplicationReport(appId);\n\t\t\t\t\tsynchronized (lock) {\n\t\t\t\t\t\tlastReport = report;\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tLOG.warn(\"Error while getting application report\", e);\n\t\t\t\t}\n\t\t\t\ttry {\n\t\t\t\t\tThread.sleep(YarnClusterClient.POLLING_THREAD_INTERVAL_MS);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\tLOG.error(\"Polling thread got interrupted\", e);\n\t\t\t\t\tThread.currentThread().interrupt(); // pass interrupt.\n\t\t\t\t\tstopRunner();\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(running.get() && !yarnClient.isInState(Service.STATE.STARTED)) {\n\t\t\t\t// == if the polling thread is still running but the yarn client is stopped.\n\t\t\t\tLOG.warn(\"YARN client is unexpected in state \" + yarnClient.getServiceState());\n\t\t\t}\n\t\t}",
            " 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486 +\n 487  \n 488  \n 489  \n 490  ",
            "\t\t@Override\n\t\tpublic void run() {\n\t\t\twhile (running.get() && yarnClient.isInState(Service.STATE.STARTED)) {\n\t\t\t\ttry {\n\t\t\t\t\tApplicationReport report = yarnClient.getApplicationReport(appId);\n\t\t\t\t\tsynchronized (lock) {\n\t\t\t\t\t\tlastReport = report;\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\tLOG.warn(\"Error while getting application report\", e);\n\t\t\t\t}\n\t\t\t\ttry {\n\t\t\t\t\tThread.sleep(YarnClusterClient.POLLING_THREAD_INTERVAL_MS);\n\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\tLOG.error(\"Polling thread got interrupted\", e);\n\t\t\t\t\tThread.currentThread().interrupt(); // pass interrupt.\n\t\t\t\t\tstopRunner();\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (running.get() && !yarnClient.isInState(Service.STATE.STARTED)) {\n\t\t\t\t// == if the polling thread is still running but the yarn client is stopped.\n\t\t\t\tLOG.warn(\"YARN client is unexpected in state \" + yarnClient.getServiceState());\n\t\t\t}\n\t\t}"
        ],
        [
            "FlinkYarnSessionCli::isActive(CommandLine,Configuration)",
            " 492  \n 493  \n 494  \n 495  \n 496 -\n 497  \n 498  ",
            "\t@Override\n\tpublic boolean isActive(CommandLine commandLine, Configuration configuration) {\n\t\tString jobManagerOption = commandLine.getOptionValue(ADDRESS_OPTION.getOpt(), null);\n\t\tboolean yarnJobManager = ID.equals(jobManagerOption);\n\t\tboolean yarnAppId = commandLine.hasOption(APPLICATION_ID.getOpt());\n\t\treturn yarnJobManager || yarnAppId || loadYarnPropertiesFile(commandLine, configuration) != null;\n\t}",
            " 492  \n 493  \n 494  \n 495  \n 496 +\n 497  \n 498  ",
            "\t@Override\n\tpublic boolean isActive(CommandLine commandLine, Configuration configuration) {\n\t\tString jobManagerOption = commandLine.getOptionValue(ADDRESS_OPTION.getOpt(), null);\n\t\tboolean yarnJobManager = ID.equals(jobManagerOption);\n\t\tboolean yarnAppId = commandLine.hasOption(applicationId.getOpt());\n\t\treturn yarnJobManager || yarnAppId || loadYarnPropertiesFile(commandLine, configuration) != null;\n\t}"
        ],
        [
            "YarnClusterClient::getApplicationStatus()",
            " 247  \n 248 -\n 249  \n 250  \n 251  \n 252 -\n 253  \n 254  \n 255  \n 256  \n 257  \n 258 -\n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267 -\n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "\tpublic ApplicationStatus getApplicationStatus() {\n\t\tif(!isConnected) {\n\t\t\tthrow new IllegalStateException(\"The cluster has been connected to the ApplicationMaster.\");\n\t\t}\n\t\tApplicationReport lastReport = null;\n\t\tif(pollingRunner == null) {\n\t\t\tLOG.warn(\"YarnClusterClient.getApplicationStatus() has been called on an uninitialized cluster.\" +\n\t\t\t\t\t\"The system might be in an erroneous state\");\n\t\t} else {\n\t\t\tlastReport = pollingRunner.getLastReport();\n\t\t}\n\t\tif(lastReport == null) {\n\t\t\tLOG.warn(\"YarnClusterClient.getApplicationStatus() has been called on a cluster that didn't receive a status so far.\" +\n\t\t\t\t\t\"The system might be in an erroneous state\");\n\t\t\treturn ApplicationStatus.UNKNOWN;\n\t\t} else {\n\t\t\tYarnApplicationState appState = lastReport.getYarnApplicationState();\n\t\t\tApplicationStatus status =\n\t\t\t\t(appState == YarnApplicationState.FAILED || appState == YarnApplicationState.KILLED) ?\n\t\t\t\t\tApplicationStatus.FAILED : ApplicationStatus.SUCCEEDED;\n\t\t\tif(status != ApplicationStatus.SUCCEEDED) {\n\t\t\t\tLOG.warn(\"YARN reported application state {}\", appState);\n\t\t\t\tLOG.warn(\"Diagnostics: {}\", lastReport.getDiagnostics());\n\t\t\t}\n\t\t\treturn status;\n\t\t}\n\t}",
            " 248  \n 249 +\n 250  \n 251  \n 252  \n 253 +\n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 +\n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "\tpublic ApplicationStatus getApplicationStatus() {\n\t\tif (!isConnected) {\n\t\t\tthrow new IllegalStateException(\"The cluster has been connected to the ApplicationMaster.\");\n\t\t}\n\t\tApplicationReport lastReport = null;\n\t\tif (pollingRunner == null) {\n\t\t\tLOG.warn(\"YarnClusterClient.getApplicationStatus() has been called on an uninitialized cluster.\" +\n\t\t\t\t\t\"The system might be in an erroneous state\");\n\t\t} else {\n\t\t\tlastReport = pollingRunner.getLastReport();\n\t\t}\n\t\tif (lastReport == null) {\n\t\t\tLOG.warn(\"YarnClusterClient.getApplicationStatus() has been called on a cluster that didn't receive a status so far.\" +\n\t\t\t\t\t\"The system might be in an erroneous state\");\n\t\t\treturn ApplicationStatus.UNKNOWN;\n\t\t} else {\n\t\t\tYarnApplicationState appState = lastReport.getYarnApplicationState();\n\t\t\tApplicationStatus status =\n\t\t\t\t(appState == YarnApplicationState.FAILED || appState == YarnApplicationState.KILLED) ?\n\t\t\t\t\tApplicationStatus.FAILED : ApplicationStatus.SUCCEEDED;\n\t\t\tif (status != ApplicationStatus.SUCCEEDED) {\n\t\t\t\tLOG.warn(\"YARN reported application state {}\", appState);\n\t\t\t\tLOG.warn(\"Diagnostics: {}\", lastReport.getDiagnostics());\n\t\t\t}\n\t\t\treturn status;\n\t\t}\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::setLocalJarPath(Path)",
            " 226  \n 227 -\n 228  \n 229  \n 230  \n 231  ",
            "\tpublic void setLocalJarPath(Path localJarPath) {\n\t\tif(!localJarPath.toString().endsWith(\"jar\")) {\n\t\t\tthrow new IllegalArgumentException(\"The passed jar path ('\" + localJarPath + \"') does not end with the 'jar' extension\");\n\t\t}\n\t\tthis.flinkJarPath = localJarPath;\n\t}",
            " 234  \n 235 +\n 236  \n 237  \n 238  \n 239  ",
            "\tpublic void setLocalJarPath(Path localJarPath) {\n\t\tif (!localJarPath.toString().endsWith(\"jar\")) {\n\t\t\tthrow new IllegalArgumentException(\"The passed jar path ('\" + localJarPath + \"') does not end with the 'jar' extension\");\n\t\t}\n\t\tthis.flinkJarPath = localJarPath;\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::allocateResource(int,int)",
            " 348  \n 349 -\n 350 -\n 351  \n 352  \n 353  \n 354  \n 355  \n 356  ",
            "\tprivate static boolean allocateResource(int[] nodeManagers, int toAllocate) {\n\t\tfor(int i = 0; i < nodeManagers.length; i++) {\n\t\t\tif(nodeManagers[i] >= toAllocate) {\n\t\t\t\tnodeManagers[i] -= toAllocate;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}",
            " 355  \n 356 +\n 357 +\n 358  \n 359  \n 360  \n 361  \n 362  \n 363  ",
            "\tprivate static boolean allocateResource(int[] nodeManagers, int toAllocate) {\n\t\tfor (int i = 0; i < nodeManagers.length; i++) {\n\t\t\tif (nodeManagers[i] >= toAllocate) {\n\t\t\t\tnodeManagers[i] -= toAllocate;\n\t\t\t\treturn true;\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}"
        ],
        [
            "YarnResourceManager::onShutdownRequest()",
            " 277  \n 278  \n 279  \n 280  \n 281  \n 282 -\n 283  \n 284  ",
            "\t@Override\n\tpublic void onShutdownRequest() {\n\t\ttry {\n\t\t\tshutDown();\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Fail to shutdown the YARN resource manager.\", e);\n\t\t}\n\t}",
            " 276  \n 277  \n 278  \n 279  \n 280  \n 281 +\n 282  \n 283  ",
            "\t@Override\n\tpublic void onShutdownRequest() {\n\t\ttry {\n\t\t\tshutDown();\n\t\t} catch (Exception e) {\n\t\t\tlog.warn(\"Fail to shutdown the YARN resource manager.\", e);\n\t\t}\n\t}"
        ],
        [
            "YarnResourceManager::requestYarnContainer(Resource,Priority)",
            " 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336 -\n 337  \n 338  ",
            "\tprivate void requestYarnContainer(Resource resource, Priority priority) {\n\t\tresourceManagerClient.addContainerRequest(\n\t\t\t\tnew AMRMClient.ContainerRequest(resource, null, null, priority));\n\t\t// make sure we transmit the request fast and receive fast news of granted allocations\n\t\tresourceManagerClient.setHeartbeatInterval(FAST_YARN_HEARTBEAT_INTERVAL_MS);\n\n\t\tnumPendingContainerRequests++;\n\t\tLOG.info(\"Requesting new TaskManager container pending requests: {}\",\n\t\t\t\tnumPendingContainerRequests);\n\t}",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 +\n 336  \n 337  ",
            "\tprivate void requestYarnContainer(Resource resource, Priority priority) {\n\t\tresourceManagerClient.addContainerRequest(\n\t\t\t\tnew AMRMClient.ContainerRequest(resource, null, null, priority));\n\t\t// make sure we transmit the request fast and receive fast news of granted allocations\n\t\tresourceManagerClient.setHeartbeatInterval(FAST_YARN_HEARTBEAT_INTERVAL_MS);\n\n\t\tnumPendingContainerRequests++;\n\t\tlog.info(\"Requesting new TaskManager container pending requests: {}\",\n\t\t\t\tnumPendingContainerRequests);\n\t}"
        ],
        [
            "YarnIntraNonHaMasterServicesTest::destroyHDFS()",
            "  87  \n  88  \n  89 -\n  90 -\n  91  \n  92 -\n  93 -\n  94  ",
            "\t@AfterClass\n\tpublic static void destroyHDFS() {\n\t\tif (HDFS_CLUSTER != null) {\n\t\t\tHDFS_CLUSTER.shutdown();\n\t\t}\n\t\tHDFS_CLUSTER = null;\n\t\tHDFS_ROOT_PATH = null;\n\t}",
            "  88  \n  89  \n  90 +\n  91 +\n  92  \n  93 +\n  94 +\n  95  ",
            "\t@AfterClass\n\tpublic static void destroyHDFS() {\n\t\tif (hdfsCluster != null) {\n\t\t\thdfsCluster.shutdown();\n\t\t}\n\t\thdfsCluster = null;\n\t\thdfsRootPath = null;\n\t}"
        ],
        [
            "YarnClusterClient::PollingThread::stopRunner()",
            " 455  \n 456 -\n 457  \n 458  \n 459  \n 460  ",
            "\t\tpublic void stopRunner() {\n\t\t\tif(!running.get()) {\n\t\t\t\tLOG.warn(\"Polling thread was already stopped\");\n\t\t\t}\n\t\t\trunning.set(false);\n\t\t}",
            " 454  \n 455 +\n 456  \n 457  \n 458  \n 459  ",
            "\t\tpublic void stopRunner() {\n\t\t\tif (!running.get()) {\n\t\t\t\tLOG.warn(\"Polling thread was already stopped\");\n\t\t\t}\n\t\t\trunning.set(false);\n\t\t}"
        ],
        [
            "AbstractYarnFlinkApplicationMasterRunner::run(String)",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 -\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "\t/**\n\t * The instance entry point for the YARN application master. Obtains user group\n\t * information and calls the main work method {@link #runApplicationMaster(org.apache.flink.configuration.Configuration)} as a\n\t * privileged action.\n\t *\n\t * @param args The command line arguments.\n\t * @return The process exit code.\n\t */\n\tprotected int run(String[] args) {\n\t\ttry {\n\t\t\tLOG.debug(\"All environment variables: {}\", ENV);\n\n\t\t\tfinal String yarnClientUsername = ENV.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\t\tPreconditions.checkArgument(yarnClientUsername != null, \"YARN client user name environment variable {} not set\",\n\t\t\t\tYarnConfigKeys.ENV_HADOOP_USER_NAME);\n\n\t\t\tfinal String currDir = ENV.get(Environment.PWD.key());\n\t\t\tPreconditions.checkArgument(currDir != null, \"Current working directory variable (%s) not set\", Environment.PWD.key());\n\t\t\tLOG.debug(\"Current working directory: {}\", currDir);\n\n\t\t\tfinal String remoteKeytabPath = ENV.get(YarnConfigKeys.KEYTAB_PATH);\n\t\t\tLOG.debug(\"Remote keytab path obtained {}\", remoteKeytabPath);\n\n\t\t\tfinal String remoteKeytabPrincipal = ENV.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\t\tLOG.info(\"Remote keytab principal obtained {}\", remoteKeytabPrincipal);\n\n\t\t\tString keytabPath = null;\n\t\t\tif(remoteKeytabPath != null) {\n\t\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\t\tkeytabPath = f.getAbsolutePath();\n\t\t\t\tLOG.debug(\"Keytab path: {}\", keytabPath);\n\t\t\t}\n\n\t\t\tUserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n\n\t\t\tLOG.info(\"YARN daemon is running as: {} Yarn client user obtainer: {}\",\n\t\t\t\t\tcurrentUser.getShortUserName(), yarnClientUsername );\n\n\t\t\t// Flink configuration\n\t\t\tfinal Map<String, String> dynamicProperties =\n\t\t\t\t\tFlinkYarnSessionCli.getDynamicProperties(ENV.get(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES));\n\t\t\tLOG.debug(\"YARN dynamic properties: {}\", dynamicProperties);\n\n\t\t\tfinal Configuration flinkConfig = createConfiguration(currDir, dynamicProperties);\n\t\t\tif (keytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);\n\t\t\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfiguration = null;\n\n\t\t\t//To support Yarn Secure Integration Test Scenario\n\t\t\tFile krb5Conf = new File(currDir, Utils.KRB5_FILE_NAME);\n\t\t\tif (krb5Conf.exists() && krb5Conf.canRead()) {\n\t\t\t\tString krb5Path = krb5Conf.getAbsolutePath();\n\t\t\t\tLOG.info(\"KRB5 Conf: {}\", krb5Path);\n\t\t\t\thadoopConfiguration = new org.apache.hadoop.conf.Configuration();\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\t\t\t}\n\n\t\t\tSecurityUtils.SecurityConfiguration sc;\n\t\t\tif(hadoopConfiguration != null) {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(flinkConfig, hadoopConfiguration);\n\t\t\t} else {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(flinkConfig);\n\t\t\t}\n\n\t\t\tSecurityUtils.install(sc);\n\n\t\t\t// Note that we use the \"appMasterHostname\" given by YARN here, to make sure\n\t\t\t// we use the hostnames given by YARN consistently throughout akka.\n\t\t\t// for akka \"localhost\" and \"localhost.localdomain\" are different actors.\n\t\t\tthis.appMasterHostname = ENV.get(Environment.NM_HOST.key());\n\t\t\tPreconditions.checkArgument(appMasterHostname != null,\n\t\t\t\t\t\"ApplicationMaster hostname variable %s not set\", Environment.NM_HOST.key());\n\t\t\tLOG.info(\"YARN assigned hostname for application master: {}\", appMasterHostname);\n\n\t\t\treturn SecurityUtils.getInstalledContext().runSecured(new Callable<Integer>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() throws Exception {\n\t\t\t\t\treturn runApplicationMaster(flinkConfig);\n\t\t\t\t}\n\t\t\t});\n\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\t}",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98 +\n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124 +\n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "\t/**\n\t * The instance entry point for the YARN application master. Obtains user group\n\t * information and calls the main work method {@link #runApplicationMaster(org.apache.flink.configuration.Configuration)} as a\n\t * privileged action.\n\t *\n\t * @param args The command line arguments.\n\t * @return The process exit code.\n\t */\n\tprotected int run(String[] args) {\n\t\ttry {\n\t\t\tLOG.debug(\"All environment variables: {}\", ENV);\n\n\t\t\tfinal String yarnClientUsername = ENV.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\t\tPreconditions.checkArgument(yarnClientUsername != null, \"YARN client user name environment variable {} not set\",\n\t\t\t\tYarnConfigKeys.ENV_HADOOP_USER_NAME);\n\n\t\t\tfinal String currDir = ENV.get(Environment.PWD.key());\n\t\t\tPreconditions.checkArgument(currDir != null, \"Current working directory variable (%s) not set\", Environment.PWD.key());\n\t\t\tLOG.debug(\"Current working directory: {}\", currDir);\n\n\t\t\tfinal String remoteKeytabPath = ENV.get(YarnConfigKeys.KEYTAB_PATH);\n\t\t\tLOG.debug(\"Remote keytab path obtained {}\", remoteKeytabPath);\n\n\t\t\tfinal String remoteKeytabPrincipal = ENV.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\t\tLOG.info(\"Remote keytab principal obtained {}\", remoteKeytabPrincipal);\n\n\t\t\tString keytabPath = null;\n\t\t\tif (remoteKeytabPath != null) {\n\t\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\t\tkeytabPath = f.getAbsolutePath();\n\t\t\t\tLOG.debug(\"Keytab path: {}\", keytabPath);\n\t\t\t}\n\n\t\t\tUserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n\n\t\t\tLOG.info(\"YARN daemon is running as: {} Yarn client user obtainer: {}\",\n\t\t\t\t\tcurrentUser.getShortUserName(), yarnClientUsername);\n\n\t\t\t// Flink configuration\n\t\t\tfinal Map<String, String> dynamicProperties =\n\t\t\t\t\tFlinkYarnSessionCli.getDynamicProperties(ENV.get(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES));\n\t\t\tLOG.debug(\"YARN dynamic properties: {}\", dynamicProperties);\n\n\t\t\tfinal Configuration flinkConfig = createConfiguration(currDir, dynamicProperties);\n\t\t\tif (keytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);\n\t\t\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfiguration = null;\n\n\t\t\t//To support Yarn Secure Integration Test Scenario\n\t\t\tFile krb5Conf = new File(currDir, Utils.KRB5_FILE_NAME);\n\t\t\tif (krb5Conf.exists() && krb5Conf.canRead()) {\n\t\t\t\tString krb5Path = krb5Conf.getAbsolutePath();\n\t\t\t\tLOG.info(\"KRB5 Conf: {}\", krb5Path);\n\t\t\t\thadoopConfiguration = new org.apache.hadoop.conf.Configuration();\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\t\t\t}\n\n\t\t\tSecurityUtils.SecurityConfiguration sc;\n\t\t\tif (hadoopConfiguration != null) {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(flinkConfig, hadoopConfiguration);\n\t\t\t} else {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(flinkConfig);\n\t\t\t}\n\n\t\t\tSecurityUtils.install(sc);\n\n\t\t\t// Note that we use the \"appMasterHostname\" given by YARN here, to make sure\n\t\t\t// we use the hostnames given by YARN consistently throughout akka.\n\t\t\t// for akka \"localhost\" and \"localhost.localdomain\" are different actors.\n\t\t\tthis.appMasterHostname = ENV.get(Environment.NM_HOST.key());\n\t\t\tPreconditions.checkArgument(appMasterHostname != null,\n\t\t\t\t\t\"ApplicationMaster hostname variable %s not set\", Environment.NM_HOST.key());\n\t\t\tLOG.info(\"YARN assigned hostname for application master: {}\", appMasterHostname);\n\n\t\t\treturn SecurityUtils.getInstalledContext().runSecured(new Callable<Integer>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() throws Exception {\n\t\t\t\t\treturn runApplicationMaster(flinkConfig);\n\t\t\t\t}\n\t\t\t});\n\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::setJobManagerMemory(int)",
            " 185  \n 186 -\n 187  \n 188 -\n 189  \n 190  \n 191  ",
            "\tpublic void setJobManagerMemory(int memoryMb) {\n\t\tif(memoryMb < MIN_JM_MEMORY) {\n\t\t\tthrow new IllegalArgumentException(\"The JobManager memory (\" + memoryMb + \") is below the minimum required memory amount \"\n\t\t\t\t+ \"of \" + MIN_JM_MEMORY+ \" MB\");\n\t\t}\n\t\tthis.jobManagerMemoryMb = memoryMb;\n\t}",
            " 193  \n 194 +\n 195  \n 196 +\n 197  \n 198  \n 199  ",
            "\tpublic void setJobManagerMemory(int memoryMb) {\n\t\tif (memoryMb < MIN_JM_MEMORY) {\n\t\t\tthrow new IllegalArgumentException(\"The JobManager memory (\" + memoryMb + \") is below the minimum required memory amount \"\n\t\t\t\t+ \"of \" + MIN_JM_MEMORY + \" MB\");\n\t\t}\n\t\tthis.jobManagerMemoryMb = memoryMb;\n\t}"
        ],
        [
            "YarnFlinkResourceManager::RegisterApplicationMasterResponseReflector::RegisterApplicationMasterResponseReflector(Logger)",
            " 627 -\n 628 -\n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  ",
            "\t\tpublic RegisterApplicationMasterResponseReflector(Logger LOG) {\n\t\t\tthis.logger = LOG;\n\n\t\t\ttry {\n\t\t\t\tmethod = RegisterApplicationMasterResponse.class\n\t\t\t\t\t.getMethod(\"getContainersFromPreviousAttempts\");\n\n\t\t\t} catch (NoSuchMethodException e) {\n\t\t\t\t// that happens in earlier Hadoop versions\n\t\t\t\tlogger.info(\"Cannot reconnect to previously allocated containers. \" +\n\t\t\t\t\t\"This YARN version does not support 'getContainersFromPreviousAttempts()'\");\n\t\t\t}\n\t\t}",
            " 625 +\n 626 +\n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  ",
            "\t\tpublic RegisterApplicationMasterResponseReflector(Logger log) {\n\t\t\tthis.logger = log;\n\n\t\t\ttry {\n\t\t\t\tmethod = RegisterApplicationMasterResponse.class\n\t\t\t\t\t.getMethod(\"getContainersFromPreviousAttempts\");\n\n\t\t\t} catch (NoSuchMethodException e) {\n\t\t\t\t// that happens in earlier Hadoop versions\n\t\t\t\tlogger.info(\"Cannot reconnect to previously allocated containers. \" +\n\t\t\t\t\t\"This YARN version does not support 'getContainersFromPreviousAttempts()'\");\n\t\t\t}\n\t\t}"
        ],
        [
            "YarnResourceManager::YarnResourceManager(RpcService,String,ResourceID,Configuration,Map,ResourceManagerConfiguration,HighAvailabilityServices,HeartbeatServices,SlotManager,MetricRegistry,JobLeaderIdService,FatalErrorHandler)",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "\tpublic YarnResourceManager(\n\t\t\tRpcService rpcService,\n\t\t\tString resourceManagerEndpointId,\n\t\t\tResourceID resourceId,\n\t\t\tConfiguration flinkConfig,\n\t\t\tMap<String, String> env,\n\t\t\tResourceManagerConfiguration resourceManagerConfiguration,\n\t\t\tHighAvailabilityServices highAvailabilityServices,\n\t\t\tHeartbeatServices heartbeatServices,\n\t\t\tSlotManager slotManager,\n\t\t\tMetricRegistry metricRegistry,\n\t\t\tJobLeaderIdService jobLeaderIdService,\n\t\t\tFatalErrorHandler fatalErrorHandler) {\n\t\tsuper(\n\t\t\trpcService,\n\t\t\tresourceManagerEndpointId,\n\t\t\tresourceId,\n\t\t\tresourceManagerConfiguration,\n\t\t\thighAvailabilityServices,\n\t\t\theartbeatServices,\n\t\t\tslotManager,\n\t\t\tmetricRegistry,\n\t\t\tjobLeaderIdService,\n\t\t\tfatalErrorHandler);\n\t\tthis.flinkConfig  = flinkConfig;\n\t\tthis.yarnConfig = new YarnConfiguration();\n\t\tthis.ENV = env;\n\t\tfinal int yarnHeartbeatIntervalMS = flinkConfig.getInteger(\n\t\t\t\tConfigConstants.YARN_HEARTBEAT_DELAY_SECONDS, DEFAULT_YARN_HEARTBEAT_INTERVAL_MS / 1000) * 1000;\n\n\t\tfinal long yarnExpiryIntervalMS = yarnConfig.getLong(\n\t\t\t\tYarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS,\n\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_EXPIRY_INTERVAL_MS);\n\n\t\tif (yarnHeartbeatIntervalMS >= yarnExpiryIntervalMS) {\n\t\t\tlog.warn(\"The heartbeat interval of the Flink Application master ({}) is greater \" +\n\t\t\t\t\t\"than YARN's expiry interval ({}). The application is likely to be killed by YARN.\",\n\t\t\t\t\tyarnHeartbeatIntervalMS, yarnExpiryIntervalMS);\n\t\t}\n\t\tyarnHeartbeatIntervalMillis = yarnHeartbeatIntervalMS;\n\t\tnumPendingContainerRequests = 0;\n\t}",
            " 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135 +\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "\tpublic YarnResourceManager(\n\t\t\tRpcService rpcService,\n\t\t\tString resourceManagerEndpointId,\n\t\t\tResourceID resourceId,\n\t\t\tConfiguration flinkConfig,\n\t\t\tMap<String, String> env,\n\t\t\tResourceManagerConfiguration resourceManagerConfiguration,\n\t\t\tHighAvailabilityServices highAvailabilityServices,\n\t\t\tHeartbeatServices heartbeatServices,\n\t\t\tSlotManager slotManager,\n\t\t\tMetricRegistry metricRegistry,\n\t\t\tJobLeaderIdService jobLeaderIdService,\n\t\t\tFatalErrorHandler fatalErrorHandler) {\n\t\tsuper(\n\t\t\trpcService,\n\t\t\tresourceManagerEndpointId,\n\t\t\tresourceId,\n\t\t\tresourceManagerConfiguration,\n\t\t\thighAvailabilityServices,\n\t\t\theartbeatServices,\n\t\t\tslotManager,\n\t\t\tmetricRegistry,\n\t\t\tjobLeaderIdService,\n\t\t\tfatalErrorHandler);\n\t\tthis.flinkConfig  = flinkConfig;\n\t\tthis.yarnConfig = new YarnConfiguration();\n\t\tthis.env = env;\n\t\tfinal int yarnHeartbeatIntervalMS = flinkConfig.getInteger(\n\t\t\t\tConfigConstants.YARN_HEARTBEAT_DELAY_SECONDS, DEFAULT_YARN_HEARTBEAT_INTERVAL_MS / 1000) * 1000;\n\n\t\tfinal long yarnExpiryIntervalMS = yarnConfig.getLong(\n\t\t\t\tYarnConfiguration.RM_AM_EXPIRY_INTERVAL_MS,\n\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_EXPIRY_INTERVAL_MS);\n\n\t\tif (yarnHeartbeatIntervalMS >= yarnExpiryIntervalMS) {\n\t\t\tlog.warn(\"The heartbeat interval of the Flink Application master ({}) is greater \" +\n\t\t\t\t\t\"than YARN's expiry interval ({}). The application is likely to be killed by YARN.\",\n\t\t\t\t\tyarnHeartbeatIntervalMS, yarnExpiryIntervalMS);\n\t\t}\n\t\tyarnHeartbeatIntervalMillis = yarnHeartbeatIntervalMS;\n\t\tnumPendingContainerRequests = 0;\n\t}"
        ],
        [
            "YarnTaskManagerRunner::runYarnTaskManager(String,Class)",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98 -\n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 -\n 157  \n 158  \n 159  \n 160  \n 161  ",
            "\tpublic static void runYarnTaskManager(String[] args, final Class<? extends YarnTaskManager> taskManager) throws IOException {\n\t\tEnvironmentInformation.logEnvironmentInfo(LOG, \"YARN TaskManager\", args);\n\t\tSignalHandler.register(LOG);\n\t\tJvmShutdownSafeguard.installAsShutdownHook(LOG);\n\n\t\t// try to parse the command line arguments\n\t\tfinal Configuration configuration;\n\t\ttry {\n\t\t\tconfiguration = TaskManager.parseArgsAndLoadConfig(args);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tLOG.error(t.getMessage(), t);\n\t\t\tSystem.exit(TaskManager.STARTUP_FAILURE_RETURN_CODE());\n\t\t\treturn;\n\t\t}\n\n\t\t// read the environment variables for YARN\n\t\tfinal Map<String, String> envs = System.getenv();\n\t\tfinal String yarnClientUsername = envs.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\tfinal String localDirs = envs.get(Environment.LOCAL_DIRS.key());\n\t\tLOG.info(\"Current working/local Directory: {}\", localDirs);\n\n\t\tfinal String currDir = envs.get(Environment.PWD.key());\n\t\tLOG.info(\"Current working Directory: {}\", currDir);\n\n\t\tfinal String remoteKeytabPath = envs.get(YarnConfigKeys.KEYTAB_PATH);\n\t\tLOG.info(\"TM: remoteKeytabPath obtained {}\", remoteKeytabPath);\n\n\t\tfinal String remoteKeytabPrincipal = envs.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\tLOG.info(\"TM: remoteKeytabPrincipal obtained {}\", remoteKeytabPrincipal);\n\n\t\t// configure local directory\n\t\tString flinkTempDirs = configuration.getString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, null);\n\t\tif (flinkTempDirs == null) {\n\t\t\tLOG.info(\"Setting directories for temporary file \" + localDirs);\n\t\t\tconfiguration.setString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, localDirs);\n\t\t}\n\t\telse {\n\t\t\tLOG.info(\"Overriding YARN's temporary file directories with those \" +\n\t\t\t\t\"specified in the Flink config: \" + flinkTempDirs);\n\t\t}\n\n\t\t// tell akka to die in case of an error\n\t\tconfiguration.setBoolean(AkkaOptions.JVM_EXIT_ON_FATAL_ERROR, true);\n\n\t\tString localKeytabPath = null;\n\t\tif(remoteKeytabPath != null) {\n\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\tlocalKeytabPath = f.getAbsolutePath();\n\t\t\tLOG.info(\"localKeytabPath: {}\", localKeytabPath);\n\t\t}\n\n\t\tUserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n\n\t\tLOG.info(\"YARN daemon is running as: {} Yarn client user obtainer: {}\",\n\t\t\t\tcurrentUser.getShortUserName(), yarnClientUsername );\n\n\t\t// Infer the resource identifier from the environment variable\n\t\tString containerID = Preconditions.checkNotNull(envs.get(YarnFlinkResourceManager.ENV_FLINK_CONTAINER_ID));\n\t\tfinal ResourceID resourceId = new ResourceID(containerID);\n\t\tLOG.info(\"ResourceID assigned for this container: {}\", resourceId);\n\n\t\ttry {\n\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfiguration = null;\n\n\t\t\t//To support Yarn Secure Integration Test Scenario\n\t\t\tFile krb5Conf = new File(currDir, Utils.KRB5_FILE_NAME);\n\t\t\tif (krb5Conf.exists() && krb5Conf.canRead()) {\n\t\t\t\tString krb5Path = krb5Conf.getAbsolutePath();\n\t\t\t\tLOG.info(\"KRB5 Conf: {}\", krb5Path);\n\t\t\t\thadoopConfiguration = new org.apache.hadoop.conf.Configuration();\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\t\t\t}\n\n\t\t\t// set keytab principal and replace path with the local path of the shipped keytab file in NodeManager\n\t\t\tif (localKeytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tconfiguration.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, localKeytabPath);\n\t\t\t\tconfiguration.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\tSecurityUtils.SecurityConfiguration sc;\n\t\t\tif (hadoopConfiguration != null) {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(configuration, hadoopConfiguration);\n\t\t\t} else {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(configuration);\n\t\t\t}\n\n\t\t\tSecurityUtils.install(sc);\n\n\t\t\tSecurityUtils.getInstalledContext().runSecured(new Callable<Object>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tTaskManager.selectNetworkInterfaceAndRunTaskManager(configuration, resourceId, taskManager);\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable t) {\n\t\t\t\t\t\tLOG.error(\"Error while starting the TaskManager\", t);\n\t\t\t\t\t\tSystem.exit(TaskManager.STARTUP_FAILURE_RETURN_CODE());\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t});\n\t\t} catch(Exception e) {\n\t\t\tLOG.error(\"Exception occurred while launching Task Manager\", e);\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\n\t}",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 +\n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  ",
            "\tpublic static void runYarnTaskManager(String[] args, final Class<? extends YarnTaskManager> taskManager) throws IOException {\n\t\tEnvironmentInformation.logEnvironmentInfo(LOG, \"YARN TaskManager\", args);\n\t\tSignalHandler.register(LOG);\n\t\tJvmShutdownSafeguard.installAsShutdownHook(LOG);\n\n\t\t// try to parse the command line arguments\n\t\tfinal Configuration configuration;\n\t\ttry {\n\t\t\tconfiguration = TaskManager.parseArgsAndLoadConfig(args);\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\tLOG.error(t.getMessage(), t);\n\t\t\tSystem.exit(TaskManager.STARTUP_FAILURE_RETURN_CODE());\n\t\t\treturn;\n\t\t}\n\n\t\t// read the environment variables for YARN\n\t\tfinal Map<String, String> envs = System.getenv();\n\t\tfinal String yarnClientUsername = envs.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\tfinal String localDirs = envs.get(Environment.LOCAL_DIRS.key());\n\t\tLOG.info(\"Current working/local Directory: {}\", localDirs);\n\n\t\tfinal String currDir = envs.get(Environment.PWD.key());\n\t\tLOG.info(\"Current working Directory: {}\", currDir);\n\n\t\tfinal String remoteKeytabPath = envs.get(YarnConfigKeys.KEYTAB_PATH);\n\t\tLOG.info(\"TM: remoteKeytabPath obtained {}\", remoteKeytabPath);\n\n\t\tfinal String remoteKeytabPrincipal = envs.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\tLOG.info(\"TM: remoteKeytabPrincipal obtained {}\", remoteKeytabPrincipal);\n\n\t\t// configure local directory\n\t\tString flinkTempDirs = configuration.getString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, null);\n\t\tif (flinkTempDirs == null) {\n\t\t\tLOG.info(\"Setting directories for temporary file \" + localDirs);\n\t\t\tconfiguration.setString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, localDirs);\n\t\t}\n\t\telse {\n\t\t\tLOG.info(\"Overriding YARN's temporary file directories with those \" +\n\t\t\t\t\"specified in the Flink config: \" + flinkTempDirs);\n\t\t}\n\n\t\t// tell akka to die in case of an error\n\t\tconfiguration.setBoolean(AkkaOptions.JVM_EXIT_ON_FATAL_ERROR, true);\n\n\t\tString localKeytabPath = null;\n\t\tif (remoteKeytabPath != null) {\n\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\tlocalKeytabPath = f.getAbsolutePath();\n\t\t\tLOG.info(\"localKeytabPath: {}\", localKeytabPath);\n\t\t}\n\n\t\tUserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n\n\t\tLOG.info(\"YARN daemon is running as: {} Yarn client user obtainer: {}\",\n\t\t\t\tcurrentUser.getShortUserName(), yarnClientUsername);\n\n\t\t// Infer the resource identifier from the environment variable\n\t\tString containerID = Preconditions.checkNotNull(envs.get(YarnFlinkResourceManager.ENV_FLINK_CONTAINER_ID));\n\t\tfinal ResourceID resourceId = new ResourceID(containerID);\n\t\tLOG.info(\"ResourceID assigned for this container: {}\", resourceId);\n\n\t\ttry {\n\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfiguration = null;\n\n\t\t\t//To support Yarn Secure Integration Test Scenario\n\t\t\tFile krb5Conf = new File(currDir, Utils.KRB5_FILE_NAME);\n\t\t\tif (krb5Conf.exists() && krb5Conf.canRead()) {\n\t\t\t\tString krb5Path = krb5Conf.getAbsolutePath();\n\t\t\t\tLOG.info(\"KRB5 Conf: {}\", krb5Path);\n\t\t\t\thadoopConfiguration = new org.apache.hadoop.conf.Configuration();\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\t\t\t}\n\n\t\t\t// set keytab principal and replace path with the local path of the shipped keytab file in NodeManager\n\t\t\tif (localKeytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tconfiguration.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, localKeytabPath);\n\t\t\t\tconfiguration.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\tSecurityUtils.SecurityConfiguration sc;\n\t\t\tif (hadoopConfiguration != null) {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(configuration, hadoopConfiguration);\n\t\t\t} else {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(configuration);\n\t\t\t}\n\n\t\t\tSecurityUtils.install(sc);\n\n\t\t\tSecurityUtils.getInstalledContext().runSecured(new Callable<Object>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\tTaskManager.selectNetworkInterfaceAndRunTaskManager(configuration, resourceId, taskManager);\n\t\t\t\t\t}\n\t\t\t\t\tcatch (Throwable t) {\n\t\t\t\t\t\tLOG.error(\"Error while starting the TaskManager\", t);\n\t\t\t\t\t\tSystem.exit(TaskManager.STARTUP_FAILURE_RETURN_CODE());\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t});\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"Exception occurred while launching Task Manager\", e);\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\n\t}"
        ],
        [
            "YarnApplicationMasterRunner::run(String)",
            " 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 -\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 -\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175 -\n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  ",
            "\t/**\n\t * The instance entry point for the YARN application master. Obtains user group\n\t * information and calls the main work method {@link #runApplicationMaster(Configuration)} as a\n\t * privileged action.\n\t *\n\t * @param args The command line arguments.\n\t * @return The process exit code.\n\t */\n\tprotected int run(String[] args) {\n\t\ttry {\n\t\t\tLOG.debug(\"All environment variables: {}\", ENV);\n\n\t\t\tfinal String yarnClientUsername = ENV.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\t\trequire(yarnClientUsername != null, \"YARN client user name environment variable {} not set\",\n\t\t\t\tYarnConfigKeys.ENV_HADOOP_USER_NAME);\n\n\t\t\tfinal String currDir = ENV.get(Environment.PWD.key());\n\t\t\trequire(currDir != null, \"Current working directory variable (%s) not set\", Environment.PWD.key());\n\t\t\tLOG.debug(\"Current working Directory: {}\", currDir);\n\n\t\t\tfinal String remoteKeytabPath = ENV.get(YarnConfigKeys.KEYTAB_PATH);\n\t\t\tLOG.debug(\"remoteKeytabPath obtained {}\", remoteKeytabPath);\n\n\t\t\tfinal String remoteKeytabPrincipal = ENV.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\t\tLOG.info(\"remoteKeytabPrincipal obtained {}\", remoteKeytabPrincipal);\n\n\t\t\tString keytabPath = null;\n\t\t\tif(remoteKeytabPath != null) {\n\t\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\t\tkeytabPath = f.getAbsolutePath();\n\t\t\t\tLOG.debug(\"keytabPath: {}\", keytabPath);\n\t\t\t}\n\n\t\t\tUserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n\n\t\t\tLOG.info(\"YARN daemon is running as: {} Yarn client user obtainer: {}\",\n\t\t\t\t\tcurrentUser.getShortUserName(), yarnClientUsername );\n\n\t\t\t// Flink configuration\n\t\t\tfinal Map<String, String> dynamicProperties =\n\t\t\t\tFlinkYarnSessionCli.getDynamicProperties(ENV.get(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES));\n\t\t\tLOG.debug(\"YARN dynamic properties: {}\", dynamicProperties);\n\n\t\t\tfinal Configuration flinkConfig = createConfiguration(currDir, dynamicProperties);\n\n\t\t\t// set keytab principal and replace path with the local path of the shipped keytab file in NodeManager\n\t\t\tif (keytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);\n\t\t\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfiguration = null;\n\n\t\t\t//To support Yarn Secure Integration Test Scenario\n\t\t\tFile krb5Conf = new File(currDir, Utils.KRB5_FILE_NAME);\n\t\t\tif(krb5Conf.exists() && krb5Conf.canRead()) {\n\t\t\t\tString krb5Path = krb5Conf.getAbsolutePath();\n\t\t\t\tLOG.info(\"KRB5 Conf: {}\", krb5Path);\n\t\t\t\thadoopConfiguration = new org.apache.hadoop.conf.Configuration();\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\t\t\t}\n\n\t\t\tSecurityUtils.SecurityConfiguration sc;\n\t\t\tif(hadoopConfiguration != null) {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(flinkConfig, hadoopConfiguration);\n\t\t\t} else {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(flinkConfig);\n\t\t\t}\n\n\t\t\tSecurityUtils.install(sc);\n\n\t\t\treturn SecurityUtils.getInstalledContext().runSecured(new Callable<Integer>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() {\n\t\t\t\t\treturn runApplicationMaster(flinkConfig);\n\t\t\t\t}\n\t\t\t});\n\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\t}",
            " 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143 +\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 +\n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 +\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  ",
            "\t/**\n\t * The instance entry point for the YARN application master. Obtains user group\n\t * information and calls the main work method {@link #runApplicationMaster(Configuration)} as a\n\t * privileged action.\n\t *\n\t * @param args The command line arguments.\n\t * @return The process exit code.\n\t */\n\tprotected int run(String[] args) {\n\t\ttry {\n\t\t\tLOG.debug(\"All environment variables: {}\", ENV);\n\n\t\t\tfinal String yarnClientUsername = ENV.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\t\trequire(yarnClientUsername != null, \"YARN client user name environment variable {} not set\",\n\t\t\t\tYarnConfigKeys.ENV_HADOOP_USER_NAME);\n\n\t\t\tfinal String currDir = ENV.get(Environment.PWD.key());\n\t\t\trequire(currDir != null, \"Current working directory variable (%s) not set\", Environment.PWD.key());\n\t\t\tLOG.debug(\"Current working Directory: {}\", currDir);\n\n\t\t\tfinal String remoteKeytabPath = ENV.get(YarnConfigKeys.KEYTAB_PATH);\n\t\t\tLOG.debug(\"remoteKeytabPath obtained {}\", remoteKeytabPath);\n\n\t\t\tfinal String remoteKeytabPrincipal = ENV.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\t\tLOG.info(\"remoteKeytabPrincipal obtained {}\", remoteKeytabPrincipal);\n\n\t\t\tString keytabPath = null;\n\t\t\tif (remoteKeytabPath != null) {\n\t\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\t\tkeytabPath = f.getAbsolutePath();\n\t\t\t\tLOG.debug(\"keytabPath: {}\", keytabPath);\n\t\t\t}\n\n\t\t\tUserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n\n\t\t\tLOG.info(\"YARN daemon is running as: {} Yarn client user obtainer: {}\",\n\t\t\t\t\tcurrentUser.getShortUserName(), yarnClientUsername);\n\n\t\t\t// Flink configuration\n\t\t\tfinal Map<String, String> dynamicProperties =\n\t\t\t\tFlinkYarnSessionCli.getDynamicProperties(ENV.get(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES));\n\t\t\tLOG.debug(\"YARN dynamic properties: {}\", dynamicProperties);\n\n\t\t\tfinal Configuration flinkConfig = createConfiguration(currDir, dynamicProperties);\n\n\t\t\t// set keytab principal and replace path with the local path of the shipped keytab file in NodeManager\n\t\t\tif (keytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);\n\t\t\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfiguration = null;\n\n\t\t\t//To support Yarn Secure Integration Test Scenario\n\t\t\tFile krb5Conf = new File(currDir, Utils.KRB5_FILE_NAME);\n\t\t\tif (krb5Conf.exists() && krb5Conf.canRead()) {\n\t\t\t\tString krb5Path = krb5Conf.getAbsolutePath();\n\t\t\t\tLOG.info(\"KRB5 Conf: {}\", krb5Path);\n\t\t\t\thadoopConfiguration = new org.apache.hadoop.conf.Configuration();\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\t\t\t}\n\n\t\t\tSecurityUtils.SecurityConfiguration sc;\n\t\t\tif (hadoopConfiguration != null) {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(flinkConfig, hadoopConfiguration);\n\t\t\t} else {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(flinkConfig);\n\t\t\t}\n\n\t\t\tSecurityUtils.install(sc);\n\n\t\t\treturn SecurityUtils.getInstalledContext().runSecured(new Callable<Integer>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() {\n\t\t\t\t\treturn runApplicationMaster(flinkConfig);\n\t\t\t\t}\n\t\t\t});\n\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\t}"
        ],
        [
            "FlinkYarnSessionCli::addRunOptions(Options)",
            " 505  \n 506  \n 507 -\n 508  \n 509  \n 510  ",
            "\t@Override\n\tpublic void addRunOptions(Options baseOptions) {\n\t\tfor (Object option : ALL_OPTIONS.getOptions()) {\n\t\t\tbaseOptions.addOption((Option) option);\n\t\t}\n\t}",
            " 505  \n 506  \n 507 +\n 508  \n 509  \n 510  ",
            "\t@Override\n\tpublic void addRunOptions(Options baseOptions) {\n\t\tfor (Object option : allOptions.getOptions()) {\n\t\t\tbaseOptions.addOption((Option) option);\n\t\t}\n\t}"
        ],
        [
            "YarnApplicationMasterRunner::createConfiguration(String,Map)",
            " 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525 -\n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  ",
            "\t/**\n\t * \n\t * @param baseDirectory\n\t * @param additional\n\t * \n\t * @return The configuration to be used by the TaskManagers.\n\t */\n\t@SuppressWarnings(\"deprecation\")\n\tprivate static Configuration createConfiguration(String baseDirectory, Map<String, String> additional) {\n\t\tLOG.info(\"Loading config from directory \" + baseDirectory);\n\n\t\tConfiguration configuration = GlobalConfiguration.loadConfiguration(baseDirectory);\n\n\t\t// add dynamic properties to JobManager configuration.\n\t\tfor (Map.Entry<String, String> property : additional.entrySet()) {\n\t\t\tconfiguration.setString(property.getKey(), property.getValue());\n\t\t}\n\n\t\t// override zookeeper namespace with user cli argument (if provided)\n\t\tString cliZKNamespace = ENV.get(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE);\n\t\tif (cliZKNamespace != null && !cliZKNamespace.isEmpty()) {\n\t\t\tconfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, cliZKNamespace);\n\t\t}\n\n\t\t// if a web monitor shall be started, set the port to random binding\n\t\tif (configuration.getInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 0) >= 0) {\n\t\t\tconfiguration.setInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 0);\n\t\t}\n\n\t\t// if the user has set the deprecated YARN-specific config keys, we add the \n\t\t// corresponding generic config keys instead. that way, later code needs not\n\t\t// deal with deprecated config keys\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(configuration,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_RATIO,\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO);\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(configuration,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_MIN,\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN);\n\n\t\tBootstrapTools.substituteDeprecatedConfigPrefix(configuration,\n\t\t\tConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX,\n\t\t\tConfigConstants.CONTAINERIZED_MASTER_ENV_PREFIX);\n\n\t\tBootstrapTools.substituteDeprecatedConfigPrefix(configuration,\n\t\t\tConfigConstants.YARN_TASK_MANAGER_ENV_PREFIX,\n\t\t\tConfigConstants.CONTAINERIZED_TASK_MANAGER_ENV_PREFIX);\n\n\t\treturn configuration;\n\t}",
            " 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517 +\n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  ",
            "\t/**\n\t * Reads the global configuration from the given directory and adds the given parameters to it.\n\t *\n\t * @param baseDirectory directory to load the configuration from\n\t * @param additional additional parameters to be included in the configuration\n\t *\n\t * @return The configuration to be used by the TaskManagers.\n\t */\n\t@SuppressWarnings(\"deprecation\")\n\tprivate static Configuration createConfiguration(String baseDirectory, Map<String, String> additional) {\n\t\tLOG.info(\"Loading config from directory \" + baseDirectory);\n\n\t\tConfiguration configuration = GlobalConfiguration.loadConfiguration(baseDirectory);\n\n\t\t// add dynamic properties to JobManager configuration.\n\t\tfor (Map.Entry<String, String> property : additional.entrySet()) {\n\t\t\tconfiguration.setString(property.getKey(), property.getValue());\n\t\t}\n\n\t\t// override zookeeper namespace with user cli argument (if provided)\n\t\tString cliZKNamespace = ENV.get(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE);\n\t\tif (cliZKNamespace != null && !cliZKNamespace.isEmpty()) {\n\t\t\tconfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, cliZKNamespace);\n\t\t}\n\n\t\t// if a web monitor shall be started, set the port to random binding\n\t\tif (configuration.getInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 0) >= 0) {\n\t\t\tconfiguration.setInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 0);\n\t\t}\n\n\t\t// if the user has set the deprecated YARN-specific config keys, we add the\n\t\t// corresponding generic config keys instead. that way, later code needs not\n\t\t// deal with deprecated config keys\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(configuration,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_RATIO,\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO);\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(configuration,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_MIN,\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN);\n\n\t\tBootstrapTools.substituteDeprecatedConfigPrefix(configuration,\n\t\t\tConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX,\n\t\t\tConfigConstants.CONTAINERIZED_MASTER_ENV_PREFIX);\n\n\t\tBootstrapTools.substituteDeprecatedConfigPrefix(configuration,\n\t\t\tConfigConstants.YARN_TASK_MANAGER_ENV_PREFIX,\n\t\t\tConfigConstants.CONTAINERIZED_TASK_MANAGER_ENV_PREFIX);\n\n\t\treturn configuration;\n\t}"
        ],
        [
            "FlinkYarnCLI::FlinkYarnCLI(String,String)",
            "  77  \n  78  \n  79 -\n  80 -\n  81 -\n  82 -\n  83 -\n  84 -\n  85 -\n  86 -\n  87 -\n  88 -\n  89 -\n  90 -\n  91 -\n  92 -\n  93 -\n  94 -\n  95  ",
            "\tpublic FlinkYarnCLI(String shortPrefix, String longPrefix) {\n\n\t\tQUEUE = new Option(shortPrefix + \"qu\", longPrefix + \"queue\", true, \"Specify YARN queue.\");\n\t\tSHIP_PATH = new Option(shortPrefix + \"t\", longPrefix + \"ship\", true, \"Ship files in the specified directory (t for transfer)\");\n\t\tFLINK_JAR = new Option(shortPrefix + \"j\", longPrefix + \"jar\", true, \"Path to Flink jar file\");\n\t\tJM_MEMORY = new Option(shortPrefix + \"jm\", longPrefix + \"jobManagerMemory\", true, \"Memory for JobManager Container [in MB]\");\n\t\tDYNAMIC_PROPERTIES = new Option(shortPrefix + \"D\", true, \"Dynamic properties\");\n\t\tDETACHED = new Option(shortPrefix + \"a\", longPrefix + \"attached\", false, \"Start attached\");\n\t\tZOOKEEPER_NAMESPACE = new Option(shortPrefix + \"z\", longPrefix + \"zookeeperNamespace\", true, \"Namespace to create the Zookeeper sub-paths for high availability mode\");\n\n\t\tALL_OPTIONS = new Options();\n\t\tALL_OPTIONS.addOption(FLINK_JAR);\n\t\tALL_OPTIONS.addOption(JM_MEMORY);\n\t\tALL_OPTIONS.addOption(QUEUE);\n\t\tALL_OPTIONS.addOption(SHIP_PATH);\n\t\tALL_OPTIONS.addOption(DYNAMIC_PROPERTIES);\n\t\tALL_OPTIONS.addOption(DETACHED);\n\t\tALL_OPTIONS.addOption(ZOOKEEPER_NAMESPACE);\n\t}",
            "  79  \n  80  \n  81 +\n  82 +\n  83 +\n  84 +\n  85 +\n  86 +\n  87 +\n  88 +\n  89 +\n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95 +\n  96 +\n  97  ",
            "\tpublic FlinkYarnCLI(String shortPrefix, String longPrefix) {\n\n\t\tqueue = new Option(shortPrefix + \"qu\", longPrefix + \"queue\", true, \"Specify YARN queue.\");\n\t\tshipPath = new Option(shortPrefix + \"t\", longPrefix + \"ship\", true, \"Ship files in the specified directory (t for transfer)\");\n\t\tflinkJar = new Option(shortPrefix + \"j\", longPrefix + \"jar\", true, \"Path to Flink jar file\");\n\t\tjmMemory = new Option(shortPrefix + \"jm\", longPrefix + \"jobManagerMemory\", true, \"Memory for JobManager Container [in MB]\");\n\t\tdynamicProperties = new Option(shortPrefix + \"D\", true, \"Dynamic properties\");\n\t\tdetached = new Option(shortPrefix + \"a\", longPrefix + \"attached\", false, \"Start attached\");\n\t\tzookeeperNamespace = new Option(shortPrefix + \"z\", longPrefix + \"zookeeperNamespace\", true, \"Namespace to create the Zookeeper sub-paths for high availability mode\");\n\n\t\tallOptions = new Options();\n\t\tallOptions.addOption(flinkJar);\n\t\tallOptions.addOption(jmMemory);\n\t\tallOptions.addOption(queue);\n\t\tallOptions.addOption(shipPath);\n\t\tallOptions.addOption(dynamicProperties);\n\t\tallOptions.addOption(detached);\n\t\tallOptions.addOption(zookeeperNamespace);\n\t}"
        ],
        [
            "YarnIntraNonHaMasterServicesTest::initConfig()",
            "  96  \n  97  \n  98  \n  99 -\n 100  ",
            "\t@Before\n\tpublic void initConfig() {\n\t\thadoopConfig = new org.apache.hadoop.conf.Configuration();\n\t\thadoopConfig.set(org.apache.hadoop.fs.FileSystem.FS_DEFAULT_NAME_KEY, HDFS_ROOT_PATH.toString());\n\t}",
            "  97  \n  98  \n  99  \n 100 +\n 101  ",
            "\t@Before\n\tpublic void initConfig() {\n\t\thadoopConfig = new org.apache.hadoop.conf.Configuration();\n\t\thadoopConfig.set(org.apache.hadoop.fs.FileSystem.FS_DEFAULT_NAME_KEY, hdfsRootPath.toString());\n\t}"
        ],
        [
            "YarnPreConfiguredMasterHaServicesTest::createHDFS()",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72 -\n  73 -\n  74  ",
            "\t@BeforeClass\n\tpublic static void createHDFS() throws Exception {\n\t\tAssume.assumeTrue(!OperatingSystem.isWindows());\n\n\t\tfinal File tempDir = TEMP_DIR.newFolder();\n\n\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, tempDir.getAbsolutePath());\n\n\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\tHDFS_CLUSTER = builder.build();\n\t\tHDFS_ROOT_PATH = new Path(HDFS_CLUSTER.getURI());\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74 +\n  75 +\n  76  ",
            "\t@BeforeClass\n\tpublic static void createHDFS() throws Exception {\n\t\tAssume.assumeTrue(!OperatingSystem.isWindows());\n\n\t\tfinal File tempDir = TEMP_DIR.newFolder();\n\n\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, tempDir.getAbsolutePath());\n\n\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\thdfsCluster = builder.build();\n\t\thdfsRootPath = new Path(hdfsCluster.getURI());\n\t}"
        ],
        [
            "YarnClusterClient::getWebInterfaceURL()",
            " 209  \n 210  \n 211  \n 212 -\n 213  \n 214  \n 215  \n 216  \n 217  ",
            "\t@Override\n\tpublic String getWebInterfaceURL() {\n\t\t// there seems to be a difference between HD 2.2.0 and 2.6.0\n\t\tif(!trackingURL.startsWith(\"http://\")) {\n\t\t\treturn \"http://\" + trackingURL;\n\t\t} else {\n\t\t\treturn trackingURL;\n\t\t}\n\t}",
            " 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  ",
            "\t@Override\n\tpublic String getWebInterfaceURL() {\n\t\t// there seems to be a difference between HD 2.2.0 and 2.6.0\n\t\tif (!trackingURL.startsWith(\"http://\")) {\n\t\t\treturn \"http://\" + trackingURL;\n\t\t} else {\n\t\t\treturn trackingURL;\n\t\t}\n\t}"
        ],
        [
            "YarnApplicationMasterRunner::runApplicationMaster(Configuration)",
            " 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301 -\n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317 -\n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329 -\n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363 -\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393 -\n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "\t/**\n\t * The main work method, must run as a privileged action.\n\t *\n\t * @return The return code for the Java process.\n\t */\n\tprotected int runApplicationMaster(Configuration config) {\n\t\tActorSystem actorSystem = null;\n\t\tWebMonitor webMonitor = null;\n\t\tHighAvailabilityServices highAvailabilityServices = null;\n\n\t\tint numberProcessors = Hardware.getNumberCPUCores();\n\n\t\tfinal ScheduledExecutorService futureExecutor = Executors.newScheduledThreadPool(\n\t\t\tnumberProcessors,\n\t\t\tnew ExecutorThreadFactory(\"yarn-jobmanager-future\"));\n\n\t\tfinal ExecutorService ioExecutor = Executors.newFixedThreadPool(\n\t\t\tnumberProcessors,\n\t\t\tnew ExecutorThreadFactory(\"yarn-jobmanager-io\"));\n\n\t\ttry {\n\t\t\t// ------- (1) load and parse / validate all configurations -------\n\n\t\t\t// loading all config values here has the advantage that the program fails fast, if any\n\t\t\t// configuration problem occurs\n\n\t\t\tfinal String currDir = ENV.get(Environment.PWD.key());\n\t\t\trequire(currDir != null, \"Current working directory variable (%s) not set\", Environment.PWD.key());\n\n\t\t\t// Note that we use the \"appMasterHostname\" given by YARN here, to make sure\n\t\t\t// we use the hostnames given by YARN consistently throughout akka.\n\t\t\t// for akka \"localhost\" and \"localhost.localdomain\" are different actors.\n\t\t\tfinal String appMasterHostname = ENV.get(Environment.NM_HOST.key());\n\t\t\trequire(appMasterHostname != null,\n\t\t\t\t\"ApplicationMaster hostname variable %s not set\", Environment.NM_HOST.key());\n\n\t\t\tLOG.info(\"YARN assigned hostname for application master: {}\", appMasterHostname);\n\n\t\t\t//Update keytab and principal path to reflect YARN container path location\n\t\t\tfinal String remoteKeytabPath = ENV.get(YarnConfigKeys.KEYTAB_PATH);\n\n\t\t\tfinal String remoteKeytabPrincipal = ENV.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\n\t\t\tString keytabPath = null;\n\t\t\tif (remoteKeytabPath != null) {\n\t\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\t\tkeytabPath = f.getAbsolutePath();\n\t\t\t\tLOG.info(\"keytabPath: {}\", keytabPath);\n\t\t\t}\n\t\t\tif (keytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tconfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);\n\t\t\t\tconfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\t// Hadoop/Yarn configuration (loads config data automatically from classpath files)\n\t\t\tfinal YarnConfiguration yarnConfig = new YarnConfiguration();\n\n\t\t\tfinal int taskManagerContainerMemory;\n\t\t\tfinal int numInitialTaskManagers;\n\t\t\tfinal int slotsPerTaskManager;\n\n\t\t\ttry {\n\t\t\t\ttaskManagerContainerMemory = Integer.parseInt(ENV.get(YarnConfigKeys.ENV_TM_MEMORY));\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t\tthrow new RuntimeException(\"Invalid value for \" + YarnConfigKeys.ENV_TM_MEMORY + \" : \"\n\t\t\t\t\t+ e.getMessage());\n\t\t\t}\n\t\t\ttry {\n\t\t\t\tnumInitialTaskManagers = Integer.parseInt(ENV.get(YarnConfigKeys.ENV_TM_COUNT));\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t\tthrow new RuntimeException(\"Invalid value for \" + YarnConfigKeys.ENV_TM_COUNT + \" : \"\n\t\t\t\t\t+ e.getMessage());\n\t\t\t}\n\t\t\ttry {\n\t\t\t\tslotsPerTaskManager = Integer.parseInt(ENV.get(YarnConfigKeys.ENV_SLOTS));\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t\tthrow new RuntimeException(\"Invalid value for \" + YarnConfigKeys.ENV_SLOTS + \" : \"\n\t\t\t\t\t+ e.getMessage());\n\t\t\t}\n\n\t\t\tfinal ContaineredTaskManagerParameters taskManagerParameters =\n\t\t\t\tContaineredTaskManagerParameters.create(config, taskManagerContainerMemory, slotsPerTaskManager);\n\n\t\t\tLOG.info(\"TaskManagers will be created with {} task slots\", taskManagerParameters.numSlots());\n\t\t\tLOG.info(\"TaskManagers will be started with container size {} MB, JVM heap size {} MB, \" +\n\t\t\t\t\"JVM direct memory limit {} MB\",\n\t\t\t\ttaskManagerParameters.taskManagerTotalMemoryMB(),\n\t\t\t\ttaskManagerParameters.taskManagerHeapSizeMB(),\n\t\t\t\ttaskManagerParameters.taskManagerDirectMemoryLimitMB());\n\n\n\t\t\t// ----------------- (2) start the actor system -------------------\n\n\t\t\t// try to start the actor system, JobManager and JobManager actor system\n\t\t\t// using the port range definition from the config.\n\t\t\tfinal String amPortRange = config.getString(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_MASTER_PORT,\n\t\t\t\t\tConfigConstants.DEFAULT_YARN_JOB_MANAGER_PORT);\n\n\t\t\tactorSystem = BootstrapTools.startActorSystem(config, appMasterHostname, amPortRange, LOG);\n\n\t\t\tfinal String akkaHostname = AkkaUtils.getAddress(actorSystem).host().get();\n\t\t\tfinal int akkaPort = (Integer) AkkaUtils.getAddress(actorSystem).port().get();\n\n\t\t\tLOG.info(\"Actor system bound to hostname {}.\", akkaHostname);\n\n\n\t\t\t// ---- (3) Generate the configuration for the TaskManagers\n\n\t\t\tfinal Configuration taskManagerConfig = BootstrapTools.generateTaskManagerConfiguration(\n\t\t\t\t\tconfig, akkaHostname, akkaPort, slotsPerTaskManager, TASKMANAGER_REGISTRATION_TIMEOUT);\n\t\t\tLOG.debug(\"TaskManager configuration: {}\", taskManagerConfig);\n\n\t\t\tfinal ContainerLaunchContext taskManagerContext = Utils.createTaskExecutorContext(\n\t\t\t\tconfig, yarnConfig, ENV,\n\t\t\t\ttaskManagerParameters, taskManagerConfig,\n\t\t\t\tcurrDir, getTaskManagerClass(), LOG);\n\n\n\t\t\t// ---- (4) start the actors and components in this order:\n\n\t\t\t// 1) JobManager & Archive (in non-HA case, the leader service takes this)\n\t\t\t// 2) Web Monitor (we need its port to register)\n\t\t\t// 3) Resource Master for YARN\n\t\t\t// 4) Process reapers for the JobManager and Resource Master\n\n\t\t\t// 0: Start the JobManager services\n\n\t\t\t// update the configuration used to create the high availability services\n\t\t\tconfig.setString(JobManagerOptions.ADDRESS, akkaHostname);\n\t\t\tconfig.setInteger(JobManagerOptions.PORT, akkaPort);\n\n\t\t\thighAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\t\tconfig,\n\t\t\t\tioExecutor,\n\t\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\t\t// 1: the JobManager\n\t\t\tLOG.debug(\"Starting JobManager actor\");\n\n\t\t\t// we start the JobManager with its standard name\n\t\t\tActorRef jobManager = JobManager.startJobManagerActors(\n\t\t\t\tconfig,\n\t\t\t\tactorSystem,\n\t\t\t\tfutureExecutor,\n\t\t\t\tioExecutor,\n\t\t\t\thighAvailabilityServices,\n\t\t\t\tnew Some<>(JobMaster.JOB_MANAGER_NAME),\n\t\t\t\tOption.<String>empty(),\n\t\t\t\tgetJobManagerClass(),\n\t\t\t\tgetArchivistClass())._1();\n\n\n\t\t\t// 2: the web monitor\n\t\t\tLOG.debug(\"Starting Web Frontend\");\n\n\t\t\twebMonitor = BootstrapTools.startWebMonitorIfConfigured(\n\t\t\t\tconfig,\n\t\t\t\thighAvailabilityServices,\n\t\t\t\tactorSystem,\n\t\t\t\tjobManager,\n\t\t\t\tLOG);\n\n\t\t\tString protocol = \"http://\";\n\t\t\tif (config.getBoolean(JobManagerOptions.WEB_SSL_ENABLED) && SSLUtils.getSSLEnabled(config)) {\n\t\t\t\tprotocol = \"https://\";\n\t\t\t}\n\t\t\tfinal String webMonitorURL = webMonitor == null ? null :\n\t\t\t\tprotocol + appMasterHostname + \":\" + webMonitor.getServerPort();\n\n\t\t\t// 3: Flink's Yarn ResourceManager\n\t\t\tLOG.debug(\"Starting YARN Flink Resource Manager\");\n\n\t\t\tProps resourceMasterProps = YarnFlinkResourceManager.createActorProps(\n\t\t\t\tgetResourceManagerClass(),\n\t\t\t\tconfig,\n\t\t\t\tyarnConfig,\n\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\tappMasterHostname,\n\t\t\t\twebMonitorURL,\n\t\t\t\ttaskManagerParameters,\n\t\t\t\ttaskManagerContext,\n\t\t\t\tnumInitialTaskManagers, \n\t\t\t\tLOG);\n\n\t\t\tActorRef resourceMaster = actorSystem.actorOf(resourceMasterProps);\n\n\t\t\t// 4: Process reapers\n\t\t\t// The process reapers ensure that upon unexpected actor death, the process exits\n\t\t\t// and does not stay lingering around unresponsive\n\n\t\t\tLOG.debug(\"Starting process reapers for JobManager and YARN Application Master\");\n\n\t\t\tactorSystem.actorOf(\n\t\t\t\tProps.create(ProcessReaper.class, resourceMaster, LOG, ACTOR_DIED_EXIT_CODE),\n\t\t\t\t\"YARN_Resource_Master_Process_Reaper\");\n\n\t\t\tactorSystem.actorOf(\n\t\t\t\tProps.create(ProcessReaper.class, jobManager, LOG, ACTOR_DIED_EXIT_CODE),\n\t\t\t\t\"JobManager_Process_Reaper\");\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\n\t\t\tif (webMonitor != null) {\n\t\t\t\ttry {\n\t\t\t\t\twebMonitor.stop();\n\t\t\t\t} catch (Throwable ignored) {\n\t\t\t\t\tLOG.warn(\"Failed to stop the web frontend\", t);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (actorSystem != null) {\n\t\t\t\ttry {\n\t\t\t\t\tactorSystem.shutdown();\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error shutting down actor system\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfutureExecutor.shutdownNow();\n\t\t\tioExecutor.shutdownNow();\n\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\n\t\t// everything started, we can wait until all is done or the process is killed\n\t\tLOG.info(\"YARN Application Master started\");\n\n\t\t// wait until everything is done\n\t\tactorSystem.awaitTermination();\n\n\t\t// if we get here, everything work out jolly all right, and we even exited smoothly\n\t\tif (webMonitor != null) {\n\t\t\ttry {\n\t\t\t\twebMonitor.stop();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to stop the web frontend\", t);\n\t\t\t}\n\t\t}\n\n\t\tif (highAvailabilityServices != null) {\n\t\t\ttry {\n\t\t\t\thighAvailabilityServices.close();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to stop the high availability services.\", t);\n\t\t\t}\n\t\t}\n\n\t\torg.apache.flink.runtime.concurrent.Executors.gracefulShutdown(\n\t\t\tAkkaUtils.getTimeout(config).toMillis(),\n\t\t\tTimeUnit.MILLISECONDS,\n\t\t\tfutureExecutor,\n\t\t\tioExecutor);\n\n\t\treturn 0;\n\t}",
            " 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385 +\n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  ",
            "\t/**\n\t * The main work method, must run as a privileged action.\n\t *\n\t * @return The return code for the Java process.\n\t */\n\tprotected int runApplicationMaster(Configuration config) {\n\t\tActorSystem actorSystem = null;\n\t\tWebMonitor webMonitor = null;\n\t\tHighAvailabilityServices highAvailabilityServices = null;\n\n\t\tint numberProcessors = Hardware.getNumberCPUCores();\n\n\t\tfinal ScheduledExecutorService futureExecutor = Executors.newScheduledThreadPool(\n\t\t\tnumberProcessors,\n\t\t\tnew ExecutorThreadFactory(\"yarn-jobmanager-future\"));\n\n\t\tfinal ExecutorService ioExecutor = Executors.newFixedThreadPool(\n\t\t\tnumberProcessors,\n\t\t\tnew ExecutorThreadFactory(\"yarn-jobmanager-io\"));\n\n\t\ttry {\n\t\t\t// ------- (1) load and parse / validate all configurations -------\n\n\t\t\t// loading all config values here has the advantage that the program fails fast, if any\n\t\t\t// configuration problem occurs\n\n\t\t\tfinal String currDir = ENV.get(Environment.PWD.key());\n\t\t\trequire(currDir != null, \"Current working directory variable (%s) not set\", Environment.PWD.key());\n\n\t\t\t// Note that we use the \"appMasterHostname\" given by YARN here, to make sure\n\t\t\t// we use the hostnames given by YARN consistently throughout akka.\n\t\t\t// for akka \"localhost\" and \"localhost.localdomain\" are different actors.\n\t\t\tfinal String appMasterHostname = ENV.get(Environment.NM_HOST.key());\n\t\t\trequire(appMasterHostname != null,\n\t\t\t\t\"ApplicationMaster hostname variable %s not set\", Environment.NM_HOST.key());\n\n\t\t\tLOG.info(\"YARN assigned hostname for application master: {}\", appMasterHostname);\n\n\t\t\t//Update keytab and principal path to reflect YARN container path location\n\t\t\tfinal String remoteKeytabPath = ENV.get(YarnConfigKeys.KEYTAB_PATH);\n\n\t\t\tfinal String remoteKeytabPrincipal = ENV.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\n\t\t\tString keytabPath = null;\n\t\t\tif (remoteKeytabPath != null) {\n\t\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\t\tkeytabPath = f.getAbsolutePath();\n\t\t\t\tLOG.info(\"keytabPath: {}\", keytabPath);\n\t\t\t}\n\t\t\tif (keytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tconfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);\n\t\t\t\tconfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\t// Hadoop/Yarn configuration (loads config data automatically from classpath files)\n\t\t\tfinal YarnConfiguration yarnConfig = new YarnConfiguration();\n\n\t\t\tfinal int taskManagerContainerMemory;\n\t\t\tfinal int numInitialTaskManagers;\n\t\t\tfinal int slotsPerTaskManager;\n\n\t\t\ttry {\n\t\t\t\ttaskManagerContainerMemory = Integer.parseInt(ENV.get(YarnConfigKeys.ENV_TM_MEMORY));\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t\tthrow new RuntimeException(\"Invalid value for \" + YarnConfigKeys.ENV_TM_MEMORY + \" : \"\n\t\t\t\t\t+ e.getMessage());\n\t\t\t}\n\t\t\ttry {\n\t\t\t\tnumInitialTaskManagers = Integer.parseInt(ENV.get(YarnConfigKeys.ENV_TM_COUNT));\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t\tthrow new RuntimeException(\"Invalid value for \" + YarnConfigKeys.ENV_TM_COUNT + \" : \"\n\t\t\t\t\t+ e.getMessage());\n\t\t\t}\n\t\t\ttry {\n\t\t\t\tslotsPerTaskManager = Integer.parseInt(ENV.get(YarnConfigKeys.ENV_SLOTS));\n\t\t\t} catch (NumberFormatException e) {\n\t\t\t\tthrow new RuntimeException(\"Invalid value for \" + YarnConfigKeys.ENV_SLOTS + \" : \"\n\t\t\t\t\t+ e.getMessage());\n\t\t\t}\n\n\t\t\tfinal ContaineredTaskManagerParameters taskManagerParameters =\n\t\t\t\tContaineredTaskManagerParameters.create(config, taskManagerContainerMemory, slotsPerTaskManager);\n\n\t\t\tLOG.info(\"TaskManagers will be created with {} task slots\", taskManagerParameters.numSlots());\n\t\t\tLOG.info(\"TaskManagers will be started with container size {} MB, JVM heap size {} MB, \" +\n\t\t\t\t\"JVM direct memory limit {} MB\",\n\t\t\t\ttaskManagerParameters.taskManagerTotalMemoryMB(),\n\t\t\t\ttaskManagerParameters.taskManagerHeapSizeMB(),\n\t\t\t\ttaskManagerParameters.taskManagerDirectMemoryLimitMB());\n\n\t\t\t// ----------------- (2) start the actor system -------------------\n\n\t\t\t// try to start the actor system, JobManager and JobManager actor system\n\t\t\t// using the port range definition from the config.\n\t\t\tfinal String amPortRange = config.getString(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_MASTER_PORT,\n\t\t\t\t\tConfigConstants.DEFAULT_YARN_JOB_MANAGER_PORT);\n\n\t\t\tactorSystem = BootstrapTools.startActorSystem(config, appMasterHostname, amPortRange, LOG);\n\n\t\t\tfinal String akkaHostname = AkkaUtils.getAddress(actorSystem).host().get();\n\t\t\tfinal int akkaPort = (Integer) AkkaUtils.getAddress(actorSystem).port().get();\n\n\t\t\tLOG.info(\"Actor system bound to hostname {}.\", akkaHostname);\n\n\t\t\t// ---- (3) Generate the configuration for the TaskManagers\n\n\t\t\tfinal Configuration taskManagerConfig = BootstrapTools.generateTaskManagerConfiguration(\n\t\t\t\t\tconfig, akkaHostname, akkaPort, slotsPerTaskManager, TASKMANAGER_REGISTRATION_TIMEOUT);\n\t\t\tLOG.debug(\"TaskManager configuration: {}\", taskManagerConfig);\n\n\t\t\tfinal ContainerLaunchContext taskManagerContext = Utils.createTaskExecutorContext(\n\t\t\t\tconfig, yarnConfig, ENV,\n\t\t\t\ttaskManagerParameters, taskManagerConfig,\n\t\t\t\tcurrDir, getTaskManagerClass(), LOG);\n\n\t\t\t// ---- (4) start the actors and components in this order:\n\n\t\t\t// 1) JobManager & Archive (in non-HA case, the leader service takes this)\n\t\t\t// 2) Web Monitor (we need its port to register)\n\t\t\t// 3) Resource Master for YARN\n\t\t\t// 4) Process reapers for the JobManager and Resource Master\n\n\t\t\t// 0: Start the JobManager services\n\n\t\t\t// update the configuration used to create the high availability services\n\t\t\tconfig.setString(JobManagerOptions.ADDRESS, akkaHostname);\n\t\t\tconfig.setInteger(JobManagerOptions.PORT, akkaPort);\n\n\t\t\thighAvailabilityServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\t\tconfig,\n\t\t\t\tioExecutor,\n\t\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\t\t// 1: the JobManager\n\t\t\tLOG.debug(\"Starting JobManager actor\");\n\n\t\t\t// we start the JobManager with its standard name\n\t\t\tActorRef jobManager = JobManager.startJobManagerActors(\n\t\t\t\tconfig,\n\t\t\t\tactorSystem,\n\t\t\t\tfutureExecutor,\n\t\t\t\tioExecutor,\n\t\t\t\thighAvailabilityServices,\n\t\t\t\tnew Some<>(JobMaster.JOB_MANAGER_NAME),\n\t\t\t\tOption.<String>empty(),\n\t\t\t\tgetJobManagerClass(),\n\t\t\t\tgetArchivistClass())._1();\n\n\t\t\t// 2: the web monitor\n\t\t\tLOG.debug(\"Starting Web Frontend\");\n\n\t\t\twebMonitor = BootstrapTools.startWebMonitorIfConfigured(\n\t\t\t\tconfig,\n\t\t\t\thighAvailabilityServices,\n\t\t\t\tactorSystem,\n\t\t\t\tjobManager,\n\t\t\t\tLOG);\n\n\t\t\tString protocol = \"http://\";\n\t\t\tif (config.getBoolean(JobManagerOptions.WEB_SSL_ENABLED) && SSLUtils.getSSLEnabled(config)) {\n\t\t\t\tprotocol = \"https://\";\n\t\t\t}\n\t\t\tfinal String webMonitorURL = webMonitor == null ? null :\n\t\t\t\tprotocol + appMasterHostname + \":\" + webMonitor.getServerPort();\n\n\t\t\t// 3: Flink's Yarn ResourceManager\n\t\t\tLOG.debug(\"Starting YARN Flink Resource Manager\");\n\n\t\t\tProps resourceMasterProps = YarnFlinkResourceManager.createActorProps(\n\t\t\t\tgetResourceManagerClass(),\n\t\t\t\tconfig,\n\t\t\t\tyarnConfig,\n\t\t\t\thighAvailabilityServices.getJobManagerLeaderRetriever(HighAvailabilityServices.DEFAULT_JOB_ID),\n\t\t\t\tappMasterHostname,\n\t\t\t\twebMonitorURL,\n\t\t\t\ttaskManagerParameters,\n\t\t\t\ttaskManagerContext,\n\t\t\t\tnumInitialTaskManagers,\n\t\t\t\tLOG);\n\n\t\t\tActorRef resourceMaster = actorSystem.actorOf(resourceMasterProps);\n\n\t\t\t// 4: Process reapers\n\t\t\t// The process reapers ensure that upon unexpected actor death, the process exits\n\t\t\t// and does not stay lingering around unresponsive\n\n\t\t\tLOG.debug(\"Starting process reapers for JobManager and YARN Application Master\");\n\n\t\t\tactorSystem.actorOf(\n\t\t\t\tProps.create(ProcessReaper.class, resourceMaster, LOG, ACTOR_DIED_EXIT_CODE),\n\t\t\t\t\"YARN_Resource_Master_Process_Reaper\");\n\n\t\t\tactorSystem.actorOf(\n\t\t\t\tProps.create(ProcessReaper.class, jobManager, LOG, ACTOR_DIED_EXIT_CODE),\n\t\t\t\t\"JobManager_Process_Reaper\");\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\n\t\t\tif (webMonitor != null) {\n\t\t\t\ttry {\n\t\t\t\t\twebMonitor.stop();\n\t\t\t\t} catch (Throwable ignored) {\n\t\t\t\t\tLOG.warn(\"Failed to stop the web frontend\", t);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (actorSystem != null) {\n\t\t\t\ttry {\n\t\t\t\t\tactorSystem.shutdown();\n\t\t\t\t} catch (Throwable tt) {\n\t\t\t\t\tLOG.error(\"Error shutting down actor system\", tt);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfutureExecutor.shutdownNow();\n\t\t\tioExecutor.shutdownNow();\n\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\n\t\t// everything started, we can wait until all is done or the process is killed\n\t\tLOG.info(\"YARN Application Master started\");\n\n\t\t// wait until everything is done\n\t\tactorSystem.awaitTermination();\n\n\t\t// if we get here, everything work out jolly all right, and we even exited smoothly\n\t\tif (webMonitor != null) {\n\t\t\ttry {\n\t\t\t\twebMonitor.stop();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to stop the web frontend\", t);\n\t\t\t}\n\t\t}\n\n\t\tif (highAvailabilityServices != null) {\n\t\t\ttry {\n\t\t\t\thighAvailabilityServices.close();\n\t\t\t} catch (Throwable t) {\n\t\t\t\tLOG.error(\"Failed to stop the high availability services.\", t);\n\t\t\t}\n\t\t}\n\n\t\torg.apache.flink.runtime.concurrent.Executors.gracefulShutdown(\n\t\t\tAkkaUtils.getTimeout(config).toMillis(),\n\t\t\tTimeUnit.MILLISECONDS,\n\t\t\tfutureExecutor,\n\t\t\tioExecutor);\n\n\t\treturn 0;\n\t}"
        ],
        [
            "FlinkYarnCLI::addRunOptions(Options)",
            " 202  \n 203  \n 204 -\n 205  \n 206  \n 207  ",
            "\t@Override\n\tpublic void addRunOptions(Options baseOptions) {\n\t\tfor (Object option : ALL_OPTIONS.getOptions()) {\n\t\t\tbaseOptions.addOption((Option) option);\n\t\t}\n\t}",
            " 204  \n 205  \n 206 +\n 207  \n 208  \n 209  ",
            "\t@Override\n\tpublic void addRunOptions(Options baseOptions) {\n\t\tfor (Object option : allOptions.getOptions()) {\n\t\t\tbaseOptions.addOption((Option) option);\n\t\t}\n\t}"
        ],
        [
            "YarnClusterClient::disconnect()",
            " 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154 -\n 155  \n 156  \n 157  \n 158  \n 159  \n 160  ",
            "\t/**\n\t * Disconnect from the Yarn cluster\n\t */\n\tpublic void disconnect() {\n\n\t\tif (hasBeenShutDown.getAndSet(true)) {\n\t\t\treturn;\n\t\t}\n\n\t\tif(!isConnected) {\n\t\t\tthrow new IllegalStateException(\"Can not disconnect from an unconnected cluster.\");\n\t\t}\n\n\t\tLOG.info(\"Disconnecting YarnClusterClient from ApplicationMaster\");\n\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(clientShutdownHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we are already in the shutdown hook\n\t\t}\n\n\t\ttry {\n\t\t\tpollingRunner.stopRunner();\n\t\t\tpollingRunner.join(1000);\n\t\t} catch(InterruptedException e) {\n\t\t\tLOG.warn(\"Shutdown of the polling runner was interrupted\", e);\n\t\t\tThread.currentThread().interrupt();\n\t\t}\n\n\t\tisConnected = false;\n\t}",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 +\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "\t/**\n\t * Disconnect from the Yarn cluster.\n\t */\n\tpublic void disconnect() {\n\n\t\tif (hasBeenShutDown.getAndSet(true)) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (!isConnected) {\n\t\t\tthrow new IllegalStateException(\"Can not disconnect from an unconnected cluster.\");\n\t\t}\n\n\t\tLOG.info(\"Disconnecting YarnClusterClient from ApplicationMaster\");\n\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(clientShutdownHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we are already in the shutdown hook\n\t\t}\n\n\t\ttry {\n\t\t\tpollingRunner.stopRunner();\n\t\t\tpollingRunner.join(1000);\n\t\t} catch (InterruptedException e) {\n\t\t\tLOG.warn(\"Shutdown of the polling runner was interrupted\", e);\n\t\t\tThread.currentThread().interrupt();\n\t\t}\n\n\t\tisConnected = false;\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::AbstractYarnClusterDescriptor()",
            " 148  \n 149  \n 150 -\n 151  \n 152  \n 153  \n 154 -\n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  ",
            "\tpublic AbstractYarnClusterDescriptor() {\n\t\t// for unit tests only\n\t\tif(System.getenv(\"IN_TESTS\") != null) {\n\t\t\ttry {\n\t\t\t\tconf.addResource(new File(System.getenv(\"YARN_CONF_DIR\") + \"/yarn-site.xml\").toURI().toURL());\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new RuntimeException(\"Error\",t);\n\t\t\t}\n\t\t}\n\n\t\t// tries to load the config through the environment, if it fails it can still be set through the setters\n\t\ttry {\n\t\t\tthis.configurationDirectory = CliFrontend.getConfigurationDirectoryFromEnv();\n\t\t\tthis.flinkConfiguration = GlobalConfiguration.loadConfiguration(configurationDirectory);\n\n\t\t\tFile confFile = new File(configurationDirectory + File.separator + GlobalConfiguration.FLINK_CONF_FILENAME);\n\t\t\tif (!confFile.exists()) {\n\t\t\t\tthrow new RuntimeException(\"Unable to locate configuration file in \" + confFile);\n\t\t\t}\n\t\t\tflinkConfigurationPath = new Path(confFile.getAbsolutePath());\n\n\t\t\tslots = flinkConfiguration.getInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 1);\n\n\t\t\tjobManagerMemoryMb = flinkConfiguration.getInteger(JobManagerOptions.JOB_MANAGER_HEAP_MEMORY);\n\t\t\ttaskManagerMemoryMb = flinkConfiguration.getInteger(TaskManagerOptions.TASK_MANAGER_HEAP_MEMORY);\n\n\t\t\tuserJarInclusion = getUserJarInclusionMode(flinkConfiguration);\n\t\t} catch (Exception e) {\n\t\t\tLOG.debug(\"Config couldn't be loaded from environment variable.\", e);\n\t\t}\n\t}",
            " 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "\tpublic AbstractYarnClusterDescriptor() {\n\t\t// for unit tests only\n\t\tif (System.getenv(\"IN_TESTS\") != null) {\n\t\t\ttry {\n\t\t\t\tconf.addResource(new File(System.getenv(\"YARN_CONF_DIR\") + \"/yarn-site.xml\").toURI().toURL());\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new RuntimeException(\"Error\", t);\n\t\t\t}\n\t\t}\n\n\t\t// tries to load the config through the environment, if it fails it can still be set through the setters\n\t\ttry {\n\t\t\tthis.configurationDirectory = CliFrontend.getConfigurationDirectoryFromEnv();\n\t\t\tthis.flinkConfiguration = GlobalConfiguration.loadConfiguration(configurationDirectory);\n\n\t\t\tFile confFile = new File(configurationDirectory + File.separator + GlobalConfiguration.FLINK_CONF_FILENAME);\n\t\t\tif (!confFile.exists()) {\n\t\t\t\tthrow new RuntimeException(\"Unable to locate configuration file in \" + confFile);\n\t\t\t}\n\t\t\tflinkConfigurationPath = new Path(confFile.getAbsolutePath());\n\n\t\t\tslots = flinkConfiguration.getInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 1);\n\n\t\t\tjobManagerMemoryMb = flinkConfiguration.getInteger(JobManagerOptions.JOB_MANAGER_HEAP_MEMORY);\n\t\t\ttaskManagerMemoryMb = flinkConfiguration.getInteger(TaskManagerOptions.TASK_MANAGER_HEAP_MEMORY);\n\n\t\t\tuserJarInclusion = getUserJarInclusionMode(flinkConfiguration);\n\t\t} catch (Exception e) {\n\t\t\tLOG.debug(\"Config couldn't be loaded from environment variable.\", e);\n\t\t}\n\t}"
        ],
        [
            "YarnPreConfiguredMasterHaServicesTest::testCloseAndCleanup()",
            " 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146 -\n 147 -\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "\t@Test\n\tpublic void testCloseAndCleanup() throws Exception {\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setString(YarnConfigOptions.APP_MASTER_RPC_ADDRESS, \"localhost\");\n\t\tflinkConfig.setInteger(YarnConfigOptions.APP_MASTER_RPC_PORT, 1427);\n\n\t\t// create the services\n\t\tYarnHighAvailabilityServices services = new YarnPreConfiguredMasterNonHaServices(\n\t\t\tflinkConfig,\n\t\t\thadoopConfig,\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\t\tservices.closeAndCleanupAllData();\n\n\t\tfinal FileSystem fileSystem = HDFS_ROOT_PATH.getFileSystem();\n\t\tfinal Path workDir = new Path(HDFS_CLUSTER.getFileSystem().getWorkingDirectory().toString());\n\t\t\n\t\ttry {\n\t\t\tfileSystem.getFileStatus(new Path(workDir, YarnHighAvailabilityServices.FLINK_RECOVERY_DATA_DIR));\n\t\t\tfail(\"Flink recovery data directory still exists\");\n\t\t}\n\t\tcatch (FileNotFoundException e) {\n\t\t\t// expected, because the directory should have been cleaned up\n\t\t}\n\n\t\tassertTrue(services.isClosed());\n\n\t\t// doing another cleanup when the services are closed should fail\n\t\ttry {\n\t\t\tservices.closeAndCleanupAllData();\n\t\t\tfail(\"should fail with an IllegalStateException\");\n\t\t} catch (IllegalStateException e) {\n\t\t\t// expected\n\t\t}\n\t}",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 +\n 148 +\n 149 +\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  ",
            "\t@Test\n\tpublic void testCloseAndCleanup() throws Exception {\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setString(YarnConfigOptions.APP_MASTER_RPC_ADDRESS, \"localhost\");\n\t\tflinkConfig.setInteger(YarnConfigOptions.APP_MASTER_RPC_PORT, 1427);\n\n\t\t// create the services\n\t\tYarnHighAvailabilityServices services = new YarnPreConfiguredMasterNonHaServices(\n\t\t\tflinkConfig,\n\t\t\thadoopConfig,\n\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\t\tservices.closeAndCleanupAllData();\n\n\t\tfinal FileSystem fileSystem = hdfsRootPath.getFileSystem();\n\t\tfinal Path workDir = new Path(hdfsCluster.getFileSystem().getWorkingDirectory().toString());\n\n\t\ttry {\n\t\t\tfileSystem.getFileStatus(new Path(workDir, YarnHighAvailabilityServices.FLINK_RECOVERY_DATA_DIR));\n\t\t\tfail(\"Flink recovery data directory still exists\");\n\t\t}\n\t\tcatch (FileNotFoundException e) {\n\t\t\t// expected, because the directory should have been cleaned up\n\t\t}\n\n\t\tassertTrue(services.isClosed());\n\n\t\t// doing another cleanup when the services are closed should fail\n\t\ttry {\n\t\t\tservices.closeAndCleanupAllData();\n\t\t\tfail(\"should fail with an IllegalStateException\");\n\t\t} catch (IllegalStateException e) {\n\t\t\t// expected\n\t\t}\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::setupApplicationMasterContainer(boolean,boolean,boolean)",
            "1275  \n1276  \n1277  \n1278  \n1279  \n1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  \n1288  \n1289  \n1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305 -\n1306  \n1307  \n1308  \n1309 -\n1310  \n1311  \n1312  \n1313  \n1314  \n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  ",
            "\tprotected ContainerLaunchContext setupApplicationMasterContainer(boolean hasLogback,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tboolean hasLog4j,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tboolean hasKrb5) {\n\t\t// ------------------ Prepare Application Master Container  ------------------------------\n\n\t\t// respect custom JVM options in the YAML file\n\t\tString javaOpts = flinkConfiguration.getString(CoreOptions.FLINK_JVM_OPTIONS);\n\t\tif (flinkConfiguration.getString(CoreOptions.FLINK_JM_JVM_OPTIONS).length() > 0) {\n\t\t\tjavaOpts += \" \" + flinkConfiguration.getString(CoreOptions.FLINK_JM_JVM_OPTIONS);\n\t\t}\n\t\t//applicable only for YarnMiniCluster secure test run\n\t\t//krb5.conf file will be available as local resource in JM/TM container\n\t\tif (hasKrb5) {\n\t\t\tjavaOpts += \" -Djava.security.krb5.conf=krb5.conf\";\n\t\t}\n\n\t\t// Set up the container launch context for the application master\n\t\tContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);\n\n\t\tfinal  Map<String, String> startCommandValues = new HashMap<>();\n\t\tstartCommandValues.put(\"java\", \"$JAVA_HOME/bin/java\");\n\t\tstartCommandValues.put(\"jvmmem\", \"-Xmx\" +\n\t\t\tUtils.calculateHeapSize(jobManagerMemoryMb, flinkConfiguration) +\n\t\t\t\"m\");\n\t\tstartCommandValues.put(\"jvmopts\", javaOpts);\n\t\tString logging = \"\";\n\n\t\tif (hasLogback || hasLog4j) {\n\t\t\tlogging = \"-Dlog.file=\\\"\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.log\\\"\";\n\n\t\t\tif(hasLogback) {\n\t\t\t\tlogging += \" -Dlogback.configurationFile=file:\" + CONFIG_FILE_LOGBACK_NAME;\n\t\t\t}\n\n\t\t\tif(hasLog4j) {\n\t\t\t\tlogging += \" -Dlog4j.configuration=file:\" + CONFIG_FILE_LOG4J_NAME;\n\t\t\t}\n\t\t}\n\n\t\tstartCommandValues.put(\"logging\", logging);\n\t\tstartCommandValues.put(\"class\", getApplicationMasterClass().getName());\n\t\tstartCommandValues.put(\"redirects\",\n\t\t\t\"1> \" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.out \" +\n\t\t\t\"2> \" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.err\");\n\t\tstartCommandValues.put(\"args\", \"\");\n\n\t\tfinal String commandTemplate = flinkConfiguration\n\t\t\t.getString(ConfigConstants.YARN_CONTAINER_START_COMMAND_TEMPLATE,\n\t\t\t\tConfigConstants.DEFAULT_YARN_CONTAINER_START_COMMAND_TEMPLATE);\n\t\tfinal String amCommand =\n\t\t\tBootstrapTools.getStartCommand(commandTemplate, startCommandValues);\n\n\t\tamContainer.setCommands(Collections.singletonList(amCommand));\n\n\t\tLOG.debug(\"Application Master start command: \" + amCommand);\n\n\t\treturn amContainer;\n\t}",
            "1280  \n1281  \n1282  \n1283  \n1284  \n1285  \n1286  \n1287  \n1288  \n1289  \n1290  \n1291  \n1292  \n1293  \n1294  \n1295  \n1296  \n1297  \n1298  \n1299  \n1300  \n1301  \n1302  \n1303  \n1304  \n1305  \n1306  \n1307  \n1308  \n1309  \n1310 +\n1311  \n1312  \n1313  \n1314 +\n1315  \n1316  \n1317  \n1318  \n1319  \n1320  \n1321  \n1322  \n1323  \n1324  \n1325  \n1326  \n1327  \n1328  \n1329  \n1330  \n1331  \n1332  \n1333  \n1334  \n1335  \n1336  \n1337  ",
            "\tprotected ContainerLaunchContext setupApplicationMasterContainer(boolean hasLogback,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tboolean hasLog4j,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tboolean hasKrb5) {\n\t\t// ------------------ Prepare Application Master Container  ------------------------------\n\n\t\t// respect custom JVM options in the YAML file\n\t\tString javaOpts = flinkConfiguration.getString(CoreOptions.FLINK_JVM_OPTIONS);\n\t\tif (flinkConfiguration.getString(CoreOptions.FLINK_JM_JVM_OPTIONS).length() > 0) {\n\t\t\tjavaOpts += \" \" + flinkConfiguration.getString(CoreOptions.FLINK_JM_JVM_OPTIONS);\n\t\t}\n\t\t//applicable only for YarnMiniCluster secure test run\n\t\t//krb5.conf file will be available as local resource in JM/TM container\n\t\tif (hasKrb5) {\n\t\t\tjavaOpts += \" -Djava.security.krb5.conf=krb5.conf\";\n\t\t}\n\n\t\t// Set up the container launch context for the application master\n\t\tContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);\n\n\t\tfinal  Map<String, String> startCommandValues = new HashMap<>();\n\t\tstartCommandValues.put(\"java\", \"$JAVA_HOME/bin/java\");\n\t\tstartCommandValues.put(\"jvmmem\", \"-Xmx\" +\n\t\t\tUtils.calculateHeapSize(jobManagerMemoryMb, flinkConfiguration) +\n\t\t\t\"m\");\n\t\tstartCommandValues.put(\"jvmopts\", javaOpts);\n\t\tString logging = \"\";\n\n\t\tif (hasLogback || hasLog4j) {\n\t\t\tlogging = \"-Dlog.file=\\\"\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.log\\\"\";\n\n\t\t\tif (hasLogback) {\n\t\t\t\tlogging += \" -Dlogback.configurationFile=file:\" + CONFIG_FILE_LOGBACK_NAME;\n\t\t\t}\n\n\t\t\tif (hasLog4j) {\n\t\t\t\tlogging += \" -Dlog4j.configuration=file:\" + CONFIG_FILE_LOG4J_NAME;\n\t\t\t}\n\t\t}\n\n\t\tstartCommandValues.put(\"logging\", logging);\n\t\tstartCommandValues.put(\"class\", getApplicationMasterClass().getName());\n\t\tstartCommandValues.put(\"redirects\",\n\t\t\t\"1> \" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.out \" +\n\t\t\t\"2> \" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.err\");\n\t\tstartCommandValues.put(\"args\", \"\");\n\n\t\tfinal String commandTemplate = flinkConfiguration\n\t\t\t.getString(ConfigConstants.YARN_CONTAINER_START_COMMAND_TEMPLATE,\n\t\t\t\tConfigConstants.DEFAULT_YARN_CONTAINER_START_COMMAND_TEMPLATE);\n\t\tfinal String amCommand =\n\t\t\tBootstrapTools.getStartCommand(commandTemplate, startCommandValues);\n\n\t\tamContainer.setCommands(Collections.singletonList(amCommand));\n\n\t\tLOG.debug(\"Application Master start command: \" + amCommand);\n\n\t\treturn amContainer;\n\t}"
        ],
        [
            "YarnResourceManager::shutDownApplication(ApplicationStatus,String)",
            " 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211 -\n 212  \n 213  ",
            "\t@Override\n\tprotected void shutDownApplication(ApplicationStatus finalStatus, String optionalDiagnostics) {\n\n\t\t// first, de-register from YARN\n\t\tFinalApplicationStatus yarnStatus = getYarnStatus(finalStatus);\n\t\tLOG.info(\"Unregistering application from the YARN Resource Manager\");\n\t\ttry {\n\t\t\tresourceManagerClient.unregisterApplicationMaster(yarnStatus, optionalDiagnostics, \"\");\n\t\t} catch (Throwable t) {\n\t\t\tLOG.error(\"Could not unregister the application master.\", t);\n\t\t}\n\t}",
            " 201  \n 202  \n 203  \n 204  \n 205  \n 206 +\n 207  \n 208  \n 209  \n 210 +\n 211  \n 212  ",
            "\t@Override\n\tprotected void shutDownApplication(ApplicationStatus finalStatus, String optionalDiagnostics) {\n\n\t\t// first, de-register from YARN\n\t\tFinalApplicationStatus yarnStatus = getYarnStatus(finalStatus);\n\t\tlog.info(\"Unregistering application from the YARN Resource Manager\");\n\t\ttry {\n\t\t\tresourceManagerClient.unregisterApplicationMaster(yarnStatus, optionalDiagnostics, \"\");\n\t\t} catch (Throwable t) {\n\t\t\tlog.error(\"Could not unregister the application master.\", t);\n\t\t}\n\t}"
        ],
        [
            "YarnFlinkApplicationMasterRunner::runApplicationMaster(Configuration)",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 -\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  ",
            "\t@Override\n\tprotected int runApplicationMaster(Configuration config) {\n\n\t\ttry {\n\t\t\t// ---- (1) create common services\n\n\t\t\t// try to start the rpc service\n\t\t\t// using the port range definition from the config.\n\t\t\tfinal String amPortRange = config.getString(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_MASTER_PORT,\n\t\t\t\t\tConfigConstants.DEFAULT_YARN_JOB_MANAGER_PORT);\n\n\t\t\tsynchronized (lock) {\n\t\t\t\tLOG.info(\"Starting High Availability Services\");\n\t\t\t\tcommonRpcService = createRpcService(config, appMasterHostname, amPortRange);\n\n\t\t\t\thaServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\t\t\tconfig,\n\t\t\t\t\tcommonRpcService.getExecutor(),\n\t\t\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\t\t\theartbeatServices = HeartbeatServices.fromConfiguration(config);\n\t\t\t\t\n\t\t\t\tmetricRegistry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(config));\n\n\t\t\t\t// ---- (2) init resource manager -------\n\t\t\t\tresourceManager = createResourceManager(config);\n\n\t\t\t\t// ---- (3) init job master parameters\n\t\t\t\tjobManagerRunner = createJobManagerRunner(config);\n\n\t\t\t\t// ---- (4) start the resource manager  and job manager runner:\n\t\t\t\tresourceManager.start();\n\t\t\t\tLOG.debug(\"YARN Flink Resource Manager started\");\n\n\t\t\t\tjobManagerRunner.start();\n\t\t\t\tLOG.debug(\"Job Manager Runner started\");\n\n\t\t\t\t// ---- (5) start the web monitor\n\t\t\t\t// TODO: add web monitor\n\t\t\t}\n\n\t\t\t// wait for resource manager to finish\n\t\t\tresourceManager.getTerminationFuture().get();\n\t\t\t// everything started, we can wait until all is done or the process is killed\n\t\t\tLOG.info(\"YARN Application Master finished\");\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\t\t\tshutdown(ApplicationStatus.FAILED, t.getMessage());\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\n\t\treturn 0;\n\t}",
            " 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 +\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  ",
            "\t@Override\n\tprotected int runApplicationMaster(Configuration config) {\n\n\t\ttry {\n\t\t\t// ---- (1) create common services\n\n\t\t\t// try to start the rpc service\n\t\t\t// using the port range definition from the config.\n\t\t\tfinal String amPortRange = config.getString(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_MASTER_PORT,\n\t\t\t\t\tConfigConstants.DEFAULT_YARN_JOB_MANAGER_PORT);\n\n\t\t\tsynchronized (lock) {\n\t\t\t\tLOG.info(\"Starting High Availability Services\");\n\t\t\t\tcommonRpcService = createRpcService(config, appMasterHostname, amPortRange);\n\n\t\t\t\thaServices = HighAvailabilityServicesUtils.createHighAvailabilityServices(\n\t\t\t\t\tconfig,\n\t\t\t\t\tcommonRpcService.getExecutor(),\n\t\t\t\t\tHighAvailabilityServicesUtils.AddressResolution.NO_ADDRESS_RESOLUTION);\n\n\t\t\t\theartbeatServices = HeartbeatServices.fromConfiguration(config);\n\n\t\t\t\tmetricRegistry = new MetricRegistry(MetricRegistryConfiguration.fromConfiguration(config));\n\n\t\t\t\t// ---- (2) init resource manager -------\n\t\t\t\tresourceManager = createResourceManager(config);\n\n\t\t\t\t// ---- (3) init job master parameters\n\t\t\t\tjobManagerRunner = createJobManagerRunner(config);\n\n\t\t\t\t// ---- (4) start the resource manager  and job manager runner:\n\t\t\t\tresourceManager.start();\n\t\t\t\tLOG.debug(\"YARN Flink Resource Manager started\");\n\n\t\t\t\tjobManagerRunner.start();\n\t\t\t\tLOG.debug(\"Job Manager Runner started\");\n\n\t\t\t\t// ---- (5) start the web monitor\n\t\t\t\t// TODO: add web monitor\n\t\t\t}\n\n\t\t\t// wait for resource manager to finish\n\t\t\tresourceManager.getTerminationFuture().get();\n\t\t\t// everything started, we can wait until all is done or the process is killed\n\t\t\tLOG.info(\"YARN Application Master finished\");\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\t\t\tshutdown(ApplicationStatus.FAILED, t.getMessage());\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\n\t\treturn 0;\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::startAppMaster(JobGraph,YarnClient,YarnClientApplication)",
            " 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672 -\n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755 -\n 756  \n 757 -\n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765 -\n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780 -\n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790 -\n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809 -\n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821 -\n 822 -\n 823  \n 824 -\n 825  \n 826  \n 827  \n 828 -\n 829  \n 830 -\n 831  \n 832  \n 833 -\n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848 -\n 849  \n 850 -\n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861 -\n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877 -\n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902 -\n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  ",
            "\tpublic ApplicationReport startAppMaster(JobGraph jobGraph, YarnClient yarnClient, YarnClientApplication yarnApplication) throws Exception {\n\n\t\t// ------------------ Set default file system scheme -------------------------\n\n\t\ttry {\n\t\t\torg.apache.flink.core.fs.FileSystem.setDefaultScheme(flinkConfiguration);\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error while setting the default \" +\n\t\t\t\t\t\"filesystem scheme from configuration.\", e);\n\t\t}\n\n\t\t// initialize file system\n\t\t// Copy the application master jar to the filesystem\n\t\t// Create a local resource to point to the destination jar path\n\t\tfinal FileSystem fs = FileSystem.get(conf);\n\n\t\t// hard coded check for the GoogleHDFS client because its not overriding the getScheme() method.\n\t\tif (!fs.getClass().getSimpleName().equals(\"GoogleHadoopFileSystem\") &&\n\t\t\t\tfs.getScheme().startsWith(\"file\")) {\n\t\t\tLOG.warn(\"The file system scheme is '\" + fs.getScheme() + \"'. This indicates that the \"\n\t\t\t\t\t+ \"specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.\"\n\t\t\t\t\t+ \"The Flink YARN client needs to store its files in a distributed file system\");\n\t\t}\n\n\t\tApplicationSubmissionContext appContext = yarnApplication.getApplicationSubmissionContext();\n\t\tSet<File> systemShipFiles = new HashSet<>(shipFiles.size());\n\t\tfor (File file : shipFiles) {\n\t\t\tsystemShipFiles.add(file.getAbsoluteFile());\n\t\t}\n\n\t\t//check if there is a logback or log4j file\n\t\tFile logbackFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOGBACK_NAME);\n\t\tfinal boolean hasLogback = logbackFile.exists();\n\t\tif (hasLogback) {\n\t\t\tsystemShipFiles.add(logbackFile);\n\t\t}\n\n\t\tFile log4jFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOG4J_NAME);\n\t\tfinal boolean hasLog4j = log4jFile.exists();\n\t\tif (hasLog4j) {\n\t\t\tsystemShipFiles.add(log4jFile);\n\t\t\tif (hasLogback) {\n\t\t\t\t// this means there is already a logback configuration file --> fail\n\t\t\t\tLOG.warn(\"The configuration directory ('\" + configurationDirectory + \"') contains both LOG4J and \" +\n\t\t\t\t\t\"Logback configuration files. Please delete or rename one of them.\");\n\t\t\t}\n\t\t}\n\n\t\taddLibFolderToShipFiles(systemShipFiles);\n\n\t\t// Set-up ApplicationSubmissionContext for the application\n\n\t\tfinal ApplicationId appId = appContext.getApplicationId();\n\n\t\t// ------------------ Add Zookeeper namespace to local flinkConfiguraton ------\n\t\tString zkNamespace = getZookeeperNamespace();\n\t\t// no user specified cli argument for namespace?\n\t\tif (zkNamespace == null || zkNamespace.isEmpty()) {\n\t\t\t// namespace defined in config? else use applicationId as default.\n\t\t\tzkNamespace = flinkConfiguration.getString(HighAvailabilityOptions.HA_CLUSTER_ID, String.valueOf(appId));\n\t\t\tsetZookeeperNamespace(zkNamespace);\n\t\t}\n\n\t\tflinkConfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\tif (HighAvailabilityMode.isHighAvailabilityModeActivated(flinkConfiguration)) {\n\t\t\t// activate re-execution of failed applications\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));\n\n\t\t\tactivateHighAvailabilitySupport(appContext);\n\t\t} else {\n\t\t\t// set number of application retries to 1 in the default case\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\t1));\n\t\t}\n\n\t\t// local resource map for Yarn\n\t\tfinal Map<String, LocalResource> localResources = new HashMap<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// list of remote paths (after upload)\n\t\tfinal List<Path> paths = new ArrayList<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// ship list that enables reuse of resources for task manager containers\n\t\tStringBuilder envShipFileList = new StringBuilder();\n\n\t\t// upload and register ship files\t\n\t\tList<String> systemClassPaths = uploadAndRegisterFiles(systemShipFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\n\t\tList<String> userClassPaths;\n\t\tif (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\tuserClassPaths = uploadAndRegisterFiles(userJarFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\t\t} else {\n\t\t\tuserClassPaths = Collections.emptyList();\n\t\t}\n\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.ORDER) {\n\t\t\tsystemClassPaths.addAll(userClassPaths);\n\t\t}\n\n\t\t// normalize classpath by sorting\n\t\tCollections.sort(systemClassPaths);\n\t\tCollections.sort(userClassPaths);\n\n\t\t// classpath assembler\n\t\tStringBuilder classPathBuilder = new StringBuilder();\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.FIRST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\t\tfor (String classPath : systemClassPaths) {\n\t\t\tclassPathBuilder.append(classPath).append(File.pathSeparator);\n\t\t}\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.LAST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\n\t\t// Setup jar for ApplicationMaster\n\t\tLocalResource appMasterJar = Records.newRecord(LocalResource.class);\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\tPath remotePathJar =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkJarPath, appMasterJar, fs.getHomeDirectory());\n\t\tPath remotePathConf =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkConfigurationPath, flinkConf, fs.getHomeDirectory());\n\t\tlocalResources.put(\"flink.jar\", appMasterJar);\n\t\tlocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\tpaths.add(remotePathJar);\n\t\tclassPathBuilder.append(\"flink.jar\").append(File.pathSeparator);\n\t\tpaths.add(remotePathConf);\n\t\tclassPathBuilder.append(\"flink-conf.yaml\").append(File.pathSeparator);\n\n\t\t// write job graph to tmp file and add it to local resource\n\t\t// TODO: server use user main method to generate job graph\n\t\tif (jobGraph != null) {\n\t\t\ttry {\n\t\t\t\tFile fp = File.createTempFile(appId.toString(), null);\n\t\t\t\tfp.deleteOnExit();\n\t\t\t\ttry (FileOutputStream output = new FileOutputStream(fp);\n\t\t\t\t\tObjectOutputStream obOutput = new ObjectOutputStream(output);){\n\t\t\t\t\tobOutput.writeObject(jobGraph);\n\t\t\t\t}\n\t\t\t\tLocalResource jobgraph = Records.newRecord(LocalResource.class);\n\t\t\t\tPath remoteJobGraph =\n\t\t\t\t\t\tUtils.setupLocalResource(fs, appId.toString(), new Path(fp.toURI()), jobgraph, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(\"job.graph\", jobgraph);\n\t\t\t\tpaths.add(remoteJobGraph);\n\t\t\t\tclassPathBuilder.append(\"job.graph\").append(File.pathSeparator);\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Add job graph to local resource fail\");\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\n\t\tPath yarnFilesDir = new Path(fs.getHomeDirectory(), \".flink/\" + appId + '/');\n\n\t\tFsPermission permission = new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n\t\tfs.setPermission(yarnFilesDir, permission); // set permission for path.\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\t//In Integration test setup, the Yarn containers created by YarnMiniCluster does not have the Yarn site XML\n\t\t//and KRB5 configuration files. We are adding these files as container local resources for the container\n\t\t//applications (JM/TMs) to have proper secure cluster setup\n\t\tPath remoteKrb5Path = null;\n\t\tPath remoteYarnSiteXmlPath = null;\n\t\tboolean hasKrb5 = false;\n\t\tif(System.getenv(\"IN_TESTS\") != null) {\n\t\t\tString krb5Config = System.getProperty(\"java.security.krb5.conf\");\n\t\t\tif(krb5Config != null && krb5Config.length() != 0) {\n\t\t\t\tFile krb5 = new File(krb5Config);\n\t\t\t\tLOG.info(\"Adding KRB5 configuration {} to the AM container local resource bucket\", krb5.getAbsolutePath());\n\t\t\t\tLocalResource krb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath krb5ConfPath = new Path(krb5.getAbsolutePath());\n\t\t\t\tremoteKrb5Path = Utils.setupLocalResource(fs, appId.toString(), krb5ConfPath, krb5ConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.KRB5_FILE_NAME, krb5ConfResource);\n\n\t\t\t\tFile f = new File(System.getenv(\"YARN_CONF_DIR\"),Utils.YARN_SITE_FILE_NAME);\n\t\t\t\tLOG.info(\"Adding Yarn configuration {} to the AM container local resource bucket\", f.getAbsolutePath());\n\t\t\t\tLocalResource yarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath yarnSitePath = new Path(f.getAbsolutePath());\n\t\t\t\tremoteYarnSiteXmlPath = Utils.setupLocalResource(fs, appId.toString(), yarnSitePath, yarnConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.YARN_SITE_FILE_NAME, yarnConfResource);\n\n\t\t\t\thasKrb5 = true;\n\t\t\t}\n\t\t}\n\n\t\t// setup security tokens\n\t\tLocalResource keytabResource = null;\n\t\tPath remotePathKeytab = null;\n\t\tString keytab = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);\n\t\tif(keytab != null) {\n\t\t\tLOG.info(\"Adding keytab {} to the AM container local resource bucket\", keytab);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(keytab);\n\t\t\tremotePathKeytab = Utils.setupLocalResource(fs, appId.toString(), keytabPath, keytabResource, fs.getHomeDirectory());\n\t\t\tlocalResources.put(Utils.KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\tfinal ContainerLaunchContext amContainer = setupApplicationMasterContainer(hasLogback, hasLog4j, hasKrb5);\n\n\t\tif ( UserGroupInformation.isSecurityEnabled() && keytab == null ) {\n\t\t\t//set tokens only when keytab is not provided\n\t\t\tLOG.info(\"Adding delegation token to the AM container..\");\n\t\t\tUtils.setTokensFor(amContainer, paths, conf);\n\t\t}\n\n\t\tamContainer.setLocalResources(localResources);\n\t\tfs.close();\n\n\t\t// Setup CLASSPATH and environment variables for ApplicationMaster\n\t\tfinal Map<String, String> appMasterEnv = new HashMap<>();\n\t\t// set user specified app master environment variables\n\t\tappMasterEnv.putAll(Utils.getEnvironmentVariables(ConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX, flinkConfiguration));\n\t\t// set Flink app class path\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_FLINK_CLASSPATH, classPathBuilder.toString());\n\n\t\t// set Flink on YARN internal configuration values\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_COUNT, String.valueOf(taskManagerCount));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_MEMORY, String.valueOf(taskManagerMemoryMb));\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_JAR_PATH, remotePathJar.toString() );\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_APP_ID, appId.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_HOME_DIR, fs.getHomeDirectory().toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_SHIP_FILES, envShipFileList.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_SLOTS, String.valueOf(slots));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_DETACHED, String.valueOf(detached));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE, getZookeeperNamespace());\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_YARN_FILES, yarnFilesDir.toUri().toString());\n\n\t\t// https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md#identity-on-an-insecure-cluster-hadoop_user_name\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif(keytabResource != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PATH, remotePathKeytab.toString() );\n\t\t\tString principal = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL);\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, principal );\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif(remoteYarnSiteXmlPath != null && remoteKrb5Path != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_YARN_SITE_XML_PATH, remoteYarnSiteXmlPath.toString());\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_KRB5_PATH, remoteKrb5Path.toString() );\n\t\t}\n\n\t\tif(dynamicPropertiesEncoded != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n\t\t}\n\n\t\t// set classpath from YARN configuration\n\t\tUtils.setupYarnClassPath(conf, appMasterEnv);\n\n\t\tamContainer.setEnvironment(appMasterEnv);\n\n\t\t// Set up resource type requirements for ApplicationMaster\n\t\tResource capability = Records.newRecord(Resource.class);\n\t\tcapability.setMemory(jobManagerMemoryMb);\n\t\tcapability.setVirtualCores(1);\n\n\t\tString name;\n\t\tif(customName == null) {\n\t\t\tname = \"Flink session with \" + taskManagerCount + \" TaskManagers\";\n\t\t\tif(detached) {\n\t\t\t\tname += \" (detached)\";\n\t\t\t}\n\t\t} else {\n\t\t\tname = customName;\n\t\t}\n\n\t\tappContext.setApplicationName(name);\n\t\tappContext.setApplicationType(\"Apache Flink\");\n\t\tappContext.setAMContainerSpec(amContainer);\n\t\tappContext.setResource(capability);\n\t\tif(yarnQueue != null) {\n\t\t\tappContext.setQueue(yarnQueue);\n\t\t}\n\n\t\tsetApplicationTags(appContext);\n\n\t\t// add a hook to clean up in case deployment fails\n\t\tThread deploymentFailureHook = new DeploymentFailureHook(yarnClient, yarnApplication, yarnFilesDir);\n\t\tRuntime.getRuntime().addShutdownHook(deploymentFailureHook);\n\t\tLOG.info(\"Submitting application master \" + appId);\n\t\tyarnClient.submitApplication(appContext);\n\n\t\tLOG.info(\"Waiting for the cluster to be allocated\");\n\t\tfinal long startTime = System.currentTimeMillis();\n\t\tApplicationReport report;\n\t\tYarnApplicationState lastAppState = YarnApplicationState.NEW;\n\t\tloop: while( true ) {\n\t\t\ttry {\n\t\t\t\treport = yarnClient.getApplicationReport(appId);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new YarnDeploymentException(\"Failed to deploy the cluster.\", e);\n\t\t\t}\n\t\t\tYarnApplicationState appState = report.getYarnApplicationState();\n\t\t\tLOG.debug(\"Application State: {}\", appState);\n\t\t\tswitch(appState) {\n\t\t\t\tcase FAILED:\n\t\t\t\tcase FINISHED: //TODO: the finished state may be valid in flip-6\n\t\t\t\tcase KILLED:\n\t\t\t\t\tthrow new YarnDeploymentException(\"The YARN application unexpectedly switched to state \"\n\t\t\t\t\t\t+ appState + \" during deployment. \\n\" +\n\t\t\t\t\t\t\"Diagnostics from YARN: \" + report.getDiagnostics() + \"\\n\" +\n\t\t\t\t\t\t\"If log aggregation is enabled on your cluster, use this command to further investigate the issue:\\n\" +\n\t\t\t\t\t\t\"yarn logs -applicationId \" + appId);\n\t\t\t\t\t//break ..\n\t\t\t\tcase RUNNING:\n\t\t\t\t\tLOG.info(\"YARN application has been deployed successfully.\");\n\t\t\t\t\tbreak loop;\n\t\t\t\tdefault:\n\t\t\t\t\tif (appState != lastAppState) {\n\t\t\t\t\t\tLOG.info(\"Deploying cluster, current state \" + appState);\n\t\t\t\t\t}\n\t\t\t\t\tif(System.currentTimeMillis() - startTime > 60000) {\n\t\t\t\t\t\tLOG.info(\"Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\");\n\t\t\t\t\t}\n\n\t\t\t}\n\t\t\tlastAppState = appState;\n\t\t\tThread.sleep(250);\n\t\t}\n\t\t// print the application id for user to cancel themselves.\n\t\tif (isDetachedMode()) {\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop \" +\n\t\t\t\t\t\"it:\\nyarn application -kill \" + appId + \"\\nPlease also note that the \" +\n\t\t\t\t\t\"temporary files of the YARN session in the home directoy will not be removed.\");\n\t\t}\n\t\t// since deployment was successful, remove the hook\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(deploymentFailureHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we're already in the shut down hook.\n\t\t}\n\t\treturn report;\n\t}",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678 +\n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761 +\n 762  \n 763 +\n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771 +\n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786 +\n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796 +\n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815 +\n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827 +\n 828 +\n 829  \n 830 +\n 831  \n 832  \n 833  \n 834 +\n 835  \n 836 +\n 837  \n 838  \n 839 +\n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854 +\n 855  \n 856 +\n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867 +\n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883 +\n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908 +\n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  ",
            "\tpublic ApplicationReport startAppMaster(JobGraph jobGraph, YarnClient yarnClient, YarnClientApplication yarnApplication) throws Exception {\n\n\t\t// ------------------ Set default file system scheme -------------------------\n\n\t\ttry {\n\t\t\torg.apache.flink.core.fs.FileSystem.setDefaultScheme(flinkConfiguration);\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error while setting the default \" +\n\t\t\t\t\t\"filesystem scheme from configuration.\", e);\n\t\t}\n\n\t\t// initialize file system\n\t\t// Copy the application master jar to the filesystem\n\t\t// Create a local resource to point to the destination jar path\n\t\tfinal FileSystem fs = FileSystem.get(conf);\n\n\t\t// hard coded check for the GoogleHDFS client because its not overriding the getScheme() method.\n\t\tif (!fs.getClass().getSimpleName().equals(\"GoogleHadoopFileSystem\") &&\n\t\t\t\tfs.getScheme().startsWith(\"file\")) {\n\t\t\tLOG.warn(\"The file system scheme is '\" + fs.getScheme() + \"'. This indicates that the \"\n\t\t\t\t\t+ \"specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.\"\n\t\t\t\t\t+ \"The Flink YARN client needs to store its files in a distributed file system\");\n\t\t}\n\n\t\tApplicationSubmissionContext appContext = yarnApplication.getApplicationSubmissionContext();\n\t\tSet<File> systemShipFiles = new HashSet<>(shipFiles.size());\n\t\tfor (File file : shipFiles) {\n\t\t\tsystemShipFiles.add(file.getAbsoluteFile());\n\t\t}\n\n\t\t//check if there is a logback or log4j file\n\t\tFile logbackFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOGBACK_NAME);\n\t\tfinal boolean hasLogback = logbackFile.exists();\n\t\tif (hasLogback) {\n\t\t\tsystemShipFiles.add(logbackFile);\n\t\t}\n\n\t\tFile log4jFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOG4J_NAME);\n\t\tfinal boolean hasLog4j = log4jFile.exists();\n\t\tif (hasLog4j) {\n\t\t\tsystemShipFiles.add(log4jFile);\n\t\t\tif (hasLogback) {\n\t\t\t\t// this means there is already a logback configuration file --> fail\n\t\t\t\tLOG.warn(\"The configuration directory ('\" + configurationDirectory + \"') contains both LOG4J and \" +\n\t\t\t\t\t\"Logback configuration files. Please delete or rename one of them.\");\n\t\t\t}\n\t\t}\n\n\t\taddLibFolderToShipFiles(systemShipFiles);\n\n\t\t// Set-up ApplicationSubmissionContext for the application\n\n\t\tfinal ApplicationId appId = appContext.getApplicationId();\n\n\t\t// ------------------ Add Zookeeper namespace to local flinkConfiguraton ------\n\t\tString zkNamespace = getZookeeperNamespace();\n\t\t// no user specified cli argument for namespace?\n\t\tif (zkNamespace == null || zkNamespace.isEmpty()) {\n\t\t\t// namespace defined in config? else use applicationId as default.\n\t\t\tzkNamespace = flinkConfiguration.getString(HighAvailabilityOptions.HA_CLUSTER_ID, String.valueOf(appId));\n\t\t\tsetZookeeperNamespace(zkNamespace);\n\t\t}\n\n\t\tflinkConfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\tif (HighAvailabilityMode.isHighAvailabilityModeActivated(flinkConfiguration)) {\n\t\t\t// activate re-execution of failed applications\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));\n\n\t\t\tactivateHighAvailabilitySupport(appContext);\n\t\t} else {\n\t\t\t// set number of application retries to 1 in the default case\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tflinkConfiguration.getInteger(\n\t\t\t\t\tConfigConstants.YARN_APPLICATION_ATTEMPTS,\n\t\t\t\t\t1));\n\t\t}\n\n\t\t// local resource map for Yarn\n\t\tfinal Map<String, LocalResource> localResources = new HashMap<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// list of remote paths (after upload)\n\t\tfinal List<Path> paths = new ArrayList<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// ship list that enables reuse of resources for task manager containers\n\t\tStringBuilder envShipFileList = new StringBuilder();\n\n\t\t// upload and register ship files\n\t\tList<String> systemClassPaths = uploadAndRegisterFiles(systemShipFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\n\t\tList<String> userClassPaths;\n\t\tif (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\tuserClassPaths = uploadAndRegisterFiles(userJarFiles, fs, appId.toString(), paths, localResources, envShipFileList);\n\t\t} else {\n\t\t\tuserClassPaths = Collections.emptyList();\n\t\t}\n\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.ORDER) {\n\t\t\tsystemClassPaths.addAll(userClassPaths);\n\t\t}\n\n\t\t// normalize classpath by sorting\n\t\tCollections.sort(systemClassPaths);\n\t\tCollections.sort(userClassPaths);\n\n\t\t// classpath assembler\n\t\tStringBuilder classPathBuilder = new StringBuilder();\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.FIRST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\t\tfor (String classPath : systemClassPaths) {\n\t\t\tclassPathBuilder.append(classPath).append(File.pathSeparator);\n\t\t}\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.LAST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\n\t\t// Setup jar for ApplicationMaster\n\t\tLocalResource appMasterJar = Records.newRecord(LocalResource.class);\n\t\tLocalResource flinkConf = Records.newRecord(LocalResource.class);\n\t\tPath remotePathJar =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkJarPath, appMasterJar, fs.getHomeDirectory());\n\t\tPath remotePathConf =\n\t\t\tUtils.setupLocalResource(fs, appId.toString(), flinkConfigurationPath, flinkConf, fs.getHomeDirectory());\n\t\tlocalResources.put(\"flink.jar\", appMasterJar);\n\t\tlocalResources.put(\"flink-conf.yaml\", flinkConf);\n\n\t\tpaths.add(remotePathJar);\n\t\tclassPathBuilder.append(\"flink.jar\").append(File.pathSeparator);\n\t\tpaths.add(remotePathConf);\n\t\tclassPathBuilder.append(\"flink-conf.yaml\").append(File.pathSeparator);\n\n\t\t// write job graph to tmp file and add it to local resource\n\t\t// TODO: server use user main method to generate job graph\n\t\tif (jobGraph != null) {\n\t\t\ttry {\n\t\t\t\tFile fp = File.createTempFile(appId.toString(), null);\n\t\t\t\tfp.deleteOnExit();\n\t\t\t\ttry (FileOutputStream output = new FileOutputStream(fp);\n\t\t\t\t\tObjectOutputStream obOutput = new ObjectOutputStream(output);){\n\t\t\t\t\tobOutput.writeObject(jobGraph);\n\t\t\t\t}\n\t\t\t\tLocalResource jobgraph = Records.newRecord(LocalResource.class);\n\t\t\t\tPath remoteJobGraph =\n\t\t\t\t\t\tUtils.setupLocalResource(fs, appId.toString(), new Path(fp.toURI()), jobgraph, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(\"job.graph\", jobgraph);\n\t\t\t\tpaths.add(remoteJobGraph);\n\t\t\t\tclassPathBuilder.append(\"job.graph\").append(File.pathSeparator);\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Add job graph to local resource fail\");\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\n\t\tPath yarnFilesDir = new Path(fs.getHomeDirectory(), \".flink/\" + appId + '/');\n\n\t\tFsPermission permission = new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n\t\tfs.setPermission(yarnFilesDir, permission); // set permission for path.\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\t//In Integration test setup, the Yarn containers created by YarnMiniCluster does not have the Yarn site XML\n\t\t//and KRB5 configuration files. We are adding these files as container local resources for the container\n\t\t//applications (JM/TMs) to have proper secure cluster setup\n\t\tPath remoteKrb5Path = null;\n\t\tPath remoteYarnSiteXmlPath = null;\n\t\tboolean hasKrb5 = false;\n\t\tif (System.getenv(\"IN_TESTS\") != null) {\n\t\t\tString krb5Config = System.getProperty(\"java.security.krb5.conf\");\n\t\t\tif (krb5Config != null && krb5Config.length() != 0) {\n\t\t\t\tFile krb5 = new File(krb5Config);\n\t\t\t\tLOG.info(\"Adding KRB5 configuration {} to the AM container local resource bucket\", krb5.getAbsolutePath());\n\t\t\t\tLocalResource krb5ConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath krb5ConfPath = new Path(krb5.getAbsolutePath());\n\t\t\t\tremoteKrb5Path = Utils.setupLocalResource(fs, appId.toString(), krb5ConfPath, krb5ConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.KRB5_FILE_NAME, krb5ConfResource);\n\n\t\t\t\tFile f = new File(System.getenv(\"YARN_CONF_DIR\"), Utils.YARN_SITE_FILE_NAME);\n\t\t\t\tLOG.info(\"Adding Yarn configuration {} to the AM container local resource bucket\", f.getAbsolutePath());\n\t\t\t\tLocalResource yarnConfResource = Records.newRecord(LocalResource.class);\n\t\t\t\tPath yarnSitePath = new Path(f.getAbsolutePath());\n\t\t\t\tremoteYarnSiteXmlPath = Utils.setupLocalResource(fs, appId.toString(), yarnSitePath, yarnConfResource, fs.getHomeDirectory());\n\t\t\t\tlocalResources.put(Utils.YARN_SITE_FILE_NAME, yarnConfResource);\n\n\t\t\t\thasKrb5 = true;\n\t\t\t}\n\t\t}\n\n\t\t// setup security tokens\n\t\tLocalResource keytabResource = null;\n\t\tPath remotePathKeytab = null;\n\t\tString keytab = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);\n\t\tif (keytab != null) {\n\t\t\tLOG.info(\"Adding keytab {} to the AM container local resource bucket\", keytab);\n\t\t\tkeytabResource = Records.newRecord(LocalResource.class);\n\t\t\tPath keytabPath = new Path(keytab);\n\t\t\tremotePathKeytab = Utils.setupLocalResource(fs, appId.toString(), keytabPath, keytabResource, fs.getHomeDirectory());\n\t\t\tlocalResources.put(Utils.KEYTAB_FILE_NAME, keytabResource);\n\t\t}\n\n\t\tfinal ContainerLaunchContext amContainer = setupApplicationMasterContainer(hasLogback, hasLog4j, hasKrb5);\n\n\t\tif (UserGroupInformation.isSecurityEnabled() && keytab == null) {\n\t\t\t//set tokens only when keytab is not provided\n\t\t\tLOG.info(\"Adding delegation token to the AM container..\");\n\t\t\tUtils.setTokensFor(amContainer, paths, conf);\n\t\t}\n\n\t\tamContainer.setLocalResources(localResources);\n\t\tfs.close();\n\n\t\t// Setup CLASSPATH and environment variables for ApplicationMaster\n\t\tfinal Map<String, String> appMasterEnv = new HashMap<>();\n\t\t// set user specified app master environment variables\n\t\tappMasterEnv.putAll(Utils.getEnvironmentVariables(ConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX, flinkConfiguration));\n\t\t// set Flink app class path\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_FLINK_CLASSPATH, classPathBuilder.toString());\n\n\t\t// set Flink on YARN internal configuration values\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_COUNT, String.valueOf(taskManagerCount));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_MEMORY, String.valueOf(taskManagerMemoryMb));\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_JAR_PATH, remotePathJar.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_APP_ID, appId.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_HOME_DIR, fs.getHomeDirectory().toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_SHIP_FILES, envShipFileList.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_SLOTS, String.valueOf(slots));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_DETACHED, String.valueOf(detached));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE, getZookeeperNamespace());\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_YARN_FILES, yarnFilesDir.toUri().toString());\n\n\t\t// https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md#identity-on-an-insecure-cluster-hadoop_user_name\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif (keytabResource != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PATH, remotePathKeytab.toString());\n\t\t\tString principal = flinkConfiguration.getString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL);\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, principal);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif (remoteYarnSiteXmlPath != null && remoteKrb5Path != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_YARN_SITE_XML_PATH, remoteYarnSiteXmlPath.toString());\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_KRB5_PATH, remoteKrb5Path.toString());\n\t\t}\n\n\t\tif (dynamicPropertiesEncoded != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n\t\t}\n\n\t\t// set classpath from YARN configuration\n\t\tUtils.setupYarnClassPath(conf, appMasterEnv);\n\n\t\tamContainer.setEnvironment(appMasterEnv);\n\n\t\t// Set up resource type requirements for ApplicationMaster\n\t\tResource capability = Records.newRecord(Resource.class);\n\t\tcapability.setMemory(jobManagerMemoryMb);\n\t\tcapability.setVirtualCores(1);\n\n\t\tString name;\n\t\tif (customName == null) {\n\t\t\tname = \"Flink session with \" + taskManagerCount + \" TaskManagers\";\n\t\t\tif (detached) {\n\t\t\t\tname += \" (detached)\";\n\t\t\t}\n\t\t} else {\n\t\t\tname = customName;\n\t\t}\n\n\t\tappContext.setApplicationName(name);\n\t\tappContext.setApplicationType(\"Apache Flink\");\n\t\tappContext.setAMContainerSpec(amContainer);\n\t\tappContext.setResource(capability);\n\t\tif (yarnQueue != null) {\n\t\t\tappContext.setQueue(yarnQueue);\n\t\t}\n\n\t\tsetApplicationTags(appContext);\n\n\t\t// add a hook to clean up in case deployment fails\n\t\tThread deploymentFailureHook = new DeploymentFailureHook(yarnClient, yarnApplication, yarnFilesDir);\n\t\tRuntime.getRuntime().addShutdownHook(deploymentFailureHook);\n\t\tLOG.info(\"Submitting application master \" + appId);\n\t\tyarnClient.submitApplication(appContext);\n\n\t\tLOG.info(\"Waiting for the cluster to be allocated\");\n\t\tfinal long startTime = System.currentTimeMillis();\n\t\tApplicationReport report;\n\t\tYarnApplicationState lastAppState = YarnApplicationState.NEW;\n\t\tloop: while (true) {\n\t\t\ttry {\n\t\t\t\treport = yarnClient.getApplicationReport(appId);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new YarnDeploymentException(\"Failed to deploy the cluster.\", e);\n\t\t\t}\n\t\t\tYarnApplicationState appState = report.getYarnApplicationState();\n\t\t\tLOG.debug(\"Application State: {}\", appState);\n\t\t\tswitch(appState) {\n\t\t\t\tcase FAILED:\n\t\t\t\tcase FINISHED: //TODO: the finished state may be valid in flip-6\n\t\t\t\tcase KILLED:\n\t\t\t\t\tthrow new YarnDeploymentException(\"The YARN application unexpectedly switched to state \"\n\t\t\t\t\t\t+ appState + \" during deployment. \\n\" +\n\t\t\t\t\t\t\"Diagnostics from YARN: \" + report.getDiagnostics() + \"\\n\" +\n\t\t\t\t\t\t\"If log aggregation is enabled on your cluster, use this command to further investigate the issue:\\n\" +\n\t\t\t\t\t\t\"yarn logs -applicationId \" + appId);\n\t\t\t\t\t//break ..\n\t\t\t\tcase RUNNING:\n\t\t\t\t\tLOG.info(\"YARN application has been deployed successfully.\");\n\t\t\t\t\tbreak loop;\n\t\t\t\tdefault:\n\t\t\t\t\tif (appState != lastAppState) {\n\t\t\t\t\t\tLOG.info(\"Deploying cluster, current state \" + appState);\n\t\t\t\t\t}\n\t\t\t\t\tif (System.currentTimeMillis() - startTime > 60000) {\n\t\t\t\t\t\tLOG.info(\"Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\");\n\t\t\t\t\t}\n\n\t\t\t}\n\t\t\tlastAppState = appState;\n\t\t\tThread.sleep(250);\n\t\t}\n\t\t// print the application id for user to cancel themselves.\n\t\tif (isDetachedMode()) {\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop \" +\n\t\t\t\t\t\"it:\\nyarn application -kill \" + appId + \"\\nPlease also note that the \" +\n\t\t\t\t\t\"temporary files of the YARN session in the home directoy will not be removed.\");\n\t\t}\n\t\t// since deployment was successful, remove the hook\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(deploymentFailureHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we're already in the shut down hook.\n\t\t}\n\t\treturn report;\n\t}"
        ],
        [
            "Utils::getEnvironmentVariables(String,org)",
            " 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265 -\n 266 -\n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "\t/**\n\t * Method to extract environment variables from the flinkConfiguration based on the given prefix String.\n\t *\n\t * @param envPrefix Prefix for the environment variables key\n\t * @param flinkConfiguration The Flink config to get the environment variable defintion from\n\t */\n\tpublic static Map<String, String> getEnvironmentVariables(String envPrefix, org.apache.flink.configuration.Configuration flinkConfiguration) {\n\t\tMap<String, String> result  = new HashMap<>();\n\t\tfor(Map.Entry<String, String> entry: flinkConfiguration.toMap().entrySet()) {\n\t\t\tif(entry.getKey().startsWith(envPrefix) && entry.getKey().length() > envPrefix.length()) {\n\t\t\t\t// remove prefix\n\t\t\t\tString key = entry.getKey().substring(envPrefix.length());\n\t\t\t\tresult.put(key, entry.getValue());\n\t\t\t}\n\t\t}\n\t\treturn result;\n\t}",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 +\n 262 +\n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  ",
            "\t/**\n\t * Method to extract environment variables from the flinkConfiguration based on the given prefix String.\n\t *\n\t * @param envPrefix Prefix for the environment variables key\n\t * @param flinkConfiguration The Flink config to get the environment variable defintion from\n\t */\n\tpublic static Map<String, String> getEnvironmentVariables(String envPrefix, org.apache.flink.configuration.Configuration flinkConfiguration) {\n\t\tMap<String, String> result  = new HashMap<>();\n\t\tfor (Map.Entry<String, String> entry: flinkConfiguration.toMap().entrySet()) {\n\t\t\tif (entry.getKey().startsWith(envPrefix) && entry.getKey().length() > envPrefix.length()) {\n\t\t\t\t// remove prefix\n\t\t\t\tString key = entry.getKey().substring(envPrefix.length());\n\t\t\t\tresult.put(key, entry.getValue());\n\t\t\t}\n\t\t}\n\t\treturn result;\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::isReadyForDeployment()",
            " 307  \n 308 -\n 309  \n 310  \n 311 -\n 312  \n 313  \n 314 -\n 315  \n 316  \n 317 -\n 318  \n 319  \n 320 -\n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340 -\n 341  \n 342  \n 343  \n 344  \n 345  \n 346  ",
            "\tprivate void isReadyForDeployment() throws YarnDeploymentException {\n\t\tif(taskManagerCount <= 0) {\n\t\t\tthrow new YarnDeploymentException(\"Taskmanager count must be positive\");\n\t\t}\n\t\tif(this.flinkJarPath == null) {\n\t\t\tthrow new YarnDeploymentException(\"The Flink jar path is null\");\n\t\t}\n\t\tif(this.configurationDirectory == null) {\n\t\t\tthrow new YarnDeploymentException(\"Configuration directory not set\");\n\t\t}\n\t\tif(this.flinkConfigurationPath == null) {\n\t\t\tthrow new YarnDeploymentException(\"Configuration path not set\");\n\t\t}\n\t\tif(this.flinkConfiguration == null) {\n\t\t\tthrow new YarnDeploymentException(\"Flink configuration object has not been set\");\n\t\t}\n\n\t\t// Check if we don't exceed YARN's maximum virtual cores.\n\t\t// The number of cores can be configured in the config.\n\t\t// If not configured, it is set to the number of task slots\n\t\tint numYarnVcores = conf.getInt(YarnConfiguration.NM_VCORES, YarnConfiguration.DEFAULT_NM_VCORES);\n\t\tint configuredVcores = flinkConfiguration.getInteger(ConfigConstants.YARN_VCORES, slots);\n\t\t// don't configure more than the maximum configured number of vcores\n\t\tif (configuredVcores > numYarnVcores) {\n\t\t\tthrow new IllegalConfigurationException(\n\t\t\t\tString.format(\"The number of virtual cores per node were configured with %d\" +\n\t\t\t\t\t\t\" but Yarn only has %d virtual cores available. Please note that the number\" +\n\t\t\t\t\t\t\" of virtual cores is set to the number of task slots by default unless configured\" +\n\t\t\t\t\t\t\" in the Flink config with '%s.'\",\n\t\t\t\t\tconfiguredVcores, numYarnVcores, ConfigConstants.YARN_VCORES));\n\t\t}\n\n\t\t// check if required Hadoop environment variables are set. If not, warn user\n\t\tif(System.getenv(\"HADOOP_CONF_DIR\") == null &&\n\t\t\tSystem.getenv(\"YARN_CONF_DIR\") == null) {\n\t\t\tLOG.warn(\"Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. \" +\n\t\t\t\t\"The Flink YARN Client needs one of these to be set to properly load the Hadoop \" +\n\t\t\t\t\"configuration for accessing YARN.\");\n\t\t}\n\t}",
            " 314  \n 315 +\n 316  \n 317  \n 318 +\n 319  \n 320  \n 321 +\n 322  \n 323  \n 324 +\n 325  \n 326  \n 327 +\n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347 +\n 348  \n 349  \n 350  \n 351  \n 352  \n 353  ",
            "\tprivate void isReadyForDeployment() throws YarnDeploymentException {\n\t\tif (taskManagerCount <= 0) {\n\t\t\tthrow new YarnDeploymentException(\"Taskmanager count must be positive\");\n\t\t}\n\t\tif (this.flinkJarPath == null) {\n\t\t\tthrow new YarnDeploymentException(\"The Flink jar path is null\");\n\t\t}\n\t\tif (this.configurationDirectory == null) {\n\t\t\tthrow new YarnDeploymentException(\"Configuration directory not set\");\n\t\t}\n\t\tif (this.flinkConfigurationPath == null) {\n\t\t\tthrow new YarnDeploymentException(\"Configuration path not set\");\n\t\t}\n\t\tif (this.flinkConfiguration == null) {\n\t\t\tthrow new YarnDeploymentException(\"Flink configuration object has not been set\");\n\t\t}\n\n\t\t// Check if we don't exceed YARN's maximum virtual cores.\n\t\t// The number of cores can be configured in the config.\n\t\t// If not configured, it is set to the number of task slots\n\t\tint numYarnVcores = conf.getInt(YarnConfiguration.NM_VCORES, YarnConfiguration.DEFAULT_NM_VCORES);\n\t\tint configuredVcores = flinkConfiguration.getInteger(ConfigConstants.YARN_VCORES, slots);\n\t\t// don't configure more than the maximum configured number of vcores\n\t\tif (configuredVcores > numYarnVcores) {\n\t\t\tthrow new IllegalConfigurationException(\n\t\t\t\tString.format(\"The number of virtual cores per node were configured with %d\" +\n\t\t\t\t\t\t\" but Yarn only has %d virtual cores available. Please note that the number\" +\n\t\t\t\t\t\t\" of virtual cores is set to the number of task slots by default unless configured\" +\n\t\t\t\t\t\t\" in the Flink config with '%s.'\",\n\t\t\t\t\tconfiguredVcores, numYarnVcores, ConfigConstants.YARN_VCORES));\n\t\t}\n\n\t\t// check if required Hadoop environment variables are set. If not, warn user\n\t\tif (System.getenv(\"HADOOP_CONF_DIR\") == null &&\n\t\t\tSystem.getenv(\"YARN_CONF_DIR\") == null) {\n\t\t\tLOG.warn(\"Neither the HADOOP_CONF_DIR nor the YARN_CONF_DIR environment variable is set. \" +\n\t\t\t\t\"The Flink YARN Client needs one of these to be set to properly load the Hadoop \" +\n\t\t\t\t\"configuration for accessing YARN.\");\n\t\t}\n\t}"
        ],
        [
            "UtilsTest::testYarnFlinkResourceManagerJobManagerLostLeadership()",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206 -\n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  ",
            "\t@Test\n\tpublic void testYarnFlinkResourceManagerJobManagerLostLeadership() throws Exception {\n\t\tnew JavaTestKit(system) {{\n\n\t\t\tfinal Deadline deadline = new FiniteDuration(3, TimeUnit.MINUTES).fromNow();\n\n\t\t\tConfiguration flinkConfig = new Configuration();\n\t\t\tYarnConfiguration yarnConfig = new YarnConfiguration();\n\t\t\tTestingLeaderRetrievalService leaderRetrievalService = new TestingLeaderRetrievalService(\n\t\t\t\tnull,\n\t\t\t\tnull);\n\t\t\tString applicationMasterHostName = \"localhost\";\n\t\t\tString webInterfaceURL = \"foobar\";\n\t\t\tContaineredTaskManagerParameters taskManagerParameters = new ContaineredTaskManagerParameters(\n\t\t\t\t1l, 1l, 1l, 1, new HashMap<String, String>());\n\t\t\tContainerLaunchContext taskManagerLaunchContext = mock(ContainerLaunchContext.class);\n\t\t\tint yarnHeartbeatIntervalMillis = 1000;\n\t\t\tint maxFailedContainers = 10;\n\t\t\tint numInitialTaskManagers = 5;\n\t\t\tfinal YarnResourceManagerCallbackHandler callbackHandler = new YarnResourceManagerCallbackHandler();\n\t\t\tAMRMClientAsync<AMRMClient.ContainerRequest> resourceManagerClient = mock(AMRMClientAsync.class);\n\t\t\tNMClient nodeManagerClient = mock(NMClient.class);\n\t\t\tUUID leaderSessionID = UUID.randomUUID();\n\n\t\t\tfinal List<Container> containerList = new ArrayList<>();\n\n\t\t\tfor (int i = 0; i < numInitialTaskManagers; i++) {\n\t\t\t\tcontainerList.add(new TestingContainer(\"container_\" + i, \"localhost\"));\n\t\t\t}\n\n\t\t\tdoAnswer(new Answer() {\n\t\t\t\tint counter = 0;\n\t\t\t\t@Override\n\t\t\t\tpublic Object answer(InvocationOnMock invocation) throws Throwable {\n\t\t\t\t\tif (counter < containerList.size()) {\n\t\t\t\t\t\tcallbackHandler.onContainersAllocated(\n\t\t\t\t\t\t\tCollections.singletonList(\n\t\t\t\t\t\t\t\tcontainerList.get(counter++)\n\t\t\t\t\t\t\t));\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}).when(resourceManagerClient).addContainerRequest(Matchers.any(AMRMClient.ContainerRequest.class));\n\n\t\t\tActorRef resourceManager = null;\n\t\t\tActorRef leader1;\n\n\t\t\ttry {\n\t\t\t\tleader1 = system.actorOf(\n\t\t\t\t\tProps.create(\n\t\t\t\t\t\tTestingUtils.ForwardingActor.class,\n\t\t\t\t\t\tgetRef(),\n\t\t\t\t\t\tOption.apply(leaderSessionID)\n\t\t\t\t\t));\n\n\t\t\t\tresourceManager = system.actorOf(\n\t\t\t\t\tProps.create(\n\t\t\t\t\t\tTestingYarnFlinkResourceManager.class,\n\t\t\t\t\t\tflinkConfig,\n\t\t\t\t\t\tyarnConfig,\n\t\t\t\t\t\tleaderRetrievalService,\n\t\t\t\t\t\tapplicationMasterHostName,\n\t\t\t\t\t\twebInterfaceURL,\n\t\t\t\t\t\ttaskManagerParameters,\n\t\t\t\t\t\ttaskManagerLaunchContext,\n\t\t\t\t\t\tyarnHeartbeatIntervalMillis,\n\t\t\t\t\t\tmaxFailedContainers,\n\t\t\t\t\t\tnumInitialTaskManagers,\n\t\t\t\t\t\tcallbackHandler,\n\t\t\t\t\t\tresourceManagerClient,\n\t\t\t\t\t\tnodeManagerClient\n\t\t\t\t\t));\n\n\t\t\t\tleaderRetrievalService.notifyListener(leader1.path().toString(), leaderSessionID);\n\n\t\t\t\tfinal AkkaActorGateway leader1Gateway = new AkkaActorGateway(leader1, leaderSessionID);\n\t\t\t\tfinal AkkaActorGateway resourceManagerGateway = new AkkaActorGateway(resourceManager, leaderSessionID);\n\n\t\t\t\tdoAnswer(new Answer() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Object answer(InvocationOnMock invocation) throws Throwable {\n\t\t\t\t\t\tContainer container = (Container) invocation.getArguments()[0];\n\t\t\t\t\t\tresourceManagerGateway.tell(new NotifyResourceStarted(YarnFlinkResourceManager.extractResourceID(container)),\n\t\t\t\t\t\t\tleader1Gateway);\n\t\t\t\t\t\treturn null;\n\t\t\t\t\t}\n\t\t\t\t}).when(nodeManagerClient).startContainer(Matchers.any(Container.class), Matchers.any(ContainerLaunchContext.class));\n\n\t\t\t\texpectMsgClass(deadline.timeLeft(), RegisterResourceManager.class);\n\n\t\t\t\tresourceManagerGateway.tell(new RegisterResourceManagerSuccessful(leader1, Collections.EMPTY_LIST));\n\n\t\t\t\tfor (int i = 0; i < containerList.size(); i++) {\n\t\t\t\t\texpectMsgClass(deadline.timeLeft(), Acknowledge.class);\n\t\t\t\t}\n\n\t\t\t\tFuture<Object> taskManagerRegisteredFuture = resourceManagerGateway.ask(new NotifyWhenResourcesRegistered(numInitialTaskManagers), deadline.timeLeft());\n\n\t\t\t\tAwait.ready(taskManagerRegisteredFuture, deadline.timeLeft());\n\n\t\t\t\tleaderRetrievalService.notifyListener(null, null);\n\n\t\t\t\tleaderRetrievalService.notifyListener(leader1.path().toString(), leaderSessionID);\n\n\t\t\t\texpectMsgClass(deadline.timeLeft(), RegisterResourceManager.class);\n\n\t\t\t\tresourceManagerGateway.tell(new RegisterResourceManagerSuccessful(leader1, Collections.EMPTY_LIST));\n\n\t\t\t\tfor (Container container: containerList) {\n\t\t\t\t\tresourceManagerGateway.tell(\n\t\t\t\t\t\tnew NotifyResourceStarted(YarnFlinkResourceManager.extractResourceID(container)),\n\t\t\t\t\t\tleader1Gateway);\n\t\t\t\t}\n\n\t\t\t\tfor (int i = 0; i < containerList.size(); i++) {\n\t\t\t\t\texpectMsgClass(deadline.timeLeft(), Acknowledge.class);\n\t\t\t\t}\n\n\t\t\t\tFuture<Object> numberOfRegisteredResourcesFuture = resourceManagerGateway.ask(RequestNumberOfRegisteredResources.Instance, deadline.timeLeft());\n\n\t\t\t\tint numberOfRegisteredResources = (Integer) Await.result(numberOfRegisteredResourcesFuture, deadline.timeLeft());\n\n\t\t\t\tassertEquals(numInitialTaskManagers, numberOfRegisteredResources);\n\t\t\t} finally {\n\t\t\t\tif (resourceManager != null) {\n\t\t\t\t\tresourceManager.tell(PoisonPill.getInstance(), ActorRef.noSender());\n\t\t\t\t}\n\t\t\t}\n\t\t}};\n\t}",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107 +\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 +\n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  ",
            "\t@Test\n\tpublic void testYarnFlinkResourceManagerJobManagerLostLeadership() throws Exception {\n\t\tnew JavaTestKit(system) {{\n\n\t\t\tfinal Deadline deadline = new FiniteDuration(3, TimeUnit.MINUTES).fromNow();\n\n\t\t\tConfiguration flinkConfig = new Configuration();\n\t\t\tYarnConfiguration yarnConfig = new YarnConfiguration();\n\t\t\tTestingLeaderRetrievalService leaderRetrievalService = new TestingLeaderRetrievalService(\n\t\t\t\tnull,\n\t\t\t\tnull);\n\t\t\tString applicationMasterHostName = \"localhost\";\n\t\t\tString webInterfaceURL = \"foobar\";\n\t\t\tContaineredTaskManagerParameters taskManagerParameters = new ContaineredTaskManagerParameters(\n\t\t\t\t1L, 1L, 1L, 1, new HashMap<String, String>());\n\t\t\tContainerLaunchContext taskManagerLaunchContext = mock(ContainerLaunchContext.class);\n\t\t\tint yarnHeartbeatIntervalMillis = 1000;\n\t\t\tint maxFailedContainers = 10;\n\t\t\tint numInitialTaskManagers = 5;\n\t\t\tfinal YarnResourceManagerCallbackHandler callbackHandler = new YarnResourceManagerCallbackHandler();\n\t\t\tAMRMClientAsync<AMRMClient.ContainerRequest> resourceManagerClient = mock(AMRMClientAsync.class);\n\t\t\tNMClient nodeManagerClient = mock(NMClient.class);\n\t\t\tUUID leaderSessionID = UUID.randomUUID();\n\n\t\t\tfinal List<Container> containerList = new ArrayList<>();\n\n\t\t\tfor (int i = 0; i < numInitialTaskManagers; i++) {\n\t\t\t\tcontainerList.add(new TestingContainer(\"container_\" + i, \"localhost\"));\n\t\t\t}\n\n\t\t\tdoAnswer(new Answer() {\n\t\t\t\tint counter = 0;\n\t\t\t\t@Override\n\t\t\t\tpublic Object answer(InvocationOnMock invocation) throws Throwable {\n\t\t\t\t\tif (counter < containerList.size()) {\n\t\t\t\t\t\tcallbackHandler.onContainersAllocated(\n\t\t\t\t\t\t\tCollections.singletonList(\n\t\t\t\t\t\t\t\tcontainerList.get(counter++)\n\t\t\t\t\t\t\t));\n\t\t\t\t\t}\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}).when(resourceManagerClient).addContainerRequest(Matchers.any(AMRMClient.ContainerRequest.class));\n\n\t\t\tActorRef resourceManager = null;\n\t\t\tActorRef leader1;\n\n\t\t\ttry {\n\t\t\t\tleader1 = system.actorOf(\n\t\t\t\t\tProps.create(\n\t\t\t\t\t\tTestingUtils.ForwardingActor.class,\n\t\t\t\t\t\tgetRef(),\n\t\t\t\t\t\tOption.apply(leaderSessionID)\n\t\t\t\t\t));\n\n\t\t\t\tresourceManager = system.actorOf(\n\t\t\t\t\tProps.create(\n\t\t\t\t\t\tTestingYarnFlinkResourceManager.class,\n\t\t\t\t\t\tflinkConfig,\n\t\t\t\t\t\tyarnConfig,\n\t\t\t\t\t\tleaderRetrievalService,\n\t\t\t\t\t\tapplicationMasterHostName,\n\t\t\t\t\t\twebInterfaceURL,\n\t\t\t\t\t\ttaskManagerParameters,\n\t\t\t\t\t\ttaskManagerLaunchContext,\n\t\t\t\t\t\tyarnHeartbeatIntervalMillis,\n\t\t\t\t\t\tmaxFailedContainers,\n\t\t\t\t\t\tnumInitialTaskManagers,\n\t\t\t\t\t\tcallbackHandler,\n\t\t\t\t\t\tresourceManagerClient,\n\t\t\t\t\t\tnodeManagerClient\n\t\t\t\t\t));\n\n\t\t\t\tleaderRetrievalService.notifyListener(leader1.path().toString(), leaderSessionID);\n\n\t\t\t\tfinal AkkaActorGateway leader1Gateway = new AkkaActorGateway(leader1, leaderSessionID);\n\t\t\t\tfinal AkkaActorGateway resourceManagerGateway = new AkkaActorGateway(resourceManager, leaderSessionID);\n\n\t\t\t\tdoAnswer(new Answer() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic Object answer(InvocationOnMock invocation) throws Throwable {\n\t\t\t\t\t\tContainer container = (Container) invocation.getArguments()[0];\n\t\t\t\t\t\tresourceManagerGateway.tell(new NotifyResourceStarted(YarnFlinkResourceManager.extractResourceID(container)),\n\t\t\t\t\t\t\tleader1Gateway);\n\t\t\t\t\t\treturn null;\n\t\t\t\t\t}\n\t\t\t\t}).when(nodeManagerClient).startContainer(Matchers.any(Container.class), Matchers.any(ContainerLaunchContext.class));\n\n\t\t\t\texpectMsgClass(deadline.timeLeft(), RegisterResourceManager.class);\n\n\t\t\t\tresourceManagerGateway.tell(new RegisterResourceManagerSuccessful(leader1, Collections.EMPTY_LIST));\n\n\t\t\t\tfor (int i = 0; i < containerList.size(); i++) {\n\t\t\t\t\texpectMsgClass(deadline.timeLeft(), Acknowledge.class);\n\t\t\t\t}\n\n\t\t\t\tFuture<Object> taskManagerRegisteredFuture = resourceManagerGateway.ask(new NotifyWhenResourcesRegistered(numInitialTaskManagers), deadline.timeLeft());\n\n\t\t\t\tAwait.ready(taskManagerRegisteredFuture, deadline.timeLeft());\n\n\t\t\t\tleaderRetrievalService.notifyListener(null, null);\n\n\t\t\t\tleaderRetrievalService.notifyListener(leader1.path().toString(), leaderSessionID);\n\n\t\t\t\texpectMsgClass(deadline.timeLeft(), RegisterResourceManager.class);\n\n\t\t\t\tresourceManagerGateway.tell(new RegisterResourceManagerSuccessful(leader1, Collections.EMPTY_LIST));\n\n\t\t\t\tfor (Container container: containerList) {\n\t\t\t\t\tresourceManagerGateway.tell(\n\t\t\t\t\t\tnew NotifyResourceStarted(YarnFlinkResourceManager.extractResourceID(container)),\n\t\t\t\t\t\tleader1Gateway);\n\t\t\t\t}\n\n\t\t\t\tfor (int i = 0; i < containerList.size(); i++) {\n\t\t\t\t\texpectMsgClass(deadline.timeLeft(), Acknowledge.class);\n\t\t\t\t}\n\n\t\t\t\tFuture<Object> numberOfRegisteredResourcesFuture = resourceManagerGateway.ask(RequestNumberOfRegisteredResources.INSTANCE, deadline.timeLeft());\n\n\t\t\t\tint numberOfRegisteredResources = (Integer) Await.result(numberOfRegisteredResourcesFuture, deadline.timeLeft());\n\n\t\t\t\tassertEquals(numInitialTaskManagers, numberOfRegisteredResources);\n\t\t\t} finally {\n\t\t\t\tif (resourceManager != null) {\n\t\t\t\t\tresourceManager.tell(PoisonPill.getInstance(), ActorRef.noSender());\n\t\t\t\t}\n\t\t\t}\n\t\t}};\n\t}"
        ],
        [
            "FlinkYarnCLI::createDescriptor(String,CommandLine)",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103 -\n 104 -\n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 -\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 -\n 142 -\n 143  \n 144  \n 145  \n 146 -\n 147 -\n 148  \n 149  \n 150  \n 151  \n 152 -\n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159 -\n 160  \n 161  \n 162  \n 163  \n 164  \n 165 -\n 166  \n 167  \n 168  \n 169 -\n 170 -\n 171  \n 172  \n 173  \n 174  \n 175  ",
            "\tpublic YarnClusterDescriptorV2 createDescriptor(String defaultApplicationName, CommandLine cmd) {\n\n\t\tYarnClusterDescriptorV2 yarnClusterDescriptor = new YarnClusterDescriptorV2();\n\n\t\t// Jar Path\n\t\tPath localJarPath;\n\t\tif (cmd.hasOption(FLINK_JAR.getOpt())) {\n\t\t\tString userPath = cmd.getOptionValue(FLINK_JAR.getOpt());\n\t\t\tif (!userPath.startsWith(\"file://\")) {\n\t\t\t\tuserPath = \"file://\" + userPath;\n\t\t\t}\n\t\t\tlocalJarPath = new Path(userPath);\n\t\t} else {\n\t\t\tLOG.info(\"No path for the flink jar passed. Using the location of \"\n\t\t\t\t+ yarnClusterDescriptor.getClass() + \" to locate the jar\");\n\t\t\tString encodedJarPath =\n\t\t\t\tyarnClusterDescriptor.getClass().getProtectionDomain().getCodeSource().getLocation().getPath();\n\t\t\ttry {\n\t\t\t\t// we have to decode the url encoded parts of the path\n\t\t\t\tString decodedPath = URLDecoder.decode(encodedJarPath, Charset.defaultCharset().name());\n\t\t\t\tlocalJarPath = new Path(new File(decodedPath).toURI());\n\t\t\t} catch (UnsupportedEncodingException e) {\n\t\t\t\tthrow new RuntimeException(\"Couldn't decode the encoded Flink dist jar path: \" + encodedJarPath +\n\t\t\t\t\t\" Please supply a path manually via the -\" + FLINK_JAR.getOpt() + \" option.\");\n\t\t\t}\n\t\t}\n\n\t\tyarnClusterDescriptor.setLocalJarPath(localJarPath);\n\n\t\tList<File> shipFiles = new ArrayList<>();\n\t\t// path to directory to ship\n\t\tif (cmd.hasOption(SHIP_PATH.getOpt())) {\n\t\t\tString shipPath = cmd.getOptionValue(SHIP_PATH.getOpt());\n\t\t\tFile shipDir = new File(shipPath);\n\t\t\tif (shipDir.isDirectory()) {\n\t\t\t\tshipFiles.add(shipDir);\n\t\t\t} else {\n\t\t\t\tLOG.warn(\"Ship directory is not a directory. Ignoring it.\");\n\t\t\t}\n\t\t}\n\n\t\tyarnClusterDescriptor.addShipFiles(shipFiles);\n\n\t\t// queue\n\t\tif (cmd.hasOption(QUEUE.getOpt())) {\n\t\t\tyarnClusterDescriptor.setQueue(cmd.getOptionValue(QUEUE.getOpt()));\n\t\t}\n\n\t\t// JobManager Memory\n\t\tif (cmd.hasOption(JM_MEMORY.getOpt())) {\n\t\t\tint jmMemory = Integer.valueOf(cmd.getOptionValue(JM_MEMORY.getOpt()));\n\t\t\tyarnClusterDescriptor.setJobManagerMemory(jmMemory);\n\t\t}\n\n\t\tString[] dynamicProperties = null;\n\t\tif (cmd.hasOption(DYNAMIC_PROPERTIES.getOpt())) {\n\t\t\tdynamicProperties = cmd.getOptionValues(DYNAMIC_PROPERTIES.getOpt());\n\t\t}\n\t\tString dynamicPropertiesEncoded = StringUtils.join(dynamicProperties, YARN_DYNAMIC_PROPERTIES_SEPARATOR);\n\n\t\tyarnClusterDescriptor.setDynamicPropertiesEncoded(dynamicPropertiesEncoded);\n\n\t\tif (cmd.hasOption(DETACHED.getOpt()) || cmd.hasOption(CliFrontendParser.DETACHED_OPTION.getOpt())) {\n\t\t\t// TODO: not support non detach mode now.\n\t\t\t//this.detachedMode = false;\n\t\t}\n\t\tyarnClusterDescriptor.setDetachedMode(this.detachedMode);\n\n\t\tif(defaultApplicationName != null) {\n\t\t\tyarnClusterDescriptor.setName(defaultApplicationName);\n\t\t}\n\n\t\tif (cmd.hasOption(ZOOKEEPER_NAMESPACE.getOpt())) {\n\t\t\tString zookeeperNamespace = cmd.getOptionValue(ZOOKEEPER_NAMESPACE.getOpt());\n\t\t\tyarnClusterDescriptor.setZookeeperNamespace(zookeeperNamespace);\n\t\t}\n\n\t\treturn yarnClusterDescriptor;\n\t}",
            "  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105 +\n 106 +\n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 +\n 131 +\n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143 +\n 144 +\n 145  \n 146  \n 147  \n 148 +\n 149 +\n 150  \n 151  \n 152  \n 153  \n 154 +\n 155 +\n 156  \n 157  \n 158  \n 159  \n 160  \n 161 +\n 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171 +\n 172 +\n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\tpublic YarnClusterDescriptorV2 createDescriptor(String defaultApplicationName, CommandLine cmd) {\n\n\t\tYarnClusterDescriptorV2 yarnClusterDescriptor = new YarnClusterDescriptorV2();\n\n\t\t// Jar Path\n\t\tPath localJarPath;\n\t\tif (cmd.hasOption(flinkJar.getOpt())) {\n\t\t\tString userPath = cmd.getOptionValue(flinkJar.getOpt());\n\t\t\tif (!userPath.startsWith(\"file://\")) {\n\t\t\t\tuserPath = \"file://\" + userPath;\n\t\t\t}\n\t\t\tlocalJarPath = new Path(userPath);\n\t\t} else {\n\t\t\tLOG.info(\"No path for the flink jar passed. Using the location of \"\n\t\t\t\t+ yarnClusterDescriptor.getClass() + \" to locate the jar\");\n\t\t\tString encodedJarPath =\n\t\t\t\tyarnClusterDescriptor.getClass().getProtectionDomain().getCodeSource().getLocation().getPath();\n\t\t\ttry {\n\t\t\t\t// we have to decode the url encoded parts of the path\n\t\t\t\tString decodedPath = URLDecoder.decode(encodedJarPath, Charset.defaultCharset().name());\n\t\t\t\tlocalJarPath = new Path(new File(decodedPath).toURI());\n\t\t\t} catch (UnsupportedEncodingException e) {\n\t\t\t\tthrow new RuntimeException(\"Couldn't decode the encoded Flink dist jar path: \" + encodedJarPath +\n\t\t\t\t\t\" Please supply a path manually via the -\" + flinkJar.getOpt() + \" option.\");\n\t\t\t}\n\t\t}\n\n\t\tyarnClusterDescriptor.setLocalJarPath(localJarPath);\n\n\t\tList<File> shipFiles = new ArrayList<>();\n\t\t// path to directory to ship\n\t\tif (cmd.hasOption(shipPath.getOpt())) {\n\t\t\tString shipPath = cmd.getOptionValue(this.shipPath.getOpt());\n\t\t\tFile shipDir = new File(shipPath);\n\t\t\tif (shipDir.isDirectory()) {\n\t\t\t\tshipFiles.add(shipDir);\n\t\t\t} else {\n\t\t\t\tLOG.warn(\"Ship directory is not a directory. Ignoring it.\");\n\t\t\t}\n\t\t}\n\n\t\tyarnClusterDescriptor.addShipFiles(shipFiles);\n\n\t\t// queue\n\t\tif (cmd.hasOption(queue.getOpt())) {\n\t\t\tyarnClusterDescriptor.setQueue(cmd.getOptionValue(queue.getOpt()));\n\t\t}\n\n\t\t// JobManager Memory\n\t\tif (cmd.hasOption(jmMemory.getOpt())) {\n\t\t\tint jmMemory = Integer.valueOf(cmd.getOptionValue(this.jmMemory.getOpt()));\n\t\t\tyarnClusterDescriptor.setJobManagerMemory(jmMemory);\n\t\t}\n\n\t\tString[] dynamicProperties = null;\n\t\tif (cmd.hasOption(this.dynamicProperties.getOpt())) {\n\t\t\tdynamicProperties = cmd.getOptionValues(this.dynamicProperties.getOpt());\n\t\t}\n\t\tString dynamicPropertiesEncoded = StringUtils.join(dynamicProperties, YARN_DYNAMIC_PROPERTIES_SEPARATOR);\n\n\t\tyarnClusterDescriptor.setDynamicPropertiesEncoded(dynamicPropertiesEncoded);\n\n\t\tif (cmd.hasOption(detached.getOpt()) || cmd.hasOption(CliFrontendParser.DETACHED_OPTION.getOpt())) {\n\t\t\t// TODO: not support non detach mode now.\n\t\t\t//this.detachedMode = false;\n\t\t}\n\t\tyarnClusterDescriptor.setDetachedMode(this.detachedMode);\n\n\t\tif (defaultApplicationName != null) {\n\t\t\tyarnClusterDescriptor.setName(defaultApplicationName);\n\t\t}\n\n\t\tif (cmd.hasOption(zookeeperNamespace.getOpt())) {\n\t\t\tString zookeeperNamespace = cmd.getOptionValue(this.zookeeperNamespace.getOpt());\n\t\t\tyarnClusterDescriptor.setZookeeperNamespace(zookeeperNamespace);\n\t\t}\n\n\t\treturn yarnClusterDescriptor;\n\t}"
        ],
        [
            "YarnClusterDescriptorTest::testSetupApplicationMasterContainer()",
            " 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143 -\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181 -\n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 -\n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 -\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 -\n 221  \n 222 -\n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 -\n 241  \n 242 -\n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  ",
            "\t@Test\n\tpublic void testSetupApplicationMasterContainer() {\n\t\tYarnClusterDescriptor clusterDescriptor = new YarnClusterDescriptor();\n\t\tfinal Configuration cfg = new Configuration();\n\t\tclusterDescriptor.setFlinkConfiguration(cfg);\n\n\t\tfinal String java = \"$JAVA_HOME/bin/java\";\n\t\tfinal String jvmmem = \"-Xmx424m\";\n\t\tfinal String jvmOpts = \"-Djvm\"; // if set\n\t\tfinal String jmJvmOpts = \"-DjmJvm\"; // if set\n\t\tfinal String krb5 = \"-Djava.security.krb5.conf=krb5.conf\";\n\t\tfinal String logfile =\n\t\t\t\"-Dlog.file=\\\"\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR +\n\t\t\t\t\"/jobmanager.log\\\"\"; // if set\n\t\tfinal String logback =\n\t\t\t\"-Dlogback.configurationFile=file:\" + FlinkYarnSessionCli.CONFIG_FILE_LOGBACK_NAME; // if set\n\t\tfinal String log4j =\n\t\t\t\"-Dlog4j.configuration=file:\" + FlinkYarnSessionCli.CONFIG_FILE_LOG4J_NAME; // if set\n\t\tfinal String mainClass = clusterDescriptor.getApplicationMasterClass().getName();\n\t\tfinal String args = \"\";\n\t\tfinal String redirects =\n\t\t\t\"1> \" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.out \" +\n\t\t\t\"2> \" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.err\";\n\n\t\t// no logging, with/out krb5\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + // jvmOpts\n\t\t\t\t\" \" + // logging\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(false, false, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + \" \" + krb5 +// jvmOpts\n\t\t\t\t\" \" + // logging\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(false, false, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// logback only, with/out krb5\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, false, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + \" \" + krb5 +// jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, false, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// log4j, with/out krb5\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(false, true, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + \" \" + krb5 +// jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(false, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// logback + log4j, with/out krb5\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + \" \" + krb5 +// jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// logback + log4j, with/out krb5, different JVM opts\n\t\tcfg.setString(CoreOptions.FLINK_JVM_OPTIONS, jvmOpts);\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + jvmOpts +\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \"  + args + \" \"+ redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + jvmOpts + \" \" + krb5 +// jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \"  + args + \" \"+ redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// logback + log4j, with/out krb5, different JVM opts\n\t\tcfg.setString(CoreOptions.FLINK_JM_JVM_OPTIONS, jmJvmOpts);\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + jvmOpts + \" \" + jmJvmOpts +\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \"  + args + \" \"+ redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + jvmOpts + \" \" + jmJvmOpts + \" \" + krb5 +// jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \"  + args + \" \"+ redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// now try some configurations with different yarn.container-start-command-template\n\n\t\tcfg.setString(ConfigConstants.YARN_CONTAINER_START_COMMAND_TEMPLATE,\n\t\t\t\"%java% 1 %jvmmem% 2 %jvmopts% 3 %logging% 4 %class% 5 %args% 6 %redirects%\");\n\t\tassertEquals(\n\t\t\tjava + \" 1 \" + jvmmem +\n\t\t\t\t\" 2 \" + jvmOpts + \" \" + jmJvmOpts + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" 3 \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" 4 \" + mainClass + \" 5 \" + args + \" 6 \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\tcfg.setString(ConfigConstants.YARN_CONTAINER_START_COMMAND_TEMPLATE,\n\t\t\t\"%java% %logging% %jvmopts% %jvmmem% %class% %args% %redirects%\");\n\t\tassertEquals(\n\t\t\tjava +\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + jvmOpts + \" \" + jmJvmOpts + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" \" + jvmmem +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\t}",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 +\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186 +\n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 +\n 226  \n 227 +\n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238 +\n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245 +\n 246  \n 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  ",
            "\t@Test\n\tpublic void testSetupApplicationMasterContainer() {\n\t\tYarnClusterDescriptor clusterDescriptor = new YarnClusterDescriptor();\n\t\tfinal Configuration cfg = new Configuration();\n\t\tclusterDescriptor.setFlinkConfiguration(cfg);\n\n\t\tfinal String java = \"$JAVA_HOME/bin/java\";\n\t\tfinal String jvmmem = \"-Xmx424m\";\n\t\tfinal String jvmOpts = \"-Djvm\"; // if set\n\t\tfinal String jmJvmOpts = \"-DjmJvm\"; // if set\n\t\tfinal String krb5 = \"-Djava.security.krb5.conf=krb5.conf\";\n\t\tfinal String logfile =\n\t\t\t\"-Dlog.file=\\\"\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR +\n\t\t\t\t\"/jobmanager.log\\\"\"; // if set\n\t\tfinal String logback =\n\t\t\t\"-Dlogback.configurationFile=file:\" + FlinkYarnSessionCli.CONFIG_FILE_LOGBACK_NAME; // if set\n\t\tfinal String log4j =\n\t\t\t\"-Dlog4j.configuration=file:\" + FlinkYarnSessionCli.CONFIG_FILE_LOG4J_NAME; // if set\n\t\tfinal String mainClass = clusterDescriptor.getApplicationMasterClass().getName();\n\t\tfinal String args = \"\";\n\t\tfinal String redirects =\n\t\t\t\"1> \" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.out \" +\n\t\t\t\"2> \" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/jobmanager.err\";\n\n\t\t// no logging, with/out krb5\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + // jvmOpts\n\t\t\t\t\" \" + // logging\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(false, false, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" \" + // logging\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(false, false, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// logback only, with/out krb5\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, false, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, false, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// log4j, with/out krb5\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(false, true, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(false, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// logback + log4j, with/out krb5\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// logback + log4j, with/out krb5, different JVM opts\n\t\tcfg.setString(CoreOptions.FLINK_JVM_OPTIONS, jvmOpts);\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + jvmOpts +\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \"  + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + jvmOpts + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \"  + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// logback + log4j, with/out krb5, different JVM opts\n\t\tcfg.setString(CoreOptions.FLINK_JM_JVM_OPTIONS, jmJvmOpts);\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + jvmOpts + \" \" + jmJvmOpts +\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \"  + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, false)\n\t\t\t\t.getCommands().get(0));\n\n\t\tassertEquals(\n\t\t\tjava + \" \" + jvmmem +\n\t\t\t\t\" \" + jvmOpts + \" \" + jmJvmOpts + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + mainClass + \" \"  + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\t// now try some configurations with different yarn.container-start-command-template\n\n\t\tcfg.setString(ConfigConstants.YARN_CONTAINER_START_COMMAND_TEMPLATE,\n\t\t\t\"%java% 1 %jvmmem% 2 %jvmopts% 3 %logging% 4 %class% 5 %args% 6 %redirects%\");\n\t\tassertEquals(\n\t\t\tjava + \" 1 \" + jvmmem +\n\t\t\t\t\" 2 \" + jvmOpts + \" \" + jmJvmOpts + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" 3 \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" 4 \" + mainClass + \" 5 \" + args + \" 6 \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\n\t\tcfg.setString(ConfigConstants.YARN_CONTAINER_START_COMMAND_TEMPLATE,\n\t\t\t\"%java% %logging% %jvmopts% %jvmmem% %class% %args% %redirects%\");\n\t\tassertEquals(\n\t\t\tjava +\n\t\t\t\t\" \" + logfile + \" \" + logback + \" \" + log4j +\n\t\t\t\t\" \" + jvmOpts + \" \" + jmJvmOpts + \" \" + krb5 + // jvmOpts\n\t\t\t\t\" \" + jvmmem +\n\t\t\t\t\" \" + mainClass + \" \" + args + \" \" + redirects,\n\t\t\tclusterDescriptor\n\t\t\t\t.setupApplicationMasterContainer(true, true, true)\n\t\t\t\t.getCommands().get(0));\n\t}"
        ],
        [
            "YarnTaskExecutorRunner::run(String)",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 -\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  ",
            "\t/**\n\t * The instance entry point for the YARN task executor. Obtains user group\n\t * information and calls the main work method {@link #runTaskExecutor(org.apache.flink.configuration.Configuration)} as a\n\t * privileged action.\n\t *\n\t * @param args The command line arguments.\n\t * @return The process exit code.\n\t */\n\tprotected int run(String[] args) {\n\t\ttry {\n\t\t\tLOG.debug(\"All environment variables: {}\", ENV);\n\n\t\t\tfinal String yarnClientUsername = ENV.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\t\tfinal String localDirs = ENV.get(Environment.LOCAL_DIRS.key());\n\t\t\tLOG.info(\"Current working/local Directory: {}\", localDirs);\n\n\t\t\tfinal String currDir = ENV.get(Environment.PWD.key());\n\t\t\tLOG.info(\"Current working Directory: {}\", currDir);\n\n\t\t\tfinal String remoteKeytabPath = ENV.get(YarnConfigKeys.KEYTAB_PATH);\n\t\t\tLOG.info(\"TM: remote keytab path obtained {}\", remoteKeytabPath);\n\n\t\t\tfinal String remoteKeytabPrincipal = ENV.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\t\tLOG.info(\"TM: remote keytab principal obtained {}\", remoteKeytabPrincipal);\n\n\t\t\tfinal Configuration configuration = GlobalConfiguration.loadConfiguration(currDir);\n\t\t\tFileSystem.setDefaultScheme(configuration);\n\n\t\t\t// configure local directory\n\t\t\tString flinkTempDirs = configuration.getString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, null);\n\t\t\tif (flinkTempDirs == null) {\n\t\t\t\tLOG.info(\"Setting directories for temporary file \" + localDirs);\n\t\t\t\tconfiguration.setString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, localDirs);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tLOG.info(\"Overriding YARN's temporary file directories with those \" +\n\t\t\t\t\t\t\"specified in the Flink config: \" + flinkTempDirs);\n\t\t\t}\n\n\t\t\t// tell akka to die in case of an error\n\t\t\tconfiguration.setBoolean(AkkaOptions.JVM_EXIT_ON_FATAL_ERROR, true);\n\n\t\t\tString keytabPath = null;\n\t\t\tif(remoteKeytabPath != null) {\n\t\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\t\tkeytabPath = f.getAbsolutePath();\n\t\t\t\tLOG.info(\"keytab path: {}\", keytabPath);\n\t\t\t}\n\n\t\t\tUserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n\n\t\t\tLOG.info(\"YARN daemon is running as: {} Yarn client user obtainer: {}\",\n\t\t\t\t\tcurrentUser.getShortUserName(), yarnClientUsername);\n\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfiguration = null;\n\n\t\t\t//To support Yarn Secure Integration Test Scenario\n\t\t\tFile krb5Conf = new File(currDir, Utils.KRB5_FILE_NAME);\n\t\t\tif (krb5Conf.exists() && krb5Conf.canRead()) {\n\t\t\t\tString krb5Path = krb5Conf.getAbsolutePath();\n\t\t\t\tLOG.info(\"KRB5 Conf: {}\", krb5Path);\n\t\t\t\thadoopConfiguration = new org.apache.hadoop.conf.Configuration();\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\t\t\t}\n\n\t\t\tSecurityUtils.SecurityConfiguration sc;\n\t\t\tif (hadoopConfiguration != null) {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(configuration, hadoopConfiguration);\n\t\t\t} else {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(configuration);\n\t\t\t}\n\n\t\t\tif (keytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tconfiguration.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);\n\t\t\t\tconfiguration.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\tSecurityUtils.install(sc);\n\n\t\t\treturn SecurityUtils.getInstalledContext().runSecured(new Callable<Integer>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() throws Exception {\n\t\t\t\t\treturn runTaskExecutor(configuration);\n\t\t\t\t}\n\t\t\t});\n\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\t}",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134 +\n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  ",
            "\t/**\n\t * The instance entry point for the YARN task executor. Obtains user group\n\t * information and calls the main work method {@link #runTaskExecutor(org.apache.flink.configuration.Configuration)} as a\n\t * privileged action.\n\t *\n\t * @param args The command line arguments.\n\t * @return The process exit code.\n\t */\n\tprotected int run(String[] args) {\n\t\ttry {\n\t\t\tLOG.debug(\"All environment variables: {}\", ENV);\n\n\t\t\tfinal String yarnClientUsername = ENV.get(YarnConfigKeys.ENV_HADOOP_USER_NAME);\n\t\t\tfinal String localDirs = ENV.get(Environment.LOCAL_DIRS.key());\n\t\t\tLOG.info(\"Current working/local Directory: {}\", localDirs);\n\n\t\t\tfinal String currDir = ENV.get(Environment.PWD.key());\n\t\t\tLOG.info(\"Current working Directory: {}\", currDir);\n\n\t\t\tfinal String remoteKeytabPath = ENV.get(YarnConfigKeys.KEYTAB_PATH);\n\t\t\tLOG.info(\"TM: remote keytab path obtained {}\", remoteKeytabPath);\n\n\t\t\tfinal String remoteKeytabPrincipal = ENV.get(YarnConfigKeys.KEYTAB_PRINCIPAL);\n\t\t\tLOG.info(\"TM: remote keytab principal obtained {}\", remoteKeytabPrincipal);\n\n\t\t\tfinal Configuration configuration = GlobalConfiguration.loadConfiguration(currDir);\n\t\t\tFileSystem.setDefaultScheme(configuration);\n\n\t\t\t// configure local directory\n\t\t\tString flinkTempDirs = configuration.getString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, null);\n\t\t\tif (flinkTempDirs == null) {\n\t\t\t\tLOG.info(\"Setting directories for temporary file \" + localDirs);\n\t\t\t\tconfiguration.setString(ConfigConstants.TASK_MANAGER_TMP_DIR_KEY, localDirs);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tLOG.info(\"Overriding YARN's temporary file directories with those \" +\n\t\t\t\t\t\t\"specified in the Flink config: \" + flinkTempDirs);\n\t\t\t}\n\n\t\t\t// tell akka to die in case of an error\n\t\t\tconfiguration.setBoolean(AkkaOptions.JVM_EXIT_ON_FATAL_ERROR, true);\n\n\t\t\tString keytabPath = null;\n\t\t\tif (remoteKeytabPath != null) {\n\t\t\t\tFile f = new File(currDir, Utils.KEYTAB_FILE_NAME);\n\t\t\t\tkeytabPath = f.getAbsolutePath();\n\t\t\t\tLOG.info(\"keytab path: {}\", keytabPath);\n\t\t\t}\n\n\t\t\tUserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n\n\t\t\tLOG.info(\"YARN daemon is running as: {} Yarn client user obtainer: {}\",\n\t\t\t\t\tcurrentUser.getShortUserName(), yarnClientUsername);\n\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfiguration = null;\n\n\t\t\t//To support Yarn Secure Integration Test Scenario\n\t\t\tFile krb5Conf = new File(currDir, Utils.KRB5_FILE_NAME);\n\t\t\tif (krb5Conf.exists() && krb5Conf.canRead()) {\n\t\t\t\tString krb5Path = krb5Conf.getAbsolutePath();\n\t\t\t\tLOG.info(\"KRB5 Conf: {}\", krb5Path);\n\t\t\t\thadoopConfiguration = new org.apache.hadoop.conf.Configuration();\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION, \"kerberos\");\n\t\t\t\thadoopConfiguration.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION, \"true\");\n\t\t\t}\n\n\t\t\tSecurityUtils.SecurityConfiguration sc;\n\t\t\tif (hadoopConfiguration != null) {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(configuration, hadoopConfiguration);\n\t\t\t} else {\n\t\t\t\tsc = new SecurityUtils.SecurityConfiguration(configuration);\n\t\t\t}\n\n\t\t\tif (keytabPath != null && remoteKeytabPrincipal != null) {\n\t\t\t\tconfiguration.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB, keytabPath);\n\t\t\t\tconfiguration.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL, remoteKeytabPrincipal);\n\t\t\t}\n\n\t\t\tSecurityUtils.install(sc);\n\n\t\t\treturn SecurityUtils.getInstalledContext().runSecured(new Callable<Integer>() {\n\t\t\t\t@Override\n\t\t\t\tpublic Integer call() throws Exception {\n\t\t\t\t\treturn runTaskExecutor(configuration);\n\t\t\t\t}\n\t\t\t});\n\n\t\t}\n\t\tcatch (Throwable t) {\n\t\t\t// make sure that everything whatever ends up in the log\n\t\t\tLOG.error(\"YARN Application Master initialization failed\", t);\n\t\t\treturn INIT_ERROR_EXIT_CODE;\n\t\t}\n\t}"
        ],
        [
            "YarnResourceManager::startNewWorker(ResourceProfile)",
            " 215  \n 216  \n 217  \n 218  \n 219  \n 220 -\n 221 -\n 222  \n 223  \n 224  ",
            "\t@Override\n\tpublic void startNewWorker(ResourceProfile resourceProfile) {\n\t\t// Priority for worker containers - priorities are intra-application\n\t\t//TODO: set priority according to the resource allocated\n\t\tPriority priority = Priority.newInstance(generatePriority(resourceProfile));\n\t\tint mem = resourceProfile.getMemoryInMB() < 0 ? DEFAULT_TSK_EXECUTOR_MEMORY_SIZE : (int)resourceProfile.getMemoryInMB();\n\t\tint vcore = resourceProfile.getCpuCores() < 1 ? 1 : (int)resourceProfile.getCpuCores();\n\t\tResource capability = Resource.newInstance(mem, vcore);\n\t\trequestYarnContainer(capability, priority);\n\t}",
            " 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220 +\n 221  \n 222  \n 223  ",
            "\t@Override\n\tpublic void startNewWorker(ResourceProfile resourceProfile) {\n\t\t// Priority for worker containers - priorities are intra-application\n\t\t//TODO: set priority according to the resource allocated\n\t\tPriority priority = Priority.newInstance(generatePriority(resourceProfile));\n\t\tint mem = resourceProfile.getMemoryInMB() < 0 ? DEFAULT_TSK_EXECUTOR_MEMORY_SIZE : (int) resourceProfile.getMemoryInMB();\n\t\tint vcore = resourceProfile.getCpuCores() < 1 ? 1 : (int) resourceProfile.getCpuCores();\n\t\tResource capability = Resource.newInstance(mem, vcore);\n\t\trequestYarnContainer(capability, priority);\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::setTaskManagerSlots(int)",
            " 211  \n 212 -\n 213  \n 214  \n 215  \n 216  ",
            "\tpublic void setTaskManagerSlots(int slots) {\n\t\tif(slots <= 0) {\n\t\t\tthrow new IllegalArgumentException(\"Number of TaskManager slots must be positive\");\n\t\t}\n\t\tthis.slots = slots;\n\t}",
            " 219  \n 220 +\n 221  \n 222  \n 223  \n 224  ",
            "\tpublic void setTaskManagerSlots(int slots) {\n\t\tif (slots <= 0) {\n\t\t\tthrow new IllegalArgumentException(\"Number of TaskManager slots must be positive\");\n\t\t}\n\t\tthis.slots = slots;\n\t}"
        ],
        [
            "YarnIntraNonHaMasterServicesTest::createHDFS()",
            "  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 -\n  84 -\n  85  ",
            "\t@BeforeClass\n\tpublic static void createHDFS() throws Exception {\n\t\tAssume.assumeTrue(!OperatingSystem.isWindows());\n\n\t\tfinal File tempDir = TEMP_DIR.newFolder();\n\n\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, tempDir.getAbsolutePath());\n\n\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\tHDFS_CLUSTER = builder.build();\n\t\tHDFS_ROOT_PATH = new Path(HDFS_CLUSTER.getURI());\n\t}",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 +\n  85 +\n  86  ",
            "\t@BeforeClass\n\tpublic static void createHDFS() throws Exception {\n\t\tAssume.assumeTrue(!OperatingSystem.isWindows());\n\n\t\tfinal File tempDir = TEMP_DIR.newFolder();\n\n\t\torg.apache.hadoop.conf.Configuration hdConf = new org.apache.hadoop.conf.Configuration();\n\t\thdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, tempDir.getAbsolutePath());\n\n\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(hdConf);\n\t\thdfsCluster = builder.build();\n\t\thdfsRootPath = new Path(hdfsCluster.getURI());\n\t}"
        ],
        [
            "YarnResourceManager::onContainersAllocated(List)",
            " 253  \n 254  \n 255  \n 256  \n 257 -\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267 -\n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "\t@Override\n\tpublic void onContainersAllocated(List<Container> containers) {\n\t\tfor (Container container : containers) {\n\t\t\tnumPendingContainerRequests = Math.max(0, numPendingContainerRequests - 1);\n\t\t\tLOG.info(\"Received new container: {} - Remaining pending container requests: {}\",\n\t\t\t\t\tcontainer.getId(), numPendingContainerRequests);\n\t\t\ttry {\n\t\t\t\t/** Context information used to start a TaskExecutor Java process */\n\t\t\t\tContainerLaunchContext taskExecutorLaunchContext =\n\t\t\t\t\t\tcreateTaskExecutorLaunchContext(container.getResource(), container.getId().toString(), container.getNodeId().getHost());\n\t\t\t\tnodeManagerClient.startContainer(container, taskExecutorLaunchContext);\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// failed to launch the container, will release the failed one and ask for a new one\n\t\t\t\tLOG.error(\"Could not start TaskManager in container {},\", container, t);\n\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\trequestYarnContainer(container.getResource(), container.getPriority());\n\t\t\t}\n\t\t}\n\t\tif (numPendingContainerRequests <= 0) {\n\t\t\tresourceManagerClient.setHeartbeatInterval(yarnHeartbeatIntervalMillis);\n\t\t}\n\t}",
            " 252  \n 253  \n 254  \n 255  \n 256 +\n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266 +\n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "\t@Override\n\tpublic void onContainersAllocated(List<Container> containers) {\n\t\tfor (Container container : containers) {\n\t\t\tnumPendingContainerRequests = Math.max(0, numPendingContainerRequests - 1);\n\t\t\tlog.info(\"Received new container: {} - Remaining pending container requests: {}\",\n\t\t\t\t\tcontainer.getId(), numPendingContainerRequests);\n\t\t\ttry {\n\t\t\t\t/** Context information used to start a TaskExecutor Java process */\n\t\t\t\tContainerLaunchContext taskExecutorLaunchContext =\n\t\t\t\t\t\tcreateTaskExecutorLaunchContext(container.getResource(), container.getId().toString(), container.getNodeId().getHost());\n\t\t\t\tnodeManagerClient.startContainer(container, taskExecutorLaunchContext);\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// failed to launch the container, will release the failed one and ask for a new one\n\t\t\t\tlog.error(\"Could not start TaskManager in container {},\", container, t);\n\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\trequestYarnContainer(container.getResource(), container.getPriority());\n\t\t\t}\n\t\t}\n\t\tif (numPendingContainerRequests <= 0) {\n\t\t\tresourceManagerClient.setHeartbeatInterval(yarnHeartbeatIntervalMillis);\n\t\t}\n\t}"
        ],
        [
            "YarnClusterClientV2::getWebInterfaceURL()",
            " 112  \n 113  \n 114  \n 115 -\n 116  \n 117  \n 118  \n 119  \n 120  ",
            "\t@Override\n\tpublic String getWebInterfaceURL() {\n\t\t// there seems to be a difference between HD 2.2.0 and 2.6.0\n\t\tif(!trackingURL.startsWith(\"http://\")) {\n\t\t\treturn \"http://\" + trackingURL;\n\t\t} else {\n\t\t\treturn trackingURL;\n\t\t}\n\t}",
            " 114  \n 115  \n 116  \n 117 +\n 118  \n 119  \n 120  \n 121  \n 122  ",
            "\t@Override\n\tpublic String getWebInterfaceURL() {\n\t\t// there seems to be a difference between HD 2.2.0 and 2.6.0\n\t\tif (!trackingURL.startsWith(\"http://\")) {\n\t\t\treturn \"http://\" + trackingURL;\n\t\t} else {\n\t\t\treturn trackingURL;\n\t\t}\n\t}"
        ],
        [
            "FlinkYarnSessionCli::addGeneralOptions(Options)",
            " 512  \n 513  \n 514 -\n 515  ",
            "\t@Override\n\tpublic void addGeneralOptions(Options baseOptions) {\n\t\tbaseOptions.addOption(APPLICATION_ID);\n\t}",
            " 512  \n 513  \n 514 +\n 515  ",
            "\t@Override\n\tpublic void addGeneralOptions(Options baseOptions) {\n\t\tbaseOptions.addOption(applicationId);\n\t}"
        ],
        [
            "Utils::setTokensFor(ContainerLaunchContext,List,Configuration)",
            " 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168 -\n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176 -\n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "\tpublic static void setTokensFor(ContainerLaunchContext amContainer, List<Path> paths, Configuration conf) throws IOException {\n\t\tCredentials credentials = new Credentials();\n\t\t// for HDFS\n\t\tTokenCache.obtainTokensForNamenodes(credentials, paths.toArray(new Path[0]), conf);\n\t\t// for HBase\n\t\tobtainTokenForHBase(credentials, conf);\n\t\t// for user\n\t\tUserGroupInformation currUsr = UserGroupInformation.getCurrentUser();\n\n\t\tCollection<Token<? extends TokenIdentifier>> usrTok = currUsr.getTokens();\n\t\tfor(Token<? extends TokenIdentifier> token : usrTok) {\n\t\t\tfinal Text id = new Text(token.getIdentifier());\n\t\t\tLOG.info(\"Adding user token \" + id + \" with \" + token);\n\t\t\tcredentials.addToken(id, token);\n\t\t}\n\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n\t\t\tcredentials.writeTokenStorageToStream(dob);\n\n\t\t\tif(LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Wrote tokens. Credentials buffer length: \" + dob.getLength());\n\t\t\t}\n\n\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\t\t\tamContainer.setTokens(securityTokens);\n\t\t}\n\t}",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172 +\n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  ",
            "\tpublic static void setTokensFor(ContainerLaunchContext amContainer, List<Path> paths, Configuration conf) throws IOException {\n\t\tCredentials credentials = new Credentials();\n\t\t// for HDFS\n\t\tTokenCache.obtainTokensForNamenodes(credentials, paths.toArray(new Path[0]), conf);\n\t\t// for HBase\n\t\tobtainTokenForHBase(credentials, conf);\n\t\t// for user\n\t\tUserGroupInformation currUsr = UserGroupInformation.getCurrentUser();\n\n\t\tCollection<Token<? extends TokenIdentifier>> usrTok = currUsr.getTokens();\n\t\tfor (Token<? extends TokenIdentifier> token : usrTok) {\n\t\t\tfinal Text id = new Text(token.getIdentifier());\n\t\t\tLOG.info(\"Adding user token \" + id + \" with \" + token);\n\t\t\tcredentials.addToken(id, token);\n\t\t}\n\t\ttry (DataOutputBuffer dob = new DataOutputBuffer()) {\n\t\t\tcredentials.writeTokenStorageToStream(dob);\n\n\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\tLOG.debug(\"Wrote tokens. Credentials buffer length: \" + dob.getLength());\n\t\t\t}\n\n\t\t\tByteBuffer securityTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\t\t\tamContainer.setTokens(securityTokens);\n\t\t}\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::addShipFiles(List)",
            " 252  \n 253  \n 254  \n 255  \n 256 -\n 257  \n 258  \n 259  \n 260  ",
            "\tpublic void addShipFiles(List<File> shipFiles) {\n\t\tfor (File shipFile: shipFiles) {\n\t\t\t// remove uberjar from ship list (by default everything in the lib/ folder is added to\n\t\t\t// the list of files to ship, but we handle the uberjar separately.\n\t\t\tif(!(shipFile.getName().startsWith(\"flink-dist\") && shipFile.getName().endsWith(\"jar\"))) {\n\t\t\t\tthis.shipFiles.add(shipFile);\n\t\t\t}\n\t\t}\n\t}",
            " 260  \n 261  \n 262  \n 263  \n 264 +\n 265  \n 266  \n 267  \n 268  ",
            "\tpublic void addShipFiles(List<File> shipFiles) {\n\t\tfor (File shipFile: shipFiles) {\n\t\t\t// remove uberjar from ship list (by default everything in the lib/ folder is added to\n\t\t\t// the list of files to ship, but we handle the uberjar separately.\n\t\t\tif (!(shipFile.getName().startsWith(\"flink-dist\") && shipFile.getName().endsWith(\"jar\"))) {\n\t\t\t\tthis.shipFiles.add(shipFile);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "YarnResourceManager::createTaskExecutorLaunchContext(Resource,String,String)",
            " 340  \n 341  \n 342  \n 343 -\n 344  \n 345  \n 346  \n 347  \n 348 -\n 349  \n 350  \n 351  \n 352  \n 353  \n 354 -\n 355  \n 356  \n 357  \n 358  \n 359 -\n 360  \n 361  \n 362 -\n 363  \n 364 -\n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  ",
            "\tprivate ContainerLaunchContext createTaskExecutorLaunchContext(Resource resource, String containerId, String host)\n\t\t\tthrows Exception {\n\t\t// init the ContainerLaunchContext\n\t\tfinal String currDir = ENV.get(ApplicationConstants.Environment.PWD.key());\n\n\t\tfinal ContaineredTaskManagerParameters taskManagerParameters =\n\t\t\t\tContaineredTaskManagerParameters.create(flinkConfig, resource.getMemory(), 1);\n\n\t\tLOG.info(\"TaskExecutor{} will be started with container size {} MB, JVM heap size {} MB, \" +\n\t\t\t\t\"JVM direct memory limit {} MB\",\n\t\t\t\tcontainerId,\n\t\t\t\ttaskManagerParameters.taskManagerTotalMemoryMB(),\n\t\t\t\ttaskManagerParameters.taskManagerHeapSizeMB(),\n\t\t\t\ttaskManagerParameters.taskManagerDirectMemoryLimitMB());\n\t\tint timeout = flinkConfig.getInteger(ConfigConstants.TASK_MANAGER_MAX_REGISTRATION_DURATION, \n\t\t\t\tDEFAULT_TASK_MANAGER_REGISTRATION_DURATION);\n\t\tFiniteDuration teRegistrationTimeout = new FiniteDuration(timeout, TimeUnit.SECONDS);\n\t\tfinal Configuration taskManagerConfig = BootstrapTools.generateTaskManagerConfiguration(\n\t\t\t\tflinkConfig, \"\", 0, 1, teRegistrationTimeout);\n\t\tLOG.debug(\"TaskManager configuration: {}\", taskManagerConfig);\n\n\t\tContainerLaunchContext taskExecutorLaunchContext = Utils.createTaskExecutorContext(\n\t\t\t\tflinkConfig, yarnConfig, ENV,\n\t\t\t\ttaskManagerParameters, taskManagerConfig,\n\t\t\t\tcurrDir, YarnTaskExecutorRunner.class, LOG);\n\n\t\t// set a special environment variable to uniquely identify this container\n\t\ttaskExecutorLaunchContext.getEnvironment()\n\t\t\t\t.put(ENV_FLINK_CONTAINER_ID, containerId);\n\t\ttaskExecutorLaunchContext.getEnvironment()\n\t\t\t\t.put(ENV_FLINK_NODE_ID, host);\n\t\treturn taskExecutorLaunchContext;\n\t}",
            " 339  \n 340  \n 341  \n 342 +\n 343  \n 344  \n 345  \n 346  \n 347 +\n 348  \n 349  \n 350  \n 351  \n 352  \n 353 +\n 354  \n 355  \n 356  \n 357  \n 358 +\n 359  \n 360  \n 361 +\n 362  \n 363 +\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  ",
            "\tprivate ContainerLaunchContext createTaskExecutorLaunchContext(Resource resource, String containerId, String host)\n\t\t\tthrows Exception {\n\t\t// init the ContainerLaunchContext\n\t\tfinal String currDir = env.get(ApplicationConstants.Environment.PWD.key());\n\n\t\tfinal ContaineredTaskManagerParameters taskManagerParameters =\n\t\t\t\tContaineredTaskManagerParameters.create(flinkConfig, resource.getMemory(), 1);\n\n\t\tlog.info(\"TaskExecutor{} will be started with container size {} MB, JVM heap size {} MB, \" +\n\t\t\t\t\"JVM direct memory limit {} MB\",\n\t\t\t\tcontainerId,\n\t\t\t\ttaskManagerParameters.taskManagerTotalMemoryMB(),\n\t\t\t\ttaskManagerParameters.taskManagerHeapSizeMB(),\n\t\t\t\ttaskManagerParameters.taskManagerDirectMemoryLimitMB());\n\t\tint timeout = flinkConfig.getInteger(ConfigConstants.TASK_MANAGER_MAX_REGISTRATION_DURATION,\n\t\t\t\tDEFAULT_TASK_MANAGER_REGISTRATION_DURATION);\n\t\tFiniteDuration teRegistrationTimeout = new FiniteDuration(timeout, TimeUnit.SECONDS);\n\t\tfinal Configuration taskManagerConfig = BootstrapTools.generateTaskManagerConfiguration(\n\t\t\t\tflinkConfig, \"\", 0, 1, teRegistrationTimeout);\n\t\tlog.debug(\"TaskManager configuration: {}\", taskManagerConfig);\n\n\t\tContainerLaunchContext taskExecutorLaunchContext = Utils.createTaskExecutorContext(\n\t\t\t\tflinkConfig, yarnConfig, env,\n\t\t\t\ttaskManagerParameters, taskManagerConfig,\n\t\t\t\tcurrDir, YarnTaskExecutorRunner.class, log);\n\n\t\t// set a special environment variable to uniquely identify this container\n\t\ttaskExecutorLaunchContext.getEnvironment()\n\t\t\t\t.put(ENV_FLINK_CONTAINER_ID, containerId);\n\t\ttaskExecutorLaunchContext.getEnvironment()\n\t\t\t\t.put(ENV_FLINK_NODE_ID, host);\n\t\treturn taskExecutorLaunchContext;\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::getCurrentFreeClusterResources(YarnClient)",
            "1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009 -\n1010  \n1011 -\n1012  \n1013  \n1014 -\n1015  \n1016  \n1017  \n1018  \n1019  ",
            "\tprivate ClusterResourceDescription getCurrentFreeClusterResources(YarnClient yarnClient) throws YarnException, IOException {\n\t\tList<NodeReport> nodes = yarnClient.getNodeReports(NodeState.RUNNING);\n\n\t\tint totalFreeMemory = 0;\n\t\tint containerLimit = 0;\n\t\tint[] nodeManagersFree = new int[nodes.size()];\n\n\t\tfor(int i = 0; i < nodes.size(); i++) {\n\t\t\tNodeReport rep = nodes.get(i);\n\t\t\tint free = rep.getCapability().getMemory() - (rep.getUsed() != null ? rep.getUsed().getMemory() : 0 );\n\t\t\tnodeManagersFree[i] = free;\n\t\t\ttotalFreeMemory += free;\n\t\t\tif(free > containerLimit) {\n\t\t\t\tcontainerLimit = free;\n\t\t\t}\n\t\t}\n\t\treturn new ClusterResourceDescription(totalFreeMemory, containerLimit, nodeManagersFree);\n\t}",
            "1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014 +\n1015  \n1016 +\n1017  \n1018  \n1019 +\n1020  \n1021  \n1022  \n1023  \n1024  ",
            "\tprivate ClusterResourceDescription getCurrentFreeClusterResources(YarnClient yarnClient) throws YarnException, IOException {\n\t\tList<NodeReport> nodes = yarnClient.getNodeReports(NodeState.RUNNING);\n\n\t\tint totalFreeMemory = 0;\n\t\tint containerLimit = 0;\n\t\tint[] nodeManagersFree = new int[nodes.size()];\n\n\t\tfor (int i = 0; i < nodes.size(); i++) {\n\t\t\tNodeReport rep = nodes.get(i);\n\t\t\tint free = rep.getCapability().getMemory() - (rep.getUsed() != null ? rep.getUsed().getMemory() : 0);\n\t\t\tnodeManagersFree[i] = free;\n\t\t\ttotalFreeMemory += free;\n\t\t\tif (free > containerLimit) {\n\t\t\t\tcontainerLimit = free;\n\t\t\t}\n\t\t}\n\t\treturn new ClusterResourceDescription(totalFreeMemory, containerLimit, nodeManagersFree);\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::setName(String)",
            "1062  \n1063 -\n1064  \n1065  \n1066  \n1067  ",
            "\tpublic void setName(String name) {\n\t\tif(name == null) {\n\t\t\tthrow new IllegalArgumentException(\"The passed name is null\");\n\t\t}\n\t\tcustomName = name;\n\t}",
            "1067  \n1068 +\n1069  \n1070  \n1071  \n1072  ",
            "\tpublic void setName(String name) {\n\t\tif (name == null) {\n\t\t\tthrow new IllegalArgumentException(\"The passed name is null\");\n\t\t}\n\t\tcustomName = name;\n\t}"
        ],
        [
            "YarnClusterClient::shutdownCluster()",
            " 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 -\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388 -\n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  ",
            "\t/**\n\t * Shuts down the Yarn application\n\t */\n\tpublic void shutdownCluster() {\n\n\t\tif (hasBeenShutDown.getAndSet(true)) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (!isConnected) {\n\t\t\tthrow new IllegalStateException(\"The cluster has been not been connected to the ApplicationMaster.\");\n\t\t}\n\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(clientShutdownHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we are already in the shutdown hook\n\t\t}\n\n\t\tLOG.info(\"Sending shutdown request to the Application Master\");\n\t\ttry {\n\t\t\tFuture<Object> response =\n\t\t\t\tPatterns.ask(applicationClient.get(),\n\t\t\t\t\tnew YarnMessages.LocalStopYarnSession(getApplicationStatus(),\n\t\t\t\t\t\t\t\"Flink YARN Client requested shutdown\"),\n\t\t\t\t\tnew Timeout(akkaDuration));\n\t\t\tAwait.ready(response, akkaDuration);\n\t\t} catch(Exception e) {\n\t\t\tLOG.warn(\"Error while stopping YARN cluster.\", e);\n\t\t}\n\n\t\ttry {\n\t\t\tFile propertiesFile = FlinkYarnSessionCli.getYarnPropertiesLocation(flinkConfig);\n\t\t\tif (propertiesFile.isFile()) {\n\t\t\t\tif (propertiesFile.delete()) {\n\t\t\t\t\tLOG.info(\"Deleted Yarn properties file at {}\", propertiesFile.getAbsoluteFile().toString());\n\t\t\t\t} else {\n\t\t\t\t\tLOG.warn(\"Couldn't delete Yarn properties file at {}\", propertiesFile.getAbsoluteFile().toString());\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Exception while deleting the JobManager address file\", e);\n\t\t}\n\n\t\ttry {\n\t\t\tpollingRunner.stopRunner();\n\t\t\tpollingRunner.join(1000);\n\t\t} catch(InterruptedException e) {\n\t\t\tLOG.warn(\"Shutdown of the polling runner was interrupted\", e);\n\t\t\tThread.currentThread().interrupt();\n\t\t}\n\n\t\ttry {\n\t\t\tApplicationReport appReport = yarnClient.getApplicationReport(appId);\n\n\t\t\tLOG.info(\"Application \" + appId + \" finished with state \" + appReport\n\t\t\t\t.getYarnApplicationState() + \" and final state \" + appReport\n\t\t\t\t.getFinalApplicationStatus() + \" at \" + appReport.getFinishTime());\n\n\t\t\tif (appReport.getYarnApplicationState() == YarnApplicationState.FAILED || appReport.getYarnApplicationState()\n\t\t\t\t== YarnApplicationState.KILLED) {\n\t\t\t\tLOG.warn(\"Application failed. Diagnostics \" + appReport.getDiagnostics());\n\t\t\t\tLOG.warn(\"If log aggregation is activated in the Hadoop cluster, we recommend to retrieve \"\n\t\t\t\t\t+ \"the full application log using this command:\"\n\t\t\t\t\t+ System.lineSeparator()\n\t\t\t\t\t+ \"\\tyarn logs -applicationId \" + appReport.getApplicationId()\n\t\t\t\t\t+ System.lineSeparator()\n\t\t\t\t\t+ \"(It sometimes takes a few seconds until the logs are aggregated)\");\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Couldn't get final report\", e);\n\t\t}\n\n\t\tLOG.info(\"YARN Client is shutting down\");\n\t\tyarnClient.stop(); // actorRunner is using the yarnClient.\n\t\tyarnClient = null; // set null to clearly see if somebody wants to access it afterwards.\n\t}",
            " 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369 +\n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 +\n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  ",
            "\t/**\n\t * Shuts down the Yarn application.\n\t */\n\tpublic void shutdownCluster() {\n\n\t\tif (hasBeenShutDown.getAndSet(true)) {\n\t\t\treturn;\n\t\t}\n\n\t\tif (!isConnected) {\n\t\t\tthrow new IllegalStateException(\"The cluster has been not been connected to the ApplicationMaster.\");\n\t\t}\n\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(clientShutdownHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we are already in the shutdown hook\n\t\t}\n\n\t\tLOG.info(\"Sending shutdown request to the Application Master\");\n\t\ttry {\n\t\t\tFuture<Object> response =\n\t\t\t\tPatterns.ask(applicationClient.get(),\n\t\t\t\t\tnew YarnMessages.LocalStopYarnSession(getApplicationStatus(),\n\t\t\t\t\t\t\t\"Flink YARN Client requested shutdown\"),\n\t\t\t\t\tnew Timeout(akkaDuration));\n\t\t\tAwait.ready(response, akkaDuration);\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Error while stopping YARN cluster.\", e);\n\t\t}\n\n\t\ttry {\n\t\t\tFile propertiesFile = FlinkYarnSessionCli.getYarnPropertiesLocation(flinkConfig);\n\t\t\tif (propertiesFile.isFile()) {\n\t\t\t\tif (propertiesFile.delete()) {\n\t\t\t\t\tLOG.info(\"Deleted Yarn properties file at {}\", propertiesFile.getAbsoluteFile().toString());\n\t\t\t\t} else {\n\t\t\t\t\tLOG.warn(\"Couldn't delete Yarn properties file at {}\", propertiesFile.getAbsoluteFile().toString());\n\t\t\t\t}\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Exception while deleting the JobManager address file\", e);\n\t\t}\n\n\t\ttry {\n\t\t\tpollingRunner.stopRunner();\n\t\t\tpollingRunner.join(1000);\n\t\t} catch (InterruptedException e) {\n\t\t\tLOG.warn(\"Shutdown of the polling runner was interrupted\", e);\n\t\t\tThread.currentThread().interrupt();\n\t\t}\n\n\t\ttry {\n\t\t\tApplicationReport appReport = yarnClient.getApplicationReport(appId);\n\n\t\t\tLOG.info(\"Application \" + appId + \" finished with state \" + appReport\n\t\t\t\t.getYarnApplicationState() + \" and final state \" + appReport\n\t\t\t\t.getFinalApplicationStatus() + \" at \" + appReport.getFinishTime());\n\n\t\t\tif (appReport.getYarnApplicationState() == YarnApplicationState.FAILED || appReport.getYarnApplicationState()\n\t\t\t\t== YarnApplicationState.KILLED) {\n\t\t\t\tLOG.warn(\"Application failed. Diagnostics \" + appReport.getDiagnostics());\n\t\t\t\tLOG.warn(\"If log aggregation is activated in the Hadoop cluster, we recommend to retrieve \"\n\t\t\t\t\t+ \"the full application log using this command:\"\n\t\t\t\t\t+ System.lineSeparator()\n\t\t\t\t\t+ \"\\tyarn logs -applicationId \" + appReport.getApplicationId()\n\t\t\t\t\t+ System.lineSeparator()\n\t\t\t\t\t+ \"(It sometimes takes a few seconds until the logs are aggregated)\");\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Couldn't get final report\", e);\n\t\t}\n\n\t\tLOG.info(\"YARN Client is shutting down\");\n\t\tyarnClient.stop(); // actorRunner is using the yarnClient.\n\t\tyarnClient = null; // set null to clearly see if somebody wants to access it afterwards.\n\t}"
        ],
        [
            "AbstractYarnFlinkApplicationMasterRunner::createConfiguration(String,Map)",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 -\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  ",
            "\t/**\n\t * @param baseDirectory  The working directory\n\t * @param additional Additional parameters\n\t * \n\t * @return The configuration to be used by the TaskExecutors.\n\t */\n\tprivate static Configuration createConfiguration(String baseDirectory, Map<String, String> additional) {\n\t\tLOG.info(\"Loading config from directory {}.\", baseDirectory);\n\n\t\tConfiguration configuration = GlobalConfiguration.loadConfiguration(baseDirectory);\n\n\t\t// add dynamic properties to JobManager configuration.\n\t\tfor (Map.Entry<String, String> property : additional.entrySet()) {\n\t\t\tconfiguration.setString(property.getKey(), property.getValue());\n\t\t}\n\n\t\t// override zookeeper namespace with user cli argument (if provided)\n\t\tString cliZKNamespace = ENV.get(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE);\n\t\tif (cliZKNamespace != null && !cliZKNamespace.isEmpty()) {\n\t\t\tconfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, cliZKNamespace);\n\t\t}\n\n\t\t// if a web monitor shall be started, set the port to random binding\n\t\tif (configuration.getInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 0) >= 0) {\n\t\t\tconfiguration.setInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 0);\n\t\t}\n\n\t\t// if the user has set the deprecated YARN-specific config keys, we add the \n\t\t// corresponding generic config keys instead. that way, later code needs not\n\t\t// deal with deprecated config keys\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(configuration,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_RATIO,\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO);\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(configuration,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_MIN,\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN);\n\n\t\tBootstrapTools.substituteDeprecatedConfigPrefix(configuration,\n\t\t\tConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX,\n\t\t\tConfigConstants.CONTAINERIZED_MASTER_ENV_PREFIX);\n\n\t\tBootstrapTools.substituteDeprecatedConfigPrefix(configuration,\n\t\t\tConfigConstants.YARN_TASK_MANAGER_ENV_PREFIX,\n\t\t\tConfigConstants.CONTAINERIZED_TASK_MANAGER_ENV_PREFIX);\n\n\t\treturn configuration;\n\t}",
            " 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 +\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  ",
            "\t/**\n\t * @param baseDirectory  The working directory\n\t * @param additional Additional parameters\n\t *\n\t * @return The configuration to be used by the TaskExecutors.\n\t */\n\tprivate static Configuration createConfiguration(String baseDirectory, Map<String, String> additional) {\n\t\tLOG.info(\"Loading config from directory {}.\", baseDirectory);\n\n\t\tConfiguration configuration = GlobalConfiguration.loadConfiguration(baseDirectory);\n\n\t\t// add dynamic properties to JobManager configuration.\n\t\tfor (Map.Entry<String, String> property : additional.entrySet()) {\n\t\t\tconfiguration.setString(property.getKey(), property.getValue());\n\t\t}\n\n\t\t// override zookeeper namespace with user cli argument (if provided)\n\t\tString cliZKNamespace = ENV.get(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE);\n\t\tif (cliZKNamespace != null && !cliZKNamespace.isEmpty()) {\n\t\t\tconfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, cliZKNamespace);\n\t\t}\n\n\t\t// if a web monitor shall be started, set the port to random binding\n\t\tif (configuration.getInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 0) >= 0) {\n\t\t\tconfiguration.setInteger(ConfigConstants.JOB_MANAGER_WEB_PORT_KEY, 0);\n\t\t}\n\n\t\t// if the user has set the deprecated YARN-specific config keys, we add the\n\t\t// corresponding generic config keys instead. that way, later code needs not\n\t\t// deal with deprecated config keys\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(configuration,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_RATIO,\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_RATIO);\n\n\t\tBootstrapTools.substituteDeprecatedConfigKey(configuration,\n\t\t\tConfigConstants.YARN_HEAP_CUTOFF_MIN,\n\t\t\tConfigConstants.CONTAINERIZED_HEAP_CUTOFF_MIN);\n\n\t\tBootstrapTools.substituteDeprecatedConfigPrefix(configuration,\n\t\t\tConfigConstants.YARN_APPLICATION_MASTER_ENV_PREFIX,\n\t\t\tConfigConstants.CONTAINERIZED_MASTER_ENV_PREFIX);\n\n\t\tBootstrapTools.substituteDeprecatedConfigPrefix(configuration,\n\t\t\tConfigConstants.YARN_TASK_MANAGER_ENV_PREFIX,\n\t\t\tConfigConstants.CONTAINERIZED_TASK_MANAGER_ENV_PREFIX);\n\n\t\treturn configuration;\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::setTaskManagerCount(int)",
            " 241  \n 242 -\n 243  \n 244  \n 245  \n 246  ",
            "\tpublic void setTaskManagerCount(int tmCount) {\n\t\tif(tmCount < 1) {\n\t\t\tthrow new IllegalArgumentException(\"The TaskManager count has to be at least 1.\");\n\t\t}\n\t\tthis.taskManagerCount = tmCount;\n\t}",
            " 249  \n 250 +\n 251  \n 252  \n 253  \n 254  ",
            "\tpublic void setTaskManagerCount(int tmCount) {\n\t\tif (tmCount < 1) {\n\t\t\tthrow new IllegalArgumentException(\"The TaskManager count has to be at least 1.\");\n\t\t}\n\t\tthis.taskManagerCount = tmCount;\n\t}"
        ],
        [
            "FlinkYarnSessionCli::loadYarnPropertiesFile(CommandLine,Configuration)",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 -\n 188 -\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  ",
            "\t/**\n\t * Tries to load a Flink Yarn properties file and returns the Yarn application id if successful\n\t * @param cmdLine The command-line parameters\n\t * @param flinkConfiguration The flink configuration\n\t * @return Yarn application id or null if none could be retrieved\n\t */\n\tprivate String loadYarnPropertiesFile(CommandLine cmdLine, Configuration flinkConfiguration) {\n\n\t\tString jobManagerOption = cmdLine.getOptionValue(ADDRESS_OPTION.getOpt(), null);\n\t\tif (jobManagerOption != null) {\n\t\t\t// don't resume from properties file if a JobManager has been specified\n\t\t\treturn null;\n\t\t}\n\n\t\tfor (Option option : cmdLine.getOptions()) {\n\t\t\tif (ALL_OPTIONS.hasOption(option.getOpt())) {\n\t\t\t\tif (!option.getOpt().equals(DETACHED.getOpt())) {\n\t\t\t\t\t// don't resume from properties file if yarn options have been specified\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// load the YARN properties\n\t\tFile propertiesFile = getYarnPropertiesLocation(flinkConfiguration);\n\t\tif (!propertiesFile.exists()) {\n\t\t\treturn null;\n\t\t}\n\n\t\tlogAndSysout(\"Found YARN properties file \" + propertiesFile.getAbsolutePath());\n\n\t\tProperties yarnProperties = new Properties();\n\t\ttry {\n\t\t\ttry (InputStream is = new FileInputStream(propertiesFile)) {\n\t\t\t\tyarnProperties.load(is);\n\t\t\t}\n\t\t}\n\t\tcatch (IOException e) {\n\t\t\tthrow new RuntimeException(\"Cannot read the YARN properties file\", e);\n\t\t}\n\n\t\t// get the Yarn application id from the properties file\n\t\tString applicationID = yarnProperties.getProperty(YARN_APPLICATION_ID_KEY);\n\t\tif (applicationID == null) {\n\t\t\tthrow new IllegalConfigurationException(\"Yarn properties file found but doesn't contain a \" +\n\t\t\t\t\"Yarn application id. Please delete the file at \" + propertiesFile.getAbsolutePath());\n\t\t}\n\n\t\ttry {\n\t\t\t// try converting id to ApplicationId\n\t\t\tConverterUtils.toApplicationId(applicationID);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"YARN properties contains an invalid entry for \" +\n\t\t\t\t\"application id: \" + applicationID, e);\n\t\t}\n\n\t\tlogAndSysout(\"Using Yarn application id from YARN properties \" + applicationID);\n\n\t\t// configure the default parallelism from YARN\n\t\tString propParallelism = yarnProperties.getProperty(YARN_PROPERTIES_PARALLELISM);\n\t\tif (propParallelism != null) { // maybe the property is not set\n\t\t\ttry {\n\t\t\t\tint parallelism = Integer.parseInt(propParallelism);\n\t\t\t\tflinkConfiguration.setInteger(ConfigConstants.DEFAULT_PARALLELISM_KEY, parallelism);\n\n\t\t\t\tlogAndSysout(\"YARN properties set default parallelism to \" + parallelism);\n\t\t\t}\n\t\t\tcatch (NumberFormatException e) {\n\t\t\t\tthrow new RuntimeException(\"Error while parsing the YARN properties: \" +\n\t\t\t\t\t\"Property \" + YARN_PROPERTIES_PARALLELISM + \" is not an integer.\");\n\t\t\t}\n\t\t}\n\n\t\t// handle the YARN client's dynamic properties\n\t\tString dynamicPropertiesEncoded = yarnProperties.getProperty(YARN_PROPERTIES_DYNAMIC_PROPERTIES_STRING);\n\t\tMap<String, String> dynamicProperties = getDynamicProperties(dynamicPropertiesEncoded);\n\t\tfor (Map.Entry<String, String> dynamicProperty : dynamicProperties.entrySet()) {\n\t\t\tflinkConfiguration.setString(dynamicProperty.getKey(), dynamicProperty.getValue());\n\t\t}\n\n\t\treturn applicationID;\n\t}",
            " 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 +\n 189 +\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "\t/**\n\t * Tries to load a Flink Yarn properties file and returns the Yarn application id if successful.\n\t * @param cmdLine The command-line parameters\n\t * @param flinkConfiguration The flink configuration\n\t * @return Yarn application id or null if none could be retrieved\n\t */\n\tprivate String loadYarnPropertiesFile(CommandLine cmdLine, Configuration flinkConfiguration) {\n\n\t\tString jobManagerOption = cmdLine.getOptionValue(ADDRESS_OPTION.getOpt(), null);\n\t\tif (jobManagerOption != null) {\n\t\t\t// don't resume from properties file if a JobManager has been specified\n\t\t\treturn null;\n\t\t}\n\n\t\tfor (Option option : cmdLine.getOptions()) {\n\t\t\tif (allOptions.hasOption(option.getOpt())) {\n\t\t\t\tif (!option.getOpt().equals(detached.getOpt())) {\n\t\t\t\t\t// don't resume from properties file if yarn options have been specified\n\t\t\t\t\treturn null;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// load the YARN properties\n\t\tFile propertiesFile = getYarnPropertiesLocation(flinkConfiguration);\n\t\tif (!propertiesFile.exists()) {\n\t\t\treturn null;\n\t\t}\n\n\t\tlogAndSysout(\"Found YARN properties file \" + propertiesFile.getAbsolutePath());\n\n\t\tProperties yarnProperties = new Properties();\n\t\ttry {\n\t\t\ttry (InputStream is = new FileInputStream(propertiesFile)) {\n\t\t\t\tyarnProperties.load(is);\n\t\t\t}\n\t\t}\n\t\tcatch (IOException e) {\n\t\t\tthrow new RuntimeException(\"Cannot read the YARN properties file\", e);\n\t\t}\n\n\t\t// get the Yarn application id from the properties file\n\t\tString applicationID = yarnProperties.getProperty(YARN_APPLICATION_ID_KEY);\n\t\tif (applicationID == null) {\n\t\t\tthrow new IllegalConfigurationException(\"Yarn properties file found but doesn't contain a \" +\n\t\t\t\t\"Yarn application id. Please delete the file at \" + propertiesFile.getAbsolutePath());\n\t\t}\n\n\t\ttry {\n\t\t\t// try converting id to ApplicationId\n\t\t\tConverterUtils.toApplicationId(applicationID);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"YARN properties contains an invalid entry for \" +\n\t\t\t\t\"application id: \" + applicationID, e);\n\t\t}\n\n\t\tlogAndSysout(\"Using Yarn application id from YARN properties \" + applicationID);\n\n\t\t// configure the default parallelism from YARN\n\t\tString propParallelism = yarnProperties.getProperty(YARN_PROPERTIES_PARALLELISM);\n\t\tif (propParallelism != null) { // maybe the property is not set\n\t\t\ttry {\n\t\t\t\tint parallelism = Integer.parseInt(propParallelism);\n\t\t\t\tflinkConfiguration.setInteger(ConfigConstants.DEFAULT_PARALLELISM_KEY, parallelism);\n\n\t\t\t\tlogAndSysout(\"YARN properties set default parallelism to \" + parallelism);\n\t\t\t}\n\t\t\tcatch (NumberFormatException e) {\n\t\t\t\tthrow new RuntimeException(\"Error while parsing the YARN properties: \" +\n\t\t\t\t\t\"Property \" + YARN_PROPERTIES_PARALLELISM + \" is not an integer.\");\n\t\t\t}\n\t\t}\n\n\t\t// handle the YARN client's dynamic properties\n\t\tString dynamicPropertiesEncoded = yarnProperties.getProperty(YARN_PROPERTIES_DYNAMIC_PROPERTIES_STRING);\n\t\tMap<String, String> dynamicProperties = getDynamicProperties(dynamicPropertiesEncoded);\n\t\tfor (Map.Entry<String, String> dynamicProperty : dynamicProperties.entrySet()) {\n\t\t\tflinkConfiguration.setString(dynamicProperty.getKey(), dynamicProperty.getValue());\n\t\t}\n\n\t\treturn applicationID;\n\t}"
        ],
        [
            "FlinkYarnSessionCli::createDescriptor(String,CommandLine)",
            " 256  \n 257  \n 258  \n 259  \n 260 -\n 261 -\n 262  \n 263 -\n 264  \n 265 -\n 266  \n 267  \n 268  \n 269 -\n 270 -\n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286 -\n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 -\n 295 -\n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307 -\n 308 -\n 309  \n 310  \n 311  \n 312 -\n 313 -\n 314  \n 315  \n 316  \n 317  \n 318 -\n 319 -\n 320  \n 321  \n 322  \n 323 -\n 324 -\n 325  \n 326  \n 327  \n 328  \n 329 -\n 330 -\n 331  \n 332  \n 333  \n 334  \n 335  \n 336 -\n 337  \n 338  \n 339  \n 340  \n 341 -\n 342 -\n 343  \n 344  \n 345 -\n 346  \n 347  \n 348  \n 349  \n 350 -\n 351 -\n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371 -\n 372  \n 373  \n 374  \n 375  \n 376  \n 377  ",
            "\tpublic AbstractYarnClusterDescriptor createDescriptor(String defaultApplicationName, CommandLine cmd) {\n\n\t\tAbstractYarnClusterDescriptor yarnClusterDescriptor = getClusterDescriptor();\n\n\t\tif (!cmd.hasOption(CONTAINER.getOpt())) { // number of containers is required option!\n\t\t\tLOG.error(\"Missing required argument {}\", CONTAINER.getOpt());\n\t\t\tprintUsage();\n\t\t\tthrow new IllegalArgumentException(\"Missing required argument \" + CONTAINER.getOpt());\n\t\t}\n\t\tyarnClusterDescriptor.setTaskManagerCount(Integer.valueOf(cmd.getOptionValue(CONTAINER.getOpt())));\n\n\t\t// Jar Path\n\t\tPath localJarPath;\n\t\tif (cmd.hasOption(FLINK_JAR.getOpt())) {\n\t\t\tString userPath = cmd.getOptionValue(FLINK_JAR.getOpt());\n\t\t\tif (!userPath.startsWith(\"file://\")) {\n\t\t\t\tuserPath = \"file://\" + userPath;\n\t\t\t}\n\t\t\tlocalJarPath = new Path(userPath);\n\t\t} else {\n\t\t\tLOG.info(\"No path for the flink jar passed. Using the location of \"\n\t\t\t\t+ yarnClusterDescriptor.getClass() + \" to locate the jar\");\n\t\t\tString encodedJarPath =\n\t\t\t\tyarnClusterDescriptor.getClass().getProtectionDomain().getCodeSource().getLocation().getPath();\n\t\t\ttry {\n\t\t\t\t// we have to decode the url encoded parts of the path\n\t\t\t\tString decodedPath = URLDecoder.decode(encodedJarPath, Charset.defaultCharset().name());\n\t\t\t\tlocalJarPath = new Path(new File(decodedPath).toURI());\n\t\t\t} catch (UnsupportedEncodingException e) {\n\t\t\t\tthrow new RuntimeException(\"Couldn't decode the encoded Flink dist jar path: \" + encodedJarPath +\n\t\t\t\t\t\" Please supply a path manually via the -\" + FLINK_JAR.getOpt() + \" option.\");\n\t\t\t}\n\t\t}\n\n\t\tyarnClusterDescriptor.setLocalJarPath(localJarPath);\n\n\t\tList<File> shipFiles = new ArrayList<>();\n\t\t// path to directory to ship\n\t\tif (cmd.hasOption(SHIP_PATH.getOpt())) {\n\t\t\tString shipPath = cmd.getOptionValue(SHIP_PATH.getOpt());\n\t\t\tFile shipDir = new File(shipPath);\n\t\t\tif (shipDir.isDirectory()) {\n\t\t\t\tshipFiles.add(shipDir);\n\t\t\t} else {\n\t\t\t\tLOG.warn(\"Ship directory is not a directory. Ignoring it.\");\n\t\t\t}\n\t\t}\n\n\t\tyarnClusterDescriptor.addShipFiles(shipFiles);\n\n\t\t// queue\n\t\tif (cmd.hasOption(QUEUE.getOpt())) {\n\t\t\tyarnClusterDescriptor.setQueue(cmd.getOptionValue(QUEUE.getOpt()));\n\t\t}\n\n\t\t// JobManager Memory\n\t\tif (cmd.hasOption(JM_MEMORY.getOpt())) {\n\t\t\tint jmMemory = Integer.valueOf(cmd.getOptionValue(JM_MEMORY.getOpt()));\n\t\t\tyarnClusterDescriptor.setJobManagerMemory(jmMemory);\n\t\t}\n\n\t\t// Task Managers memory\n\t\tif (cmd.hasOption(TM_MEMORY.getOpt())) {\n\t\t\tint tmMemory = Integer.valueOf(cmd.getOptionValue(TM_MEMORY.getOpt()));\n\t\t\tyarnClusterDescriptor.setTaskManagerMemory(tmMemory);\n\t\t}\n\n\t\tif (cmd.hasOption(SLOTS.getOpt())) {\n\t\t\tint slots = Integer.valueOf(cmd.getOptionValue(SLOTS.getOpt()));\n\t\t\tyarnClusterDescriptor.setTaskManagerSlots(slots);\n\t\t}\n\n\t\tString[] dynamicProperties = null;\n\t\tif (cmd.hasOption(DYNAMIC_PROPERTIES.getOpt())) {\n\t\t\tdynamicProperties = cmd.getOptionValues(DYNAMIC_PROPERTIES.getOpt());\n\t\t}\n\t\tString dynamicPropertiesEncoded = StringUtils.join(dynamicProperties, YARN_DYNAMIC_PROPERTIES_SEPARATOR);\n\n\t\tyarnClusterDescriptor.setDynamicPropertiesEncoded(dynamicPropertiesEncoded);\n\n\t\tif (cmd.hasOption(DETACHED.getOpt()) || cmd.hasOption(CliFrontendParser.DETACHED_OPTION.getOpt())) {\n\t\t\tthis.detachedMode = true;\n\t\t\tyarnClusterDescriptor.setDetachedMode(true);\n\t\t}\n\n\t\tif(cmd.hasOption(NAME.getOpt())) {\n\t\t\tyarnClusterDescriptor.setName(cmd.getOptionValue(NAME.getOpt()));\n\t\t} else {\n\t\t\t// set the default application name, if none is specified\n\t\t\tif(defaultApplicationName != null) {\n\t\t\t\tyarnClusterDescriptor.setName(defaultApplicationName);\n\t\t\t}\n\t\t}\n\n\t\tif (cmd.hasOption(ZOOKEEPER_NAMESPACE.getOpt())) {\n\t\t\tString zookeeperNamespace = cmd.getOptionValue(ZOOKEEPER_NAMESPACE.getOpt());\n\t\t\tyarnClusterDescriptor.setZookeeperNamespace(zookeeperNamespace);\n\t\t}\n\n\t\t// ----- Convenience -----\n\n\t\t// the number of slots available from YARN:\n\t\tint yarnTmSlots = yarnClusterDescriptor.getTaskManagerSlots();\n\t\tif (yarnTmSlots == -1) {\n\t\t\tyarnTmSlots = 1;\n\t\t\tyarnClusterDescriptor.setTaskManagerSlots(yarnTmSlots);\n\t\t}\n\n\t\tint maxSlots = yarnTmSlots * yarnClusterDescriptor.getTaskManagerCount();\n\t\tint userParallelism = Integer.valueOf(cmd.getOptionValue(CliFrontendParser.PARALLELISM_OPTION.getOpt(), \"-1\"));\n\t\tif (userParallelism != -1) {\n\t\t\tint slotsPerTM = (int) Math.ceil((double) userParallelism / yarnClusterDescriptor.getTaskManagerCount());\n\t\t\tString message = \"The YARN cluster has \" + maxSlots + \" slots available, \" +\n\t\t\t\t\"but the user requested a parallelism of \" + userParallelism + \" on YARN. \" +\n\t\t\t\t\"Each of the \" + yarnClusterDescriptor.getTaskManagerCount() + \" TaskManagers \" +\n\t\t\t\t\"will get \"+slotsPerTM+\" slots.\";\n\t\t\tlogAndSysout(message);\n\t\t\tyarnClusterDescriptor.setTaskManagerSlots(slotsPerTM);\n\t\t}\n\n\t\treturn yarnClusterDescriptor;\n\t}",
            " 257  \n 258  \n 259  \n 260  \n 261 +\n 262 +\n 263  \n 264 +\n 265  \n 266 +\n 267  \n 268  \n 269  \n 270 +\n 271 +\n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287 +\n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295 +\n 296 +\n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308 +\n 309 +\n 310  \n 311  \n 312  \n 313 +\n 314 +\n 315  \n 316  \n 317  \n 318  \n 319 +\n 320 +\n 321  \n 322  \n 323  \n 324 +\n 325 +\n 326  \n 327  \n 328  \n 329  \n 330 +\n 331 +\n 332  \n 333  \n 334  \n 335  \n 336  \n 337 +\n 338  \n 339  \n 340  \n 341  \n 342 +\n 343 +\n 344  \n 345  \n 346 +\n 347  \n 348  \n 349  \n 350  \n 351 +\n 352 +\n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 +\n 373  \n 374  \n 375  \n 376  \n 377  \n 378  ",
            "\tpublic AbstractYarnClusterDescriptor createDescriptor(String defaultApplicationName, CommandLine cmd) {\n\n\t\tAbstractYarnClusterDescriptor yarnClusterDescriptor = getClusterDescriptor();\n\n\t\tif (!cmd.hasOption(container.getOpt())) { // number of containers is required option!\n\t\t\tLOG.error(\"Missing required argument {}\", container.getOpt());\n\t\t\tprintUsage();\n\t\t\tthrow new IllegalArgumentException(\"Missing required argument \" + container.getOpt());\n\t\t}\n\t\tyarnClusterDescriptor.setTaskManagerCount(Integer.valueOf(cmd.getOptionValue(container.getOpt())));\n\n\t\t// Jar Path\n\t\tPath localJarPath;\n\t\tif (cmd.hasOption(flinkJar.getOpt())) {\n\t\t\tString userPath = cmd.getOptionValue(flinkJar.getOpt());\n\t\t\tif (!userPath.startsWith(\"file://\")) {\n\t\t\t\tuserPath = \"file://\" + userPath;\n\t\t\t}\n\t\t\tlocalJarPath = new Path(userPath);\n\t\t} else {\n\t\t\tLOG.info(\"No path for the flink jar passed. Using the location of \"\n\t\t\t\t+ yarnClusterDescriptor.getClass() + \" to locate the jar\");\n\t\t\tString encodedJarPath =\n\t\t\t\tyarnClusterDescriptor.getClass().getProtectionDomain().getCodeSource().getLocation().getPath();\n\t\t\ttry {\n\t\t\t\t// we have to decode the url encoded parts of the path\n\t\t\t\tString decodedPath = URLDecoder.decode(encodedJarPath, Charset.defaultCharset().name());\n\t\t\t\tlocalJarPath = new Path(new File(decodedPath).toURI());\n\t\t\t} catch (UnsupportedEncodingException e) {\n\t\t\t\tthrow new RuntimeException(\"Couldn't decode the encoded Flink dist jar path: \" + encodedJarPath +\n\t\t\t\t\t\" Please supply a path manually via the -\" + flinkJar.getOpt() + \" option.\");\n\t\t\t}\n\t\t}\n\n\t\tyarnClusterDescriptor.setLocalJarPath(localJarPath);\n\n\t\tList<File> shipFiles = new ArrayList<>();\n\t\t// path to directory to ship\n\t\tif (cmd.hasOption(shipPath.getOpt())) {\n\t\t\tString shipPath = cmd.getOptionValue(this.shipPath.getOpt());\n\t\t\tFile shipDir = new File(shipPath);\n\t\t\tif (shipDir.isDirectory()) {\n\t\t\t\tshipFiles.add(shipDir);\n\t\t\t} else {\n\t\t\t\tLOG.warn(\"Ship directory is not a directory. Ignoring it.\");\n\t\t\t}\n\t\t}\n\n\t\tyarnClusterDescriptor.addShipFiles(shipFiles);\n\n\t\t// queue\n\t\tif (cmd.hasOption(queue.getOpt())) {\n\t\t\tyarnClusterDescriptor.setQueue(cmd.getOptionValue(queue.getOpt()));\n\t\t}\n\n\t\t// JobManager Memory\n\t\tif (cmd.hasOption(jmMemory.getOpt())) {\n\t\t\tint jmMemory = Integer.valueOf(cmd.getOptionValue(this.jmMemory.getOpt()));\n\t\t\tyarnClusterDescriptor.setJobManagerMemory(jmMemory);\n\t\t}\n\n\t\t// Task Managers memory\n\t\tif (cmd.hasOption(tmMemory.getOpt())) {\n\t\t\tint tmMemory = Integer.valueOf(cmd.getOptionValue(this.tmMemory.getOpt()));\n\t\t\tyarnClusterDescriptor.setTaskManagerMemory(tmMemory);\n\t\t}\n\n\t\tif (cmd.hasOption(slots.getOpt())) {\n\t\t\tint slots = Integer.valueOf(cmd.getOptionValue(this.slots.getOpt()));\n\t\t\tyarnClusterDescriptor.setTaskManagerSlots(slots);\n\t\t}\n\n\t\tString[] dynamicProperties = null;\n\t\tif (cmd.hasOption(dynamicproperties.getOpt())) {\n\t\t\tdynamicProperties = cmd.getOptionValues(dynamicproperties.getOpt());\n\t\t}\n\t\tString dynamicPropertiesEncoded = StringUtils.join(dynamicProperties, YARN_DYNAMIC_PROPERTIES_SEPARATOR);\n\n\t\tyarnClusterDescriptor.setDynamicPropertiesEncoded(dynamicPropertiesEncoded);\n\n\t\tif (cmd.hasOption(detached.getOpt()) || cmd.hasOption(CliFrontendParser.DETACHED_OPTION.getOpt())) {\n\t\t\tthis.detachedMode = true;\n\t\t\tyarnClusterDescriptor.setDetachedMode(true);\n\t\t}\n\n\t\tif (cmd.hasOption(name.getOpt())) {\n\t\t\tyarnClusterDescriptor.setName(cmd.getOptionValue(name.getOpt()));\n\t\t} else {\n\t\t\t// set the default application name, if none is specified\n\t\t\tif (defaultApplicationName != null) {\n\t\t\t\tyarnClusterDescriptor.setName(defaultApplicationName);\n\t\t\t}\n\t\t}\n\n\t\tif (cmd.hasOption(zookeeperNamespace.getOpt())) {\n\t\t\tString zookeeperNamespace = cmd.getOptionValue(this.zookeeperNamespace.getOpt());\n\t\t\tyarnClusterDescriptor.setZookeeperNamespace(zookeeperNamespace);\n\t\t}\n\n\t\t// ----- Convenience -----\n\n\t\t// the number of slots available from YARN:\n\t\tint yarnTmSlots = yarnClusterDescriptor.getTaskManagerSlots();\n\t\tif (yarnTmSlots == -1) {\n\t\t\tyarnTmSlots = 1;\n\t\t\tyarnClusterDescriptor.setTaskManagerSlots(yarnTmSlots);\n\t\t}\n\n\t\tint maxSlots = yarnTmSlots * yarnClusterDescriptor.getTaskManagerCount();\n\t\tint userParallelism = Integer.valueOf(cmd.getOptionValue(CliFrontendParser.PARALLELISM_OPTION.getOpt(), \"-1\"));\n\t\tif (userParallelism != -1) {\n\t\t\tint slotsPerTM = (int) Math.ceil((double) userParallelism / yarnClusterDescriptor.getTaskManagerCount());\n\t\t\tString message = \"The YARN cluster has \" + maxSlots + \" slots available, \" +\n\t\t\t\t\"but the user requested a parallelism of \" + userParallelism + \" on YARN. \" +\n\t\t\t\t\"Each of the \" + yarnClusterDescriptor.getTaskManagerCount() + \" TaskManagers \" +\n\t\t\t\t\"will get \" + slotsPerTM + \" slots.\";\n\t\t\tlogAndSysout(message);\n\t\t\tyarnClusterDescriptor.setTaskManagerSlots(slotsPerTM);\n\t\t}\n\n\t\treturn yarnClusterDescriptor;\n\t}"
        ],
        [
            "FlinkYarnSessionCli::retrieveCluster(CommandLine,Configuration)",
            " 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523 -\n 524 -\n 525  \n 526  \n 527 -\n 528 -\n 529 -\n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  ",
            "\t@Override\n\tpublic YarnClusterClient retrieveCluster(\n\t\t\tCommandLine cmdLine,\n\t\t\tConfiguration config) throws UnsupportedOperationException {\n\n\t\t// first check for an application id, then try to load from yarn properties\n\t\tString applicationID = cmdLine.hasOption(APPLICATION_ID.getOpt()) ?\n\t\t\t\tcmdLine.getOptionValue(APPLICATION_ID.getOpt())\n\t\t\t\t: loadYarnPropertiesFile(cmdLine, config);\n\n\t\tif(null != applicationID) {\n\t\t\tString zkNamespace = cmdLine.hasOption(ZOOKEEPER_NAMESPACE.getOpt()) ?\n\t\t\t\t\tcmdLine.getOptionValue(ZOOKEEPER_NAMESPACE.getOpt())\n\t\t\t\t\t: config.getString(HighAvailabilityOptions.HA_CLUSTER_ID, applicationID);\n\t\t\tconfig.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\t\tAbstractYarnClusterDescriptor yarnDescriptor = getClusterDescriptor();\n\t\t\tyarnDescriptor.setFlinkConfiguration(config);\n\t\t\treturn yarnDescriptor.retrieve(applicationID);\n\t\t} else {\n\t\t\tthrow new UnsupportedOperationException(\"Could not resume a Yarn cluster.\");\n\t\t}\n\t}",
            " 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523 +\n 524 +\n 525  \n 526  \n 527 +\n 528 +\n 529 +\n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  ",
            "\t@Override\n\tpublic YarnClusterClient retrieveCluster(\n\t\t\tCommandLine cmdLine,\n\t\t\tConfiguration config) throws UnsupportedOperationException {\n\n\t\t// first check for an application id, then try to load from yarn properties\n\t\tString applicationID = cmdLine.hasOption(applicationId.getOpt()) ?\n\t\t\t\tcmdLine.getOptionValue(applicationId.getOpt())\n\t\t\t\t: loadYarnPropertiesFile(cmdLine, config);\n\n\t\tif (null != applicationID) {\n\t\t\tString zkNamespace = cmdLine.hasOption(zookeeperNamespace.getOpt()) ?\n\t\t\t\t\tcmdLine.getOptionValue(zookeeperNamespace.getOpt())\n\t\t\t\t\t: config.getString(HighAvailabilityOptions.HA_CLUSTER_ID, applicationID);\n\t\t\tconfig.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\t\tAbstractYarnClusterDescriptor yarnDescriptor = getClusterDescriptor();\n\t\t\tyarnDescriptor.setFlinkConfiguration(config);\n\t\t\treturn yarnDescriptor.retrieve(applicationID);\n\t\t} else {\n\t\t\tthrow new UnsupportedOperationException(\"Could not resume a Yarn cluster.\");\n\t\t}\n\t}"
        ],
        [
            "Utils::obtainTokenForHBase(Credentials,Configuration)",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196 -\n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223 -\n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  ",
            "\t/**\n\t * Obtain Kerberos security token for HBase.\n\t */\n\tprivate static void obtainTokenForHBase(Credentials credentials, Configuration conf) throws IOException {\n\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\tLOG.info(\"Attempting to obtain Kerberos security token for HBase\");\n\t\t\ttry {\n\t\t\t\t// ----\n\t\t\t\t// Intended call: HBaseConfiguration.addHbaseResources(conf);\n\t\t\t\tClass\n\t\t\t\t\t\t.forName(\"org.apache.hadoop.hbase.HBaseConfiguration\")\n\t\t\t\t\t\t.getMethod(\"addHbaseResources\", Configuration.class )\n\t\t\t\t\t\t.invoke(null, conf);\n\t\t\t\t// ----\n\n\t\t\t\tLOG.info(\"HBase security setting: {}\", conf.get(\"hbase.security.authentication\"));\n\n\t\t\t\tif (!\"kerberos\".equals(conf.get(\"hbase.security.authentication\"))) {\n\t\t\t\t\tLOG.info(\"HBase has not been configured to use Kerberos.\");\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tLOG.info(\"Obtaining Kerberos security token for HBase\");\n\t\t\t\t// ----\n\t\t\t\t// Intended call: Token<AuthenticationTokenIdentifier> token = TokenUtil.obtainToken(conf);\n\t\t\t\tToken<?> token = (Token<?>) Class\n\t\t\t\t\t\t.forName(\"org.apache.hadoop.hbase.security.token.TokenUtil\")\n\t\t\t\t\t\t.getMethod(\"obtainToken\", Configuration.class)\n\t\t\t\t\t\t.invoke(null, conf);\n\t\t\t\t// ----\n\n\t\t\t\tif (token == null) {\n\t\t\t\t\tLOG.error(\"No Kerberos security token for HBase available\");\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tcredentials.addToken(token.getService(), token);\n\t\t\t\tLOG.info(\"Added HBase Kerberos security token to credentials.\");\n\t\t\t} catch ( ClassNotFoundException\n\t\t\t\t\t| NoSuchMethodException\n\t\t\t\t\t| IllegalAccessException\n\t\t\t\t\t| InvocationTargetException e) {\n\t\t\t\tLOG.info(\"HBase is not available (not packaged with this application): {} : \\\"{}\\\".\",\n\t\t\t\t\t\te.getClass().getSimpleName(), e.getMessage());\n\t\t\t}\n\t\t}\n\t}",
            " 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 +\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "\t/**\n\t * Obtain Kerberos security token for HBase.\n\t */\n\tprivate static void obtainTokenForHBase(Credentials credentials, Configuration conf) throws IOException {\n\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\tLOG.info(\"Attempting to obtain Kerberos security token for HBase\");\n\t\t\ttry {\n\t\t\t\t// ----\n\t\t\t\t// Intended call: HBaseConfiguration.addHbaseResources(conf);\n\t\t\t\tClass\n\t\t\t\t\t\t.forName(\"org.apache.hadoop.hbase.HBaseConfiguration\")\n\t\t\t\t\t\t.getMethod(\"addHbaseResources\", Configuration.class)\n\t\t\t\t\t\t.invoke(null, conf);\n\t\t\t\t// ----\n\n\t\t\t\tLOG.info(\"HBase security setting: {}\", conf.get(\"hbase.security.authentication\"));\n\n\t\t\t\tif (!\"kerberos\".equals(conf.get(\"hbase.security.authentication\"))) {\n\t\t\t\t\tLOG.info(\"HBase has not been configured to use Kerberos.\");\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tLOG.info(\"Obtaining Kerberos security token for HBase\");\n\t\t\t\t// ----\n\t\t\t\t// Intended call: Token<AuthenticationTokenIdentifier> token = TokenUtil.obtainToken(conf);\n\t\t\t\tToken<?> token = (Token<?>) Class\n\t\t\t\t\t\t.forName(\"org.apache.hadoop.hbase.security.token.TokenUtil\")\n\t\t\t\t\t\t.getMethod(\"obtainToken\", Configuration.class)\n\t\t\t\t\t\t.invoke(null, conf);\n\t\t\t\t// ----\n\n\t\t\t\tif (token == null) {\n\t\t\t\t\tLOG.error(\"No Kerberos security token for HBase available\");\n\t\t\t\t\treturn;\n\t\t\t\t}\n\n\t\t\t\tcredentials.addToken(token.getService(), token);\n\t\t\t\tLOG.info(\"Added HBase Kerberos security token to credentials.\");\n\t\t\t} catch (ClassNotFoundException\n\t\t\t\t\t| NoSuchMethodException\n\t\t\t\t\t| IllegalAccessException\n\t\t\t\t\t| InvocationTargetException e) {\n\t\t\t\tLOG.info(\"HBase is not available (not packaged with this application): {} : \\\"{}\\\".\",\n\t\t\t\t\t\te.getClass().getSimpleName(), e.getMessage());\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "YarnClusterClientV2::submitJob(JobGraph,ClassLoader)",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98 -\n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "\t@Override\n\tprotected JobSubmissionResult submitJob(JobGraph jobGraph, ClassLoader classLoader) throws ProgramInvocationException {\n\t\ttry {\n\t\t\t// Create application via yarnClient\n\t\t\tfinal YarnClientApplication yarnApplication = yarnClient.createApplication();\n\t\t\tApplicationReport report = this.clusterDescriptor.startAppMaster(jobGraph, yarnClient, yarnApplication);\n\t\t\tif (report.getYarnApplicationState().equals(YarnApplicationState.RUNNING)) {\n\t\t\t\tappId = report.getApplicationId();\n\t\t\t\ttrackingURL = report.getTrackingUrl();\n\t\t\t\tlogAndSysout(\"Please refer to \" + getWebInterfaceURL() \n\t\t\t\t\t\t+ \" for the running status of job \" +  jobGraph.getJobID().toString());\n\t\t\t\t//TODO: not support attach mode now\n\t\t\t\treturn new JobSubmissionResult(jobGraph.getJobID());\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new ProgramInvocationException(\"Fail to submit the job.\");\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new ProgramInvocationException(\"Fail to submit the job\", e.getCause());\n\t\t}\n\t}",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  ",
            "\t@Override\n\tprotected JobSubmissionResult submitJob(JobGraph jobGraph, ClassLoader classLoader) throws ProgramInvocationException {\n\t\ttry {\n\t\t\t// Create application via yarnClient\n\t\t\tfinal YarnClientApplication yarnApplication = yarnClient.createApplication();\n\t\t\tApplicationReport report = this.clusterDescriptor.startAppMaster(jobGraph, yarnClient, yarnApplication);\n\t\t\tif (report.getYarnApplicationState().equals(YarnApplicationState.RUNNING)) {\n\t\t\t\tappId = report.getApplicationId();\n\t\t\t\ttrackingURL = report.getTrackingUrl();\n\t\t\t\tlogAndSysout(\"Please refer to \" + getWebInterfaceURL()\n\t\t\t\t\t\t+ \" for the running status of job \" +  jobGraph.getJobID().toString());\n\t\t\t\t//TODO: not support attach mode now\n\t\t\t\treturn new JobSubmissionResult(jobGraph.getJobID());\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow new ProgramInvocationException(\"Fail to submit the job.\");\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new ProgramInvocationException(\"Fail to submit the job\", e.getCause());\n\t\t}\n\t}"
        ]
    ],
    "7292c8743d981d61b0f860367e0266b307e1362f": [
        [
            "SequenceFileWriter::open(FileSystem,Path)",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 -\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "\t@Override\n\tpublic void open(FileSystem fs, Path path) throws IOException {\n\t\tsuper.open(fs, path);\n\t\tif (keyClass == null) {\n\t\t\tthrow new IllegalStateException(\"Key Class has not been initialized.\");\n\t\t}\n\t\tif (valueClass == null) {\n\t\t\tthrow new IllegalStateException(\"Value Class has not been initialized.\");\n\t\t}\n\n\t\tCompressionCodec codec = null;\n\t\t\n\t\tConfiguration conf = HadoopFileSystem.getHadoopConfiguration();\n\n\t\tif (!compressionCodecName.equals(\"None\")) {\n\t\t\tCompressionCodecFactory codecFactory = new CompressionCodecFactory(conf);\n\t\t\tcodec = codecFactory.getCodecByName(compressionCodecName);\n\t\t\tif (codec == null) {\n\t\t\t\tthrow new RuntimeException(\"Codec \" + compressionCodecName + \" not found.\");\n\t\t\t}\n\t\t}\n\n\t\t// the non-deprecated constructor syntax is only available in recent hadoop versions...\n\t\twriter = SequenceFile.createWriter(conf,\n\t\t\t\tgetStream(),\n\t\t\t\tkeyClass,\n\t\t\t\tvalueClass,\n\t\t\t\tcompressionType,\n\t\t\t\tcodec);\n\t}",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92 +\n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "\t@Override\n\tpublic void open(FileSystem fs, Path path) throws IOException {\n\t\tsuper.open(fs, path);\n\t\tif (keyClass == null) {\n\t\t\tthrow new IllegalStateException(\"Key Class has not been initialized.\");\n\t\t}\n\t\tif (valueClass == null) {\n\t\t\tthrow new IllegalStateException(\"Value Class has not been initialized.\");\n\t\t}\n\n\t\tCompressionCodec codec = null;\n\n\t\tConfiguration conf = HadoopFileSystem.getHadoopConfiguration();\n\n\t\tif (!compressionCodecName.equals(\"None\")) {\n\t\t\tCompressionCodecFactory codecFactory = new CompressionCodecFactory(conf);\n\t\t\tcodec = codecFactory.getCodecByName(compressionCodecName);\n\t\t\tif (codec == null) {\n\t\t\t\tthrow new RuntimeException(\"Codec \" + compressionCodecName + \" not found.\");\n\t\t\t}\n\t\t}\n\n\t\t// the non-deprecated constructor syntax is only available in recent hadoop versions...\n\t\twriter = SequenceFile.createWriter(conf,\n\t\t\t\tgetStream(),\n\t\t\t\tkeyClass,\n\t\t\t\tvalueClass,\n\t\t\t\tcompressionType,\n\t\t\t\tcodec);\n\t}"
        ],
        [
            "RollingSinkITCase::testBucketStateTransitions()",
            " 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667 -\n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  ",
            "\t@Test\n\tpublic void testBucketStateTransitions() throws Exception {\n\t\tfinal File outDir = tempFolder.newFolder();\n\n\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0);\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\t// we have a bucket size of 5 bytes, so each record will get its own bucket,\n\t\t// i.e. the bucket should roll after every record.\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test1\", 1L));\n\t\ttestHarness.processElement(new StreamRecord<>(\"test2\", 1L));\n\t\tcheckFs(outDir, 1, 1 ,0, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test3\", 1L));\n\t\tcheckFs(outDir, 1, 2, 0, 0);\n\n\t\ttestHarness.snapshot(0, 0);\n\t\tcheckFs(outDir, 1, 2, 0, 0);\n\n\t\ttestHarness.notifyOfCompletedCheckpoint(0);\n\t\tcheckFs(outDir, 1, 0, 2, 0);\n\n\t\tOperatorStateHandles snapshot = testHarness.snapshot(1, 0);\n\n\t\ttestHarness.close();\n\t\tcheckFs(outDir, 0, 1, 2, 0);\n\n\t\ttestHarness = createRescalingTestSink(outDir, 1, 0);\n\t\ttestHarness.setup();\n\t\ttestHarness.initializeState(snapshot);\n\t\ttestHarness.open();\n\t\tcheckFs(outDir, 0, 0, 3, 1);\n\n\t\tsnapshot = testHarness.snapshot(2, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test4\", 10));\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\n\t\ttestHarness = createRescalingTestSink(outDir, 1, 0);\n\t\ttestHarness.setup();\n\t\ttestHarness.initializeState(snapshot);\n\t\ttestHarness.open();\n\n\t\t// the in-progress file remains as we do not clean up now\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\n\t\ttestHarness.close();\n\n\t\t// at close it is not moved to final because it is not part\n\t\t// of the current task's state, it was just a not cleaned up leftover.\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\t}",
            " 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651 +\n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  ",
            "\t@Test\n\tpublic void testBucketStateTransitions() throws Exception {\n\t\tfinal File outDir = tempFolder.newFolder();\n\n\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0);\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\t// we have a bucket size of 5 bytes, so each record will get its own bucket,\n\t\t// i.e. the bucket should roll after every record.\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test1\", 1L));\n\t\ttestHarness.processElement(new StreamRecord<>(\"test2\", 1L));\n\t\tcheckFs(outDir, 1, 1 , 0, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test3\", 1L));\n\t\tcheckFs(outDir, 1, 2, 0, 0);\n\n\t\ttestHarness.snapshot(0, 0);\n\t\tcheckFs(outDir, 1, 2, 0, 0);\n\n\t\ttestHarness.notifyOfCompletedCheckpoint(0);\n\t\tcheckFs(outDir, 1, 0, 2, 0);\n\n\t\tOperatorStateHandles snapshot = testHarness.snapshot(1, 0);\n\n\t\ttestHarness.close();\n\t\tcheckFs(outDir, 0, 1, 2, 0);\n\n\t\ttestHarness = createRescalingTestSink(outDir, 1, 0);\n\t\ttestHarness.setup();\n\t\ttestHarness.initializeState(snapshot);\n\t\ttestHarness.open();\n\t\tcheckFs(outDir, 0, 0, 3, 1);\n\n\t\tsnapshot = testHarness.snapshot(2, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test4\", 10));\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\n\t\ttestHarness = createRescalingTestSink(outDir, 1, 0);\n\t\ttestHarness.setup();\n\t\ttestHarness.initializeState(snapshot);\n\t\ttestHarness.open();\n\n\t\t// the in-progress file remains as we do not clean up now\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\n\t\ttestHarness.close();\n\n\t\t// at close it is not moved to final because it is not part\n\t\t// of the current task's state, it was just a not cleaned up leftover.\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\t}"
        ],
        [
            "RollingSinkITCase::testNonRollingSequenceFileWithCompressionWriter()",
            " 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 -\n 264 -\n 265  \n 266  \n 267 -\n 268  \n 269 -\n 270  \n 271  \n 272  \n 273 -\n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 -\n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305 -\n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322 -\n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  ",
            "\t/**\n\t * This tests {@link SequenceFileWriter}\n\t * with non-rolling output but with compression.\n\t */\n\t@Test\n\tpublic void testNonRollingSequenceFileWithCompressionWriter() throws Exception {\n\t\tfinal int NUM_ELEMENTS = 20;\n\t\tfinal int PARALLELISM = 2;\n\t\tfinal String outPath = hdfsURI + \"/seq-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(PARALLELISM);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\t\tDataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer,String>, Tuple2<IntWritable, Text>>() {\n\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {\n\t\t\t\treturn Tuple2.of(new IntWritable(value.f0), new Text(value.f1));\n\t\t\t}\n\t\t});\n\n\n\t\tRollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)\n\t\t\t\t.setWriter(new SequenceFileWriter<IntWritable, Text>(\"Default\", SequenceFile.CompressionType.BLOCK))\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tmapped.addSink(sink);\n\n\t\tenv.execute(\"RollingSink String Write Test\");\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\n\t\tSequenceFile.Reader reader = new SequenceFile.Reader(inStream,\n\t\t\t\t1000,\n\t\t\t\t0,\n\t\t\t\t100000,\n\t\t\t\tnew Configuration());\n\n\t\tIntWritable intWritable = new IntWritable();\n\t\tText txt = new Text();\n\n\t\tfor (int i = 0; i < NUM_ELEMENTS; i += 2) {\n\t\t\treader.next(intWritable, txt);\n\t\t\tAssert.assertEquals(i, intWritable.get());\n\t\t\tAssert.assertEquals(\"message #\" + i, txt.toString());\n\t\t}\n\n\t\treader.close();\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\n\t\treader = new SequenceFile.Reader(inStream,\n\t\t\t\t1000,\n\t\t\t\t0,\n\t\t\t\t100000,\n\t\t\t\tnew Configuration());\n\n\t\tfor (int i = 1; i < NUM_ELEMENTS; i += 2) {\n\t\t\treader.next(intWritable, txt);\n\t\t\tAssert.assertEquals(i, intWritable.get());\n\t\t\tAssert.assertEquals(\"message #\" + i, txt.toString());\n\t\t}\n\n\t\treader.close();\n\t\tinStream.close();\n\t}",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260  \n 261  \n 262 +\n 263  \n 264 +\n 265  \n 266  \n 267  \n 268 +\n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299 +\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316 +\n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  ",
            "\t/**\n\t * This tests {@link SequenceFileWriter}\n\t * with non-rolling output but with compression.\n\t */\n\t@Test\n\tpublic void testNonRollingSequenceFileWithCompressionWriter() throws Exception {\n\t\tfinal int numElements = 20;\n\t\tfinal String outPath = hdfsURI + \"/seq-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(2);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\t\tDataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {\n\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {\n\t\t\t\treturn Tuple2.of(new IntWritable(value.f0), new Text(value.f1));\n\t\t\t}\n\t\t});\n\n\t\tRollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)\n\t\t\t\t.setWriter(new SequenceFileWriter<IntWritable, Text>(\"Default\", SequenceFile.CompressionType.BLOCK))\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tmapped.addSink(sink);\n\n\t\tenv.execute(\"RollingSink String Write Test\");\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\n\t\tSequenceFile.Reader reader = new SequenceFile.Reader(inStream,\n\t\t\t\t1000,\n\t\t\t\t0,\n\t\t\t\t100000,\n\t\t\t\tnew Configuration());\n\n\t\tIntWritable intWritable = new IntWritable();\n\t\tText txt = new Text();\n\n\t\tfor (int i = 0; i < numElements; i += 2) {\n\t\t\treader.next(intWritable, txt);\n\t\t\tAssert.assertEquals(i, intWritable.get());\n\t\t\tAssert.assertEquals(\"message #\" + i, txt.toString());\n\t\t}\n\n\t\treader.close();\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\n\t\treader = new SequenceFile.Reader(inStream,\n\t\t\t\t1000,\n\t\t\t\t0,\n\t\t\t\t100000,\n\t\t\t\tnew Configuration());\n\n\t\tfor (int i = 1; i < numElements; i += 2) {\n\t\t\treader.next(intWritable, txt);\n\t\t\tAssert.assertEquals(i, intWritable.get());\n\t\t\tAssert.assertEquals(\"message #\" + i, txt.toString());\n\t\t}\n\n\t\treader.close();\n\t\tinStream.close();\n\t}"
        ],
        [
            "AvroKeyValueSinkWriter::AvroKeyValueSinkWriter(Map)",
            "  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88 -\n  89  \n  90  \n  91  \n  92  \n  93 -\n  94 -\n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  ",
            "\t/**\n\t * C'tor for the writer\n\t * <p>\n\t * You can provide different properties that will be used to configure avro key-value writer as simple properties map(see example above)\n\t * @param properties\n\t */\n\t@SuppressWarnings(\"deprecation\")\n\tpublic AvroKeyValueSinkWriter(Map<String, String> properties) {\n\t\tthis.properties = properties;\n\t\t\n\t\tString keySchemaString = properties.get(CONF_OUTPUT_KEY_SCHEMA);\n\t\tif (keySchemaString == null) {\n\t\t\tthrow new IllegalStateException(\"No key schema provided, set '\" + CONF_OUTPUT_KEY_SCHEMA + \"' property\");\n\t\t}\n\t\tSchema.parse(keySchemaString);//verifying that schema valid\n\t\t\n\t\tString valueSchemaString = properties.get(CONF_OUTPUT_VALUE_SCHEMA);\n\t\tif (valueSchemaString == null) {\n\t\t\tthrow new IllegalStateException(\"No value schema provided, set '\" + CONF_OUTPUT_VALUE_SCHEMA + \"' property\");\n\t\t}\n\t\tSchema.parse(valueSchemaString);//verifying that schema valid\n\t}",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  \n  91  \n  92  \n  93  \n  94 +\n  95 +\n  96  \n  97  \n  98  \n  99  \n 100 +\n 101  ",
            "\t/**\n\t * C'tor for the writer.\n\t *\n\t * <p>You can provide different properties that will be used to configure avro key-value writer as simple properties map(see example above)\n\t * @param properties\n\t */\n\t@SuppressWarnings(\"deprecation\")\n\tpublic AvroKeyValueSinkWriter(Map<String, String> properties) {\n\t\tthis.properties = properties;\n\n\t\tString keySchemaString = properties.get(CONF_OUTPUT_KEY_SCHEMA);\n\t\tif (keySchemaString == null) {\n\t\t\tthrow new IllegalStateException(\"No key schema provided, set '\" + CONF_OUTPUT_KEY_SCHEMA + \"' property\");\n\t\t}\n\t\tSchema.parse(keySchemaString); //verifying that schema valid\n\n\t\tString valueSchemaString = properties.get(CONF_OUTPUT_VALUE_SCHEMA);\n\t\tif (valueSchemaString == null) {\n\t\t\tthrow new IllegalStateException(\"No value schema provided, set '\" + CONF_OUTPUT_VALUE_SCHEMA + \"' property\");\n\t\t}\n\t\tSchema.parse(valueSchemaString); //verifying that schema valid\n\t}"
        ],
        [
            "RollingSinkITCase::testNonRollingSequenceFileWithoutCompressionWriter()",
            " 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 -\n 189 -\n 190  \n 191  \n 192 -\n 193  \n 194 -\n 195  \n 196  \n 197  \n 198 -\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230 -\n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "\t/**\n\t * This tests {@link SequenceFileWriter}\n\t * with non-rolling output and without compression.\n\t */\n\t@Test\n\tpublic void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {\n\t\tfinal int NUM_ELEMENTS = 20;\n\t\tfinal int PARALLELISM = 2;\n\t\tfinal String outPath = hdfsURI + \"/seq-no-comp-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(PARALLELISM);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\t\tDataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer,String>, Tuple2<IntWritable, Text>>() {\n\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {\n\t\t\t\treturn Tuple2.of(new IntWritable(value.f0), new Text(value.f1));\n\t\t\t}\n\t\t});\n\n\n\t\tRollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)\n\t\t\t\t.setWriter(new SequenceFileWriter<IntWritable, Text>())\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tmapped.addSink(sink);\n\n\t\tenv.execute(\"RollingSink String Write Test\");\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\n\t\tSequenceFile.Reader reader = new SequenceFile.Reader(inStream,\n\t\t\t\t1000,\n\t\t\t\t0,\n\t\t\t\t100000,\n\t\t\t\tnew Configuration());\n\n\t\tIntWritable intWritable = new IntWritable();\n\t\tText txt = new Text();\n\n\t\tfor (int i = 0; i < NUM_ELEMENTS; i += 2) {\n\t\t\treader.next(intWritable, txt);\n\t\t\tAssert.assertEquals(i, intWritable.get());\n\t\t\tAssert.assertEquals(\"message #\" + i, txt.toString());\n\t\t}\n\n\t\treader.close();\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\n\t\treader = new SequenceFile.Reader(inStream,\n\t\t\t\t1000,\n\t\t\t\t0,\n\t\t\t\t100000,\n\t\t\t\tnew Configuration());\n\n\t\tfor (int i = 1; i < NUM_ELEMENTS; i += 2) {\n\t\t\treader.next(intWritable, txt);\n\t\t\tAssert.assertEquals(i, intWritable.get());\n\t\t\tAssert.assertEquals(\"message #\" + i, txt.toString());\n\t\t}\n\n\t\treader.close();\n\t\tinStream.close();\n\t}",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186 +\n 187  \n 188  \n 189 +\n 190  \n 191 +\n 192  \n 193  \n 194  \n 195 +\n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226 +\n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243 +\n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  ",
            "\t/**\n\t * This tests {@link SequenceFileWriter}\n\t * with non-rolling output and without compression.\n\t */\n\t@Test\n\tpublic void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {\n\t\tfinal int numElements = 20;\n\t\tfinal String outPath = hdfsURI + \"/seq-no-comp-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(2);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\t\tDataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {\n\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t@Override\n\t\t\tpublic Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {\n\t\t\t\treturn Tuple2.of(new IntWritable(value.f0), new Text(value.f1));\n\t\t\t}\n\t\t});\n\n\t\tRollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)\n\t\t\t\t.setWriter(new SequenceFileWriter<IntWritable, Text>())\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tmapped.addSink(sink);\n\n\t\tenv.execute(\"RollingSink String Write Test\");\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\n\t\tSequenceFile.Reader reader = new SequenceFile.Reader(inStream,\n\t\t\t\t1000,\n\t\t\t\t0,\n\t\t\t\t100000,\n\t\t\t\tnew Configuration());\n\n\t\tIntWritable intWritable = new IntWritable();\n\t\tText txt = new Text();\n\n\t\tfor (int i = 0; i < numElements; i += 2) {\n\t\t\treader.next(intWritable, txt);\n\t\t\tAssert.assertEquals(i, intWritable.get());\n\t\t\tAssert.assertEquals(\"message #\" + i, txt.toString());\n\t\t}\n\n\t\treader.close();\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\n\t\treader = new SequenceFile.Reader(inStream,\n\t\t\t\t1000,\n\t\t\t\t0,\n\t\t\t\t100000,\n\t\t\t\tnew Configuration());\n\n\t\tfor (int i = 1; i < numElements; i += 2) {\n\t\t\treader.next(intWritable, txt);\n\t\t\tAssert.assertEquals(i, intWritable.get());\n\t\t\tAssert.assertEquals(\"message #\" + i, txt.toString());\n\t\t}\n\n\t\treader.close();\n\t\tinStream.close();\n\t}"
        ],
        [
            "BucketingSinkTest::testCustomBucketingInactiveBucketCleanup()",
            " 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761 -\n 762  \n 763  \n 764  \n 765  \n 766  \n 767 -\n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  ",
            "\t/**\n\t * This uses a custom bucketing function which determines the bucket from the input.\n\t * We use a simulated clock to reduce the number of buckets being written to over time.\n\t * This causes buckets to become 'inactive' and their file parts 'closed' by the sink.\n\t */\n\t@Test\n\tpublic void testCustomBucketingInactiveBucketCleanup() throws Exception {\n\t\tFile dataDir = tempFolder.newFolder();\n\n\t\tfinal int step1NumIds = 4;\n\t\tfinal int step2NumIds = 2;\n\t\tfinal int numElementsPerStep = 20;\n\n\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0);\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\tfor (int i = 0; i < numElementsPerStep; i++) {\n\t\t\ttestHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)));\n\t\t}\n\n\t\ttestHarness.setProcessingTime(2*60*1000L);\n\n\t\tfor (int i = 0; i < numElementsPerStep; i++) {\n\t\t\ttestHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)));\n\t\t}\n\n\t\ttestHarness.setProcessingTime(6*60*1000L);\n\n\t\tfor (int i = 0; i < numElementsPerStep; i++) {\n\t\t\ttestHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)));\n\t\t}\n\n\t\t// we should have 4 buckets, with 1 file each\n\t\t// 2 of these buckets should have been finalised due to becoming inactive\n\t\tint numFiles = 0;\n\t\tint numInProgress = 0;\n\t\tfor (File file: FileUtils.listFiles(dataDir, null, true)) {\n\t\t\tif (file.getAbsolutePath().endsWith(\"crc\")) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {\n\t\t\t\tnumInProgress++;\n\t\t\t}\n\t\t\tnumFiles++;\n\t\t}\n\n\t\ttestHarness.close();\n\n\t\tAssert.assertEquals(4, numFiles);\n\t\tAssert.assertEquals(2, numInProgress);\n\t}",
            " 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766 +\n 767  \n 768  \n 769  \n 770  \n 771  \n 772 +\n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  ",
            "\t/**\n\t * This uses a custom bucketing function which determines the bucket from the input.\n\t * We use a simulated clock to reduce the number of buckets being written to over time.\n\t * This causes buckets to become 'inactive' and their file parts 'closed' by the sink.\n\t */\n\t@Test\n\tpublic void testCustomBucketingInactiveBucketCleanup() throws Exception {\n\t\tFile dataDir = tempFolder.newFolder();\n\n\t\tfinal int step1NumIds = 4;\n\t\tfinal int step2NumIds = 2;\n\t\tfinal int numElementsPerStep = 20;\n\n\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createTestSink(dataDir, 1, 0);\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\tfor (int i = 0; i < numElementsPerStep; i++) {\n\t\t\ttestHarness.processElement(new StreamRecord<>(Integer.toString(i % step1NumIds)));\n\t\t}\n\n\t\ttestHarness.setProcessingTime(2 * 60 * 1000L);\n\n\t\tfor (int i = 0; i < numElementsPerStep; i++) {\n\t\t\ttestHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)));\n\t\t}\n\n\t\ttestHarness.setProcessingTime(6 * 60 * 1000L);\n\n\t\tfor (int i = 0; i < numElementsPerStep; i++) {\n\t\t\ttestHarness.processElement(new StreamRecord<>(Integer.toString(i % step2NumIds)));\n\t\t}\n\n\t\t// we should have 4 buckets, with 1 file each\n\t\t// 2 of these buckets should have been finalised due to becoming inactive\n\t\tint numFiles = 0;\n\t\tint numInProgress = 0;\n\t\tfor (File file: FileUtils.listFiles(dataDir, null, true)) {\n\t\t\tif (file.getAbsolutePath().endsWith(\"crc\")) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tif (file.getPath().endsWith(IN_PROGRESS_SUFFIX)) {\n\t\t\t\tnumInProgress++;\n\t\t\t}\n\t\t\tnumFiles++;\n\t\t}\n\n\t\ttestHarness.close();\n\n\t\tAssert.assertEquals(4, numFiles);\n\t\tAssert.assertEquals(2, numInProgress);\n\t}"
        ],
        [
            "AvroKeyValueSinkWriter::getInt(Map,String,int)",
            " 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "\tprivate int getInt(Map<String,String> conf, String key, int def) {\n\t\tString value = conf.get(key);\n\t\tif (value == null) {\n\t\t\treturn def;\n\t\t}\n\t\treturn Integer.parseInt(value);\n\t}",
            " 111 +\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "\tprivate int getInt(Map<String, String> conf, String key, int def) {\n\t\tString value = conf.get(key);\n\t\tif (value == null) {\n\t\t\treturn def;\n\t\t}\n\t\treturn Integer.parseInt(value);\n\t}"
        ],
        [
            "StreamWriterBase::hflushOrSync(FSDataOutputStream)",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83 -\n  84 -\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  ",
            "\t/**\n\t * If hflush is available in this version of HDFS, then this method calls\n\t * hflush, else it calls sync.\n\t * @param os - The stream to flush/sync\n\t * @throws java.io.IOException\n\t *\n\t * <p>\n\t * Note: This code comes from Flume\n\t */\n\tprotected void hflushOrSync(FSDataOutputStream os) throws IOException {\n\t\ttry {\n\t\t\t// At this point the refHflushOrSync cannot be null,\n\t\t\t// since register method would have thrown if it was.\n\t\t\tthis.refHflushOrSync.invoke(os);\n\n\t\t\tif (os instanceof HdfsDataOutputStream) {\n\t\t\t\t((HdfsDataOutputStream) os).hsync(EnumSet.of(HdfsDataOutputStream.SyncFlag.UPDATE_LENGTH));\n\t\t\t}\n\t\t} catch (InvocationTargetException e) {\n\t\t\tString msg = \"Error while trying to hflushOrSync!\";\n\t\t\tLOG.error(msg + \" \" + e.getCause());\n\t\t\tThrowable cause = e.getCause();\n\t\t\tif(cause != null && cause instanceof IOException) {\n\t\t\t\tthrow (IOException)cause;\n\t\t\t}\n\t\t\tthrow new RuntimeException(msg, e);\n\t\t} catch (Exception e) {\n\t\t\tString msg = \"Error while trying to hflushOrSync!\";\n\t\t\tLOG.error(msg + \" \" + e);\n\t\t\tthrow new RuntimeException(msg, e);\n\t\t}\n\t}",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86 +\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  ",
            "\t/**\n\t * If hflush is available in this version of HDFS, then this method calls\n\t * hflush, else it calls sync.\n\t *\n\t * <p>Note: This code comes from Flume\n\t *\n\t * @param os - The stream to flush/sync\n\t * @throws java.io.IOException\n\t */\n\tprotected void hflushOrSync(FSDataOutputStream os) throws IOException {\n\t\ttry {\n\t\t\t// At this point the refHflushOrSync cannot be null,\n\t\t\t// since register method would have thrown if it was.\n\t\t\tthis.refHflushOrSync.invoke(os);\n\n\t\t\tif (os instanceof HdfsDataOutputStream) {\n\t\t\t\t((HdfsDataOutputStream) os).hsync(EnumSet.of(HdfsDataOutputStream.SyncFlag.UPDATE_LENGTH));\n\t\t\t}\n\t\t} catch (InvocationTargetException e) {\n\t\t\tString msg = \"Error while trying to hflushOrSync!\";\n\t\t\tLOG.error(msg + \" \" + e.getCause());\n\t\t\tThrowable cause = e.getCause();\n\t\t\tif (cause != null && cause instanceof IOException) {\n\t\t\t\tthrow (IOException) cause;\n\t\t\t}\n\t\t\tthrow new RuntimeException(msg, e);\n\t\t} catch (Exception e) {\n\t\t\tString msg = \"Error while trying to hflushOrSync!\";\n\t\t\tLOG.error(msg + \" \" + e);\n\t\t\tthrow new RuntimeException(msg, e);\n\t\t}\n\t}"
        ],
        [
            "RollingSink::snapshotState(FunctionSnapshotContext)",
            " 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607 -\n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  ",
            "\t@Override\n\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n\t\tPreconditions.checkNotNull(restoredBucketStates,\n\t\t\t\"The \" + getClass().getSimpleName() + \" has not been properly initialized.\");\n\n\t\tint subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n\t\t\n\t\tif (isWriterOpen) {\n\t\t\tbucketState.currentFile = currentPartPath.toString();\n\t\t\tbucketState.currentFileValidLength = writer.flush();\n\t\t}\n\n\t\tsynchronized (bucketState.pendingFilesPerCheckpoint) {\n\t\t\tbucketState.pendingFilesPerCheckpoint.put(context.getCheckpointId(), bucketState.pendingFiles);\n\t\t}\n\t\tbucketState.pendingFiles = new ArrayList<>();\n\n\t\trestoredBucketStates.clear();\n\t\trestoredBucketStates.add(bucketState);\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"{} (taskIdx={}) checkpointed {}.\", getClass().getSimpleName(), subtaskIdx, bucketState);\n\t\t}\n\t}",
            " 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606 +\n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  ",
            "\t@Override\n\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n\t\tPreconditions.checkNotNull(restoredBucketStates,\n\t\t\t\"The \" + getClass().getSimpleName() + \" has not been properly initialized.\");\n\n\t\tint subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n\n\t\tif (isWriterOpen) {\n\t\t\tbucketState.currentFile = currentPartPath.toString();\n\t\t\tbucketState.currentFileValidLength = writer.flush();\n\t\t}\n\n\t\tsynchronized (bucketState.pendingFilesPerCheckpoint) {\n\t\t\tbucketState.pendingFilesPerCheckpoint.put(context.getCheckpointId(), bucketState.pendingFiles);\n\t\t}\n\t\tbucketState.pendingFiles = new ArrayList<>();\n\n\t\trestoredBucketStates.clear();\n\t\trestoredBucketStates.add(bucketState);\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"{} (taskIdx={}) checkpointed {}.\", getClass().getSimpleName(), subtaskIdx, bucketState);\n\t\t}\n\t}"
        ],
        [
            "AvroKeyValueSinkWriter::getCompressionCodec(Map)",
            " 119 -\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  ",
            "\tprivate CodecFactory getCompressionCodec(Map<String,String> conf) {\n\t\tif (getBoolean(conf, CONF_COMPRESS, false)) {\n\t\t\tint deflateLevel = getInt(conf, CONF_DEFLATE_LEVEL, CodecFactory.DEFAULT_DEFLATE_LEVEL);\n\t\t\tint xzLevel = getInt(conf, CONF_XZ_LEVEL, CodecFactory.DEFAULT_XZ_LEVEL);\n\n\t\t\tString outputCodec = conf.get(CONF_COMPRESS_CODEC);\n\n\t\t\tif (DataFileConstants.DEFLATE_CODEC.equals(outputCodec)) {\n\t\t\t\treturn CodecFactory.deflateCodec(deflateLevel);\n\t\t\t} else if (DataFileConstants.XZ_CODEC.equals(outputCodec)) {\n\t\t\t\treturn CodecFactory.xzCodec(xzLevel);\n\t\t\t} else {\n\t\t\t\treturn CodecFactory.fromString(outputCodec);\n\t\t\t}\n\t\t}\n\t\treturn CodecFactory.nullCodec();\n\t}",
            " 120 +\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "\tprivate CodecFactory getCompressionCodec(Map<String, String> conf) {\n\t\tif (getBoolean(conf, CONF_COMPRESS, false)) {\n\t\t\tint deflateLevel = getInt(conf, CONF_DEFLATE_LEVEL, CodecFactory.DEFAULT_DEFLATE_LEVEL);\n\t\t\tint xzLevel = getInt(conf, CONF_XZ_LEVEL, CodecFactory.DEFAULT_XZ_LEVEL);\n\n\t\t\tString outputCodec = conf.get(CONF_COMPRESS_CODEC);\n\n\t\t\tif (DataFileConstants.DEFLATE_CODEC.equals(outputCodec)) {\n\t\t\t\treturn CodecFactory.deflateCodec(deflateLevel);\n\t\t\t} else if (DataFileConstants.XZ_CODEC.equals(outputCodec)) {\n\t\t\t\treturn CodecFactory.xzCodec(xzLevel);\n\t\t\t} else {\n\t\t\t\treturn CodecFactory.fromString(outputCodec);\n\t\t\t}\n\t\t}\n\t\treturn CodecFactory.nullCodec();\n\t}"
        ],
        [
            "RollingSinkFaultToleranceITCase::testProgram(StreamExecutionEnvironment)",
            " 104  \n 105  \n 106  \n 107  \n 108 -\n 109 -\n 110  \n 111 -\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t@Override\n\tpublic void testProgram(StreamExecutionEnvironment env) {\n\t\tassertTrue(\"Broken test setup\", NUM_STRINGS % 40 == 0);\n\n\t\tint PARALLELISM = 12;\n\n\t\tenv.enableCheckpointing(20);\n\t\tenv.setParallelism(PARALLELISM);\n\t\tenv.disableOperatorChaining();\n\n\t\tDataStream<String> stream = env.addSource(new StringGeneratingSourceFunction(NUM_STRINGS)).startNewChain();\n\n\t\tDataStream<String> mapped = stream\n\t\t\t\t.map(new OnceFailingIdentityMapper(NUM_STRINGS));\n\n\t\tRollingSink<String> sink = new RollingSink<String>(outPath)\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setBatchSize(10000)\n\t\t\t\t.setValidLengthPrefix(\"\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(PENDING_SUFFIX)\n\t\t\t\t.setInProgressSuffix(IN_PROGRESS_SUFFIX);\n\n\t\tmapped.addSink(sink);\n\n\t}",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111 +\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t@Override\n\tpublic void testProgram(StreamExecutionEnvironment env) {\n\t\tassertTrue(\"Broken test setup\", NUM_STRINGS % 40 == 0);\n\n\t\tenv.enableCheckpointing(20);\n\t\tenv.setParallelism(12);\n\t\tenv.disableOperatorChaining();\n\n\t\tDataStream<String> stream = env.addSource(new StringGeneratingSourceFunction(NUM_STRINGS)).startNewChain();\n\n\t\tDataStream<String> mapped = stream\n\t\t\t\t.map(new OnceFailingIdentityMapper(NUM_STRINGS));\n\n\t\tRollingSink<String> sink = new RollingSink<String>(outPath)\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setBatchSize(10000)\n\t\t\t\t.setValidLengthPrefix(\"\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(PENDING_SUFFIX)\n\t\t\t\t.setInProgressSuffix(IN_PROGRESS_SUFFIX);\n\n\t\tmapped.addSink(sink);\n\n\t}"
        ],
        [
            "RollingSinkITCase::testNonRollingStringWriter()",
            " 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 -\n 132 -\n 133  \n 134  \n 135 -\n 136  \n 137 -\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163 -\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 -\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  ",
            "\t/**\n\t * This tests {@link StringWriter} with\n\t * non-rolling output.\n\t */\n\t@Test\n\tpublic void testNonRollingStringWriter() throws Exception {\n\t\tfinal int NUM_ELEMENTS = 20;\n\t\tfinal int PARALLELISM = 2;\n\t\tfinal String outPath = hdfsURI + \"/string-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(PARALLELISM);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\t\tRollingSink<String> sink = new RollingSink<String>(outPath)\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tsource\n\t\t\t\t.map(new MapFunction<Tuple2<Integer,String>, String>() {\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic String map(Tuple2<Integer, String> value) throws Exception {\n\t\t\t\t\t\treturn value.f1;\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t\t.addSink(sink);\n\n\t\tenv.execute(\"RollingSink String Write Test\");\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\n\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\tfor (int i = 0; i < NUM_ELEMENTS; i += 2) {\n\t\t\tString line = br.readLine();\n\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t}\n\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\n\t\tbr = new BufferedReader(new InputStreamReader(inStream));\n\n\t\tfor (int i = 1; i < NUM_ELEMENTS; i += 2) {\n\t\t\tString line = br.readLine();\n\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t}\n\n\t\tinStream.close();\n\t}",
            " 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 +\n 131  \n 132  \n 133 +\n 134  \n 135 +\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161 +\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172 +\n 173  \n 174  \n 175  \n 176  \n 177  \n 178  ",
            "\t/**\n\t * This tests {@link StringWriter} with\n\t * non-rolling output.\n\t */\n\t@Test\n\tpublic void testNonRollingStringWriter() throws Exception {\n\t\tfinal int numElements = 20;\n\t\tfinal String outPath = hdfsURI + \"/string-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(2);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\t\tRollingSink<String> sink = new RollingSink<String>(outPath)\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tsource\n\t\t\t\t.map(new MapFunction<Tuple2<Integer, String>, String>() {\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic String map(Tuple2<Integer, String> value) throws Exception {\n\t\t\t\t\t\treturn value.f1;\n\t\t\t\t\t}\n\t\t\t\t})\n\t\t\t\t.addSink(sink);\n\n\t\tenv.execute(\"RollingSink String Write Test\");\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\n\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\tfor (int i = 0; i < numElements; i += 2) {\n\t\t\tString line = br.readLine();\n\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t}\n\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\n\t\tbr = new BufferedReader(new InputStreamReader(inStream));\n\n\t\tfor (int i = 1; i < numElements; i += 2) {\n\t\t\tString line = br.readLine();\n\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t}\n\n\t\tinStream.close();\n\t}"
        ],
        [
            "BucketingSink::setFSConfig(org)",
            " 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333 -\n 334  \n 335  \n 336  \n 337  ",
            "\t/**\n\t * Specify a custom {@code Configuration} that will be used when creating\n\t * the {@link FileSystem} for writing.\n\t */\n\tpublic BucketingSink<T> setFSConfig(org.apache.hadoop.conf.Configuration config) {\n\t\tthis.fsConfig = new Configuration();\n\t\tfor(Map.Entry<String, String> entry : config) {\n\t\t\tfsConfig.setString(entry.getKey(), entry.getValue());\n\t\t}\n\t\treturn this;\n\t}",
            " 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334 +\n 335  \n 336  \n 337  \n 338  ",
            "\t/**\n\t * Specify a custom {@code Configuration} that will be used when creating\n\t * the {@link FileSystem} for writing.\n\t */\n\tpublic BucketingSink<T> setFSConfig(org.apache.hadoop.conf.Configuration config) {\n\t\tthis.fsConfig = new Configuration();\n\t\tfor (Map.Entry<String, String> entry : config) {\n\t\t\tfsConfig.setString(entry.getKey(), entry.getValue());\n\t\t}\n\t\treturn this;\n\t}"
        ],
        [
            "AvroKeyValueSinkWriter::getBoolean(Map,String,boolean)",
            " 102 -\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  ",
            "\tprivate boolean getBoolean(Map<String,String> conf, String key, boolean def) {\n\t\tString value = conf.get(key);\n\t\tif (value == null) {\n\t\t\treturn def;\n\t\t}\n\t\treturn Boolean.parseBoolean(value);\n\t}",
            " 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "\tprivate boolean getBoolean(Map<String, String> conf, String key, boolean def) {\n\t\tString value = conf.get(key);\n\t\tif (value == null) {\n\t\t\treturn def;\n\t\t}\n\t\treturn Boolean.parseBoolean(value);\n\t}"
        ],
        [
            "RollingSink::setFSConfig(org)",
            " 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322 -\n 323  \n 324  \n 325  \n 326  ",
            "\t/**\n\t * Specify a custom {@code Configuration} that will be used when creating\n\t * the {@link FileSystem} for writing.\n\t */\n\tpublic RollingSink<T> setFSConfig(org.apache.hadoop.conf.Configuration config) {\n\t\tthis.fsConfig = new Configuration();\n\t\tfor(Map.Entry<String, String> entry : config) {\n\t\t\tfsConfig.setString(entry.getKey(), entry.getValue());\n\t\t}\n\t\treturn this;\n\t}",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321 +\n 322  \n 323  \n 324  \n 325  ",
            "\t/**\n\t * Specify a custom {@code Configuration} that will be used when creating\n\t * the {@link FileSystem} for writing.\n\t */\n\tpublic RollingSink<T> setFSConfig(org.apache.hadoop.conf.Configuration config) {\n\t\tthis.fsConfig = new Configuration();\n\t\tfor (Map.Entry<String, String> entry : config) {\n\t\t\tfsConfig.setString(entry.getKey(), entry.getValue());\n\t\t}\n\t\treturn this;\n\t}"
        ],
        [
            "BucketingSinkTest::testUserDefinedConfiguration()",
            " 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813 -\n 814  \n 815  \n 816 -\n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  ",
            "\t/**\n\t * This tests user defined hdfs configuration\n\t * @throws Exception\n\t */\n\t@Test\n\tpublic void testUserDefinedConfiguration() throws Exception {\n\t\tfinal String outPath = hdfsURI + \"/string-non-rolling-with-config\";\n\t\tfinal int numElements = 20;\n\n\t\tMap<String, String> properties = new HashMap<>();\n\t\tSchema keySchema = Schema.create(Schema.Type.INT);\n\t\tSchema valueSchema = Schema.create(Schema.Type.STRING);\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true));\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC);\n\n\t\tConfiguration conf = new Configuration();\n\t\tconf.set(\"io.file.buffer.size\", \"40960\");\n\n\t\tBucketingSink<Tuple2<Integer,String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)\n\t\t\t.setFSConfig(conf)\n\t\t\t.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, \"io.file.buffer.size\", \"40960\"))\n\t\t\t.setBucketer(new BasePathBucketer<Tuple2<Integer,String>>())\n\t\t\t.setPartPrefix(PART_PREFIX)\n\t\t\t.setPendingPrefix(\"\")\n\t\t\t.setPendingSuffix(\"\");\n\n\t\tOneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =\n\t\t\tcreateTestSink(sink, 1, 0);\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\tfor (int i = 0; i < numElements; i++) {\n\t\t\ttestHarness.processElement(new StreamRecord<>(Tuple2.of(\n\t\t\t\ti, \"message #\" + Integer.toString(i)\n\t\t\t)));\n\t\t}\n\n\t\ttestHarness.close();\n\n\t\tGenericData.setStringType(valueSchema, GenericData.StringType.String);\n\t\tSchema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema);\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/\" + PART_PREFIX + \"-0-0\"));\n\n\t\tSpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema);\n\t\tDataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader);\n\t\tfor (int i = 0; i < numElements; i++) {\n\t\t\tAvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =\n\t\t\t\tnew AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\t}",
            " 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818 +\n 819  \n 820  \n 821 +\n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  ",
            "\t/**\n\t * This tests user defined hdfs configuration.\n\t * @throws Exception\n\t */\n\t@Test\n\tpublic void testUserDefinedConfiguration() throws Exception {\n\t\tfinal String outPath = hdfsURI + \"/string-non-rolling-with-config\";\n\t\tfinal int numElements = 20;\n\n\t\tMap<String, String> properties = new HashMap<>();\n\t\tSchema keySchema = Schema.create(Schema.Type.INT);\n\t\tSchema valueSchema = Schema.create(Schema.Type.STRING);\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true));\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC);\n\n\t\tConfiguration conf = new Configuration();\n\t\tconf.set(\"io.file.buffer.size\", \"40960\");\n\n\t\tBucketingSink<Tuple2<Integer, String>> sink = new BucketingSink<Tuple2<Integer, String>>(outPath)\n\t\t\t.setFSConfig(conf)\n\t\t\t.setWriter(new StreamWriterWithConfigCheck<Integer, String>(properties, \"io.file.buffer.size\", \"40960\"))\n\t\t\t.setBucketer(new BasePathBucketer<Tuple2<Integer, String>>())\n\t\t\t.setPartPrefix(PART_PREFIX)\n\t\t\t.setPendingPrefix(\"\")\n\t\t\t.setPendingSuffix(\"\");\n\n\t\tOneInputStreamOperatorTestHarness<Tuple2<Integer, String>, Object> testHarness =\n\t\t\tcreateTestSink(sink, 1, 0);\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\tfor (int i = 0; i < numElements; i++) {\n\t\t\ttestHarness.processElement(new StreamRecord<>(Tuple2.of(\n\t\t\t\ti, \"message #\" + Integer.toString(i)\n\t\t\t)));\n\t\t}\n\n\t\ttestHarness.close();\n\n\t\tGenericData.setStringType(valueSchema, GenericData.StringType.String);\n\t\tSchema elementSchema = AvroKeyValueSinkWriter.AvroKeyValue.getSchema(keySchema, valueSchema);\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/\" + PART_PREFIX + \"-0-0\"));\n\n\t\tSpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<>(elementSchema);\n\t\tDataFileStream<GenericRecord> dataFileStream = new DataFileStream<>(inStream, elementReader);\n\t\tfor (int i = 0; i < numElements; i++) {\n\t\t\tAvroKeyValueSinkWriter.AvroKeyValue<Integer, String> wrappedEntry =\n\t\t\t\tnew AvroKeyValueSinkWriter.AvroKeyValue<>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\t}"
        ],
        [
            "AvroKeyValueSinkWriter::close()",
            " 148  \n 149  \n 150 -\n 151  \n 152  \n 153  \n 154  ",
            "\t@Override\n\tpublic void close() throws IOException {\n\t\tsuper.close();//the order is important since super.close flushes inside\n\t\tif (keyValueWriter != null) {\n\t\t\tkeyValueWriter.close();\n\t\t}\n\t}",
            " 149  \n 150  \n 151 +\n 152  \n 153  \n 154  \n 155  ",
            "\t@Override\n\tpublic void close() throws IOException {\n\t\tsuper.close(); //the order is important since super.close flushes inside\n\t\tif (keyValueWriter != null) {\n\t\t\tkeyValueWriter.close();\n\t\t}\n\t}"
        ],
        [
            "RollingSinkITCase::testDateTimeRollingStringWriter()",
            " 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539 -\n 540 -\n 541  \n 542  \n 543  \n 544  \n 545  \n 546 -\n 547 -\n 548 -\n 549  \n 550  \n 551 -\n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  ",
            "\t/**\n\t * This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to\n\t * produce rolling files. The clock of DateTimeBucketer is set to\n\t * {@link ModifyableClock} to keep the time in lockstep with the processing of elements using\n\t * latches.\n\t */\n\t@Test\n\tpublic void testDateTimeRollingStringWriter() throws Exception {\n\t\tfinal int NUM_ELEMENTS = 20;\n\t\tfinal int PARALLELISM = 2;\n\t\tfinal String outPath = hdfsURI + \"/rolling-out\";\n\t\tDateTimeBucketer.setClock(new ModifyableClock());\n\t\tModifyableClock.setCurrentTime(0);\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(PARALLELISM);\n\n\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(\n\t\t\t\tNUM_ELEMENTS))\n\t\t\t\t.broadcast();\n\n\t\t// the parallel flatMap is chained to the sink, so when it has seen 5 elements it can\n\t\t// fire the latch\n\t\tDataStream<String> mapped = source\n\t\t\t\t.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\tint count = 0;\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void flatMap(Tuple2<Integer, String> value,\n\t\t\t\t\t\t\tCollector<String> out) throws Exception {\n\t\t\t\t\t\tout.collect(value.f1);\n\t\t\t\t\t\tcount++;\n\t\t\t\t\t\tif (count >= 5) {\n\t\t\t\t\t\t\tif (getRuntimeContext().getIndexOfThisSubtask() == 0) {\n\t\t\t\t\t\t\t\tlatch1.trigger();\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tlatch2.trigger();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcount = 0;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t});\n\n\t\tRollingSink<String> sink = new RollingSink<String>(outPath)\n\t\t\t\t.setBucketer(new DateTimeBucketer(\"ss\"))\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tmapped.addSink(sink);\n\n\t\tenv.execute(\"RollingSink String Write Test\");\n\n\t\tRemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true);\n\n\t\t// we should have 8 rolling files, 4 time intervals and parallelism of 2\n\t\tint numFiles = 0;\n\t\twhile (files.hasNext()) {\n\t\t\tLocatedFileStatus file = files.next();\n\t\t\tnumFiles++;\n\t\t\tif (file.getPath().toString().contains(\"rolling-out/00\")) {\n\t\t\t\tFSDataInputStream inStream = dfs.open(file.getPath());\n\n\t\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\t\t\tfor (int i = 0; i < 5; i++) {\n\t\t\t\t\tString line = br.readLine();\n\t\t\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t\t\t}\n\n\t\t\t\tinStream.close();\n\t\t\t} else if (file.getPath().toString().contains(\"rolling-out/05\")) {\n\t\t\t\tFSDataInputStream inStream = dfs.open(file.getPath());\n\n\t\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\t\t\tfor (int i = 5; i < 10; i++) {\n\t\t\t\t\tString line = br.readLine();\n\t\t\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t\t\t}\n\n\t\t\t\tinStream.close();\n\t\t\t} else if (file.getPath().toString().contains(\"rolling-out/10\")) {\n\t\t\t\tFSDataInputStream inStream = dfs.open(file.getPath());\n\n\t\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\t\t\tfor (int i = 10; i < 15; i++) {\n\t\t\t\t\tString line = br.readLine();\n\t\t\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t\t\t}\n\n\t\t\t\tinStream.close();\n\t\t\t} else if (file.getPath().toString().contains(\"rolling-out/15\")) {\n\t\t\t\tFSDataInputStream inStream = dfs.open(file.getPath());\n\n\t\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\t\t\tfor (int i = 15; i < 20; i++) {\n\t\t\t\t\tString line = br.readLine();\n\t\t\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t\t\t}\n\n\t\t\t\tinStream.close();\n\t\t\t} else {\n\t\t\t\tAssert.fail(\"File \" + file + \" does not match any expected roll pattern.\");\n\t\t\t}\n\t\t}\n\n\t\tAssert.assertEquals(8, numFiles);\n\t}",
            " 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526 +\n 527  \n 528  \n 529  \n 530  \n 531  \n 532 +\n 533  \n 534  \n 535 +\n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  ",
            "\t/**\n\t * This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to\n\t * produce rolling files. The clock of DateTimeBucketer is set to\n\t * {@link ModifyableClock} to keep the time in lockstep with the processing of elements using\n\t * latches.\n\t */\n\t@Test\n\tpublic void testDateTimeRollingStringWriter() throws Exception {\n\t\tfinal int numElements = 20;\n\t\tfinal String outPath = hdfsURI + \"/rolling-out\";\n\t\tDateTimeBucketer.setClock(new ModifyableClock());\n\t\tModifyableClock.setCurrentTime(0);\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(2);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(\n\t\t\t\tnumElements))\n\t\t\t\t.broadcast();\n\n\t\t// the parallel flatMap is chained to the sink, so when it has seen 5 elements it can\n\t\t// fire the latch\n\t\tDataStream<String> mapped = source\n\t\t\t\t.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {\n\t\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t\tint count = 0;\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void flatMap(Tuple2<Integer, String> value,\n\t\t\t\t\t\t\tCollector<String> out) throws Exception {\n\t\t\t\t\t\tout.collect(value.f1);\n\t\t\t\t\t\tcount++;\n\t\t\t\t\t\tif (count >= 5) {\n\t\t\t\t\t\t\tif (getRuntimeContext().getIndexOfThisSubtask() == 0) {\n\t\t\t\t\t\t\t\tlatch1.trigger();\n\t\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t\tlatch2.trigger();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcount = 0;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t});\n\n\t\tRollingSink<String> sink = new RollingSink<String>(outPath)\n\t\t\t\t.setBucketer(new DateTimeBucketer(\"ss\"))\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tmapped.addSink(sink);\n\n\t\tenv.execute(\"RollingSink String Write Test\");\n\n\t\tRemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true);\n\n\t\t// we should have 8 rolling files, 4 time intervals and parallelism of 2\n\t\tint numFiles = 0;\n\t\twhile (files.hasNext()) {\n\t\t\tLocatedFileStatus file = files.next();\n\t\t\tnumFiles++;\n\t\t\tif (file.getPath().toString().contains(\"rolling-out/00\")) {\n\t\t\t\tFSDataInputStream inStream = dfs.open(file.getPath());\n\n\t\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\t\t\tfor (int i = 0; i < 5; i++) {\n\t\t\t\t\tString line = br.readLine();\n\t\t\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t\t\t}\n\n\t\t\t\tinStream.close();\n\t\t\t} else if (file.getPath().toString().contains(\"rolling-out/05\")) {\n\t\t\t\tFSDataInputStream inStream = dfs.open(file.getPath());\n\n\t\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\t\t\tfor (int i = 5; i < 10; i++) {\n\t\t\t\t\tString line = br.readLine();\n\t\t\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t\t\t}\n\n\t\t\t\tinStream.close();\n\t\t\t} else if (file.getPath().toString().contains(\"rolling-out/10\")) {\n\t\t\t\tFSDataInputStream inStream = dfs.open(file.getPath());\n\n\t\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\t\t\tfor (int i = 10; i < 15; i++) {\n\t\t\t\t\tString line = br.readLine();\n\t\t\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t\t\t}\n\n\t\t\t\tinStream.close();\n\t\t\t} else if (file.getPath().toString().contains(\"rolling-out/15\")) {\n\t\t\t\tFSDataInputStream inStream = dfs.open(file.getPath());\n\n\t\t\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\t\t\tfor (int i = 15; i < 20; i++) {\n\t\t\t\t\tString line = br.readLine();\n\t\t\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t\t\t}\n\n\t\t\t\tinStream.close();\n\t\t\t} else {\n\t\t\t\tAssert.fail(\"File \" + file + \" does not match any expected roll pattern.\");\n\t\t\t}\n\t\t}\n\n\t\tAssert.assertEquals(8, numFiles);\n\t}"
        ],
        [
            "BucketingSinkTest::testInactivityPeriodWithLateNotify()",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 -\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  ",
            "\t@Test\n\tpublic void testInactivityPeriodWithLateNotify() throws Exception {\n\t\tfinal File outDir = tempFolder.newFolder();\n\n\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0, 100);\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test1\", 1L));\n\t\ttestHarness.processElement(new StreamRecord<>(\"test2\", 1L));\n\t\tcheckFs(outDir, 2, 0 ,0, 0);\n\n\t\ttestHarness.setProcessingTime(101L);\t// put some in pending\n\t\tcheckFs(outDir, 0, 2, 0, 0);\n\n\t\ttestHarness.snapshot(0, 0);\t\t\t\t// put them in pending for 0\n\t\tcheckFs(outDir, 0, 2, 0, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test3\", 1L));\n\t\ttestHarness.processElement(new StreamRecord<>(\"test4\", 1L));\n\n\t\ttestHarness.setProcessingTime(202L);\t// put some in pending\n\n\t\ttestHarness.snapshot(1, 0);\t\t\t\t// put them in pending for 1\n\t\tcheckFs(outDir, 0, 4, 0, 0);\n\n\t\ttestHarness.notifyOfCompletedCheckpoint(0);\t// put the pending for 0 to the \"committed\" state\n\t\tcheckFs(outDir, 0, 2, 2, 0);\n\n\t\ttestHarness.notifyOfCompletedCheckpoint(1); // put the pending for 1 to the \"committed\" state\n\t\tcheckFs(outDir, 0, 0, 4, 0);\n\t}",
            " 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183 +\n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  ",
            "\t@Test\n\tpublic void testInactivityPeriodWithLateNotify() throws Exception {\n\t\tfinal File outDir = tempFolder.newFolder();\n\n\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0, 100);\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test1\", 1L));\n\t\ttestHarness.processElement(new StreamRecord<>(\"test2\", 1L));\n\t\tcheckFs(outDir, 2, 0 , 0, 0);\n\n\t\ttestHarness.setProcessingTime(101L);\t// put some in pending\n\t\tcheckFs(outDir, 0, 2, 0, 0);\n\n\t\ttestHarness.snapshot(0, 0);\t\t\t\t// put them in pending for 0\n\t\tcheckFs(outDir, 0, 2, 0, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test3\", 1L));\n\t\ttestHarness.processElement(new StreamRecord<>(\"test4\", 1L));\n\n\t\ttestHarness.setProcessingTime(202L);\t// put some in pending\n\n\t\ttestHarness.snapshot(1, 0);\t\t\t\t// put them in pending for 1\n\t\tcheckFs(outDir, 0, 4, 0, 0);\n\n\t\ttestHarness.notifyOfCompletedCheckpoint(0);\t// put the pending for 0 to the \"committed\" state\n\t\tcheckFs(outDir, 0, 2, 2, 0);\n\n\t\ttestHarness.notifyOfCompletedCheckpoint(1); // put the pending for 1 to the \"committed\" state\n\t\tcheckFs(outDir, 0, 0, 4, 0);\n\t}"
        ],
        [
            "RollingSink::handleRestoredBucketState(BucketState)",
            " 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671 -\n 672  \n 673  \n 674 -\n 675 -\n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694 -\n 695 -\n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752 -\n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  ",
            "\tprivate void handleRestoredBucketState(BucketState bucketState) {\n\t\t// we can clean all the pending files since they were renamed to\n\t\t// final files after this checkpoint was successful\n\t\t// (we re-start from the last **successful** checkpoint)\n\t\tbucketState.pendingFiles.clear();\n\n\t\tif (bucketState.currentFile != null) {\n\t\t\t// We were writing to a file when the last checkpoint occurred. This file can either\n\t\t\t// be still in-progress or became a pending file at some point after the checkpoint.\n\t\t\t// Either way, we have to truncate it back to a valid state (or write a .valid-length\n\t\t\t// file that specifies up to which length it is valid) and rename it to the final name\n\t\t\t// before starting a new bucket file.\n\t\t\tPath partPath = new Path(bucketState.currentFile);\n\t\t\ttry {\n\t\t\t\tPath partPendingPath = getPendingPathFor(partPath);\n\t\t\t\tPath partInProgressPath = getInProgressPathFor(partPath);\n\n\t\t\t\tif (fs.exists(partPendingPath)) {\n\t\t\t\t\tLOG.debug(\"In-progress file {} has been moved to pending after checkpoint, moving to final location.\", partPath);\n\t\t\t\t\t// has been moved to pending in the mean time, rename to final location\n\t\t\t\t\tfs.rename(partPendingPath, partPath);\n\t\t\t\t} else if (fs.exists(partInProgressPath)) {\n\t\t\t\t\tLOG.debug(\"In-progress file {} is still in-progress, moving to final location.\", partPath);\n\t\t\t\t\t// it was still in progress, rename to final path\n\t\t\t\t\tfs.rename(partInProgressPath, partPath);\n\t\t\t\t} else if (fs.exists(partPath)) {\n\t\t\t\t\tLOG.debug(\"In-Progress file {} was already moved to final location {}.\", bucketState.currentFile, partPath);\n\t\t\t\t} else {\n\t\t\t\t\tLOG.debug(\"In-Progress file {} was neither moved to pending nor is still in progress. Possibly, \" +\n\t\t\t\t\t\t\t\"it was moved to final location by a previous snapshot restore\", bucketState.currentFile);\n\t\t\t\t}\n\n\t\t\t\tif (this.refTruncate == null) {\n\t\t\t\t\tthis.refTruncate = reflectTruncate(fs);\n\t\t\t\t}\n\n\t\t\t\t// truncate it or write a \".valid-length\" file to specify up to which point it is valid\n\t\t\t\tif (refTruncate != null) {\n\t\t\t\t\tLOG.debug(\"Truncating {} to valid length {}\", partPath, bucketState.currentFileValidLength);\n\t\t\t\t\t// some-one else might still hold the lease from a previous try, we are\n\t\t\t\t\t// recovering, after all ...\n\t\t\t\t\tif (fs instanceof DistributedFileSystem) {\n\t\t\t\t\t\tDistributedFileSystem dfs = (DistributedFileSystem) fs;\n\t\t\t\t\t\tLOG.debug(\"Trying to recover file lease {}\", partPath);\n\t\t\t\t\t\tdfs.recoverLease(partPath);\n\t\t\t\t\t\tboolean isclosed= dfs.isFileClosed(partPath);\n\t\t\t\t\t\tStopWatch sw = new StopWatch();\n\t\t\t\t\t\tsw.start();\n\t\t\t\t\t\twhile(!isclosed) {\n\t\t\t\t\t\t\tif(sw.getTime() > asyncTimeout) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tThread.sleep(500);\n\t\t\t\t\t\t\t} catch (InterruptedException e1) {\n\t\t\t\t\t\t\t\t// ignore it\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tisclosed = dfs.isFileClosed(partPath);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tBoolean truncated = (Boolean) refTruncate.invoke(fs, partPath, bucketState.currentFileValidLength);\n\t\t\t\t\tif (!truncated) {\n\t\t\t\t\t\tLOG.debug(\"Truncate did not immediately complete for {}, waiting...\", partPath);\n\n\t\t\t\t\t\t// we must wait for the asynchronous truncate operation to complete\n\t\t\t\t\t\tStopWatch sw = new StopWatch();\n\t\t\t\t\t\tsw.start();\n\t\t\t\t\t\tlong newLen = fs.getFileStatus(partPath).getLen();\n\t\t\t\t\t\twhile(newLen != bucketState.currentFileValidLength) {\n\t\t\t\t\t\t\tif(sw.getTime() > asyncTimeout) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tThread.sleep(500);\n\t\t\t\t\t\t\t} catch (InterruptedException e1) {\n\t\t\t\t\t\t\t\t// ignore it\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnewLen = fs.getFileStatus(partPath).getLen();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (newLen != bucketState.currentFileValidLength) {\n\t\t\t\t\t\t\tthrow new RuntimeException(\"Truncate did not truncate to right length. Should be \" + bucketState.currentFileValidLength + \" is \" + newLen + \".\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t} else {\n\t\t\t\t\tLOG.debug(\"Writing valid-length file for {} to specify valid length {}\", partPath, bucketState.currentFileValidLength);\n\t\t\t\t\tPath validLengthFilePath = getValidLengthPathFor(partPath);\n\t\t\t\t\tif (!fs.exists(validLengthFilePath) && fs.exists(partPath)) {\n\t\t\t\t\t\tFSDataOutputStream lengthFileOut = fs.create(validLengthFilePath);\n\t\t\t\t\t\tlengthFileOut.writeUTF(Long.toString(bucketState.currentFileValidLength));\n\t\t\t\t\t\tlengthFileOut.close();\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// invalidate in the state object\n\t\t\t\tbucketState.currentFile = null;\n\t\t\t\tbucketState.currentFileValidLength = -1;\n\t\t\t\tisWriterOpen = false;\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.error(\"Error while restoring RollingSink state.\", e);\n\t\t\t\tthrow new RuntimeException(\"Error while restoring RollingSink state.\", e);\n\t\t\t} catch (InvocationTargetException | IllegalAccessException e) {\n\t\t\t\tLOG.error(\"Could not invoke truncate.\", e);\n\t\t\t\tthrow new RuntimeException(\"Could not invoke truncate.\", e);\n\t\t\t}\n\t\t}\n\n\t\t// Move files that are confirmed by a checkpoint but did not get moved to final location\n\t\t// because the checkpoint notification did not happen before a failure\n\n\t\tSet<Long> pastCheckpointIds = bucketState.pendingFilesPerCheckpoint.keySet();\n\t\tLOG.debug(\"Moving pending files to final location on restore.\");\n\t\tfor (Long pastCheckpointId : pastCheckpointIds) {\n\t\t\t// All the pending files are buckets that have been completed but are waiting to be renamed\n\t\t\t// to their final name\n\t\t\tfor (String filename : bucketState.pendingFilesPerCheckpoint.get(pastCheckpointId)) {\n\t\t\t\tPath finalPath = new Path(filename);\n\t\t\t\tPath pendingPath = getPendingPathFor(finalPath);\n\n\t\t\t\ttry {\n\t\t\t\t\tif (fs.exists(pendingPath)) {\n\t\t\t\t\t\tLOG.debug(\"(RESTORE) Moving pending file {} to final location after complete checkpoint {}.\", pendingPath, pastCheckpointId);\n\t\t\t\t\t\tfs.rename(pendingPath, finalPath);\n\t\t\t\t\t}\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tLOG.error(\"(RESTORE) Error while renaming pending file {} to final path {}: {}\", pendingPath, finalPath, e);\n\t\t\t\t\tthrow new RuntimeException(\"Error while renaming pending file \" + pendingPath+ \" to final path \" + finalPath, e);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tsynchronized (bucketState.pendingFilesPerCheckpoint) {\n\t\t\tbucketState.pendingFilesPerCheckpoint.clear();\n\t\t}\n\t}",
            " 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670 +\n 671  \n 672  \n 673 +\n 674 +\n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693 +\n 694 +\n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751 +\n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  ",
            "\tprivate void handleRestoredBucketState(BucketState bucketState) {\n\t\t// we can clean all the pending files since they were renamed to\n\t\t// final files after this checkpoint was successful\n\t\t// (we re-start from the last **successful** checkpoint)\n\t\tbucketState.pendingFiles.clear();\n\n\t\tif (bucketState.currentFile != null) {\n\t\t\t// We were writing to a file when the last checkpoint occurred. This file can either\n\t\t\t// be still in-progress or became a pending file at some point after the checkpoint.\n\t\t\t// Either way, we have to truncate it back to a valid state (or write a .valid-length\n\t\t\t// file that specifies up to which length it is valid) and rename it to the final name\n\t\t\t// before starting a new bucket file.\n\t\t\tPath partPath = new Path(bucketState.currentFile);\n\t\t\ttry {\n\t\t\t\tPath partPendingPath = getPendingPathFor(partPath);\n\t\t\t\tPath partInProgressPath = getInProgressPathFor(partPath);\n\n\t\t\t\tif (fs.exists(partPendingPath)) {\n\t\t\t\t\tLOG.debug(\"In-progress file {} has been moved to pending after checkpoint, moving to final location.\", partPath);\n\t\t\t\t\t// has been moved to pending in the mean time, rename to final location\n\t\t\t\t\tfs.rename(partPendingPath, partPath);\n\t\t\t\t} else if (fs.exists(partInProgressPath)) {\n\t\t\t\t\tLOG.debug(\"In-progress file {} is still in-progress, moving to final location.\", partPath);\n\t\t\t\t\t// it was still in progress, rename to final path\n\t\t\t\t\tfs.rename(partInProgressPath, partPath);\n\t\t\t\t} else if (fs.exists(partPath)) {\n\t\t\t\t\tLOG.debug(\"In-Progress file {} was already moved to final location {}.\", bucketState.currentFile, partPath);\n\t\t\t\t} else {\n\t\t\t\t\tLOG.debug(\"In-Progress file {} was neither moved to pending nor is still in progress. Possibly, \" +\n\t\t\t\t\t\t\t\"it was moved to final location by a previous snapshot restore\", bucketState.currentFile);\n\t\t\t\t}\n\n\t\t\t\tif (this.refTruncate == null) {\n\t\t\t\t\tthis.refTruncate = reflectTruncate(fs);\n\t\t\t\t}\n\n\t\t\t\t// truncate it or write a \".valid-length\" file to specify up to which point it is valid\n\t\t\t\tif (refTruncate != null) {\n\t\t\t\t\tLOG.debug(\"Truncating {} to valid length {}\", partPath, bucketState.currentFileValidLength);\n\t\t\t\t\t// some-one else might still hold the lease from a previous try, we are\n\t\t\t\t\t// recovering, after all ...\n\t\t\t\t\tif (fs instanceof DistributedFileSystem) {\n\t\t\t\t\t\tDistributedFileSystem dfs = (DistributedFileSystem) fs;\n\t\t\t\t\t\tLOG.debug(\"Trying to recover file lease {}\", partPath);\n\t\t\t\t\t\tdfs.recoverLease(partPath);\n\t\t\t\t\t\tboolean isclosed = dfs.isFileClosed(partPath);\n\t\t\t\t\t\tStopWatch sw = new StopWatch();\n\t\t\t\t\t\tsw.start();\n\t\t\t\t\t\twhile (!isclosed) {\n\t\t\t\t\t\t\tif (sw.getTime() > asyncTimeout) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tThread.sleep(500);\n\t\t\t\t\t\t\t} catch (InterruptedException e1) {\n\t\t\t\t\t\t\t\t// ignore it\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tisclosed = dfs.isFileClosed(partPath);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tBoolean truncated = (Boolean) refTruncate.invoke(fs, partPath, bucketState.currentFileValidLength);\n\t\t\t\t\tif (!truncated) {\n\t\t\t\t\t\tLOG.debug(\"Truncate did not immediately complete for {}, waiting...\", partPath);\n\n\t\t\t\t\t\t// we must wait for the asynchronous truncate operation to complete\n\t\t\t\t\t\tStopWatch sw = new StopWatch();\n\t\t\t\t\t\tsw.start();\n\t\t\t\t\t\tlong newLen = fs.getFileStatus(partPath).getLen();\n\t\t\t\t\t\twhile (newLen != bucketState.currentFileValidLength) {\n\t\t\t\t\t\t\tif (sw.getTime() > asyncTimeout) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tThread.sleep(500);\n\t\t\t\t\t\t\t} catch (InterruptedException e1) {\n\t\t\t\t\t\t\t\t// ignore it\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnewLen = fs.getFileStatus(partPath).getLen();\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (newLen != bucketState.currentFileValidLength) {\n\t\t\t\t\t\t\tthrow new RuntimeException(\"Truncate did not truncate to right length. Should be \" + bucketState.currentFileValidLength + \" is \" + newLen + \".\");\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t} else {\n\t\t\t\t\tLOG.debug(\"Writing valid-length file for {} to specify valid length {}\", partPath, bucketState.currentFileValidLength);\n\t\t\t\t\tPath validLengthFilePath = getValidLengthPathFor(partPath);\n\t\t\t\t\tif (!fs.exists(validLengthFilePath) && fs.exists(partPath)) {\n\t\t\t\t\t\tFSDataOutputStream lengthFileOut = fs.create(validLengthFilePath);\n\t\t\t\t\t\tlengthFileOut.writeUTF(Long.toString(bucketState.currentFileValidLength));\n\t\t\t\t\t\tlengthFileOut.close();\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// invalidate in the state object\n\t\t\t\tbucketState.currentFile = null;\n\t\t\t\tbucketState.currentFileValidLength = -1;\n\t\t\t\tisWriterOpen = false;\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.error(\"Error while restoring RollingSink state.\", e);\n\t\t\t\tthrow new RuntimeException(\"Error while restoring RollingSink state.\", e);\n\t\t\t} catch (InvocationTargetException | IllegalAccessException e) {\n\t\t\t\tLOG.error(\"Could not invoke truncate.\", e);\n\t\t\t\tthrow new RuntimeException(\"Could not invoke truncate.\", e);\n\t\t\t}\n\t\t}\n\n\t\t// Move files that are confirmed by a checkpoint but did not get moved to final location\n\t\t// because the checkpoint notification did not happen before a failure\n\n\t\tSet<Long> pastCheckpointIds = bucketState.pendingFilesPerCheckpoint.keySet();\n\t\tLOG.debug(\"Moving pending files to final location on restore.\");\n\t\tfor (Long pastCheckpointId : pastCheckpointIds) {\n\t\t\t// All the pending files are buckets that have been completed but are waiting to be renamed\n\t\t\t// to their final name\n\t\t\tfor (String filename : bucketState.pendingFilesPerCheckpoint.get(pastCheckpointId)) {\n\t\t\t\tPath finalPath = new Path(filename);\n\t\t\t\tPath pendingPath = getPendingPathFor(finalPath);\n\n\t\t\t\ttry {\n\t\t\t\t\tif (fs.exists(pendingPath)) {\n\t\t\t\t\t\tLOG.debug(\"(RESTORE) Moving pending file {} to final location after complete checkpoint {}.\", pendingPath, pastCheckpointId);\n\t\t\t\t\t\tfs.rename(pendingPath, finalPath);\n\t\t\t\t\t}\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tLOG.error(\"(RESTORE) Error while renaming pending file {} to final path {}: {}\", pendingPath, finalPath, e);\n\t\t\t\t\tthrow new RuntimeException(\"Error while renaming pending file \" + pendingPath + \" to final path \" + finalPath, e);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tsynchronized (bucketState.pendingFilesPerCheckpoint) {\n\t\t\tbucketState.pendingFilesPerCheckpoint.clear();\n\t\t}\n\t}"
        ],
        [
            "BucketingSinkTest::testBucketStateTransitions()",
            " 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 -\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "\t@Test\n\tpublic void testBucketStateTransitions() throws Exception {\n\t\tfinal File outDir = tempFolder.newFolder();\n\n\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0, 100);\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test1\", 1L));\n\t\ttestHarness.processElement(new StreamRecord<>(\"test2\", 1L));\n\t\tcheckFs(outDir, 2, 0 ,0, 0);\n\n\t\t// this is to check the inactivity threshold\n\t\ttestHarness.setProcessingTime(101L);\n\t\tcheckFs(outDir, 0, 2, 0, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test3\", 1L));\n\t\tcheckFs(outDir, 1, 2, 0, 0);\n\n\t\ttestHarness.snapshot(0, 0);\n\t\tcheckFs(outDir, 1, 2, 0, 0);\n\n\t\ttestHarness.notifyOfCompletedCheckpoint(0);\n\t\tcheckFs(outDir, 1, 0, 2, 0);\n\n\t\tOperatorStateHandles snapshot = testHarness.snapshot(1, 0);\n\n\t\ttestHarness.close();\n\t\tcheckFs(outDir, 0, 1, 2, 0);\n\n\t\ttestHarness = createRescalingTestSink(outDir, 1, 0, 100);\n\t\ttestHarness.setup();\n\t\ttestHarness.initializeState(snapshot);\n\t\ttestHarness.open();\n\t\tcheckFs(outDir, 0, 0, 3, 1);\n\n\t\tsnapshot = testHarness.snapshot(2, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test4\", 10));\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\n\t\ttestHarness = createRescalingTestSink(outDir, 1, 0, 100);\n\t\ttestHarness.setup();\n\t\ttestHarness.initializeState(snapshot);\n\t\ttestHarness.open();\n\n\t\t// the in-progress file remains as we do not clean up now\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\n\t\ttestHarness.close();\n\n\t\t// at close it is not moved to final because it is not part\n\t\t// of the current task's state, it was just a not cleaned up leftover.\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\t}",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  ",
            "\t@Test\n\tpublic void testBucketStateTransitions() throws Exception {\n\t\tfinal File outDir = tempFolder.newFolder();\n\n\t\tOneInputStreamOperatorTestHarness<String, Object> testHarness = createRescalingTestSink(outDir, 1, 0, 100);\n\t\ttestHarness.setup();\n\t\ttestHarness.open();\n\n\t\ttestHarness.setProcessingTime(0L);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test1\", 1L));\n\t\ttestHarness.processElement(new StreamRecord<>(\"test2\", 1L));\n\t\tcheckFs(outDir, 2, 0 , 0, 0);\n\n\t\t// this is to check the inactivity threshold\n\t\ttestHarness.setProcessingTime(101L);\n\t\tcheckFs(outDir, 0, 2, 0, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test3\", 1L));\n\t\tcheckFs(outDir, 1, 2, 0, 0);\n\n\t\ttestHarness.snapshot(0, 0);\n\t\tcheckFs(outDir, 1, 2, 0, 0);\n\n\t\ttestHarness.notifyOfCompletedCheckpoint(0);\n\t\tcheckFs(outDir, 1, 0, 2, 0);\n\n\t\tOperatorStateHandles snapshot = testHarness.snapshot(1, 0);\n\n\t\ttestHarness.close();\n\t\tcheckFs(outDir, 0, 1, 2, 0);\n\n\t\ttestHarness = createRescalingTestSink(outDir, 1, 0, 100);\n\t\ttestHarness.setup();\n\t\ttestHarness.initializeState(snapshot);\n\t\ttestHarness.open();\n\t\tcheckFs(outDir, 0, 0, 3, 1);\n\n\t\tsnapshot = testHarness.snapshot(2, 0);\n\n\t\ttestHarness.processElement(new StreamRecord<>(\"test4\", 10));\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\n\t\ttestHarness = createRescalingTestSink(outDir, 1, 0, 100);\n\t\ttestHarness.setup();\n\t\ttestHarness.initializeState(snapshot);\n\t\ttestHarness.open();\n\n\t\t// the in-progress file remains as we do not clean up now\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\n\t\ttestHarness.close();\n\n\t\t// at close it is not moved to final because it is not part\n\t\t// of the current task's state, it was just a not cleaned up leftover.\n\t\tcheckFs(outDir, 1, 0, 3, 1);\n\t}"
        ],
        [
            "BucketingSink::reflectTruncate(FileSystem)",
            " 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580 -\n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  ",
            "\t/**\n\t * Gets the truncate() call using reflection.\n\t * <p>\n\t * <b>NOTE:</b> This code comes from Flume.\n\t */\n\tprivate Method reflectTruncate(FileSystem fs) {\n\t\tMethod m = null;\n\t\tif(fs != null) {\n\t\t\tClass<?> fsClass = fs.getClass();\n\t\t\ttry {\n\t\t\t\tm = fsClass.getMethod(\"truncate\", Path.class, long.class);\n\t\t\t} catch (NoSuchMethodException ex) {\n\t\t\t\tLOG.debug(\"Truncate not found. Will write a file with suffix '{}' \" +\n\t\t\t\t\t\" and prefix '{}' to specify how many bytes in a bucket are valid.\", validLengthSuffix, validLengthPrefix);\n\t\t\t\treturn null;\n\t\t\t}\n\n\t\t\t// verify that truncate actually works\n\t\t\tFSDataOutputStream outputStream;\n\t\t\tPath testPath = new Path(UUID.randomUUID().toString());\n\t\t\ttry {\n\t\t\t\toutputStream = fs.create(testPath);\n\t\t\t\toutputStream.writeUTF(\"hello\");\n\t\t\t\toutputStream.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.error(\"Could not create file for checking if truncate works.\", e);\n\t\t\t\tthrow new RuntimeException(\"Could not create file for checking if truncate works.\", e);\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tm.invoke(fs, testPath, 2);\n\t\t\t} catch (IllegalAccessException | InvocationTargetException e) {\n\t\t\t\tLOG.debug(\"Truncate is not supported.\", e);\n\t\t\t\tm = null;\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tfs.delete(testPath, false);\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.error(\"Could not delete truncate test file.\", e);\n\t\t\t\tthrow new RuntimeException(\"Could not delete truncate test file.\", e);\n\t\t\t}\n\t\t}\n\t\treturn m;\n\t}",
            " 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581 +\n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  ",
            "\t/**\n\t * Gets the truncate() call using reflection.\n\t *\n\t * <p><b>NOTE:</b> This code comes from Flume.\n\t */\n\tprivate Method reflectTruncate(FileSystem fs) {\n\t\tMethod m = null;\n\t\tif (fs != null) {\n\t\t\tClass<?> fsClass = fs.getClass();\n\t\t\ttry {\n\t\t\t\tm = fsClass.getMethod(\"truncate\", Path.class, long.class);\n\t\t\t} catch (NoSuchMethodException ex) {\n\t\t\t\tLOG.debug(\"Truncate not found. Will write a file with suffix '{}' \" +\n\t\t\t\t\t\" and prefix '{}' to specify how many bytes in a bucket are valid.\", validLengthSuffix, validLengthPrefix);\n\t\t\t\treturn null;\n\t\t\t}\n\n\t\t\t// verify that truncate actually works\n\t\t\tFSDataOutputStream outputStream;\n\t\t\tPath testPath = new Path(UUID.randomUUID().toString());\n\t\t\ttry {\n\t\t\t\toutputStream = fs.create(testPath);\n\t\t\t\toutputStream.writeUTF(\"hello\");\n\t\t\t\toutputStream.close();\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.error(\"Could not create file for checking if truncate works.\", e);\n\t\t\t\tthrow new RuntimeException(\"Could not create file for checking if truncate works.\", e);\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tm.invoke(fs, testPath, 2);\n\t\t\t} catch (IllegalAccessException | InvocationTargetException e) {\n\t\t\t\tLOG.debug(\"Truncate is not supported.\", e);\n\t\t\t\tm = null;\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tfs.delete(testPath, false);\n\t\t\t} catch (IOException e) {\n\t\t\t\tLOG.error(\"Could not delete truncate test file.\", e);\n\t\t\t\tthrow new RuntimeException(\"Could not delete truncate test file.\", e);\n\t\t\t}\n\t\t}\n\t\treturn m;\n\t}"
        ],
        [
            "RollingSinkSecuredITCase::startSecureCluster()",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144 -\n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  ",
            "\t@BeforeClass\n\tpublic static void startSecureCluster() throws Exception {\n\n\t\tskipIfHadoopVersionIsNotAppropriate();\n\n\t\tLOG.info(\"starting secure cluster environment for testing\");\n\n\t\tdataDir = tempFolder.newFolder();\n\n\t\tconf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, dataDir.getAbsolutePath());\n\n\t\tSecureTestEnvironment.prepare(tempFolder);\n\n\t\tpopulateSecureConfigurations();\n\n\t\tConfiguration flinkConfig = new Configuration();\n\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB,\n\t\t\t\tSecureTestEnvironment.getTestKeytab());\n\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL,\n\t\t\t\tSecureTestEnvironment.getHadoopServicePrincipal());\n\n\t\tSecurityUtils.SecurityConfiguration ctx = new SecurityUtils.SecurityConfiguration(flinkConfig, conf);\n\t\ttry {\n\t\t\tTestingSecurityContext.install(ctx, SecureTestEnvironment.getClientSecurityConfigurationMap());\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Exception occurred while setting up secure test context. Reason: {}\", e);\n\t\t}\n\n\t\tFile hdfsSiteXML = new File(dataDir.getAbsolutePath() + \"/hdfs-site.xml\");\n\n\t\tFileWriter writer = new FileWriter(hdfsSiteXML);\n\t\tconf.writeXml(writer);\n\t\twriter.flush();\n\t\twriter.close();\n\n\t\tMap<String, String> map = new HashMap<String, String>(System.getenv());\n\t\tmap.put(\"HADOOP_CONF_DIR\", hdfsSiteXML.getParentFile().getAbsolutePath());\n\t\tTestBaseUtils.setEnv(map);\n\n\n\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);\n\t\tbuilder.checkDataNodeAddrConfig(true);\n\t\tbuilder.checkDataNodeHostConfig(true);\n\t\thdfsCluster = builder.build();\n\n\t\tdfs = hdfsCluster.getFileSystem();\n\n\t\thdfsURI = \"hdfs://\"\n\t\t\t\t+ NetUtils.hostAndPortToUrlString(hdfsCluster.getURI().getHost(), hdfsCluster.getNameNodePort())\n\t\t\t\t+ \"/\";\n\n\t\tstartSecureFlinkClusterWithRecoveryModeEnabled();\n\t}",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  ",
            "\t@BeforeClass\n\tpublic static void startSecureCluster() throws Exception {\n\n\t\tskipIfHadoopVersionIsNotAppropriate();\n\n\t\tLOG.info(\"starting secure cluster environment for testing\");\n\n\t\tdataDir = tempFolder.newFolder();\n\n\t\tconf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, dataDir.getAbsolutePath());\n\n\t\tSecureTestEnvironment.prepare(tempFolder);\n\n\t\tpopulateSecureConfigurations();\n\n\t\tConfiguration flinkConfig = new Configuration();\n\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB,\n\t\t\t\tSecureTestEnvironment.getTestKeytab());\n\t\tflinkConfig.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL,\n\t\t\t\tSecureTestEnvironment.getHadoopServicePrincipal());\n\n\t\tSecurityUtils.SecurityConfiguration ctx = new SecurityUtils.SecurityConfiguration(flinkConfig, conf);\n\t\ttry {\n\t\t\tTestingSecurityContext.install(ctx, SecureTestEnvironment.getClientSecurityConfigurationMap());\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Exception occurred while setting up secure test context. Reason: {}\", e);\n\t\t}\n\n\t\tFile hdfsSiteXML = new File(dataDir.getAbsolutePath() + \"/hdfs-site.xml\");\n\n\t\tFileWriter writer = new FileWriter(hdfsSiteXML);\n\t\tconf.writeXml(writer);\n\t\twriter.flush();\n\t\twriter.close();\n\n\t\tMap<String, String> map = new HashMap<String, String>(System.getenv());\n\t\tmap.put(\"HADOOP_CONF_DIR\", hdfsSiteXML.getParentFile().getAbsolutePath());\n\t\tTestBaseUtils.setEnv(map);\n\n\t\tMiniDFSCluster.Builder builder = new MiniDFSCluster.Builder(conf);\n\t\tbuilder.checkDataNodeAddrConfig(true);\n\t\tbuilder.checkDataNodeHostConfig(true);\n\t\thdfsCluster = builder.build();\n\n\t\tdfs = hdfsCluster.getFileSystem();\n\n\t\thdfsURI = \"hdfs://\"\n\t\t\t\t+ NetUtils.hostAndPortToUrlString(hdfsCluster.getURI().getHost(), hdfsCluster.getNameNodePort())\n\t\t\t\t+ \"/\";\n\n\t\tstartSecureFlinkClusterWithRecoveryModeEnabled();\n\t}"
        ],
        [
            "RollingSinkITCase::testNonRollingAvroKeyValueWithCompressionWriter()",
            " 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404 -\n 405 -\n 406  \n 407  \n 408 -\n 409  \n 410 -\n 411  \n 412  \n 413  \n 414 -\n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439 -\n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453 -\n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  ",
            "\t/**\n\t * This tests {@link AvroKeyValueSinkWriter}\n\t * with non-rolling output and with compression.\n\t */\n\t@Test\n\tpublic void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {\n\t\tfinal int NUM_ELEMENTS = 20;\n\t\tfinal int PARALLELISM = 2;\n\t\tfinal String outPath = hdfsURI + \"/avro-kv-no-comp-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(PARALLELISM);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\n\t\tMap<String, String> properties = new HashMap<>();\n\t\tSchema keySchema = Schema.create(Type.INT);\n\t\tSchema valueSchema = Schema.create(Type.STRING);\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true));\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC);\n\t\tRollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)\n\t\t\t\t.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tsource.addSink(sink);\n\n\t\tenv.execute(\"RollingSink Avro KeyValue Writer Test\");\n\n\t\tGenericData.setStringType(valueSchema, StringType.String);\n\t\tSchema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema);\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\t\tSpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema);\n\t\tDataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader);\n\t\tfor (int i = 0; i < NUM_ELEMENTS; i += 2) {\n\t\t\tAvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey().intValue();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\t\tdataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader);\n\n\t\tfor (int i = 1; i < NUM_ELEMENTS; i += 2) {\n\t\t\tAvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey().intValue();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\t}",
            " 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395 +\n 396  \n 397  \n 398 +\n 399  \n 400 +\n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428 +\n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442 +\n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  ",
            "\t/**\n\t * This tests {@link AvroKeyValueSinkWriter}\n\t * with non-rolling output and with compression.\n\t */\n\t@Test\n\tpublic void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {\n\t\tfinal int numElements = 20;\n\t\tfinal String outPath = hdfsURI + \"/avro-kv-no-comp-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(2);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\t\tMap<String, String> properties = new HashMap<>();\n\t\tSchema keySchema = Schema.create(Type.INT);\n\t\tSchema valueSchema = Schema.create(Type.STRING);\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true));\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC);\n\t\tRollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)\n\t\t\t\t.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tsource.addSink(sink);\n\n\t\tenv.execute(\"RollingSink Avro KeyValue Writer Test\");\n\n\t\tGenericData.setStringType(valueSchema, StringType.String);\n\t\tSchema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema);\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\t\tSpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema);\n\t\tDataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader);\n\t\tfor (int i = 0; i < numElements; i += 2) {\n\t\t\tAvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey().intValue();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\t\tdataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader);\n\n\t\tfor (int i = 1; i < numElements; i += 2) {\n\t\t\tAvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey().intValue();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\t}"
        ],
        [
            "StreamWriterBase::reflectHflushOrSync(FSDataOutputStream)",
            "  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  ",
            "\t/**\n\t * Gets the hflush call using reflection. Fallback to sync if hflush is not available.\n\t *\n\t * <p>\n\t * Note: This code comes from Flume\n\t */\n\tprivate Method reflectHflushOrSync(FSDataOutputStream os) {\n\t\tMethod m = null;\n\t\tif(os != null) {\n\t\t\tClass<?> fsDataOutputStreamClass = os.getClass();\n\t\t\ttry {\n\t\t\t\tm = fsDataOutputStreamClass.getMethod(\"hflush\");\n\t\t\t} catch (NoSuchMethodException ex) {\n\t\t\t\tLOG.debug(\"HFlush not found. Will use sync() instead\");\n\t\t\t\ttry {\n\t\t\t\t\tm = fsDataOutputStreamClass.getMethod(\"sync\");\n\t\t\t\t} catch (Exception ex1) {\n\t\t\t\t\tString msg = \"Neither hflush not sync were found. That seems to be \" +\n\t\t\t\t\t\t\t\"a problem!\";\n\t\t\t\t\tLOG.error(msg);\n\t\t\t\t\tthrow new RuntimeException(msg, ex1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn m;\n\t}",
            "  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  ",
            "\t/**\n\t * Gets the hflush call using reflection. Fallback to sync if hflush is not available.\n\t *\n\t * <p>Note: This code comes from Flume\n\t */\n\tprivate Method reflectHflushOrSync(FSDataOutputStream os) {\n\t\tMethod m = null;\n\t\tif (os != null) {\n\t\t\tClass<?> fsDataOutputStreamClass = os.getClass();\n\t\t\ttry {\n\t\t\t\tm = fsDataOutputStreamClass.getMethod(\"hflush\");\n\t\t\t} catch (NoSuchMethodException ex) {\n\t\t\t\tLOG.debug(\"HFlush not found. Will use sync() instead\");\n\t\t\t\ttry {\n\t\t\t\t\tm = fsDataOutputStreamClass.getMethod(\"sync\");\n\t\t\t\t} catch (Exception ex1) {\n\t\t\t\t\tString msg = \"Neither hflush not sync were found. That seems to be \" +\n\t\t\t\t\t\t\t\"a problem!\";\n\t\t\t\t\tLOG.error(msg);\n\t\t\t\t\tthrow new RuntimeException(msg, ex1);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn m;\n\t}"
        ],
        [
            "RollingSinkITCase::testNonRollingAvroKeyValueWithoutCompressionWriter()",
            " 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339 -\n 340 -\n 341  \n 342  \n 343 -\n 344  \n 345 -\n 346  \n 347  \n 348  \n 349 -\n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 -\n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386 -\n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  ",
            "\t/**\n\t * This tests {@link AvroKeyValueSinkWriter}\n\t * with non-rolling output and without compression.\n\t */\n\t@Test\n\tpublic void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {\n\t\tfinal int NUM_ELEMENTS = 20;\n\t\tfinal int PARALLELISM = 2;\n\t\tfinal String outPath = hdfsURI + \"/avro-kv-no-comp-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(PARALLELISM);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\n\t\tMap<String, String> properties = new HashMap<>();\n\t\tSchema keySchema = Schema.create(Type.INT);\n\t\tSchema valueSchema = Schema.create(Type.STRING);\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString());\n\t\tRollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)\n\t\t\t\t.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tsource.addSink(sink);\n\n\t\tenv.execute(\"RollingSink Avro KeyValue Writer Test\");\n\n\t\tGenericData.setStringType(valueSchema, StringType.String);\n\t\tSchema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema);\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\t\tSpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema);\n\t\tDataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader);\n\t\tfor (int i = 0; i < NUM_ELEMENTS; i += 2) {\n\t\t\tAvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey().intValue();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\t\tdataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader);\n\n\t\tfor (int i = 1; i < NUM_ELEMENTS; i += 2) {\n\t\t\tAvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey().intValue();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\t}",
            " 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332 +\n 333  \n 334  \n 335 +\n 336  \n 337 +\n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363 +\n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377 +\n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  ",
            "\t/**\n\t * This tests {@link AvroKeyValueSinkWriter}\n\t * with non-rolling output and without compression.\n\t */\n\t@Test\n\tpublic void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {\n\t\tfinal int numElements = 20;\n\t\tfinal String outPath = hdfsURI + \"/avro-kv-no-comp-non-rolling-out\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(2);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))\n\t\t\t\t.broadcast()\n\t\t\t\t.filter(new OddEvenFilter());\n\n\t\tMap<String, String> properties = new HashMap<>();\n\t\tSchema keySchema = Schema.create(Type.INT);\n\t\tSchema valueSchema = Schema.create(Type.STRING);\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString());\n\t\tproperties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString());\n\t\tRollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)\n\t\t\t\t.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))\n\t\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t\t.setPartPrefix(\"part\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(\"\");\n\n\t\tsource.addSink(sink);\n\n\t\tenv.execute(\"RollingSink Avro KeyValue Writer Test\");\n\n\t\tGenericData.setStringType(valueSchema, StringType.String);\n\t\tSchema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema);\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\t\tSpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema);\n\t\tDataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader);\n\t\tfor (int i = 0; i < numElements; i += 2) {\n\t\t\tAvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey().intValue();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\t\tdataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader);\n\n\t\tfor (int i = 1; i < numElements; i += 2) {\n\t\t\tAvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next());\n\t\t\tint key = wrappedEntry.getKey().intValue();\n\t\t\tAssert.assertEquals(i, key);\n\t\t\tString value = wrappedEntry.getValue();\n\t\t\tAssert.assertEquals(\"message #\" + i, value);\n\t\t}\n\n\t\tdataFileStream.close();\n\t\tinStream.close();\n\t}"
        ],
        [
            "BucketingSinkFaultToleranceITCase::testProgram(StreamExecutionEnvironment)",
            " 101  \n 102  \n 103  \n 104  \n 105 -\n 106 -\n 107  \n 108 -\n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  ",
            "\t@Override\n\tpublic void testProgram(StreamExecutionEnvironment env) {\n\t\tassertTrue(\"Broken test setup\", NUM_STRINGS % 40 == 0);\n\n\t\tint PARALLELISM = 12;\n\n\t\tenv.enableCheckpointing(20);\n\t\tenv.setParallelism(PARALLELISM);\n\t\tenv.disableOperatorChaining();\n\n\t\tDataStream<String> stream = env.addSource(new StringGeneratingSourceFunction(NUM_STRINGS)).startNewChain();\n\n\t\tDataStream<String> mapped = stream\n\t\t\t\t.map(new OnceFailingIdentityMapper(NUM_STRINGS));\n\n\t\tBucketingSink<String> sink = new BucketingSink<String>(outPath)\n\t\t\t\t.setBucketer(new BasePathBucketer<String>())\n\t\t\t\t.setBatchSize(10000)\n\t\t\t\t.setValidLengthPrefix(\"\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(PENDING_SUFFIX)\n\t\t\t\t.setInProgressSuffix(IN_PROGRESS_SUFFIX);\n\n\t\tmapped.addSink(sink);\n\n\t}",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108 +\n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  ",
            "\t@Override\n\tpublic void testProgram(StreamExecutionEnvironment env) {\n\t\tassertTrue(\"Broken test setup\", NUM_STRINGS % 40 == 0);\n\n\t\tenv.enableCheckpointing(20);\n\t\tenv.setParallelism(12);\n\t\tenv.disableOperatorChaining();\n\n\t\tDataStream<String> stream = env.addSource(new StringGeneratingSourceFunction(NUM_STRINGS)).startNewChain();\n\n\t\tDataStream<String> mapped = stream\n\t\t\t\t.map(new OnceFailingIdentityMapper(NUM_STRINGS));\n\n\t\tBucketingSink<String> sink = new BucketingSink<String>(outPath)\n\t\t\t\t.setBucketer(new BasePathBucketer<String>())\n\t\t\t\t.setBatchSize(10000)\n\t\t\t\t.setValidLengthPrefix(\"\")\n\t\t\t\t.setPendingPrefix(\"\")\n\t\t\t\t.setPendingSuffix(PENDING_SUFFIX)\n\t\t\t\t.setInProgressSuffix(IN_PROGRESS_SUFFIX);\n\n\t\tmapped.addSink(sink);\n\n\t}"
        ],
        [
            "BucketingSinkTest::createTestSink(File,int,int)",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  ",
            "\tprivate OneInputStreamOperatorTestHarness<String, Object> createTestSink(File dataDir, int totalParallelism, int taskIdx) throws Exception {\n\t\tBucketingSink<String> sink = new BucketingSink<String>(dataDir.getAbsolutePath())\n\t\t\t.setBucketer(new Bucketer<String>() {\n\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t@Override\n\t\t\t\tpublic Path getBucketPath(Clock clock, Path basePath, String element) {\n\t\t\t\t\treturn new Path(basePath, element);\n\t\t\t\t}\n\t\t\t})\n\t\t\t.setWriter(new StringWriter<String>())\n\t\t\t.setPartPrefix(PART_PREFIX)\n\t\t\t.setPendingPrefix(\"\")\n\t\t\t.setInactiveBucketCheckInterval(5*60*1000L)\n\t\t\t.setInactiveBucketThreshold(5*60*1000L)\n\t\t\t.setPendingSuffix(PENDING_SUFFIX)\n\t\t\t.setInProgressSuffix(IN_PROGRESS_SUFFIX);\n\n\t\treturn createTestSink(sink, totalParallelism, taskIdx);\n\t}",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123 +\n 124 +\n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\tprivate OneInputStreamOperatorTestHarness<String, Object> createTestSink(File dataDir, int totalParallelism, int taskIdx) throws Exception {\n\t\tBucketingSink<String> sink = new BucketingSink<String>(dataDir.getAbsolutePath())\n\t\t\t.setBucketer(new Bucketer<String>() {\n\t\t\t\tprivate static final long serialVersionUID = 1L;\n\n\t\t\t\t@Override\n\t\t\t\tpublic Path getBucketPath(Clock clock, Path basePath, String element) {\n\t\t\t\t\treturn new Path(basePath, element);\n\t\t\t\t}\n\t\t\t})\n\t\t\t.setWriter(new StringWriter<String>())\n\t\t\t.setPartPrefix(PART_PREFIX)\n\t\t\t.setPendingPrefix(\"\")\n\t\t\t.setInactiveBucketCheckInterval(5 * 60 * 1000L)\n\t\t\t.setInactiveBucketThreshold(5 * 60 * 1000L)\n\t\t\t.setPendingSuffix(PENDING_SUFFIX)\n\t\t\t.setInProgressSuffix(IN_PROGRESS_SUFFIX);\n\n\t\treturn createTestSink(sink, totalParallelism, taskIdx);\n\t}"
        ],
        [
            "RollingSinkITCase::testUserDefinedConfiguration()",
            " 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472 -\n 473 -\n 474  \n 475  \n 476 -\n 477  \n 478 -\n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493 -\n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508 -\n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519 -\n 520  \n 521  \n 522  \n 523  \n 524  \n 525  ",
            "\t/**\n\t * This tests user defined hdfs configuration\n\t * @throws Exception\n     */\n\t@Test\n\tpublic void testUserDefinedConfiguration() throws Exception {\n\t\tfinal int NUM_ELEMENTS = 20;\n\t\tfinal int PARALLELISM = 2;\n\t\tfinal String outPath = hdfsURI + \"/string-non-rolling-with-config\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(PARALLELISM);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))\n\t\t\t.broadcast()\n\t\t\t.filter(new OddEvenFilter());\n\n\t\tConfiguration conf = new Configuration();\n\t\tconf.set(\"io.file.buffer.size\", \"40960\");\n\t\tRollingSink<String> sink = new RollingSink<String>(outPath)\n\t\t\t.setFSConfig(conf)\n\t\t\t.setWriter(new StreamWriterWithConfigCheck<String>(\"io.file.buffer.size\", \"40960\"))\n\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t.setPartPrefix(\"part\")\n\t\t\t.setPendingPrefix(\"\")\n\t\t\t.setPendingSuffix(\"\");\n\n\t\tsource\n\t\t\t.map(new MapFunction<Tuple2<Integer,String>, String>() {\n\t\t\t\tprivate static final long serialVersionUID = 1L;\n\t\t\t\t@Override\n\t\t\t\tpublic String map(Tuple2<Integer, String> value) throws Exception {\n\t\t\t\t\treturn value.f1;\n\t\t\t\t}\n\t\t\t})\n\t\t\t.addSink(sink);\n\n\t\tenv.execute(\"RollingSink with configuration Test\");\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\n\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\tfor (int i = 0; i < NUM_ELEMENTS; i += 2) {\n\t\t\tString line = br.readLine();\n\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t}\n\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\n\t\tbr = new BufferedReader(new InputStreamReader(inStream));\n\n\t\tfor (int i = 1; i < NUM_ELEMENTS; i += 2) {\n\t\t\tString line = br.readLine();\n\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t}\n\n\t\tinStream.close();\n\t}",
            " 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460 +\n 461  \n 462  \n 463 +\n 464  \n 465 +\n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480 +\n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495 +\n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506 +\n 507  \n 508  \n 509  \n 510  \n 511  \n 512  ",
            "\t/**\n\t * This tests user defined hdfs configuration.\n\t * @throws Exception\n     */\n\t@Test\n\tpublic void testUserDefinedConfiguration() throws Exception {\n\t\tfinal int numElements = 20;\n\t\tfinal String outPath = hdfsURI + \"/string-non-rolling-with-config\";\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(2);\n\n\t\tDataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))\n\t\t\t.broadcast()\n\t\t\t.filter(new OddEvenFilter());\n\n\t\tConfiguration conf = new Configuration();\n\t\tconf.set(\"io.file.buffer.size\", \"40960\");\n\t\tRollingSink<String> sink = new RollingSink<String>(outPath)\n\t\t\t.setFSConfig(conf)\n\t\t\t.setWriter(new StreamWriterWithConfigCheck<String>(\"io.file.buffer.size\", \"40960\"))\n\t\t\t.setBucketer(new NonRollingBucketer())\n\t\t\t.setPartPrefix(\"part\")\n\t\t\t.setPendingPrefix(\"\")\n\t\t\t.setPendingSuffix(\"\");\n\n\t\tsource\n\t\t\t.map(new MapFunction<Tuple2<Integer, String>, String>() {\n\t\t\t\tprivate static final long serialVersionUID = 1L;\n\t\t\t\t@Override\n\t\t\t\tpublic String map(Tuple2<Integer, String> value) throws Exception {\n\t\t\t\t\treturn value.f1;\n\t\t\t\t}\n\t\t\t})\n\t\t\t.addSink(sink);\n\n\t\tenv.execute(\"RollingSink with configuration Test\");\n\n\t\tFSDataInputStream inStream = dfs.open(new Path(outPath + \"/part-0-0\"));\n\n\t\tBufferedReader br = new BufferedReader(new InputStreamReader(inStream));\n\n\t\tfor (int i = 0; i < numElements; i += 2) {\n\t\t\tString line = br.readLine();\n\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t}\n\n\t\tinStream.close();\n\n\t\tinStream = dfs.open(new Path(outPath + \"/part-1-0\"));\n\n\t\tbr = new BufferedReader(new InputStreamReader(inStream));\n\n\t\tfor (int i = 1; i < numElements; i += 2) {\n\t\t\tString line = br.readLine();\n\t\t\tAssert.assertEquals(\"message #\" + i, line);\n\t\t}\n\n\t\tinStream.close();\n\t}"
        ]
    ],
    "f2af1a9f916bd9b941a48a1da577d19fc07badde": [
        [
            "SerializationProxiesTest::testKeyedBackendSerializationProxyRoundtrip()",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70 -\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 -\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  ",
            "\t@Test\n\tpublic void testKeyedBackendSerializationProxyRoundtrip() throws Exception {\n\n\t\tTypeSerializer<?> keySerializer = IntSerializer.INSTANCE;\n\t\tTypeSerializer<?> namespaceSerializer = LongSerializer.INSTANCE;\n\t\tTypeSerializer<?> stateSerializer = DoubleSerializer.INSTANCE;\n\n\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> stateMetaInfoList = new ArrayList<>();\n\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"a\", namespaceSerializer, stateSerializer).snapshot());\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"b\", namespaceSerializer, stateSerializer).snapshot());\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"c\", namespaceSerializer, stateSerializer).snapshot());\n\n\t\tKeyedBackendSerializationProxy serializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy(keySerializer, stateMetaInfoList);\n\n\t\tbyte[] serialized;\n\t\ttry (ByteArrayOutputStreamWithPos out = new ByteArrayOutputStreamWithPos()) {\n\t\t\tserializationProxy.write(new DataOutputViewStreamWrapper(out));\n\t\t\tserialized = out.toByteArray();\n\t\t}\n\n\t\tserializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy(Thread.currentThread().getContextClassLoader());\n\n\t\ttry (ByteArrayInputStreamWithPos in = new ByteArrayInputStreamWithPos(serialized)) {\n\t\t\tserializationProxy.read(new DataInputViewStreamWrapper(in));\n\t\t}\n\n\t\tAssert.assertEquals(keySerializer, serializationProxy.getKeySerializer());\n\t\tAssert.assertEquals(keySerializer.snapshotConfiguration(), serializationProxy.getKeySerializerConfigSnapshot());\n\t\tAssert.assertEquals(stateMetaInfoList, serializationProxy.getStateMetaInfoSnapshots());\n\t}",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 +\n  70 +\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79 +\n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  ",
            "\t@Test\n\tpublic void testKeyedBackendSerializationProxyRoundtrip() throws Exception {\n\n\t\tTypeSerializer<?> keySerializer = IntSerializer.INSTANCE;\n\t\tTypeSerializer<?> namespaceSerializer = LongSerializer.INSTANCE;\n\t\tTypeSerializer<?> stateSerializer = DoubleSerializer.INSTANCE;\n\n\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> stateMetaInfoList = new ArrayList<>();\n\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"a\", namespaceSerializer, stateSerializer).snapshot());\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"b\", namespaceSerializer, stateSerializer).snapshot());\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"c\", namespaceSerializer, stateSerializer).snapshot());\n\n\t\tKeyedBackendSerializationProxy<?> serializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy<>(keySerializer, stateMetaInfoList);\n\n\t\tbyte[] serialized;\n\t\ttry (ByteArrayOutputStreamWithPos out = new ByteArrayOutputStreamWithPos()) {\n\t\t\tserializationProxy.write(new DataOutputViewStreamWrapper(out));\n\t\t\tserialized = out.toByteArray();\n\t\t}\n\n\t\tserializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy<>(Thread.currentThread().getContextClassLoader());\n\n\t\ttry (ByteArrayInputStreamWithPos in = new ByteArrayInputStreamWithPos(serialized)) {\n\t\t\tserializationProxy.read(new DataInputViewStreamWrapper(in));\n\t\t}\n\n\t\tAssert.assertEquals(keySerializer, serializationProxy.getKeySerializer());\n\t\tAssert.assertEquals(keySerializer.snapshotConfiguration(), serializationProxy.getKeySerializerConfigSnapshot());\n\t\tAssert.assertEquals(stateMetaInfoList, serializationProxy.getStateMetaInfoSnapshots());\n\t}"
        ],
        [
            "KeyedBackendSerializationProxy::getKeySerializer()",
            "  73 -\n  74  \n  75  ",
            "\tpublic TypeSerializer<?> getKeySerializer() {\n\t\treturn keySerializer;\n\t}",
            "  73 +\n  74  \n  75  ",
            "\tpublic TypeSerializer<K> getKeySerializer() {\n\t\treturn keySerializer;\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::snapshotIncrementally(long,long,CheckpointStreamFactory)",
            " 312  \n 313  \n 314  \n 315  \n 316  \n 317 -\n 318 -\n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  ",
            "\tprivate RunnableFuture<KeyedStateHandle> snapshotIncrementally(\n\t\t\tfinal long checkpointId,\n\t\t\tfinal long checkpointTimestamp,\n\t\t\tfinal CheckpointStreamFactory checkpointStreamFactory) throws Exception {\n\n\t\tfinal RocksDBIncrementalSnapshotOperation snapshotOperation =\n\t\t\tnew RocksDBIncrementalSnapshotOperation(\n\t\t\t\tthis,\n\t\t\t\tcheckpointStreamFactory,\n\t\t\t\tcheckpointId,\n\t\t\t\tcheckpointTimestamp);\n\n\t\tsynchronized (asyncSnapshotLock) {\n\t\t\tif (db == null) {\n\t\t\t\tthrow new IOException(\"RocksDB closed.\");\n\t\t\t}\n\n\t\t\tif (!hasRegisteredState()) {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"Asynchronous RocksDB snapshot performed on empty keyed state at \" +\n\t\t\t\t\t\t\tcheckpointTimestamp + \" . Returning null.\");\n\t\t\t\t}\n\t\t\t\treturn DoneFuture.nullValue();\n\t\t\t}\n\n\t\t\tsnapshotOperation.takeSnapshot();\n\t\t}\n\n\t\treturn new FutureTask<KeyedStateHandle>(\n\t\t\tnew Callable<KeyedStateHandle>() {\n\t\t\t\t@Override\n\t\t\t\tpublic KeyedStateHandle call() throws Exception {\n\t\t\t\t\treturn snapshotOperation.materializeSnapshot();\n\t\t\t\t}\n\t\t\t}\n\t\t) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean mayInterruptIfRunning) {\n\t\t\t\tsnapshotOperation.stop();\n\t\t\t\treturn super.cancel(mayInterruptIfRunning);\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tprotected void done() {\n\t\t\t\tsnapshotOperation.releaseResources(isCancelled());\n\t\t\t}\n\t\t};\n\t}",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314 +\n 315 +\n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  ",
            "\tprivate RunnableFuture<KeyedStateHandle> snapshotIncrementally(\n\t\t\tfinal long checkpointId,\n\t\t\tfinal long checkpointTimestamp,\n\t\t\tfinal CheckpointStreamFactory checkpointStreamFactory) throws Exception {\n\n\t\tfinal RocksDBIncrementalSnapshotOperation<K> snapshotOperation =\n\t\t\tnew RocksDBIncrementalSnapshotOperation<>(\n\t\t\t\tthis,\n\t\t\t\tcheckpointStreamFactory,\n\t\t\t\tcheckpointId,\n\t\t\t\tcheckpointTimestamp);\n\n\t\tsynchronized (asyncSnapshotLock) {\n\t\t\tif (db == null) {\n\t\t\t\tthrow new IOException(\"RocksDB closed.\");\n\t\t\t}\n\n\t\t\tif (!hasRegisteredState()) {\n\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\tLOG.debug(\"Asynchronous RocksDB snapshot performed on empty keyed state at \" +\n\t\t\t\t\t\t\tcheckpointTimestamp + \" . Returning null.\");\n\t\t\t\t}\n\t\t\t\treturn DoneFuture.nullValue();\n\t\t\t}\n\n\t\t\tsnapshotOperation.takeSnapshot();\n\t\t}\n\n\t\treturn new FutureTask<KeyedStateHandle>(\n\t\t\tnew Callable<KeyedStateHandle>() {\n\t\t\t\t@Override\n\t\t\t\tpublic KeyedStateHandle call() throws Exception {\n\t\t\t\t\treturn snapshotOperation.materializeSnapshot();\n\t\t\t\t}\n\t\t\t}\n\t\t) {\n\t\t\t@Override\n\t\t\tpublic boolean cancel(boolean mayInterruptIfRunning) {\n\t\t\t\tsnapshotOperation.stop();\n\t\t\t\treturn super.cancel(mayInterruptIfRunning);\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tprotected void done() {\n\t\t\t\tsnapshotOperation.releaseResources(isCancelled());\n\t\t\t}\n\t\t};\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBKeyedStateBackend(JobID,String,ClassLoader,File,DBOptions,ColumnFamilyOptions,TaskKvStateRegistry,TypeSerializer,int,KeyGroupRange,ExecutionConfig,boolean)",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 -\n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  ",
            "\tpublic RocksDBKeyedStateBackend(\n\t\t\tJobID jobId,\n\t\t\tString operatorIdentifier,\n\t\t\tClassLoader userCodeClassLoader,\n\t\t\tFile instanceBasePath,\n\t\t\tDBOptions dbOptions,\n\t\t\tColumnFamilyOptions columnFamilyOptions,\n\t\t\tTaskKvStateRegistry kvStateRegistry,\n\t\t\tTypeSerializer<K> keySerializer,\n\t\t\tint numberOfKeyGroups,\n\t\t\tKeyGroupRange keyGroupRange,\n\t\t\tExecutionConfig executionConfig,\n\t\t\tboolean enableIncrementalCheckpointing\n\t) throws IOException {\n\n\t\tsuper(kvStateRegistry, keySerializer, userCodeClassLoader, numberOfKeyGroups, keyGroupRange, executionConfig);\n\n\t\tthis.jobId = Preconditions.checkNotNull(jobId);\n\t\tthis.operatorIdentifier = Preconditions.checkNotNull(operatorIdentifier);\n\n\t\tthis.enableIncrementalCheckpointing = enableIncrementalCheckpointing;\n\n\t\tthis.columnOptions = Preconditions.checkNotNull(columnFamilyOptions);\n\t\tthis.dbOptions = Preconditions.checkNotNull(dbOptions);\n\n\t\tthis.instanceBasePath = Preconditions.checkNotNull(instanceBasePath);\n\t\tthis.instanceRocksDBPath = new File(instanceBasePath, \"db\");\n\n\t\tif (!instanceBasePath.exists()) {\n\t\t\tif (!instanceBasePath.mkdirs()) {\n\t\t\t\tthrow new IOException(\"Could not create RocksDB data directory.\");\n\t\t\t}\n\t\t}\n\n\t\t// clean it, this will remove the last part of the path but RocksDB will recreate it\n\t\ttry {\n\t\t\tif (instanceRocksDBPath.exists()) {\n\t\t\t\tLOG.warn(\"Deleting already existing db directory {}.\", instanceRocksDBPath);\n\t\t\t\tFileUtils.deleteDirectory(instanceRocksDBPath);\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error cleaning RocksDB data directory.\", e);\n\t\t}\n\n\t\tkeyGroupPrefixBytes = getNumberOfKeyGroups() > (Byte.MAX_VALUE + 1) ? 2 : 1;\n\t\tkvStateInformation = new HashMap<>();\n\t}",
            " 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "\tpublic RocksDBKeyedStateBackend(\n\t\t\tJobID jobId,\n\t\t\tString operatorIdentifier,\n\t\t\tClassLoader userCodeClassLoader,\n\t\t\tFile instanceBasePath,\n\t\t\tDBOptions dbOptions,\n\t\t\tColumnFamilyOptions columnFamilyOptions,\n\t\t\tTaskKvStateRegistry kvStateRegistry,\n\t\t\tTypeSerializer<K> keySerializer,\n\t\t\tint numberOfKeyGroups,\n\t\t\tKeyGroupRange keyGroupRange,\n\t\t\tExecutionConfig executionConfig,\n\t\t\tboolean enableIncrementalCheckpointing\n\t) throws IOException {\n\n\t\tsuper(kvStateRegistry, keySerializer, userCodeClassLoader, numberOfKeyGroups, keyGroupRange, executionConfig);\n\n\t\tthis.operatorIdentifier = Preconditions.checkNotNull(operatorIdentifier);\n\n\t\tthis.enableIncrementalCheckpointing = enableIncrementalCheckpointing;\n\n\t\tthis.columnOptions = Preconditions.checkNotNull(columnFamilyOptions);\n\t\tthis.dbOptions = Preconditions.checkNotNull(dbOptions);\n\n\t\tthis.instanceBasePath = Preconditions.checkNotNull(instanceBasePath);\n\t\tthis.instanceRocksDBPath = new File(instanceBasePath, \"db\");\n\n\t\tif (!instanceBasePath.exists()) {\n\t\t\tif (!instanceBasePath.mkdirs()) {\n\t\t\t\tthrow new IOException(\"Could not create RocksDB data directory.\");\n\t\t\t}\n\t\t}\n\n\t\t// clean it, this will remove the last part of the path but RocksDB will recreate it\n\t\ttry {\n\t\t\tif (instanceRocksDBPath.exists()) {\n\t\t\t\tLOG.warn(\"Deleting already existing db directory {}.\", instanceRocksDBPath);\n\t\t\t\tFileUtils.deleteDirectory(instanceRocksDBPath);\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error cleaning RocksDB data directory.\", e);\n\t\t}\n\n\t\tkeyGroupPrefixBytes = getNumberOfKeyGroups() > (Byte.MAX_VALUE + 1) ? 2 : 1;\n\t\tkvStateInformation = new HashMap<>();\n\t}"
        ],
        [
            "HeapKeyedStateBackend::restorePartitionedState(Collection)",
            " 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386 -\n 387 -\n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398 -\n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408 -\n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  ",
            "\t@SuppressWarnings({\"unchecked\"})\n\tprivate void restorePartitionedState(Collection<KeyedStateHandle> state) throws Exception {\n\n\t\tfinal Map<Integer, String> kvStatesById = new HashMap<>();\n\t\tint numRegisteredKvStates = 0;\n\t\tstateTables.clear();\n\n\t\tboolean keySerializerRestored = false;\n\n\t\tfor (KeyedStateHandle keyedStateHandle : state) {\n\n\t\t\tif (keyedStateHandle == null) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!(keyedStateHandle instanceof KeyGroupsStateHandle)) {\n\t\t\t\tthrow new IllegalStateException(\"Unexpected state handle type, \" +\n\t\t\t\t\t\t\"expected: \" + KeyGroupsStateHandle.class +\n\t\t\t\t\t\t\", but found: \" + keyedStateHandle.getClass());\n\t\t\t}\n\n\t\t\tKeyGroupsStateHandle keyGroupsStateHandle = (KeyGroupsStateHandle) keyedStateHandle;\n\t\t\tFSDataInputStream fsDataInputStream = keyGroupsStateHandle.openInputStream();\n\t\t\tcancelStreamRegistry.registerClosable(fsDataInputStream);\n\n\t\t\ttry {\n\t\t\t\tDataInputViewStreamWrapper inView = new DataInputViewStreamWrapper(fsDataInputStream);\n\n\t\t\t\tKeyedBackendSerializationProxy serializationProxy =\n\t\t\t\t\t\tnew KeyedBackendSerializationProxy(userCodeClassLoader);\n\n\t\t\t\tserializationProxy.read(inView);\n\n\t\t\t\tif (!keySerializerRestored) {\n\t\t\t\t\t// check for key serializer compatibility; this also reconfigures the\n\t\t\t\t\t// key serializer to be compatible, if it is required and is possible\n\t\t\t\t\tif (StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\t\tserializationProxy.getKeySerializer(),\n\t\t\t\t\t\tTypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer.class,\n\t\t\t\t\t\tserializationProxy.getKeySerializerConfigSnapshot(),\n\t\t\t\t\t\t(TypeSerializer) keySerializer)\n\t\t\t\t\t\t.isRequiresMigration()) {\n\n\t\t\t\t\t\t// TODO replace with state migration; note that key hash codes need to remain the same after migration\n\t\t\t\t\t\tthrow new RuntimeException(\"The new key serializer is not compatible to read previous keys. \" +\n\t\t\t\t\t\t\t\"Aborting now since state migration is currently not available\");\n\t\t\t\t\t}\n\n\t\t\t\t\tkeySerializerRestored = true;\n\t\t\t\t}\n\t\t\t\t\n\t\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> restoredMetaInfos =\n\t\t\t\t\t\tserializationProxy.getStateMetaInfoSnapshots();\n\n\t\t\t\tfor (RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?> restoredMetaInfo : restoredMetaInfos) {\n\n\t\t\t\t\tif (restoredMetaInfo.getStateSerializer() == null ||\n\t\t\t\t\t\t\trestoredMetaInfo.getStateSerializer()\n\t\t\t\t\t\t\t\tinstanceof TypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer) {\n\n\t\t\t\t\t\t// must fail now if the previous serializer cannot be restored because there is no serializer\n\t\t\t\t\t\t// capable of reading previous state\n\t\t\t\t\t\t// TODO when eager state registration is in place, we can try to get a convert deserializer\n\t\t\t\t\t\t// TODO from the newly registered serializer instead of simply failing here\n\n\t\t\t\t\t\tthrow new IOException(\"Unable to restore keyed state [\" + restoredMetaInfo.getName() + \"].\" +\n\t\t\t\t\t\t\t\" For memory-backed keyed state, the previous serializer of the keyed state must be\" +\n\t\t\t\t\t\t\t\" present; the serializer could have been removed from the classpath, or its implementation\" +\n\t\t\t\t\t\t\t\" have changed and could not be loaded. This is a temporary restriction that will be fixed\" +\n\t\t\t\t\t\t\t\" in future versions.\");\n\t\t\t\t\t}\n\n\t\t\t\t\tStateTable<K, ?, ?> stateTable = stateTables.get(restoredMetaInfo.getName());\n\n\t\t\t\t\t//important: only create a new table we did not already create it previously\n\t\t\t\t\tif (null == stateTable) {\n\n\t\t\t\t\t\tRegisteredKeyedBackendStateMetaInfo<?, ?> registeredKeyedBackendStateMetaInfo =\n\t\t\t\t\t\t\t\tnew RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\t\t\t\t\t\t\trestoredMetaInfo.getStateType(),\n\t\t\t\t\t\t\t\t\trestoredMetaInfo.getName(),\n\t\t\t\t\t\t\t\t\trestoredMetaInfo.getNamespaceSerializer(),\n\t\t\t\t\t\t\t\t\trestoredMetaInfo.getStateSerializer());\n\n\t\t\t\t\t\tstateTable = newStateTable(registeredKeyedBackendStateMetaInfo);\n\t\t\t\t\t\tstateTables.put(restoredMetaInfo.getName(), stateTable);\n\t\t\t\t\t\tkvStatesById.put(numRegisteredKvStates, restoredMetaInfo.getName());\n\t\t\t\t\t\t++numRegisteredKvStates;\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tfor (Tuple2<Integer, Long> groupOffset : keyGroupsStateHandle.getGroupRangeOffsets()) {\n\t\t\t\t\tint keyGroupIndex = groupOffset.f0;\n\t\t\t\t\tlong offset = groupOffset.f1;\n\n\t\t\t\t\t// Check that restored key groups all belong to the backend.\n\t\t\t\t\tPreconditions.checkState(keyGroupRange.contains(keyGroupIndex), \"The key group must belong to the backend.\");\n\n\t\t\t\t\tfsDataInputStream.seek(offset);\n\n\t\t\t\t\tint writtenKeyGroupIndex = inView.readInt();\n\n\t\t\t\t\tPreconditions.checkState(writtenKeyGroupIndex == keyGroupIndex,\n\t\t\t\t\t\t\t\"Unexpected key-group in restore.\");\n\n\t\t\t\t\tfor (int i = 0; i < restoredMetaInfos.size(); i++) {\n\t\t\t\t\t\tint kvStateId = inView.readShort();\n\t\t\t\t\t\tStateTable<K, ?, ?> stateTable = stateTables.get(kvStatesById.get(kvStateId));\n\n\t\t\t\t\t\tStateTableByKeyGroupReader keyGroupReader =\n\t\t\t\t\t\t\t\tStateTableByKeyGroupReaders.readerForVersion(\n\t\t\t\t\t\t\t\t\t\tstateTable,\n\t\t\t\t\t\t\t\t\t\tserializationProxy.getReadVersion());\n\n\t\t\t\t\t\tkeyGroupReader.readMappingsInKeyGroup(inView, keyGroupIndex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} finally {\n\t\t\t\tcancelStreamRegistry.unregisterClosable(fsDataInputStream);\n\t\t\t\tIOUtils.closeQuietly(fsDataInputStream);\n\t\t\t}\n\t\t}\n\t}",
            " 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386 +\n 387 +\n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398 +\n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408 +\n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  ",
            "\t@SuppressWarnings({\"unchecked\"})\n\tprivate void restorePartitionedState(Collection<KeyedStateHandle> state) throws Exception {\n\n\t\tfinal Map<Integer, String> kvStatesById = new HashMap<>();\n\t\tint numRegisteredKvStates = 0;\n\t\tstateTables.clear();\n\n\t\tboolean keySerializerRestored = false;\n\n\t\tfor (KeyedStateHandle keyedStateHandle : state) {\n\n\t\t\tif (keyedStateHandle == null) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (!(keyedStateHandle instanceof KeyGroupsStateHandle)) {\n\t\t\t\tthrow new IllegalStateException(\"Unexpected state handle type, \" +\n\t\t\t\t\t\t\"expected: \" + KeyGroupsStateHandle.class +\n\t\t\t\t\t\t\", but found: \" + keyedStateHandle.getClass());\n\t\t\t}\n\n\t\t\tKeyGroupsStateHandle keyGroupsStateHandle = (KeyGroupsStateHandle) keyedStateHandle;\n\t\t\tFSDataInputStream fsDataInputStream = keyGroupsStateHandle.openInputStream();\n\t\t\tcancelStreamRegistry.registerClosable(fsDataInputStream);\n\n\t\t\ttry {\n\t\t\t\tDataInputViewStreamWrapper inView = new DataInputViewStreamWrapper(fsDataInputStream);\n\n\t\t\t\tKeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\t\t\tnew KeyedBackendSerializationProxy<>(userCodeClassLoader);\n\n\t\t\t\tserializationProxy.read(inView);\n\n\t\t\t\tif (!keySerializerRestored) {\n\t\t\t\t\t// check for key serializer compatibility; this also reconfigures the\n\t\t\t\t\t// key serializer to be compatible, if it is required and is possible\n\t\t\t\t\tif (StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\t\tserializationProxy.getKeySerializer(),\n\t\t\t\t\t\tTypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer.class,\n\t\t\t\t\t\tserializationProxy.getKeySerializerConfigSnapshot(),\n\t\t\t\t\t\tkeySerializer)\n\t\t\t\t\t\t.isRequiresMigration()) {\n\n\t\t\t\t\t\t// TODO replace with state migration; note that key hash codes need to remain the same after migration\n\t\t\t\t\t\tthrow new RuntimeException(\"The new key serializer is not compatible to read previous keys. \" +\n\t\t\t\t\t\t\t\"Aborting now since state migration is currently not available\");\n\t\t\t\t\t}\n\n\t\t\t\t\tkeySerializerRestored = true;\n\t\t\t\t}\n\n\t\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> restoredMetaInfos =\n\t\t\t\t\t\tserializationProxy.getStateMetaInfoSnapshots();\n\n\t\t\t\tfor (RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?> restoredMetaInfo : restoredMetaInfos) {\n\n\t\t\t\t\tif (restoredMetaInfo.getStateSerializer() == null ||\n\t\t\t\t\t\t\trestoredMetaInfo.getStateSerializer()\n\t\t\t\t\t\t\t\tinstanceof TypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer) {\n\n\t\t\t\t\t\t// must fail now if the previous serializer cannot be restored because there is no serializer\n\t\t\t\t\t\t// capable of reading previous state\n\t\t\t\t\t\t// TODO when eager state registration is in place, we can try to get a convert deserializer\n\t\t\t\t\t\t// TODO from the newly registered serializer instead of simply failing here\n\n\t\t\t\t\t\tthrow new IOException(\"Unable to restore keyed state [\" + restoredMetaInfo.getName() + \"].\" +\n\t\t\t\t\t\t\t\" For memory-backed keyed state, the previous serializer of the keyed state must be\" +\n\t\t\t\t\t\t\t\" present; the serializer could have been removed from the classpath, or its implementation\" +\n\t\t\t\t\t\t\t\" have changed and could not be loaded. This is a temporary restriction that will be fixed\" +\n\t\t\t\t\t\t\t\" in future versions.\");\n\t\t\t\t\t}\n\n\t\t\t\t\tStateTable<K, ?, ?> stateTable = stateTables.get(restoredMetaInfo.getName());\n\n\t\t\t\t\t//important: only create a new table we did not already create it previously\n\t\t\t\t\tif (null == stateTable) {\n\n\t\t\t\t\t\tRegisteredKeyedBackendStateMetaInfo<?, ?> registeredKeyedBackendStateMetaInfo =\n\t\t\t\t\t\t\t\tnew RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\t\t\t\t\t\t\trestoredMetaInfo.getStateType(),\n\t\t\t\t\t\t\t\t\trestoredMetaInfo.getName(),\n\t\t\t\t\t\t\t\t\trestoredMetaInfo.getNamespaceSerializer(),\n\t\t\t\t\t\t\t\t\trestoredMetaInfo.getStateSerializer());\n\n\t\t\t\t\t\tstateTable = newStateTable(registeredKeyedBackendStateMetaInfo);\n\t\t\t\t\t\tstateTables.put(restoredMetaInfo.getName(), stateTable);\n\t\t\t\t\t\tkvStatesById.put(numRegisteredKvStates, restoredMetaInfo.getName());\n\t\t\t\t\t\t++numRegisteredKvStates;\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tfor (Tuple2<Integer, Long> groupOffset : keyGroupsStateHandle.getGroupRangeOffsets()) {\n\t\t\t\t\tint keyGroupIndex = groupOffset.f0;\n\t\t\t\t\tlong offset = groupOffset.f1;\n\n\t\t\t\t\t// Check that restored key groups all belong to the backend.\n\t\t\t\t\tPreconditions.checkState(keyGroupRange.contains(keyGroupIndex), \"The key group must belong to the backend.\");\n\n\t\t\t\t\tfsDataInputStream.seek(offset);\n\n\t\t\t\t\tint writtenKeyGroupIndex = inView.readInt();\n\n\t\t\t\t\tPreconditions.checkState(writtenKeyGroupIndex == keyGroupIndex,\n\t\t\t\t\t\t\t\"Unexpected key-group in restore.\");\n\n\t\t\t\t\tfor (int i = 0; i < restoredMetaInfos.size(); i++) {\n\t\t\t\t\t\tint kvStateId = inView.readShort();\n\t\t\t\t\t\tStateTable<K, ?, ?> stateTable = stateTables.get(kvStatesById.get(kvStateId));\n\n\t\t\t\t\t\tStateTableByKeyGroupReader keyGroupReader =\n\t\t\t\t\t\t\t\tStateTableByKeyGroupReaders.readerForVersion(\n\t\t\t\t\t\t\t\t\t\tstateTable,\n\t\t\t\t\t\t\t\t\t\tserializationProxy.getReadVersion());\n\n\t\t\t\t\t\tkeyGroupReader.readMappingsInKeyGroup(inView, keyGroupIndex);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} finally {\n\t\t\t\tcancelStreamRegistry.unregisterClosable(fsDataInputStream);\n\t\t\t\tIOUtils.closeQuietly(fsDataInputStream);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBFullSnapshotOperation::writeKVStateMetaData()",
            " 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604 -\n 605 -\n 606  \n 607  \n 608  ",
            "\t\tprivate void writeKVStateMetaData() throws IOException {\n\n\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> metaInfoSnapshots =\n\t\t\t\t\tnew ArrayList<>(stateBackend.kvStateInformation.size());\n\n\t\t\tint kvStateId = 0;\n\t\t\tfor (Map.Entry<String, Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>>> column :\n\t\t\t\t\tstateBackend.kvStateInformation.entrySet()) {\n\n\t\t\t\tmetaInfoSnapshots.add(column.getValue().f1.snapshot());\n\n\t\t\t\t//retrieve iterator for this k/v states\n\t\t\t\treadOptions = new ReadOptions();\n\t\t\t\treadOptions.setSnapshot(snapshot);\n\n\t\t\t\tkvStateIterators.add(\n\t\t\t\t\t\tnew Tuple2<>(stateBackend.db.newIterator(column.getValue().f0, readOptions), kvStateId));\n\n\t\t\t\t++kvStateId;\n\t\t\t}\n\n\t\t\tKeyedBackendSerializationProxy serializationProxy =\n\t\t\t\t\tnew KeyedBackendSerializationProxy(stateBackend.getKeySerializer(), metaInfoSnapshots);\n\n\t\t\tserializationProxy.write(outputView);\n\t\t}",
            " 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601 +\n 602 +\n 603  \n 604  \n 605  ",
            "\t\tprivate void writeKVStateMetaData() throws IOException {\n\n\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> metaInfoSnapshots =\n\t\t\t\t\tnew ArrayList<>(stateBackend.kvStateInformation.size());\n\n\t\t\tint kvStateId = 0;\n\t\t\tfor (Map.Entry<String, Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>>> column :\n\t\t\t\t\tstateBackend.kvStateInformation.entrySet()) {\n\n\t\t\t\tmetaInfoSnapshots.add(column.getValue().f1.snapshot());\n\n\t\t\t\t//retrieve iterator for this k/v states\n\t\t\t\treadOptions = new ReadOptions();\n\t\t\t\treadOptions.setSnapshot(snapshot);\n\n\t\t\t\tkvStateIterators.add(\n\t\t\t\t\t\tnew Tuple2<>(stateBackend.db.newIterator(column.getValue().f0, readOptions), kvStateId));\n\n\t\t\t\t++kvStateId;\n\t\t\t}\n\n\t\t\tKeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\t\tnew KeyedBackendSerializationProxy<>(stateBackend.getKeySerializer(), metaInfoSnapshots);\n\n\t\t\tserializationProxy.write(outputView);\n\t\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBIncrementalRestoreOperation::RocksDBIncrementalRestoreOperation(RocksDBKeyedStateBackend)",
            "1228 -\n1229  \n1230  ",
            "\t\tprivate RocksDBIncrementalRestoreOperation(RocksDBKeyedStateBackend<?> stateBackend) {\n\t\t\tthis.stateBackend = stateBackend;\n\t\t}",
            "1224 +\n1225  \n1226  ",
            "\t\tprivate RocksDBIncrementalRestoreOperation(RocksDBKeyedStateBackend<T> stateBackend) {\n\t\t\tthis.stateBackend = stateBackend;\n\t\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBFullSnapshotOperation::RocksDBFullSnapshotOperation(RocksDBKeyedStateBackend,CheckpointStreamFactory)",
            " 463  \n 464 -\n 465  \n 466  \n 467  \n 468  \n 469  \n 470  ",
            "\t\tRocksDBFullSnapshotOperation(\n\t\t\t\tRocksDBKeyedStateBackend<?> stateBackend,\n\t\t\t\tCheckpointStreamFactory checkpointStreamFactory) {\n\n\t\t\tthis.stateBackend = stateBackend;\n\t\t\tthis.checkpointStreamFactory = checkpointStreamFactory;\n\t\t\tthis.keyGroupRangeOffsets = new KeyGroupRangeOffsets(stateBackend.keyGroupRange);\n\t\t}",
            " 460  \n 461 +\n 462  \n 463  \n 464  \n 465  \n 466  \n 467  ",
            "\t\tRocksDBFullSnapshotOperation(\n\t\t\t\tRocksDBKeyedStateBackend<K> stateBackend,\n\t\t\t\tCheckpointStreamFactory checkpointStreamFactory) {\n\n\t\t\tthis.stateBackend = stateBackend;\n\t\t\tthis.checkpointStreamFactory = checkpointStreamFactory;\n\t\t\tthis.keyGroupRangeOffsets = new KeyGroupRangeOffsets(stateBackend.keyGroupRange);\n\t\t}"
        ],
        [
            "RocksDBKeyedStateBackend::getColumnFamily(StateDescriptor,TypeSerializer)",
            "1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535  \n1536  \n1537  \n1538  \n1539 -\n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554  \n1555  \n1556  \n1557  \n1558  \n1559 -\n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  \n1589  \n1590  \n1591  \n1592  \n1593  ",
            "\t/**\n\t * Creates a column family handle for use with a k/v state. When restoring from a snapshot\n\t * we don't restore the individual k/v states, just the global RocksDB data base and the\n\t * list of column families. When a k/v state is first requested we check here whether we\n\t * already have a column family for that and return it or create a new one if it doesn't exist.\n\t *\n\t * <p>This also checks whether the {@link StateDescriptor} for a state matches the one\n\t * that we checkpointed, i.e. is already in the map of column families.\n\t */\n\t@SuppressWarnings(\"rawtypes, unchecked\")\n\tprotected <N, S> ColumnFamilyHandle getColumnFamily(\n\t\t\tStateDescriptor<?, S> descriptor, TypeSerializer<N> namespaceSerializer) throws IOException {\n\n\t\tTuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>> stateInfo =\n\t\t\t\tkvStateInformation.get(descriptor.getName());\n\n\t\tRegisteredKeyedBackendStateMetaInfo<N, S> newMetaInfo = new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tdescriptor.getType(),\n\t\t\tdescriptor.getName(),\n\t\t\tnamespaceSerializer,\n\t\t\tdescriptor.getSerializer());\n\n\t\tif (stateInfo != null) {\n\t\t\t// TODO with eager registration in place, these checks should be moved to restore()\n\n\t\t\tRegisteredKeyedBackendStateMetaInfo.Snapshot<N, S> restoredMetaInfo =\n\t\t\t\trestoredKvStateMetaInfos.get(descriptor.getName());\n\n\t\t\tPreconditions.checkState(\n\t\t\t\tnewMetaInfo.getName().equals(restoredMetaInfo.getName()),\n\t\t\t\t\"Incompatible state names. \" +\n\t\t\t\t\t\"Was [\" + restoredMetaInfo.getName() + \"], \" +\n\t\t\t\t\t\"registered with [\" + newMetaInfo.getName() + \"].\");\n\n\t\t\tif (!newMetaInfo.getStateType().equals(StateDescriptor.Type.UNKNOWN)\n\t\t\t\t&& !restoredMetaInfo.getStateType().equals(StateDescriptor.Type.UNKNOWN)) {\n\n\t\t\t\tPreconditions.checkState(\n\t\t\t\t\tnewMetaInfo.getStateType().equals(restoredMetaInfo.getStateType()),\n\t\t\t\t\t\"Incompatible state types. \" +\n\t\t\t\t\t\t\"Was [\" + restoredMetaInfo.getStateType() + \"], \" +\n\t\t\t\t\t\t\"registered with [\" + newMetaInfo.getStateType() + \"].\");\n\t\t\t}\n\n\t\t\t// check compatibility results to determine if state migration is required\n\n\t\t\tCompatibilityResult<N> namespaceCompatibility = StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\trestoredMetaInfo.getNamespaceSerializer(),\n\t\t\t\t\tMigrationNamespaceSerializerProxy.class,\n\t\t\t\t\trestoredMetaInfo.getNamespaceSerializerConfigSnapshot(),\n\t\t\t\t\tnewMetaInfo.getNamespaceSerializer());\n\n\t\t\tCompatibilityResult<S> stateCompatibility = StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\trestoredMetaInfo.getStateSerializer(),\n\t\t\t\t\tTypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer.class,\n\t\t\t\t\trestoredMetaInfo.getStateSerializerConfigSnapshot(),\n\t\t\t\t\tnewMetaInfo.getStateSerializer());\n\n\t\t\tif (!namespaceCompatibility.isRequiresMigration() && !stateCompatibility.isRequiresMigration()) {\n\t\t\t\tstateInfo.f1 = newMetaInfo;\n\t\t\t\treturn stateInfo.f0;\n\t\t\t} else {\n\t\t\t\t// TODO state migration currently isn't possible.\n\t\t\t\tthrow new RuntimeException(\"State migration currently isn't supported.\");\n\t\t\t}\n\t\t}\n\n\t\tColumnFamilyDescriptor columnDescriptor = new ColumnFamilyDescriptor(\n\t\t\t\tdescriptor.getName().getBytes(ConfigConstants.DEFAULT_CHARSET), columnOptions);\n\n\t\ttry {\n\t\t\tColumnFamilyHandle columnFamily = db.createColumnFamily(columnDescriptor);\n\t\t\tTuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<N, S>> tuple =\n\t\t\t\t\tnew Tuple2<>(columnFamily, newMetaInfo);\n\t\t\tMap rawAccess = kvStateInformation;\n\t\t\trawAccess.put(descriptor.getName(), tuple);\n\t\t\treturn columnFamily;\n\t\t} catch (RocksDBException e) {\n\t\t\tthrow new IOException(\"Error creating ColumnFamilyHandle.\", e);\n\t\t}\n\t}",
            "1508  \n1509  \n1510  \n1511  \n1512  \n1513  \n1514  \n1515  \n1516  \n1517  \n1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534 +\n1535  \n1536  \n1537  \n1538  \n1539  \n1540  \n1541  \n1542  \n1543  \n1544  \n1545  \n1546  \n1547  \n1548  \n1549  \n1550  \n1551  \n1552  \n1553  \n1554 +\n1555  \n1556  \n1557  \n1558  \n1559  \n1560  \n1561  \n1562  \n1563  \n1564  \n1565  \n1566  \n1567  \n1568  \n1569  \n1570  \n1571  \n1572  \n1573  \n1574  \n1575  \n1576  \n1577  \n1578  \n1579  \n1580  \n1581  \n1582  \n1583  \n1584  \n1585  \n1586  \n1587  \n1588  ",
            "\t/**\n\t * Creates a column family handle for use with a k/v state. When restoring from a snapshot\n\t * we don't restore the individual k/v states, just the global RocksDB data base and the\n\t * list of column families. When a k/v state is first requested we check here whether we\n\t * already have a column family for that and return it or create a new one if it doesn't exist.\n\t *\n\t * <p>This also checks whether the {@link StateDescriptor} for a state matches the one\n\t * that we checkpointed, i.e. is already in the map of column families.\n\t */\n\t@SuppressWarnings(\"rawtypes, unchecked\")\n\tprotected <N, S> ColumnFamilyHandle getColumnFamily(\n\t\t\tStateDescriptor<?, S> descriptor, TypeSerializer<N> namespaceSerializer) throws IOException {\n\n\t\tTuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>> stateInfo =\n\t\t\t\tkvStateInformation.get(descriptor.getName());\n\n\t\tRegisteredKeyedBackendStateMetaInfo<N, S> newMetaInfo = new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tdescriptor.getType(),\n\t\t\tdescriptor.getName(),\n\t\t\tnamespaceSerializer,\n\t\t\tdescriptor.getSerializer());\n\n\t\tif (stateInfo != null) {\n\t\t\t// TODO with eager registration in place, these checks should be moved to restore()\n\n\t\t\tRegisteredKeyedBackendStateMetaInfo.Snapshot<N, S> restoredMetaInfo =\n\t\t\t\t(RegisteredKeyedBackendStateMetaInfo.Snapshot<N, S>) restoredKvStateMetaInfos.get(descriptor.getName());\n\n\t\t\tPreconditions.checkState(\n\t\t\t\tnewMetaInfo.getName().equals(restoredMetaInfo.getName()),\n\t\t\t\t\"Incompatible state names. \" +\n\t\t\t\t\t\"Was [\" + restoredMetaInfo.getName() + \"], \" +\n\t\t\t\t\t\"registered with [\" + newMetaInfo.getName() + \"].\");\n\n\t\t\tif (!newMetaInfo.getStateType().equals(StateDescriptor.Type.UNKNOWN)\n\t\t\t\t&& !restoredMetaInfo.getStateType().equals(StateDescriptor.Type.UNKNOWN)) {\n\n\t\t\t\tPreconditions.checkState(\n\t\t\t\t\tnewMetaInfo.getStateType().equals(restoredMetaInfo.getStateType()),\n\t\t\t\t\t\"Incompatible state types. \" +\n\t\t\t\t\t\t\"Was [\" + restoredMetaInfo.getStateType() + \"], \" +\n\t\t\t\t\t\t\"registered with [\" + newMetaInfo.getStateType() + \"].\");\n\t\t\t}\n\n\t\t\t// check compatibility results to determine if state migration is required\n\n\t\t\tCompatibilityResult<?> namespaceCompatibility = StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\trestoredMetaInfo.getNamespaceSerializer(),\n\t\t\t\t\tMigrationNamespaceSerializerProxy.class,\n\t\t\t\t\trestoredMetaInfo.getNamespaceSerializerConfigSnapshot(),\n\t\t\t\t\tnewMetaInfo.getNamespaceSerializer());\n\n\t\t\tCompatibilityResult<S> stateCompatibility = StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\trestoredMetaInfo.getStateSerializer(),\n\t\t\t\t\tTypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer.class,\n\t\t\t\t\trestoredMetaInfo.getStateSerializerConfigSnapshot(),\n\t\t\t\t\tnewMetaInfo.getStateSerializer());\n\n\t\t\tif (!namespaceCompatibility.isRequiresMigration() && !stateCompatibility.isRequiresMigration()) {\n\t\t\t\tstateInfo.f1 = newMetaInfo;\n\t\t\t\treturn stateInfo.f0;\n\t\t\t} else {\n\t\t\t\t// TODO state migration currently isn't possible.\n\t\t\t\tthrow new RuntimeException(\"State migration currently isn't supported.\");\n\t\t\t}\n\t\t}\n\n\t\tColumnFamilyDescriptor columnDescriptor = new ColumnFamilyDescriptor(\n\t\t\t\tdescriptor.getName().getBytes(ConfigConstants.DEFAULT_CHARSET), columnOptions);\n\n\t\ttry {\n\t\t\tColumnFamilyHandle columnFamily = db.createColumnFamily(columnDescriptor);\n\t\t\tTuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<N, S>> tuple =\n\t\t\t\t\tnew Tuple2<>(columnFamily, newMetaInfo);\n\t\t\tMap rawAccess = kvStateInformation;\n\t\t\trawAccess.put(descriptor.getName(), tuple);\n\t\t\treturn columnFamily;\n\t\t} catch (RocksDBException e) {\n\t\t\tthrow new IOException(\"Error creating ColumnFamilyHandle.\", e);\n\t\t}\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBIncrementalSnapshotOperation::RocksDBIncrementalSnapshotOperation(RocksDBKeyedStateBackend,CheckpointStreamFactory,long,long)",
            " 750  \n 751 -\n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  ",
            "\t\tprivate RocksDBIncrementalSnapshotOperation(\n\t\t\t\tRocksDBKeyedStateBackend<?> stateBackend,\n\t\t\t\tCheckpointStreamFactory checkpointStreamFactory,\n\t\t\t\tlong checkpointId,\n\t\t\t\tlong checkpointTimestamp) {\n\n\t\t\tthis.stateBackend = stateBackend;\n\t\t\tthis.checkpointStreamFactory = checkpointStreamFactory;\n\t\t\tthis.checkpointId = checkpointId;\n\t\t\tthis.checkpointTimestamp = checkpointTimestamp;\n\t\t}",
            " 747  \n 748 +\n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  ",
            "\t\tprivate RocksDBIncrementalSnapshotOperation(\n\t\t\t\tRocksDBKeyedStateBackend<K> stateBackend,\n\t\t\t\tCheckpointStreamFactory checkpointStreamFactory,\n\t\t\t\tlong checkpointId,\n\t\t\t\tlong checkpointTimestamp) {\n\n\t\t\tthis.stateBackend = stateBackend;\n\t\t\tthis.checkpointStreamFactory = checkpointStreamFactory;\n\t\t\tthis.checkpointId = checkpointId;\n\t\t\tthis.checkpointTimestamp = checkpointTimestamp;\n\t\t}"
        ],
        [
            "HeapKeyedStateBackend::snapshot(long,long,CheckpointStreamFactory,CheckpointOptions)",
            " 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275 -\n 276 -\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  ",
            "\t@Override\n\t@SuppressWarnings(\"unchecked\")\n\tpublic  RunnableFuture<KeyedStateHandle> snapshot(\n\t\t\tfinal long checkpointId,\n\t\t\tfinal long timestamp,\n\t\t\tfinal CheckpointStreamFactory streamFactory,\n\t\t\tCheckpointOptions checkpointOptions) throws Exception {\n\n\t\tif (!hasRegisteredState()) {\n\t\t\treturn DoneFuture.nullValue();\n\t\t}\n\n\t\tlong syncStartTime = System.currentTimeMillis();\n\n\t\tPreconditions.checkState(stateTables.size() <= Short.MAX_VALUE,\n\t\t\t\t\"Too many KV-States: \" + stateTables.size() +\n\t\t\t\t\t\t\". Currently at most \" + Short.MAX_VALUE + \" states are supported\");\n\n\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> metaInfoSnapshots = new ArrayList<>(stateTables.size());\n\n\t\tfinal Map<String, Integer> kVStateToId = new HashMap<>(stateTables.size());\n\n\t\tfinal Map<StateTable<K, ?, ?>, StateTableSnapshot> cowStateStableSnapshots = new HashedMap(stateTables.size());\n\n\t\tfor (Map.Entry<String, StateTable<K, ?, ?>> kvState : stateTables.entrySet()) {\n\t\t\tmetaInfoSnapshots.add(kvState.getValue().getMetaInfo().snapshot());\n\t\t\tkVStateToId.put(kvState.getKey(), kVStateToId.size());\n\t\t\tStateTable<K, ?, ?> stateTable = kvState.getValue();\n\t\t\tif (null != stateTable) {\n\t\t\t\tcowStateStableSnapshots.put(stateTable, stateTable.createSnapshot());\n\t\t\t}\n\t\t}\n\n\t\tfinal KeyedBackendSerializationProxy serializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy(keySerializer, metaInfoSnapshots);\n\n\t\t//--------------------------------------------------- this becomes the end of sync part\n\n\t\t// implementation of the async IO operation, based on FutureTask\n\t\tfinal AbstractAsyncSnapshotIOCallable<KeyedStateHandle> ioCallable =\n\t\t\tnew AbstractAsyncSnapshotIOCallable<KeyedStateHandle>(\n\t\t\t\tcheckpointId,\n\t\t\t\ttimestamp,\n\t\t\t\tstreamFactory,\n\t\t\t\tcancelStreamRegistry) {\n\n\t\t\t\t@Override\n\t\t\t\tpublic KeyGroupsStateHandle performOperation() throws Exception {\n\t\t\t\t\tlong asyncStartTime = System.currentTimeMillis();\n\t\t\t\t\tCheckpointStreamFactory.CheckpointStateOutputStream stream = getIoHandle();\n\t\t\t\t\tDataOutputViewStreamWrapper outView = new DataOutputViewStreamWrapper(stream);\n\t\t\t\t\tserializationProxy.write(outView);\n\n\t\t\t\t\tlong[] keyGroupRangeOffsets = new long[keyGroupRange.getNumberOfKeyGroups()];\n\n\t\t\t\t\tfor (int keyGroupPos = 0; keyGroupPos < keyGroupRange.getNumberOfKeyGroups(); ++keyGroupPos) {\n\t\t\t\t\t\tint keyGroupId = keyGroupRange.getKeyGroupId(keyGroupPos);\n\t\t\t\t\t\tkeyGroupRangeOffsets[keyGroupPos] = stream.getPos();\n\t\t\t\t\t\toutView.writeInt(keyGroupId);\n\n\t\t\t\t\t\tfor (Map.Entry<String, StateTable<K, ?, ?>> kvState : stateTables.entrySet()) {\n\t\t\t\t\t\t\toutView.writeShort(kVStateToId.get(kvState.getKey()));\n\t\t\t\t\t\t\tcowStateStableSnapshots.get(kvState.getValue()).writeMappingsInKeyGroup(outView, keyGroupId);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tfinal StreamStateHandle streamStateHandle = closeStreamAndGetStateHandle();\n\n\t\t\t\t\tif (asynchronousSnapshots) {\n\t\t\t\t\t\tLOG.info(\"Heap backend snapshot ({}, asynchronous part) in thread {} took {} ms.\",\n\t\t\t\t\t\t\tstreamFactory, Thread.currentThread(), (System.currentTimeMillis() - asyncStartTime));\n\t\t\t\t\t}\n\n\t\t\t\t\tif (streamStateHandle == null) {\n\t\t\t\t\t\treturn null;\n\t\t\t\t\t}\n\n\t\t\t\t\tKeyGroupRangeOffsets offsets = new KeyGroupRangeOffsets(keyGroupRange, keyGroupRangeOffsets);\n\t\t\t\t\tfinal KeyGroupsStateHandle keyGroupsStateHandle = new KeyGroupsStateHandle(offsets, streamStateHandle);\n\n\t\t\t\t\treturn keyGroupsStateHandle;\n\t\t\t\t}\n\t\t\t};\n\n\t\tAsyncStoppableTaskWithCallback<KeyedStateHandle> task = AsyncStoppableTaskWithCallback.from(ioCallable);\n\n\t\tif (!asynchronousSnapshots) {\n\t\t\ttask.run();\n\t\t}\n\n\t\tLOG.info(\"Heap backend snapshot (\" + streamFactory + \", synchronous part) in thread \" +\n\t\t\t\tThread.currentThread() + \" took \" + (System.currentTimeMillis() - syncStartTime) + \" ms.\");\n\n\t\treturn task;\n\t}",
            " 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275 +\n 276 +\n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  ",
            "\t@Override\n\t@SuppressWarnings(\"unchecked\")\n\tpublic  RunnableFuture<KeyedStateHandle> snapshot(\n\t\t\tfinal long checkpointId,\n\t\t\tfinal long timestamp,\n\t\t\tfinal CheckpointStreamFactory streamFactory,\n\t\t\tCheckpointOptions checkpointOptions) throws Exception {\n\n\t\tif (!hasRegisteredState()) {\n\t\t\treturn DoneFuture.nullValue();\n\t\t}\n\n\t\tlong syncStartTime = System.currentTimeMillis();\n\n\t\tPreconditions.checkState(stateTables.size() <= Short.MAX_VALUE,\n\t\t\t\t\"Too many KV-States: \" + stateTables.size() +\n\t\t\t\t\t\t\". Currently at most \" + Short.MAX_VALUE + \" states are supported\");\n\n\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> metaInfoSnapshots = new ArrayList<>(stateTables.size());\n\n\t\tfinal Map<String, Integer> kVStateToId = new HashMap<>(stateTables.size());\n\n\t\tfinal Map<StateTable<K, ?, ?>, StateTableSnapshot> cowStateStableSnapshots = new HashedMap(stateTables.size());\n\n\t\tfor (Map.Entry<String, StateTable<K, ?, ?>> kvState : stateTables.entrySet()) {\n\t\t\tmetaInfoSnapshots.add(kvState.getValue().getMetaInfo().snapshot());\n\t\t\tkVStateToId.put(kvState.getKey(), kVStateToId.size());\n\t\t\tStateTable<K, ?, ?> stateTable = kvState.getValue();\n\t\t\tif (null != stateTable) {\n\t\t\t\tcowStateStableSnapshots.put(stateTable, stateTable.createSnapshot());\n\t\t\t}\n\t\t}\n\n\t\tfinal KeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy<>(keySerializer, metaInfoSnapshots);\n\n\t\t//--------------------------------------------------- this becomes the end of sync part\n\n\t\t// implementation of the async IO operation, based on FutureTask\n\t\tfinal AbstractAsyncSnapshotIOCallable<KeyedStateHandle> ioCallable =\n\t\t\tnew AbstractAsyncSnapshotIOCallable<KeyedStateHandle>(\n\t\t\t\tcheckpointId,\n\t\t\t\ttimestamp,\n\t\t\t\tstreamFactory,\n\t\t\t\tcancelStreamRegistry) {\n\n\t\t\t\t@Override\n\t\t\t\tpublic KeyGroupsStateHandle performOperation() throws Exception {\n\t\t\t\t\tlong asyncStartTime = System.currentTimeMillis();\n\t\t\t\t\tCheckpointStreamFactory.CheckpointStateOutputStream stream = getIoHandle();\n\t\t\t\t\tDataOutputViewStreamWrapper outView = new DataOutputViewStreamWrapper(stream);\n\t\t\t\t\tserializationProxy.write(outView);\n\n\t\t\t\t\tlong[] keyGroupRangeOffsets = new long[keyGroupRange.getNumberOfKeyGroups()];\n\n\t\t\t\t\tfor (int keyGroupPos = 0; keyGroupPos < keyGroupRange.getNumberOfKeyGroups(); ++keyGroupPos) {\n\t\t\t\t\t\tint keyGroupId = keyGroupRange.getKeyGroupId(keyGroupPos);\n\t\t\t\t\t\tkeyGroupRangeOffsets[keyGroupPos] = stream.getPos();\n\t\t\t\t\t\toutView.writeInt(keyGroupId);\n\n\t\t\t\t\t\tfor (Map.Entry<String, StateTable<K, ?, ?>> kvState : stateTables.entrySet()) {\n\t\t\t\t\t\t\toutView.writeShort(kVStateToId.get(kvState.getKey()));\n\t\t\t\t\t\t\tcowStateStableSnapshots.get(kvState.getValue()).writeMappingsInKeyGroup(outView, keyGroupId);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tfinal StreamStateHandle streamStateHandle = closeStreamAndGetStateHandle();\n\n\t\t\t\t\tif (asynchronousSnapshots) {\n\t\t\t\t\t\tLOG.info(\"Heap backend snapshot ({}, asynchronous part) in thread {} took {} ms.\",\n\t\t\t\t\t\t\tstreamFactory, Thread.currentThread(), (System.currentTimeMillis() - asyncStartTime));\n\t\t\t\t\t}\n\n\t\t\t\t\tif (streamStateHandle == null) {\n\t\t\t\t\t\treturn null;\n\t\t\t\t\t}\n\n\t\t\t\t\tKeyGroupRangeOffsets offsets = new KeyGroupRangeOffsets(keyGroupRange, keyGroupRangeOffsets);\n\t\t\t\t\tfinal KeyGroupsStateHandle keyGroupsStateHandle = new KeyGroupsStateHandle(offsets, streamStateHandle);\n\n\t\t\t\t\treturn keyGroupsStateHandle;\n\t\t\t\t}\n\t\t\t};\n\n\t\tAsyncStoppableTaskWithCallback<KeyedStateHandle> task = AsyncStoppableTaskWithCallback.from(ioCallable);\n\n\t\tif (!asynchronousSnapshots) {\n\t\t\ttask.run();\n\t\t}\n\n\t\tLOG.info(\"Heap backend snapshot (\" + streamFactory + \", synchronous part) in thread \" +\n\t\t\t\tThread.currentThread() + \" took \" + (System.currentTimeMillis() - syncStartTime) + \" ms.\");\n\n\t\treturn task;\n\t}"
        ],
        [
            "KeyedBackendSerializationProxy::read(DataInputView)",
            " 121  \n 122  \n 123  \n 124  \n 125 -\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "\t@Override\n\tpublic void read(DataInputView in) throws IOException {\n\t\tsuper.read(in);\n\n\t\tfinal TypeSerializerSerializationProxy<?> keySerializerProxy =\n\t\t\tnew TypeSerializerSerializationProxy<>(userCodeClassLoader);\n\n\t\t// only starting from version 3, we have the key serializer and its config snapshot written\n\t\tif (getReadVersion() >= 3) {\n\t\t\tint keySerializerConfigSnapshotOffset = in.readInt();\n\t\t\tint numBufferedBytes = in.readInt();\n\t\t\tbyte[] keySerializerAndConfigBytes = new byte[numBufferedBytes];\n\t\t\tin.readFully(keySerializerAndConfigBytes);\n\n\t\t\ttry (\n\t\t\t\tByteArrayInputStreamWithPos buffer = new ByteArrayInputStreamWithPos(keySerializerAndConfigBytes);\n\t\t\t\tDataInputViewStreamWrapper bufferWrapper = new DataInputViewStreamWrapper(buffer)) {\n\n\t\t\t\ttry {\n\t\t\t\t\tkeySerializerProxy.read(bufferWrapper);\n\t\t\t\t\tthis.keySerializer = keySerializerProxy.getTypeSerializer();\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tthis.keySerializer = null;\n\t\t\t\t}\n\n\t\t\t\tbuffer.setPosition(keySerializerConfigSnapshotOffset);\n\t\t\t\tthis.keySerializerConfigSnapshot =\n\t\t\t\t\tTypeSerializerUtil.readSerializerConfigSnapshot(bufferWrapper, userCodeClassLoader);\n\t\t\t}\n\t\t} else {\n\t\t\tkeySerializerProxy.read(in);\n\t\t\tthis.keySerializer = keySerializerProxy.getTypeSerializer();\n\t\t\tthis.keySerializerConfigSnapshot = null;\n\t\t}\n\n\t\tint numKvStates = in.readShort();\n\t\tstateMetaInfoSnapshots = new ArrayList<>(numKvStates);\n\t\tfor (int i = 0; i < numKvStates; i++) {\n\t\t\tstateMetaInfoSnapshots.add(\n\t\t\t\tKeyedBackendStateMetaInfoSnapshotReaderWriters\n\t\t\t\t\t.getReaderForVersion(getReadVersion(), userCodeClassLoader)\n\t\t\t\t\t.readStateMetaInfo(in));\n\t\t}\n\t}",
            " 121  \n 122  \n 123  \n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "\t@Override\n\tpublic void read(DataInputView in) throws IOException {\n\t\tsuper.read(in);\n\n\t\tfinal TypeSerializerSerializationProxy<K> keySerializerProxy =\n\t\t\tnew TypeSerializerSerializationProxy<>(userCodeClassLoader);\n\n\t\t// only starting from version 3, we have the key serializer and its config snapshot written\n\t\tif (getReadVersion() >= 3) {\n\t\t\tint keySerializerConfigSnapshotOffset = in.readInt();\n\t\t\tint numBufferedBytes = in.readInt();\n\t\t\tbyte[] keySerializerAndConfigBytes = new byte[numBufferedBytes];\n\t\t\tin.readFully(keySerializerAndConfigBytes);\n\n\t\t\ttry (\n\t\t\t\tByteArrayInputStreamWithPos buffer = new ByteArrayInputStreamWithPos(keySerializerAndConfigBytes);\n\t\t\t\tDataInputViewStreamWrapper bufferWrapper = new DataInputViewStreamWrapper(buffer)) {\n\n\t\t\t\ttry {\n\t\t\t\t\tkeySerializerProxy.read(bufferWrapper);\n\t\t\t\t\tthis.keySerializer = keySerializerProxy.getTypeSerializer();\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tthis.keySerializer = null;\n\t\t\t\t}\n\n\t\t\t\tbuffer.setPosition(keySerializerConfigSnapshotOffset);\n\t\t\t\tthis.keySerializerConfigSnapshot =\n\t\t\t\t\tTypeSerializerUtil.readSerializerConfigSnapshot(bufferWrapper, userCodeClassLoader);\n\t\t\t}\n\t\t} else {\n\t\t\tkeySerializerProxy.read(in);\n\t\t\tthis.keySerializer = keySerializerProxy.getTypeSerializer();\n\t\t\tthis.keySerializerConfigSnapshot = null;\n\t\t}\n\n\t\tint numKvStates = in.readShort();\n\t\tstateMetaInfoSnapshots = new ArrayList<>(numKvStates);\n\t\tfor (int i = 0; i < numKvStates; i++) {\n\t\t\tstateMetaInfoSnapshots.add(\n\t\t\t\tKeyedBackendStateMetaInfoSnapshotReaderWriters\n\t\t\t\t\t.getReaderForVersion(getReadVersion(), userCodeClassLoader)\n\t\t\t\t\t.readStateMetaInfo(in));\n\t\t}\n\t}"
        ],
        [
            "KeyedBackendSerializationProxy::KeyedBackendSerializationProxy(TypeSerializer,List)",
            "  57  \n  58 -\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  ",
            "\tpublic KeyedBackendSerializationProxy(\n\t\t\tTypeSerializer<?> keySerializer,\n\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> stateMetaInfoSnapshots) {\n\n\t\tthis.keySerializer = Preconditions.checkNotNull(keySerializer);\n\t\tthis.keySerializerConfigSnapshot = Preconditions.checkNotNull(keySerializer.snapshotConfiguration());\n\n\t\tPreconditions.checkNotNull(stateMetaInfoSnapshots);\n\t\tPreconditions.checkArgument(stateMetaInfoSnapshots.size() <= Short.MAX_VALUE);\n\t\tthis.stateMetaInfoSnapshots = stateMetaInfoSnapshots;\n\t}",
            "  57  \n  58 +\n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  ",
            "\tpublic KeyedBackendSerializationProxy(\n\t\t\tTypeSerializer<K> keySerializer,\n\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> stateMetaInfoSnapshots) {\n\n\t\tthis.keySerializer = Preconditions.checkNotNull(keySerializer);\n\t\tthis.keySerializerConfigSnapshot = Preconditions.checkNotNull(keySerializer.snapshotConfiguration());\n\n\t\tPreconditions.checkNotNull(stateMetaInfoSnapshots);\n\t\tPreconditions.checkArgument(stateMetaInfoSnapshots.size() <= Short.MAX_VALUE);\n\t\tthis.stateMetaInfoSnapshots = stateMetaInfoSnapshots;\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::snapshotFully(long,long,CheckpointStreamFactory)",
            " 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 -\n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  ",
            "\tprivate RunnableFuture<KeyedStateHandle> snapshotFully(\n\t\t\tfinal long checkpointId,\n\t\t\tfinal long timestamp,\n\t\t\tfinal CheckpointStreamFactory streamFactory) throws Exception {\n\n\t\tlong startTime = System.currentTimeMillis();\n\n\t\tfinal RocksDBFullSnapshotOperation snapshotOperation = new RocksDBFullSnapshotOperation(this, streamFactory);\n\t\t// hold the db lock while operation on the db to guard us against async db disposal\n\t\tsynchronized (asyncSnapshotLock) {\n\n\t\t\tif (db != null) {\n\n\t\t\t\tif (!hasRegisteredState()) {\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"Asynchronous RocksDB snapshot performed on empty keyed state at \" + timestamp +\n\t\t\t\t\t\t\t\t\" . Returning null.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn DoneFuture.nullValue();\n\t\t\t\t}\n\n\t\t\t\tsnapshotOperation.takeDBSnapShot(checkpointId, timestamp);\n\t\t\t} else {\n\t\t\t\tthrow new IOException(\"RocksDB closed.\");\n\t\t\t}\n\t\t}\n\n\t\t// implementation of the async IO operation, based on FutureTask\n\t\tAbstractAsyncIOCallable<KeyedStateHandle, CheckpointStreamFactory.CheckpointStateOutputStream> ioCallable =\n\t\t\t\tnew AbstractAsyncIOCallable<KeyedStateHandle, CheckpointStreamFactory.CheckpointStateOutputStream>() {\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic CheckpointStreamFactory.CheckpointStateOutputStream openIOHandle() throws Exception {\n\t\t\t\t\t\tsnapshotOperation.openCheckpointStream();\n\t\t\t\t\t\treturn snapshotOperation.getOutStream();\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic KeyGroupsStateHandle performOperation() throws Exception {\n\t\t\t\t\t\tlong startTime = System.currentTimeMillis();\n\t\t\t\t\t\tsynchronized (asyncSnapshotLock) {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t// hold the db lock while operation on the db to guard us against async db disposal\n\t\t\t\t\t\t\t\tif (db == null) {\n\t\t\t\t\t\t\t\t\tthrow new IOException(\"RocksDB closed.\");\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tsnapshotOperation.writeDBSnapshot();\n\n\t\t\t\t\t\t\t} finally {\n\t\t\t\t\t\t\t\tsnapshotOperation.closeCheckpointStream();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tLOG.info(\"Asynchronous RocksDB snapshot ({}, asynchronous part) in thread {} took {} ms.\",\n\t\t\t\t\t\t\tstreamFactory, Thread.currentThread(), (System.currentTimeMillis() - startTime));\n\n\t\t\t\t\t\treturn snapshotOperation.getSnapshotResultStateHandle();\n\t\t\t\t\t}\n\n\t\t\t\t\tprivate void releaseSnapshotOperationResources(boolean canceled) {\n\t\t\t\t\t\t// hold the db lock while operation on the db to guard us against async db disposal\n\t\t\t\t\t\tsynchronized (asyncSnapshotLock) {\n\t\t\t\t\t\t\tsnapshotOperation.releaseSnapshotResources(canceled);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void done(boolean canceled) {\n\t\t\t\t\t\treleaseSnapshotOperationResources(canceled);\n\t\t\t\t\t}\n\t\t\t\t};\n\n\t\tLOG.info(\"Asynchronous RocksDB snapshot (\" + streamFactory + \", synchronous part) in thread \" +\n\t\t\t\tThread.currentThread() + \" took \" + (System.currentTimeMillis() - startTime) + \" ms.\");\n\n\t\treturn AsyncStoppableTaskWithCallback.from(ioCallable);\n\t}",
            " 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365 +\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  ",
            "\tprivate RunnableFuture<KeyedStateHandle> snapshotFully(\n\t\t\tfinal long checkpointId,\n\t\t\tfinal long timestamp,\n\t\t\tfinal CheckpointStreamFactory streamFactory) throws Exception {\n\n\t\tlong startTime = System.currentTimeMillis();\n\n\t\tfinal RocksDBFullSnapshotOperation<K> snapshotOperation = new RocksDBFullSnapshotOperation<>(this, streamFactory);\n\t\t// hold the db lock while operation on the db to guard us against async db disposal\n\t\tsynchronized (asyncSnapshotLock) {\n\n\t\t\tif (db != null) {\n\n\t\t\t\tif (!hasRegisteredState()) {\n\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\tLOG.debug(\"Asynchronous RocksDB snapshot performed on empty keyed state at \" + timestamp +\n\t\t\t\t\t\t\t\t\" . Returning null.\");\n\t\t\t\t\t}\n\t\t\t\t\treturn DoneFuture.nullValue();\n\t\t\t\t}\n\n\t\t\t\tsnapshotOperation.takeDBSnapShot(checkpointId, timestamp);\n\t\t\t} else {\n\t\t\t\tthrow new IOException(\"RocksDB closed.\");\n\t\t\t}\n\t\t}\n\n\t\t// implementation of the async IO operation, based on FutureTask\n\t\tAbstractAsyncIOCallable<KeyedStateHandle, CheckpointStreamFactory.CheckpointStateOutputStream> ioCallable =\n\t\t\t\tnew AbstractAsyncIOCallable<KeyedStateHandle, CheckpointStreamFactory.CheckpointStateOutputStream>() {\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic CheckpointStreamFactory.CheckpointStateOutputStream openIOHandle() throws Exception {\n\t\t\t\t\t\tsnapshotOperation.openCheckpointStream();\n\t\t\t\t\t\treturn snapshotOperation.getOutStream();\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic KeyGroupsStateHandle performOperation() throws Exception {\n\t\t\t\t\t\tlong startTime = System.currentTimeMillis();\n\t\t\t\t\t\tsynchronized (asyncSnapshotLock) {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t// hold the db lock while operation on the db to guard us against async db disposal\n\t\t\t\t\t\t\t\tif (db == null) {\n\t\t\t\t\t\t\t\t\tthrow new IOException(\"RocksDB closed.\");\n\t\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t\tsnapshotOperation.writeDBSnapshot();\n\n\t\t\t\t\t\t\t} finally {\n\t\t\t\t\t\t\t\tsnapshotOperation.closeCheckpointStream();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tLOG.info(\"Asynchronous RocksDB snapshot ({}, asynchronous part) in thread {} took {} ms.\",\n\t\t\t\t\t\t\tstreamFactory, Thread.currentThread(), (System.currentTimeMillis() - startTime));\n\n\t\t\t\t\t\treturn snapshotOperation.getSnapshotResultStateHandle();\n\t\t\t\t\t}\n\n\t\t\t\t\tprivate void releaseSnapshotOperationResources(boolean canceled) {\n\t\t\t\t\t\t// hold the db lock while operation on the db to guard us against async db disposal\n\t\t\t\t\t\tsynchronized (asyncSnapshotLock) {\n\t\t\t\t\t\t\tsnapshotOperation.releaseSnapshotResources(canceled);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void done(boolean canceled) {\n\t\t\t\t\t\treleaseSnapshotOperationResources(canceled);\n\t\t\t\t\t}\n\t\t\t\t};\n\n\t\tLOG.info(\"Asynchronous RocksDB snapshot (\" + streamFactory + \", synchronous part) in thread \" +\n\t\t\t\tThread.currentThread() + \" took \" + (System.currentTimeMillis() - startTime) + \" ms.\");\n\n\t\treturn AsyncStoppableTaskWithCallback.from(ioCallable);\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::restoreOldSavepointKeyedState(Collection)",
            "1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927  \n1928  \n1929  \n1930  \n1931  \n1932 -\n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  \n1987  \n1988  \n1989  \n1990  \n1991  ",
            "\t/**\n\t * For backwards compatibility, remove again later!\n\t *\n\t * @deprecated Internal method used for backwards compatibility.\n\t */\n\t@Deprecated\n\tprivate void restoreOldSavepointKeyedState(Collection<KeyedStateHandle> restoreState) throws Exception {\n\t\tcreateDB();\n\n\t\tPreconditions.checkState(1 == restoreState.size(), \"Only one element expected here.\");\n\n\t\tKeyedStateHandle keyedStateHandle = restoreState.iterator().next();\n\t\tif (!(keyedStateHandle instanceof MigrationKeyGroupStateHandle)) {\n\t\t\tthrow new IllegalStateException(\"Unexpected state handle type, \" +\n\t\t\t\t\t\"expected: \" + MigrationKeyGroupStateHandle.class +\n\t\t\t\t\t\", but found: \" + keyedStateHandle.getClass());\n\t\t}\n\n\t\tMigrationKeyGroupStateHandle keyGroupStateHandle = (MigrationKeyGroupStateHandle) keyedStateHandle;\n\n\t\tHashMap<String, RocksDBStateBackend.FinalFullyAsyncSnapshot> namedStates;\n\t\ttry (FSDataInputStream inputStream = keyGroupStateHandle.openInputStream()) {\n\t\t\tnamedStates = InstantiationUtil.deserializeObject(inputStream, userCodeClassLoader);\n\t\t}\n\n\t\tPreconditions.checkState(1 == namedStates.size(), \"Only one element expected here.\");\n\t\tDataInputView inputView = namedStates.values().iterator().next().stateHandle.getState(userCodeClassLoader);\n\n\t\t// clear k/v state information before filling it\n\t\tkvStateInformation.clear();\n\n\t\trestoredKvStateMetaInfos = new HashMap<>(namedStates.size());\n\n\t\t// first get the column family mapping\n\t\tint numColumns = inputView.readInt();\n\t\tMap<Byte, StateDescriptor<?, ?>> columnFamilyMapping = new HashMap<>(numColumns);\n\t\tfor (int i = 0; i < numColumns; i++) {\n\t\t\tbyte mappingByte = inputView.readByte();\n\n\t\t\tObjectInputStream ooIn =\n\t\t\t\t\tnew InstantiationUtil.ClassLoaderObjectInputStream(\n\t\t\t\t\t\t\tnew DataInputViewStream(inputView), userCodeClassLoader);\n\n\t\t\tStateDescriptor stateDescriptor = (StateDescriptor) ooIn.readObject();\n\n\t\t\tcolumnFamilyMapping.put(mappingByte, stateDescriptor);\n\n\t\t\t// mimic a restored kv state meta info\n\t\t\trestoredKvStateMetaInfos.put(\n\t\t\t\tstateDescriptor.getName(),\n\t\t\t\tnew RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\t\t\tstateDescriptor.getType(),\n\t\t\t\t\tstateDescriptor.getName(),\n\t\t\t\t\tMigrationNamespaceSerializerProxy.INSTANCE,\n\t\t\t\t\tstateDescriptor.getSerializer()).snapshot());\n\n\t\t\t// this will fill in the k/v state information\n\t\t\tgetColumnFamily(stateDescriptor, MigrationNamespaceSerializerProxy.INSTANCE);\n\t\t}\n\n\t\t// try and read until EOF\n\t\ttry {\n\t\t\t// the EOFException will get us out of this...\n\t\t\twhile (true) {\n\t\t\t\tbyte mappingByte = inputView.readByte();\n\t\t\t\tColumnFamilyHandle handle = getColumnFamily(\n\t\t\t\t\t\tcolumnFamilyMapping.get(mappingByte), MigrationNamespaceSerializerProxy.INSTANCE);\n\n\t\t\t\tbyte[] keyAndNamespace = BytePrimitiveArraySerializer.INSTANCE.deserialize(inputView);\n\n\t\t\t\tByteArrayInputStreamWithPos bis = new ByteArrayInputStreamWithPos(keyAndNamespace);\n\n\t\t\t\tK reconstructedKey = keySerializer.deserialize(new DataInputViewStreamWrapper(bis));\n\t\t\t\tint len = bis.getPosition();\n\n\t\t\t\tint keyGroup = (byte) KeyGroupRangeAssignment.assignToKeyGroup(reconstructedKey, numberOfKeyGroups);\n\n\t\t\t\tif (keyGroupPrefixBytes == 1) {\n\t\t\t\t\t// copy and override one byte (42) between key and namespace\n\t\t\t\t\tSystem.arraycopy(keyAndNamespace, 0, keyAndNamespace, 1, len);\n\t\t\t\t\tkeyAndNamespace[0] = (byte) keyGroup;\n\t\t\t\t} else {\n\t\t\t\t\tbyte[] largerKey = new byte[1 + keyAndNamespace.length];\n\n\t\t\t\t\t// write key-group\n\t\t\t\t\tlargerKey[0] = (byte) ((keyGroup >> 8) & 0xFF);\n\t\t\t\t\tlargerKey[1] = (byte) (keyGroup & 0xFF);\n\n\t\t\t\t\t// write key\n\t\t\t\t\tSystem.arraycopy(keyAndNamespace, 0, largerKey, 2, len);\n\n\t\t\t\t\t//skip one byte (42), write namespace\n\t\t\t\t\tSystem.arraycopy(keyAndNamespace, 1 + len, largerKey, 2 + len, keyAndNamespace.length - len - 1);\n\t\t\t\t\tkeyAndNamespace = largerKey;\n\t\t\t\t}\n\n\t\t\t\tbyte[] value = BytePrimitiveArraySerializer.INSTANCE.deserialize(inputView);\n\t\t\t\tdb.put(handle, keyAndNamespace, value);\n\t\t\t}\n\t\t} catch (EOFException e) {\n\t\t\t// expected\n\t\t}\n\t}",
            "1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908  \n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918  \n1919  \n1920  \n1921  \n1922  \n1923  \n1924  \n1925  \n1926  \n1927 +\n1928  \n1929  \n1930  \n1931  \n1932  \n1933  \n1934  \n1935  \n1936  \n1937  \n1938  \n1939  \n1940  \n1941  \n1942  \n1943  \n1944  \n1945  \n1946  \n1947  \n1948  \n1949  \n1950  \n1951  \n1952  \n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960  \n1961  \n1962  \n1963  \n1964  \n1965  \n1966  \n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975  \n1976  \n1977  \n1978  \n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986  ",
            "\t/**\n\t * For backwards compatibility, remove again later!\n\t *\n\t * @deprecated Internal method used for backwards compatibility.\n\t */\n\t@Deprecated\n\tprivate void restoreOldSavepointKeyedState(Collection<KeyedStateHandle> restoreState) throws Exception {\n\t\tcreateDB();\n\n\t\tPreconditions.checkState(1 == restoreState.size(), \"Only one element expected here.\");\n\n\t\tKeyedStateHandle keyedStateHandle = restoreState.iterator().next();\n\t\tif (!(keyedStateHandle instanceof MigrationKeyGroupStateHandle)) {\n\t\t\tthrow new IllegalStateException(\"Unexpected state handle type, \" +\n\t\t\t\t\t\"expected: \" + MigrationKeyGroupStateHandle.class +\n\t\t\t\t\t\", but found: \" + keyedStateHandle.getClass());\n\t\t}\n\n\t\tMigrationKeyGroupStateHandle keyGroupStateHandle = (MigrationKeyGroupStateHandle) keyedStateHandle;\n\n\t\tHashMap<String, RocksDBStateBackend.FinalFullyAsyncSnapshot> namedStates;\n\t\ttry (FSDataInputStream inputStream = keyGroupStateHandle.openInputStream()) {\n\t\t\tnamedStates = InstantiationUtil.deserializeObject(inputStream, userCodeClassLoader);\n\t\t}\n\n\t\tPreconditions.checkState(1 == namedStates.size(), \"Only one element expected here.\");\n\t\tDataInputView inputView = namedStates.values().iterator().next().stateHandle.getState(userCodeClassLoader);\n\n\t\t// clear k/v state information before filling it\n\t\tkvStateInformation.clear();\n\n\t\trestoredKvStateMetaInfos = new HashMap<>(namedStates.size());\n\n\t\t// first get the column family mapping\n\t\tint numColumns = inputView.readInt();\n\t\tMap<Byte, StateDescriptor<?, ?>> columnFamilyMapping = new HashMap<>(numColumns);\n\t\tfor (int i = 0; i < numColumns; i++) {\n\t\t\tbyte mappingByte = inputView.readByte();\n\n\t\t\tObjectInputStream ooIn =\n\t\t\t\t\tnew InstantiationUtil.ClassLoaderObjectInputStream(\n\t\t\t\t\t\t\tnew DataInputViewStream(inputView), userCodeClassLoader);\n\n\t\t\tStateDescriptor<?, ?> stateDescriptor = (StateDescriptor<?, ?>) ooIn.readObject();\n\n\t\t\tcolumnFamilyMapping.put(mappingByte, stateDescriptor);\n\n\t\t\t// mimic a restored kv state meta info\n\t\t\trestoredKvStateMetaInfos.put(\n\t\t\t\tstateDescriptor.getName(),\n\t\t\t\tnew RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\t\t\tstateDescriptor.getType(),\n\t\t\t\t\tstateDescriptor.getName(),\n\t\t\t\t\tMigrationNamespaceSerializerProxy.INSTANCE,\n\t\t\t\t\tstateDescriptor.getSerializer()).snapshot());\n\n\t\t\t// this will fill in the k/v state information\n\t\t\tgetColumnFamily(stateDescriptor, MigrationNamespaceSerializerProxy.INSTANCE);\n\t\t}\n\n\t\t// try and read until EOF\n\t\ttry {\n\t\t\t// the EOFException will get us out of this...\n\t\t\twhile (true) {\n\t\t\t\tbyte mappingByte = inputView.readByte();\n\t\t\t\tColumnFamilyHandle handle = getColumnFamily(\n\t\t\t\t\t\tcolumnFamilyMapping.get(mappingByte), MigrationNamespaceSerializerProxy.INSTANCE);\n\n\t\t\t\tbyte[] keyAndNamespace = BytePrimitiveArraySerializer.INSTANCE.deserialize(inputView);\n\n\t\t\t\tByteArrayInputStreamWithPos bis = new ByteArrayInputStreamWithPos(keyAndNamespace);\n\n\t\t\t\tK reconstructedKey = keySerializer.deserialize(new DataInputViewStreamWrapper(bis));\n\t\t\t\tint len = bis.getPosition();\n\n\t\t\t\tint keyGroup = (byte) KeyGroupRangeAssignment.assignToKeyGroup(reconstructedKey, numberOfKeyGroups);\n\n\t\t\t\tif (keyGroupPrefixBytes == 1) {\n\t\t\t\t\t// copy and override one byte (42) between key and namespace\n\t\t\t\t\tSystem.arraycopy(keyAndNamespace, 0, keyAndNamespace, 1, len);\n\t\t\t\t\tkeyAndNamespace[0] = (byte) keyGroup;\n\t\t\t\t} else {\n\t\t\t\t\tbyte[] largerKey = new byte[1 + keyAndNamespace.length];\n\n\t\t\t\t\t// write key-group\n\t\t\t\t\tlargerKey[0] = (byte) ((keyGroup >> 8) & 0xFF);\n\t\t\t\t\tlargerKey[1] = (byte) (keyGroup & 0xFF);\n\n\t\t\t\t\t// write key\n\t\t\t\t\tSystem.arraycopy(keyAndNamespace, 0, largerKey, 2, len);\n\n\t\t\t\t\t//skip one byte (42), write namespace\n\t\t\t\t\tSystem.arraycopy(keyAndNamespace, 1 + len, largerKey, 2 + len, keyAndNamespace.length - len - 1);\n\t\t\t\t\tkeyAndNamespace = largerKey;\n\t\t\t\t}\n\n\t\t\t\tbyte[] value = BytePrimitiveArraySerializer.INSTANCE.deserialize(inputView);\n\t\t\t\tdb.put(handle, keyAndNamespace, value);\n\t\t\t}\n\t\t} catch (EOFException e) {\n\t\t\t// expected\n\t\t}\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBIncrementalSnapshotOperation::materializeMetaData()",
            " 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813 -\n 814 -\n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  ",
            "\t\tprivate StreamStateHandle materializeMetaData() throws Exception {\n\t\t\tCheckpointStreamFactory.CheckpointStateOutputStream outputStream = null;\n\n\t\t\ttry {\n\t\t\t\toutputStream = checkpointStreamFactory\n\t\t\t\t\t.createCheckpointStateOutputStream(checkpointId, checkpointTimestamp);\n\t\t\t\tcloseableRegistry.registerClosable(outputStream);\n\n\t\t\t\tKeyedBackendSerializationProxy serializationProxy =\n\t\t\t\t\tnew KeyedBackendSerializationProxy(stateBackend.keySerializer, stateMetaInfoSnapshots);\n\t\t\t\tDataOutputView out = new DataOutputViewStreamWrapper(outputStream);\n\n\t\t\t\tserializationProxy.write(out);\n\n\t\t\t\tcloseableRegistry.unregisterClosable(outputStream);\n\t\t\t\tStreamStateHandle result = outputStream.closeAndGetHandle();\n\t\t\t\toutputStream = null;\n\n\t\t\t\treturn result;\n\t\t\t} finally {\n\t\t\t\tif (outputStream != null) {\n\t\t\t\t\tcloseableRegistry.unregisterClosable(outputStream);\n\t\t\t\t\toutputStream.close();\n\t\t\t\t}\n\t\t\t}\n\t\t}",
            " 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810 +\n 811 +\n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  ",
            "\t\tprivate StreamStateHandle materializeMetaData() throws Exception {\n\t\t\tCheckpointStreamFactory.CheckpointStateOutputStream outputStream = null;\n\n\t\t\ttry {\n\t\t\t\toutputStream = checkpointStreamFactory\n\t\t\t\t\t.createCheckpointStateOutputStream(checkpointId, checkpointTimestamp);\n\t\t\t\tcloseableRegistry.registerClosable(outputStream);\n\n\t\t\t\tKeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\t\tnew KeyedBackendSerializationProxy<>(stateBackend.keySerializer, stateMetaInfoSnapshots);\n\t\t\t\tDataOutputView out = new DataOutputViewStreamWrapper(outputStream);\n\n\t\t\t\tserializationProxy.write(out);\n\n\t\t\t\tcloseableRegistry.unregisterClosable(outputStream);\n\t\t\t\tStreamStateHandle result = outputStream.closeAndGetHandle();\n\t\t\t\toutputStream = null;\n\n\t\t\t\treturn result;\n\t\t\t} finally {\n\t\t\t\tif (outputStream != null) {\n\t\t\t\t\tcloseableRegistry.unregisterClosable(outputStream);\n\t\t\t\t\toutputStream.close();\n\t\t\t\t}\n\t\t\t}\n\t\t}"
        ],
        [
            "RocksDBKeyedStateBackend::restore(Collection)",
            " 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967 -\n 968  \n 969  \n 970 -\n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  ",
            "\t@Override\n\tpublic void restore(Collection<KeyedStateHandle> restoreState) throws Exception {\n\t\tLOG.info(\"Initializing RocksDB keyed state backend from snapshot.\");\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"Restoring snapshot from state handles: {}.\", restoreState);\n\t\t}\n\n\t\ttry {\n\t\t\tif (restoreState == null || restoreState.isEmpty()) {\n\t\t\t\tcreateDB();\n\t\t\t} else if (MigrationUtil.isOldSavepointKeyedState(restoreState)) {\n\t\t\t\tLOG.info(\"Converting RocksDB state from old savepoint.\");\n\t\t\t\trestoreOldSavepointKeyedState(restoreState);\n\t\t\t} else if (restoreState.iterator().next() instanceof IncrementalKeyedStateHandle) {\n\t\t\t\tRocksDBIncrementalRestoreOperation restoreOperation = new RocksDBIncrementalRestoreOperation(this);\n\t\t\t\trestoreOperation.restore(restoreState);\n\t\t\t} else {\n\t\t\t\tRocksDBFullRestoreOperation restoreOperation = new RocksDBFullRestoreOperation(this);\n\t\t\t\trestoreOperation.doRestore(restoreState);\n\t\t\t}\n\t\t} catch (Exception ex) {\n\t\t\tdispose();\n\t\t\tthrow ex;\n\t\t}\n\t}",
            " 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964 +\n 965  \n 966  \n 967 +\n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  ",
            "\t@Override\n\tpublic void restore(Collection<KeyedStateHandle> restoreState) throws Exception {\n\t\tLOG.info(\"Initializing RocksDB keyed state backend from snapshot.\");\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"Restoring snapshot from state handles: {}.\", restoreState);\n\t\t}\n\n\t\ttry {\n\t\t\tif (restoreState == null || restoreState.isEmpty()) {\n\t\t\t\tcreateDB();\n\t\t\t} else if (MigrationUtil.isOldSavepointKeyedState(restoreState)) {\n\t\t\t\tLOG.info(\"Converting RocksDB state from old savepoint.\");\n\t\t\t\trestoreOldSavepointKeyedState(restoreState);\n\t\t\t} else if (restoreState.iterator().next() instanceof IncrementalKeyedStateHandle) {\n\t\t\t\tRocksDBIncrementalRestoreOperation<K> restoreOperation = new RocksDBIncrementalRestoreOperation<>(this);\n\t\t\t\trestoreOperation.restore(restoreState);\n\t\t\t} else {\n\t\t\t\tRocksDBFullRestoreOperation<K> restoreOperation = new RocksDBFullRestoreOperation<>(this);\n\t\t\t\trestoreOperation.doRestore(restoreState);\n\t\t\t}\n\t\t} catch (Exception ex) {\n\t\t\tdispose();\n\t\t\tthrow ex;\n\t\t}\n\t}"
        ],
        [
            "SerializationProxiesTest::testKeyedBackendSerializationProxyRoundtripWithSerializerSerializationFailures()",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 -\n 107 -\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 -\n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "\t@Test\n\tpublic void testKeyedBackendSerializationProxyRoundtripWithSerializerSerializationFailures() throws Exception {\n\n\t\tTypeSerializer<?> keySerializer = IntSerializer.INSTANCE;\n\t\tTypeSerializer<?> namespaceSerializer = LongSerializer.INSTANCE;\n\t\tTypeSerializer<?> stateSerializer = DoubleSerializer.INSTANCE;\n\n\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> stateMetaInfoList = new ArrayList<>();\n\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"a\", namespaceSerializer, stateSerializer).snapshot());\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"b\", namespaceSerializer, stateSerializer).snapshot());\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"c\", namespaceSerializer, stateSerializer).snapshot());\n\n\t\tKeyedBackendSerializationProxy serializationProxy =\n\t\t\tnew KeyedBackendSerializationProxy(keySerializer, stateMetaInfoList);\n\n\t\tbyte[] serialized;\n\t\ttry (ByteArrayOutputStreamWithPos out = new ByteArrayOutputStreamWithPos()) {\n\t\t\tserializationProxy.write(new DataOutputViewStreamWrapper(out));\n\t\t\tserialized = out.toByteArray();\n\t\t}\n\n\t\tserializationProxy =\n\t\t\tnew KeyedBackendSerializationProxy(Thread.currentThread().getContextClassLoader());\n\n\t\t// mock failure when deserializing serializers\n\t\tTypeSerializerSerializationProxy<?> mockProxy = mock(TypeSerializerSerializationProxy.class);\n\t\tdoThrow(new IOException()).when(mockProxy).read(any(DataInputViewStreamWrapper.class));\n\t\tPowerMockito.whenNew(TypeSerializerSerializationProxy.class).withAnyArguments().thenReturn(mockProxy);\n\n\t\ttry (ByteArrayInputStreamWithPos in = new ByteArrayInputStreamWithPos(serialized)) {\n\t\t\tserializationProxy.read(new DataInputViewStreamWrapper(in));\n\t\t}\n\n\t\tAssert.assertEquals(null, serializationProxy.getKeySerializer());\n\t\tAssert.assertEquals(keySerializer.snapshotConfiguration(), serializationProxy.getKeySerializerConfigSnapshot());\n\n\t\tfor (RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?> meta : serializationProxy.getStateMetaInfoSnapshots()) {\n\t\t\tAssert.assertEquals(null, meta.getNamespaceSerializer());\n\t\t\tAssert.assertEquals(null, meta.getStateSerializer());\n\t\t\tAssert.assertEquals(namespaceSerializer.snapshotConfiguration(), meta.getNamespaceSerializerConfigSnapshot());\n\t\t\tAssert.assertEquals(stateSerializer.snapshotConfiguration(), meta.getStateSerializerConfigSnapshot());\n\t\t}\n\t}",
            "  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106 +\n 107 +\n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "\t@Test\n\tpublic void testKeyedBackendSerializationProxyRoundtripWithSerializerSerializationFailures() throws Exception {\n\n\t\tTypeSerializer<?> keySerializer = IntSerializer.INSTANCE;\n\t\tTypeSerializer<?> namespaceSerializer = LongSerializer.INSTANCE;\n\t\tTypeSerializer<?> stateSerializer = DoubleSerializer.INSTANCE;\n\n\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> stateMetaInfoList = new ArrayList<>();\n\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"a\", namespaceSerializer, stateSerializer).snapshot());\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"b\", namespaceSerializer, stateSerializer).snapshot());\n\t\tstateMetaInfoList.add(new RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\tStateDescriptor.Type.VALUE, \"c\", namespaceSerializer, stateSerializer).snapshot());\n\n\t\tKeyedBackendSerializationProxy<?> serializationProxy =\n\t\t\tnew KeyedBackendSerializationProxy<>(keySerializer, stateMetaInfoList);\n\n\t\tbyte[] serialized;\n\t\ttry (ByteArrayOutputStreamWithPos out = new ByteArrayOutputStreamWithPos()) {\n\t\t\tserializationProxy.write(new DataOutputViewStreamWrapper(out));\n\t\t\tserialized = out.toByteArray();\n\t\t}\n\n\t\tserializationProxy =\n\t\t\tnew KeyedBackendSerializationProxy<>(Thread.currentThread().getContextClassLoader());\n\n\t\t// mock failure when deserializing serializers\n\t\tTypeSerializerSerializationProxy<?> mockProxy = mock(TypeSerializerSerializationProxy.class);\n\t\tdoThrow(new IOException()).when(mockProxy).read(any(DataInputViewStreamWrapper.class));\n\t\tPowerMockito.whenNew(TypeSerializerSerializationProxy.class).withAnyArguments().thenReturn(mockProxy);\n\n\t\ttry (ByteArrayInputStreamWithPos in = new ByteArrayInputStreamWithPos(serialized)) {\n\t\t\tserializationProxy.read(new DataInputViewStreamWrapper(in));\n\t\t}\n\n\t\tAssert.assertEquals(null, serializationProxy.getKeySerializer());\n\t\tAssert.assertEquals(keySerializer.snapshotConfiguration(), serializationProxy.getKeySerializerConfigSnapshot());\n\n\t\tfor (RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?> meta : serializationProxy.getStateMetaInfoSnapshots()) {\n\t\t\tAssert.assertEquals(null, meta.getNamespaceSerializer());\n\t\t\tAssert.assertEquals(null, meta.getStateSerializer());\n\t\t\tAssert.assertEquals(namespaceSerializer.snapshotConfiguration(), meta.getNamespaceSerializerConfigSnapshot());\n\t\t\tAssert.assertEquals(stateSerializer.snapshotConfiguration(), meta.getStateSerializerConfigSnapshot());\n\t\t}\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBFullRestoreOperation::RocksDBFullRestoreOperation(RocksDBKeyedStateBackend)",
            "1053  \n1054  \n1055  \n1056  \n1057  \n1058 -\n1059  \n1060  ",
            "\t\t/**\n\t\t * Creates a restore operation object for the given state backend instance.\n\t\t *\n\t\t * @param rocksDBKeyedStateBackend the state backend into which we restore\n\t\t */\n\t\tpublic RocksDBFullRestoreOperation(RocksDBKeyedStateBackend<?> rocksDBKeyedStateBackend) {\n\t\t\tthis.rocksDBKeyedStateBackend = Preconditions.checkNotNull(rocksDBKeyedStateBackend);\n\t\t}",
            "1050  \n1051  \n1052  \n1053  \n1054  \n1055 +\n1056  \n1057  ",
            "\t\t/**\n\t\t * Creates a restore operation object for the given state backend instance.\n\t\t *\n\t\t * @param rocksDBKeyedStateBackend the state backend into which we restore\n\t\t */\n\t\tpublic RocksDBFullRestoreOperation(RocksDBKeyedStateBackend<K> rocksDBKeyedStateBackend) {\n\t\t\tthis.rocksDBKeyedStateBackend = Preconditions.checkNotNull(rocksDBKeyedStateBackend);\n\t\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBIncrementalRestoreOperation::readMetaData(StreamStateHandle)",
            "1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241  \n1242 -\n1243 -\n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253 -\n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  ",
            "\t\t@SuppressWarnings(\"unchecked\")\n\t\tprivate List<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> readMetaData(\n\t\t\t\tStreamStateHandle metaStateHandle) throws Exception {\n\n\t\t\tFSDataInputStream inputStream = null;\n\n\t\t\ttry {\n\t\t\t\tinputStream = metaStateHandle.openInputStream();\n\t\t\t\tstateBackend.cancelStreamRegistry.registerClosable(inputStream);\n\n\t\t\t\tKeyedBackendSerializationProxy serializationProxy =\n\t\t\t\t\tnew KeyedBackendSerializationProxy(stateBackend.userCodeClassLoader);\n\t\t\t\tDataInputView in = new DataInputViewStreamWrapper(inputStream);\n\t\t\t\tserializationProxy.read(in);\n\n\t\t\t\t// check for key serializer compatibility; this also reconfigures the\n\t\t\t\t// key serializer to be compatible, if it is required and is possible\n\t\t\t\tif (StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\t\tserializationProxy.getKeySerializer(),\n\t\t\t\t\t\tTypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer.class,\n\t\t\t\t\t\tserializationProxy.getKeySerializerConfigSnapshot(),\n\t\t\t\t\t\t(TypeSerializer) stateBackend.keySerializer)\n\t\t\t\t\t.isRequiresMigration()) {\n\n\t\t\t\t\t// TODO replace with state migration; note that key hash codes need to remain the same after migration\n\t\t\t\t\tthrow new RuntimeException(\"The new key serializer is not compatible to read previous keys. \" +\n\t\t\t\t\t\t\"Aborting now since state migration is currently not available\");\n\t\t\t\t}\n\n\t\t\t\treturn serializationProxy.getStateMetaInfoSnapshots();\n\t\t\t} finally {\n\t\t\t\tif (inputStream != null) {\n\t\t\t\t\tstateBackend.cancelStreamRegistry.unregisterClosable(inputStream);\n\t\t\t\t\tinputStream.close();\n\t\t\t\t}\n\t\t\t}\n\t\t}",
            "1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237 +\n1238 +\n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248 +\n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  ",
            "\t\tprivate List<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> readMetaData(\n\t\t\t\tStreamStateHandle metaStateHandle) throws Exception {\n\n\t\t\tFSDataInputStream inputStream = null;\n\n\t\t\ttry {\n\t\t\t\tinputStream = metaStateHandle.openInputStream();\n\t\t\t\tstateBackend.cancelStreamRegistry.registerClosable(inputStream);\n\n\t\t\t\tKeyedBackendSerializationProxy<T> serializationProxy =\n\t\t\t\t\tnew KeyedBackendSerializationProxy<>(stateBackend.userCodeClassLoader);\n\t\t\t\tDataInputView in = new DataInputViewStreamWrapper(inputStream);\n\t\t\t\tserializationProxy.read(in);\n\n\t\t\t\t// check for key serializer compatibility; this also reconfigures the\n\t\t\t\t// key serializer to be compatible, if it is required and is possible\n\t\t\t\tif (StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\t\tserializationProxy.getKeySerializer(),\n\t\t\t\t\t\tTypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer.class,\n\t\t\t\t\t\tserializationProxy.getKeySerializerConfigSnapshot(),\n\t\t\t\t\t\tstateBackend.keySerializer)\n\t\t\t\t\t.isRequiresMigration()) {\n\n\t\t\t\t\t// TODO replace with state migration; note that key hash codes need to remain the same after migration\n\t\t\t\t\tthrow new RuntimeException(\"The new key serializer is not compatible to read previous keys. \" +\n\t\t\t\t\t\t\"Aborting now since state migration is currently not available\");\n\t\t\t\t}\n\n\t\t\t\treturn serializationProxy.getStateMetaInfoSnapshots();\n\t\t\t} finally {\n\t\t\t\tif (inputStream != null) {\n\t\t\t\t\tstateBackend.cancelStreamRegistry.unregisterClosable(inputStream);\n\t\t\t\t\tinputStream.close();\n\t\t\t\t}\n\t\t\t}\n\t\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBFullRestoreOperation::restoreKVStateMetaData()",
            "1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120 -\n1121  \n1122 -\n1123 -\n1124  \n1125  \n1126  \n1127  \n1128  \n1129  \n1130  \n1131  \n1132  \n1133 -\n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  \n1173  \n1174  ",
            "\t\t/**\n\t\t * Restore the KV-state / ColumnFamily meta data for all key-groups referenced by the current state handle\n\t\t *\n\t\t * @throws IOException\n\t\t * @throws ClassNotFoundException\n\t\t * @throws RocksDBException\n\t\t */\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tprivate void restoreKVStateMetaData() throws IOException, ClassNotFoundException, RocksDBException {\n\n\t\t\tKeyedBackendSerializationProxy serializationProxy =\n\t\t\t\t\tnew KeyedBackendSerializationProxy(rocksDBKeyedStateBackend.userCodeClassLoader);\n\n\t\t\tserializationProxy.read(currentStateHandleInView);\n\n\t\t\t// check for key serializer compatibility; this also reconfigures the\n\t\t\t// key serializer to be compatible, if it is required and is possible\n\t\t\tif (StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\tserializationProxy.getKeySerializer(),\n\t\t\t\t\tTypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer.class,\n\t\t\t\t\tserializationProxy.getKeySerializerConfigSnapshot(),\n\t\t\t\t\t(TypeSerializer) rocksDBKeyedStateBackend.keySerializer)\n\t\t\t\t.isRequiresMigration()) {\n\n\t\t\t\t// TODO replace with state migration; note that key hash codes need to remain the same after migration\n\t\t\t\tthrow new RuntimeException(\"The new key serializer is not compatible to read previous keys. \" +\n\t\t\t\t\t\"Aborting now since state migration is currently not available\");\n\t\t\t}\n\n\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> restoredMetaInfos =\n\t\t\t\t\tserializationProxy.getStateMetaInfoSnapshots();\n\n\t\t\tcurrentStateHandleKVStateColumnFamilies = new ArrayList<>(restoredMetaInfos.size());\n\t\t\trocksDBKeyedStateBackend.restoredKvStateMetaInfos = new HashMap<>(restoredMetaInfos.size());\n\n\t\t\tfor (RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?> restoredMetaInfo : restoredMetaInfos) {\n\n\t\t\t\tif (!rocksDBKeyedStateBackend.kvStateInformation.containsKey(restoredMetaInfo.getName())) {\n\t\t\t\t\tColumnFamilyDescriptor columnFamilyDescriptor = new ColumnFamilyDescriptor(\n\t\t\t\t\t\trestoredMetaInfo.getName().getBytes(ConfigConstants.DEFAULT_CHARSET),\n\t\t\t\t\t\trocksDBKeyedStateBackend.columnOptions);\n\n\t\t\t\t\tRegisteredKeyedBackendStateMetaInfo<?, ?> stateMetaInfo =\n\t\t\t\t\t\t\tnew RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\t\t\t\t\t\trestoredMetaInfo.getStateType(),\n\t\t\t\t\t\t\t\trestoredMetaInfo.getName(),\n\t\t\t\t\t\t\t\trestoredMetaInfo.getNamespaceSerializer(),\n\t\t\t\t\t\t\t\trestoredMetaInfo.getStateSerializer());\n\n\t\t\t\t\trocksDBKeyedStateBackend.restoredKvStateMetaInfos.put(restoredMetaInfo.getName(), restoredMetaInfo);\n\n\t\t\t\t\tColumnFamilyHandle columnFamily = rocksDBKeyedStateBackend.db.createColumnFamily(columnFamilyDescriptor);\n\n\t\t\t\t\trocksDBKeyedStateBackend.kvStateInformation.put(\n\t\t\t\t\t\tstateMetaInfo.getName(),\n\t\t\t\t\t\tnew Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>>(columnFamily, stateMetaInfo));\n\n\t\t\t\t\tcurrentStateHandleKVStateColumnFamilies.add(columnFamily);\n\t\t\t\t} else {\n\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t}\n\t\t\t}\n\t\t}",
            "1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116 +\n1117  \n1118 +\n1119 +\n1120  \n1121  \n1122  \n1123  \n1124  \n1125  \n1126  \n1127  \n1128  \n1129 +\n1130  \n1131  \n1132  \n1133  \n1134  \n1135  \n1136  \n1137  \n1138  \n1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  ",
            "\t\t/**\n\t\t * Restore the KV-state / ColumnFamily meta data for all key-groups referenced by the current state handle\n\t\t *\n\t\t * @throws IOException\n\t\t * @throws ClassNotFoundException\n\t\t * @throws RocksDBException\n\t\t */\n\t\tprivate void restoreKVStateMetaData() throws IOException, RocksDBException {\n\n\t\t\tKeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\t\tnew KeyedBackendSerializationProxy<>(rocksDBKeyedStateBackend.userCodeClassLoader);\n\n\t\t\tserializationProxy.read(currentStateHandleInView);\n\n\t\t\t// check for key serializer compatibility; this also reconfigures the\n\t\t\t// key serializer to be compatible, if it is required and is possible\n\t\t\tif (StateMigrationUtil.resolveCompatibilityResult(\n\t\t\t\t\tserializationProxy.getKeySerializer(),\n\t\t\t\t\tTypeSerializerSerializationProxy.ClassNotFoundDummyTypeSerializer.class,\n\t\t\t\t\tserializationProxy.getKeySerializerConfigSnapshot(),\n\t\t\t\t\trocksDBKeyedStateBackend.keySerializer)\n\t\t\t\t.isRequiresMigration()) {\n\n\t\t\t\t// TODO replace with state migration; note that key hash codes need to remain the same after migration\n\t\t\t\tthrow new RuntimeException(\"The new key serializer is not compatible to read previous keys. \" +\n\t\t\t\t\t\"Aborting now since state migration is currently not available\");\n\t\t\t}\n\n\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> restoredMetaInfos =\n\t\t\t\t\tserializationProxy.getStateMetaInfoSnapshots();\n\n\t\t\tcurrentStateHandleKVStateColumnFamilies = new ArrayList<>(restoredMetaInfos.size());\n\t\t\trocksDBKeyedStateBackend.restoredKvStateMetaInfos = new HashMap<>(restoredMetaInfos.size());\n\n\t\t\tfor (RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?> restoredMetaInfo : restoredMetaInfos) {\n\n\t\t\t\tif (!rocksDBKeyedStateBackend.kvStateInformation.containsKey(restoredMetaInfo.getName())) {\n\t\t\t\t\tColumnFamilyDescriptor columnFamilyDescriptor = new ColumnFamilyDescriptor(\n\t\t\t\t\t\trestoredMetaInfo.getName().getBytes(ConfigConstants.DEFAULT_CHARSET),\n\t\t\t\t\t\trocksDBKeyedStateBackend.columnOptions);\n\n\t\t\t\t\tRegisteredKeyedBackendStateMetaInfo<?, ?> stateMetaInfo =\n\t\t\t\t\t\t\tnew RegisteredKeyedBackendStateMetaInfo<>(\n\t\t\t\t\t\t\t\trestoredMetaInfo.getStateType(),\n\t\t\t\t\t\t\t\trestoredMetaInfo.getName(),\n\t\t\t\t\t\t\t\trestoredMetaInfo.getNamespaceSerializer(),\n\t\t\t\t\t\t\t\trestoredMetaInfo.getStateSerializer());\n\n\t\t\t\t\trocksDBKeyedStateBackend.restoredKvStateMetaInfos.put(restoredMetaInfo.getName(), restoredMetaInfo);\n\n\t\t\t\t\tColumnFamilyHandle columnFamily = rocksDBKeyedStateBackend.db.createColumnFamily(columnFamilyDescriptor);\n\n\t\t\t\t\trocksDBKeyedStateBackend.kvStateInformation.put(\n\t\t\t\t\t\tstateMetaInfo.getName(),\n\t\t\t\t\t\tnew Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>>(columnFamily, stateMetaInfo));\n\n\t\t\t\t\tcurrentStateHandleKVStateColumnFamilies.add(columnFamily);\n\t\t\t\t} else {\n\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t}\n\t\t\t}\n\t\t}"
        ]
    ],
    "c131546eaadd07baf950bd6a44d07ee42d109e4c": [
        [
            "AkkaRpcActorTest::testPostStopExceptionPropagation()",
            " 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249 -\n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "\t/**\n\t * Tests that exception thrown in the postStop method are returned by the termination\n\t * future.\n\t */\n\t@Test\n\tpublic void testPostStopExceptionPropagation() throws Exception {\n\t\tFailingPostStopEndpoint rpcEndpoint = new FailingPostStopEndpoint(akkaRpcService, \"FailingPostStopEndpoint\");\n\t\trpcEndpoint.start();\n\n\t\trpcEndpoint.shutDown();\n\n\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n\n\t\ttry {\n\t\t\tterminationFuture.get();\n\t\t} catch (ExecutionException e) {\n\t\t\tassertTrue(e.getCause() instanceof FailingPostStopEndpoint.PostStopException);\n\t\t}\n\t}",
            " 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249 +\n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "\t/**\n\t * Tests that exception thrown in the postStop method are returned by the termination\n\t * future.\n\t */\n\t@Test\n\tpublic void testPostStopExceptionPropagation() throws Exception {\n\t\tFailingPostStopEndpoint rpcEndpoint = new FailingPostStopEndpoint(akkaRpcService, \"FailingPostStopEndpoint\");\n\t\trpcEndpoint.start();\n\n\t\trpcEndpoint.shutDown();\n\n\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n\n\t\ttry {\n\t\t\tterminationFuture.get();\n\t\t} catch (ExecutionException e) {\n\t\t\tassertTrue(e.getCause() instanceof FailingPostStopEndpoint.PostStopException);\n\t\t}\n\t}"
        ],
        [
            "AkkaInvocationHandler::getTerminationFuture()",
            " 343  \n 344 -\n 345  \n 346  ",
            "\t@Override\n\tpublic CompletableFuture<Boolean> getTerminationFuture() {\n\t\treturn terminationFuture;\n\t}",
            " 343  \n 344 +\n 345  \n 346  ",
            "\t@Override\n\tpublic CompletableFuture<Void> getTerminationFuture() {\n\t\treturn terminationFuture;\n\t}"
        ],
        [
            "JobManagerRunner::shutdownInternally()",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 -\n 229  \n 230  \n 231 -\n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  ",
            "\tprivate CompletableFuture<Void> shutdownInternally() {\n\t\tsynchronized (lock) {\n\t\t\tif (!shutdown) {\n\t\t\t\tshutdown = true;\n\n\t\t\t\tjobManager.shutDown();\n\n\t\t\t\tfinal CompletableFuture<Boolean> jobManagerTerminationFuture = jobManager.getTerminationFuture();\n\n\t\t\t\tjobManagerTerminationFuture.whenComplete(\n\t\t\t\t\t(Boolean ignored, Throwable throwable) -> {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tleaderElectionService.stop();\n\t\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(t, ExceptionUtils.stripCompletionException(throwable));\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// make all registered metrics go away\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tjobManagerMetricGroup.close();\n\t\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(t, throwable);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (throwable != null) {\n\t\t\t\t\t\t\tterminationFuture.completeExceptionally(\n\t\t\t\t\t\t\t\tnew FlinkException(\"Could not properly shut down the JobManagerRunner\", throwable));\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tterminationFuture.complete(null);\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\n\t\t\t\tterminationFuture.whenComplete(\n\t\t\t\t\t(Void ignored, Throwable throwable) -> {\n\t\t\t\t\t\tresultFuture.completeExceptionally(new JobNotFinishedException(jobGraph.getJobID()));\n\t\t\t\t\t});\n\t\t\t}\n\n\t\t\treturn terminationFuture;\n\t\t}\n\t}",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228 +\n 229  \n 230  \n 231 +\n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  ",
            "\tprivate CompletableFuture<Void> shutdownInternally() {\n\t\tsynchronized (lock) {\n\t\t\tif (!shutdown) {\n\t\t\t\tshutdown = true;\n\n\t\t\t\tjobManager.shutDown();\n\n\t\t\t\tfinal CompletableFuture<Void> jobManagerTerminationFuture = jobManager.getTerminationFuture();\n\n\t\t\t\tjobManagerTerminationFuture.whenComplete(\n\t\t\t\t\t(Void ignored, Throwable throwable) -> {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tleaderElectionService.stop();\n\t\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(t, ExceptionUtils.stripCompletionException(throwable));\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// make all registered metrics go away\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tjobManagerMetricGroup.close();\n\t\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(t, throwable);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (throwable != null) {\n\t\t\t\t\t\t\tterminationFuture.completeExceptionally(\n\t\t\t\t\t\t\t\tnew FlinkException(\"Could not properly shut down the JobManagerRunner\", throwable));\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tterminationFuture.complete(null);\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\n\t\t\t\tterminationFuture.whenComplete(\n\t\t\t\t\t(Void ignored, Throwable throwable) -> {\n\t\t\t\t\t\tresultFuture.completeExceptionally(new JobNotFinishedException(jobGraph.getJobID()));\n\t\t\t\t\t});\n\t\t\t}\n\n\t\t\treturn terminationFuture;\n\t\t}\n\t}"
        ],
        [
            "AkkaInvocationHandler::AkkaInvocationHandler(String,String,ActorRef,Time,long,CompletableFuture)",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "\tAkkaInvocationHandler(\n\t\t\tString address,\n\t\t\tString hostname,\n\t\t\tActorRef rpcEndpoint,\n\t\t\tTime timeout,\n\t\t\tlong maximumFramesize,\n\t\t\t@Nullable CompletableFuture<Boolean> terminationFuture) {\n\n\t\tthis.address = Preconditions.checkNotNull(address);\n\t\tthis.hostname = Preconditions.checkNotNull(hostname);\n\t\tthis.rpcEndpoint = Preconditions.checkNotNull(rpcEndpoint);\n\t\tthis.isLocal = this.rpcEndpoint.path().address().hasLocalScope();\n\t\tthis.timeout = Preconditions.checkNotNull(timeout);\n\t\tthis.maximumFramesize = maximumFramesize;\n\t\tthis.terminationFuture = terminationFuture;\n\t}",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 +\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "\tAkkaInvocationHandler(\n\t\t\tString address,\n\t\t\tString hostname,\n\t\t\tActorRef rpcEndpoint,\n\t\t\tTime timeout,\n\t\t\tlong maximumFramesize,\n\t\t\t@Nullable CompletableFuture<Void> terminationFuture) {\n\n\t\tthis.address = Preconditions.checkNotNull(address);\n\t\tthis.hostname = Preconditions.checkNotNull(hostname);\n\t\tthis.rpcEndpoint = Preconditions.checkNotNull(rpcEndpoint);\n\t\tthis.isLocal = this.rpcEndpoint.path().address().hasLocalScope();\n\t\tthis.timeout = Preconditions.checkNotNull(timeout);\n\t\tthis.maximumFramesize = maximumFramesize;\n\t\tthis.terminationFuture = terminationFuture;\n\t}"
        ],
        [
            "FencedAkkaInvocationHandler::FencedAkkaInvocationHandler(String,String,ActorRef,Time,long,CompletableFuture,Supplier)",
            "  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  ",
            "\tpublic FencedAkkaInvocationHandler(\n\t\t\tString address,\n\t\t\tString hostname,\n\t\t\tActorRef rpcEndpoint,\n\t\t\tTime timeout,\n\t\t\tlong maximumFramesize,\n\t\t\t@Nullable CompletableFuture<Boolean> terminationFuture,\n\t\t\tSupplier<F> fencingTokenSupplier) {\n\t\tsuper(address, hostname, rpcEndpoint, timeout, maximumFramesize, terminationFuture);\n\n\t\tthis.fencingTokenSupplier = Preconditions.checkNotNull(fencingTokenSupplier);\n\t}",
            "  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  ",
            "\tpublic FencedAkkaInvocationHandler(\n\t\t\tString address,\n\t\t\tString hostname,\n\t\t\tActorRef rpcEndpoint,\n\t\t\tTime timeout,\n\t\t\tlong maximumFramesize,\n\t\t\t@Nullable CompletableFuture<Void> terminationFuture,\n\t\t\tSupplier<F> fencingTokenSupplier) {\n\t\tsuper(address, hostname, rpcEndpoint, timeout, maximumFramesize, terminationFuture);\n\n\t\tthis.fencingTokenSupplier = Preconditions.checkNotNull(fencingTokenSupplier);\n\t}"
        ],
        [
            "ClusterEntrypoint::runCluster(Configuration)",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "\tprotected void runCluster(Configuration configuration) throws Exception {\n\t\tsynchronized (lock) {\n\t\t\tinitializeServices(configuration);\n\n\t\t\t// write host information into configuration\n\t\t\tconfiguration.setString(JobManagerOptions.ADDRESS, commonRpcService.getAddress());\n\t\t\tconfiguration.setInteger(JobManagerOptions.PORT, commonRpcService.getPort());\n\n\t\t\tstartClusterComponents(\n\t\t\t\tconfiguration,\n\t\t\t\tcommonRpcService,\n\t\t\t\thaServices,\n\t\t\t\tblobServer,\n\t\t\t\theartbeatServices,\n\t\t\t\tmetricRegistry);\n\n\t\t\t// TODO: Make shutDownAndTerminate non blocking to not use the global executor\n\t\t\tdispatcher.getTerminationFuture().whenCompleteAsync(\n\t\t\t\t(Boolean success, Throwable throwable) -> {\n\t\t\t\t\tif (throwable != null) {\n\t\t\t\t\t\tLOG.info(\"Could not properly terminate the Dispatcher.\", throwable);\n\t\t\t\t\t}\n\n\t\t\t\t\tshutDownAndTerminate(\n\t\t\t\t\t\tSUCCESS_RETURN_CODE,\n\t\t\t\t\t\tApplicationStatus.SUCCEEDED,\n\t\t\t\t\t\ttrue);\n\t\t\t\t});\n\t\t}\n\t}",
            " 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 +\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "\tprotected void runCluster(Configuration configuration) throws Exception {\n\t\tsynchronized (lock) {\n\t\t\tinitializeServices(configuration);\n\n\t\t\t// write host information into configuration\n\t\t\tconfiguration.setString(JobManagerOptions.ADDRESS, commonRpcService.getAddress());\n\t\t\tconfiguration.setInteger(JobManagerOptions.PORT, commonRpcService.getPort());\n\n\t\t\tstartClusterComponents(\n\t\t\t\tconfiguration,\n\t\t\t\tcommonRpcService,\n\t\t\t\thaServices,\n\t\t\t\tblobServer,\n\t\t\t\theartbeatServices,\n\t\t\t\tmetricRegistry);\n\n\t\t\t// TODO: Make shutDownAndTerminate non blocking to not use the global executor\n\t\t\tdispatcher.getTerminationFuture().whenCompleteAsync(\n\t\t\t\t(Void value, Throwable throwable) -> {\n\t\t\t\t\tif (throwable != null) {\n\t\t\t\t\t\tLOG.info(\"Could not properly terminate the Dispatcher.\", throwable);\n\t\t\t\t\t}\n\n\t\t\t\t\tshutDownAndTerminate(\n\t\t\t\t\t\tSUCCESS_RETURN_CODE,\n\t\t\t\t\t\tApplicationStatus.SUCCEEDED,\n\t\t\t\t\t\ttrue);\n\t\t\t\t});\n\t\t}\n\t}"
        ],
        [
            "AkkaRpcActorTest::testActorTerminationWhenServiceShutdown()",
            " 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288 -\n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  ",
            "\t/**\n\t * Tests that actors are properly terminated when the AkkaRpcService is shut down.\n\t */\n\t@Test\n\tpublic void testActorTerminationWhenServiceShutdown() throws Exception {\n\t\tfinal ActorSystem rpcActorSystem = AkkaUtils.createDefaultActorSystem();\n\t\tfinal RpcService rpcService = new AkkaRpcService(rpcActorSystem, timeout);\n\n\t\ttry {\n\t\t\tSimpleRpcEndpoint rpcEndpoint = new SimpleRpcEndpoint(rpcService, SimpleRpcEndpoint.class.getSimpleName());\n\n\t\t\trpcEndpoint.start();\n\n\t\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n\n\t\t\trpcService.stopService();\n\n\t\t\tterminationFuture.get(timeout.toMilliseconds(), TimeUnit.MILLISECONDS);\n\t\t} finally {\n\t\t\trpcActorSystem.shutdown();\n\t\t\trpcActorSystem.awaitTermination(FutureUtils.toFiniteDuration(timeout));\n\t\t}\n\t}",
            " 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288 +\n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  ",
            "\t/**\n\t * Tests that actors are properly terminated when the AkkaRpcService is shut down.\n\t */\n\t@Test\n\tpublic void testActorTerminationWhenServiceShutdown() throws Exception {\n\t\tfinal ActorSystem rpcActorSystem = AkkaUtils.createDefaultActorSystem();\n\t\tfinal RpcService rpcService = new AkkaRpcService(rpcActorSystem, timeout);\n\n\t\ttry {\n\t\t\tSimpleRpcEndpoint rpcEndpoint = new SimpleRpcEndpoint(rpcService, SimpleRpcEndpoint.class.getSimpleName());\n\n\t\t\trpcEndpoint.start();\n\n\t\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n\n\t\t\trpcService.stopService();\n\n\t\t\tterminationFuture.get(timeout.toMilliseconds(), TimeUnit.MILLISECONDS);\n\t\t} finally {\n\t\t\trpcActorSystem.shutdown();\n\t\t\trpcActorSystem.awaitTermination(FutureUtils.toFiniteDuration(timeout));\n\t\t}\n\t}"
        ],
        [
            "RpcEndpoint::getTerminationFuture()",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231 -\n 232  \n 233  ",
            "\t/**\n\t * Return a future which is completed with true when the rpc endpoint has been terminated.\n\t * In case of a failure, this future is completed with the occurring exception.\n\t *\n\t * @return Future which is completed when the rpc endpoint has been terminated.\n\t */\n\tpublic CompletableFuture<Boolean> getTerminationFuture() {\n\t\treturn rpcServer.getTerminationFuture();\n\t}",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231 +\n 232  \n 233  ",
            "\t/**\n\t * Return a future which is completed with true when the rpc endpoint has been terminated.\n\t * In case of a failure, this future is completed with the occurring exception.\n\t *\n\t * @return Future which is completed when the rpc endpoint has been terminated.\n\t */\n\tpublic CompletableFuture<Void> getTerminationFuture() {\n\t\treturn rpcServer.getTerminationFuture();\n\t}"
        ],
        [
            "JobMaster::postStop()",
            " 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410 -\n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  ",
            "\t/**\n\t * Suspend the job and shutdown all other services including rpc.\n\t */\n\t@Override\n\tpublic void postStop() throws Exception {\n\t\tlog.info(\"Stopping the JobMaster for job \" + jobGraph.getName() + '(' + jobGraph.getJobID() + \").\");\n\n\t\t// disconnect from all registered TaskExecutors\n\t\tfinal Set<ResourceID> taskManagerResourceIds = new HashSet<>(registeredTaskManagers.keySet());\n\t\tfinal FlinkException cause = new FlinkException(\"Stopping JobMaster for job \" + jobGraph.getName() +\n\t\t\t'(' + jobGraph.getJobID() + \").\");\n\n\t\tfor (ResourceID taskManagerResourceId : taskManagerResourceIds) {\n\t\t\tdisconnectTaskManager(taskManagerResourceId, cause);\n\t\t}\n\n\t\ttaskManagerHeartbeatManager.stop();\n\t\tresourceManagerHeartbeatManager.stop();\n\n\t\t// make sure there is a graceful exit\n\t\tsuspendExecution(new FlinkException(\"JobManager is shutting down.\"));\n\n\t\t// shut down will internally release all registered slots\n\t\tslotPool.shutDown();\n\t\tCompletableFuture<Boolean> terminationFuture = slotPool.getTerminationFuture();\n\n\t\tException exception = null;\n\n\t\t// wait for the slot pool shut down\n\t\ttry {\n\t\t\tterminationFuture.get(rpcTimeout.toMilliseconds(), TimeUnit.MILLISECONDS);\n\t\t} catch (Exception e) {\n\t\t\texception = e;\n\t\t}\n\n\t\ttry {\n\t\t\tsuper.postStop();\n\t\t} catch (Exception e) {\n\t\t\texception = ExceptionUtils.firstOrSuppressed(e, exception);\n\t\t}\n\n\t\tif (exception != null) {\n\t\t\tthrow exception;\n\t\t}\n\n\t\tlog.info(\"Stopped the JobMaster for job \" + jobGraph.getName() + '(' + jobGraph.getJobID() + \").\");\n\t}",
            " 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410 +\n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  ",
            "\t/**\n\t * Suspend the job and shutdown all other services including rpc.\n\t */\n\t@Override\n\tpublic void postStop() throws Exception {\n\t\tlog.info(\"Stopping the JobMaster for job \" + jobGraph.getName() + '(' + jobGraph.getJobID() + \").\");\n\n\t\t// disconnect from all registered TaskExecutors\n\t\tfinal Set<ResourceID> taskManagerResourceIds = new HashSet<>(registeredTaskManagers.keySet());\n\t\tfinal FlinkException cause = new FlinkException(\"Stopping JobMaster for job \" + jobGraph.getName() +\n\t\t\t'(' + jobGraph.getJobID() + \").\");\n\n\t\tfor (ResourceID taskManagerResourceId : taskManagerResourceIds) {\n\t\t\tdisconnectTaskManager(taskManagerResourceId, cause);\n\t\t}\n\n\t\ttaskManagerHeartbeatManager.stop();\n\t\tresourceManagerHeartbeatManager.stop();\n\n\t\t// make sure there is a graceful exit\n\t\tsuspendExecution(new FlinkException(\"JobManager is shutting down.\"));\n\n\t\t// shut down will internally release all registered slots\n\t\tslotPool.shutDown();\n\t\tCompletableFuture<Void> terminationFuture = slotPool.getTerminationFuture();\n\n\t\tException exception = null;\n\n\t\t// wait for the slot pool shut down\n\t\ttry {\n\t\t\tterminationFuture.get(rpcTimeout.toMilliseconds(), TimeUnit.MILLISECONDS);\n\t\t} catch (Exception e) {\n\t\t\texception = e;\n\t\t}\n\n\t\ttry {\n\t\t\tsuper.postStop();\n\t\t} catch (Exception e) {\n\t\t\texception = ExceptionUtils.firstOrSuppressed(e, exception);\n\t\t}\n\n\t\tif (exception != null) {\n\t\t\tthrow exception;\n\t\t}\n\n\t\tlog.info(\"Stopped the JobMaster for job \" + jobGraph.getName() + '(' + jobGraph.getJobID() + \").\");\n\t}"
        ],
        [
            "AkkaRpcService::startServer(C)",
            " 194  \n 195  \n 196  \n 197  \n 198 -\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  ",
            "\t@Override\n\tpublic <C extends RpcEndpoint & RpcGateway> RpcServer startServer(C rpcEndpoint) {\n\t\tcheckNotNull(rpcEndpoint, \"rpc endpoint\");\n\n\t\tCompletableFuture<Boolean> terminationFuture = new CompletableFuture<>();\n\t\tfinal Props akkaRpcActorProps;\n\n\t\tif (rpcEndpoint instanceof FencedRpcEndpoint) {\n\t\t\takkaRpcActorProps = Props.create(FencedAkkaRpcActor.class, rpcEndpoint, terminationFuture);\n\t\t} else {\n\t\t\takkaRpcActorProps = Props.create(AkkaRpcActor.class, rpcEndpoint, terminationFuture);\n\t\t}\n\n\t\tActorRef actorRef;\n\n\t\tsynchronized (lock) {\n\t\t\tcheckState(!stopped, \"RpcService is stopped\");\n\t\t\tactorRef = actorSystem.actorOf(akkaRpcActorProps, rpcEndpoint.getEndpointId());\n\t\t\tactors.put(actorRef, rpcEndpoint);\n\t\t}\n\n\t\tLOG.info(\"Starting RPC endpoint for {} at {} .\", rpcEndpoint.getClass().getName(), actorRef.path());\n\n\t\tfinal String akkaAddress = AkkaUtils.getAkkaURL(actorSystem, actorRef);\n\t\tfinal String hostname;\n\t\tOption<String> host = actorRef.path().address().host();\n\t\tif (host.isEmpty()) {\n\t\t\thostname = \"localhost\";\n\t\t} else {\n\t\t\thostname = host.get();\n\t\t}\n\n\t\tSet<Class<?>> implementedRpcGateways = new HashSet<>(RpcUtils.extractImplementedRpcGateways(rpcEndpoint.getClass()));\n\n\t\timplementedRpcGateways.add(RpcServer.class);\n\t\timplementedRpcGateways.add(AkkaBasedEndpoint.class);\n\n\t\tfinal InvocationHandler akkaInvocationHandler;\n\n\t\tif (rpcEndpoint instanceof FencedRpcEndpoint) {\n\t\t\t// a FencedRpcEndpoint needs a FencedAkkaInvocationHandler\n\t\t\takkaInvocationHandler = new FencedAkkaInvocationHandler<>(\n\t\t\t\takkaAddress,\n\t\t\t\thostname,\n\t\t\t\tactorRef,\n\t\t\t\ttimeout,\n\t\t\t\tmaximumFramesize,\n\t\t\t\tterminationFuture,\n\t\t\t\t((FencedRpcEndpoint<?>) rpcEndpoint)::getFencingToken);\n\n\t\t\timplementedRpcGateways.add(FencedMainThreadExecutable.class);\n\t\t} else {\n\t\t\takkaInvocationHandler = new AkkaInvocationHandler(\n\t\t\t\takkaAddress,\n\t\t\t\thostname,\n\t\t\t\tactorRef,\n\t\t\t\ttimeout,\n\t\t\t\tmaximumFramesize,\n\t\t\t\tterminationFuture);\n\t\t}\n\n\t\t// Rather than using the System ClassLoader directly, we derive the ClassLoader\n\t\t// from this class . That works better in cases where Flink runs embedded and all Flink\n\t\t// code is loaded dynamically (for example from an OSGI bundle) through a custom ClassLoader\n\t\tClassLoader classLoader = getClass().getClassLoader();\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tRpcServer server = (RpcServer) Proxy.newProxyInstance(\n\t\t\tclassLoader,\n\t\t\timplementedRpcGateways.toArray(new Class<?>[implementedRpcGateways.size()]),\n\t\t\takkaInvocationHandler);\n\n\t\treturn server;\n\t}",
            " 194  \n 195  \n 196  \n 197  \n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  ",
            "\t@Override\n\tpublic <C extends RpcEndpoint & RpcGateway> RpcServer startServer(C rpcEndpoint) {\n\t\tcheckNotNull(rpcEndpoint, \"rpc endpoint\");\n\n\t\tCompletableFuture<Void> terminationFuture = new CompletableFuture<>();\n\t\tfinal Props akkaRpcActorProps;\n\n\t\tif (rpcEndpoint instanceof FencedRpcEndpoint) {\n\t\t\takkaRpcActorProps = Props.create(FencedAkkaRpcActor.class, rpcEndpoint, terminationFuture);\n\t\t} else {\n\t\t\takkaRpcActorProps = Props.create(AkkaRpcActor.class, rpcEndpoint, terminationFuture);\n\t\t}\n\n\t\tActorRef actorRef;\n\n\t\tsynchronized (lock) {\n\t\t\tcheckState(!stopped, \"RpcService is stopped\");\n\t\t\tactorRef = actorSystem.actorOf(akkaRpcActorProps, rpcEndpoint.getEndpointId());\n\t\t\tactors.put(actorRef, rpcEndpoint);\n\t\t}\n\n\t\tLOG.info(\"Starting RPC endpoint for {} at {} .\", rpcEndpoint.getClass().getName(), actorRef.path());\n\n\t\tfinal String akkaAddress = AkkaUtils.getAkkaURL(actorSystem, actorRef);\n\t\tfinal String hostname;\n\t\tOption<String> host = actorRef.path().address().host();\n\t\tif (host.isEmpty()) {\n\t\t\thostname = \"localhost\";\n\t\t} else {\n\t\t\thostname = host.get();\n\t\t}\n\n\t\tSet<Class<?>> implementedRpcGateways = new HashSet<>(RpcUtils.extractImplementedRpcGateways(rpcEndpoint.getClass()));\n\n\t\timplementedRpcGateways.add(RpcServer.class);\n\t\timplementedRpcGateways.add(AkkaBasedEndpoint.class);\n\n\t\tfinal InvocationHandler akkaInvocationHandler;\n\n\t\tif (rpcEndpoint instanceof FencedRpcEndpoint) {\n\t\t\t// a FencedRpcEndpoint needs a FencedAkkaInvocationHandler\n\t\t\takkaInvocationHandler = new FencedAkkaInvocationHandler<>(\n\t\t\t\takkaAddress,\n\t\t\t\thostname,\n\t\t\t\tactorRef,\n\t\t\t\ttimeout,\n\t\t\t\tmaximumFramesize,\n\t\t\t\tterminationFuture,\n\t\t\t\t((FencedRpcEndpoint<?>) rpcEndpoint)::getFencingToken);\n\n\t\t\timplementedRpcGateways.add(FencedMainThreadExecutable.class);\n\t\t} else {\n\t\t\takkaInvocationHandler = new AkkaInvocationHandler(\n\t\t\t\takkaAddress,\n\t\t\t\thostname,\n\t\t\t\tactorRef,\n\t\t\t\ttimeout,\n\t\t\t\tmaximumFramesize,\n\t\t\t\tterminationFuture);\n\t\t}\n\n\t\t// Rather than using the System ClassLoader directly, we derive the ClassLoader\n\t\t// from this class . That works better in cases where Flink runs embedded and all Flink\n\t\t// code is loaded dynamically (for example from an OSGI bundle) through a custom ClassLoader\n\t\tClassLoader classLoader = getClass().getClassLoader();\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tRpcServer server = (RpcServer) Proxy.newProxyInstance(\n\t\t\tclassLoader,\n\t\t\timplementedRpcGateways.toArray(new Class<?>[implementedRpcGateways.size()]),\n\t\t\takkaInvocationHandler);\n\n\t\treturn server;\n\t}"
        ],
        [
            "AkkaRpcActorTest::testPostStopExecutedByMainThread()",
            " 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 -\n 269  \n 270  \n 271  \n 272  \n 273  ",
            "\t/**\n\t * Checks that the postStop callback is executed within the main thread.\n\t */\n\t@Test\n\tpublic void testPostStopExecutedByMainThread() throws Exception {\n\t\tSimpleRpcEndpoint simpleRpcEndpoint = new SimpleRpcEndpoint(akkaRpcService, \"SimpleRpcEndpoint\");\n\t\tsimpleRpcEndpoint.start();\n\n\t\tsimpleRpcEndpoint.shutDown();\n\n\t\tCompletableFuture<Boolean> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n\n\t\t// check that we executed the postStop method in the main thread, otherwise an exception\n\t\t// would be thrown here.\n\t\tterminationFuture.get();\n\t}",
            " 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268 +\n 269  \n 270  \n 271  \n 272  \n 273  ",
            "\t/**\n\t * Checks that the postStop callback is executed within the main thread.\n\t */\n\t@Test\n\tpublic void testPostStopExecutedByMainThread() throws Exception {\n\t\tSimpleRpcEndpoint simpleRpcEndpoint = new SimpleRpcEndpoint(akkaRpcService, \"SimpleRpcEndpoint\");\n\t\tsimpleRpcEndpoint.start();\n\n\t\tsimpleRpcEndpoint.shutDown();\n\n\t\tCompletableFuture<Void> terminationFuture = simpleRpcEndpoint.getTerminationFuture();\n\n\t\t// check that we executed the postStop method in the main thread, otherwise an exception\n\t\t// would be thrown here.\n\t\tterminationFuture.get();\n\t}"
        ],
        [
            "MiniDispatcherTest::testJobResultRetrieval()",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "\t/**\n\t * Tests that the {@link MiniDispatcher} only terminates in {@link ClusterEntrypoint.ExecutionMode#NORMAL}\n\t * after it has served the {@link org.apache.flink.runtime.jobmaster.JobResult} once.\n\t */\n\t@Test\n\tpublic void testJobResultRetrieval() throws Exception {\n\t\tfinal MiniDispatcher miniDispatcher = createMiniDispatcher(ClusterEntrypoint.ExecutionMode.NORMAL);\n\n\t\tminiDispatcher.start();\n\n\t\ttry {\n\t\t\t// wait until the Dispatcher is the leader\n\t\t\tdispatcherLeaderElectionService.isLeader(UUID.randomUUID()).get();\n\n\t\t\t// wait until we have submitted the job\n\t\t\tjobGraphFuture.get();\n\n\t\t\tresultFuture.complete(archivedExecutionGraph);\n\n\t\t\tfinal CompletableFuture<Boolean> terminationFuture = miniDispatcher.getTerminationFuture();\n\n\t\t\tassertThat(terminationFuture.isDone(), is(false));\n\n\t\t\tfinal DispatcherGateway dispatcherGateway = miniDispatcher.getSelfGateway(DispatcherGateway.class);\n\n\t\t\tfinal CompletableFuture<JobResult> jobResultFuture = dispatcherGateway.requestJobResult(jobGraph.getJobID(), timeout);\n\n\t\t\tfinal JobResult jobResult = jobResultFuture.get();\n\n\t\t\tassertThat(jobResult.getJobId(), is(jobGraph.getJobID()));\n\n\t\t\tterminationFuture.get();\n\t\t} finally {\n\t\t\tRpcUtils.terminateRpcEndpoint(miniDispatcher, timeout);\n\t\t}\n\t}",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  ",
            "\t/**\n\t * Tests that the {@link MiniDispatcher} only terminates in {@link ClusterEntrypoint.ExecutionMode#NORMAL}\n\t * after it has served the {@link org.apache.flink.runtime.jobmaster.JobResult} once.\n\t */\n\t@Test\n\tpublic void testJobResultRetrieval() throws Exception {\n\t\tfinal MiniDispatcher miniDispatcher = createMiniDispatcher(ClusterEntrypoint.ExecutionMode.NORMAL);\n\n\t\tminiDispatcher.start();\n\n\t\ttry {\n\t\t\t// wait until the Dispatcher is the leader\n\t\t\tdispatcherLeaderElectionService.isLeader(UUID.randomUUID()).get();\n\n\t\t\t// wait until we have submitted the job\n\t\t\tjobGraphFuture.get();\n\n\t\t\tresultFuture.complete(archivedExecutionGraph);\n\n\t\t\tfinal CompletableFuture<Void> terminationFuture = miniDispatcher.getTerminationFuture();\n\n\t\t\tassertThat(terminationFuture.isDone(), is(false));\n\n\t\t\tfinal DispatcherGateway dispatcherGateway = miniDispatcher.getSelfGateway(DispatcherGateway.class);\n\n\t\t\tfinal CompletableFuture<JobResult> jobResultFuture = dispatcherGateway.requestJobResult(jobGraph.getJobID(), timeout);\n\n\t\t\tfinal JobResult jobResult = jobResultFuture.get();\n\n\t\t\tassertThat(jobResult.getJobId(), is(jobGraph.getJobID()));\n\n\t\t\tterminationFuture.get();\n\t\t} finally {\n\t\t\tRpcUtils.terminateRpcEndpoint(miniDispatcher, timeout);\n\t\t}\n\t}"
        ],
        [
            "TaskManagerRunner::getTerminationFuture()",
            " 201 -\n 202  \n 203  ",
            "\tpublic CompletableFuture<Boolean> getTerminationFuture() {\n\t\treturn taskManager.getTerminationFuture();\n\t}",
            " 201 +\n 202  \n 203  ",
            "\tpublic CompletableFuture<Void> getTerminationFuture() {\n\t\treturn taskManager.getTerminationFuture();\n\t}"
        ],
        [
            "AkkaRpcActorTest::testRpcEndpointTerminationFuture()",
            " 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 -\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  ",
            "\t/**\n\t * Tests that we can wait for a RpcEndpoint to terminate.\n\t *\n\t * @throws ExecutionException\n\t * @throws InterruptedException\n\t */\n\t@Test(timeout=5000)\n\tpublic void testRpcEndpointTerminationFuture() throws Exception {\n\t\tfinal DummyRpcEndpoint rpcEndpoint = new DummyRpcEndpoint(akkaRpcService);\n\t\trpcEndpoint.start();\n\n\t\tCompletableFuture<Boolean> terminationFuture = rpcEndpoint.getTerminationFuture();\n\n\t\tassertFalse(terminationFuture.isDone());\n\n\t\tCompletableFuture.runAsync(\n\t\t\t() -> rpcEndpoint.shutDown(),\n\t\t\tactorSystem.dispatcher());\n\n\t\t// wait until the rpc endpoint has terminated\n\t\tterminationFuture.get();\n\t}",
            " 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 +\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  ",
            "\t/**\n\t * Tests that we can wait for a RpcEndpoint to terminate.\n\t *\n\t * @throws ExecutionException\n\t * @throws InterruptedException\n\t */\n\t@Test(timeout=5000)\n\tpublic void testRpcEndpointTerminationFuture() throws Exception {\n\t\tfinal DummyRpcEndpoint rpcEndpoint = new DummyRpcEndpoint(akkaRpcService);\n\t\trpcEndpoint.start();\n\n\t\tCompletableFuture<Void> terminationFuture = rpcEndpoint.getTerminationFuture();\n\n\t\tassertFalse(terminationFuture.isDone());\n\n\t\tCompletableFuture.runAsync(\n\t\t\t() -> rpcEndpoint.shutDown(),\n\t\t\tactorSystem.dispatcher());\n\n\t\t// wait until the rpc endpoint has terminated\n\t\tterminationFuture.get();\n\t}"
        ]
    ],
    "107c8e04be86e9fd893a5c9e0f9c528d1453c3de": [
        [
            "SlotPool::internalAllocateSlot(SlotRequestId,ScheduledUnit,ResourceProfile,Collection,boolean,Time)",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 -\n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "\tprivate CompletableFuture<LogicalSlot> internalAllocateSlot(\n\t\t\tSlotRequestId slotRequestId,\n\t\t\tScheduledUnit task,\n\t\t\tResourceProfile resourceProfile,\n\t\t\tCollection<TaskManagerLocation> locationPreferences,\n\t\t\tboolean allowQueuedScheduling,\n\t\t\tTime allocationTimeout) {\n\n\t\tfinal SlotSharingGroupId slotSharingGroupId = task.getSlotSharingGroupId();\n\n\t\tif (slotSharingGroupId != null) {\n\t\t\t// allocate slot with slot sharing\n\t\t\tfinal SlotSharingManager multiTaskSlotManager = slotSharingManagers.computeIfAbsent(\n\t\t\t\tslotSharingGroupId,\n\t\t\t\tid -> new SlotSharingManager(\n\t\t\t\t\tid,\n\t\t\t\t\tthis,\n\t\t\t\t\tproviderAndOwner));\n\n\t\t\tfinal SlotSharingManager.MultiTaskSlotLocality multiTaskSlotLocality;\n\n\t\t\ttry {\n\t\t\t\tif (task.getCoLocationConstraint() != null) {\n\t\t\t\t\tmultiTaskSlotLocality = allocateCoLocatedMultiTaskSlot(\n\t\t\t\t\t\ttask.getCoLocationConstraint(),\n\t\t\t\t\t\tmultiTaskSlotManager,\n\t\t\t\t\t\tresourceProfile,\n\t\t\t\t\t\tlocationPreferences,\n\t\t\t\t\t\tallowQueuedScheduling,\n\t\t\t\t\t\tallocationTimeout);\n\t\t\t\t} else {\n\t\t\t\t\tmultiTaskSlotLocality = allocateMultiTaskSlot(\n\t\t\t\t\t\ttask.getJobVertexId(), multiTaskSlotManager,\n\t\t\t\t\t\tresourceProfile,\n\t\t\t\t\t\tlocationPreferences,\n\t\t\t\t\t\tallowQueuedScheduling,\n\t\t\t\t\t\tallocationTimeout);\n\t\t\t\t}\n\t\t\t} catch (NoResourceAvailableException noResourceException) {\n\t\t\t\treturn FutureUtils.completedExceptionally(noResourceException);\n\t\t\t}\n\n\t\t\t// sanity check\n\t\t\tPreconditions.checkState(!multiTaskSlotLocality.getMultiTaskSlot().contains(task.getJobVertexId()));\n\n\t\t\tfinal SlotSharingManager.SingleTaskSlot leaf = multiTaskSlotLocality.getMultiTaskSlot().allocateSingleTaskSlot(\n\t\t\t\tslotRequestId,\n\t\t\t\ttask.getJobVertexId(),\n\t\t\t\tmultiTaskSlotLocality.getLocality());\n\n\t\t\treturn leaf.getLogicalSlotFuture();\n\t\t} else {\n\t\t\t// request an allocated slot to assign a single logical slot to\n\t\t\tCompletableFuture<SlotAndLocality> slotAndLocalityFuture = requestAllocatedSlot(\n\t\t\t\tslotRequestId,\n\t\t\t\tresourceProfile,\n\t\t\t\tlocationPreferences,\n\t\t\t\tallowQueuedScheduling,\n\t\t\t\tallocationTimeout);\n\n\t\t\treturn slotAndLocalityFuture.thenApply(\n\t\t\t\t(SlotAndLocality slotAndLocality) -> {\n\t\t\t\t\tfinal AllocatedSlot allocatedSlot = slotAndLocality.getSlot();\n\n\t\t\t\t\tfinal SingleLogicalSlot singleTaskSlot = new SingleLogicalSlot(\n\t\t\t\t\t\tslotRequestId,\n\t\t\t\t\t\tallocatedSlot,\n\t\t\t\t\t\tnull,\n\t\t\t\t\t\tslotAndLocality.getLocality(),\n\t\t\t\t\t\tproviderAndOwner);\n\n\t\t\t\t\tif (allocatedSlot.tryAssignPayload(singleTaskSlot)) {\n\t\t\t\t\t\treturn singleTaskSlot;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfinal FlinkException flinkException = new FlinkException(\"Could not assign payload to allocated slot \" + allocatedSlot.getAllocationId() + '.');\n\t\t\t\t\t\treleaseSlot(slotRequestId, null, flinkException);\n\t\t\t\t\t\tthrow new CompletionException(flinkException);\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t}\n\t}",
            " 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344 +\n 345 +\n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  ",
            "\tprivate CompletableFuture<LogicalSlot> internalAllocateSlot(\n\t\t\tSlotRequestId slotRequestId,\n\t\t\tScheduledUnit task,\n\t\t\tResourceProfile resourceProfile,\n\t\t\tCollection<TaskManagerLocation> locationPreferences,\n\t\t\tboolean allowQueuedScheduling,\n\t\t\tTime allocationTimeout) {\n\n\t\tfinal SlotSharingGroupId slotSharingGroupId = task.getSlotSharingGroupId();\n\n\t\tif (slotSharingGroupId != null) {\n\t\t\t// allocate slot with slot sharing\n\t\t\tfinal SlotSharingManager multiTaskSlotManager = slotSharingManagers.computeIfAbsent(\n\t\t\t\tslotSharingGroupId,\n\t\t\t\tid -> new SlotSharingManager(\n\t\t\t\t\tid,\n\t\t\t\t\tthis,\n\t\t\t\t\tproviderAndOwner));\n\n\t\t\tfinal SlotSharingManager.MultiTaskSlotLocality multiTaskSlotLocality;\n\n\t\t\ttry {\n\t\t\t\tif (task.getCoLocationConstraint() != null) {\n\t\t\t\t\tmultiTaskSlotLocality = allocateCoLocatedMultiTaskSlot(\n\t\t\t\t\t\ttask.getCoLocationConstraint(),\n\t\t\t\t\t\tmultiTaskSlotManager,\n\t\t\t\t\t\tresourceProfile,\n\t\t\t\t\t\tlocationPreferences,\n\t\t\t\t\t\tallowQueuedScheduling,\n\t\t\t\t\t\tallocationTimeout);\n\t\t\t\t} else {\n\t\t\t\t\tmultiTaskSlotLocality = allocateMultiTaskSlot(\n\t\t\t\t\t\ttask.getJobVertexId(),\n\t\t\t\t\t\tmultiTaskSlotManager,\n\t\t\t\t\t\tresourceProfile,\n\t\t\t\t\t\tlocationPreferences,\n\t\t\t\t\t\tallowQueuedScheduling,\n\t\t\t\t\t\tallocationTimeout);\n\t\t\t\t}\n\t\t\t} catch (NoResourceAvailableException noResourceException) {\n\t\t\t\treturn FutureUtils.completedExceptionally(noResourceException);\n\t\t\t}\n\n\t\t\t// sanity check\n\t\t\tPreconditions.checkState(!multiTaskSlotLocality.getMultiTaskSlot().contains(task.getJobVertexId()));\n\n\t\t\tfinal SlotSharingManager.SingleTaskSlot leaf = multiTaskSlotLocality.getMultiTaskSlot().allocateSingleTaskSlot(\n\t\t\t\tslotRequestId,\n\t\t\t\ttask.getJobVertexId(),\n\t\t\t\tmultiTaskSlotLocality.getLocality());\n\n\t\t\treturn leaf.getLogicalSlotFuture();\n\t\t} else {\n\t\t\t// request an allocated slot to assign a single logical slot to\n\t\t\tCompletableFuture<SlotAndLocality> slotAndLocalityFuture = requestAllocatedSlot(\n\t\t\t\tslotRequestId,\n\t\t\t\tresourceProfile,\n\t\t\t\tlocationPreferences,\n\t\t\t\tallowQueuedScheduling,\n\t\t\t\tallocationTimeout);\n\n\t\t\treturn slotAndLocalityFuture.thenApply(\n\t\t\t\t(SlotAndLocality slotAndLocality) -> {\n\t\t\t\t\tfinal AllocatedSlot allocatedSlot = slotAndLocality.getSlot();\n\n\t\t\t\t\tfinal SingleLogicalSlot singleTaskSlot = new SingleLogicalSlot(\n\t\t\t\t\t\tslotRequestId,\n\t\t\t\t\t\tallocatedSlot,\n\t\t\t\t\t\tnull,\n\t\t\t\t\t\tslotAndLocality.getLocality(),\n\t\t\t\t\t\tproviderAndOwner);\n\n\t\t\t\t\tif (allocatedSlot.tryAssignPayload(singleTaskSlot)) {\n\t\t\t\t\t\treturn singleTaskSlot;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tfinal FlinkException flinkException = new FlinkException(\"Could not assign payload to allocated slot \" + allocatedSlot.getAllocationId() + '.');\n\t\t\t\t\t\treleaseSlot(slotRequestId, null, flinkException);\n\t\t\t\t\t\tthrow new CompletionException(flinkException);\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t}\n\t}"
        ],
        [
            "SlotPool::releaseSlot(SlotRequestId,SlotSharingGroupId,Throwable)",
            " 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760 -\n 761  \n 762  \n 763 -\n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779 -\n 780  \n 781  \n 782  \n 783  \n 784  \n 785  ",
            "\t@Override\n\tpublic CompletableFuture<Acknowledge> releaseSlot(SlotRequestId slotRequestId, @Nullable SlotSharingGroupId slotSharingGroupId, Throwable cause) {\n\n\t\tif (slotSharingGroupId != null) {\n\t\t\tfinal SlotSharingManager multiTaskSlotManager = slotSharingManagers.get(slotSharingGroupId);\n\n\t\t\tif (multiTaskSlotManager != null) {\n\t\t\t\tfinal SlotSharingManager.TaskSlot taskSlot = multiTaskSlotManager.getTaskSlot(slotRequestId);\n\n\t\t\t\tif (taskSlot != null) {\n\t\t\t\t\ttaskSlot.release(cause);\n\t\t\t\t} else {\n\t\t\t\t\tlog.debug(\"Could not find slot {} in slot sharing group {}. Ignoring release slot request.\", slotRequestId, slotSharingGroupId, cause);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tlog.debug(\"Could not find slot sharing group {}. Ignoring release slot request.\", slotSharingGroupId, cause);\n\t\t\t}\n\t\t} else {\n\t\t\tfinal PendingRequest pendingRequest = removePendingRequest(slotRequestId);\n\n\t\t\tif (pendingRequest != null) {\n\t\t\t\tfailPendingRequest(pendingRequest, new FlinkException(\"Pending slot request with \" + slotRequestId + \" has been released.\"));\n\t\t\t} else {\n\t\t\t\tfinal AllocatedSlot allocatedSlot = allocatedSlots.remove(slotRequestId);\n\n\t\t\t\tif (allocatedSlot != null) {\n\t\t\t\t\t// sanity check\n\t\t\t\t\tif (allocatedSlot.releasePayload(cause)) {\n\t\t\t\t\t\ttryFulfillSlotRequestOrMakeAvailable(allocatedSlot);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tlog.debug(\"There is no allocated slot with allocation id {}. Ignoring the release slot request.\", slotRequestId, cause);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn CompletableFuture.completedFuture(Acknowledge.get());\n\t}",
            " 754  \n 755  \n 756 +\n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767 +\n 768  \n 769  \n 770 +\n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786 +\n 787  \n 788  \n 789  \n 790  \n 791  \n 792  ",
            "\t@Override\n\tpublic CompletableFuture<Acknowledge> releaseSlot(SlotRequestId slotRequestId, @Nullable SlotSharingGroupId slotSharingGroupId, Throwable cause) {\n\t\tlog.debug(\"Releasing slot with slot request id {}.\", slotRequestId, cause);\n\n\t\tif (slotSharingGroupId != null) {\n\t\t\tfinal SlotSharingManager multiTaskSlotManager = slotSharingManagers.get(slotSharingGroupId);\n\n\t\t\tif (multiTaskSlotManager != null) {\n\t\t\t\tfinal SlotSharingManager.TaskSlot taskSlot = multiTaskSlotManager.getTaskSlot(slotRequestId);\n\n\t\t\t\tif (taskSlot != null) {\n\t\t\t\t\ttaskSlot.release(cause);\n\t\t\t\t} else {\n\t\t\t\t\tlog.debug(\"Could not find slot {} in slot sharing group {}. Ignoring release slot request.\", slotRequestId, slotSharingGroupId);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tlog.debug(\"Could not find slot sharing group {}. Ignoring release slot request.\", slotSharingGroupId);\n\t\t\t}\n\t\t} else {\n\t\t\tfinal PendingRequest pendingRequest = removePendingRequest(slotRequestId);\n\n\t\t\tif (pendingRequest != null) {\n\t\t\t\tfailPendingRequest(pendingRequest, new FlinkException(\"Pending slot request with \" + slotRequestId + \" has been released.\"));\n\t\t\t} else {\n\t\t\t\tfinal AllocatedSlot allocatedSlot = allocatedSlots.remove(slotRequestId);\n\n\t\t\t\tif (allocatedSlot != null) {\n\t\t\t\t\t// sanity check\n\t\t\t\t\tif (allocatedSlot.releasePayload(cause)) {\n\t\t\t\t\t\ttryFulfillSlotRequestOrMakeAvailable(allocatedSlot);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tlog.debug(\"There is no allocated slot with slot request id {}. Ignoring the release slot request.\", slotRequestId);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\treturn CompletableFuture.completedFuture(Acknowledge.get());\n\t}"
        ],
        [
            "SlotSharingManager::MultiTaskSlot::MultiTaskSlot(SlotRequestId,AbstractID,MultiTaskSlot,CompletableFuture,SlotRequestId)",
            " 418  \n 419  \n 420  \n 421 -\n 422  \n 423 -\n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  ",
            "\t\tprivate MultiTaskSlot(\n\t\t\t\tSlotRequestId slotRequestId,\n\t\t\t\t@Nullable AbstractID groupId,\n\t\t\t\tMultiTaskSlot parent,\n\t\t\t\tCompletableFuture<? extends SlotContext> slotContextFuture,\n\t\t\t\tSlotRequestId allocatedSlotRequestId) {\n\t\t\tsuper(slotRequestId, groupId);\n\n\t\t\tthis.parent = parent;\n\t\t\tthis.slotContextFuture = Preconditions.checkNotNull(slotContextFuture);\n\t\t\tthis.allocatedSlotRequestId = allocatedSlotRequestId;\n\n\t\t\tthis.children = new HashMap<>(16);\n\t\t\tthis.releasingChildren = false;\n\n\t\t\tslotContextFuture.whenComplete(\n\t\t\t\t(SlotContext ignored, Throwable throwable) -> {\n\t\t\t\t\tif (throwable != null) {\n\t\t\t\t\t\trelease(throwable);\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t}",
            " 418  \n 419  \n 420  \n 421 +\n 422  \n 423 +\n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  ",
            "\t\tprivate MultiTaskSlot(\n\t\t\t\tSlotRequestId slotRequestId,\n\t\t\t\t@Nullable AbstractID groupId,\n\t\t\t\t@Nullable MultiTaskSlot parent,\n\t\t\t\tCompletableFuture<? extends SlotContext> slotContextFuture,\n\t\t\t\t@Nullable SlotRequestId allocatedSlotRequestId) {\n\t\t\tsuper(slotRequestId, groupId);\n\n\t\t\tthis.parent = parent;\n\t\t\tthis.slotContextFuture = Preconditions.checkNotNull(slotContextFuture);\n\t\t\tthis.allocatedSlotRequestId = allocatedSlotRequestId;\n\n\t\t\tthis.children = new HashMap<>(16);\n\t\t\tthis.releasingChildren = false;\n\n\t\t\tslotContextFuture.whenComplete(\n\t\t\t\t(SlotContext ignored, Throwable throwable) -> {\n\t\t\t\t\tif (throwable != null) {\n\t\t\t\t\t\trelease(throwable);\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t}"
        ],
        [
            "SlotPool::suspend()",
            " 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  ",
            "\t/**\n\t * Suspends this pool, meaning it has lost its authority to accept and distribute slots.\n\t */\n\t@Override\n\tpublic void suspend() {\n\t\tvalidateRunsInMainThread();\n\n\t\t// suspend this RPC endpoint\n\t\tstop();\n\n\t\t// do not accept any requests\n\t\tjobMasterId = null;\n\t\tresourceManagerGateway = null;\n\n\t\t// Clear (but not release!) the available slots. The TaskManagers should re-register them\n\t\t// at the new leader JobManager/SlotPool\n\t\tclear();\n\t}",
            " 221  \n 222  \n 223  \n 224  \n 225  \n 226 +\n 227 +\n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  ",
            "\t/**\n\t * Suspends this pool, meaning it has lost its authority to accept and distribute slots.\n\t */\n\t@Override\n\tpublic void suspend() {\n\t\tlog.info(\"Suspending SlotPool.\");\n\n\t\tvalidateRunsInMainThread();\n\n\t\t// suspend this RPC endpoint\n\t\tstop();\n\n\t\t// do not accept any requests\n\t\tjobMasterId = null;\n\t\tresourceManagerGateway = null;\n\n\t\t// Clear (but not release!) the available slots. The TaskManagers should re-register them\n\t\t// at the new leader JobManager/SlotPool\n\t\tclear();\n\t}"
        ],
        [
            "SlotPool::requestSlotFromResourceManager(ResourceManagerGateway,PendingRequest)",
            " 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694 -\n 695 -\n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  ",
            "\tprivate void requestSlotFromResourceManager(\n\t\t\tfinal ResourceManagerGateway resourceManagerGateway,\n\t\t\tfinal PendingRequest pendingRequest) {\n\n\t\tcheckNotNull(resourceManagerGateway);\n\t\tcheckNotNull(pendingRequest);\n\n\t\tlog.info(\"Requesting slot with profile {} from resource manager (request = {}).\", pendingRequest.getResourceProfile(), pendingRequest.getSlotRequestId());\n\n\t\tfinal AllocationID allocationId = new AllocationID();\n\n\t\tpendingRequests.put(pendingRequest.getSlotRequestId(), allocationId, pendingRequest);\n\n\t\tpendingRequest.getAllocatedSlotFuture().whenComplete(\n\t\t\t(value, throwable) -> {\n\t\t\t\tif (throwable != null) {\n\t\t\t\t\tresourceManagerGateway.cancelSlotRequest(allocationId);\n\t\t\t\t}\n\t\t\t});\n\n\t\tCompletableFuture<Acknowledge> rmResponse = resourceManagerGateway.requestSlot(\n\t\t\tjobMasterId,\n\t\t\tnew SlotRequest(jobId, allocationId, pendingRequest.getResourceProfile(), jobManagerAddress),\n\t\t\trpcTimeout);\n\n\t\t// on failure, fail the request future\n\t\trmResponse.whenCompleteAsync(\n\t\t\t(Acknowledge ignored, Throwable failure) -> {\n\t\t\t\tif (failure != null) {\n\t\t\t\t\tslotRequestToResourceManagerFailed(pendingRequest.getSlotRequestId(), failure);\n\t\t\t\t}\n\t\t\t},\n\t\t\tgetMainThreadExecutor());\n\t}",
            " 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698 +\n 699 +\n 700 +\n 701 +\n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  ",
            "\tprivate void requestSlotFromResourceManager(\n\t\t\tfinal ResourceManagerGateway resourceManagerGateway,\n\t\t\tfinal PendingRequest pendingRequest) {\n\n\t\tcheckNotNull(resourceManagerGateway);\n\t\tcheckNotNull(pendingRequest);\n\n\t\tlog.info(\"Requesting slot with profile {} from resource manager (request = {}).\", pendingRequest.getResourceProfile(), pendingRequest.getSlotRequestId());\n\n\t\tfinal AllocationID allocationId = new AllocationID();\n\n\t\tpendingRequests.put(pendingRequest.getSlotRequestId(), allocationId, pendingRequest);\n\n\t\tpendingRequest.getAllocatedSlotFuture().whenComplete(\n\t\t\t(AllocatedSlot allocatedSlot, Throwable throwable) -> {\n\t\t\t\tif (throwable != null || allocationId.equals(allocatedSlot.getAllocationId())) {\n\t\t\t\t\t// cancel the slot request if there is a failure or if the pending request has\n\t\t\t\t\t// been completed with another allocated slot\n\t\t\t\t\tresourceManagerGateway.cancelSlotRequest(allocationId);\n\t\t\t\t}\n\t\t\t});\n\n\t\tCompletableFuture<Acknowledge> rmResponse = resourceManagerGateway.requestSlot(\n\t\t\tjobMasterId,\n\t\t\tnew SlotRequest(jobId, allocationId, pendingRequest.getResourceProfile(), jobManagerAddress),\n\t\t\trpcTimeout);\n\n\t\t// on failure, fail the request future\n\t\trmResponse.whenCompleteAsync(\n\t\t\t(Acknowledge ignored, Throwable failure) -> {\n\t\t\t\tif (failure != null) {\n\t\t\t\t\tslotRequestToResourceManagerFailed(pendingRequest.getSlotRequestId(), failure);\n\t\t\t\t}\n\t\t\t},\n\t\t\tgetMainThreadExecutor());\n\t}"
        ],
        [
            "SlotPool::postStop()",
            " 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  ",
            "\t@Override\n\tpublic CompletableFuture<Void> postStop() {\n\t\t// cancel all pending allocations\n\t\tSet<AllocationID> allocationIds = pendingRequests.keySetB();\n\n\t\tfor (AllocationID allocationId : allocationIds) {\n\t\t\tresourceManagerGateway.cancelSlotRequest(allocationId);\n\t\t}\n\n\t\t// release all registered slots by releasing the corresponding TaskExecutors\n\t\tfor (ResourceID taskManagerResourceId : registeredTaskManagers) {\n\t\t\treleaseTaskManagerInternal(taskManagerResourceId);\n\t\t}\n\n\t\tclear();\n\n\t\treturn CompletableFuture.completedFuture(null);\n\t}",
            " 201  \n 202  \n 203 +\n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  ",
            "\t@Override\n\tpublic CompletableFuture<Void> postStop() {\n\t\tlog.info(\"Stopping SlotPool.\");\n\t\t// cancel all pending allocations\n\t\tSet<AllocationID> allocationIds = pendingRequests.keySetB();\n\n\t\tfor (AllocationID allocationId : allocationIds) {\n\t\t\tresourceManagerGateway.cancelSlotRequest(allocationId);\n\t\t}\n\n\t\t// release all registered slots by releasing the corresponding TaskExecutors\n\t\tfor (ResourceID taskManagerResourceId : registeredTaskManagers) {\n\t\t\treleaseTaskManagerInternal(taskManagerResourceId);\n\t\t}\n\n\t\tclear();\n\n\t\treturn CompletableFuture.completedFuture(null);\n\t}"
        ],
        [
            "SlotManager::unregisterSlotRequest(AllocationID)",
            " 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299 -\n 300  \n 301  \n 302  \n 303  ",
            "\t/**\n\t * Cancels and removes a pending slot request with the given allocation id. If there is no such\n\t * pending request, then nothing is done.\n\t *\n\t * @param allocationId identifying the pending slot request\n\t * @return True if a pending slot request was found; otherwise false\n\t */\n\tpublic boolean unregisterSlotRequest(AllocationID allocationId) {\n\t\tcheckInit();\n\n\t\tPendingSlotRequest pendingSlotRequest = pendingSlotRequests.remove(allocationId);\n\n\t\tif (null != pendingSlotRequest) {\n\t\t\tcancelPendingSlotRequest(pendingSlotRequest);\n\n\t\t\treturn true;\n\t\t} else {\n\t\t\tLOG.debug(\"No pending slot request with allocation id {} found.\", allocationId);\n\n\t\t\treturn false;\n\t\t}\n\t}",
            " 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295 +\n 296 +\n 297  \n 298  \n 299  \n 300  \n 301 +\n 302  \n 303  \n 304  \n 305  ",
            "\t/**\n\t * Cancels and removes a pending slot request with the given allocation id. If there is no such\n\t * pending request, then nothing is done.\n\t *\n\t * @param allocationId identifying the pending slot request\n\t * @return True if a pending slot request was found; otherwise false\n\t */\n\tpublic boolean unregisterSlotRequest(AllocationID allocationId) {\n\t\tcheckInit();\n\n\t\tPendingSlotRequest pendingSlotRequest = pendingSlotRequests.remove(allocationId);\n\n\t\tif (null != pendingSlotRequest) {\n\t\t\tLOG.debug(\"Cancel slot request {}.\", allocationId);\n\n\t\t\tcancelPendingSlotRequest(pendingSlotRequest);\n\n\t\t\treturn true;\n\t\t} else {\n\t\t\tLOG.debug(\"No pending slot request with allocation id {} found. Ignoring unregistration request.\", allocationId);\n\n\t\t\treturn false;\n\t\t}\n\t}"
        ]
    ],
    "b007d30cb1c209c0a0ce7c8197ee5311c7af2fa1": [
        [
            "TaskExecutor::postStop()",
            " 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  ",
            "\t/**\n\t * Called to shut down the TaskManager. The method closes all TaskManager services.\n\t */\n\t@Override\n\tpublic CompletableFuture<Void> postStop() {\n\t\tlog.info(\"Stopping TaskManager {}.\", getAddress());\n\n\t\tThrowable throwable = null;\n\n\t\tif (isConnectedToResourceManager()) {\n\t\t\tresourceManagerConnection.close();\n\t\t}\n\n\t\tfor (JobManagerConnection jobManagerConnection : jobManagerConnections.values()) {\n\t\t\ttry {\n\t\t\t\tdisassociateFromJobManager(jobManagerConnection, new FlinkException(\"The TaskExecutor is shutting down.\"));\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(t, throwable);\n\t\t\t}\n\t\t}\n\n\t\tjobManagerHeartbeatManager.stop();\n\n\t\tresourceManagerHeartbeatManager.stop();\n\n\t\ttry {\n\t\t\ttaskExecutorServices.shutDown();\n\t\t} catch (Throwable t) {\n\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(t, throwable);\n\t\t}\n\n\t\tif (throwable != null) {\n\t\t\treturn FutureUtils.completedExceptionally(new FlinkException(\"Error while shutting the TaskExecutor down.\", throwable));\n\t\t} else {\n\t\t\treturn CompletableFuture.completedFuture(null);\n\t\t}\n\t}",
            " 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 +\n 283 +\n 284 +\n 285 +\n 286 +\n 287 +\n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  ",
            "\t/**\n\t * Called to shut down the TaskManager. The method closes all TaskManager services.\n\t */\n\t@Override\n\tpublic CompletableFuture<Void> postStop() {\n\t\tlog.info(\"Stopping TaskManager {}.\", getAddress());\n\n\t\tThrowable throwable = null;\n\n\t\tif (isConnectedToResourceManager()) {\n\t\t\tresourceManagerConnection.close();\n\t\t}\n\n\t\tfor (JobManagerConnection jobManagerConnection : jobManagerConnections.values()) {\n\t\t\ttry {\n\t\t\t\tdisassociateFromJobManager(jobManagerConnection, new FlinkException(\"The TaskExecutor is shutting down.\"));\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(t, throwable);\n\t\t\t}\n\t\t}\n\n\t\tjobManagerHeartbeatManager.stop();\n\n\t\tresourceManagerHeartbeatManager.stop();\n\n\t\ttry {\n\t\t\tresourceManagerLeaderRetriever.stop();\n\t\t} catch (Exception e) {\n\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(e, throwable);\n\t\t}\n\n\t\ttry {\n\t\t\ttaskExecutorServices.shutDown();\n\t\t} catch (Throwable t) {\n\t\t\tthrowable = ExceptionUtils.firstOrSuppressed(t, throwable);\n\t\t}\n\n\t\tif (throwable != null) {\n\t\t\treturn FutureUtils.completedExceptionally(new FlinkException(\"Error while shutting the TaskExecutor down.\", throwable));\n\t\t} else {\n\t\t\treturn CompletableFuture.completedFuture(null);\n\t\t}\n\t}"
        ],
        [
            "TaskExecutor::start()",
            " 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241 -\n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  ",
            "\t@Override\n\tpublic void start() throws Exception {\n\t\tsuper.start();\n\n\t\t// start by connecting to the ResourceManager\n\t\ttry {\n\t\t\thaServices.getResourceManagerLeaderRetriever().start(new ResourceManagerLeaderListener());\n\t\t} catch (Exception e) {\n\t\t\tonFatalError(e);\n\t\t}\n\n\t\t// tell the task slot table who's responsible for the task slot actions\n\t\ttaskSlotTable.start(new SlotActionsImpl());\n\n\t\t// start the job leader service\n\t\tjobLeaderService.start(getAddress(), getRpcService(), haServices, new JobLeaderListenerImpl());\n\t}",
            " 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245 +\n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "\t@Override\n\tpublic void start() throws Exception {\n\t\tsuper.start();\n\n\t\t// start by connecting to the ResourceManager\n\t\ttry {\n\t\t\tresourceManagerLeaderRetriever.start(new ResourceManagerLeaderListener());\n\t\t} catch (Exception e) {\n\t\t\tonFatalError(e);\n\t\t}\n\n\t\t// tell the task slot table who's responsible for the task slot actions\n\t\ttaskSlotTable.start(new SlotActionsImpl());\n\n\t\t// start the job leader service\n\t\tjobLeaderService.start(getAddress(), getRpcService(), haServices, new JobLeaderListenerImpl());\n\t}"
        ],
        [
            "TaskExecutor::TaskExecutor(RpcService,TaskManagerConfiguration,HighAvailabilityServices,TaskManagerServices,HeartbeatServices,TaskManagerMetricGroup,BlobCacheService,FatalErrorHandler)",
            " 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "\tpublic TaskExecutor(\n\t\t\tRpcService rpcService,\n\t\t\tTaskManagerConfiguration taskManagerConfiguration,\n\t\t\tHighAvailabilityServices haServices,\n\t\t\tTaskManagerServices taskExecutorServices,\n\t\t\tHeartbeatServices heartbeatServices,\n\t\t\tTaskManagerMetricGroup taskManagerMetricGroup,\n\t\t\tBlobCacheService blobCacheService,\n\t\t\tFatalErrorHandler fatalErrorHandler) {\n\n\t\tsuper(rpcService, AkkaRpcServiceUtils.createRandomName(TASK_MANAGER_NAME));\n\n\t\tcheckArgument(taskManagerConfiguration.getNumberSlots() > 0, \"The number of slots has to be larger than 0.\");\n\n\t\tthis.taskManagerConfiguration = checkNotNull(taskManagerConfiguration);\n\t\tthis.taskExecutorServices = checkNotNull(taskExecutorServices);\n\t\tthis.haServices = checkNotNull(haServices);\n\t\tthis.fatalErrorHandler = checkNotNull(fatalErrorHandler);\n\t\tthis.taskManagerMetricGroup = checkNotNull(taskManagerMetricGroup);\n\t\tthis.blobCacheService = checkNotNull(blobCacheService);\n\n\t\tthis.taskSlotTable = taskExecutorServices.getTaskSlotTable();\n\t\tthis.jobManagerTable = taskExecutorServices.getJobManagerTable();\n\t\tthis.jobLeaderService = taskExecutorServices.getJobLeaderService();\n\t\tthis.taskManagerLocation = taskExecutorServices.getTaskManagerLocation();\n\t\tthis.localStateStoresManager = taskExecutorServices.getTaskManagerStateStore();\n\t\tthis.networkEnvironment = taskExecutorServices.getNetworkEnvironment();\n\n\t\tthis.jobManagerConnections = new HashMap<>(4);\n\n\t\tfinal ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();\n\n\t\tthis.jobManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n\t\t\tresourceId,\n\t\t\tnew JobManagerHeartbeatListener(),\n\t\t\trpcService.getScheduledExecutor(),\n\t\t\tlog);\n\n\t\tthis.resourceManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n\t\t\tresourceId,\n\t\t\tnew ResourceManagerHeartbeatListener(),\n\t\t\trpcService.getScheduledExecutor(),\n\t\t\tlog);\n\n\t\thardwareDescription = HardwareDescription.extractFromSystem(\n\t\t\ttaskExecutorServices.getMemoryManager().getMemorySize());\n\t}",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  ",
            "\tpublic TaskExecutor(\n\t\t\tRpcService rpcService,\n\t\t\tTaskManagerConfiguration taskManagerConfiguration,\n\t\t\tHighAvailabilityServices haServices,\n\t\t\tTaskManagerServices taskExecutorServices,\n\t\t\tHeartbeatServices heartbeatServices,\n\t\t\tTaskManagerMetricGroup taskManagerMetricGroup,\n\t\t\tBlobCacheService blobCacheService,\n\t\t\tFatalErrorHandler fatalErrorHandler) {\n\n\t\tsuper(rpcService, AkkaRpcServiceUtils.createRandomName(TASK_MANAGER_NAME));\n\n\t\tcheckArgument(taskManagerConfiguration.getNumberSlots() > 0, \"The number of slots has to be larger than 0.\");\n\n\t\tthis.taskManagerConfiguration = checkNotNull(taskManagerConfiguration);\n\t\tthis.taskExecutorServices = checkNotNull(taskExecutorServices);\n\t\tthis.haServices = checkNotNull(haServices);\n\t\tthis.fatalErrorHandler = checkNotNull(fatalErrorHandler);\n\t\tthis.taskManagerMetricGroup = checkNotNull(taskManagerMetricGroup);\n\t\tthis.blobCacheService = checkNotNull(blobCacheService);\n\n\t\tthis.taskSlotTable = taskExecutorServices.getTaskSlotTable();\n\t\tthis.jobManagerTable = taskExecutorServices.getJobManagerTable();\n\t\tthis.jobLeaderService = taskExecutorServices.getJobLeaderService();\n\t\tthis.taskManagerLocation = taskExecutorServices.getTaskManagerLocation();\n\t\tthis.localStateStoresManager = taskExecutorServices.getTaskManagerStateStore();\n\t\tthis.networkEnvironment = taskExecutorServices.getNetworkEnvironment();\n\t\tthis.resourceManagerLeaderRetriever = haServices.getResourceManagerLeaderRetriever();\n\n\t\tthis.jobManagerConnections = new HashMap<>(4);\n\n\t\tfinal ResourceID resourceId = taskExecutorServices.getTaskManagerLocation().getResourceID();\n\n\t\tthis.jobManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n\t\t\tresourceId,\n\t\t\tnew JobManagerHeartbeatListener(),\n\t\t\trpcService.getScheduledExecutor(),\n\t\t\tlog);\n\n\t\tthis.resourceManagerHeartbeatManager = heartbeatServices.createHeartbeatManager(\n\t\t\tresourceId,\n\t\t\tnew ResourceManagerHeartbeatListener(),\n\t\t\trpcService.getScheduledExecutor(),\n\t\t\tlog);\n\n\t\thardwareDescription = HardwareDescription.extractFromSystem(\n\t\t\ttaskExecutorServices.getMemoryManager().getMemorySize());\n\t}"
        ]
    ],
    "b550ac67fbf525863d5812d9d2a1010672a0169b": [
        [
            "YarnTestBase::start(YarnConfiguration,String,String)",
            " 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515 -\n 516 -\n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523 -\n 524  \n 525  \n 526 -\n 527 -\n 528 -\n 529  \n 530 -\n 531 -\n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  ",
            "\tprivate static void start(YarnConfiguration conf, String principal, String keytab) {\n\t\t// set the home directory to a temp directory. Flink on YARN is using the home dir to distribute the file\n\t\tFile homeDir = null;\n\t\ttry {\n\t\t\thomeDir = tmp.newFolder();\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(e.getMessage());\n\t\t}\n\t\tSystem.setProperty(\"user.home\", homeDir.getAbsolutePath());\n\t\tString uberjarStartLoc = \"..\";\n\t\tLOG.info(\"Trying to locate uberjar in {}\", new File(uberjarStartLoc));\n\t\tflinkUberjar = findFile(uberjarStartLoc, new RootDirFilenameFilter());\n\t\tAssert.assertNotNull(\"Flink uberjar not found\", flinkUberjar);\n\t\tString flinkDistRootDir = flinkUberjar.getParentFile().getParent();\n\t\tflinkLibFolder = flinkUberjar.getParentFile(); // the uberjar is located in lib/\n\t\tAssert.assertNotNull(\"Flink flinkLibFolder not found\", flinkLibFolder);\n\t\tAssert.assertTrue(\"lib folder not found\", flinkLibFolder.exists());\n\t\tAssert.assertTrue(\"lib folder not found\", flinkLibFolder.isDirectory());\n\n\t\tif (!flinkUberjar.exists()) {\n\t\t\tAssert.fail(\"Unable to locate yarn-uberjar.jar\");\n\t\t}\n\n\t\ttry {\n\t\t\tLOG.info(\"Starting up MiniYARNCluster\");\n\t\t\tif (yarnCluster == null) {\n\t\t\t\tfinal String testName = conf.get(YarnTestBase.TEST_CLUSTER_NAME_KEY);\n\t\t\t\tyarnCluster = new MiniYARNCluster(\n\t\t\t\t\ttestName == null ? \"YarnTest_\" + UUID.randomUUID() : testName,\n\t\t\t\t\tNUM_NODEMANAGERS,\n\t\t\t\t\t1,\n\t\t\t\t\t1);\n\n\t\t\t\tyarnCluster.init(conf);\n\t\t\t\tyarnCluster.start();\n\t\t\t}\n\n\t\t\tMap<String, String> map = new HashMap<String, String>(System.getenv());\n\n\t\t\tFile flinkConfDirPath = findFile(flinkDistRootDir, new ContainsName(new String[]{\"flink-conf.yaml\"}));\n\t\t\tAssert.assertNotNull(flinkConfDirPath);\n\t\t\tflinkConfiguration =\n\t\t\t\t\tGlobalConfiguration.loadConfiguration();\n\n\t\t\tif (!StringUtils.isBlank(principal) && !StringUtils.isBlank(keytab)) {\n\n\t\t\t\t//copy conf dir to test temporary workspace location\n\t\t\t\ttempConfPathForSecureRun = tmp.newFolder(\"conf\");\n\n\t\t\t\tString confDirPath = flinkConfDirPath.getParentFile().getAbsolutePath();\n\t\t\t\tFileUtils.copyDirectory(new File(confDirPath), tempConfPathForSecureRun);\n\n\t\t\t\tflinkConfiguration.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB.key(), keytab);\n\t\t\t\tflinkConfiguration.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL.key(), principal);\n\t\t\t\tflinkConfiguration.setString(CoreOptions.MODE.key(), OLD_MODE);\n\n\t\t\t\tBootstrapTools.writeConfiguration(flinkConfiguration,\n\t\t\t\t\t\tnew File(tempConfPathForSecureRun, \"flink-conf.yaml\"));\n\n\t\t\t\tString configDir = tempConfPathForSecureRun.getAbsolutePath();\n\n\t\t\t\tLOG.info(\"Temporary Flink configuration directory to be used for secure test: {}\", configDir);\n\n\t\t\t\tAssert.assertNotNull(configDir);\n\n\t\t\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, configDir);\n\n\t\t\t} else {\n\t\t\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, flinkConfDirPath.getParent());\n\t\t\t}\n\n\t\t\tFile yarnConfFile = writeYarnSiteConfigXML(conf);\n\t\t\tmap.put(\"YARN_CONF_DIR\", yarnConfFile.getParentFile().getAbsolutePath());\n\t\t\tmap.put(\"IN_TESTS\", \"yes we are in tests\"); // see YarnClusterDescriptor() for more infos\n\t\t\tTestBaseUtils.setEnv(map);\n\n\t\t\tAssert.assertTrue(yarnCluster.getServiceState() == Service.STATE.STARTED);\n\n\t\t\t// wait for the nodeManagers to connect\n\t\t\twhile (!yarnCluster.waitForNodeManagersToConnect(500)) {\n\t\t\t\tLOG.info(\"Waiting for Nodemanagers to connect\");\n\t\t\t}\n\t\t} catch (Exception ex) {\n\t\t\tex.printStackTrace();\n\t\t\tLOG.error(\"setup failure\", ex);\n\t\t\tAssert.fail();\n\t\t}\n\n\t}",
            " 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518 +\n 519 +\n 520 +\n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529 +\n 530 +\n 531 +\n 532  \n 533 +\n 534 +\n 535 +\n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  ",
            "\tprivate static void start(YarnConfiguration conf, String principal, String keytab) {\n\t\t// set the home directory to a temp directory. Flink on YARN is using the home dir to distribute the file\n\t\tFile homeDir = null;\n\t\ttry {\n\t\t\thomeDir = tmp.newFolder();\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t\tAssert.fail(e.getMessage());\n\t\t}\n\t\tSystem.setProperty(\"user.home\", homeDir.getAbsolutePath());\n\t\tString uberjarStartLoc = \"..\";\n\t\tLOG.info(\"Trying to locate uberjar in {}\", new File(uberjarStartLoc));\n\t\tflinkUberjar = findFile(uberjarStartLoc, new RootDirFilenameFilter());\n\t\tAssert.assertNotNull(\"Flink uberjar not found\", flinkUberjar);\n\t\tString flinkDistRootDir = flinkUberjar.getParentFile().getParent();\n\t\tflinkLibFolder = flinkUberjar.getParentFile(); // the uberjar is located in lib/\n\t\tAssert.assertNotNull(\"Flink flinkLibFolder not found\", flinkLibFolder);\n\t\tAssert.assertTrue(\"lib folder not found\", flinkLibFolder.exists());\n\t\tAssert.assertTrue(\"lib folder not found\", flinkLibFolder.isDirectory());\n\n\t\tif (!flinkUberjar.exists()) {\n\t\t\tAssert.fail(\"Unable to locate yarn-uberjar.jar\");\n\t\t}\n\n\t\ttry {\n\t\t\tLOG.info(\"Starting up MiniYARNCluster\");\n\t\t\tif (yarnCluster == null) {\n\t\t\t\tfinal String testName = conf.get(YarnTestBase.TEST_CLUSTER_NAME_KEY);\n\t\t\t\tyarnCluster = new MiniYARNCluster(\n\t\t\t\t\ttestName == null ? \"YarnTest_\" + UUID.randomUUID() : testName,\n\t\t\t\t\tNUM_NODEMANAGERS,\n\t\t\t\t\t1,\n\t\t\t\t\t1);\n\n\t\t\t\tyarnCluster.init(conf);\n\t\t\t\tyarnCluster.start();\n\t\t\t}\n\n\t\t\tMap<String, String> map = new HashMap<String, String>(System.getenv());\n\n\t\t\tFile flinkConfDirPath = findFile(flinkDistRootDir, new ContainsName(new String[]{\"flink-conf.yaml\"}));\n\t\t\tAssert.assertNotNull(flinkConfDirPath);\n\n\t\t\tfinal String confDirPath = flinkConfDirPath.getParentFile().getAbsolutePath();\n\t\t\tglobalConfiguration = GlobalConfiguration.loadConfiguration(confDirPath);\n\n\t\t\tif (!StringUtils.isBlank(principal) && !StringUtils.isBlank(keytab)) {\n\n\t\t\t\t//copy conf dir to test temporary workspace location\n\t\t\t\ttempConfPathForSecureRun = tmp.newFolder(\"conf\");\n\n\t\t\t\tFileUtils.copyDirectory(new File(confDirPath), tempConfPathForSecureRun);\n\n\t\t\t\tglobalConfiguration.setString(SecurityOptions.KERBEROS_LOGIN_KEYTAB.key(), keytab);\n\t\t\t\tglobalConfiguration.setString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL.key(), principal);\n\t\t\t\tglobalConfiguration.setString(CoreOptions.MODE.key(), OLD_MODE);\n\n\t\t\t\tBootstrapTools.writeConfiguration(\n\t\t\t\t\tglobalConfiguration,\n\t\t\t\t\tnew File(tempConfPathForSecureRun, \"flink-conf.yaml\"));\n\n\t\t\t\tString configDir = tempConfPathForSecureRun.getAbsolutePath();\n\n\t\t\t\tLOG.info(\"Temporary Flink configuration directory to be used for secure test: {}\", configDir);\n\n\t\t\t\tAssert.assertNotNull(configDir);\n\n\t\t\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, configDir);\n\n\t\t\t} else {\n\t\t\t\tmap.put(ConfigConstants.ENV_FLINK_CONF_DIR, flinkConfDirPath.getParent());\n\t\t\t}\n\n\t\t\tFile yarnConfFile = writeYarnSiteConfigXML(conf);\n\t\t\tmap.put(\"YARN_CONF_DIR\", yarnConfFile.getParentFile().getAbsolutePath());\n\t\t\tmap.put(\"IN_TESTS\", \"yes we are in tests\"); // see YarnClusterDescriptor() for more infos\n\t\t\tTestBaseUtils.setEnv(map);\n\n\t\t\tAssert.assertTrue(yarnCluster.getServiceState() == Service.STATE.STARTED);\n\n\t\t\t// wait for the nodeManagers to connect\n\t\t\twhile (!yarnCluster.waitForNodeManagersToConnect(500)) {\n\t\t\t\tLOG.info(\"Waiting for Nodemanagers to connect\");\n\t\t\t}\n\t\t} catch (Exception ex) {\n\t\t\tex.printStackTrace();\n\t\t\tLOG.error(\"setup failure\", ex);\n\t\t\tAssert.fail();\n\t\t}\n\n\t}"
        ],
        [
            "YarnTestBase::checkClusterEmpty()",
            " 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  ",
            "\t@Before\n\tpublic void checkClusterEmpty() throws IOException, YarnException {\n\t\tif (yarnClient == null) {\n\t\t\tyarnClient = YarnClient.createYarnClient();\n\t\t\tyarnClient.init(getYarnConfiguration());\n\t\t\tyarnClient.start();\n\t\t}\n\n\t\tList<ApplicationReport> apps = yarnClient.getApplications();\n\t\tfor (ApplicationReport app : apps) {\n\t\t\tif (app.getYarnApplicationState() != YarnApplicationState.FINISHED\n\t\t\t\t\t&& app.getYarnApplicationState() != YarnApplicationState.KILLED\n\t\t\t\t\t&& app.getYarnApplicationState() != YarnApplicationState.FAILED) {\n\t\t\t\tAssert.fail(\"There is at least one application on the cluster is not finished.\" +\n\t\t\t\t\t\t\"App \" + app.getApplicationId() + \" is in state \" + app.getYarnApplicationState());\n\t\t\t}\n\t\t}\n\n\t\tflip6 = CoreOptions.FLIP6_MODE.equalsIgnoreCase(flinkConfiguration.getString(CoreOptions.MODE));\n\t}",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222 +\n 223  \n 224  ",
            "\t@Before\n\tpublic void checkClusterEmpty() throws IOException, YarnException {\n\t\tif (yarnClient == null) {\n\t\t\tyarnClient = YarnClient.createYarnClient();\n\t\t\tyarnClient.init(getYarnConfiguration());\n\t\t\tyarnClient.start();\n\t\t}\n\n\t\tList<ApplicationReport> apps = yarnClient.getApplications();\n\t\tfor (ApplicationReport app : apps) {\n\t\t\tif (app.getYarnApplicationState() != YarnApplicationState.FINISHED\n\t\t\t\t\t&& app.getYarnApplicationState() != YarnApplicationState.KILLED\n\t\t\t\t\t&& app.getYarnApplicationState() != YarnApplicationState.FAILED) {\n\t\t\t\tAssert.fail(\"There is at least one application on the cluster is not finished.\" +\n\t\t\t\t\t\t\"App \" + app.getApplicationId() + \" is in state \" + app.getYarnApplicationState());\n\t\t\t}\n\t\t}\n\n\t\tflinkConfiguration = new org.apache.flink.configuration.Configuration(globalConfiguration);\n\t\tflip6 = CoreOptions.FLIP6_MODE.equalsIgnoreCase(flinkConfiguration.getString(CoreOptions.MODE));\n\t}"
        ],
        [
            "YarnConfigurationITCase::testFlinkContainerMemory()",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81 -\n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "\t/**\n\t * Tests that the Flink components are started with the correct\n\t * memory settings.\n\t */\n\t@Test(timeout = 60000)\n\tpublic void testFlinkContainerMemory() throws Exception {\n\t\tfinal YarnClient yarnClient = getYarnClient();\n\t\tfinal Configuration configuration = new Configuration(flinkConfiguration.clone());\n\n\t\tfinal int masterMemory = 64;\n\t\tfinal int taskManagerMemory = 128;\n\t\tfinal int slotsPerTaskManager = 3;\n\n\t\t// disable heap cutoff min\n\t\tconfiguration.setInteger(ResourceManagerOptions.CONTAINERIZED_HEAP_CUTOFF_MIN, 0);\n\t\tconfiguration.setLong(TaskManagerOptions.NETWORK_BUFFERS_MEMORY_MIN, (1L << 20));\n\t\tconfiguration.setLong(TaskManagerOptions.NETWORK_BUFFERS_MEMORY_MAX, (4L << 20));\n\n\t\tfinal YarnConfiguration yarnConfiguration = getYarnConfiguration();\n\t\tfinal Flip6YarnClusterDescriptor clusterDescriptor = new Flip6YarnClusterDescriptor(\n\t\t\tconfiguration,\n\t\t\tyarnConfiguration,\n\t\t\tCliFrontend.getConfigurationDirectoryFromEnv(),\n\t\t\tyarnClient,\n\t\t\ttrue);\n\n\t\tclusterDescriptor.setLocalJarPath(new Path(flinkUberjar.getAbsolutePath()));\n\t\tclusterDescriptor.addShipFiles(Arrays.asList(flinkLibFolder.listFiles()));\n\n\t\tfinal File streamingWordCountFile = new File(\"target/programs/WindowJoin.jar\");\n\n\t\tassertThat(streamingWordCountFile.exists(), is(true));\n\n\t\tfinal PackagedProgram packagedProgram = new PackagedProgram(streamingWordCountFile);\n\t\tfinal JobGraph jobGraph = PackagedProgramUtils.createJobGraph(packagedProgram, configuration, 1);\n\n\t\ttry {\n\t\t\tfinal ClusterSpecification clusterSpecification = new ClusterSpecification.ClusterSpecificationBuilder()\n\t\t\t\t.setMasterMemoryMB(masterMemory)\n\t\t\t\t.setTaskManagerMemoryMB(taskManagerMemory)\n\t\t\t\t.setSlotsPerTaskManager(slotsPerTaskManager)\n\t\t\t\t.createClusterSpecification();\n\n\t\t\tfinal ClusterClient<ApplicationId> clusterClient = clusterDescriptor.deployJobCluster(clusterSpecification, jobGraph, true);\n\n\t\t\tfinal ApplicationId clusterId = clusterClient.getClusterId();\n\n\t\t\tfinal RestClient restClient = new RestClient(RestClientConfiguration.fromConfiguration(configuration), TestingUtils.defaultExecutor());\n\n\t\t\ttry {\n\t\t\t\tfinal ApplicationReport applicationReport = yarnClient.getApplicationReport(clusterId);\n\n\t\t\t\tfinal ApplicationAttemptId currentApplicationAttemptId = applicationReport.getCurrentApplicationAttemptId();\n\n\t\t\t\t// wait until we have second container allocated\n\t\t\t\tList<ContainerReport> containers = yarnClient.getContainers(currentApplicationAttemptId);\n\n\t\t\t\twhile (containers.size() < 2) {\n\t\t\t\t\t// this is nasty but Yarn does not offer a better way to wait\n\t\t\t\t\tThread.sleep(50L);\n\t\t\t\t\tcontainers = yarnClient.getContainers(currentApplicationAttemptId);\n\t\t\t\t}\n\n\t\t\t\tfor (ContainerReport container : containers) {\n\t\t\t\t\tif (container.getContainerId().getId() == 1) {\n\t\t\t\t\t\t// this should be the application master\n\t\t\t\t\t\tassertThat(container.getAllocatedResource().getMemory(), is(masterMemory));\n\t\t\t\t\t} else {\n\t\t\t\t\t\tassertThat(container.getAllocatedResource().getMemory(), is(taskManagerMemory));\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tfinal URI webURI = new URI(clusterClient.getWebInterfaceURL());\n\n\t\t\t\tCompletableFuture<TaskManagersInfo> taskManagersInfoCompletableFuture;\n\t\t\t\tCollection<TaskManagerInfo> taskManagerInfos;\n\n\t\t\t\twhile (true) {\n\t\t\t\t\ttaskManagersInfoCompletableFuture = restClient.sendRequest(\n\t\t\t\t\t\twebURI.getHost(),\n\t\t\t\t\t\twebURI.getPort(),\n\t\t\t\t\t\tTaskManagersHeaders.getInstance(),\n\t\t\t\t\t\tEmptyMessageParameters.getInstance(),\n\t\t\t\t\t\tEmptyRequestBody.getInstance());\n\n\t\t\t\t\tfinal TaskManagersInfo taskManagersInfo = taskManagersInfoCompletableFuture.get();\n\n\t\t\t\t\ttaskManagerInfos = taskManagersInfo.getTaskManagerInfos();\n\n\t\t\t\t\tif (taskManagerInfos.isEmpty()) {\n\t\t\t\t\t\tThread.sleep(100L);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// there should be at least one TaskManagerInfo\n\t\t\t\tfinal TaskManagerInfo taskManagerInfo = taskManagerInfos.iterator().next();\n\n\t\t\t\tassertThat(taskManagerInfo.getNumberSlots(), is(slotsPerTaskManager));\n\n\t\t\t\tfinal ContaineredTaskManagerParameters containeredTaskManagerParameters = ContaineredTaskManagerParameters.create(\n\t\t\t\t\tconfiguration,\n\t\t\t\t\ttaskManagerMemory,\n\t\t\t\t\tslotsPerTaskManager);\n\n\t\t\t\tfinal long expectedHeadSize = containeredTaskManagerParameters.taskManagerHeapSizeMB() << 20L;\n\n\t\t\t\tassertThat((double) taskManagerInfo.getHardwareDescription().getSizeOfJvmHeap() / (double) expectedHeadSize, is(closeTo(1.0, 0.1)));\n\t\t\t} finally {\n\t\t\t\trestClient.shutdown(TIMEOUT);\n\t\t\t\tclusterClient.shutdown();\n\t\t\t}\n\n\t\t\tclusterDescriptor.terminateCluster(clusterId);\n\n\t\t} finally {\n\t\t\tclusterDescriptor.close();\n\t\t}\n\t}",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81 +\n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "\t/**\n\t * Tests that the Flink components are started with the correct\n\t * memory settings.\n\t */\n\t@Test(timeout = 60000)\n\tpublic void testFlinkContainerMemory() throws Exception {\n\t\tfinal YarnClient yarnClient = getYarnClient();\n\t\tfinal Configuration configuration = new Configuration(flinkConfiguration);\n\n\t\tfinal int masterMemory = 64;\n\t\tfinal int taskManagerMemory = 128;\n\t\tfinal int slotsPerTaskManager = 3;\n\n\t\t// disable heap cutoff min\n\t\tconfiguration.setInteger(ResourceManagerOptions.CONTAINERIZED_HEAP_CUTOFF_MIN, 0);\n\t\tconfiguration.setLong(TaskManagerOptions.NETWORK_BUFFERS_MEMORY_MIN, (1L << 20));\n\t\tconfiguration.setLong(TaskManagerOptions.NETWORK_BUFFERS_MEMORY_MAX, (4L << 20));\n\n\t\tfinal YarnConfiguration yarnConfiguration = getYarnConfiguration();\n\t\tfinal Flip6YarnClusterDescriptor clusterDescriptor = new Flip6YarnClusterDescriptor(\n\t\t\tconfiguration,\n\t\t\tyarnConfiguration,\n\t\t\tCliFrontend.getConfigurationDirectoryFromEnv(),\n\t\t\tyarnClient,\n\t\t\ttrue);\n\n\t\tclusterDescriptor.setLocalJarPath(new Path(flinkUberjar.getAbsolutePath()));\n\t\tclusterDescriptor.addShipFiles(Arrays.asList(flinkLibFolder.listFiles()));\n\n\t\tfinal File streamingWordCountFile = new File(\"target/programs/WindowJoin.jar\");\n\n\t\tassertThat(streamingWordCountFile.exists(), is(true));\n\n\t\tfinal PackagedProgram packagedProgram = new PackagedProgram(streamingWordCountFile);\n\t\tfinal JobGraph jobGraph = PackagedProgramUtils.createJobGraph(packagedProgram, configuration, 1);\n\n\t\ttry {\n\t\t\tfinal ClusterSpecification clusterSpecification = new ClusterSpecification.ClusterSpecificationBuilder()\n\t\t\t\t.setMasterMemoryMB(masterMemory)\n\t\t\t\t.setTaskManagerMemoryMB(taskManagerMemory)\n\t\t\t\t.setSlotsPerTaskManager(slotsPerTaskManager)\n\t\t\t\t.createClusterSpecification();\n\n\t\t\tfinal ClusterClient<ApplicationId> clusterClient = clusterDescriptor.deployJobCluster(clusterSpecification, jobGraph, true);\n\n\t\t\tfinal ApplicationId clusterId = clusterClient.getClusterId();\n\n\t\t\tfinal RestClient restClient = new RestClient(RestClientConfiguration.fromConfiguration(configuration), TestingUtils.defaultExecutor());\n\n\t\t\ttry {\n\t\t\t\tfinal ApplicationReport applicationReport = yarnClient.getApplicationReport(clusterId);\n\n\t\t\t\tfinal ApplicationAttemptId currentApplicationAttemptId = applicationReport.getCurrentApplicationAttemptId();\n\n\t\t\t\t// wait until we have second container allocated\n\t\t\t\tList<ContainerReport> containers = yarnClient.getContainers(currentApplicationAttemptId);\n\n\t\t\t\twhile (containers.size() < 2) {\n\t\t\t\t\t// this is nasty but Yarn does not offer a better way to wait\n\t\t\t\t\tThread.sleep(50L);\n\t\t\t\t\tcontainers = yarnClient.getContainers(currentApplicationAttemptId);\n\t\t\t\t}\n\n\t\t\t\tfor (ContainerReport container : containers) {\n\t\t\t\t\tif (container.getContainerId().getId() == 1) {\n\t\t\t\t\t\t// this should be the application master\n\t\t\t\t\t\tassertThat(container.getAllocatedResource().getMemory(), is(masterMemory));\n\t\t\t\t\t} else {\n\t\t\t\t\t\tassertThat(container.getAllocatedResource().getMemory(), is(taskManagerMemory));\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tfinal URI webURI = new URI(clusterClient.getWebInterfaceURL());\n\n\t\t\t\tCompletableFuture<TaskManagersInfo> taskManagersInfoCompletableFuture;\n\t\t\t\tCollection<TaskManagerInfo> taskManagerInfos;\n\n\t\t\t\twhile (true) {\n\t\t\t\t\ttaskManagersInfoCompletableFuture = restClient.sendRequest(\n\t\t\t\t\t\twebURI.getHost(),\n\t\t\t\t\t\twebURI.getPort(),\n\t\t\t\t\t\tTaskManagersHeaders.getInstance(),\n\t\t\t\t\t\tEmptyMessageParameters.getInstance(),\n\t\t\t\t\t\tEmptyRequestBody.getInstance());\n\n\t\t\t\t\tfinal TaskManagersInfo taskManagersInfo = taskManagersInfoCompletableFuture.get();\n\n\t\t\t\t\ttaskManagerInfos = taskManagersInfo.getTaskManagerInfos();\n\n\t\t\t\t\tif (taskManagerInfos.isEmpty()) {\n\t\t\t\t\t\tThread.sleep(100L);\n\t\t\t\t\t} else {\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// there should be at least one TaskManagerInfo\n\t\t\t\tfinal TaskManagerInfo taskManagerInfo = taskManagerInfos.iterator().next();\n\n\t\t\t\tassertThat(taskManagerInfo.getNumberSlots(), is(slotsPerTaskManager));\n\n\t\t\t\tfinal ContaineredTaskManagerParameters containeredTaskManagerParameters = ContaineredTaskManagerParameters.create(\n\t\t\t\t\tconfiguration,\n\t\t\t\t\ttaskManagerMemory,\n\t\t\t\t\tslotsPerTaskManager);\n\n\t\t\t\tfinal long expectedHeadSize = containeredTaskManagerParameters.taskManagerHeapSizeMB() << 20L;\n\n\t\t\t\tassertThat((double) taskManagerInfo.getHardwareDescription().getSizeOfJvmHeap() / (double) expectedHeadSize, is(closeTo(1.0, 0.1)));\n\t\t\t} finally {\n\t\t\t\trestClient.shutdown(TIMEOUT);\n\t\t\t\tclusterClient.shutdown();\n\t\t\t}\n\n\t\t\tclusterDescriptor.terminateCluster(clusterId);\n\n\t\t} finally {\n\t\t\tclusterDescriptor.close();\n\t\t}\n\t}"
        ],
        [
            "YARNSessionFIFOSecuredITCase::testDetachedMode()",
            " 102  \n 103  \n 104  \n 105 -\n 106 -\n 107 -\n 108 -\n 109 -\n 110 -\n 111  ",
            "\t@Override\n\tpublic void testDetachedMode() throws InterruptedException, IOException {\n\t\tsuper.testDetachedMode();\n\t\tif (!verifyStringsInNamedLogFiles(\n\t\t\t\tnew String[]{\"Login successful for user\", \"using keytab file\"}, \"jobmanager.log\") ||\n\t\t\t\t!verifyStringsInNamedLogFiles(\n\t\t\t\t\t\tnew String[]{\"Login successful for user\", \"using keytab file\"}, \"taskmanager.log\")) {\n\t\t\tAssert.fail(\"Can not find expected strings in log files.\");\n\t\t}\n\t}",
            " 103  \n 104  \n 105  \n 106 +\n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117  ",
            "\t@Override\n\tpublic void testDetachedMode() throws InterruptedException, IOException {\n\t\tsuper.testDetachedMode();\n\t\tfinal String[] mustHave = {\"Login successful for user\", \"using keytab file\"};\n\t\tfinal boolean jobManagerRunsWithKerberos = verifyStringsInNamedLogFiles(\n\t\t\tmustHave,\n\t\t\t\"jobmanager.log\");\n\t\tfinal boolean taskManagerRunsWithKerberos = verifyStringsInNamedLogFiles(\n\t\t\tmustHave, \"taskmanager.log\");\n\n\t\tAssert.assertThat(\n\t\t\t\"The JobManager and the TaskManager should both run with Kerberos.\",\n\t\t\tjobManagerRunsWithKerberos && taskManagerRunsWithKerberos,\n\t\t\tMatchers.is(true));\n\t}"
        ]
    ],
    "4b89b5d0abe4684dbeacd9a01fe3297621050eb7": [
        [
            "AbstractYarnClusterDescriptor::deployInternal(ClusterSpecification,String,JobGraph,boolean)",
            " 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "\t/**\n\t * This method will block until the ApplicationMaster/JobManager have been\n\t * deployed on YARN.\n\t *\n\t * @param clusterSpecification Initial cluster specification for the to be deployed Flink cluster\n\t * @param yarnClusterEntrypoint Class name of the Yarn cluster entry point.\n\t * @param jobGraph A job graph which is deployed with the Flink cluster, {@code null} if none\n\t * @param detached True if the cluster should be started in detached mode\n\t */\n\tprotected ClusterClient<ApplicationId> deployInternal(\n\t\t\tClusterSpecification clusterSpecification,\n\t\t\tString yarnClusterEntrypoint,\n\t\t\t@Nullable JobGraph jobGraph,\n\t\t\tboolean detached) throws Exception {\n\n\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\t// note: UGI::hasKerberosCredentials inaccurately reports false\n\t\t\t// for logins based on a keytab (fixed in Hadoop 2.6.1, see HADOOP-10786),\n\t\t\t// so we check only in ticket cache scenario.\n\t\t\tboolean useTicketCache = flinkConfiguration.getBoolean(SecurityOptions.KERBEROS_LOGIN_USETICKETCACHE);\n\n\t\t\tUserGroupInformation loginUser = UserGroupInformation.getCurrentUser();\n\t\t\tif (loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS\n\t\t\t\t&& useTicketCache && !loginUser.hasKerberosCredentials()) {\n\t\t\t\tLOG.error(\"Hadoop security with Kerberos is enabled but the login user does not have Kerberos credentials\");\n\t\t\t\tthrow new RuntimeException(\"Hadoop security with Kerberos is enabled but the login user \" +\n\t\t\t\t\t\"does not have Kerberos credentials\");\n\t\t\t}\n\t\t}\n\n\t\tisReadyForDeployment(clusterSpecification);\n\n\t\t// ------------------ Check if the specified queue exists --------------------\n\n\t\tcheckYarnQueues(yarnClient);\n\n\t\t// ------------------ Add dynamic properties to local flinkConfiguraton ------\n\t\tMap<String, String> dynProperties = getDynamicProperties(dynamicPropertiesEncoded);\n\t\tfor (Map.Entry<String, String> dynProperty : dynProperties.entrySet()) {\n\t\t\tflinkConfiguration.setString(dynProperty.getKey(), dynProperty.getValue());\n\t\t}\n\n\t\t// ------------------ Check if the YARN ClusterClient has the requested resources --------------\n\n\t\t// Create application via yarnClient\n\t\tfinal YarnClientApplication yarnApplication = yarnClient.createApplication();\n\t\tfinal GetNewApplicationResponse appResponse = yarnApplication.getNewApplicationResponse();\n\n\t\tResource maxRes = appResponse.getMaximumResourceCapability();\n\n\t\tfinal ClusterResourceDescription freeClusterMem;\n\t\ttry {\n\t\t\tfreeClusterMem = getCurrentFreeClusterResources(yarnClient);\n\t\t} catch (YarnException | IOException e) {\n\t\t\tfailSessionDuringDeployment(yarnClient, yarnApplication);\n\t\t\tthrow new YarnDeploymentException(\"Could not retrieve information about free cluster resources.\", e);\n\t\t}\n\n\t\tfinal int yarnMinAllocationMB = yarnConfiguration.getInt(\"yarn.scheduler.minimum-allocation-mb\", 0);\n\n\t\tfinal ClusterSpecification validClusterSpecification;\n\t\ttry {\n\t\t\tvalidClusterSpecification = validateClusterResources(\n\t\t\t\tclusterSpecification,\n\t\t\t\tyarnMinAllocationMB,\n\t\t\t\tmaxRes,\n\t\t\t\tfreeClusterMem);\n\t\t} catch (YarnDeploymentException yde) {\n\t\t\tfailSessionDuringDeployment(yarnClient, yarnApplication);\n\t\t\tthrow yde;\n\t\t}\n\n\t\tLOG.info(\"Cluster specification: {}\", validClusterSpecification);\n\n\t\tfinal ClusterEntrypoint.ExecutionMode executionMode = detached ?\n\t\t\tClusterEntrypoint.ExecutionMode.DETACHED\n\t\t\t: ClusterEntrypoint.ExecutionMode.NORMAL;\n\n\t\tflinkConfiguration.setString(ClusterEntrypoint.EXECUTION_MODE, executionMode.toString());\n\n\t\tApplicationReport report = startAppMaster(\n\t\t\tflinkConfiguration,\n\t\t\tyarnClusterEntrypoint,\n\t\t\tjobGraph,\n\t\t\tyarnClient,\n\t\t\tyarnApplication,\n\t\t\tclusterSpecification);\n\n\t\tString host = report.getHost();\n\t\tint port = report.getRpcPort();\n\n\t\t// Correctly initialize the Flink config\n\t\tflinkConfiguration.setString(JobManagerOptions.ADDRESS, host);\n\t\tflinkConfiguration.setInteger(JobManagerOptions.PORT, port);\n\n\t\tflinkConfiguration.setString(RestOptions.REST_ADDRESS, host);\n\t\tflinkConfiguration.setInteger(RestOptions.REST_PORT, port);\n\n\t\t// the Flink cluster is deployed in YARN. Represent cluster\n\t\treturn createYarnClusterClient(\n\t\t\tthis,\n\t\t\tclusterSpecification.getNumberTaskManagers(),\n\t\t\tclusterSpecification.getSlotsPerTaskManager(),\n\t\t\treport,\n\t\t\tflinkConfiguration,\n\t\t\ttrue);\n\t}",
            " 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 +\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  ",
            "\t/**\n\t * This method will block until the ApplicationMaster/JobManager have been\n\t * deployed on YARN.\n\t *\n\t * @param clusterSpecification Initial cluster specification for the to be deployed Flink cluster\n\t * @param yarnClusterEntrypoint Class name of the Yarn cluster entry point.\n\t * @param jobGraph A job graph which is deployed with the Flink cluster, {@code null} if none\n\t * @param detached True if the cluster should be started in detached mode\n\t */\n\tprotected ClusterClient<ApplicationId> deployInternal(\n\t\t\tClusterSpecification clusterSpecification,\n\t\t\tString yarnClusterEntrypoint,\n\t\t\t@Nullable JobGraph jobGraph,\n\t\t\tboolean detached) throws Exception {\n\n\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\t// note: UGI::hasKerberosCredentials inaccurately reports false\n\t\t\t// for logins based on a keytab (fixed in Hadoop 2.6.1, see HADOOP-10786),\n\t\t\t// so we check only in ticket cache scenario.\n\t\t\tboolean useTicketCache = flinkConfiguration.getBoolean(SecurityOptions.KERBEROS_LOGIN_USETICKETCACHE);\n\n\t\t\tUserGroupInformation loginUser = UserGroupInformation.getCurrentUser();\n\t\t\tif (loginUser.getAuthenticationMethod() == UserGroupInformation.AuthenticationMethod.KERBEROS\n\t\t\t\t&& useTicketCache && !loginUser.hasKerberosCredentials()) {\n\t\t\t\tLOG.error(\"Hadoop security with Kerberos is enabled but the login user does not have Kerberos credentials\");\n\t\t\t\tthrow new RuntimeException(\"Hadoop security with Kerberos is enabled but the login user \" +\n\t\t\t\t\t\"does not have Kerberos credentials\");\n\t\t\t}\n\t\t}\n\n\t\tisReadyForDeployment(clusterSpecification);\n\n\t\t// ------------------ Check if the specified queue exists --------------------\n\n\t\tcheckYarnQueues(yarnClient);\n\n\t\t// ------------------ Add dynamic properties to local flinkConfiguraton ------\n\t\tMap<String, String> dynProperties = getDynamicProperties(dynamicPropertiesEncoded);\n\t\tfor (Map.Entry<String, String> dynProperty : dynProperties.entrySet()) {\n\t\t\tflinkConfiguration.setString(dynProperty.getKey(), dynProperty.getValue());\n\t\t}\n\n\t\t// ------------------ Check if the YARN ClusterClient has the requested resources --------------\n\n\t\t// Create application via yarnClient\n\t\tfinal YarnClientApplication yarnApplication = yarnClient.createApplication();\n\t\tfinal GetNewApplicationResponse appResponse = yarnApplication.getNewApplicationResponse();\n\n\t\tResource maxRes = appResponse.getMaximumResourceCapability();\n\n\t\tfinal ClusterResourceDescription freeClusterMem;\n\t\ttry {\n\t\t\tfreeClusterMem = getCurrentFreeClusterResources(yarnClient);\n\t\t} catch (YarnException | IOException e) {\n\t\t\tfailSessionDuringDeployment(yarnClient, yarnApplication);\n\t\t\tthrow new YarnDeploymentException(\"Could not retrieve information about free cluster resources.\", e);\n\t\t}\n\n\t\tfinal int yarnMinAllocationMB = yarnConfiguration.getInt(\"yarn.scheduler.minimum-allocation-mb\", 0);\n\n\t\tfinal ClusterSpecification validClusterSpecification;\n\t\ttry {\n\t\t\tvalidClusterSpecification = validateClusterResources(\n\t\t\t\tclusterSpecification,\n\t\t\t\tyarnMinAllocationMB,\n\t\t\t\tmaxRes,\n\t\t\t\tfreeClusterMem);\n\t\t} catch (YarnDeploymentException yde) {\n\t\t\tfailSessionDuringDeployment(yarnClient, yarnApplication);\n\t\t\tthrow yde;\n\t\t}\n\n\t\tLOG.info(\"Cluster specification: {}\", validClusterSpecification);\n\n\t\tfinal ClusterEntrypoint.ExecutionMode executionMode = detached ?\n\t\t\tClusterEntrypoint.ExecutionMode.DETACHED\n\t\t\t: ClusterEntrypoint.ExecutionMode.NORMAL;\n\n\t\tflinkConfiguration.setString(ClusterEntrypoint.EXECUTION_MODE, executionMode.toString());\n\n\t\tApplicationReport report = startAppMaster(\n\t\t\tnew Configuration(flinkConfiguration),\n\t\t\tyarnClusterEntrypoint,\n\t\t\tjobGraph,\n\t\t\tyarnClient,\n\t\t\tyarnApplication,\n\t\t\tclusterSpecification);\n\n\t\tString host = report.getHost();\n\t\tint port = report.getRpcPort();\n\n\t\t// Correctly initialize the Flink config\n\t\tflinkConfiguration.setString(JobManagerOptions.ADDRESS, host);\n\t\tflinkConfiguration.setInteger(JobManagerOptions.PORT, port);\n\n\t\tflinkConfiguration.setString(RestOptions.REST_ADDRESS, host);\n\t\tflinkConfiguration.setInteger(RestOptions.REST_PORT, port);\n\n\t\t// the Flink cluster is deployed in YARN. Represent cluster\n\t\treturn createYarnClusterClient(\n\t\t\tthis,\n\t\t\tclusterSpecification.getNumberTaskManagers(),\n\t\t\tclusterSpecification.getSlotsPerTaskManager(),\n\t\t\treport,\n\t\t\tflinkConfiguration,\n\t\t\ttrue);\n\t}"
        ],
        [
            "AbstractYarnClusterDescriptor::startAppMaster(Configuration,String,JobGraph,YarnClient,YarnClientApplication,ClusterSpecification)",
            " 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  ",
            "\tpublic ApplicationReport startAppMaster(\n\t\t\tConfiguration configuration,\n\t\t\tString yarnClusterEntrypoint,\n\t\t\tJobGraph jobGraph,\n\t\t\tYarnClient yarnClient,\n\t\t\tYarnClientApplication yarnApplication,\n\t\t\tClusterSpecification clusterSpecification) throws Exception {\n\n\t\t// ------------------ Initialize the file systems -------------------------\n\n\t\ttry {\n\t\t\torg.apache.flink.core.fs.FileSystem.initialize(configuration);\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error while setting the default \" +\n\t\t\t\t\t\"filesystem scheme from configuration.\", e);\n\t\t}\n\n\t\t// initialize file system\n\t\t// Copy the application master jar to the filesystem\n\t\t// Create a local resource to point to the destination jar path\n\t\tfinal FileSystem fs = FileSystem.get(yarnConfiguration);\n\t\tfinal Path homeDir = fs.getHomeDirectory();\n\n\t\t// hard coded check for the GoogleHDFS client because its not overriding the getScheme() method.\n\t\tif (!fs.getClass().getSimpleName().equals(\"GoogleHadoopFileSystem\") &&\n\t\t\t\tfs.getScheme().startsWith(\"file\")) {\n\t\t\tLOG.warn(\"The file system scheme is '\" + fs.getScheme() + \"'. This indicates that the \"\n\t\t\t\t\t+ \"specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.\"\n\t\t\t\t\t+ \"The Flink YARN client needs to store its files in a distributed file system\");\n\t\t}\n\n\t\tApplicationSubmissionContext appContext = yarnApplication.getApplicationSubmissionContext();\n\t\tSet<File> systemShipFiles = new HashSet<>(shipFiles.size());\n\t\tfor (File file : shipFiles) {\n\t\t\tsystemShipFiles.add(file.getAbsoluteFile());\n\t\t}\n\n\t\t//check if there is a logback or log4j file\n\t\tFile logbackFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOGBACK_NAME);\n\t\tfinal boolean hasLogback = logbackFile.exists();\n\t\tif (hasLogback) {\n\t\t\tsystemShipFiles.add(logbackFile);\n\t\t}\n\n\t\tFile log4jFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOG4J_NAME);\n\t\tfinal boolean hasLog4j = log4jFile.exists();\n\t\tif (hasLog4j) {\n\t\t\tsystemShipFiles.add(log4jFile);\n\t\t\tif (hasLogback) {\n\t\t\t\t// this means there is already a logback configuration file --> fail\n\t\t\t\tLOG.warn(\"The configuration directory ('\" + configurationDirectory + \"') contains both LOG4J and \" +\n\t\t\t\t\t\"Logback configuration files. Please delete or rename one of them.\");\n\t\t\t}\n\t\t}\n\n\t\taddLibFolderToShipFiles(systemShipFiles);\n\n\t\t// Set-up ApplicationSubmissionContext for the application\n\n\t\tfinal ApplicationId appId = appContext.getApplicationId();\n\n\t\t// ------------------ Add Zookeeper namespace to local flinkConfiguraton ------\n\t\tString zkNamespace = getZookeeperNamespace();\n\t\t// no user specified cli argument for namespace?\n\t\tif (zkNamespace == null || zkNamespace.isEmpty()) {\n\t\t\t// namespace defined in config? else use applicationId as default.\n\t\t\tzkNamespace = configuration.getString(HighAvailabilityOptions.HA_CLUSTER_ID, String.valueOf(appId));\n\t\t\tsetZookeeperNamespace(zkNamespace);\n\t\t}\n\n\t\tconfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\tif (HighAvailabilityMode.isHighAvailabilityModeActivated(configuration)) {\n\t\t\t// activate re-execution of failed applications\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tconfiguration.getInteger(\n\t\t\t\t\tYarnConfigOptions.APPLICATION_ATTEMPTS.key(),\n\t\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));\n\n\t\t\tactivateHighAvailabilitySupport(appContext);\n\t\t} else {\n\t\t\t// set number of application retries to 1 in the default case\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tconfiguration.getInteger(\n\t\t\t\t\tYarnConfigOptions.APPLICATION_ATTEMPTS.key(),\n\t\t\t\t\t1));\n\t\t}\n\n\t\tif (jobGraph != null) {\n\t\t\t// add the user code jars from the provided JobGraph\n\t\t\tfor (org.apache.flink.core.fs.Path path : jobGraph.getUserJars()) {\n\t\t\t\tuserJarFiles.add(new File(path.toUri()));\n\t\t\t}\n\t\t}\n\n\t\t// local resource map for Yarn\n\t\tfinal Map<String, LocalResource> localResources = new HashMap<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// list of remote paths (after upload)\n\t\tfinal List<Path> paths = new ArrayList<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// ship list that enables reuse of resources for task manager containers\n\t\tStringBuilder envShipFileList = new StringBuilder();\n\n\t\t// upload and register ship files\n\t\tList<String> systemClassPaths = uploadAndRegisterFiles(\n\t\t\tsystemShipFiles,\n\t\t\tfs,\n\t\t\thomeDir,\n\t\t\tappId,\n\t\t\tpaths,\n\t\t\tlocalResources,\n\t\t\tenvShipFileList);\n\n\t\tList<String> userClassPaths;\n\t\tif (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\tuserClassPaths = uploadAndRegisterFiles(\n\t\t\t\tuserJarFiles,\n\t\t\t\tfs,\n\t\t\t\thomeDir,\n\t\t\t\tappId,\n\t\t\t\tpaths,\n\t\t\t\tlocalResources,\n\t\t\t\tenvShipFileList);\n\t\t} else {\n\t\t\tuserClassPaths = Collections.emptyList();\n\t\t}\n\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.ORDER) {\n\t\t\tsystemClassPaths.addAll(userClassPaths);\n\t\t}\n\n\t\t// normalize classpath by sorting\n\t\tCollections.sort(systemClassPaths);\n\t\tCollections.sort(userClassPaths);\n\n\t\t// classpath assembler\n\t\tStringBuilder classPathBuilder = new StringBuilder();\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.FIRST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\t\tfor (String classPath : systemClassPaths) {\n\t\t\tclassPathBuilder.append(classPath).append(File.pathSeparator);\n\t\t}\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.LAST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\n\t\t// Setup jar for ApplicationMaster\n\t\tPath remotePathJar = setupSingleLocalResource(\n\t\t\t\"flink.jar\",\n\t\t\tfs,\n\t\t\tappId,\n\t\t\tflinkJarPath,\n\t\t\tlocalResources,\n\t\t\thomeDir,\n\t\t\t\"\");\n\n\t\t// Upload the flink configuration\n\t\t// write out configuration file\n\t\tFile tmpConfigurationFile = File.createTempFile(appId + \"-flink-conf.yaml\", null);\n\t\ttmpConfigurationFile.deleteOnExit();\n\t\tBootstrapTools.writeConfiguration(configuration, tmpConfigurationFile);\n\n\t\tPath remotePathConf = setupSingleLocalResource(\n\t\t\t\"flink-conf.yaml\",\n\t\t\tfs,\n\t\t\tappId,\n\t\t\tnew Path(tmpConfigurationFile.getAbsolutePath()),\n\t\t\tlocalResources,\n\t\t\thomeDir,\n\t\t\t\"\");\n\n\t\tpaths.add(remotePathJar);\n\t\tclassPathBuilder.append(\"flink.jar\").append(File.pathSeparator);\n\t\tpaths.add(remotePathConf);\n\t\tclassPathBuilder.append(\"flink-conf.yaml\").append(File.pathSeparator);\n\n\t\t// write job graph to tmp file and add it to local resource\n\t\t// TODO: server use user main method to generate job graph\n\t\tif (jobGraph != null) {\n\t\t\ttry {\n\t\t\t\tFile fp = File.createTempFile(appId.toString(), null);\n\t\t\t\tfp.deleteOnExit();\n\t\t\t\ttry (FileOutputStream output = new FileOutputStream(fp);\n\t\t\t\t\tObjectOutputStream obOutput = new ObjectOutputStream(output);){\n\t\t\t\t\tobOutput.writeObject(jobGraph);\n\t\t\t\t}\n\n\t\t\t\tPath pathFromYarnURL = setupSingleLocalResource(\n\t\t\t\t\t\"job.graph\",\n\t\t\t\t\tfs,\n\t\t\t\t\tappId,\n\t\t\t\t\tnew Path(fp.toURI()),\n\t\t\t\t\tlocalResources,\n\t\t\t\t\thomeDir,\n\t\t\t\t\t\"\");\n\t\t\t\tpaths.add(pathFromYarnURL);\n\t\t\t\tclassPathBuilder.append(\"job.graph\").append(File.pathSeparator);\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Add job graph to local resource fail\");\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\n\t\tPath yarnFilesDir = new Path(homeDir, \".flink/\" + appId + '/');\n\n\t\tFsPermission permission = new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n\t\tfs.setPermission(yarnFilesDir, permission); // set permission for path.\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\t//In Integration test setup, the Yarn containers created by YarnMiniCluster does not have the Yarn site XML\n\t\t//and KRB5 configuration files. We are adding these files as container local resources for the container\n\t\t//applications (JM/TMs) to have proper secure cluster setup\n\t\tPath remoteKrb5Path = null;\n\t\tPath remoteYarnSiteXmlPath = null;\n\t\tboolean hasKrb5 = false;\n\t\tif (System.getenv(\"IN_TESTS\") != null) {\n\t\t\tString krb5Config = System.getProperty(\"java.security.krb5.conf\");\n\t\t\tif (krb5Config != null && krb5Config.length() != 0) {\n\t\t\t\tFile krb5 = new File(krb5Config);\n\t\t\t\tLOG.info(\"Adding KRB5 configuration {} to the AM container local resource bucket\", krb5.getAbsolutePath());\n\t\t\t\tPath krb5ConfPath = new Path(krb5.getAbsolutePath());\n\t\t\t\tremoteKrb5Path = setupSingleLocalResource(\n\t\t\t\t\tUtils.KRB5_FILE_NAME,\n\t\t\t\t\tfs,\n\t\t\t\t\tappId,\n\t\t\t\t\tkrb5ConfPath,\n\t\t\t\t\tlocalResources,\n\t\t\t\t\thomeDir,\n\t\t\t\t\t\"\");\n\n\t\t\t\tFile f = new File(System.getenv(\"YARN_CONF_DIR\"), Utils.YARN_SITE_FILE_NAME);\n\t\t\t\tLOG.info(\"Adding Yarn configuration {} to the AM container local resource bucket\", f.getAbsolutePath());\n\t\t\t\tPath yarnSitePath = new Path(f.getAbsolutePath());\n\t\t\t\tremoteYarnSiteXmlPath = setupSingleLocalResource(\n\t\t\t\t\tUtils.YARN_SITE_FILE_NAME,\n\t\t\t\t\tfs,\n\t\t\t\t\tappId,\n\t\t\t\t\tyarnSitePath,\n\t\t\t\t\tlocalResources,\n\t\t\t\t\thomeDir,\n\t\t\t\t\t\"\");\n\t\t\t\thasKrb5 = true;\n\t\t\t}\n\t\t}\n\n\t\t// setup security tokens\n\t\tPath remotePathKeytab = null;\n\t\tString keytab = configuration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);\n\t\tif (keytab != null) {\n\t\t\tLOG.info(\"Adding keytab {} to the AM container local resource bucket\", keytab);\n\t\t\tremotePathKeytab = setupSingleLocalResource(\n\t\t\t\tUtils.KEYTAB_FILE_NAME,\n\t\t\t\tfs,\n\t\t\t\tappId,\n\t\t\t\tnew Path(keytab),\n\t\t\t\tlocalResources,\n\t\t\t\thomeDir,\n\t\t\t\t\"\");\n\t\t}\n\n\t\tfinal ContainerLaunchContext amContainer = setupApplicationMasterContainer(\n\t\t\tyarnClusterEntrypoint,\n\t\t\thasLogback,\n\t\t\thasLog4j,\n\t\t\thasKrb5,\n\t\t\tclusterSpecification.getMasterMemoryMB());\n\n\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\t// set HDFS delegation tokens when security is enabled\n\t\t\tLOG.info(\"Adding delegation token to the AM container..\");\n\t\t\tUtils.setTokensFor(amContainer, paths, yarnConfiguration);\n\t\t}\n\n\t\tamContainer.setLocalResources(localResources);\n\t\tfs.close();\n\n\t\t// Setup CLASSPATH and environment variables for ApplicationMaster\n\t\tfinal Map<String, String> appMasterEnv = new HashMap<>();\n\t\t// set user specified app master environment variables\n\t\tappMasterEnv.putAll(Utils.getEnvironmentVariables(ResourceManagerOptions.CONTAINERIZED_MASTER_ENV_PREFIX, configuration));\n\t\t// set Flink app class path\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_FLINK_CLASSPATH, classPathBuilder.toString());\n\n\t\t// set Flink on YARN internal configuration values\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_COUNT, String.valueOf(clusterSpecification.getNumberTaskManagers()));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_MEMORY, String.valueOf(clusterSpecification.getTaskManagerMemoryMB()));\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_JAR_PATH, remotePathJar.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_APP_ID, appId.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_HOME_DIR, homeDir.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_SHIP_FILES, envShipFileList.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_SLOTS, String.valueOf(clusterSpecification.getSlotsPerTaskManager()));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_DETACHED, String.valueOf(detached));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE, getZookeeperNamespace());\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_YARN_FILES, yarnFilesDir.toUri().toString());\n\n\t\t// https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md#identity-on-an-insecure-cluster-hadoop_user_name\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif (remotePathKeytab != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PATH, remotePathKeytab.toString());\n\t\t\tString principal = configuration.getString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL);\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, principal);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif (remoteYarnSiteXmlPath != null && remoteKrb5Path != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_YARN_SITE_XML_PATH, remoteYarnSiteXmlPath.toString());\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_KRB5_PATH, remoteKrb5Path.toString());\n\t\t}\n\n\t\tif (dynamicPropertiesEncoded != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n\t\t}\n\n\t\t// set classpath from YARN configuration\n\t\tUtils.setupYarnClassPath(yarnConfiguration, appMasterEnv);\n\n\t\tamContainer.setEnvironment(appMasterEnv);\n\n\t\t// Set up resource type requirements for ApplicationMaster\n\t\tResource capability = Records.newRecord(Resource.class);\n\t\tcapability.setMemory(clusterSpecification.getMasterMemoryMB());\n\t\tcapability.setVirtualCores(1);\n\n\t\tString name;\n\t\tif (customName == null) {\n\t\t\tname = \"Flink session with \" + clusterSpecification.getNumberTaskManagers() + \" TaskManagers\";\n\t\t\tif (detached) {\n\t\t\t\tname += \" (detached)\";\n\t\t\t}\n\t\t} else {\n\t\t\tname = customName;\n\t\t}\n\n\t\tappContext.setApplicationName(name);\n\t\tappContext.setApplicationType(\"Apache Flink\");\n\t\tappContext.setAMContainerSpec(amContainer);\n\t\tappContext.setResource(capability);\n\t\tif (yarnQueue != null) {\n\t\t\tappContext.setQueue(yarnQueue);\n\t\t}\n\n\t\tsetApplicationTags(appContext);\n\n\t\t// add a hook to clean up in case deployment fails\n\t\tThread deploymentFailureHook = new DeploymentFailureHook(yarnClient, yarnApplication, yarnFilesDir);\n\t\tRuntime.getRuntime().addShutdownHook(deploymentFailureHook);\n\t\tLOG.info(\"Submitting application master \" + appId);\n\t\tyarnClient.submitApplication(appContext);\n\n\t\tLOG.info(\"Waiting for the cluster to be allocated\");\n\t\tfinal long startTime = System.currentTimeMillis();\n\t\tApplicationReport report;\n\t\tYarnApplicationState lastAppState = YarnApplicationState.NEW;\n\t\tloop: while (true) {\n\t\t\ttry {\n\t\t\t\treport = yarnClient.getApplicationReport(appId);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new YarnDeploymentException(\"Failed to deploy the cluster.\", e);\n\t\t\t}\n\t\t\tYarnApplicationState appState = report.getYarnApplicationState();\n\t\t\tLOG.debug(\"Application State: {}\", appState);\n\t\t\tswitch(appState) {\n\t\t\t\tcase FAILED:\n\t\t\t\tcase FINISHED: //TODO: the finished state may be valid in flip-6\n\t\t\t\tcase KILLED:\n\t\t\t\t\tthrow new YarnDeploymentException(\"The YARN application unexpectedly switched to state \"\n\t\t\t\t\t\t+ appState + \" during deployment. \\n\" +\n\t\t\t\t\t\t\"Diagnostics from YARN: \" + report.getDiagnostics() + \"\\n\" +\n\t\t\t\t\t\t\"If log aggregation is enabled on your cluster, use this command to further investigate the issue:\\n\" +\n\t\t\t\t\t\t\"yarn logs -applicationId \" + appId);\n\t\t\t\t\t//break ..\n\t\t\t\tcase RUNNING:\n\t\t\t\t\tLOG.info(\"YARN application has been deployed successfully.\");\n\t\t\t\t\tbreak loop;\n\t\t\t\tdefault:\n\t\t\t\t\tif (appState != lastAppState) {\n\t\t\t\t\t\tLOG.info(\"Deploying cluster, current state \" + appState);\n\t\t\t\t\t}\n\t\t\t\t\tif (System.currentTimeMillis() - startTime > 60000) {\n\t\t\t\t\t\tLOG.info(\"Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\");\n\t\t\t\t\t}\n\n\t\t\t}\n\t\t\tlastAppState = appState;\n\t\t\tThread.sleep(250);\n\t\t}\n\t\t// print the application id for user to cancel themselves.\n\t\tif (isDetachedMode()) {\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop \" +\n\t\t\t\t\t\"it:\\nyarn application -kill \" + appId + \"\\nPlease also note that the \" +\n\t\t\t\t\t\"temporary files of the YARN session in the home directoy will not be removed.\");\n\t\t}\n\t\t// since deployment was successful, remove the hook\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(deploymentFailureHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we're already in the shut down hook.\n\t\t}\n\t\treturn report;\n\t}",
            " 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797 +\n 798 +\n 799 +\n 800 +\n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  \n 944  \n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  \n 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  ",
            "\tpublic ApplicationReport startAppMaster(\n\t\t\tConfiguration configuration,\n\t\t\tString yarnClusterEntrypoint,\n\t\t\tJobGraph jobGraph,\n\t\t\tYarnClient yarnClient,\n\t\t\tYarnClientApplication yarnApplication,\n\t\t\tClusterSpecification clusterSpecification) throws Exception {\n\n\t\t// ------------------ Initialize the file systems -------------------------\n\n\t\ttry {\n\t\t\torg.apache.flink.core.fs.FileSystem.initialize(configuration);\n\t\t} catch (IOException e) {\n\t\t\tthrow new IOException(\"Error while setting the default \" +\n\t\t\t\t\t\"filesystem scheme from configuration.\", e);\n\t\t}\n\n\t\t// initialize file system\n\t\t// Copy the application master jar to the filesystem\n\t\t// Create a local resource to point to the destination jar path\n\t\tfinal FileSystem fs = FileSystem.get(yarnConfiguration);\n\t\tfinal Path homeDir = fs.getHomeDirectory();\n\n\t\t// hard coded check for the GoogleHDFS client because its not overriding the getScheme() method.\n\t\tif (!fs.getClass().getSimpleName().equals(\"GoogleHadoopFileSystem\") &&\n\t\t\t\tfs.getScheme().startsWith(\"file\")) {\n\t\t\tLOG.warn(\"The file system scheme is '\" + fs.getScheme() + \"'. This indicates that the \"\n\t\t\t\t\t+ \"specified Hadoop configuration path is wrong and the system is using the default Hadoop configuration values.\"\n\t\t\t\t\t+ \"The Flink YARN client needs to store its files in a distributed file system\");\n\t\t}\n\n\t\tApplicationSubmissionContext appContext = yarnApplication.getApplicationSubmissionContext();\n\t\tSet<File> systemShipFiles = new HashSet<>(shipFiles.size());\n\t\tfor (File file : shipFiles) {\n\t\t\tsystemShipFiles.add(file.getAbsoluteFile());\n\t\t}\n\n\t\t//check if there is a logback or log4j file\n\t\tFile logbackFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOGBACK_NAME);\n\t\tfinal boolean hasLogback = logbackFile.exists();\n\t\tif (hasLogback) {\n\t\t\tsystemShipFiles.add(logbackFile);\n\t\t}\n\n\t\tFile log4jFile = new File(configurationDirectory + File.separator + CONFIG_FILE_LOG4J_NAME);\n\t\tfinal boolean hasLog4j = log4jFile.exists();\n\t\tif (hasLog4j) {\n\t\t\tsystemShipFiles.add(log4jFile);\n\t\t\tif (hasLogback) {\n\t\t\t\t// this means there is already a logback configuration file --> fail\n\t\t\t\tLOG.warn(\"The configuration directory ('\" + configurationDirectory + \"') contains both LOG4J and \" +\n\t\t\t\t\t\"Logback configuration files. Please delete or rename one of them.\");\n\t\t\t}\n\t\t}\n\n\t\taddLibFolderToShipFiles(systemShipFiles);\n\n\t\t// Set-up ApplicationSubmissionContext for the application\n\n\t\tfinal ApplicationId appId = appContext.getApplicationId();\n\n\t\t// ------------------ Add Zookeeper namespace to local flinkConfiguraton ------\n\t\tString zkNamespace = getZookeeperNamespace();\n\t\t// no user specified cli argument for namespace?\n\t\tif (zkNamespace == null || zkNamespace.isEmpty()) {\n\t\t\t// namespace defined in config? else use applicationId as default.\n\t\t\tzkNamespace = configuration.getString(HighAvailabilityOptions.HA_CLUSTER_ID, String.valueOf(appId));\n\t\t\tsetZookeeperNamespace(zkNamespace);\n\t\t}\n\n\t\tconfiguration.setString(HighAvailabilityOptions.HA_CLUSTER_ID, zkNamespace);\n\n\t\tif (HighAvailabilityMode.isHighAvailabilityModeActivated(configuration)) {\n\t\t\t// activate re-execution of failed applications\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tconfiguration.getInteger(\n\t\t\t\t\tYarnConfigOptions.APPLICATION_ATTEMPTS.key(),\n\t\t\t\t\tYarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS));\n\n\t\t\tactivateHighAvailabilitySupport(appContext);\n\t\t} else {\n\t\t\t// set number of application retries to 1 in the default case\n\t\t\tappContext.setMaxAppAttempts(\n\t\t\t\tconfiguration.getInteger(\n\t\t\t\t\tYarnConfigOptions.APPLICATION_ATTEMPTS.key(),\n\t\t\t\t\t1));\n\t\t}\n\n\t\tif (jobGraph != null) {\n\t\t\t// add the user code jars from the provided JobGraph\n\t\t\tfor (org.apache.flink.core.fs.Path path : jobGraph.getUserJars()) {\n\t\t\t\tuserJarFiles.add(new File(path.toUri()));\n\t\t\t}\n\t\t}\n\n\t\t// local resource map for Yarn\n\t\tfinal Map<String, LocalResource> localResources = new HashMap<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// list of remote paths (after upload)\n\t\tfinal List<Path> paths = new ArrayList<>(2 + systemShipFiles.size() + userJarFiles.size());\n\t\t// ship list that enables reuse of resources for task manager containers\n\t\tStringBuilder envShipFileList = new StringBuilder();\n\n\t\t// upload and register ship files\n\t\tList<String> systemClassPaths = uploadAndRegisterFiles(\n\t\t\tsystemShipFiles,\n\t\t\tfs,\n\t\t\thomeDir,\n\t\t\tappId,\n\t\t\tpaths,\n\t\t\tlocalResources,\n\t\t\tenvShipFileList);\n\n\t\tList<String> userClassPaths;\n\t\tif (userJarInclusion != YarnConfigOptions.UserJarInclusion.DISABLED) {\n\t\t\tuserClassPaths = uploadAndRegisterFiles(\n\t\t\t\tuserJarFiles,\n\t\t\t\tfs,\n\t\t\t\thomeDir,\n\t\t\t\tappId,\n\t\t\t\tpaths,\n\t\t\t\tlocalResources,\n\t\t\t\tenvShipFileList);\n\t\t} else {\n\t\t\tuserClassPaths = Collections.emptyList();\n\t\t}\n\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.ORDER) {\n\t\t\tsystemClassPaths.addAll(userClassPaths);\n\t\t}\n\n\t\t// normalize classpath by sorting\n\t\tCollections.sort(systemClassPaths);\n\t\tCollections.sort(userClassPaths);\n\n\t\t// classpath assembler\n\t\tStringBuilder classPathBuilder = new StringBuilder();\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.FIRST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\t\tfor (String classPath : systemClassPaths) {\n\t\t\tclassPathBuilder.append(classPath).append(File.pathSeparator);\n\t\t}\n\t\tif (userJarInclusion == YarnConfigOptions.UserJarInclusion.LAST) {\n\t\t\tfor (String userClassPath : userClassPaths) {\n\t\t\t\tclassPathBuilder.append(userClassPath).append(File.pathSeparator);\n\t\t\t}\n\t\t}\n\n\t\t// Setup jar for ApplicationMaster\n\t\tPath remotePathJar = setupSingleLocalResource(\n\t\t\t\"flink.jar\",\n\t\t\tfs,\n\t\t\tappId,\n\t\t\tflinkJarPath,\n\t\t\tlocalResources,\n\t\t\thomeDir,\n\t\t\t\"\");\n\n\t\tconfiguration.setInteger(\n\t\t\tConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS,\n\t\t\tclusterSpecification.getSlotsPerTaskManager());\n\n\t\t// Upload the flink configuration\n\t\t// write out configuration file\n\t\tFile tmpConfigurationFile = File.createTempFile(appId + \"-flink-conf.yaml\", null);\n\t\ttmpConfigurationFile.deleteOnExit();\n\t\tBootstrapTools.writeConfiguration(configuration, tmpConfigurationFile);\n\n\t\tPath remotePathConf = setupSingleLocalResource(\n\t\t\t\"flink-conf.yaml\",\n\t\t\tfs,\n\t\t\tappId,\n\t\t\tnew Path(tmpConfigurationFile.getAbsolutePath()),\n\t\t\tlocalResources,\n\t\t\thomeDir,\n\t\t\t\"\");\n\n\t\tpaths.add(remotePathJar);\n\t\tclassPathBuilder.append(\"flink.jar\").append(File.pathSeparator);\n\t\tpaths.add(remotePathConf);\n\t\tclassPathBuilder.append(\"flink-conf.yaml\").append(File.pathSeparator);\n\n\t\t// write job graph to tmp file and add it to local resource\n\t\t// TODO: server use user main method to generate job graph\n\t\tif (jobGraph != null) {\n\t\t\ttry {\n\t\t\t\tFile fp = File.createTempFile(appId.toString(), null);\n\t\t\t\tfp.deleteOnExit();\n\t\t\t\ttry (FileOutputStream output = new FileOutputStream(fp);\n\t\t\t\t\tObjectOutputStream obOutput = new ObjectOutputStream(output);){\n\t\t\t\t\tobOutput.writeObject(jobGraph);\n\t\t\t\t}\n\n\t\t\t\tPath pathFromYarnURL = setupSingleLocalResource(\n\t\t\t\t\t\"job.graph\",\n\t\t\t\t\tfs,\n\t\t\t\t\tappId,\n\t\t\t\t\tnew Path(fp.toURI()),\n\t\t\t\t\tlocalResources,\n\t\t\t\t\thomeDir,\n\t\t\t\t\t\"\");\n\t\t\t\tpaths.add(pathFromYarnURL);\n\t\t\t\tclassPathBuilder.append(\"job.graph\").append(File.pathSeparator);\n\t\t\t} catch (Exception e) {\n\t\t\t\tLOG.warn(\"Add job graph to local resource fail\");\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\n\t\tPath yarnFilesDir = new Path(homeDir, \".flink/\" + appId + '/');\n\n\t\tFsPermission permission = new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE);\n\t\tfs.setPermission(yarnFilesDir, permission); // set permission for path.\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\t//In Integration test setup, the Yarn containers created by YarnMiniCluster does not have the Yarn site XML\n\t\t//and KRB5 configuration files. We are adding these files as container local resources for the container\n\t\t//applications (JM/TMs) to have proper secure cluster setup\n\t\tPath remoteKrb5Path = null;\n\t\tPath remoteYarnSiteXmlPath = null;\n\t\tboolean hasKrb5 = false;\n\t\tif (System.getenv(\"IN_TESTS\") != null) {\n\t\t\tString krb5Config = System.getProperty(\"java.security.krb5.conf\");\n\t\t\tif (krb5Config != null && krb5Config.length() != 0) {\n\t\t\t\tFile krb5 = new File(krb5Config);\n\t\t\t\tLOG.info(\"Adding KRB5 configuration {} to the AM container local resource bucket\", krb5.getAbsolutePath());\n\t\t\t\tPath krb5ConfPath = new Path(krb5.getAbsolutePath());\n\t\t\t\tremoteKrb5Path = setupSingleLocalResource(\n\t\t\t\t\tUtils.KRB5_FILE_NAME,\n\t\t\t\t\tfs,\n\t\t\t\t\tappId,\n\t\t\t\t\tkrb5ConfPath,\n\t\t\t\t\tlocalResources,\n\t\t\t\t\thomeDir,\n\t\t\t\t\t\"\");\n\n\t\t\t\tFile f = new File(System.getenv(\"YARN_CONF_DIR\"), Utils.YARN_SITE_FILE_NAME);\n\t\t\t\tLOG.info(\"Adding Yarn configuration {} to the AM container local resource bucket\", f.getAbsolutePath());\n\t\t\t\tPath yarnSitePath = new Path(f.getAbsolutePath());\n\t\t\t\tremoteYarnSiteXmlPath = setupSingleLocalResource(\n\t\t\t\t\tUtils.YARN_SITE_FILE_NAME,\n\t\t\t\t\tfs,\n\t\t\t\t\tappId,\n\t\t\t\t\tyarnSitePath,\n\t\t\t\t\tlocalResources,\n\t\t\t\t\thomeDir,\n\t\t\t\t\t\"\");\n\t\t\t\thasKrb5 = true;\n\t\t\t}\n\t\t}\n\n\t\t// setup security tokens\n\t\tPath remotePathKeytab = null;\n\t\tString keytab = configuration.getString(SecurityOptions.KERBEROS_LOGIN_KEYTAB);\n\t\tif (keytab != null) {\n\t\t\tLOG.info(\"Adding keytab {} to the AM container local resource bucket\", keytab);\n\t\t\tremotePathKeytab = setupSingleLocalResource(\n\t\t\t\tUtils.KEYTAB_FILE_NAME,\n\t\t\t\tfs,\n\t\t\t\tappId,\n\t\t\t\tnew Path(keytab),\n\t\t\t\tlocalResources,\n\t\t\t\thomeDir,\n\t\t\t\t\"\");\n\t\t}\n\n\t\tfinal ContainerLaunchContext amContainer = setupApplicationMasterContainer(\n\t\t\tyarnClusterEntrypoint,\n\t\t\thasLogback,\n\t\t\thasLog4j,\n\t\t\thasKrb5,\n\t\t\tclusterSpecification.getMasterMemoryMB());\n\n\t\tif (UserGroupInformation.isSecurityEnabled()) {\n\t\t\t// set HDFS delegation tokens when security is enabled\n\t\t\tLOG.info(\"Adding delegation token to the AM container..\");\n\t\t\tUtils.setTokensFor(amContainer, paths, yarnConfiguration);\n\t\t}\n\n\t\tamContainer.setLocalResources(localResources);\n\t\tfs.close();\n\n\t\t// Setup CLASSPATH and environment variables for ApplicationMaster\n\t\tfinal Map<String, String> appMasterEnv = new HashMap<>();\n\t\t// set user specified app master environment variables\n\t\tappMasterEnv.putAll(Utils.getEnvironmentVariables(ResourceManagerOptions.CONTAINERIZED_MASTER_ENV_PREFIX, configuration));\n\t\t// set Flink app class path\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_FLINK_CLASSPATH, classPathBuilder.toString());\n\n\t\t// set Flink on YARN internal configuration values\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_COUNT, String.valueOf(clusterSpecification.getNumberTaskManagers()));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_TM_MEMORY, String.valueOf(clusterSpecification.getTaskManagerMemoryMB()));\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_JAR_PATH, remotePathJar.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_APP_ID, appId.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_HOME_DIR, homeDir.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_CLIENT_SHIP_FILES, envShipFileList.toString());\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_SLOTS, String.valueOf(clusterSpecification.getSlotsPerTaskManager()));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_DETACHED, String.valueOf(detached));\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_ZOOKEEPER_NAMESPACE, getZookeeperNamespace());\n\t\tappMasterEnv.put(YarnConfigKeys.FLINK_YARN_FILES, yarnFilesDir.toUri().toString());\n\n\t\t// https://github.com/apache/hadoop/blob/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnApplicationSecurity.md#identity-on-an-insecure-cluster-hadoop_user_name\n\t\tappMasterEnv.put(YarnConfigKeys.ENV_HADOOP_USER_NAME, UserGroupInformation.getCurrentUser().getUserName());\n\n\t\tif (remotePathKeytab != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PATH, remotePathKeytab.toString());\n\t\t\tString principal = configuration.getString(SecurityOptions.KERBEROS_LOGIN_PRINCIPAL);\n\t\t\tappMasterEnv.put(YarnConfigKeys.KEYTAB_PRINCIPAL, principal);\n\t\t}\n\n\t\t//To support Yarn Secure Integration Test Scenario\n\t\tif (remoteYarnSiteXmlPath != null && remoteKrb5Path != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_YARN_SITE_XML_PATH, remoteYarnSiteXmlPath.toString());\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_KRB5_PATH, remoteKrb5Path.toString());\n\t\t}\n\n\t\tif (dynamicPropertiesEncoded != null) {\n\t\t\tappMasterEnv.put(YarnConfigKeys.ENV_DYNAMIC_PROPERTIES, dynamicPropertiesEncoded);\n\t\t}\n\n\t\t// set classpath from YARN configuration\n\t\tUtils.setupYarnClassPath(yarnConfiguration, appMasterEnv);\n\n\t\tamContainer.setEnvironment(appMasterEnv);\n\n\t\t// Set up resource type requirements for ApplicationMaster\n\t\tResource capability = Records.newRecord(Resource.class);\n\t\tcapability.setMemory(clusterSpecification.getMasterMemoryMB());\n\t\tcapability.setVirtualCores(1);\n\n\t\tString name;\n\t\tif (customName == null) {\n\t\t\tname = \"Flink session with \" + clusterSpecification.getNumberTaskManagers() + \" TaskManagers\";\n\t\t\tif (detached) {\n\t\t\t\tname += \" (detached)\";\n\t\t\t}\n\t\t} else {\n\t\t\tname = customName;\n\t\t}\n\n\t\tappContext.setApplicationName(name);\n\t\tappContext.setApplicationType(\"Apache Flink\");\n\t\tappContext.setAMContainerSpec(amContainer);\n\t\tappContext.setResource(capability);\n\t\tif (yarnQueue != null) {\n\t\t\tappContext.setQueue(yarnQueue);\n\t\t}\n\n\t\tsetApplicationTags(appContext);\n\n\t\t// add a hook to clean up in case deployment fails\n\t\tThread deploymentFailureHook = new DeploymentFailureHook(yarnClient, yarnApplication, yarnFilesDir);\n\t\tRuntime.getRuntime().addShutdownHook(deploymentFailureHook);\n\t\tLOG.info(\"Submitting application master \" + appId);\n\t\tyarnClient.submitApplication(appContext);\n\n\t\tLOG.info(\"Waiting for the cluster to be allocated\");\n\t\tfinal long startTime = System.currentTimeMillis();\n\t\tApplicationReport report;\n\t\tYarnApplicationState lastAppState = YarnApplicationState.NEW;\n\t\tloop: while (true) {\n\t\t\ttry {\n\t\t\t\treport = yarnClient.getApplicationReport(appId);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new YarnDeploymentException(\"Failed to deploy the cluster.\", e);\n\t\t\t}\n\t\t\tYarnApplicationState appState = report.getYarnApplicationState();\n\t\t\tLOG.debug(\"Application State: {}\", appState);\n\t\t\tswitch(appState) {\n\t\t\t\tcase FAILED:\n\t\t\t\tcase FINISHED: //TODO: the finished state may be valid in flip-6\n\t\t\t\tcase KILLED:\n\t\t\t\t\tthrow new YarnDeploymentException(\"The YARN application unexpectedly switched to state \"\n\t\t\t\t\t\t+ appState + \" during deployment. \\n\" +\n\t\t\t\t\t\t\"Diagnostics from YARN: \" + report.getDiagnostics() + \"\\n\" +\n\t\t\t\t\t\t\"If log aggregation is enabled on your cluster, use this command to further investigate the issue:\\n\" +\n\t\t\t\t\t\t\"yarn logs -applicationId \" + appId);\n\t\t\t\t\t//break ..\n\t\t\t\tcase RUNNING:\n\t\t\t\t\tLOG.info(\"YARN application has been deployed successfully.\");\n\t\t\t\t\tbreak loop;\n\t\t\t\tdefault:\n\t\t\t\t\tif (appState != lastAppState) {\n\t\t\t\t\t\tLOG.info(\"Deploying cluster, current state \" + appState);\n\t\t\t\t\t}\n\t\t\t\t\tif (System.currentTimeMillis() - startTime > 60000) {\n\t\t\t\t\t\tLOG.info(\"Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\");\n\t\t\t\t\t}\n\n\t\t\t}\n\t\t\tlastAppState = appState;\n\t\t\tThread.sleep(250);\n\t\t}\n\t\t// print the application id for user to cancel themselves.\n\t\tif (isDetachedMode()) {\n\t\t\tLOG.info(\"The Flink YARN client has been started in detached mode. In order to stop \" +\n\t\t\t\t\t\"Flink on YARN, use the following command or a YARN web interface to stop \" +\n\t\t\t\t\t\"it:\\nyarn application -kill \" + appId + \"\\nPlease also note that the \" +\n\t\t\t\t\t\"temporary files of the YARN session in the home directoy will not be removed.\");\n\t\t}\n\t\t// since deployment was successful, remove the hook\n\t\ttry {\n\t\t\tRuntime.getRuntime().removeShutdownHook(deploymentFailureHook);\n\t\t} catch (IllegalStateException e) {\n\t\t\t// we're already in the shut down hook.\n\t\t}\n\t\treturn report;\n\t}"
        ]
    ],
    "4debc6033cc2099b763d7fef86829b9f6734d91a": [
        [
            "DispatcherTest::testJobSubmission()",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 -\n  91 -\n  92 -\n  93 -\n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t/**\n\t * Tests that we can submit a job to the Dispatcher which then spawns a\n\t * new JobManagerRunner.\n\t */\n\t@Test\n\tpublic void testJobSubmission() throws Exception {\n\t\tTestingFatalErrorHandler fatalErrorHandler = new TestingFatalErrorHandler();\n\t\tHighAvailabilityServices haServices = new StandaloneHaServices(\n\t\t\t\"localhost\",\n\t\t\t\"localhost\",\n\t\t\t\"localhost\");\n\t\tHeartbeatServices heartbeatServices = new HeartbeatServices(1000L, 10000L);\n\t\tJobManagerRunner jobManagerRunner = mock(JobManagerRunner.class);\n\n\t\tfinal JobGraph jobGraph = mock(JobGraph.class);\n\t\tfinal JobID jobId = new JobID();\n\t\twhen(jobGraph.getJobID()).thenReturn(jobId);\n\n\t\tfinal TestingDispatcher dispatcher = new TestingDispatcher(\n\t\t\trpcService,\n\t\t\tDispatcher.DISPATCHER_NAME,\n\t\t\tnew Configuration(),\n\t\t\thaServices,\n\t\t\tmock(BlobServer.class),\n\t\t\theartbeatServices,\n\t\t\tmock(MetricRegistry.class),\n\t\t\tfatalErrorHandler,\n\t\t\tjobManagerRunner,\n\t\t\tjobId);\n\n\t\ttry {\n\t\t\tdispatcher.start();\n\n\t\t\tDispatcherGateway dispatcherGateway = dispatcher.getSelfGateway(DispatcherGateway.class);\n\n\t\t\tCompletableFuture<Acknowledge> acknowledgeFuture = dispatcherGateway.submitJob(jobGraph, timeout);\n\n\t\t\tacknowledgeFuture.get();\n\n\t\t\tverify(jobManagerRunner, Mockito.timeout(timeout.toMilliseconds())).start();\n\n\t\t\t// check that no error has occurred\n\t\t\tfatalErrorHandler.rethrowError();\n\t\t} finally {\n\t\t\tRpcUtils.terminateRpcEndpoint(dispatcher, timeout);\n\t\t}\n\t}",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95 +\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 +\n 119 +\n 120 +\n 121 +\n 122 +\n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  ",
            "\t/**\n\t * Tests that we can submit a job to the Dispatcher which then spawns a\n\t * new JobManagerRunner.\n\t */\n\t@Test\n\tpublic void testJobSubmission() throws Exception {\n\t\tTestingFatalErrorHandler fatalErrorHandler = new TestingFatalErrorHandler();\n\n\t\tTestingLeaderElectionService dispatcherLeaderElectionService = new TestingLeaderElectionService();\n\t\tTestingHighAvailabilityServices haServices = new TestingHighAvailabilityServices();\n\t\thaServices.setDispatcherLeaderElectionService(dispatcherLeaderElectionService);\n\t\thaServices.setSubmittedJobGraphStore(new StandaloneSubmittedJobGraphStore());\n\n\t\tHeartbeatServices heartbeatServices = new HeartbeatServices(1000L, 10000L);\n\t\tJobManagerRunner jobManagerRunner = mock(JobManagerRunner.class);\n\n\t\tfinal JobGraph jobGraph = mock(JobGraph.class);\n\t\tfinal JobID jobId = new JobID();\n\t\twhen(jobGraph.getJobID()).thenReturn(jobId);\n\n\t\tfinal TestingDispatcher dispatcher = new TestingDispatcher(\n\t\t\trpcService,\n\t\t\tDispatcher.DISPATCHER_NAME,\n\t\t\tnew Configuration(),\n\t\t\thaServices,\n\t\t\tmock(BlobServer.class),\n\t\t\theartbeatServices,\n\t\t\tmock(MetricRegistry.class),\n\t\t\tfatalErrorHandler,\n\t\t\tjobManagerRunner,\n\t\t\tjobId);\n\n\t\ttry {\n\t\t\tdispatcher.start();\n\n\t\t\tCompletableFuture<UUID> leaderFuture = dispatcherLeaderElectionService.isLeader(UUID.randomUUID());\n\n\t\t\t// wait for the leader to be elected\n\t\t\tleaderFuture.get();\n\n\t\t\tDispatcherGateway dispatcherGateway = dispatcher.getSelfGateway(DispatcherGateway.class);\n\n\t\t\tCompletableFuture<Acknowledge> acknowledgeFuture = dispatcherGateway.submitJob(jobGraph, timeout);\n\n\t\t\tacknowledgeFuture.get();\n\n\t\t\tverify(jobManagerRunner, Mockito.timeout(timeout.toMilliseconds())).start();\n\n\t\t\t// check that no error has occurred\n\t\t\tfatalErrorHandler.rethrowError();\n\t\t} finally {\n\t\t\tRpcUtils.terminateRpcEndpoint(dispatcher, timeout);\n\t\t}\n\t}"
        ]
    ],
    "727370aacf63cefc6aed7c46dc2d63517e4b708d": [
        [
            "CheckpointCoordinator::restoreSavepoint(String,boolean,Map,ClassLoader)",
            "1059  \n1060  \n1061  \n1062  \n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079 -\n1080 -\n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094 -\n1095  \n1096  \n1097  ",
            "\t/**\n\t * Restore the state with given savepoint.\n\t *\n\t * @param savepointPointer The pointer to the savepoint.\n\t * @param allowNonRestored True if allowing checkpoint state that cannot be\n\t *                         mapped to any job vertex in tasks.\n\t * @param tasks            Map of job vertices to restore. State for these\n\t *                         vertices is restored via\n\t *                         {@link Execution#setInitialState(JobManagerTaskRestore)}.\n\t * @param userClassLoader  The class loader to resolve serialized classes in\n\t *                         legacy savepoint versions.\n\t */\n\tpublic boolean restoreSavepoint(\n\t\t\tString savepointPointer,\n\t\t\tboolean allowNonRestored,\n\t\t\tMap<JobVertexID, ExecutionJobVertex> tasks,\n\t\t\tClassLoader userClassLoader) throws Exception {\n\n\t\tPreconditions.checkNotNull(savepointPointer, \"The savepoint path cannot be null.\");\n\n\t\tLOG.info(\"Starting job from savepoint {} ({})\",\n\t\t\t\tsavepointPointer, (allowNonRestored ? \"allowing non restored state\" : \"\"));\n\n\t\tfinal CompletedCheckpointStorageLocation checkpointLocation = checkpointStorage.resolveCheckpoint(savepointPointer);\n\n\t\t// Load the savepoint as a checkpoint into the system\n\t\tCompletedCheckpoint savepoint = Checkpoints.loadAndValidateCheckpoint(\n\t\t\t\tjob, tasks, checkpointLocation, userClassLoader, allowNonRestored);\n\n\t\tcompletedCheckpointStore.addCheckpoint(savepoint);\n\n\t\t// Reset the checkpoint ID counter\n\t\tlong nextCheckpointId = savepoint.getCheckpointID() + 1;\n\t\tcheckpointIdCounter.setCount(nextCheckpointId);\n\n\t\tLOG.info(\"Reset the checkpoint ID to {}.\", nextCheckpointId);\n\n\t\treturn restoreLatestCheckpointedState(tasks, true, allowNonRestored);\n\t}",
            "1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084 +\n1085 +\n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099 +\n1100  \n1101  \n1102  ",
            "\t/**\n\t * Restore the state with given savepoint.\n\t *\n\t * @param savepointPointer The pointer to the savepoint.\n\t * @param allowNonRestored True if allowing checkpoint state that cannot be\n\t *                         mapped to any job vertex in tasks.\n\t * @param tasks            Map of job vertices to restore. State for these\n\t *                         vertices is restored via\n\t *                         {@link Execution#setInitialState(JobManagerTaskRestore)}.\n\t * @param userClassLoader  The class loader to resolve serialized classes in\n\t *                         legacy savepoint versions.\n\t */\n\tpublic boolean restoreSavepoint(\n\t\t\tString savepointPointer,\n\t\t\tboolean allowNonRestored,\n\t\t\tMap<JobVertexID, ExecutionJobVertex> tasks,\n\t\t\tClassLoader userClassLoader) throws Exception {\n\n\t\tPreconditions.checkNotNull(savepointPointer, \"The savepoint path cannot be null.\");\n\n\t\tLOG.info(\"Starting job {} from savepoint {} ({})\",\n\t\t\t\tjob, savepointPointer, (allowNonRestored ? \"allowing non restored state\" : \"\"));\n\n\t\tfinal CompletedCheckpointStorageLocation checkpointLocation = checkpointStorage.resolveCheckpoint(savepointPointer);\n\n\t\t// Load the savepoint as a checkpoint into the system\n\t\tCompletedCheckpoint savepoint = Checkpoints.loadAndValidateCheckpoint(\n\t\t\t\tjob, tasks, checkpointLocation, userClassLoader, allowNonRestored);\n\n\t\tcompletedCheckpointStore.addCheckpoint(savepoint);\n\n\t\t// Reset the checkpoint ID counter\n\t\tlong nextCheckpointId = savepoint.getCheckpointID() + 1;\n\t\tcheckpointIdCounter.setCount(nextCheckpointId);\n\n\t\tLOG.info(\"Reset the checkpoint ID of job {} to {}.\", job, nextCheckpointId);\n\n\t\treturn restoreLatestCheckpointedState(tasks, true, allowNonRestored);\n\t}"
        ],
        [
            "CheckpointCoordinator::completePendingCheckpoint(PendingCheckpoint)",
            " 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837 -\n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860 -\n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  ",
            "\t/**\n\t * Try to complete the given pending checkpoint.\n\t *\n\t * <p>Important: This method should only be called in the checkpoint lock scope.\n\t *\n\t * @param pendingCheckpoint to complete\n\t * @throws CheckpointException if the completion failed\n\t */\n\tprivate void completePendingCheckpoint(PendingCheckpoint pendingCheckpoint) throws CheckpointException {\n\t\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\t\tfinal CompletedCheckpoint completedCheckpoint;\n\n\t\t// As a first step to complete the checkpoint, we register its state with the registry\n\t\tMap<OperatorID, OperatorState> operatorStates = pendingCheckpoint.getOperatorStates();\n\t\tsharedStateRegistry.registerAll(operatorStates.values());\n\n\t\ttry {\n\t\t\ttry {\n\t\t\t\tcompletedCheckpoint = pendingCheckpoint.finalizeCheckpoint();\n\t\t\t}\n\t\t\tcatch (Exception e1) {\n\t\t\t\t// abort the current pending checkpoint if we fails to finalize the pending checkpoint.\n\t\t\t\tif (!pendingCheckpoint.isDiscarded()) {\n\t\t\t\t\tpendingCheckpoint.abortError(e1);\n\t\t\t\t}\n\n\t\t\t\tthrow new CheckpointException(\"Could not finalize the pending checkpoint \" + checkpointId + '.', e1);\n\t\t\t}\n\n\t\t\t// the pending checkpoint must be discarded after the finalization\n\t\t\tPreconditions.checkState(pendingCheckpoint.isDiscarded() && completedCheckpoint != null);\n\n\t\t\t// TODO: add savepoints to completed checkpoint store once FLINK-4815 has been completed\n\t\t\tif (!completedCheckpoint.getProperties().isSavepoint()) {\n\t\t\t\ttry {\n\t\t\t\t\tcompletedCheckpointStore.addCheckpoint(completedCheckpoint);\n\t\t\t\t} catch (Exception exception) {\n\t\t\t\t\t// we failed to store the completed checkpoint. Let's clean up\n\t\t\t\t\texecutor.execute(new Runnable() {\n\t\t\t\t\t\t@Override\n\t\t\t\t\t\tpublic void run() {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tcompletedCheckpoint.discardOnFailedStoring();\n\t\t\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\t\t\tLOG.warn(\"Could not properly discard completed checkpoint {}.\", completedCheckpoint.getCheckpointID(), t);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\n\t\t\t\t\tthrow new CheckpointException(\"Could not complete the pending checkpoint \" + checkpointId + '.', exception);\n\t\t\t\t}\n\n\t\t\t\t// drop those pending checkpoints that are at prior to the completed one\n\t\t\t\tdropSubsumedCheckpoints(checkpointId);\n\t\t\t}\n\t\t} finally {\n\t\t\tpendingCheckpoints.remove(checkpointId);\n\n\t\t\ttriggerQueuedRequests();\n\t\t}\n\n\t\trememberRecentCheckpointId(checkpointId);\n\n\t\t// record the time when this was completed, to calculate\n\t\t// the 'min delay between checkpoints'\n\t\tlastCheckpointCompletionNanos = System.nanoTime();\n\n\t\tLOG.info(\"Completed checkpoint {} ({} bytes in {} ms).\", checkpointId,\n\t\t\tcompletedCheckpoint.getStateSize(), completedCheckpoint.getDuration());\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tStringBuilder builder = new StringBuilder();\n\t\t\tbuilder.append(\"Checkpoint state: \");\n\t\t\tfor (OperatorState state : completedCheckpoint.getOperatorStates().values()) {\n\t\t\t\tbuilder.append(state);\n\t\t\t\tbuilder.append(\", \");\n\t\t\t}\n\t\t\t// Remove last two chars \", \"\n\t\t\tbuilder.setLength(builder.length() - 2);\n\n\t\t\tLOG.debug(builder.toString());\n\t\t}\n\n\t\t// send the \"notify complete\" call to all vertices\n\t\tfinal long timestamp = completedCheckpoint.getTimestamp();\n\n\t\tfor (ExecutionVertex ev : tasksToCommitTo) {\n\t\t\tExecution ee = ev.getCurrentExecutionAttempt();\n\t\t\tif (ee != null) {\n\t\t\t\tee.notifyCheckpointComplete(checkpointId, timestamp);\n\t\t\t}\n\t\t}\n\t}",
            " 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842 +\n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865 +\n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  ",
            "\t/**\n\t * Try to complete the given pending checkpoint.\n\t *\n\t * <p>Important: This method should only be called in the checkpoint lock scope.\n\t *\n\t * @param pendingCheckpoint to complete\n\t * @throws CheckpointException if the completion failed\n\t */\n\tprivate void completePendingCheckpoint(PendingCheckpoint pendingCheckpoint) throws CheckpointException {\n\t\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\t\tfinal CompletedCheckpoint completedCheckpoint;\n\n\t\t// As a first step to complete the checkpoint, we register its state with the registry\n\t\tMap<OperatorID, OperatorState> operatorStates = pendingCheckpoint.getOperatorStates();\n\t\tsharedStateRegistry.registerAll(operatorStates.values());\n\n\t\ttry {\n\t\t\ttry {\n\t\t\t\tcompletedCheckpoint = pendingCheckpoint.finalizeCheckpoint();\n\t\t\t}\n\t\t\tcatch (Exception e1) {\n\t\t\t\t// abort the current pending checkpoint if we fails to finalize the pending checkpoint.\n\t\t\t\tif (!pendingCheckpoint.isDiscarded()) {\n\t\t\t\t\tpendingCheckpoint.abortError(e1);\n\t\t\t\t}\n\n\t\t\t\tthrow new CheckpointException(\"Could not finalize the pending checkpoint \" + checkpointId + '.', e1);\n\t\t\t}\n\n\t\t\t// the pending checkpoint must be discarded after the finalization\n\t\t\tPreconditions.checkState(pendingCheckpoint.isDiscarded() && completedCheckpoint != null);\n\n\t\t\t// TODO: add savepoints to completed checkpoint store once FLINK-4815 has been completed\n\t\t\tif (!completedCheckpoint.getProperties().isSavepoint()) {\n\t\t\t\ttry {\n\t\t\t\t\tcompletedCheckpointStore.addCheckpoint(completedCheckpoint);\n\t\t\t\t} catch (Exception exception) {\n\t\t\t\t\t// we failed to store the completed checkpoint. Let's clean up\n\t\t\t\t\texecutor.execute(new Runnable() {\n\t\t\t\t\t\t@Override\n\t\t\t\t\t\tpublic void run() {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tcompletedCheckpoint.discardOnFailedStoring();\n\t\t\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\t\t\tLOG.warn(\"Could not properly discard completed checkpoint {} of job {}.\", completedCheckpoint.getCheckpointID(), job, t);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\n\t\t\t\t\tthrow new CheckpointException(\"Could not complete the pending checkpoint \" + checkpointId + '.', exception);\n\t\t\t\t}\n\n\t\t\t\t// drop those pending checkpoints that are at prior to the completed one\n\t\t\t\tdropSubsumedCheckpoints(checkpointId);\n\t\t\t}\n\t\t} finally {\n\t\t\tpendingCheckpoints.remove(checkpointId);\n\n\t\t\ttriggerQueuedRequests();\n\t\t}\n\n\t\trememberRecentCheckpointId(checkpointId);\n\n\t\t// record the time when this was completed, to calculate\n\t\t// the 'min delay between checkpoints'\n\t\tlastCheckpointCompletionNanos = System.nanoTime();\n\n\t\tLOG.info(\"Completed checkpoint {} for job {} ({} bytes in {} ms).\", checkpointId, job,\n\t\t\tcompletedCheckpoint.getStateSize(), completedCheckpoint.getDuration());\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tStringBuilder builder = new StringBuilder();\n\t\t\tbuilder.append(\"Checkpoint state: \");\n\t\t\tfor (OperatorState state : completedCheckpoint.getOperatorStates().values()) {\n\t\t\t\tbuilder.append(state);\n\t\t\t\tbuilder.append(\", \");\n\t\t\t}\n\t\t\t// Remove last two chars \", \"\n\t\t\tbuilder.setLength(builder.length() - 2);\n\n\t\t\tLOG.debug(builder.toString());\n\t\t}\n\n\t\t// send the \"notify complete\" call to all vertices\n\t\tfinal long timestamp = completedCheckpoint.getTimestamp();\n\n\t\tfor (ExecutionVertex ev : tasksToCommitTo) {\n\t\t\tExecution ee = ev.getCurrentExecutionAttempt();\n\t\t\tif (ee != null) {\n\t\t\t\tee.notifyCheckpointComplete(checkpointId, timestamp);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "CheckpointCoordinator::discardCheckpoint(PendingCheckpoint,Throwable)",
            "1222  \n1223  \n1224  \n1225  \n1226  \n1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236 -\n1237  \n1238  \n1239  \n1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  ",
            "\t/**\n\t * Discards the given pending checkpoint because of the given cause.\n\t *\n\t * @param pendingCheckpoint to discard\n\t * @param cause for discarding the checkpoint\n\t */\n\tprivate void discardCheckpoint(PendingCheckpoint pendingCheckpoint, @Nullable Throwable cause) {\n\t\tassert(Thread.holdsLock(lock));\n\t\tPreconditions.checkNotNull(pendingCheckpoint);\n\n\t\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\n\t\tfinal String reason = (cause != null) ? cause.getMessage() : \"\";\n\n\t\tLOG.info(\"Discarding checkpoint {} because: {}\", checkpointId, reason);\n\n\t\tpendingCheckpoint.abortDeclined();\n\t\trememberRecentCheckpointId(checkpointId);\n\n\t\t// we don't have to schedule another \"dissolving\" checkpoint any more because the\n\t\t// cancellation barriers take care of breaking downstream alignments\n\t\t// we only need to make sure that suspended queued requests are resumed\n\n\t\tboolean haveMoreRecentPending = false;\n\t\tfor (PendingCheckpoint p : pendingCheckpoints.values()) {\n\t\t\tif (!p.isDiscarded() && p.getCheckpointId() >= pendingCheckpoint.getCheckpointId()) {\n\t\t\t\thaveMoreRecentPending = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!haveMoreRecentPending) {\n\t\t\ttriggerQueuedRequests();\n\t\t}\n\t}",
            "1227  \n1228  \n1229  \n1230  \n1231  \n1232  \n1233  \n1234  \n1235  \n1236  \n1237  \n1238  \n1239  \n1240  \n1241 +\n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255  \n1256  \n1257  \n1258  \n1259  \n1260  \n1261  ",
            "\t/**\n\t * Discards the given pending checkpoint because of the given cause.\n\t *\n\t * @param pendingCheckpoint to discard\n\t * @param cause for discarding the checkpoint\n\t */\n\tprivate void discardCheckpoint(PendingCheckpoint pendingCheckpoint, @Nullable Throwable cause) {\n\t\tassert(Thread.holdsLock(lock));\n\t\tPreconditions.checkNotNull(pendingCheckpoint);\n\n\t\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\n\t\tfinal String reason = (cause != null) ? cause.getMessage() : \"\";\n\n\t\tLOG.info(\"Discarding checkpoint {} of job {} because: {}\", checkpointId, job, reason);\n\n\t\tpendingCheckpoint.abortDeclined();\n\t\trememberRecentCheckpointId(checkpointId);\n\n\t\t// we don't have to schedule another \"dissolving\" checkpoint any more because the\n\t\t// cancellation barriers take care of breaking downstream alignments\n\t\t// we only need to make sure that suspended queued requests are resumed\n\n\t\tboolean haveMoreRecentPending = false;\n\t\tfor (PendingCheckpoint p : pendingCheckpoints.values()) {\n\t\t\tif (!p.isDiscarded() && p.getCheckpointId() >= pendingCheckpoint.getCheckpointId()) {\n\t\t\t\thaveMoreRecentPending = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!haveMoreRecentPending) {\n\t\t\ttriggerQueuedRequests();\n\t\t}\n\t}"
        ],
        [
            "CheckpointCoordinator::restoreLatestCheckpointedState(Map,boolean,boolean)",
            " 964  \n 965  \n 966  \n 967  \n 968  \n 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010 -\n1011  \n1012  \n1013  \n1014  \n1015  \n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023 -\n1024  \n1025  \n1026  \n1027  \n1028  \n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  ",
            "\t/**\n\t * Restores the latest checkpointed state.\n\t *\n\t * @param tasks Map of job vertices to restore. State for these vertices is\n\t * restored via {@link Execution#setInitialState(JobManagerTaskRestore)}.\n\t * @param errorIfNoCheckpoint Fail if no completed checkpoint is available to\n\t * restore from.\n\t * @param allowNonRestoredState Allow checkpoint state that cannot be mapped\n\t * to any job vertex in tasks.\n\t * @return <code>true</code> if state was restored, <code>false</code> otherwise.\n\t * @throws IllegalStateException If the CheckpointCoordinator is shut down.\n\t * @throws IllegalStateException If no completed checkpoint is available and\n\t *                               the <code>failIfNoCheckpoint</code> flag has been set.\n\t * @throws IllegalStateException If the checkpoint contains state that cannot be\n\t *                               mapped to any job vertex in <code>tasks</code> and the\n\t *                               <code>allowNonRestoredState</code> flag has not been set.\n\t * @throws IllegalStateException If the max parallelism changed for an operator\n\t *                               that restores state from this checkpoint.\n\t * @throws IllegalStateException If the parallelism changed for an operator\n\t *                               that restores <i>non-partitioned</i> state from this\n\t *                               checkpoint.\n\t */\n\tpublic boolean restoreLatestCheckpointedState(\n\t\t\tMap<JobVertexID, ExecutionJobVertex> tasks,\n\t\t\tboolean errorIfNoCheckpoint,\n\t\t\tboolean allowNonRestoredState) throws Exception {\n\n\t\tsynchronized (lock) {\n\t\t\tif (shutdown) {\n\t\t\t\tthrow new IllegalStateException(\"CheckpointCoordinator is shut down\");\n\t\t\t}\n\n\t\t\t// We create a new shared state registry object, so that all pending async disposal requests from previous\n\t\t\t// runs will go against the old object (were they can do no harm).\n\t\t\t// This must happen under the checkpoint lock.\n\t\t\tsharedStateRegistry.close();\n\t\t\tsharedStateRegistry = sharedStateRegistryFactory.create(executor);\n\n\t\t\t// Recover the checkpoints, TODO this could be done only when there is a new leader, not on each recovery\n\t\t\tcompletedCheckpointStore.recover();\n\n\t\t\t// Now, we re-register all (shared) states from the checkpoint store with the new registry\n\t\t\tfor (CompletedCheckpoint completedCheckpoint : completedCheckpointStore.getAllCheckpoints()) {\n\t\t\t\tcompletedCheckpoint.registerSharedStatesAfterRestored(sharedStateRegistry);\n\t\t\t}\n\n\t\t\tLOG.debug(\"Status of the shared state registry after restore: {}.\", sharedStateRegistry);\n\n\t\t\t// Restore from the latest checkpoint\n\t\t\tCompletedCheckpoint latest = completedCheckpointStore.getLatestCheckpoint();\n\n\t\t\tif (latest == null) {\n\t\t\t\tif (errorIfNoCheckpoint) {\n\t\t\t\t\tthrow new IllegalStateException(\"No completed checkpoint available\");\n\t\t\t\t} else {\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tLOG.info(\"Restoring from latest valid checkpoint: {}.\", latest);\n\n\t\t\t// re-assign the task states\n\t\t\tfinal Map<OperatorID, OperatorState> operatorStates = latest.getOperatorStates();\n\n\t\t\tStateAssignmentOperation stateAssignmentOperation =\n\t\t\t\t\tnew StateAssignmentOperation(latest.getCheckpointID(), tasks, operatorStates, allowNonRestoredState);\n\n\t\t\tstateAssignmentOperation.assignStates();\n\n\t\t\t// call master hooks for restore\n\n\t\t\tMasterHooks.restoreMasterHooks(\n\t\t\t\t\tmasterHooks,\n\t\t\t\t\tlatest.getMasterHookStates(),\n\t\t\t\t\tlatest.getCheckpointID(),\n\t\t\t\t\tallowNonRestoredState,\n\t\t\t\t\tLOG);\n\n\t\t\t// update metrics\n\n\t\t\tif (statsTracker != null) {\n\t\t\t\tlong restoreTimestamp = System.currentTimeMillis();\n\t\t\t\tRestoredCheckpointStats restored = new RestoredCheckpointStats(\n\t\t\t\t\tlatest.getCheckpointID(),\n\t\t\t\t\tlatest.getProperties(),\n\t\t\t\t\trestoreTimestamp,\n\t\t\t\t\tlatest.getExternalPointer());\n\n\t\t\t\tstatsTracker.reportRestoredCheckpoint(restored);\n\t\t\t}\n\n\t\t\treturn true;\n\t\t}\n\t}",
            " 969  \n 970  \n 971  \n 972  \n 973  \n 974  \n 975  \n 976  \n 977  \n 978  \n 979  \n 980  \n 981  \n 982  \n 983  \n 984  \n 985  \n 986  \n 987  \n 988  \n 989  \n 990  \n 991  \n 992  \n 993  \n 994  \n 995  \n 996  \n 997  \n 998  \n 999  \n1000  \n1001  \n1002  \n1003  \n1004  \n1005  \n1006  \n1007  \n1008  \n1009  \n1010  \n1011  \n1012  \n1013  \n1014  \n1015 +\n1016  \n1017  \n1018  \n1019  \n1020  \n1021  \n1022  \n1023  \n1024  \n1025  \n1026  \n1027  \n1028 +\n1029  \n1030  \n1031  \n1032  \n1033  \n1034  \n1035  \n1036  \n1037  \n1038  \n1039  \n1040  \n1041  \n1042  \n1043  \n1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050  \n1051  \n1052  \n1053  \n1054  \n1055  \n1056  \n1057  \n1058  \n1059  \n1060  \n1061  \n1062  ",
            "\t/**\n\t * Restores the latest checkpointed state.\n\t *\n\t * @param tasks Map of job vertices to restore. State for these vertices is\n\t * restored via {@link Execution#setInitialState(JobManagerTaskRestore)}.\n\t * @param errorIfNoCheckpoint Fail if no completed checkpoint is available to\n\t * restore from.\n\t * @param allowNonRestoredState Allow checkpoint state that cannot be mapped\n\t * to any job vertex in tasks.\n\t * @return <code>true</code> if state was restored, <code>false</code> otherwise.\n\t * @throws IllegalStateException If the CheckpointCoordinator is shut down.\n\t * @throws IllegalStateException If no completed checkpoint is available and\n\t *                               the <code>failIfNoCheckpoint</code> flag has been set.\n\t * @throws IllegalStateException If the checkpoint contains state that cannot be\n\t *                               mapped to any job vertex in <code>tasks</code> and the\n\t *                               <code>allowNonRestoredState</code> flag has not been set.\n\t * @throws IllegalStateException If the max parallelism changed for an operator\n\t *                               that restores state from this checkpoint.\n\t * @throws IllegalStateException If the parallelism changed for an operator\n\t *                               that restores <i>non-partitioned</i> state from this\n\t *                               checkpoint.\n\t */\n\tpublic boolean restoreLatestCheckpointedState(\n\t\t\tMap<JobVertexID, ExecutionJobVertex> tasks,\n\t\t\tboolean errorIfNoCheckpoint,\n\t\t\tboolean allowNonRestoredState) throws Exception {\n\n\t\tsynchronized (lock) {\n\t\t\tif (shutdown) {\n\t\t\t\tthrow new IllegalStateException(\"CheckpointCoordinator is shut down\");\n\t\t\t}\n\n\t\t\t// We create a new shared state registry object, so that all pending async disposal requests from previous\n\t\t\t// runs will go against the old object (were they can do no harm).\n\t\t\t// This must happen under the checkpoint lock.\n\t\t\tsharedStateRegistry.close();\n\t\t\tsharedStateRegistry = sharedStateRegistryFactory.create(executor);\n\n\t\t\t// Recover the checkpoints, TODO this could be done only when there is a new leader, not on each recovery\n\t\t\tcompletedCheckpointStore.recover();\n\n\t\t\t// Now, we re-register all (shared) states from the checkpoint store with the new registry\n\t\t\tfor (CompletedCheckpoint completedCheckpoint : completedCheckpointStore.getAllCheckpoints()) {\n\t\t\t\tcompletedCheckpoint.registerSharedStatesAfterRestored(sharedStateRegistry);\n\t\t\t}\n\n\t\t\tLOG.debug(\"Status of the shared state registry of job {} after restore: {}.\", job, sharedStateRegistry);\n\n\t\t\t// Restore from the latest checkpoint\n\t\t\tCompletedCheckpoint latest = completedCheckpointStore.getLatestCheckpoint();\n\n\t\t\tif (latest == null) {\n\t\t\t\tif (errorIfNoCheckpoint) {\n\t\t\t\t\tthrow new IllegalStateException(\"No completed checkpoint available\");\n\t\t\t\t} else {\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tLOG.info(\"Restoring job {} from latest valid checkpoint: {}.\", job, latest);\n\n\t\t\t// re-assign the task states\n\t\t\tfinal Map<OperatorID, OperatorState> operatorStates = latest.getOperatorStates();\n\n\t\t\tStateAssignmentOperation stateAssignmentOperation =\n\t\t\t\t\tnew StateAssignmentOperation(latest.getCheckpointID(), tasks, operatorStates, allowNonRestoredState);\n\n\t\t\tstateAssignmentOperation.assignStates();\n\n\t\t\t// call master hooks for restore\n\n\t\t\tMasterHooks.restoreMasterHooks(\n\t\t\t\t\tmasterHooks,\n\t\t\t\t\tlatest.getMasterHookStates(),\n\t\t\t\t\tlatest.getCheckpointID(),\n\t\t\t\t\tallowNonRestoredState,\n\t\t\t\t\tLOG);\n\n\t\t\t// update metrics\n\n\t\t\tif (statsTracker != null) {\n\t\t\t\tlong restoreTimestamp = System.currentTimeMillis();\n\t\t\t\tRestoredCheckpointStats restored = new RestoredCheckpointStats(\n\t\t\t\t\tlatest.getCheckpointID(),\n\t\t\t\t\tlatest.getProperties(),\n\t\t\t\t\trestoreTimestamp,\n\t\t\t\t\tlatest.getExternalPointer());\n\n\t\t\t\tstatsTracker.reportRestoredCheckpoint(restored);\n\t\t\t}\n\n\t\t\treturn true;\n\t\t}\n\t}"
        ],
        [
            "CheckpointCoordinator::shutdown(JobStatus)",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319 -\n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  ",
            "\t/**\n\t * Shuts down the checkpoint coordinator.\n\t *\n\t * <p>After this method has been called, the coordinator does not accept\n\t * and further messages and cannot trigger any further checkpoints.\n\t */\n\tpublic void shutdown(JobStatus jobStatus) throws Exception {\n\t\tsynchronized (lock) {\n\t\t\tif (!shutdown) {\n\t\t\t\tshutdown = true;\n\t\t\t\tLOG.info(\"Stopping checkpoint coordinator for job \" + job);\n\n\t\t\t\tperiodicScheduling = false;\n\t\t\t\ttriggerRequestQueued = false;\n\n\t\t\t\t// shut down the thread that handles the timeouts and pending triggers\n\t\t\t\ttimer.shutdownNow();\n\n\t\t\t\t// clear and discard all pending checkpoints\n\t\t\t\tfor (PendingCheckpoint pending : pendingCheckpoints.values()) {\n\t\t\t\t\tpending.abortError(new Exception(\"Checkpoint Coordinator is shutting down\"));\n\t\t\t\t}\n\t\t\t\tpendingCheckpoints.clear();\n\n\t\t\t\tcompletedCheckpointStore.shutdown(jobStatus);\n\t\t\t\tcheckpointIdCounter.shutdown(jobStatus);\n\t\t\t}\n\t\t}\n\t}",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319 +\n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  ",
            "\t/**\n\t * Shuts down the checkpoint coordinator.\n\t *\n\t * <p>After this method has been called, the coordinator does not accept\n\t * and further messages and cannot trigger any further checkpoints.\n\t */\n\tpublic void shutdown(JobStatus jobStatus) throws Exception {\n\t\tsynchronized (lock) {\n\t\t\tif (!shutdown) {\n\t\t\t\tshutdown = true;\n\t\t\t\tLOG.info(\"Stopping checkpoint coordinator for job {}.\", job);\n\n\t\t\t\tperiodicScheduling = false;\n\t\t\t\ttriggerRequestQueued = false;\n\n\t\t\t\t// shut down the thread that handles the timeouts and pending triggers\n\t\t\t\ttimer.shutdownNow();\n\n\t\t\t\t// clear and discard all pending checkpoints\n\t\t\t\tfor (PendingCheckpoint pending : pendingCheckpoints.values()) {\n\t\t\t\t\tpending.abortError(new Exception(\"Checkpoint Coordinator is shutting down\"));\n\t\t\t\t}\n\t\t\t\tpendingCheckpoints.clear();\n\n\t\t\t\tcompletedCheckpointStore.shutdown(jobStatus);\n\t\t\t\tcheckpointIdCounter.shutdown(jobStatus);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "CheckpointCoordinator::ScheduledTrigger::run()",
            "1211  \n1212  \n1213  \n1214  \n1215  \n1216  \n1217 -\n1218  \n1219  ",
            "\t\t@Override\n\t\tpublic void run() {\n\t\t\ttry {\n\t\t\t\ttriggerCheckpoint(System.currentTimeMillis(), true);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\tLOG.error(\"Exception while triggering checkpoint.\", e);\n\t\t\t}\n\t\t}",
            "1216  \n1217  \n1218  \n1219  \n1220  \n1221  \n1222 +\n1223  \n1224  ",
            "\t\t@Override\n\t\tpublic void run() {\n\t\t\ttry {\n\t\t\t\ttriggerCheckpoint(System.currentTimeMillis(), true);\n\t\t\t}\n\t\t\tcatch (Exception e) {\n\t\t\t\tLOG.error(\"Exception while triggering checkpoint for job {}.\", job, e);\n\t\t\t}\n\t\t}"
        ],
        [
            "CheckpointCoordinator::receiveDeclineMessage(DeclineCheckpoint)",
            " 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676 -\n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687 -\n 688 -\n 689  \n 690  \n 691 -\n 692 -\n 693  \n 694  \n 695  \n 696  ",
            "\t/**\n\t * Receives a {@link DeclineCheckpoint} message for a pending checkpoint.\n\t *\n\t * @param message Checkpoint decline from the task manager\n\t */\n\tpublic void receiveDeclineMessage(DeclineCheckpoint message) {\n\t\tif (shutdown || message == null) {\n\t\t\treturn;\n\t\t}\n\t\tif (!job.equals(message.getJob())) {\n\t\t\tthrow new IllegalArgumentException(\"Received DeclineCheckpoint message for job \" +\n\t\t\t\tmessage.getJob() + \" while this coordinator handles job \" + job);\n\t\t}\n\n\t\tfinal long checkpointId = message.getCheckpointId();\n\t\tfinal String reason = (message.getReason() != null ? message.getReason().getMessage() : \"\");\n\n\t\tPendingCheckpoint checkpoint;\n\n\t\tsynchronized (lock) {\n\t\t\t// we need to check inside the lock for being shutdown as well, otherwise we\n\t\t\t// get races and invalid error log messages\n\t\t\tif (shutdown) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tcheckpoint = pendingCheckpoints.remove(checkpointId);\n\n\t\t\tif (checkpoint != null && !checkpoint.isDiscarded()) {\n\t\t\t\tLOG.info(\"Decline checkpoint {} by task {}.\", checkpointId, message.getTaskExecutionId());\n\t\t\t\tdiscardCheckpoint(checkpoint, message.getReason());\n\t\t\t}\n\t\t\telse if (checkpoint != null) {\n\t\t\t\t// this should not happen\n\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\"Received message for discarded but non-removed checkpoint \" + checkpointId);\n\t\t\t}\n\t\t\telse if (LOG.isDebugEnabled()) {\n\t\t\t\tif (recentPendingCheckpoints.contains(checkpointId)) {\n\t\t\t\t\t// message is for an unknown checkpoint, or comes too late (checkpoint disposed)\n\t\t\t\t\tLOG.debug(\"Received another decline message for now expired checkpoint attempt {} : {}\",\n\t\t\t\t\t\t\tcheckpointId, reason);\n\t\t\t\t} else {\n\t\t\t\t\t// message is for an unknown checkpoint. might be so old that we don't even remember it any more\n\t\t\t\t\tLOG.debug(\"Received decline message for unknown (too old?) checkpoint attempt {} : {}\",\n\t\t\t\t\t\t\tcheckpointId, reason);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}",
            " 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681 +\n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692 +\n 693 +\n 694  \n 695  \n 696 +\n 697 +\n 698  \n 699  \n 700  \n 701  ",
            "\t/**\n\t * Receives a {@link DeclineCheckpoint} message for a pending checkpoint.\n\t *\n\t * @param message Checkpoint decline from the task manager\n\t */\n\tpublic void receiveDeclineMessage(DeclineCheckpoint message) {\n\t\tif (shutdown || message == null) {\n\t\t\treturn;\n\t\t}\n\t\tif (!job.equals(message.getJob())) {\n\t\t\tthrow new IllegalArgumentException(\"Received DeclineCheckpoint message for job \" +\n\t\t\t\tmessage.getJob() + \" while this coordinator handles job \" + job);\n\t\t}\n\n\t\tfinal long checkpointId = message.getCheckpointId();\n\t\tfinal String reason = (message.getReason() != null ? message.getReason().getMessage() : \"\");\n\n\t\tPendingCheckpoint checkpoint;\n\n\t\tsynchronized (lock) {\n\t\t\t// we need to check inside the lock for being shutdown as well, otherwise we\n\t\t\t// get races and invalid error log messages\n\t\t\tif (shutdown) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tcheckpoint = pendingCheckpoints.remove(checkpointId);\n\n\t\t\tif (checkpoint != null && !checkpoint.isDiscarded()) {\n\t\t\t\tLOG.info(\"Decline checkpoint {} by task {} of job {}.\", checkpointId, message.getTaskExecutionId(), job);\n\t\t\t\tdiscardCheckpoint(checkpoint, message.getReason());\n\t\t\t}\n\t\t\telse if (checkpoint != null) {\n\t\t\t\t// this should not happen\n\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\"Received message for discarded but non-removed checkpoint \" + checkpointId);\n\t\t\t}\n\t\t\telse if (LOG.isDebugEnabled()) {\n\t\t\t\tif (recentPendingCheckpoints.contains(checkpointId)) {\n\t\t\t\t\t// message is for an unknown checkpoint, or comes too late (checkpoint disposed)\n\t\t\t\t\tLOG.debug(\"Received another decline message for now expired checkpoint attempt {} of job {} : {}\",\n\t\t\t\t\t\t\tcheckpointId, job, reason);\n\t\t\t\t} else {\n\t\t\t\t\t// message is for an unknown checkpoint. might be so old that we don't even remember it any more\n\t\t\t\t\tLOG.debug(\"Received decline message for unknown (too old?) checkpoint attempt {} of job {} : {}\",\n\t\t\t\t\t\t\tcheckpointId, job, reason);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "CheckpointCoordinator::triggerCheckpoint(long,CheckpointProperties,String,boolean)",
            " 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417 -\n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458 -\n 459 -\n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473 -\n 474 -\n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501 -\n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529 -\n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550 -\n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582 -\n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623 -\n 624 -\n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  ",
            "\t@VisibleForTesting\n\tpublic CheckpointTriggerResult triggerCheckpoint(\n\t\t\tlong timestamp,\n\t\t\tCheckpointProperties props,\n\t\t\t@Nullable String externalSavepointLocation,\n\t\t\tboolean isPeriodic) {\n\n\t\t// make some eager pre-checks\n\t\tsynchronized (lock) {\n\t\t\t// abort if the coordinator has been shutdown in the meantime\n\t\t\tif (shutdown) {\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.COORDINATOR_SHUTDOWN);\n\t\t\t}\n\n\t\t\t// Don't allow periodic checkpoint if scheduling has been disabled\n\t\t\tif (isPeriodic && !periodicScheduling) {\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.PERIODIC_SCHEDULER_SHUTDOWN);\n\t\t\t}\n\n\t\t\t// validate whether the checkpoint can be triggered, with respect to the limit of\n\t\t\t// concurrent checkpoints, and the minimum time between checkpoints.\n\t\t\t// these checks are not relevant for savepoints\n\t\t\tif (!props.forceCheckpoint()) {\n\t\t\t\t// sanity check: there should never be more than one trigger request queued\n\t\t\t\tif (triggerRequestQueued) {\n\t\t\t\t\tLOG.warn(\"Trying to trigger another checkpoint while one was queued already\");\n\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.ALREADY_QUEUED);\n\t\t\t\t}\n\n\t\t\t\t// if too many checkpoints are currently in progress, we need to mark that a request is queued\n\t\t\t\tif (pendingCheckpoints.size() >= maxConcurrentCheckpointAttempts) {\n\t\t\t\t\ttriggerRequestQueued = true;\n\t\t\t\t\tif (currentPeriodicTrigger != null) {\n\t\t\t\t\t\tcurrentPeriodicTrigger.cancel(false);\n\t\t\t\t\t\tcurrentPeriodicTrigger = null;\n\t\t\t\t\t}\n\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.TOO_MANY_CONCURRENT_CHECKPOINTS);\n\t\t\t\t}\n\n\t\t\t\t// make sure the minimum interval between checkpoints has passed\n\t\t\t\tfinal long earliestNext = lastCheckpointCompletionNanos + minPauseBetweenCheckpointsNanos;\n\t\t\t\tfinal long durationTillNextMillis = (earliestNext - System.nanoTime()) / 1_000_000;\n\n\t\t\t\tif (durationTillNextMillis > 0) {\n\t\t\t\t\tif (currentPeriodicTrigger != null) {\n\t\t\t\t\t\tcurrentPeriodicTrigger.cancel(false);\n\t\t\t\t\t\tcurrentPeriodicTrigger = null;\n\t\t\t\t\t}\n\t\t\t\t\t// Reassign the new trigger to the currentPeriodicTrigger\n\t\t\t\t\tcurrentPeriodicTrigger = timer.scheduleAtFixedRate(\n\t\t\t\t\t\t\tnew ScheduledTrigger(),\n\t\t\t\t\t\t\tdurationTillNextMillis, baseInterval, TimeUnit.MILLISECONDS);\n\n\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.MINIMUM_TIME_BETWEEN_CHECKPOINTS);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// check if all tasks that we need to trigger are running.\n\t\t// if not, abort the checkpoint\n\t\tExecution[] executions = new Execution[tasksToTrigger.length];\n\t\tfor (int i = 0; i < tasksToTrigger.length; i++) {\n\t\t\tExecution ee = tasksToTrigger[i].getCurrentExecutionAttempt();\n\t\t\tif (ee != null && ee.getState() == ExecutionState.RUNNING) {\n\t\t\t\texecutions[i] = ee;\n\t\t\t} else {\n\t\t\t\tLOG.info(\"Checkpoint triggering task {} is not being executed at the moment. Aborting checkpoint.\",\n\t\t\t\t\t\ttasksToTrigger[i].getTaskNameWithSubtaskIndex());\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.NOT_ALL_REQUIRED_TASKS_RUNNING);\n\t\t\t}\n\t\t}\n\n\t\t// next, check if all tasks that need to acknowledge the checkpoint are running.\n\t\t// if not, abort the checkpoint\n\t\tMap<ExecutionAttemptID, ExecutionVertex> ackTasks = new HashMap<>(tasksToWaitFor.length);\n\n\t\tfor (ExecutionVertex ev : tasksToWaitFor) {\n\t\t\tExecution ee = ev.getCurrentExecutionAttempt();\n\t\t\tif (ee != null) {\n\t\t\t\tackTasks.put(ee.getAttemptId(), ev);\n\t\t\t} else {\n\t\t\t\tLOG.info(\"Checkpoint acknowledging task {} is not being executed at the moment. Aborting checkpoint.\",\n\t\t\t\t\t\tev.getTaskNameWithSubtaskIndex());\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.NOT_ALL_REQUIRED_TASKS_RUNNING);\n\t\t\t}\n\t\t}\n\n\t\t// we will actually trigger this checkpoint!\n\n\t\t// we lock with a special lock to make sure that trigger requests do not overtake each other.\n\t\t// this is not done with the coordinator-wide lock, because the 'checkpointIdCounter'\n\t\t// may issue blocking operations. Using a different lock than the coordinator-wide lock,\n\t\t// we avoid blocking the processing of 'acknowledge/decline' messages during that time.\n\t\tsynchronized (triggerLock) {\n\n\t\t\tfinal CheckpointStorageLocation checkpointStorageLocation;\n\t\t\tfinal long checkpointID;\n\n\t\t\ttry {\n\t\t\t\t// this must happen outside the coordinator-wide lock, because it communicates\n\t\t\t\t// with external services (in HA mode) and may block for a while.\n\t\t\t\tcheckpointID = checkpointIdCounter.getAndIncrement();\n\n\t\t\t\tcheckpointStorageLocation = props.isSavepoint() ?\n\t\t\t\t\t\tcheckpointStorage.initializeLocationForSavepoint(checkpointID, externalSavepointLocation) :\n\t\t\t\t\t\tcheckpointStorage.initializeLocationForCheckpoint(checkpointID);\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tint numUnsuccessful = numUnsuccessfulCheckpointsTriggers.incrementAndGet();\n\t\t\t\tLOG.warn(\"Failed to trigger checkpoint (\" + numUnsuccessful + \" consecutive failed attempts so far)\", t);\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.EXCEPTION);\n\t\t\t}\n\n\t\t\tfinal PendingCheckpoint checkpoint = new PendingCheckpoint(\n\t\t\t\tjob,\n\t\t\t\tcheckpointID,\n\t\t\t\ttimestamp,\n\t\t\t\tackTasks,\n\t\t\t\tprops,\n\t\t\t\tcheckpointStorageLocation,\n\t\t\t\texecutor);\n\n\t\t\tif (statsTracker != null) {\n\t\t\t\tPendingCheckpointStats callback = statsTracker.reportPendingCheckpoint(\n\t\t\t\t\tcheckpointID,\n\t\t\t\t\ttimestamp,\n\t\t\t\t\tprops);\n\n\t\t\t\tcheckpoint.setStatsCallback(callback);\n\t\t\t}\n\n\t\t\t// schedule the timer that will clean up the expired checkpoints\n\t\t\tfinal Runnable canceller = () -> {\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\t// only do the work if the checkpoint is not discarded anyways\n\t\t\t\t\t// note that checkpoint completion discards the pending checkpoint object\n\t\t\t\t\tif (!checkpoint.isDiscarded()) {\n\t\t\t\t\t\tLOG.info(\"Checkpoint \" + checkpointID + \" expired before completing.\");\n\n\t\t\t\t\t\tcheckpoint.abortExpired();\n\t\t\t\t\t\tpendingCheckpoints.remove(checkpointID);\n\t\t\t\t\t\trememberRecentCheckpointId(checkpointID);\n\n\t\t\t\t\t\ttriggerQueuedRequests();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\n\t\t\ttry {\n\t\t\t\t// re-acquire the coordinator-wide lock\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\t// since we released the lock in the meantime, we need to re-check\n\t\t\t\t\t// that the conditions still hold.\n\t\t\t\t\tif (shutdown) {\n\t\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.COORDINATOR_SHUTDOWN);\n\t\t\t\t\t}\n\t\t\t\t\telse if (!props.forceCheckpoint()) {\n\t\t\t\t\t\tif (triggerRequestQueued) {\n\t\t\t\t\t\t\tLOG.warn(\"Trying to trigger another checkpoint while one was queued already\");\n\t\t\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.ALREADY_QUEUED);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (pendingCheckpoints.size() >= maxConcurrentCheckpointAttempts) {\n\t\t\t\t\t\t\ttriggerRequestQueued = true;\n\t\t\t\t\t\t\tif (currentPeriodicTrigger != null) {\n\t\t\t\t\t\t\t\tcurrentPeriodicTrigger.cancel(false);\n\t\t\t\t\t\t\t\tcurrentPeriodicTrigger = null;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.TOO_MANY_CONCURRENT_CHECKPOINTS);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// make sure the minimum interval between checkpoints has passed\n\t\t\t\t\t\tfinal long earliestNext = lastCheckpointCompletionNanos + minPauseBetweenCheckpointsNanos;\n\t\t\t\t\t\tfinal long durationTillNextMillis = (earliestNext - System.nanoTime()) / 1_000_000;\n\n\t\t\t\t\t\tif (durationTillNextMillis > 0) {\n\t\t\t\t\t\t\tif (currentPeriodicTrigger != null) {\n\t\t\t\t\t\t\t\tcurrentPeriodicTrigger.cancel(false);\n\t\t\t\t\t\t\t\tcurrentPeriodicTrigger = null;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// Reassign the new trigger to the currentPeriodicTrigger\n\t\t\t\t\t\t\tcurrentPeriodicTrigger = timer.scheduleAtFixedRate(\n\t\t\t\t\t\t\t\t\tnew ScheduledTrigger(),\n\t\t\t\t\t\t\t\t\tdurationTillNextMillis, baseInterval, TimeUnit.MILLISECONDS);\n\n\t\t\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.MINIMUM_TIME_BETWEEN_CHECKPOINTS);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tLOG.info(\"Triggering checkpoint \" + checkpointID + \" @ \" + timestamp);\n\n\t\t\t\t\tpendingCheckpoints.put(checkpointID, checkpoint);\n\n\t\t\t\t\tScheduledFuture<?> cancellerHandle = timer.schedule(\n\t\t\t\t\t\t\tcanceller,\n\t\t\t\t\t\t\tcheckpointTimeout, TimeUnit.MILLISECONDS);\n\n\t\t\t\t\tif (!checkpoint.setCancellerHandle(cancellerHandle)) {\n\t\t\t\t\t\t// checkpoint is already disposed!\n\t\t\t\t\t\tcancellerHandle.cancel(false);\n\t\t\t\t\t}\n\n\t\t\t\t\t// trigger the master hooks for the checkpoint\n\t\t\t\t\tfinal List<MasterState> masterStates = MasterHooks.triggerMasterHooks(masterHooks.values(),\n\t\t\t\t\t\t\tcheckpointID, timestamp, executor, Time.milliseconds(checkpointTimeout));\n\t\t\t\t\tfor (MasterState s : masterStates) {\n\t\t\t\t\t\tcheckpoint.addMasterState(s);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// end of lock scope\n\n\t\t\t\tfinal CheckpointOptions checkpointOptions = new CheckpointOptions(\n\t\t\t\t\t\tprops.getCheckpointType(),\n\t\t\t\t\t\tcheckpointStorageLocation.getLocationReference());\n\n\t\t\t\t// send the messages to the tasks that trigger their checkpoint\n\t\t\t\tfor (Execution execution: executions) {\n\t\t\t\t\texecution.triggerCheckpoint(checkpointID, timestamp, checkpointOptions);\n\t\t\t\t}\n\n\t\t\t\tnumUnsuccessfulCheckpointsTriggers.set(0);\n\t\t\t\treturn new CheckpointTriggerResult(checkpoint);\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// guard the map against concurrent modifications\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\tpendingCheckpoints.remove(checkpointID);\n\t\t\t\t}\n\n\t\t\t\tint numUnsuccessful = numUnsuccessfulCheckpointsTriggers.incrementAndGet();\n\t\t\t\tLOG.warn(\"Failed to trigger checkpoint {}. ({} consecutive failed attempts so far)\",\n\t\t\t\t\t\tcheckpointID, numUnsuccessful, t);\n\n\t\t\t\tif (!checkpoint.isDiscarded()) {\n\t\t\t\t\tcheckpoint.abortError(new Exception(\"Failed to trigger checkpoint\", t));\n\t\t\t\t}\n\n\t\t\t\ttry {\n\t\t\t\t\tcheckpointStorageLocation.disposeOnFailure();\n\t\t\t\t}\n\t\t\t\tcatch (Throwable t2) {\n\t\t\t\t\tLOG.warn(\"Cannot dispose failed checkpoint storage location {}\", checkpointStorageLocation, t2);\n\t\t\t\t}\n\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.EXCEPTION);\n\t\t\t}\n\n\t\t} // end trigger lock\n\t}",
            " 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417 +\n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458 +\n 459 +\n 460 +\n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474 +\n 475 +\n 476 +\n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503 +\n 504 +\n 505 +\n 506 +\n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534 +\n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555 +\n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587 +\n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628 +\n 629 +\n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  ",
            "\t@VisibleForTesting\n\tpublic CheckpointTriggerResult triggerCheckpoint(\n\t\t\tlong timestamp,\n\t\t\tCheckpointProperties props,\n\t\t\t@Nullable String externalSavepointLocation,\n\t\t\tboolean isPeriodic) {\n\n\t\t// make some eager pre-checks\n\t\tsynchronized (lock) {\n\t\t\t// abort if the coordinator has been shutdown in the meantime\n\t\t\tif (shutdown) {\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.COORDINATOR_SHUTDOWN);\n\t\t\t}\n\n\t\t\t// Don't allow periodic checkpoint if scheduling has been disabled\n\t\t\tif (isPeriodic && !periodicScheduling) {\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.PERIODIC_SCHEDULER_SHUTDOWN);\n\t\t\t}\n\n\t\t\t// validate whether the checkpoint can be triggered, with respect to the limit of\n\t\t\t// concurrent checkpoints, and the minimum time between checkpoints.\n\t\t\t// these checks are not relevant for savepoints\n\t\t\tif (!props.forceCheckpoint()) {\n\t\t\t\t// sanity check: there should never be more than one trigger request queued\n\t\t\t\tif (triggerRequestQueued) {\n\t\t\t\t\tLOG.warn(\"Trying to trigger another checkpoint for job {} while one was queued already.\", job);\n\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.ALREADY_QUEUED);\n\t\t\t\t}\n\n\t\t\t\t// if too many checkpoints are currently in progress, we need to mark that a request is queued\n\t\t\t\tif (pendingCheckpoints.size() >= maxConcurrentCheckpointAttempts) {\n\t\t\t\t\ttriggerRequestQueued = true;\n\t\t\t\t\tif (currentPeriodicTrigger != null) {\n\t\t\t\t\t\tcurrentPeriodicTrigger.cancel(false);\n\t\t\t\t\t\tcurrentPeriodicTrigger = null;\n\t\t\t\t\t}\n\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.TOO_MANY_CONCURRENT_CHECKPOINTS);\n\t\t\t\t}\n\n\t\t\t\t// make sure the minimum interval between checkpoints has passed\n\t\t\t\tfinal long earliestNext = lastCheckpointCompletionNanos + minPauseBetweenCheckpointsNanos;\n\t\t\t\tfinal long durationTillNextMillis = (earliestNext - System.nanoTime()) / 1_000_000;\n\n\t\t\t\tif (durationTillNextMillis > 0) {\n\t\t\t\t\tif (currentPeriodicTrigger != null) {\n\t\t\t\t\t\tcurrentPeriodicTrigger.cancel(false);\n\t\t\t\t\t\tcurrentPeriodicTrigger = null;\n\t\t\t\t\t}\n\t\t\t\t\t// Reassign the new trigger to the currentPeriodicTrigger\n\t\t\t\t\tcurrentPeriodicTrigger = timer.scheduleAtFixedRate(\n\t\t\t\t\t\t\tnew ScheduledTrigger(),\n\t\t\t\t\t\t\tdurationTillNextMillis, baseInterval, TimeUnit.MILLISECONDS);\n\n\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.MINIMUM_TIME_BETWEEN_CHECKPOINTS);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// check if all tasks that we need to trigger are running.\n\t\t// if not, abort the checkpoint\n\t\tExecution[] executions = new Execution[tasksToTrigger.length];\n\t\tfor (int i = 0; i < tasksToTrigger.length; i++) {\n\t\t\tExecution ee = tasksToTrigger[i].getCurrentExecutionAttempt();\n\t\t\tif (ee != null && ee.getState() == ExecutionState.RUNNING) {\n\t\t\t\texecutions[i] = ee;\n\t\t\t} else {\n\t\t\t\tLOG.info(\"Checkpoint triggering task {} of job {} is not being executed at the moment. Aborting checkpoint.\",\n\t\t\t\t\t\ttasksToTrigger[i].getTaskNameWithSubtaskIndex(),\n\t\t\t\t\t\tjob);\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.NOT_ALL_REQUIRED_TASKS_RUNNING);\n\t\t\t}\n\t\t}\n\n\t\t// next, check if all tasks that need to acknowledge the checkpoint are running.\n\t\t// if not, abort the checkpoint\n\t\tMap<ExecutionAttemptID, ExecutionVertex> ackTasks = new HashMap<>(tasksToWaitFor.length);\n\n\t\tfor (ExecutionVertex ev : tasksToWaitFor) {\n\t\t\tExecution ee = ev.getCurrentExecutionAttempt();\n\t\t\tif (ee != null) {\n\t\t\t\tackTasks.put(ee.getAttemptId(), ev);\n\t\t\t} else {\n\t\t\t\tLOG.info(\"Checkpoint acknowledging task {} of job {} is not being executed at the moment. Aborting checkpoint.\",\n\t\t\t\t\t\tev.getTaskNameWithSubtaskIndex(),\n\t\t\t\t\t\tjob);\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.NOT_ALL_REQUIRED_TASKS_RUNNING);\n\t\t\t}\n\t\t}\n\n\t\t// we will actually trigger this checkpoint!\n\n\t\t// we lock with a special lock to make sure that trigger requests do not overtake each other.\n\t\t// this is not done with the coordinator-wide lock, because the 'checkpointIdCounter'\n\t\t// may issue blocking operations. Using a different lock than the coordinator-wide lock,\n\t\t// we avoid blocking the processing of 'acknowledge/decline' messages during that time.\n\t\tsynchronized (triggerLock) {\n\n\t\t\tfinal CheckpointStorageLocation checkpointStorageLocation;\n\t\t\tfinal long checkpointID;\n\n\t\t\ttry {\n\t\t\t\t// this must happen outside the coordinator-wide lock, because it communicates\n\t\t\t\t// with external services (in HA mode) and may block for a while.\n\t\t\t\tcheckpointID = checkpointIdCounter.getAndIncrement();\n\n\t\t\t\tcheckpointStorageLocation = props.isSavepoint() ?\n\t\t\t\t\t\tcheckpointStorage.initializeLocationForSavepoint(checkpointID, externalSavepointLocation) :\n\t\t\t\t\t\tcheckpointStorage.initializeLocationForCheckpoint(checkpointID);\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tint numUnsuccessful = numUnsuccessfulCheckpointsTriggers.incrementAndGet();\n\t\t\t\tLOG.warn(\"Failed to trigger checkpoint for job {} ({} consecutive failed attempts so far).\",\n\t\t\t\t\t\tjob,\n\t\t\t\t\t\tnumUnsuccessful,\n\t\t\t\t\t\tt);\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.EXCEPTION);\n\t\t\t}\n\n\t\t\tfinal PendingCheckpoint checkpoint = new PendingCheckpoint(\n\t\t\t\tjob,\n\t\t\t\tcheckpointID,\n\t\t\t\ttimestamp,\n\t\t\t\tackTasks,\n\t\t\t\tprops,\n\t\t\t\tcheckpointStorageLocation,\n\t\t\t\texecutor);\n\n\t\t\tif (statsTracker != null) {\n\t\t\t\tPendingCheckpointStats callback = statsTracker.reportPendingCheckpoint(\n\t\t\t\t\tcheckpointID,\n\t\t\t\t\ttimestamp,\n\t\t\t\t\tprops);\n\n\t\t\t\tcheckpoint.setStatsCallback(callback);\n\t\t\t}\n\n\t\t\t// schedule the timer that will clean up the expired checkpoints\n\t\t\tfinal Runnable canceller = () -> {\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\t// only do the work if the checkpoint is not discarded anyways\n\t\t\t\t\t// note that checkpoint completion discards the pending checkpoint object\n\t\t\t\t\tif (!checkpoint.isDiscarded()) {\n\t\t\t\t\t\tLOG.info(\"Checkpoint {} of job {} expired before completing.\", checkpointID, job);\n\n\t\t\t\t\t\tcheckpoint.abortExpired();\n\t\t\t\t\t\tpendingCheckpoints.remove(checkpointID);\n\t\t\t\t\t\trememberRecentCheckpointId(checkpointID);\n\n\t\t\t\t\t\ttriggerQueuedRequests();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\n\t\t\ttry {\n\t\t\t\t// re-acquire the coordinator-wide lock\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\t// since we released the lock in the meantime, we need to re-check\n\t\t\t\t\t// that the conditions still hold.\n\t\t\t\t\tif (shutdown) {\n\t\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.COORDINATOR_SHUTDOWN);\n\t\t\t\t\t}\n\t\t\t\t\telse if (!props.forceCheckpoint()) {\n\t\t\t\t\t\tif (triggerRequestQueued) {\n\t\t\t\t\t\t\tLOG.warn(\"Trying to trigger another checkpoint for job {} while one was queued already.\", job);\n\t\t\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.ALREADY_QUEUED);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (pendingCheckpoints.size() >= maxConcurrentCheckpointAttempts) {\n\t\t\t\t\t\t\ttriggerRequestQueued = true;\n\t\t\t\t\t\t\tif (currentPeriodicTrigger != null) {\n\t\t\t\t\t\t\t\tcurrentPeriodicTrigger.cancel(false);\n\t\t\t\t\t\t\t\tcurrentPeriodicTrigger = null;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.TOO_MANY_CONCURRENT_CHECKPOINTS);\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\t// make sure the minimum interval between checkpoints has passed\n\t\t\t\t\t\tfinal long earliestNext = lastCheckpointCompletionNanos + minPauseBetweenCheckpointsNanos;\n\t\t\t\t\t\tfinal long durationTillNextMillis = (earliestNext - System.nanoTime()) / 1_000_000;\n\n\t\t\t\t\t\tif (durationTillNextMillis > 0) {\n\t\t\t\t\t\t\tif (currentPeriodicTrigger != null) {\n\t\t\t\t\t\t\t\tcurrentPeriodicTrigger.cancel(false);\n\t\t\t\t\t\t\t\tcurrentPeriodicTrigger = null;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// Reassign the new trigger to the currentPeriodicTrigger\n\t\t\t\t\t\t\tcurrentPeriodicTrigger = timer.scheduleAtFixedRate(\n\t\t\t\t\t\t\t\t\tnew ScheduledTrigger(),\n\t\t\t\t\t\t\t\t\tdurationTillNextMillis, baseInterval, TimeUnit.MILLISECONDS);\n\n\t\t\t\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.MINIMUM_TIME_BETWEEN_CHECKPOINTS);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tLOG.info(\"Triggering checkpoint {} @ {} for job {}.\", checkpointID, timestamp, job);\n\n\t\t\t\t\tpendingCheckpoints.put(checkpointID, checkpoint);\n\n\t\t\t\t\tScheduledFuture<?> cancellerHandle = timer.schedule(\n\t\t\t\t\t\t\tcanceller,\n\t\t\t\t\t\t\tcheckpointTimeout, TimeUnit.MILLISECONDS);\n\n\t\t\t\t\tif (!checkpoint.setCancellerHandle(cancellerHandle)) {\n\t\t\t\t\t\t// checkpoint is already disposed!\n\t\t\t\t\t\tcancellerHandle.cancel(false);\n\t\t\t\t\t}\n\n\t\t\t\t\t// trigger the master hooks for the checkpoint\n\t\t\t\t\tfinal List<MasterState> masterStates = MasterHooks.triggerMasterHooks(masterHooks.values(),\n\t\t\t\t\t\t\tcheckpointID, timestamp, executor, Time.milliseconds(checkpointTimeout));\n\t\t\t\t\tfor (MasterState s : masterStates) {\n\t\t\t\t\t\tcheckpoint.addMasterState(s);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\t// end of lock scope\n\n\t\t\t\tfinal CheckpointOptions checkpointOptions = new CheckpointOptions(\n\t\t\t\t\t\tprops.getCheckpointType(),\n\t\t\t\t\t\tcheckpointStorageLocation.getLocationReference());\n\n\t\t\t\t// send the messages to the tasks that trigger their checkpoint\n\t\t\t\tfor (Execution execution: executions) {\n\t\t\t\t\texecution.triggerCheckpoint(checkpointID, timestamp, checkpointOptions);\n\t\t\t\t}\n\n\t\t\t\tnumUnsuccessfulCheckpointsTriggers.set(0);\n\t\t\t\treturn new CheckpointTriggerResult(checkpoint);\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// guard the map against concurrent modifications\n\t\t\t\tsynchronized (lock) {\n\t\t\t\t\tpendingCheckpoints.remove(checkpointID);\n\t\t\t\t}\n\n\t\t\t\tint numUnsuccessful = numUnsuccessfulCheckpointsTriggers.incrementAndGet();\n\t\t\t\tLOG.warn(\"Failed to trigger checkpoint {} for job {}. ({} consecutive failed attempts so far)\",\n\t\t\t\t\t\tcheckpointID, job, numUnsuccessful, t);\n\n\t\t\t\tif (!checkpoint.isDiscarded()) {\n\t\t\t\t\tcheckpoint.abortError(new Exception(\"Failed to trigger checkpoint\", t));\n\t\t\t\t}\n\n\t\t\t\ttry {\n\t\t\t\t\tcheckpointStorageLocation.disposeOnFailure();\n\t\t\t\t}\n\t\t\t\tcatch (Throwable t2) {\n\t\t\t\t\tLOG.warn(\"Cannot dispose failed checkpoint storage location {}\", checkpointStorageLocation, t2);\n\t\t\t\t}\n\n\t\t\t\treturn new CheckpointTriggerResult(CheckpointDeclineReason.EXCEPTION);\n\t\t\t}\n\n\t\t} // end trigger lock\n\t}"
        ]
    ],
    "99c07d967a7a27ce06ded601cbc4c36d1b6702e2": [
        [
            "ConfigOptionsDocGenerator::createTable(String,String,String,String)",
            "  70  \n  71  \n  72  \n  73 -\n  74 -\n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tprivate static void createTable(String rootDir, String module, String packageName, String outputDirectory) throws IOException, ClassNotFoundException {\n\t\tPath configDir = Paths.get(rootDir, module, \"src/main/java\", packageName.replaceAll(\"\\\\.\", \"/\"));\n\n\t\tPattern p = Pattern.compile(\"(([a-zA-Z]*)(Options))\\\\.java\");\n\t\ttry (DirectoryStream<Path> stream = Files.newDirectoryStream(configDir, \"*Options.java\")) {\n\t\t\tfor (Path entry : stream) {\n\t\t\t\tString fileName = entry.getFileName().toString();\n\t\t\t\tMatcher matcher = p.matcher(fileName);\n\t\t\t\tif (!fileName.equals(\"ConfigOptions.java\") && matcher.matches()) {\n\t\t\t\t\tClass<?> optionsClass = Class.forName(packageName + \".\" + matcher.group(1));\n\t\t\t\t\tList<Tuple2<ConfigGroup, String>> tables = generateTablesForClass(optionsClass);\n\t\t\t\t\tif (tables.size() > 0) {\n\t\t\t\t\t\tfor (Tuple2<ConfigGroup, String> group : tables) {\n\n\t\t\t\t\t\t\tString name = group.f0 == null\n\t\t\t\t\t\t\t\t? matcher.group(2).replaceAll(\"(.)(\\\\p{Upper})\", \"$1_$2\").toLowerCase()\n\t\t\t\t\t\t\t\t: group.f0.name().replaceAll(\"(.)(\\\\p{Upper})\", \"$1_$2\").toLowerCase();\n\n\t\t\t\t\t\t\tString outputFile = name + \"_configuration.html\";\n\t\t\t\t\t\t\tFiles.write(Paths.get(outputDirectory, outputFile), group.f1.getBytes(StandardCharsets.UTF_8));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}",
            "  70  \n  71  \n  72  \n  73 +\n  74 +\n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tprivate static void createTable(String rootDir, String module, String packageName, String outputDirectory) throws IOException, ClassNotFoundException {\n\t\tPath configDir = Paths.get(rootDir, module, \"src/main/java\", packageName.replaceAll(\"\\\\.\", \"/\"));\n\n\t\tPattern p = Pattern.compile(\"(([a-zA-Z]*)(Options|Parameters))\\\\.java\");\n\t\ttry (DirectoryStream<Path> stream = Files.newDirectoryStream(configDir)) {\n\t\t\tfor (Path entry : stream) {\n\t\t\t\tString fileName = entry.getFileName().toString();\n\t\t\t\tMatcher matcher = p.matcher(fileName);\n\t\t\t\tif (!fileName.equals(\"ConfigOptions.java\") && matcher.matches()) {\n\t\t\t\t\tClass<?> optionsClass = Class.forName(packageName + \".\" + matcher.group(1));\n\t\t\t\t\tList<Tuple2<ConfigGroup, String>> tables = generateTablesForClass(optionsClass);\n\t\t\t\t\tif (tables.size() > 0) {\n\t\t\t\t\t\tfor (Tuple2<ConfigGroup, String> group : tables) {\n\n\t\t\t\t\t\t\tString name = group.f0 == null\n\t\t\t\t\t\t\t\t? matcher.group(2).replaceAll(\"(.)(\\\\p{Upper})\", \"$1_$2\").toLowerCase()\n\t\t\t\t\t\t\t\t: group.f0.name().replaceAll(\"(.)(\\\\p{Upper})\", \"$1_$2\").toLowerCase();\n\n\t\t\t\t\t\t\tString outputFile = name + \"_configuration.html\";\n\t\t\t\t\t\t\tFiles.write(Paths.get(outputDirectory, outputFile), group.f1.getBytes(StandardCharsets.UTF_8));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}"
        ]
    ],
    "0ae7364bdd2a0ad1db1ec02dc6b1b730187f2b78": [
        [
            "FlinkKafkaConsumerBase::run(SourceContext)",
            " 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604 -\n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  ",
            "\t@Override\n\tpublic void run(SourceContext<T> sourceContext) throws Exception {\n\t\tif (subscribedPartitionsToStartOffsets == null) {\n\t\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t\t}\n\n\t\t// initialize commit metrics and default offset callback method\n\t\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\t\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\t\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t\t@Override\n\t\t\tpublic void onSuccess() {\n\t\t\t\tsuccessfulCommits.inc();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void onException(Throwable cause) {\n\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\t\tfailedCommits.inc();\n\t\t\t}\n\t\t};\n\n\t\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t\t// status will automatically be triggered back to be active.\n\t\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\t\tsourceContext.markAsTemporarilyIdle();\n\t\t}\n\n\t\t// from this point forward:\n\t\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t\t//     Kafka through the fetcher, if configured to do so)\n\t\tthis.kafkaFetcher = createFetcher(\n\t\t\t\tsourceContext,\n\t\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\t\tperiodicWatermarkAssigner,\n\t\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\t\toffsetCommitMode,\n\t\t\t\tgetRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n\t\t\t\tuseMetrics);\n\n\t\tif (!running) {\n\t\t\treturn;\n\t\t}\n\n\t\t// depending on whether we were restored with the current state version (1.3),\n\t\t// remaining logic branches off into 2 paths:\n\t\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t\t//                 thread running the main fetcher loop\n\t\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\t\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\t\twhile (running) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\");\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t\t} finally {\n\t\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\t\tcancel();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tdiscoveryLoopThread.start();\n\t\t\tkafkaFetcher.runFetchLoop();\n\n\t\t\t// --------------------------------------------------------------------\n\n\t\t\t// make sure that the partition discoverer is properly closed\n\t\t\tpartitionDiscoverer.close();\n\t\t\tdiscoveryLoopThread.join();\n\n\t\t\t// rethrow any fetcher errors\n\t\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\t\tif (discoveryLoopError != null) {\n\t\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t\t}\n\t\t} else {\n\t\t\t// won't be using the discoverer\n\t\t\tpartitionDiscoverer.close();\n\n\t\t\tkafkaFetcher.runFetchLoop();\n\t\t}\n\t}",
            " 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604 +\n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  ",
            "\t@Override\n\tpublic void run(SourceContext<T> sourceContext) throws Exception {\n\t\tif (subscribedPartitionsToStartOffsets == null) {\n\t\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t\t}\n\n\t\t// initialize commit metrics and default offset callback method\n\t\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\t\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\t\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t\t@Override\n\t\t\tpublic void onSuccess() {\n\t\t\t\tsuccessfulCommits.inc();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void onException(Throwable cause) {\n\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\t\tfailedCommits.inc();\n\t\t\t}\n\t\t};\n\n\t\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t\t// status will automatically be triggered back to be active.\n\t\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\t\tsourceContext.markAsTemporarilyIdle();\n\t\t}\n\n\t\t// from this point forward:\n\t\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t\t//     Kafka through the fetcher, if configured to do so)\n\t\tthis.kafkaFetcher = createFetcher(\n\t\t\t\tsourceContext,\n\t\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\t\tperiodicWatermarkAssigner,\n\t\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\t\toffsetCommitMode,\n\t\t\t\tgetRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n\t\t\t\tuseMetrics);\n\n\t\tif (!running) {\n\t\t\treturn;\n\t\t}\n\n\t\t// depending on whether we were restored with the current state version (1.3),\n\t\t// remaining logic branches off into 2 paths:\n\t\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t\t//                 thread running the main fetcher loop\n\t\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\t\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\t\twhile (running) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t\t} finally {\n\t\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\t\tcancel();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tdiscoveryLoopThread.start();\n\t\t\tkafkaFetcher.runFetchLoop();\n\n\t\t\t// --------------------------------------------------------------------\n\n\t\t\t// make sure that the partition discoverer is properly closed\n\t\t\tpartitionDiscoverer.close();\n\t\t\tdiscoveryLoopThread.join();\n\n\t\t\t// rethrow any fetcher errors\n\t\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\t\tif (discoveryLoopError != null) {\n\t\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t\t}\n\t\t} else {\n\t\t\t// won't be using the discoverer\n\t\t\tpartitionDiscoverer.close();\n\n\t\t\tkafkaFetcher.runFetchLoop();\n\t\t}\n\t}"
        ]
    ],
    "fa63c3356b3fa89e549c058cbb6c6ecc19e61d8a": [
        [
            "FlinkKafkaConsumerBase::run(SourceContext)",
            " 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718 -\n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  ",
            "\t@Override\n\tpublic void run(SourceContext<T> sourceContext) throws Exception {\n\t\tif (subscribedPartitionsToStartOffsets == null) {\n\t\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t\t}\n\n\t\t// initialize commit metrics and default offset callback method\n\t\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\t\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\t\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t\t@Override\n\t\t\tpublic void onSuccess() {\n\t\t\t\tsuccessfulCommits.inc();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void onException(Throwable cause) {\n\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\t\tfailedCommits.inc();\n\t\t\t}\n\t\t};\n\n\t\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t\t// status will automatically be triggered back to be active.\n\t\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\t\tsourceContext.markAsTemporarilyIdle();\n\t\t}\n\n\t\t// from this point forward:\n\t\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t\t//     Kafka through the fetcher, if configured to do so)\n\t\tthis.kafkaFetcher = createFetcher(\n\t\t\t\tsourceContext,\n\t\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\t\tperiodicWatermarkAssigner,\n\t\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\t\toffsetCommitMode,\n\t\t\t\tgetRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n\t\t\t\tuseMetrics);\n\n\t\tif (!running) {\n\t\t\treturn;\n\t\t}\n\n\t\t// depending on whether we were restored with the current state version (1.3),\n\t\t// remaining logic branches off into 2 paths:\n\t\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t\t//                 thread running the main fetcher loop\n\t\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\t\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\t\twhile (running) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t\t} finally {\n\t\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\t\t// (if not running, cancel() was already called)\n\t\t\t\t\t\tif (running) {\n\t\t\t\t\t\t\tcancel();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tdiscoveryLoopThread.start();\n\t\t\tkafkaFetcher.runFetchLoop();\n\n\t\t\t// --------------------------------------------------------------------\n\n\t\t\t// make sure that the partition discoverer is properly closed\n\t\t\tpartitionDiscoverer.close();\n\t\t\tdiscoveryLoopThread.join();\n\n\t\t\t// rethrow any fetcher errors\n\t\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\t\tif (discoveryLoopError != null) {\n\t\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t\t}\n\t\t} else {\n\t\t\t// won't be using the discoverer\n\t\t\tpartitionDiscoverer.close();\n\n\t\t\tkafkaFetcher.runFetchLoop();\n\t\t}\n\t}",
            " 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718 +\n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  ",
            "\t@Override\n\tpublic void run(SourceContext<T> sourceContext) throws Exception {\n\t\tif (subscribedPartitionsToStartOffsets == null) {\n\t\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t\t}\n\n\t\t// initialize commit metrics and default offset callback method\n\t\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\t\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\t\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t\t@Override\n\t\t\tpublic void onSuccess() {\n\t\t\t\tsuccessfulCommits.inc();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void onException(Throwable cause) {\n\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\t\tfailedCommits.inc();\n\t\t\t}\n\t\t};\n\n\t\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t\t// status will automatically be triggered back to be active.\n\t\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\t\tsourceContext.markAsTemporarilyIdle();\n\t\t}\n\n\t\t// from this point forward:\n\t\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t\t//     Kafka through the fetcher, if configured to do so)\n\t\tthis.kafkaFetcher = createFetcher(\n\t\t\t\tsourceContext,\n\t\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\t\tperiodicWatermarkAssigner,\n\t\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\t\toffsetCommitMode,\n\t\t\t\tgetRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n\t\t\t\tuseMetrics);\n\n\t\tif (!running) {\n\t\t\treturn;\n\t\t}\n\n\t\t// depending on whether we were restored with the current state version (1.3),\n\t\t// remaining logic branches off into 2 paths:\n\t\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t\t//                 thread running the main fetcher loop\n\t\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\t\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\t\twhile (running) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t\t} finally {\n\t\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\t\t// (if not running, cancel() was already called)\n\t\t\t\t\t\tif (running) {\n\t\t\t\t\t\t\tcancel();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}, \"Kafka Partition Discovery for \" + getRuntimeContext().getTaskNameWithSubtasks());\n\n\t\t\tdiscoveryLoopThread.start();\n\t\t\tkafkaFetcher.runFetchLoop();\n\n\t\t\t// --------------------------------------------------------------------\n\n\t\t\t// make sure that the partition discoverer is properly closed\n\t\t\tpartitionDiscoverer.close();\n\t\t\tdiscoveryLoopThread.join();\n\n\t\t\t// rethrow any fetcher errors\n\t\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\t\tif (discoveryLoopError != null) {\n\t\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t\t}\n\t\t} else {\n\t\t\t// won't be using the discoverer\n\t\t\tpartitionDiscoverer.close();\n\n\t\t\tkafkaFetcher.runFetchLoop();\n\t\t}\n\t}"
        ]
    ],
    "21cf59d5fffdca9e8335e1990c75e0c3cd684212": [
        [
            "RocksDBKeyedStateBackend::RocksDBFullSnapshotOperation::writeKVStateMetaData()",
            "1947  \n1948  \n1949 -\n1950 -\n1951 -\n1952 -\n1953  \n1954  \n1955  \n1956  \n1957  \n1958  \n1959  \n1960 -\n1961 -\n1962 -\n1963 -\n1964  \n1965  \n1966 -\n1967  \n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974 -\n1975  \n1976  \n1977  \n1978  \n1979  \n1980  ",
            "\t\tprivate void writeKVStateMetaData() throws IOException {\n\n\t\t\tList<RegisteredKeyedBackendStateMetaInfo.Snapshot<?, ?>> metaInfoSnapshots =\n\t\t\t\tnew ArrayList<>(kvStateInformationCopy.size());\n\n\t\t\tthis.kvStateIterators = new ArrayList<>(kvStateInformationCopy.size());\n\n\t\t\tint kvStateId = 0;\n\n\t\t\t//retrieve iterator for this k/v states\n\t\t\treadOptions = new ReadOptions();\n\t\t\treadOptions.setSnapshot(snapshot);\n\n\t\t\tfor (Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>> column :\n\t\t\t\tkvStateInformationCopy) {\n\n\t\t\t\tmetaInfoSnapshots.add(column.f1.snapshot());\n\n\t\t\t\tkvStateIterators.add(\n\t\t\t\t\tnew Tuple2<>(stateBackend.db.newIterator(column.f0, readOptions), kvStateId));\n\n\t\t\t\t++kvStateId;\n\t\t\t}\n\n\t\t\tKeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy<>(\n\t\t\t\t\tstateBackend.getKeySerializer(),\n\t\t\t\t\tmetaInfoSnapshots,\n\t\t\t\t\t!Objects.equals(\n\t\t\t\t\t\tUncompressedStreamCompressionDecorator.INSTANCE,\n\t\t\t\t\t\tstateBackend.keyGroupCompressionDecorator));\n\n\t\t\tserializationProxy.write(outputView);\n\t\t}",
            "1965  \n1966  \n1967 +\n1968  \n1969  \n1970  \n1971  \n1972  \n1973  \n1974  \n1975 +\n1976  \n1977  \n1978 +\n1979  \n1980  \n1981  \n1982  \n1983  \n1984  \n1985  \n1986 +\n1987  \n1988  \n1989  \n1990  \n1991  \n1992  ",
            "\t\tprivate void writeKVStateMetaData() throws IOException {\n\n\t\t\tthis.kvStateIterators = new ArrayList<>(copiedColumnFamilyHandles.size());\n\n\t\t\tint kvStateId = 0;\n\n\t\t\t//retrieve iterator for this k/v states\n\t\t\treadOptions = new ReadOptions();\n\t\t\treadOptions.setSnapshot(snapshot);\n\n\t\t\tfor (ColumnFamilyHandle columnFamilyHandle : copiedColumnFamilyHandles) {\n\n\t\t\t\tkvStateIterators.add(\n\t\t\t\t\tnew Tuple2<>(stateBackend.db.newIterator(columnFamilyHandle, readOptions), kvStateId));\n\n\t\t\t\t++kvStateId;\n\t\t\t}\n\n\t\t\tKeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy<>(\n\t\t\t\t\tstateBackend.getKeySerializer(),\n\t\t\t\t\tstateMetaInfoSnapshots,\n\t\t\t\t\t!Objects.equals(\n\t\t\t\t\t\tUncompressedStreamCompressionDecorator.INSTANCE,\n\t\t\t\t\t\tstateBackend.keyGroupCompressionDecorator));\n\n\t\t\tserializationProxy.write(outputView);\n\t\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBFullSnapshotOperation::takeDBSnapShot()",
            "1857  \n1858  \n1859  \n1860  \n1861  \n1862  \n1863 -\n1864  \n1865  ",
            "\t\t/**\n\t\t * 1) Create a snapshot object from RocksDB.\n\t\t *\n\t\t */\n\t\tpublic void takeDBSnapShot() {\n\t\t\tPreconditions.checkArgument(snapshot == null, \"Only one ongoing snapshot allowed!\");\n\t\t\tthis.kvStateInformationCopy = new ArrayList<>(stateBackend.kvStateInformation.values());\n\t\t\tthis.snapshot = stateBackend.db.getSnapshot();\n\t\t}",
            "1863  \n1864  \n1865  \n1866  \n1867  \n1868  \n1869 +\n1870 +\n1871 +\n1872 +\n1873 +\n1874 +\n1875 +\n1876 +\n1877 +\n1878 +\n1879 +\n1880 +\n1881 +\n1882  \n1883  ",
            "\t\t/**\n\t\t * 1) Create a snapshot object from RocksDB.\n\t\t *\n\t\t */\n\t\tpublic void takeDBSnapShot() {\n\t\t\tPreconditions.checkArgument(snapshot == null, \"Only one ongoing snapshot allowed!\");\n\n\t\t\tthis.stateMetaInfoSnapshots = new ArrayList<>(stateBackend.kvStateInformation.size());\n\n\t\t\tthis.copiedColumnFamilyHandles = new ArrayList<>(stateBackend.kvStateInformation.size());\n\n\t\t\tfor (Tuple2<ColumnFamilyHandle, RegisteredKeyedBackendStateMetaInfo<?, ?>> tuple2 :\n\t\t\t\tstateBackend.kvStateInformation.values()) {\n\t\t\t\t// snapshot meta info\n\t\t\t\tthis.stateMetaInfoSnapshots.add(tuple2.f1.snapshot());\n\n\t\t\t\t// copy column family handle\n\t\t\t\tthis.copiedColumnFamilyHandles.add(tuple2.f0);\n\t\t\t}\n\t\t\tthis.snapshot = stateBackend.db.getSnapshot();\n\t\t}"
        ]
    ],
    "97f556ebd122dfe16dd8e148d3ac7c5386fd08ac": [
        [
            "RocksDBKeyedStateBackend::RocksDBKeyedStateBackend(String,ClassLoader,File,DBOptions,ColumnFamilyOptions,TaskKvStateRegistry,TypeSerializer,int,KeyGroupRange,ExecutionConfig,boolean,LocalRecoveryConfig,RocksDBStateBackend,TtlTimeProvider,RocksDBNativeMetricOptions,MetricGroup)",
            " 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299 -\n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  ",
            "\tpublic RocksDBKeyedStateBackend(\n\t\tString operatorIdentifier,\n\t\tClassLoader userCodeClassLoader,\n\t\tFile instanceBasePath,\n\t\tDBOptions dbOptions,\n\t\tColumnFamilyOptions columnFamilyOptions,\n\t\tTaskKvStateRegistry kvStateRegistry,\n\t\tTypeSerializer<K> keySerializer,\n\t\tint numberOfKeyGroups,\n\t\tKeyGroupRange keyGroupRange,\n\t\tExecutionConfig executionConfig,\n\t\tboolean enableIncrementalCheckpointing,\n\t\tLocalRecoveryConfig localRecoveryConfig,\n\t\tRocksDBStateBackend.PriorityQueueStateType priorityQueueStateType,\n\t\tTtlTimeProvider ttlTimeProvider,\n\t\tRocksDBNativeMetricOptions metricOptions,\n\t\tMetricGroup metricGroup\n\t) throws IOException {\n\n\t\tsuper(kvStateRegistry, keySerializer, userCodeClassLoader,\n\t\t\tnumberOfKeyGroups, keyGroupRange, executionConfig, ttlTimeProvider);\n\n\t\tthis.operatorIdentifier = Preconditions.checkNotNull(operatorIdentifier);\n\n\t\tthis.enableIncrementalCheckpointing = enableIncrementalCheckpointing;\n\t\tthis.rocksDBResourceGuard = new ResourceGuard();\n\n\t\t// ensure that we use the right merge operator, because other code relies on this\n\t\tthis.columnOptions = Preconditions.checkNotNull(columnFamilyOptions)\n\t\t\t.setMergeOperatorName(MERGE_OPERATOR_NAME);\n\n\t\tthis.dbOptions = Preconditions.checkNotNull(dbOptions);\n\n\t\tthis.instanceBasePath = Preconditions.checkNotNull(instanceBasePath);\n\t\tthis.instanceRocksDBPath = new File(instanceBasePath, \"db\");\n\n\t\tcheckAndCreateDirectory(instanceBasePath);\n\n\t\tif (instanceRocksDBPath.exists()) {\n\t\t\t// Clear the base directory when the backend is created\n\t\t\t// in case something crashed and the backend never reached dispose()\n\t\t\tcleanInstanceBasePath();\n\t\t}\n\n\t\tthis.localRecoveryConfig = Preconditions.checkNotNull(localRecoveryConfig);\n\t\tthis.keyGroupPrefixBytes =\n\t\t\tRocksDBKeySerializationUtils.computeRequiredBytesInKeyGroupPrefix(getNumberOfKeyGroups());\n\t\tthis.kvStateInformation = new LinkedHashMap<>();\n\t\tthis.restoredKvStateMetaInfos = new HashMap<>();\n\n\t\tthis.writeOptions = new WriteOptions().setDisableWAL(true);\n\n\t\tthis.metricOptions = metricOptions;\n\t\tthis.metricGroup = metricGroup;\n\n\t\tswitch (priorityQueueStateType) {\n\t\t\tcase HEAP:\n\t\t\t\tthis.priorityQueueFactory = new HeapPriorityQueueSetFactory(keyGroupRange, numberOfKeyGroups, 128);\n\t\t\t\tbreak;\n\t\t\tcase ROCKSDB:\n\t\t\t\tthis.priorityQueueFactory = new RocksDBPriorityQueueSetFactory();\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalArgumentException(\"Unknown priority queue state type: \" + priorityQueueStateType);\n\t\t}\n\t}",
            " 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "\tpublic RocksDBKeyedStateBackend(\n\t\tString operatorIdentifier,\n\t\tClassLoader userCodeClassLoader,\n\t\tFile instanceBasePath,\n\t\tDBOptions dbOptions,\n\t\tColumnFamilyOptions columnFamilyOptions,\n\t\tTaskKvStateRegistry kvStateRegistry,\n\t\tTypeSerializer<K> keySerializer,\n\t\tint numberOfKeyGroups,\n\t\tKeyGroupRange keyGroupRange,\n\t\tExecutionConfig executionConfig,\n\t\tboolean enableIncrementalCheckpointing,\n\t\tLocalRecoveryConfig localRecoveryConfig,\n\t\tRocksDBStateBackend.PriorityQueueStateType priorityQueueStateType,\n\t\tTtlTimeProvider ttlTimeProvider,\n\t\tRocksDBNativeMetricOptions metricOptions,\n\t\tMetricGroup metricGroup\n\t) throws IOException {\n\n\t\tsuper(kvStateRegistry, keySerializer, userCodeClassLoader,\n\t\t\tnumberOfKeyGroups, keyGroupRange, executionConfig, ttlTimeProvider);\n\n\t\tthis.operatorIdentifier = Preconditions.checkNotNull(operatorIdentifier);\n\n\t\tthis.enableIncrementalCheckpointing = enableIncrementalCheckpointing;\n\t\tthis.rocksDBResourceGuard = new ResourceGuard();\n\n\t\t// ensure that we use the right merge operator, because other code relies on this\n\t\tthis.columnOptions = Preconditions.checkNotNull(columnFamilyOptions)\n\t\t\t.setMergeOperatorName(MERGE_OPERATOR_NAME);\n\n\t\tthis.dbOptions = Preconditions.checkNotNull(dbOptions);\n\n\t\tthis.instanceBasePath = Preconditions.checkNotNull(instanceBasePath);\n\t\tthis.instanceRocksDBPath = new File(instanceBasePath, \"db\");\n\n\t\tcheckAndCreateDirectory(instanceBasePath);\n\n\t\tif (instanceRocksDBPath.exists()) {\n\t\t\t// Clear the base directory when the backend is created\n\t\t\t// in case something crashed and the backend never reached dispose()\n\t\t\tcleanInstanceBasePath();\n\t\t}\n\n\t\tthis.localRecoveryConfig = Preconditions.checkNotNull(localRecoveryConfig);\n\t\tthis.keyGroupPrefixBytes =\n\t\t\tRocksDBKeySerializationUtils.computeRequiredBytesInKeyGroupPrefix(getNumberOfKeyGroups());\n\t\tthis.kvStateInformation = new LinkedHashMap<>();\n\n\t\tthis.writeOptions = new WriteOptions().setDisableWAL(true);\n\n\t\tthis.metricOptions = metricOptions;\n\t\tthis.metricGroup = metricGroup;\n\n\t\tswitch (priorityQueueStateType) {\n\t\t\tcase HEAP:\n\t\t\t\tthis.priorityQueueFactory = new HeapPriorityQueueSetFactory(keyGroupRange, numberOfKeyGroups, 128);\n\t\t\t\tbreak;\n\t\t\tcase ROCKSDB:\n\t\t\t\tthis.priorityQueueFactory = new RocksDBPriorityQueueSetFactory();\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalArgumentException(\"Unknown priority queue state type: \" + priorityQueueStateType);\n\t\t}\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::dispose()",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427 -\n 428  \n 429  \n 430  \n 431  ",
            "\t/**\n\t * Should only be called by one thread, and only after all accesses to the DB happened.\n\t */\n\t@Override\n\tpublic void dispose() {\n\t\tsuper.dispose();\n\n\t\t// This call will block until all clients that still acquire access to the RocksDB instance have released it,\n\t\t// so that we cannot release the native resources while clients are still working with it in parallel.\n\t\trocksDBResourceGuard.close();\n\n\t\t// IMPORTANT: null reference to signal potential async checkpoint workers that the db was disposed, as\n\t\t// working on the disposed object results in SEGFAULTS.\n\t\tif (db != null) {\n\n\t\t\tIOUtils.closeQuietly(writeBatchWrapper);\n\n\t\t\t// Metric collection occurs on a background thread. When this method returns\n\t\t\t// it is guaranteed that thr RocksDB reference has been invalidated\n\t\t\t// and no more metric collection will be attempted against the database.\n\t\t\tif (nativeMetricMonitor != null) {\n\t\t\t\tnativeMetricMonitor.close();\n\t\t\t}\n\n\t\t\t// RocksDB's native memory management requires that *all* CFs (including default) are closed before the\n\t\t\t// DB is closed. See:\n\t\t\t// https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#opening-a-database-with-column-families\n\t\t\t// Start with default CF ...\n\t\t\tIOUtils.closeQuietly(defaultColumnFamily);\n\n\t\t\t// ... continue with the ones created by Flink...\n\t\t\tfor (Tuple2<ColumnFamilyHandle, RegisteredStateMetaInfoBase> columnMetaData :\n\t\t\t\tkvStateInformation.values()) {\n\t\t\t\tIOUtils.closeQuietly(columnMetaData.f0);\n\t\t\t}\n\n\t\t\t// ... and finally close the DB instance ...\n\t\t\tIOUtils.closeQuietly(db);\n\n\t\t\t// invalidate the reference\n\t\t\tdb = null;\n\n\t\t\tIOUtils.closeQuietly(columnOptions);\n\t\t\tIOUtils.closeQuietly(dbOptions);\n\t\t\tIOUtils.closeQuietly(writeOptions);\n\t\t\tkvStateInformation.clear();\n\t\t\trestoredKvStateMetaInfos.clear();\n\n\t\t\tcleanInstanceBasePath();\n\t\t}\n\t}",
            " 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  ",
            "\t/**\n\t * Should only be called by one thread, and only after all accesses to the DB happened.\n\t */\n\t@Override\n\tpublic void dispose() {\n\t\tsuper.dispose();\n\n\t\t// This call will block until all clients that still acquire access to the RocksDB instance have released it,\n\t\t// so that we cannot release the native resources while clients are still working with it in parallel.\n\t\trocksDBResourceGuard.close();\n\n\t\t// IMPORTANT: null reference to signal potential async checkpoint workers that the db was disposed, as\n\t\t// working on the disposed object results in SEGFAULTS.\n\t\tif (db != null) {\n\n\t\t\tIOUtils.closeQuietly(writeBatchWrapper);\n\n\t\t\t// Metric collection occurs on a background thread. When this method returns\n\t\t\t// it is guaranteed that thr RocksDB reference has been invalidated\n\t\t\t// and no more metric collection will be attempted against the database.\n\t\t\tif (nativeMetricMonitor != null) {\n\t\t\t\tnativeMetricMonitor.close();\n\t\t\t}\n\n\t\t\t// RocksDB's native memory management requires that *all* CFs (including default) are closed before the\n\t\t\t// DB is closed. See:\n\t\t\t// https://github.com/facebook/rocksdb/wiki/RocksJava-Basics#opening-a-database-with-column-families\n\t\t\t// Start with default CF ...\n\t\t\tIOUtils.closeQuietly(defaultColumnFamily);\n\n\t\t\t// ... continue with the ones created by Flink...\n\t\t\tfor (Tuple2<ColumnFamilyHandle, RegisteredStateMetaInfoBase> columnMetaData :\n\t\t\t\tkvStateInformation.values()) {\n\t\t\t\tIOUtils.closeQuietly(columnMetaData.f0);\n\t\t\t}\n\n\t\t\t// ... and finally close the DB instance ...\n\t\t\tIOUtils.closeQuietly(db);\n\n\t\t\t// invalidate the reference\n\t\t\tdb = null;\n\n\t\t\tIOUtils.closeQuietly(columnOptions);\n\t\t\tIOUtils.closeQuietly(dbOptions);\n\t\t\tIOUtils.closeQuietly(writeOptions);\n\t\t\tkvStateInformation.clear();\n\n\t\t\tcleanInstanceBasePath();\n\t\t}\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::restore(Collection)",
            " 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513 -\n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  ",
            "\t@Override\n\tpublic void restore(Collection<KeyedStateHandle> restoreState) throws Exception {\n\n\t\tLOG.info(\"Initializing RocksDB keyed state backend.\");\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"Restoring snapshot from state handles: {}.\", restoreState);\n\t\t}\n\n\t\t// clear all meta data\n\t\tkvStateInformation.clear();\n\t\trestoredKvStateMetaInfos.clear();\n\n\t\ttry {\n\t\t\tRocksDBIncrementalRestoreOperation<K> incrementalRestoreOperation = null;\n\t\t\tif (restoreState == null || restoreState.isEmpty()) {\n\t\t\t\tcreateDB();\n\t\t\t} else {\n\t\t\t\tKeyedStateHandle firstStateHandle = restoreState.iterator().next();\n\t\t\t\tif (firstStateHandle instanceof IncrementalKeyedStateHandle\n\t\t\t\t\t|| firstStateHandle instanceof IncrementalLocalKeyedStateHandle) {\n\t\t\t\t\tincrementalRestoreOperation = new RocksDBIncrementalRestoreOperation<>(this);\n\t\t\t\t\tincrementalRestoreOperation.restore(restoreState);\n\t\t\t\t} else {\n\t\t\t\t\tRocksDBFullRestoreOperation<K> fullRestoreOperation = new RocksDBFullRestoreOperation<>(this);\n\t\t\t\t\tfullRestoreOperation.doRestore(restoreState);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tinitializeSnapshotStrategy(incrementalRestoreOperation);\n\t\t} catch (Exception ex) {\n\t\t\tdispose();\n\t\t\tthrow ex;\n\t\t}\n\t}",
            " 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  ",
            "\t@Override\n\tpublic void restore(Collection<KeyedStateHandle> restoreState) throws Exception {\n\n\t\tLOG.info(\"Initializing RocksDB keyed state backend.\");\n\n\t\tif (LOG.isDebugEnabled()) {\n\t\t\tLOG.debug(\"Restoring snapshot from state handles: {}.\", restoreState);\n\t\t}\n\n\t\t// clear all meta data\n\t\tkvStateInformation.clear();\n\n\t\ttry {\n\t\t\tRocksDBIncrementalRestoreOperation<K> incrementalRestoreOperation = null;\n\t\t\tif (restoreState == null || restoreState.isEmpty()) {\n\t\t\t\tcreateDB();\n\t\t\t} else {\n\t\t\t\tKeyedStateHandle firstStateHandle = restoreState.iterator().next();\n\t\t\t\tif (firstStateHandle instanceof IncrementalKeyedStateHandle\n\t\t\t\t\t|| firstStateHandle instanceof IncrementalLocalKeyedStateHandle) {\n\t\t\t\t\tincrementalRestoreOperation = new RocksDBIncrementalRestoreOperation<>(this);\n\t\t\t\t\tincrementalRestoreOperation.restore(restoreState);\n\t\t\t\t} else {\n\t\t\t\t\tRocksDBFullRestoreOperation<K> fullRestoreOperation = new RocksDBFullRestoreOperation<>(this);\n\t\t\t\t\tfullRestoreOperation.doRestore(restoreState);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tinitializeSnapshotStrategy(incrementalRestoreOperation);\n\t\t} catch (Exception ex) {\n\t\t\tdispose();\n\t\t\tthrow ex;\n\t\t}\n\t}"
        ],
        [
            "HeapKeyedStateBackend::create(String,TypeSerializer)",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205 -\n 206 -\n 207 -\n 208 -\n 209 -\n 210 -\n 211  \n 212  \n 213 -\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "\t@SuppressWarnings(\"unchecked\")\n\t@Nonnull\n\t@Override\n\tpublic <T extends HeapPriorityQueueElement & PriorityComparable & Keyed> KeyGroupedInternalPriorityQueue<T> create(\n\t\t@Nonnull String stateName,\n\t\t@Nonnull TypeSerializer<T> byteOrderedElementSerializer) {\n\n\t\tfinal HeapPriorityQueueSnapshotRestoreWrapper existingState = registeredPQStates.get(stateName);\n\n\t\tif (existingState != null) {\n\t\t\t// TODO we implement the simple way of supporting the current functionality, mimicking keyed state\n\t\t\t// because this should be reworked in FLINK-9376 and then we should have a common algorithm over\n\t\t\t// StateMetaInfoSnapshot that avoids this code duplication.\n\t\t\tStateMetaInfoSnapshot restoredMetaInfoSnapshot =\n\t\t\t\trestoredStateMetaInfo.get(StateUID.of(stateName, StateMetaInfoSnapshot.BackendStateType.PRIORITY_QUEUE));\n\n\t\t\tPreconditions.checkState(\n\t\t\t\trestoredMetaInfoSnapshot != null,\n\t\t\t\t\"Requested to check compatibility of a restored RegisteredKeyedBackendStateMetaInfo,\" +\n\t\t\t\t\t\" but its corresponding restored snapshot cannot be found.\");\n\n\t\t\tStateMetaInfoSnapshot.CommonSerializerKeys serializerKey =\n\t\t\t\tStateMetaInfoSnapshot.CommonSerializerKeys.VALUE_SERIALIZER;\n\n\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\tTypeSerializerSnapshot<T> serializerSnapshot = Preconditions.checkNotNull(\n\t\t\t\t(TypeSerializerSnapshot<T>) restoredMetaInfoSnapshot.getTypeSerializerConfigSnapshot(serializerKey));\n\n\t\t\tTypeSerializerSchemaCompatibility<T> compatibilityResult =\n\t\t\t\tserializerSnapshot.resolveSchemaCompatibility(byteOrderedElementSerializer);\n\n\t\t\tif (compatibilityResult.isIncompatible()) {\n\t\t\t\tthrow new FlinkRuntimeException(new StateMigrationException(\"For heap backends, the new priority queue serializer must not be incompatible.\"));\n\t\t\t} else {\n\t\t\t\tregisteredPQStates.put(\n\t\t\t\t\tstateName,\n\t\t\t\t\texistingState.forUpdatedSerializer(byteOrderedElementSerializer));\n\t\t\t}\n\n\t\t\treturn existingState.getPriorityQueue();\n\t\t} else {\n\t\t\tfinal RegisteredPriorityQueueStateBackendMetaInfo<T> metaInfo =\n\t\t\t\tnew RegisteredPriorityQueueStateBackendMetaInfo<>(stateName, byteOrderedElementSerializer);\n\t\t\treturn createInternal(metaInfo);\n\t\t}\n\t}",
            " 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190 +\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "\t@SuppressWarnings(\"unchecked\")\n\t@Nonnull\n\t@Override\n\tpublic <T extends HeapPriorityQueueElement & PriorityComparable & Keyed> KeyGroupedInternalPriorityQueue<T> create(\n\t\t@Nonnull String stateName,\n\t\t@Nonnull TypeSerializer<T> byteOrderedElementSerializer) {\n\n\t\tfinal HeapPriorityQueueSnapshotRestoreWrapper existingState = registeredPQStates.get(stateName);\n\n\t\tif (existingState != null) {\n\t\t\t// TODO we implement the simple way of supporting the current functionality, mimicking keyed state\n\t\t\t// because this should be reworked in FLINK-9376 and then we should have a common algorithm over\n\t\t\t// StateMetaInfoSnapshot that avoids this code duplication.\n\n\t\t\tTypeSerializerSchemaCompatibility<T> compatibilityResult =\n\t\t\t\texistingState.getMetaInfo().updateElementSerializer(byteOrderedElementSerializer);\n\n\t\t\tif (compatibilityResult.isIncompatible()) {\n\t\t\t\tthrow new FlinkRuntimeException(new StateMigrationException(\"For heap backends, the new priority queue serializer must not be incompatible.\"));\n\t\t\t} else {\n\t\t\t\tregisteredPQStates.put(\n\t\t\t\t\tstateName,\n\t\t\t\t\texistingState.forUpdatedSerializer(byteOrderedElementSerializer));\n\t\t\t}\n\n\t\t\treturn existingState.getPriorityQueue();\n\t\t} else {\n\t\t\tfinal RegisteredPriorityQueueStateBackendMetaInfo<T> metaInfo =\n\t\t\t\tnew RegisteredPriorityQueueStateBackendMetaInfo<>(stateName, byteOrderedElementSerializer);\n\t\t\treturn createInternal(metaInfo);\n\t\t}\n\t}"
        ],
        [
            "HeapKeyedStateBackend::HeapKeyedStateBackend(TaskKvStateRegistry,TypeSerializer,ClassLoader,int,KeyGroupRange,boolean,ExecutionConfig,LocalRecoveryConfig,HeapPriorityQueueSetFactory,TtlTimeProvider)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176 -\n 177  \n 178  ",
            "\tpublic HeapKeyedStateBackend(\n\t\t\tTaskKvStateRegistry kvStateRegistry,\n\t\t\tTypeSerializer<K> keySerializer,\n\t\t\tClassLoader userCodeClassLoader,\n\t\t\tint numberOfKeyGroups,\n\t\t\tKeyGroupRange keyGroupRange,\n\t\t\tboolean asynchronousSnapshots,\n\t\t\tExecutionConfig executionConfig,\n\t\t\tLocalRecoveryConfig localRecoveryConfig,\n\t\t\tHeapPriorityQueueSetFactory priorityQueueSetFactory,\n\t\t\tTtlTimeProvider ttlTimeProvider) {\n\n\t\tsuper(kvStateRegistry, keySerializer, userCodeClassLoader,\n\t\t\tnumberOfKeyGroups, keyGroupRange, executionConfig, ttlTimeProvider);\n\n\t\tthis.registeredKVStates = new HashMap<>();\n\t\tthis.registeredPQStates = new HashMap<>();\n\t\tthis.localRecoveryConfig = Preconditions.checkNotNull(localRecoveryConfig);\n\n\t\tSnapshotStrategySynchronicityBehavior<K> synchronicityTrait = asynchronousSnapshots ?\n\t\t\tnew AsyncSnapshotStrategySynchronicityBehavior() :\n\t\t\tnew SyncSnapshotStrategySynchronicityBehavior();\n\n\t\tthis.snapshotStrategy = new HeapSnapshotStrategy(synchronicityTrait);\n\t\tLOG.info(\"Initializing heap keyed state backend with stream factory.\");\n\t\tthis.restoredStateMetaInfo = new HashMap<>();\n\t\tthis.priorityQueueSetFactory = priorityQueueSetFactory;\n\t}",
            " 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "\tpublic HeapKeyedStateBackend(\n\t\t\tTaskKvStateRegistry kvStateRegistry,\n\t\t\tTypeSerializer<K> keySerializer,\n\t\t\tClassLoader userCodeClassLoader,\n\t\t\tint numberOfKeyGroups,\n\t\t\tKeyGroupRange keyGroupRange,\n\t\t\tboolean asynchronousSnapshots,\n\t\t\tExecutionConfig executionConfig,\n\t\t\tLocalRecoveryConfig localRecoveryConfig,\n\t\t\tHeapPriorityQueueSetFactory priorityQueueSetFactory,\n\t\t\tTtlTimeProvider ttlTimeProvider) {\n\n\t\tsuper(kvStateRegistry, keySerializer, userCodeClassLoader,\n\t\t\tnumberOfKeyGroups, keyGroupRange, executionConfig, ttlTimeProvider);\n\n\t\tthis.registeredKVStates = new HashMap<>();\n\t\tthis.registeredPQStates = new HashMap<>();\n\t\tthis.localRecoveryConfig = Preconditions.checkNotNull(localRecoveryConfig);\n\n\t\tSnapshotStrategySynchronicityBehavior<K> synchronicityTrait = asynchronousSnapshots ?\n\t\t\tnew AsyncSnapshotStrategySynchronicityBehavior() :\n\t\t\tnew SyncSnapshotStrategySynchronicityBehavior();\n\n\t\tthis.snapshotStrategy = new HeapSnapshotStrategy(synchronicityTrait);\n\t\tLOG.info(\"Initializing heap keyed state backend with stream factory.\");\n\t\tthis.priorityQueueSetFactory = priorityQueueSetFactory;\n\t}"
        ],
        [
            "DefaultOperatorStateBackend::getBroadcastState(MapStateDescriptor)",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229 -\n 230  \n 231  \n 232 -\n 233 -\n 234 -\n 235  \n 236 -\n 237  \n 238  \n 239  \n 240  \n 241 -\n 242 -\n 243 -\n 244  \n 245 -\n 246  \n 247  \n 248  \n 249  \n 250 -\n 251 -\n 252 -\n 253 -\n 254 -\n 255 -\n 256 -\n 257  \n 258  \n 259  \n 260  \n 261  ",
            "\t@SuppressWarnings(\"unchecked\")\n\t@Override\n\tpublic <K, V> BroadcastState<K, V> getBroadcastState(final MapStateDescriptor<K, V> stateDescriptor) throws StateMigrationException {\n\n\t\tPreconditions.checkNotNull(stateDescriptor);\n\t\tString name = Preconditions.checkNotNull(stateDescriptor.getName());\n\n\t\tBackendWritableBroadcastState<K, V> previous =\n\t\t\t(BackendWritableBroadcastState<K, V>) accessedBroadcastStatesByName.get(name);\n\n\t\tif (previous != null) {\n\t\t\tcheckStateNameAndMode(\n\t\t\t\t\tprevious.getStateMetaInfo().getName(),\n\t\t\t\t\tname,\n\t\t\t\t\tprevious.getStateMetaInfo().getAssignmentMode(),\n\t\t\t\t\tOperatorStateHandle.Mode.BROADCAST);\n\t\t\treturn previous;\n\t\t}\n\n\t\tstateDescriptor.initializeSerializerUnlessSet(getExecutionConfig());\n\t\tTypeSerializer<K> broadcastStateKeySerializer = Preconditions.checkNotNull(stateDescriptor.getKeySerializer());\n\t\tTypeSerializer<V> broadcastStateValueSerializer = Preconditions.checkNotNull(stateDescriptor.getValueSerializer());\n\n\t\tBackendWritableBroadcastState<K, V> broadcastState =\n\t\t\t(BackendWritableBroadcastState<K, V>) registeredBroadcastStates.get(name);\n\n\t\tif (broadcastState == null) {\n\t\t\tbroadcastState = new HeapBroadcastState<>(\n\t\t\t\t\tnew RegisteredBroadcastStateBackendMetaInfo<>(\n\t\t\t\t\t\t\tname,\n\t\t\t\t\t\t\tOperatorStateHandle.Mode.BROADCAST,\n\t\t\t\t\t\t\tbroadcastStateKeySerializer,\n\t\t\t\t\t\t\tbroadcastStateValueSerializer));\n\t\t\tregisteredBroadcastStates.put(name, broadcastState);\n\t\t} else {\n\t\t\t// has restored state; check compatibility of new state access\n\n\t\t\tcheckStateNameAndMode(\n\t\t\t\t\tbroadcastState.getStateMetaInfo().getName(),\n\t\t\t\t\tname,\n\t\t\t\t\tbroadcastState.getStateMetaInfo().getAssignmentMode(),\n\t\t\t\t\tOperatorStateHandle.Mode.BROADCAST);\n\n\t\t\tfinal StateMetaInfoSnapshot metaInfoSnapshot = restoredBroadcastStateMetaInfos.get(name);\n\n\t\t\t// check whether new serializers are incompatible\n\t\t\tTypeSerializerSnapshot<K> keySerializerSnapshot = Preconditions.checkNotNull(\n\t\t\t\t(TypeSerializerSnapshot<K>) metaInfoSnapshot.getTypeSerializerConfigSnapshot(StateMetaInfoSnapshot.CommonSerializerKeys.KEY_SERIALIZER));\n\n\t\t\tTypeSerializerSchemaCompatibility<K> keyCompatibility =\n\t\t\t\tkeySerializerSnapshot.resolveSchemaCompatibility(broadcastStateKeySerializer);\n\t\t\tif (keyCompatibility.isIncompatible()) {\n\t\t\t\tthrow new StateMigrationException(\"The new key serializer for broadcast state must not be incompatible.\");\n\t\t\t}\n\n\t\t\tTypeSerializerSnapshot<V> valueSerializerSnapshot = Preconditions.checkNotNull(\n\t\t\t\t(TypeSerializerSnapshot<V>) metaInfoSnapshot.getTypeSerializerConfigSnapshot(StateMetaInfoSnapshot.CommonSerializerKeys.VALUE_SERIALIZER));\n\n\t\t\tTypeSerializerSchemaCompatibility<V> valueCompatibility =\n\t\t\t\tvalueSerializerSnapshot.resolveSchemaCompatibility(broadcastStateValueSerializer);\n\t\t\tif (valueCompatibility.isIncompatible()) {\n\t\t\t\tthrow new StateMigrationException(\"The new value serializer for broadcast state must not be incompatible.\");\n\t\t\t}\n\n\t\t\t// new serializer is compatible; use it to replace the old serializer\n\t\t\tbroadcastState.setStateMetaInfo(\n\t\t\t\t\tnew RegisteredBroadcastStateBackendMetaInfo<>(\n\t\t\t\t\t\t\tname,\n\t\t\t\t\t\t\tOperatorStateHandle.Mode.BROADCAST,\n\t\t\t\t\t\t\tbroadcastStateKeySerializer,\n\t\t\t\t\t\t\tbroadcastStateValueSerializer));\n\t\t}\n\n\t\taccessedBroadcastStatesByName.put(name, broadcastState);\n\t\treturn broadcastState;\n\t}",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217 +\n 218  \n 219  \n 220  \n 221  \n 222  \n 223 +\n 224  \n 225  \n 226  \n 227  \n 228 +\n 229  \n 230  \n 231  \n 232  \n 233  ",
            "\t@SuppressWarnings(\"unchecked\")\n\t@Override\n\tpublic <K, V> BroadcastState<K, V> getBroadcastState(final MapStateDescriptor<K, V> stateDescriptor) throws StateMigrationException {\n\n\t\tPreconditions.checkNotNull(stateDescriptor);\n\t\tString name = Preconditions.checkNotNull(stateDescriptor.getName());\n\n\t\tBackendWritableBroadcastState<K, V> previous =\n\t\t\t(BackendWritableBroadcastState<K, V>) accessedBroadcastStatesByName.get(name);\n\n\t\tif (previous != null) {\n\t\t\tcheckStateNameAndMode(\n\t\t\t\t\tprevious.getStateMetaInfo().getName(),\n\t\t\t\t\tname,\n\t\t\t\t\tprevious.getStateMetaInfo().getAssignmentMode(),\n\t\t\t\t\tOperatorStateHandle.Mode.BROADCAST);\n\t\t\treturn previous;\n\t\t}\n\n\t\tstateDescriptor.initializeSerializerUnlessSet(getExecutionConfig());\n\t\tTypeSerializer<K> broadcastStateKeySerializer = Preconditions.checkNotNull(stateDescriptor.getKeySerializer());\n\t\tTypeSerializer<V> broadcastStateValueSerializer = Preconditions.checkNotNull(stateDescriptor.getValueSerializer());\n\n\t\tBackendWritableBroadcastState<K, V> broadcastState =\n\t\t\t(BackendWritableBroadcastState<K, V>) registeredBroadcastStates.get(name);\n\n\t\tif (broadcastState == null) {\n\t\t\tbroadcastState = new HeapBroadcastState<>(\n\t\t\t\t\tnew RegisteredBroadcastStateBackendMetaInfo<>(\n\t\t\t\t\t\t\tname,\n\t\t\t\t\t\t\tOperatorStateHandle.Mode.BROADCAST,\n\t\t\t\t\t\t\tbroadcastStateKeySerializer,\n\t\t\t\t\t\t\tbroadcastStateValueSerializer));\n\t\t\tregisteredBroadcastStates.put(name, broadcastState);\n\t\t} else {\n\t\t\t// has restored state; check compatibility of new state access\n\n\t\t\tcheckStateNameAndMode(\n\t\t\t\t\tbroadcastState.getStateMetaInfo().getName(),\n\t\t\t\t\tname,\n\t\t\t\t\tbroadcastState.getStateMetaInfo().getAssignmentMode(),\n\t\t\t\t\tOperatorStateHandle.Mode.BROADCAST);\n\n\t\t\tRegisteredBroadcastStateBackendMetaInfo<K, V> restoredBroadcastStateMetaInfo = broadcastState.getStateMetaInfo();\n\n\t\t\t// check whether new serializers are incompatible\n\t\t\tTypeSerializerSchemaCompatibility<K> keyCompatibility =\n\t\t\t\trestoredBroadcastStateMetaInfo.updateKeySerializer(broadcastStateKeySerializer);\n\t\t\tif (keyCompatibility.isIncompatible()) {\n\t\t\t\tthrow new StateMigrationException(\"The new key serializer for broadcast state must not be incompatible.\");\n\t\t\t}\n\n\t\t\tTypeSerializerSchemaCompatibility<V> valueCompatibility =\n\t\t\t\trestoredBroadcastStateMetaInfo.updateValueSerializer(broadcastStateValueSerializer);\n\t\t\tif (valueCompatibility.isIncompatible()) {\n\t\t\t\tthrow new StateMigrationException(\"The new value serializer for broadcast state must not be incompatible.\");\n\t\t\t}\n\n\t\t\tbroadcastState.setStateMetaInfo(restoredBroadcastStateMetaInfo);\n\t\t}\n\n\t\taccessedBroadcastStatesByName.put(name, broadcastState);\n\t\treturn broadcastState;\n\t}"
        ],
        [
            "DefaultOperatorStateBackend::DefaultOperatorStateBackend(ClassLoader,ExecutionConfig,boolean)",
            " 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 -\n 152 -\n 153  \n 154  ",
            "\tpublic DefaultOperatorStateBackend(\n\t\tClassLoader userClassLoader,\n\t\tExecutionConfig executionConfig,\n\t\tboolean asynchronousSnapshots) {\n\n\t\tthis.closeStreamOnCancelRegistry = new CloseableRegistry();\n\t\tthis.userClassloader = Preconditions.checkNotNull(userClassLoader);\n\t\tthis.executionConfig = executionConfig;\n\t\tthis.javaSerializer = new JavaSerializer<>();\n\t\tthis.registeredOperatorStates = new HashMap<>();\n\t\tthis.registeredBroadcastStates = new HashMap<>();\n\t\tthis.asynchronousSnapshots = asynchronousSnapshots;\n\t\tthis.accessedStatesByName = new HashMap<>();\n\t\tthis.accessedBroadcastStatesByName = new HashMap<>();\n\t\tthis.restoredOperatorStateMetaInfos = new HashMap<>();\n\t\tthis.restoredBroadcastStateMetaInfos = new HashMap<>();\n\t\tthis.snapshotStrategy = new DefaultOperatorStateBackendSnapshotStrategy();\n\t}",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  ",
            "\tpublic DefaultOperatorStateBackend(\n\t\tClassLoader userClassLoader,\n\t\tExecutionConfig executionConfig,\n\t\tboolean asynchronousSnapshots) {\n\n\t\tthis.closeStreamOnCancelRegistry = new CloseableRegistry();\n\t\tthis.userClassloader = Preconditions.checkNotNull(userClassLoader);\n\t\tthis.executionConfig = executionConfig;\n\t\tthis.javaSerializer = new JavaSerializer<>();\n\t\tthis.registeredOperatorStates = new HashMap<>();\n\t\tthis.registeredBroadcastStates = new HashMap<>();\n\t\tthis.asynchronousSnapshots = asynchronousSnapshots;\n\t\tthis.accessedStatesByName = new HashMap<>();\n\t\tthis.accessedBroadcastStatesByName = new HashMap<>();\n\t\tthis.snapshotStrategy = new DefaultOperatorStateBackendSnapshotStrategy();\n\t}"
        ],
        [
            "DefaultOperatorStateBackend::restore(Collection)",
            " 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348 -\n 349 -\n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384 -\n 385 -\n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  ",
            "\tpublic void restore(Collection<OperatorStateHandle> restoreSnapshots) throws Exception {\n\n\t\tif (null == restoreSnapshots || restoreSnapshots.isEmpty()) {\n\t\t\treturn;\n\t\t}\n\n\t\tfor (OperatorStateHandle stateHandle : restoreSnapshots) {\n\n\t\t\tif (stateHandle == null) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tFSDataInputStream in = stateHandle.openInputStream();\n\t\t\tcloseStreamOnCancelRegistry.registerCloseable(in);\n\n\t\t\tClassLoader restoreClassLoader = Thread.currentThread().getContextClassLoader();\n\n\t\t\ttry {\n\t\t\t\tThread.currentThread().setContextClassLoader(userClassloader);\n\t\t\t\tOperatorBackendSerializationProxy backendSerializationProxy =\n\t\t\t\t\t\tnew OperatorBackendSerializationProxy(userClassloader);\n\n\t\t\t\tbackendSerializationProxy.read(new DataInputViewStreamWrapper(in));\n\n\t\t\t\tList<StateMetaInfoSnapshot> restoredOperatorMetaInfoSnapshots =\n\t\t\t\t\t\tbackendSerializationProxy.getOperatorStateMetaInfoSnapshots();\n\n\t\t\t\t// Recreate all PartitionableListStates from the meta info\n\t\t\t\tfor (StateMetaInfoSnapshot restoredSnapshot : restoredOperatorMetaInfoSnapshots) {\n\n\t\t\t\t\tfinal RegisteredOperatorStateBackendMetaInfo<?> restoredMetaInfo =\n\t\t\t\t\t\tnew RegisteredOperatorStateBackendMetaInfo<>(restoredSnapshot);\n\n\t\t\t\t\tif (restoredMetaInfo.getPartitionStateSerializer() instanceof UnloadableDummyTypeSerializer) {\n\n\t\t\t\t\t\t// must fail now if the previous serializer cannot be restored because there is no serializer\n\t\t\t\t\t\t// capable of reading previous state\n\t\t\t\t\t\t// TODO when eager state registration is in place, we can try to get a convert deserializer\n\t\t\t\t\t\t// TODO from the newly registered serializer instead of simply failing here\n\n\t\t\t\t\t\tthrow new IOException(\"Unable to restore operator state [\" + restoredSnapshot.getName() + \"].\" +\n\t\t\t\t\t\t\t\" The previous serializer of the operator state must be present; the serializer could\" +\n\t\t\t\t\t\t\t\" have been removed from the classpath, or its implementation have changed and could\" +\n\t\t\t\t\t\t\t\" not be loaded. This is a temporary restriction that will be fixed in future versions.\");\n\t\t\t\t\t}\n\n\t\t\t\t\trestoredOperatorStateMetaInfos.put(restoredSnapshot.getName(), restoredSnapshot);\n\n\t\t\t\t\tPartitionableListState<?> listState = registeredOperatorStates.get(restoredSnapshot.getName());\n\n\t\t\t\t\tif (null == listState) {\n\t\t\t\t\t\tlistState = new PartitionableListState<>(restoredMetaInfo);\n\n\t\t\t\t\t\tregisteredOperatorStates.put(listState.getStateMetaInfo().getName(), listState);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// ... and then get back the broadcast state.\n\t\t\t\tList<StateMetaInfoSnapshot> restoredBroadcastMetaInfoSnapshots =\n\t\t\t\t\t\tbackendSerializationProxy.getBroadcastStateMetaInfoSnapshots();\n\n\t\t\t\tfor (StateMetaInfoSnapshot restoredSnapshot : restoredBroadcastMetaInfoSnapshots) {\n\n\t\t\t\t\tfinal RegisteredBroadcastStateBackendMetaInfo<?, ?> restoredMetaInfo =\n\t\t\t\t\t\tnew RegisteredBroadcastStateBackendMetaInfo<>(restoredSnapshot);\n\n\t\t\t\t\tif (restoredMetaInfo.getKeySerializer() instanceof UnloadableDummyTypeSerializer ||\n\t\t\t\t\t\trestoredMetaInfo.getValueSerializer() instanceof UnloadableDummyTypeSerializer) {\n\n\t\t\t\t\t\t// must fail now if the previous serializer cannot be restored because there is no serializer\n\t\t\t\t\t\t// capable of reading previous state\n\t\t\t\t\t\t// TODO when eager state registration is in place, we can try to get a convert deserializer\n\t\t\t\t\t\t// TODO from the newly registered serializer instead of simply failing here\n\n\t\t\t\t\t\tthrow new IOException(\"Unable to restore broadcast state [\" + restoredSnapshot.getName() + \"].\" +\n\t\t\t\t\t\t\t\t\" The previous key and value serializers of the state must be present; the serializers could\" +\n\t\t\t\t\t\t\t\t\" have been removed from the classpath, or their implementations have changed and could\" +\n\t\t\t\t\t\t\t\t\" not be loaded. This is a temporary restriction that will be fixed in future versions.\");\n\t\t\t\t\t}\n\n\t\t\t\t\trestoredBroadcastStateMetaInfos.put(restoredSnapshot.getName(), restoredSnapshot);\n\n\t\t\t\t\tBackendWritableBroadcastState<? ,?> broadcastState = registeredBroadcastStates.get(restoredSnapshot.getName());\n\n\t\t\t\t\tif (broadcastState == null) {\n\t\t\t\t\t\tbroadcastState = new HeapBroadcastState<>(restoredMetaInfo);\n\n\t\t\t\t\t\tregisteredBroadcastStates.put(broadcastState.getStateMetaInfo().getName(), broadcastState);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Restore all the states\n\t\t\t\tfor (Map.Entry<String, OperatorStateHandle.StateMetaInfo> nameToOffsets :\n\t\t\t\t\t\tstateHandle.getStateNameToPartitionOffsets().entrySet()) {\n\n\t\t\t\t\tfinal String stateName = nameToOffsets.getKey();\n\n\t\t\t\t\tPartitionableListState<?> listStateForName = registeredOperatorStates.get(stateName);\n\t\t\t\t\tif (listStateForName == null) {\n\t\t\t\t\t\tBackendWritableBroadcastState<?, ?> broadcastStateForName = registeredBroadcastStates.get(stateName);\n\t\t\t\t\t\tPreconditions.checkState(broadcastStateForName != null, \"Found state without \" +\n\t\t\t\t\t\t\t\t\"corresponding meta info: \" + stateName);\n\t\t\t\t\t\tdeserializeBroadcastStateValues(broadcastStateForName, in, nameToOffsets.getValue());\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdeserializeOperatorStateValues(listStateForName, in, nameToOffsets.getValue());\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t} finally {\n\t\t\t\tThread.currentThread().setContextClassLoader(restoreClassLoader);\n\t\t\t\tif (closeStreamOnCancelRegistry.unregisterCloseable(in)) {\n\t\t\t\t\tIOUtils.closeQuietly(in);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}",
            " 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  ",
            "\tpublic void restore(Collection<OperatorStateHandle> restoreSnapshots) throws Exception {\n\n\t\tif (null == restoreSnapshots || restoreSnapshots.isEmpty()) {\n\t\t\treturn;\n\t\t}\n\n\t\tfor (OperatorStateHandle stateHandle : restoreSnapshots) {\n\n\t\t\tif (stateHandle == null) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tFSDataInputStream in = stateHandle.openInputStream();\n\t\t\tcloseStreamOnCancelRegistry.registerCloseable(in);\n\n\t\t\tClassLoader restoreClassLoader = Thread.currentThread().getContextClassLoader();\n\n\t\t\ttry {\n\t\t\t\tThread.currentThread().setContextClassLoader(userClassloader);\n\t\t\t\tOperatorBackendSerializationProxy backendSerializationProxy =\n\t\t\t\t\t\tnew OperatorBackendSerializationProxy(userClassloader);\n\n\t\t\t\tbackendSerializationProxy.read(new DataInputViewStreamWrapper(in));\n\n\t\t\t\tList<StateMetaInfoSnapshot> restoredOperatorMetaInfoSnapshots =\n\t\t\t\t\t\tbackendSerializationProxy.getOperatorStateMetaInfoSnapshots();\n\n\t\t\t\t// Recreate all PartitionableListStates from the meta info\n\t\t\t\tfor (StateMetaInfoSnapshot restoredSnapshot : restoredOperatorMetaInfoSnapshots) {\n\n\t\t\t\t\tfinal RegisteredOperatorStateBackendMetaInfo<?> restoredMetaInfo =\n\t\t\t\t\t\tnew RegisteredOperatorStateBackendMetaInfo<>(restoredSnapshot);\n\n\t\t\t\t\tif (restoredMetaInfo.getPartitionStateSerializer() instanceof UnloadableDummyTypeSerializer) {\n\n\t\t\t\t\t\t// must fail now if the previous serializer cannot be restored because there is no serializer\n\t\t\t\t\t\t// capable of reading previous state\n\t\t\t\t\t\t// TODO when eager state registration is in place, we can try to get a convert deserializer\n\t\t\t\t\t\t// TODO from the newly registered serializer instead of simply failing here\n\n\t\t\t\t\t\tthrow new IOException(\"Unable to restore operator state [\" + restoredSnapshot.getName() + \"].\" +\n\t\t\t\t\t\t\t\" The previous serializer of the operator state must be present; the serializer could\" +\n\t\t\t\t\t\t\t\" have been removed from the classpath, or its implementation have changed and could\" +\n\t\t\t\t\t\t\t\" not be loaded. This is a temporary restriction that will be fixed in future versions.\");\n\t\t\t\t\t}\n\n\t\t\t\t\tPartitionableListState<?> listState = registeredOperatorStates.get(restoredSnapshot.getName());\n\n\t\t\t\t\tif (null == listState) {\n\t\t\t\t\t\tlistState = new PartitionableListState<>(restoredMetaInfo);\n\n\t\t\t\t\t\tregisteredOperatorStates.put(listState.getStateMetaInfo().getName(), listState);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// ... and then get back the broadcast state.\n\t\t\t\tList<StateMetaInfoSnapshot> restoredBroadcastMetaInfoSnapshots =\n\t\t\t\t\t\tbackendSerializationProxy.getBroadcastStateMetaInfoSnapshots();\n\n\t\t\t\tfor (StateMetaInfoSnapshot restoredSnapshot : restoredBroadcastMetaInfoSnapshots) {\n\n\t\t\t\t\tfinal RegisteredBroadcastStateBackendMetaInfo<?, ?> restoredMetaInfo =\n\t\t\t\t\t\tnew RegisteredBroadcastStateBackendMetaInfo<>(restoredSnapshot);\n\n\t\t\t\t\tif (restoredMetaInfo.getKeySerializer() instanceof UnloadableDummyTypeSerializer ||\n\t\t\t\t\t\trestoredMetaInfo.getValueSerializer() instanceof UnloadableDummyTypeSerializer) {\n\n\t\t\t\t\t\t// must fail now if the previous serializer cannot be restored because there is no serializer\n\t\t\t\t\t\t// capable of reading previous state\n\t\t\t\t\t\t// TODO when eager state registration is in place, we can try to get a convert deserializer\n\t\t\t\t\t\t// TODO from the newly registered serializer instead of simply failing here\n\n\t\t\t\t\t\tthrow new IOException(\"Unable to restore broadcast state [\" + restoredSnapshot.getName() + \"].\" +\n\t\t\t\t\t\t\t\t\" The previous key and value serializers of the state must be present; the serializers could\" +\n\t\t\t\t\t\t\t\t\" have been removed from the classpath, or their implementations have changed and could\" +\n\t\t\t\t\t\t\t\t\" not be loaded. This is a temporary restriction that will be fixed in future versions.\");\n\t\t\t\t\t}\n\n\t\t\t\t\tBackendWritableBroadcastState<? ,?> broadcastState = registeredBroadcastStates.get(restoredSnapshot.getName());\n\n\t\t\t\t\tif (broadcastState == null) {\n\t\t\t\t\t\tbroadcastState = new HeapBroadcastState<>(restoredMetaInfo);\n\n\t\t\t\t\t\tregisteredBroadcastStates.put(broadcastState.getStateMetaInfo().getName(), broadcastState);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// Restore all the states\n\t\t\t\tfor (Map.Entry<String, OperatorStateHandle.StateMetaInfo> nameToOffsets :\n\t\t\t\t\t\tstateHandle.getStateNameToPartitionOffsets().entrySet()) {\n\n\t\t\t\t\tfinal String stateName = nameToOffsets.getKey();\n\n\t\t\t\t\tPartitionableListState<?> listStateForName = registeredOperatorStates.get(stateName);\n\t\t\t\t\tif (listStateForName == null) {\n\t\t\t\t\t\tBackendWritableBroadcastState<?, ?> broadcastStateForName = registeredBroadcastStates.get(stateName);\n\t\t\t\t\t\tPreconditions.checkState(broadcastStateForName != null, \"Found state without \" +\n\t\t\t\t\t\t\t\t\"corresponding meta info: \" + stateName);\n\t\t\t\t\t\tdeserializeBroadcastStateValues(broadcastStateForName, in, nameToOffsets.getValue());\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdeserializeOperatorStateValues(listStateForName, in, nameToOffsets.getValue());\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t} finally {\n\t\t\t\tThread.currentThread().setContextClassLoader(restoreClassLoader);\n\t\t\t\tif (closeStreamOnCancelRegistry.unregisterCloseable(in)) {\n\t\t\t\t\tIOUtils.closeQuietly(in);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "HeapKeyedStateBackend::tryRegisterStateTable(TypeSerializer,StateDescriptor,StateSnapshotTransformer)",
            " 252  \n 253  \n 254  \n 255 -\n 256  \n 257  \n 258  \n 259  \n 260  \n 261 -\n 262 -\n 263 -\n 264 -\n 265 -\n 266 -\n 267  \n 268  \n 269 -\n 270 -\n 271 -\n 272 -\n 273 -\n 274 -\n 275 -\n 276 -\n 277 -\n 278  \n 279 -\n 280 -\n 281 -\n 282 -\n 283  \n 284  \n 285 -\n 286  \n 287  \n 288  \n 289  \n 290 -\n 291 -\n 292 -\n 293 -\n 294 -\n 295 -\n 296  \n 297  \n 298 -\n 299  \n 300  \n 301  \n 302  \n 303  \n 304 -\n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  ",
            "\tprivate <N, V> StateTable<K, N, V> tryRegisterStateTable(\n\t\t\tTypeSerializer<N> namespaceSerializer,\n\t\t\tStateDescriptor<?, V> stateDesc,\n\t\t\tStateSnapshotTransformer<V> snapshotTransformer) throws StateMigrationException {\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tStateTable<K, N, V> stateTable = (StateTable<K, N, V>) registeredKVStates.get(stateDesc.getName());\n\n\t\tTypeSerializer<V> newStateSerializer = stateDesc.getSerializer();\n\t\tRegisteredKeyValueStateBackendMetaInfo<N, V> newMetaInfo = new RegisteredKeyValueStateBackendMetaInfo<>(\n\t\t\tstateDesc.getType(),\n\t\t\tstateDesc.getName(),\n\t\t\tnamespaceSerializer,\n\t\t\tnewStateSerializer,\n\t\t\tsnapshotTransformer);\n\n\t\tif (stateTable != null) {\n\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\tStateMetaInfoSnapshot restoredMetaInfoSnapshot =\n\t\t\t\trestoredStateMetaInfo.get(\n\t\t\t\t\tStateUID.of(stateDesc.getName(), StateMetaInfoSnapshot.BackendStateType.KEY_VALUE));\n\n\t\t\tPreconditions.checkState(\n\t\t\t\trestoredMetaInfoSnapshot != null,\n\t\t\t\t\"Requested to check compatibility of a restored RegisteredKeyedBackendStateMetaInfo,\" +\n\t\t\t\t\t\" but its corresponding restored snapshot cannot be found.\");\n\n\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\tTypeSerializerSnapshot<N> namespaceSerializerSnapshot = Preconditions.checkNotNull(\n\t\t\t\t(TypeSerializerSnapshot<N>) restoredMetaInfoSnapshot.getTypeSerializerConfigSnapshot(\n\t\t\t\t\tStateMetaInfoSnapshot.CommonSerializerKeys.NAMESPACE_SERIALIZER.toString()));\n\n\t\t\tTypeSerializerSchemaCompatibility<N> namespaceCompatibility =\n\t\t\t\tnamespaceSerializerSnapshot.resolveSchemaCompatibility(namespaceSerializer);\n\t\t\tif (!namespaceCompatibility.isCompatibleAsIs()) {\n\t\t\t\tthrow new StateMigrationException(\"For heap backends, the new namespace serializer must be compatible.\");\n\t\t\t}\n\n\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\tTypeSerializerSnapshot<V> stateSerializerSnapshot = Preconditions.checkNotNull(\n\t\t\t\t(TypeSerializerSnapshot<V>) restoredMetaInfoSnapshot.getTypeSerializerConfigSnapshot(\n\t\t\t\t\tStateMetaInfoSnapshot.CommonSerializerKeys.VALUE_SERIALIZER.toString()));\n\n\t\t\tRegisteredKeyValueStateBackendMetaInfo.checkStateMetaInfo(restoredMetaInfoSnapshot, stateDesc);\n\n\t\t\tTypeSerializerSchemaCompatibility<V> stateCompatibility =\n\t\t\t\tstateSerializerSnapshot.resolveSchemaCompatibility(newStateSerializer);\n\n\t\t\tif (stateCompatibility.isIncompatible()) {\n\t\t\t\tthrow new StateMigrationException(\"For heap backends, the new state serializer must not be incompatible.\");\n\t\t\t}\n\n\t\t\tstateTable.setMetaInfo(newMetaInfo);\n\t\t} else {\n\t\t\tstateTable = snapshotStrategy.newStateTable(newMetaInfo);\n\t\t\tregisteredKVStates.put(stateDesc.getName(), stateTable);\n\t\t}\n\n\t\treturn stateTable;\n\t}",
            " 229  \n 230  \n 231  \n 232 +\n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 +\n 241  \n 242 +\n 243  \n 244  \n 245 +\n 246  \n 247  \n 248  \n 249  \n 250 +\n 251  \n 252  \n 253 +\n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260  \n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "\tprivate <N, V> StateTable<K, N, V> tryRegisterStateTable(\n\t\t\tTypeSerializer<N> namespaceSerializer,\n\t\t\tStateDescriptor<?, V> stateDesc,\n\t\t\t@Nullable StateSnapshotTransformer<V> snapshotTransformer) throws StateMigrationException {\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tStateTable<K, N, V> stateTable = (StateTable<K, N, V>) registeredKVStates.get(stateDesc.getName());\n\n\t\tTypeSerializer<V> newStateSerializer = stateDesc.getSerializer();\n\n\t\tif (stateTable != null) {\n\t\t\tRegisteredKeyValueStateBackendMetaInfo<N, V> restoredKvMetaInfo = stateTable.getMetaInfo();\n\n\t\t\trestoredKvMetaInfo.updateSnapshotTransformer(snapshotTransformer);\n\n\t\t\tTypeSerializerSchemaCompatibility<N> namespaceCompatibility =\n\t\t\t\trestoredKvMetaInfo.updateNamespaceSerializer(namespaceSerializer);\n\t\t\tif (!namespaceCompatibility.isCompatibleAsIs()) {\n\t\t\t\tthrow new StateMigrationException(\"For heap backends, the new namespace serializer must be compatible.\");\n\t\t\t}\n\n\t\t\trestoredKvMetaInfo.checkStateMetaInfo(stateDesc);\n\n\t\t\tTypeSerializerSchemaCompatibility<V> stateCompatibility =\n\t\t\t\trestoredKvMetaInfo.updateStateSerializer(newStateSerializer);\n\n\t\t\tif (stateCompatibility.isIncompatible()) {\n\t\t\t\tthrow new StateMigrationException(\"For heap backends, the new state serializer must not be incompatible.\");\n\t\t\t}\n\n\t\t\tstateTable.setMetaInfo(restoredKvMetaInfo);\n\t\t} else {\n\t\t\tRegisteredKeyValueStateBackendMetaInfo<N, V> newMetaInfo = new RegisteredKeyValueStateBackendMetaInfo<>(\n\t\t\t\tstateDesc.getType(),\n\t\t\t\tstateDesc.getName(),\n\t\t\t\tnamespaceSerializer,\n\t\t\t\tnewStateSerializer,\n\t\t\t\tsnapshotTransformer);\n\n\t\t\tstateTable = snapshotStrategy.newStateTable(newMetaInfo);\n\t\t\tregisteredKVStates.put(stateDesc.getName(), stateTable);\n\t\t}\n\n\t\treturn stateTable;\n\t}"
        ],
        [
            "DefaultOperatorStateBackend::getListState(ListStateDescriptor,OperatorStateHandle)",
            " 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593 -\n 594 -\n 595 -\n 596  \n 597 -\n 598  \n 599  \n 600 -\n 601 -\n 602 -\n 603 -\n 604  \n 605 -\n 606  \n 607  \n 608  \n 609  \n 610 -\n 611 -\n 612  \n 613  \n 614  \n 615  \n 616  ",
            "\tprivate <S> ListState<S> getListState(\n\t\t\tListStateDescriptor<S> stateDescriptor,\n\t\t\tOperatorStateHandle.Mode mode) throws StateMigrationException {\n\n\t\tPreconditions.checkNotNull(stateDescriptor);\n\t\tString name = Preconditions.checkNotNull(stateDescriptor.getName());\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tPartitionableListState<S> previous = (PartitionableListState<S>) accessedStatesByName.get(name);\n\t\tif (previous != null) {\n\t\t\tcheckStateNameAndMode(\n\t\t\t\t\tprevious.getStateMetaInfo().getName(),\n\t\t\t\t\tname,\n\t\t\t\t\tprevious.getStateMetaInfo().getAssignmentMode(),\n\t\t\t\t\tmode);\n\t\t\treturn previous;\n\t\t}\n\n\t\t// end up here if its the first time access after execution for the\n\t\t// provided state name; check compatibility of restored state, if any\n\t\t// TODO with eager registration in place, these checks should be moved to restore()\n\n\t\tstateDescriptor.initializeSerializerUnlessSet(getExecutionConfig());\n\t\tTypeSerializer<S> partitionStateSerializer = Preconditions.checkNotNull(stateDescriptor.getElementSerializer());\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tPartitionableListState<S> partitionableListState = (PartitionableListState<S>) registeredOperatorStates.get(name);\n\n\t\tif (null == partitionableListState) {\n\t\t\t// no restored state for the state name; simply create new state holder\n\n\t\t\tpartitionableListState = new PartitionableListState<>(\n\t\t\t\tnew RegisteredOperatorStateBackendMetaInfo<>(\n\t\t\t\t\tname,\n\t\t\t\t\tpartitionStateSerializer,\n\t\t\t\t\tmode));\n\n\t\t\tregisteredOperatorStates.put(name, partitionableListState);\n\t\t} else {\n\t\t\t// has restored state; check compatibility of new state access\n\n\t\t\tcheckStateNameAndMode(\n\t\t\t\t\tpartitionableListState.getStateMetaInfo().getName(),\n\t\t\t\t\tname,\n\t\t\t\t\tpartitionableListState.getStateMetaInfo().getAssignmentMode(),\n\t\t\t\t\tmode);\n\n\t\t\tStateMetaInfoSnapshot restoredSnapshot = restoredOperatorStateMetaInfos.get(name);\n\t\t\tRegisteredOperatorStateBackendMetaInfo<S> metaInfo =\n\t\t\t\tnew RegisteredOperatorStateBackendMetaInfo<>(restoredSnapshot);\n\n\t\t\t// check compatibility to determine if state migration is required\n\t\t\tTypeSerializer<S> newPartitionStateSerializer = partitionStateSerializer.duplicate();\n\n\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\tTypeSerializerSnapshot<S> stateSerializerSnapshot = Preconditions.checkNotNull(\n\t\t\t\t(TypeSerializerSnapshot<S>) restoredSnapshot.getTypeSerializerConfigSnapshot(StateMetaInfoSnapshot.CommonSerializerKeys.VALUE_SERIALIZER));\n\n\t\t\tTypeSerializerSchemaCompatibility<S> stateCompatibility =\n\t\t\t\tstateSerializerSnapshot.resolveSchemaCompatibility(newPartitionStateSerializer);\n\t\t\tif (stateCompatibility.isIncompatible()) {\n\t\t\t\tthrow new StateMigrationException(\"The new state serializer for operator state must not be incompatible.\");\n\t\t\t}\n\n\t\t\tpartitionableListState.setStateMetaInfo(\n\t\t\t\tnew RegisteredOperatorStateBackendMetaInfo<>(name, newPartitionStateSerializer, mode));\n\t\t}\n\n\t\taccessedStatesByName.put(name, partitionableListState);\n\t\treturn partitionableListState;\n\t}",
            " 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561 +\n 562 +\n 563  \n 564 +\n 565  \n 566  \n 567  \n 568 +\n 569  \n 570  \n 571  \n 572  \n 573 +\n 574  \n 575  \n 576  \n 577  \n 578  ",
            "\tprivate <S> ListState<S> getListState(\n\t\t\tListStateDescriptor<S> stateDescriptor,\n\t\t\tOperatorStateHandle.Mode mode) throws StateMigrationException {\n\n\t\tPreconditions.checkNotNull(stateDescriptor);\n\t\tString name = Preconditions.checkNotNull(stateDescriptor.getName());\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tPartitionableListState<S> previous = (PartitionableListState<S>) accessedStatesByName.get(name);\n\t\tif (previous != null) {\n\t\t\tcheckStateNameAndMode(\n\t\t\t\t\tprevious.getStateMetaInfo().getName(),\n\t\t\t\t\tname,\n\t\t\t\t\tprevious.getStateMetaInfo().getAssignmentMode(),\n\t\t\t\t\tmode);\n\t\t\treturn previous;\n\t\t}\n\n\t\t// end up here if its the first time access after execution for the\n\t\t// provided state name; check compatibility of restored state, if any\n\t\t// TODO with eager registration in place, these checks should be moved to restore()\n\n\t\tstateDescriptor.initializeSerializerUnlessSet(getExecutionConfig());\n\t\tTypeSerializer<S> partitionStateSerializer = Preconditions.checkNotNull(stateDescriptor.getElementSerializer());\n\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tPartitionableListState<S> partitionableListState = (PartitionableListState<S>) registeredOperatorStates.get(name);\n\n\t\tif (null == partitionableListState) {\n\t\t\t// no restored state for the state name; simply create new state holder\n\n\t\t\tpartitionableListState = new PartitionableListState<>(\n\t\t\t\tnew RegisteredOperatorStateBackendMetaInfo<>(\n\t\t\t\t\tname,\n\t\t\t\t\tpartitionStateSerializer,\n\t\t\t\t\tmode));\n\n\t\t\tregisteredOperatorStates.put(name, partitionableListState);\n\t\t} else {\n\t\t\t// has restored state; check compatibility of new state access\n\n\t\t\tcheckStateNameAndMode(\n\t\t\t\t\tpartitionableListState.getStateMetaInfo().getName(),\n\t\t\t\t\tname,\n\t\t\t\t\tpartitionableListState.getStateMetaInfo().getAssignmentMode(),\n\t\t\t\t\tmode);\n\n\t\t\tRegisteredOperatorStateBackendMetaInfo<S> restoredPartitionableListStateMetaInfo =\n\t\t\t\tpartitionableListState.getStateMetaInfo();\n\n\t\t\t// check compatibility to determine if new serializers are incompatible\n\t\t\tTypeSerializer<S> newPartitionStateSerializer = partitionStateSerializer.duplicate();\n\n\t\t\tTypeSerializerSchemaCompatibility<S> stateCompatibility =\n\t\t\t\trestoredPartitionableListStateMetaInfo.updatePartitionStateSerializer(newPartitionStateSerializer);\n\t\t\tif (stateCompatibility.isIncompatible()) {\n\t\t\t\tthrow new StateMigrationException(\"The new state serializer for operator state must not be incompatible.\");\n\t\t\t}\n\n\t\t\tpartitionableListState.setStateMetaInfo(restoredPartitionableListStateMetaInfo);\n\t\t}\n\n\t\taccessedStatesByName.put(name, partitionableListState);\n\t\treturn partitionableListState;\n\t}"
        ],
        [
            "HeapKeyedStateBackend::createOrCheckStateForMetaInfo(List,Map)",
            " 534  \n 535  \n 536  \n 537  \n 538  \n 539 -\n 540 -\n 541 -\n 542 -\n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  ",
            "\tprivate void createOrCheckStateForMetaInfo(\n\t\tList<StateMetaInfoSnapshot> restoredMetaInfo,\n\t\tMap<Integer, StateMetaInfoSnapshot> kvStatesById) {\n\n\t\tfor (StateMetaInfoSnapshot metaInfoSnapshot : restoredMetaInfo) {\n\t\t\trestoredStateMetaInfo.put(\n\t\t\t\tStateUID.of(metaInfoSnapshot.getName(), metaInfoSnapshot.getBackendStateType()),\n\t\t\t\tmetaInfoSnapshot);\n\n\t\t\tfinal StateSnapshotRestore registeredState;\n\n\t\t\tswitch (metaInfoSnapshot.getBackendStateType()) {\n\t\t\t\tcase KEY_VALUE:\n\t\t\t\t\tregisteredState = registeredKVStates.get(metaInfoSnapshot.getName());\n\t\t\t\t\tif (registeredState == null) {\n\t\t\t\t\t\tRegisteredKeyValueStateBackendMetaInfo<?, ?> registeredKeyedBackendStateMetaInfo =\n\t\t\t\t\t\t\tnew RegisteredKeyValueStateBackendMetaInfo<>(metaInfoSnapshot);\n\t\t\t\t\t\tregisteredKVStates.put(\n\t\t\t\t\t\t\tmetaInfoSnapshot.getName(),\n\t\t\t\t\t\t\tsnapshotStrategy.newStateTable(registeredKeyedBackendStateMetaInfo));\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tcase PRIORITY_QUEUE:\n\t\t\t\t\tregisteredState = registeredPQStates.get(metaInfoSnapshot.getName());\n\t\t\t\t\tif (registeredState == null) {\n\t\t\t\t\t\tcreateInternal(new RegisteredPriorityQueueStateBackendMetaInfo<>(metaInfoSnapshot));\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tthrow new IllegalStateException(\"Unexpected state type: \" +\n\t\t\t\t\t\tmetaInfoSnapshot.getBackendStateType() + \".\");\n\t\t\t}\n\n\t\t\tif (registeredState == null) {\n\t\t\t\tkvStatesById.put(kvStatesById.size(), metaInfoSnapshot);\n\t\t\t}\n\t\t}\n\t}",
            " 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  ",
            "\tprivate void createOrCheckStateForMetaInfo(\n\t\tList<StateMetaInfoSnapshot> restoredMetaInfo,\n\t\tMap<Integer, StateMetaInfoSnapshot> kvStatesById) {\n\n\t\tfor (StateMetaInfoSnapshot metaInfoSnapshot : restoredMetaInfo) {\n\t\t\tfinal StateSnapshotRestore registeredState;\n\n\t\t\tswitch (metaInfoSnapshot.getBackendStateType()) {\n\t\t\t\tcase KEY_VALUE:\n\t\t\t\t\tregisteredState = registeredKVStates.get(metaInfoSnapshot.getName());\n\t\t\t\t\tif (registeredState == null) {\n\t\t\t\t\t\tRegisteredKeyValueStateBackendMetaInfo<?, ?> registeredKeyedBackendStateMetaInfo =\n\t\t\t\t\t\t\tnew RegisteredKeyValueStateBackendMetaInfo<>(metaInfoSnapshot);\n\t\t\t\t\t\tregisteredKVStates.put(\n\t\t\t\t\t\t\tmetaInfoSnapshot.getName(),\n\t\t\t\t\t\t\tsnapshotStrategy.newStateTable(registeredKeyedBackendStateMetaInfo));\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tcase PRIORITY_QUEUE:\n\t\t\t\t\tregisteredState = registeredPQStates.get(metaInfoSnapshot.getName());\n\t\t\t\t\tif (registeredState == null) {\n\t\t\t\t\t\tcreateInternal(new RegisteredPriorityQueueStateBackendMetaInfo<>(metaInfoSnapshot));\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tthrow new IllegalStateException(\"Unexpected state type: \" +\n\t\t\t\t\t\tmetaInfoSnapshot.getBackendStateType() + \".\");\n\t\t\t}\n\n\t\t\tif (registeredState == null) {\n\t\t\t\tkvStatesById.put(kvStatesById.size(), metaInfoSnapshot);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBIncrementalRestoreOperation::createAndRegisterColumnFamilyDescriptors(List)",
            "1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161  \n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169 -\n1170  \n1171  \n1172  ",
            "\t\t/**\n\t\t * This method recreates and registers all {@link ColumnFamilyDescriptor} from Flink's state meta data snapshot.\n\t\t */\n\t\tprivate List<ColumnFamilyDescriptor> createAndRegisterColumnFamilyDescriptors(\n\t\t\tList<StateMetaInfoSnapshot> stateMetaInfoSnapshots) {\n\n\t\t\tList<ColumnFamilyDescriptor> columnFamilyDescriptors =\n\t\t\t\tnew ArrayList<>(stateMetaInfoSnapshots.size());\n\n\t\t\tfor (StateMetaInfoSnapshot stateMetaInfoSnapshot : stateMetaInfoSnapshots) {\n\n\t\t\t\tColumnFamilyDescriptor columnFamilyDescriptor = new ColumnFamilyDescriptor(\n\t\t\t\t\tstateMetaInfoSnapshot.getName().getBytes(ConfigConstants.DEFAULT_CHARSET),\n\t\t\t\t\tstateBackend.columnOptions);\n\n\t\t\t\tcolumnFamilyDescriptors.add(columnFamilyDescriptor);\n\t\t\t\tstateBackend.restoredKvStateMetaInfos.put(stateMetaInfoSnapshot.getName(), stateMetaInfoSnapshot);\n\t\t\t}\n\t\t\treturn columnFamilyDescriptors;\n\t\t}",
            "1139  \n1140  \n1141  \n1142  \n1143  \n1144  \n1145  \n1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  ",
            "\t\t/**\n\t\t * This method recreates and registers all {@link ColumnFamilyDescriptor} from Flink's state meta data snapshot.\n\t\t */\n\t\tprivate List<ColumnFamilyDescriptor> createAndRegisterColumnFamilyDescriptors(\n\t\t\tList<StateMetaInfoSnapshot> stateMetaInfoSnapshots) {\n\n\t\t\tList<ColumnFamilyDescriptor> columnFamilyDescriptors =\n\t\t\t\tnew ArrayList<>(stateMetaInfoSnapshots.size());\n\n\t\t\tfor (StateMetaInfoSnapshot stateMetaInfoSnapshot : stateMetaInfoSnapshots) {\n\n\t\t\t\tColumnFamilyDescriptor columnFamilyDescriptor = new ColumnFamilyDescriptor(\n\t\t\t\t\tstateMetaInfoSnapshot.getName().getBytes(ConfigConstants.DEFAULT_CHARSET),\n\t\t\t\t\tstateBackend.columnOptions);\n\n\t\t\t\tcolumnFamilyDescriptors.add(columnFamilyDescriptor);\n\t\t\t}\n\t\t\treturn columnFamilyDescriptors;\n\t\t}"
        ],
        [
            "RocksDBKeyedStateBackend::RocksDBFullRestoreOperation::restoreKVStateMetaData()",
            " 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756 -\n 757 -\n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  ",
            "\t\t/**\n\t\t * Restore the KV-state / ColumnFamily meta data for all key-groups referenced by the current state handle.\n\t\t */\n\t\tprivate void restoreKVStateMetaData() throws IOException, StateMigrationException, RocksDBException {\n\n\t\t\t// isSerializerPresenceRequired flag is set to false, since for the RocksDB state backend,\n\t\t\t// deserialization of state happens lazily during runtime; we depend on the fact\n\t\t\t// that the new serializer for states could be compatible, and therefore the restore can continue\n\t\t\t// without old serializers required to be present.\n\t\t\tKeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy<>(rocksDBKeyedStateBackend.userCodeClassLoader);\n\n\t\t\tserializationProxy.read(currentStateHandleInView);\n\n\t\t\t// check for key serializer compatibility; this also reconfigures the\n\t\t\t// key serializer to be compatible, if it is required and is possible\n\t\t\tif (!serializationProxy.getKeySerializerConfigSnapshot()\n\t\t\t\t\t.resolveSchemaCompatibility(rocksDBKeyedStateBackend.keySerializer).isCompatibleAsIs()) {\n\t\t\t\tthrow new StateMigrationException(\"The new key serializer must be compatible.\");\n\t\t\t}\n\n\t\t\tthis.keygroupStreamCompressionDecorator = serializationProxy.isUsingKeyGroupCompression() ?\n\t\t\t\tSnappyStreamCompressionDecorator.INSTANCE : UncompressedStreamCompressionDecorator.INSTANCE;\n\n\t\t\tList<StateMetaInfoSnapshot> restoredMetaInfos =\n\t\t\t\tserializationProxy.getStateMetaInfoSnapshots();\n\t\t\tcurrentStateHandleKVStateColumnFamilies = new ArrayList<>(restoredMetaInfos.size());\n\n\t\t\tfor (StateMetaInfoSnapshot restoredMetaInfo : restoredMetaInfos) {\n\n\t\t\t\tTuple2<ColumnFamilyHandle, RegisteredStateMetaInfoBase> registeredColumn =\n\t\t\t\t\trocksDBKeyedStateBackend.kvStateInformation.get(restoredMetaInfo.getName());\n\n\t\t\t\tif (registeredColumn == null) {\n\t\t\t\t\tbyte[] nameBytes = restoredMetaInfo.getName().getBytes(ConfigConstants.DEFAULT_CHARSET);\n\n\t\t\t\t\tColumnFamilyDescriptor columnFamilyDescriptor = new ColumnFamilyDescriptor(\n\t\t\t\t\t\tnameBytes,\n\t\t\t\t\t\trocksDBKeyedStateBackend.columnOptions);\n\n\t\t\t\t\trocksDBKeyedStateBackend.restoredKvStateMetaInfos.put(restoredMetaInfo.getName(), restoredMetaInfo);\n\n\t\t\t\t\tColumnFamilyHandle columnFamily = rocksDBKeyedStateBackend.db.createColumnFamily(columnFamilyDescriptor);\n\n\t\t\t\t\t// create a meta info for the state on restore;\n\t\t\t\t\t// this allows us to retain the state in future snapshots even if it wasn't accessed\n\t\t\t\t\tRegisteredStateMetaInfoBase stateMetaInfo =\n\t\t\t\t\t\tRegisteredStateMetaInfoBase.fromMetaInfoSnapshot(restoredMetaInfo);\n\t\t\t\t\tregisteredColumn = new Tuple2<>(columnFamily, stateMetaInfo);\n\t\t\t\t\trocksDBKeyedStateBackend.kvStateInformation.put(restoredMetaInfo.getName(), registeredColumn);\n\n\t\t\t\t} else {\n\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t}\n\t\t\t\tcurrentStateHandleKVStateColumnFamilies.add(registeredColumn.f0);\n\t\t\t}\n\t\t}",
            " 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  ",
            "\t\t/**\n\t\t * Restore the KV-state / ColumnFamily meta data for all key-groups referenced by the current state handle.\n\t\t */\n\t\tprivate void restoreKVStateMetaData() throws IOException, StateMigrationException, RocksDBException {\n\n\t\t\t// isSerializerPresenceRequired flag is set to false, since for the RocksDB state backend,\n\t\t\t// deserialization of state happens lazily during runtime; we depend on the fact\n\t\t\t// that the new serializer for states could be compatible, and therefore the restore can continue\n\t\t\t// without old serializers required to be present.\n\t\t\tKeyedBackendSerializationProxy<K> serializationProxy =\n\t\t\t\tnew KeyedBackendSerializationProxy<>(rocksDBKeyedStateBackend.userCodeClassLoader);\n\n\t\t\tserializationProxy.read(currentStateHandleInView);\n\n\t\t\t// check for key serializer compatibility; this also reconfigures the\n\t\t\t// key serializer to be compatible, if it is required and is possible\n\t\t\tif (!serializationProxy.getKeySerializerConfigSnapshot()\n\t\t\t\t\t.resolveSchemaCompatibility(rocksDBKeyedStateBackend.keySerializer).isCompatibleAsIs()) {\n\t\t\t\tthrow new StateMigrationException(\"The new key serializer must be compatible.\");\n\t\t\t}\n\n\t\t\tthis.keygroupStreamCompressionDecorator = serializationProxy.isUsingKeyGroupCompression() ?\n\t\t\t\tSnappyStreamCompressionDecorator.INSTANCE : UncompressedStreamCompressionDecorator.INSTANCE;\n\n\t\t\tList<StateMetaInfoSnapshot> restoredMetaInfos =\n\t\t\t\tserializationProxy.getStateMetaInfoSnapshots();\n\t\t\tcurrentStateHandleKVStateColumnFamilies = new ArrayList<>(restoredMetaInfos.size());\n\n\t\t\tfor (StateMetaInfoSnapshot restoredMetaInfo : restoredMetaInfos) {\n\n\t\t\t\tTuple2<ColumnFamilyHandle, RegisteredStateMetaInfoBase> registeredColumn =\n\t\t\t\t\trocksDBKeyedStateBackend.kvStateInformation.get(restoredMetaInfo.getName());\n\n\t\t\t\tif (registeredColumn == null) {\n\t\t\t\t\tbyte[] nameBytes = restoredMetaInfo.getName().getBytes(ConfigConstants.DEFAULT_CHARSET);\n\n\t\t\t\t\tColumnFamilyDescriptor columnFamilyDescriptor = new ColumnFamilyDescriptor(\n\t\t\t\t\t\tnameBytes,\n\t\t\t\t\t\trocksDBKeyedStateBackend.columnOptions);\n\n\t\t\t\t\tColumnFamilyHandle columnFamily = rocksDBKeyedStateBackend.db.createColumnFamily(columnFamilyDescriptor);\n\n\t\t\t\t\t// create a meta info for the state on restore;\n\t\t\t\t\t// this allows us to retain the state in future snapshots even if it wasn't accessed\n\t\t\t\t\tRegisteredStateMetaInfoBase stateMetaInfo =\n\t\t\t\t\t\tRegisteredStateMetaInfoBase.fromMetaInfoSnapshot(restoredMetaInfo);\n\t\t\t\t\tregisteredColumn = new Tuple2<>(columnFamily, stateMetaInfo);\n\t\t\t\t\trocksDBKeyedStateBackend.kvStateInformation.put(restoredMetaInfo.getName(), registeredColumn);\n\n\t\t\t\t} else {\n\t\t\t\t\t// TODO with eager state registration in place, check here for serializer migration strategies\n\t\t\t\t}\n\t\t\t\tcurrentStateHandleKVStateColumnFamilies.add(registeredColumn.f0);\n\t\t\t}\n\t\t}"
        ]
    ],
    "8868ff5b057decec8257b972d2d4c3e13dd2a438": [
        [
            "SlotProfile::LocalityAwareRequirementsToSlotMatcher::findMatchWithLocality(Stream,Function,Predicate,BiFunction)",
            " 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206 -\n 207 -\n 208  \n 209  \n 210 -\n 211 -\n 212  \n 213  \n 214  \n 215  \n 216 -\n 217 -\n 218  \n 219  \n 220 -\n 221  \n 222 -\n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229 -\n 230 -\n 231 -\n 232 -\n 233 -\n 234  \n 235 -\n 236 -\n 237 -\n 238 -\n 239 -\n 240 -\n 241 -\n 242 -\n 243 -\n 244 -\n 245 -\n 246 -\n 247 -\n 248  \n 249 -\n 250 -\n 251 -\n 252 -\n 253 -\n 254 -\n 255  \n 256  \n 257  \n 258  \n 259  \n 260 -\n 261 -\n 262 -\n 263 -\n 264  \n 265  \n 266  \n 267  ",
            "\t\t@Override\n\t\tpublic <IN, OUT> OUT findMatchWithLocality(\n\t\t\t@Nonnull Stream<IN> candidates,\n\t\t\t@Nonnull Function<IN, SlotContext> contextExtractor,\n\t\t\t@Nonnull Predicate<IN> additionalRequirementsFilter,\n\t\t\t@Nonnull BiFunction<IN, Locality, OUT> resultProducer) {\n\n\t\t\t// if we have no location preferences, we can only filter by the additional requirements.\n\t\t\tif (locationPreferences.isEmpty()) {\n\t\t\t\treturn candidates\n\t\t\t\t\t.filter(additionalRequirementsFilter)\n\t\t\t\t\t.findFirst()\n\t\t\t\t\t.map((result) -> resultProducer.apply(result, Locality.UNCONSTRAINED))\n\t\t\t\t\t.orElse(null);\n\t\t\t}\n\n\t\t\t// we build up two indexes, one for resource id and one for host names of the preferred locations.\n\t\t\tHashSet<ResourceID> preferredResourceIDs = new HashSet<>(locationPreferences.size());\n\t\t\tHashSet<String> preferredFQHostNames = new HashSet<>(locationPreferences.size());\n\n\t\t\tfor (TaskManagerLocation locationPreference : locationPreferences) {\n\t\t\t\tpreferredResourceIDs.add(locationPreference.getResourceID());\n\t\t\t\tpreferredFQHostNames.add(locationPreference.getFQDNHostname());\n\t\t\t}\n\n\t\t\tIterator<IN> iterator = candidates.iterator();\n\n\t\t\tIN matchByHostName = null;\n\t\t\tIN matchByAdditionalRequirements = null;\n\n\t\t\twhile (iterator.hasNext()) {\n\n\t\t\t\tIN candidate = iterator.next();\n\t\t\t\tSlotContext slotContext = contextExtractor.apply(candidate);\n\n\t\t\t\t// this if checks if the candidate has is a local slot\n\t\t\t\tif (preferredResourceIDs.contains(slotContext.getTaskManagerLocation().getResourceID())) {\n\t\t\t\t\tif (additionalRequirementsFilter.test(candidate)) {\n\t\t\t\t\t\t// we can stop, because we found a match with best possible locality.\n\t\t\t\t\t\treturn resultProducer.apply(candidate, Locality.LOCAL);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// next candidate because this failed on the additional requirements.\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\t// this if checks if the candidate is at least host-local, if we did not find another host-local\n\t\t\t\t// candidate before.\n\t\t\t\tif (matchByHostName == null) {\n\t\t\t\t\tif (preferredFQHostNames.contains(slotContext.getTaskManagerLocation().getFQDNHostname())) {\n\t\t\t\t\t\tif (additionalRequirementsFilter.test(candidate)) {\n\t\t\t\t\t\t\t// We remember the candidate, but still continue because there might still be a candidate\n\t\t\t\t\t\t\t// that is local to the desired task manager.\n\t\t\t\t\t\t\tmatchByHostName = candidate;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\t// next candidate because this failed on the additional requirements.\n\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// this if checks if the candidate at least fulfils the resource requirements, and is only required\n\t\t\t\t\t// if we did not yet find a valid candidate with better locality.\n\t\t\t\t\tif (matchByAdditionalRequirements == null\n\t\t\t\t\t\t&& additionalRequirementsFilter.test(candidate)) {\n\t\t\t\t\t\t// Again, we remember but continue in hope for a candidate with better locality.\n\t\t\t\t\t\tmatchByAdditionalRequirements = candidate;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// at the end of the iteration, we return the candidate with best possible locality or null.\n\t\t\tif (matchByHostName != null) {\n\t\t\t\treturn resultProducer.apply(matchByHostName, Locality.HOST_LOCAL);\n\t\t\t} else if (matchByAdditionalRequirements != null) {\n\t\t\t\treturn resultProducer.apply(matchByAdditionalRequirements, Locality.NON_LOCAL);\n\t\t\t} else {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}",
            " 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214 +\n 215 +\n 216  \n 217  \n 218 +\n 219 +\n 220  \n 221  \n 222  \n 223  \n 224 +\n 225 +\n 226 +\n 227  \n 228  \n 229  \n 230 +\n 231 +\n 232  \n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238  \n 239 +\n 240 +\n 241 +\n 242 +\n 243 +\n 244  \n 245  \n 246  \n 247  \n 248  \n 249 +\n 250 +\n 251  \n 252  \n 253  \n 254  ",
            "\t\t@Override\n\t\tpublic <IN, OUT> OUT findMatchWithLocality(\n\t\t\t@Nonnull Stream<IN> candidates,\n\t\t\t@Nonnull Function<IN, SlotContext> contextExtractor,\n\t\t\t@Nonnull Predicate<IN> additionalRequirementsFilter,\n\t\t\t@Nonnull BiFunction<IN, Locality, OUT> resultProducer) {\n\n\t\t\t// if we have no location preferences, we can only filter by the additional requirements.\n\t\t\tif (locationPreferences.isEmpty()) {\n\t\t\t\treturn candidates\n\t\t\t\t\t.filter(additionalRequirementsFilter)\n\t\t\t\t\t.findFirst()\n\t\t\t\t\t.map((result) -> resultProducer.apply(result, Locality.UNCONSTRAINED))\n\t\t\t\t\t.orElse(null);\n\t\t\t}\n\n\t\t\t// we build up two indexes, one for resource id and one for host names of the preferred locations.\n\t\t\tfinal Map<ResourceID, Integer> preferredResourceIDs = new HashMap<>(locationPreferences.size());\n\t\t\tfinal Map<String, Integer> preferredFQHostNames = new HashMap<>(locationPreferences.size());\n\n\t\t\tfor (TaskManagerLocation locationPreference : locationPreferences) {\n\t\t\t\tpreferredResourceIDs.merge(locationPreference.getResourceID(), 1, Integer::sum);\n\t\t\t\tpreferredFQHostNames.merge(locationPreference.getFQDNHostname(), 1, Integer::sum);\n\t\t\t}\n\n\t\t\tIterator<IN> iterator = candidates.iterator();\n\n\t\t\tIN bestCandidate = null;\n\t\t\tint bestCandidateScore = Integer.MIN_VALUE;\n\t\t\tLocality bestCandidateLocality = null;\n\n\t\t\twhile (iterator.hasNext()) {\n\t\t\t\tIN candidate = iterator.next();\n\t\t\t\tif (additionalRequirementsFilter.test(candidate)) {\n\t\t\t\t\tSlotContext slotContext = contextExtractor.apply(candidate);\n\n\t\t\t\t\t// this gets candidate is local-weigh\n\t\t\t\t\tInteger localWeigh = preferredResourceIDs.getOrDefault(slotContext.getTaskManagerLocation().getResourceID(), 0);\n\n\t\t\t\t\t// this gets candidate is host-local-weigh\n\t\t\t\t\tInteger hostLocalWeigh = preferredFQHostNames.getOrDefault(slotContext.getTaskManagerLocation().getFQDNHostname(), 0);\n\n\t\t\t\t\tint candidateScore = LOCALITY_EVALUATION_FUNCTION.apply(localWeigh, hostLocalWeigh);\n\t\t\t\t\tif (candidateScore > bestCandidateScore) {\n\t\t\t\t\t\tbestCandidateScore = candidateScore;\n\t\t\t\t\t\tbestCandidate = candidate;\n\t\t\t\t\t\tbestCandidateLocality = localWeigh > 0 ? Locality.LOCAL : hostLocalWeigh > 0 ? Locality.HOST_LOCAL : Locality.NON_LOCAL;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// at the end of the iteration, we return the candidate with best possible locality or null.\n\t\t\tif (bestCandidate != null) {\n\t\t\t\treturn resultProducer.apply(bestCandidate, bestCandidateLocality);\n\t\t\t} else {\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}"
        ],
        [
            "SlotProfileTest::matchPreferredLocation()",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  ",
            "\t@Test\n\tpublic void matchPreferredLocation() {\n\n\t\tSlotProfile slotProfile = new SlotProfile(resourceProfile, Collections.singletonList(tml2), Collections.emptyList());\n\t\tSlotContext match = runMatching(slotProfile);\n\n\t\tAssert.assertEquals(ssc2, match);\n\n\t\tslotProfile = new SlotProfile(resourceProfile, Arrays.asList(tmlX, tml4), Collections.emptyList());\n\t\tmatch = runMatching(slotProfile);\n\n\t\tAssert.assertEquals(ssc4, match);\n\t}",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101 +\n 102 +\n 103 +\n 104 +\n 105  ",
            "\t@Test\n\tpublic void matchPreferredLocation() {\n\n\t\tSlotProfile slotProfile = new SlotProfile(resourceProfile, Collections.singletonList(tml2), Collections.emptyList());\n\t\tSlotContext match = runMatching(slotProfile);\n\n\t\tAssert.assertEquals(ssc2, match);\n\n\t\tslotProfile = new SlotProfile(resourceProfile, Arrays.asList(tmlX, tml4), Collections.emptyList());\n\t\tmatch = runMatching(slotProfile);\n\n\t\tAssert.assertEquals(ssc4, match);\n\n\t\tslotProfile = new SlotProfile(resourceProfile, Arrays.asList(tml3, tml1, tml3, tmlX), Collections.emptyList());\n\t\tmatch = runMatching(slotProfile);\n\n\t\tAssert.assertEquals(ssc3, match);\n\t}"
        ]
    ],
    "be419e2560ef89683b7795c75eb08ae2337fefee": [
        [
            "FlinkKafkaProducer::resumeTransaction(long,short)",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 -\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "\t/**\n\t * Instead of obtaining producerId and epoch from the transaction coordinator, re-use previously obtained ones,\n\t * so that we can resume transaction after a restart. Implementation of this method is based on\n\t * {@link org.apache.kafka.clients.producer.KafkaProducer#initTransactions}.\n\t */\n\tpublic void resumeTransaction(long producerId, short epoch) {\n\t\tPreconditions.checkState(producerId >= 0 && epoch >= 0, \"Incorrect values for producerId {} and epoch {}\", producerId, epoch);\n\t\tLOG.info(\"Attempting to resume transaction {} with producerId {} and epoch {}\", transactionalId, producerId, epoch);\n\n\t\tObject transactionManager = getValue(kafkaProducer, \"transactionManager\");\n\t\tsynchronized (transactionManager) {\n\t\t\tObject sequenceNumbers = getValue(transactionManager, \"sequenceNumbers\");\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.INITIALIZING\"));\n\t\t\tinvoke(sequenceNumbers, \"clear\");\n\n\t\t\tObject producerIdAndEpoch = getValue(transactionManager, \"producerIdAndEpoch\");\n\t\t\tsetValue(producerIdAndEpoch, \"producerId\", producerId);\n\t\t\tsetValue(producerIdAndEpoch, \"epoch\", epoch);\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.READY\"));\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.IN_TRANSACTION\"));\n\t\t\tsetValue(transactionManager, \"transactionStarted\", true);\n\t\t}\n\t}",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 +\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  ",
            "\t/**\n\t * Instead of obtaining producerId and epoch from the transaction coordinator, re-use previously obtained ones,\n\t * so that we can resume transaction after a restart. Implementation of this method is based on\n\t * {@link org.apache.kafka.clients.producer.KafkaProducer#initTransactions}.\n\t */\n\tpublic void resumeTransaction(long producerId, short epoch) {\n\t\tPreconditions.checkState(producerId >= 0 && epoch >= 0, \"Incorrect values for producerId %s and epoch %s\", producerId, epoch);\n\t\tLOG.info(\"Attempting to resume transaction {} with producerId {} and epoch {}\", transactionalId, producerId, epoch);\n\n\t\tObject transactionManager = getValue(kafkaProducer, \"transactionManager\");\n\t\tsynchronized (transactionManager) {\n\t\t\tObject sequenceNumbers = getValue(transactionManager, \"sequenceNumbers\");\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.INITIALIZING\"));\n\t\t\tinvoke(sequenceNumbers, \"clear\");\n\n\t\t\tObject producerIdAndEpoch = getValue(transactionManager, \"producerIdAndEpoch\");\n\t\t\tsetValue(producerIdAndEpoch, \"producerId\", producerId);\n\t\t\tsetValue(producerIdAndEpoch, \"epoch\", epoch);\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.READY\"));\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.IN_TRANSACTION\"));\n\t\t\tsetValue(transactionManager, \"transactionStarted\", true);\n\t\t}\n\t}"
        ],
        [
            "ExponentialWaitStrategy::sleepTime(long)",
            "  40  \n  41  \n  42 -\n  43  \n  44  \n  45  ",
            "\t@Override\n\tpublic long sleepTime(final long attempt) {\n\t\tcheckArgument(attempt >= 0, \"attempt must not be negative (%d)\", attempt);\n\t\tfinal long exponentialSleepTime = initialWait * Math.round(Math.pow(2, attempt));\n\t\treturn exponentialSleepTime >= 0 && exponentialSleepTime < maxWait ? exponentialSleepTime : maxWait;\n\t}",
            "  40  \n  41  \n  42 +\n  43  \n  44  \n  45  ",
            "\t@Override\n\tpublic long sleepTime(final long attempt) {\n\t\tcheckArgument(attempt >= 0, \"attempt must not be negative (%s)\", attempt);\n\t\tfinal long exponentialSleepTime = initialWait * Math.round(Math.pow(2, attempt));\n\t\treturn exponentialSleepTime >= 0 && exponentialSleepTime < maxWait ? exponentialSleepTime : maxWait;\n\t}"
        ],
        [
            "LinkedOptionalMapSerializer::readOptionalMap(DataInputView,BiFunctionWithException,BiFunctionWithException)",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 -\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "\tpublic static <K, V> LinkedOptionalMap<K, V> readOptionalMap(\n\t\tDataInputView in,\n\t\tBiFunctionWithException<DataInputView, String, K, IOException> keyReader,\n\t\tBiFunctionWithException<DataInputView, String, V, IOException> valueReader) throws IOException {\n\n\t\tfinal long header = in.readLong();\n\t\tcheckState(header == HEADER, \"Corrupted stream received header %d\", header);\n\n\t\tlong mapSize = in.readInt();\n\t\tLinkedOptionalMap<K, V> map = new LinkedOptionalMap<>();\n\t\tfor (int i = 0; i < mapSize; i++) {\n\t\t\tString keyName = in.readUTF();\n\n\t\t\tfinal K key;\n\t\t\tif (in.readBoolean()) {\n\t\t\t\tkey = tryReadFrame(in, keyName, keyReader);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tkey = null;\n\t\t\t}\n\n\t\t\tfinal V value;\n\t\t\tif (in.readBoolean()) {\n\t\t\t\tvalue = tryReadFrame(in, keyName, valueReader);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tvalue = null;\n\t\t\t}\n\n\t\t\tmap.put(keyName, key, value);\n\t\t}\n\t\treturn map;\n\t}",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84 +\n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "\tpublic static <K, V> LinkedOptionalMap<K, V> readOptionalMap(\n\t\tDataInputView in,\n\t\tBiFunctionWithException<DataInputView, String, K, IOException> keyReader,\n\t\tBiFunctionWithException<DataInputView, String, V, IOException> valueReader) throws IOException {\n\n\t\tfinal long header = in.readLong();\n\t\tcheckState(header == HEADER, \"Corrupted stream received header %s\", header);\n\n\t\tlong mapSize = in.readInt();\n\t\tLinkedOptionalMap<K, V> map = new LinkedOptionalMap<>();\n\t\tfor (int i = 0; i < mapSize; i++) {\n\t\t\tString keyName = in.readUTF();\n\n\t\t\tfinal K key;\n\t\t\tif (in.readBoolean()) {\n\t\t\t\tkey = tryReadFrame(in, keyName, keyReader);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tkey = null;\n\t\t\t}\n\n\t\t\tfinal V value;\n\t\t\tif (in.readBoolean()) {\n\t\t\t\tvalue = tryReadFrame(in, keyName, valueReader);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tvalue = null;\n\t\t\t}\n\n\t\t\tmap.put(keyName, key, value);\n\t\t}\n\t\treturn map;\n\t}"
        ],
        [
            "FlinkKafkaInternalProducer::resumeTransaction(long,short)",
            " 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 -\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "\t/**\n\t * Instead of obtaining producerId and epoch from the transaction coordinator, re-use previously obtained ones,\n\t * so that we can resume transaction after a restart. Implementation of this method is based on\n\t * {@link KafkaProducer#initTransactions}.\n\t * https://github.com/apache/kafka/commit/5d2422258cb975a137a42a4e08f03573c49a387e#diff-f4ef1afd8792cd2a2e9069cd7ddea630\n\t */\n\tpublic void resumeTransaction(long producerId, short epoch) {\n\t\tPreconditions.checkState(producerId >= 0 && epoch >= 0, \"Incorrect values for producerId {} and epoch {}\", producerId, epoch);\n\t\tLOG.info(\"Attempting to resume transaction {} with producerId {} and epoch {}\", transactionalId, producerId, epoch);\n\n\t\tObject transactionManager = getValue(kafkaProducer, \"transactionManager\");\n\t\tsynchronized (transactionManager) {\n\t\t\tObject nextSequence = getValue(transactionManager, \"nextSequence\");\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.INITIALIZING\"));\n\t\t\tinvoke(nextSequence, \"clear\");\n\n\t\t\tObject producerIdAndEpoch = getValue(transactionManager, \"producerIdAndEpoch\");\n\t\t\tsetValue(producerIdAndEpoch, \"producerId\", producerId);\n\t\t\tsetValue(producerIdAndEpoch, \"epoch\", epoch);\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.READY\"));\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.IN_TRANSACTION\"));\n\t\t\tsetValue(transactionManager, \"transactionStarted\", true);\n\t\t}\n\t}",
            " 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151 +\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "\t/**\n\t * Instead of obtaining producerId and epoch from the transaction coordinator, re-use previously obtained ones,\n\t * so that we can resume transaction after a restart. Implementation of this method is based on\n\t * {@link KafkaProducer#initTransactions}.\n\t * https://github.com/apache/kafka/commit/5d2422258cb975a137a42a4e08f03573c49a387e#diff-f4ef1afd8792cd2a2e9069cd7ddea630\n\t */\n\tpublic void resumeTransaction(long producerId, short epoch) {\n\t\tPreconditions.checkState(producerId >= 0 && epoch >= 0, \"Incorrect values for producerId %s and epoch %s\", producerId, epoch);\n\t\tLOG.info(\"Attempting to resume transaction {} with producerId {} and epoch {}\", transactionalId, producerId, epoch);\n\n\t\tObject transactionManager = getValue(kafkaProducer, \"transactionManager\");\n\t\tsynchronized (transactionManager) {\n\t\t\tObject nextSequence = getValue(transactionManager, \"nextSequence\");\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.INITIALIZING\"));\n\t\t\tinvoke(nextSequence, \"clear\");\n\n\t\t\tObject producerIdAndEpoch = getValue(transactionManager, \"producerIdAndEpoch\");\n\t\t\tsetValue(producerIdAndEpoch, \"producerId\", producerId);\n\t\t\tsetValue(producerIdAndEpoch, \"epoch\", epoch);\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.READY\"));\n\n\t\t\tinvoke(transactionManager, \"transitionTo\", getEnum(\"org.apache.kafka.clients.producer.internals.TransactionManager$State.IN_TRANSACTION\"));\n\t\t\tsetValue(transactionManager, \"transactionStarted\", true);\n\t\t}\n\t}"
        ],
        [
            "PythonStreamBinderTest::testProgram()",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "\t@Test\n\tpublic void testProgram() throws Exception {\n\t\tPath testEntryPoint = new Path(getBaseTestPythonDir(), \"run_all_tests.py\");\n\t\tList<String> testFiles = findTestFiles();\n\n\t\tPreconditions.checkState(testFiles.size() > 0, \"No test files were found in {}.\", getBaseTestPythonDir());\n\n\t\tString[] arguments = new String[1 + 1 + testFiles.size()];\n\t\targuments[0] = testEntryPoint.getPath();\n\t\targuments[1] = findUtilsModule().getPath();\n\t\tint index = 2;\n\t\tfor (String testFile : testFiles) {\n\t\t\targuments[index] = testFile;\n\t\t\tindex++;\n\t\t}\n\t\ttry {\n\t\t\tnew PythonStreamBinder(new Configuration())\n\t\t\t\t.runPlan(arguments);\n\t\t} catch (PyException e) {\n\t\t\tif (e.getCause() instanceof JobExecutionException) {\n\t\t\t\t// JobExecutionExceptions are wrapped again by the jython interpreter resulting in horrible stacktraces\n\t\t\t\tthrow (JobExecutionException) e.getCause();\n\t\t\t} else {\n\t\t\t\t// probably caused by some issue in the main script itself\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  ",
            "\t@Test\n\tpublic void testProgram() throws Exception {\n\t\tPath testEntryPoint = new Path(getBaseTestPythonDir(), \"run_all_tests.py\");\n\t\tList<String> testFiles = findTestFiles();\n\n\t\tPreconditions.checkState(testFiles.size() > 0, \"No test files were found in %s.\", getBaseTestPythonDir());\n\n\t\tString[] arguments = new String[1 + 1 + testFiles.size()];\n\t\targuments[0] = testEntryPoint.getPath();\n\t\targuments[1] = findUtilsModule().getPath();\n\t\tint index = 2;\n\t\tfor (String testFile : testFiles) {\n\t\t\targuments[index] = testFile;\n\t\t\tindex++;\n\t\t}\n\t\ttry {\n\t\t\tnew PythonStreamBinder(new Configuration())\n\t\t\t\t.runPlan(arguments);\n\t\t} catch (PyException e) {\n\t\t\tif (e.getCause() instanceof JobExecutionException) {\n\t\t\t\t// JobExecutionExceptions are wrapped again by the jython interpreter resulting in horrible stacktraces\n\t\t\t\tthrow (JobExecutionException) e.getCause();\n\t\t\t} else {\n\t\t\t\t// probably caused by some issue in the main script itself\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "RestServerEndpointConfiguration::RestServerEndpointConfiguration(String,String,String,SSLHandlerFactory,Path,int,Map)",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 -\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "\tprivate RestServerEndpointConfiguration(\n\t\t\tfinal String restAddress,\n\t\t\t@Nullable String restBindAddress,\n\t\t\tString restBindPortRange,\n\t\t\t@Nullable SSLHandlerFactory sslHandlerFactory,\n\t\t\tfinal Path uploadDir,\n\t\t\tfinal int maxContentLength,\n\t\t\tfinal Map<String, String> responseHeaders) {\n\n\t\tPreconditions.checkArgument(maxContentLength > 0, \"maxContentLength must be positive, was: %d\", maxContentLength);\n\n\t\tthis.restAddress = requireNonNull(restAddress);\n\t\tthis.restBindAddress = restBindAddress;\n\t\tthis.restBindPortRange = requireNonNull(restBindPortRange);\n\t\tthis.sslHandlerFactory = sslHandlerFactory;\n\t\tthis.uploadDir = requireNonNull(uploadDir);\n\t\tthis.maxContentLength = maxContentLength;\n\t\tthis.responseHeaders = Collections.unmodifiableMap(requireNonNull(responseHeaders));\n\t}",
            "  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "\tprivate RestServerEndpointConfiguration(\n\t\t\tfinal String restAddress,\n\t\t\t@Nullable String restBindAddress,\n\t\t\tString restBindPortRange,\n\t\t\t@Nullable SSLHandlerFactory sslHandlerFactory,\n\t\t\tfinal Path uploadDir,\n\t\t\tfinal int maxContentLength,\n\t\t\tfinal Map<String, String> responseHeaders) {\n\n\t\tPreconditions.checkArgument(maxContentLength > 0, \"maxContentLength must be positive, was: %s\", maxContentLength);\n\n\t\tthis.restAddress = requireNonNull(restAddress);\n\t\tthis.restBindAddress = restBindAddress;\n\t\tthis.restBindPortRange = requireNonNull(restBindPortRange);\n\t\tthis.sslHandlerFactory = sslHandlerFactory;\n\t\tthis.uploadDir = requireNonNull(uploadDir);\n\t\tthis.maxContentLength = maxContentLength;\n\t\tthis.responseHeaders = Collections.unmodifiableMap(requireNonNull(responseHeaders));\n\t}"
        ],
        [
            "RestClientConfiguration::RestClientConfiguration(SSLHandlerFactory,long,long,int)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52 -\n  53  \n  54  \n  55  \n  56  \n  57  ",
            "\tprivate RestClientConfiguration(\n\t\t\t@Nullable final SSLHandlerFactory sslHandlerFactory,\n\t\t\tfinal long connectionTimeout,\n\t\t\tfinal long idlenessTimeout,\n\t\t\tfinal int maxContentLength) {\n\t\tcheckArgument(maxContentLength > 0, \"maxContentLength must be positive, was: %d\", maxContentLength);\n\t\tthis.sslHandlerFactory = sslHandlerFactory;\n\t\tthis.connectionTimeout = connectionTimeout;\n\t\tthis.idlenessTimeout = idlenessTimeout;\n\t\tthis.maxContentLength = maxContentLength;\n\t}",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52 +\n  53  \n  54  \n  55  \n  56  \n  57  ",
            "\tprivate RestClientConfiguration(\n\t\t\t@Nullable final SSLHandlerFactory sslHandlerFactory,\n\t\t\tfinal long connectionTimeout,\n\t\t\tfinal long idlenessTimeout,\n\t\t\tfinal int maxContentLength) {\n\t\tcheckArgument(maxContentLength > 0, \"maxContentLength must be positive, was: %s\", maxContentLength);\n\t\tthis.sslHandlerFactory = sslHandlerFactory;\n\t\tthis.connectionTimeout = connectionTimeout;\n\t\tthis.idlenessTimeout = idlenessTimeout;\n\t\tthis.maxContentLength = maxContentLength;\n\t}"
        ],
        [
            "JobDetails::JobDetails(JobID,String,long,long,long,JobStatus,long,int,int)",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 -\n  98  \n  99  \n 100  ",
            "\tpublic JobDetails(\n\t\t\tJobID jobId,\n\t\t\tString jobName,\n\t\t\tlong startTime,\n\t\t\tlong endTime,\n\t\t\tlong duration,\n\t\t\tJobStatus status,\n\t\t\tlong lastUpdateTime,\n\t\t\tint[] tasksPerState,\n\t\t\tint numTasks) {\n\n\t\tthis.jobId = checkNotNull(jobId);\n\t\tthis.jobName = checkNotNull(jobName);\n\t\tthis.startTime = startTime;\n\t\tthis.endTime = endTime;\n\t\tthis.duration = duration;\n\t\tthis.status = checkNotNull(status);\n\t\tthis.lastUpdateTime = lastUpdateTime;\n\t\tPreconditions.checkArgument(tasksPerState.length == ExecutionState.values().length, \n\t\t\t\"tasksPerState argument must be of size {}.\", ExecutionState.values().length);\n\t\tthis.tasksPerState = checkNotNull(tasksPerState);\n\t\tthis.numTasks = numTasks;\n\t}",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 +\n  98  \n  99  \n 100  ",
            "\tpublic JobDetails(\n\t\t\tJobID jobId,\n\t\t\tString jobName,\n\t\t\tlong startTime,\n\t\t\tlong endTime,\n\t\t\tlong duration,\n\t\t\tJobStatus status,\n\t\t\tlong lastUpdateTime,\n\t\t\tint[] tasksPerState,\n\t\t\tint numTasks) {\n\n\t\tthis.jobId = checkNotNull(jobId);\n\t\tthis.jobName = checkNotNull(jobName);\n\t\tthis.startTime = startTime;\n\t\tthis.endTime = endTime;\n\t\tthis.duration = duration;\n\t\tthis.status = checkNotNull(status);\n\t\tthis.lastUpdateTime = lastUpdateTime;\n\t\tPreconditions.checkArgument(tasksPerState.length == ExecutionState.values().length, \n\t\t\t\"tasksPerState argument must be of size %s.\", ExecutionState.values().length);\n\t\tthis.tasksPerState = checkNotNull(tasksPerState);\n\t\tthis.numTasks = numTasks;\n\t}"
        ],
        [
            "NullableSerializer::NullableSerializerSnapshot::createOuterSerializerWithNestedSerializers(TypeSerializer)",
            " 329  \n 330  \n 331  \n 332 -\n 333  \n 334  \n 335  \n 336  \n 337  \n 338  ",
            "\t\t@Override\n\t\tprotected NullableSerializer<T> createOuterSerializerWithNestedSerializers(TypeSerializer<?>[] nestedSerializers) {\n\t\t\tcheckState(nullPaddingLength >= 0,\n\t\t\t\t\"Negative padding size after serializer construction: %d\",\n\t\t\t\tnullPaddingLength);\n\n\t\t\tfinal byte[] padding = (nullPaddingLength == 0) ? EMPTY_BYTE_ARRAY : new byte[nullPaddingLength];\n\t\t\tTypeSerializer<T> nestedSerializer = (TypeSerializer<T>) nestedSerializers[0];\n\t\t\treturn new NullableSerializer<>(nestedSerializer, padding);\n\t\t}",
            " 329  \n 330  \n 331  \n 332 +\n 333  \n 334  \n 335  \n 336  \n 337  \n 338  ",
            "\t\t@Override\n\t\tprotected NullableSerializer<T> createOuterSerializerWithNestedSerializers(TypeSerializer<?>[] nestedSerializers) {\n\t\t\tcheckState(nullPaddingLength >= 0,\n\t\t\t\t\"Negative padding size after serializer construction: %s\",\n\t\t\t\tnullPaddingLength);\n\n\t\t\tfinal byte[] padding = (nullPaddingLength == 0) ? EMPTY_BYTE_ARRAY : new byte[nullPaddingLength];\n\t\t\tTypeSerializer<T> nestedSerializer = (TypeSerializer<T>) nestedSerializers[0];\n\t\t\treturn new NullableSerializer<>(nestedSerializer, padding);\n\t\t}"
        ],
        [
            "PojoSerializerSnapshot::readSnapshot(int,DataInputView,ClassLoader)",
            " 122  \n 123  \n 124 -\n 125  \n 126  ",
            "\t@Override\n\tpublic void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {\n\t\tcheckArgument(readVersion == 2, \"unrecognized read version %d\", readVersion);\n\t\tsnapshotData = PojoSerializerSnapshotData.createFrom(in, userCodeClassLoader);\n\t}",
            " 122  \n 123  \n 124 +\n 125  \n 126  ",
            "\t@Override\n\tpublic void readSnapshot(int readVersion, DataInputView in, ClassLoader userCodeClassLoader) throws IOException {\n\t\tcheckArgument(readVersion == 2, \"unrecognized read version %s\", readVersion);\n\t\tsnapshotData = PojoSerializerSnapshotData.createFrom(in, userCodeClassLoader);\n\t}"
        ],
        [
            "NullableSerializer::NullableSerializerSnapshot::NullableSerializerSnapshot(int)",
            " 310  \n 311  \n 312  \n 313 -\n 314  \n 315  \n 316  \n 317  ",
            "\t\tprivate NullableSerializerSnapshot(int nullPaddingLength) {\n\t\t\tsuper(NullableSerializer.class);\n\t\t\tcheckArgument(nullPaddingLength >= 0,\n\t\t\t\t\"Computed NULL padding can not be negative. %d\",\n\t\t\t\tnullPaddingLength);\n\n\t\t\tthis.nullPaddingLength = nullPaddingLength;\n\t\t}",
            " 310  \n 311  \n 312  \n 313 +\n 314  \n 315  \n 316  \n 317  ",
            "\t\tprivate NullableSerializerSnapshot(int nullPaddingLength) {\n\t\t\tsuper(NullableSerializer.class);\n\t\t\tcheckArgument(nullPaddingLength >= 0,\n\t\t\t\t\"Computed NULL padding can not be negative. %s\",\n\t\t\t\tnullPaddingLength);\n\n\t\t\tthis.nullPaddingLength = nullPaddingLength;\n\t\t}"
        ]
    ],
    "098979c744319fc7e92123eebfc391f204c7b96f": [
        [
            "HiveInspectors::getConversion(ObjectInspector,DataType)",
            " 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185 -\n 186 -\n 187 -\n 188 -\n 189 -\n 190 -\n 191 -\n 192 -\n 193 -\n 194 -\n 195 -\n 196 -\n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205 -\n 206 -\n 207 -\n 208 -\n 209 -\n 210 -\n 211 -\n 212 -\n 213 -\n 214 -\n 215 -\n 216 -\n 217 -\n 218 -\n 219 -\n 220 -\n 221 -\n 222 -\n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229 -\n 230 -\n 231 -\n 232 -\n 233 -\n 234 -\n 235 -\n 236 -\n 237 -\n 238 -\n 239 -\n 240 -\n 241 -\n 242 -\n 243 -\n 244 -\n 245 -\n 246 -\n 247 -\n 248 -\n 249 -\n 250 -\n 251 -\n 252 -\n 253 -\n 254 -\n 255 -\n 256 -\n 257 -\n 258 -\n 259 -\n 260 -\n 261 -\n 262 -\n 263 -\n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  ",
            "\t/**\n\t * Get conversion for converting Flink object to Hive object from an ObjectInspector and the corresponding Flink DataType.\n\t */\n\tpublic static HiveObjectConversion getConversion(ObjectInspector inspector, DataType dataType) {\n\t\tif (inspector instanceof PrimitiveObjectInspector) {\n\t\t\tif (inspector instanceof JavaBooleanObjectInspector) {\n\t\t\t\tif (((JavaBooleanObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new BooleanWritable((Boolean) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaStringObjectInspector) {\n\t\t\t\tif (((StringObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new Text((String) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaByteObjectInspector) {\n\t\t\t\tif (((JavaByteObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new ByteWritable((Byte) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaShortObjectInspector) {\n\t\t\t\tif (((JavaShortObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new ShortWritable((Short) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaIntObjectInspector) {\n\t\t\t\tif (((JavaIntObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new IntWritable((Integer) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaLongObjectInspector) {\n\t\t\t\tif (((JavaLongObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new LongWritable((Long) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaFloatObjectInspector) {\n\t\t\t\tif (((JavaFloatObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new FloatWritable((Float) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaDoubleObjectInspector) {\n\t\t\t\tif (((JavaDoubleObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new DoubleWritable((Double) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaDateObjectInspector) {\n\t\t\t\tif (((JavaDateObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new DateWritable((Date) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaTimestampObjectInspector) {\n\t\t\t\tif (((JavaTimestampObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new TimestampWritable((Timestamp) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaBinaryObjectInspector) {\n\t\t\t\tif (((JavaBinaryObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new BytesWritable((byte[]) o);\n\t\t\t\t} else {\n\t\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaHiveCharObjectInspector) {\n\t\t\t\tif (((JavaHiveCharObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new HiveCharWritable(\n\t\t\t\t\t\tnew HiveChar((String) o, ((CharType) dataType.getLogicalType()).getLength()));\n\t\t\t\t} else {\n\t\t\t\t\treturn o -> new HiveChar((String) o, ((CharType) dataType.getLogicalType()).getLength());\n\t\t\t\t}\n\t\t\t} else if (inspector instanceof JavaHiveVarcharObjectInspector) {\n\t\t\t\tif (((JavaHiveVarcharObjectInspector) inspector).preferWritable()) {\n\t\t\t\t\treturn o -> new HiveVarcharWritable(\n\t\t\t\t\t\tnew HiveVarchar((String) o, ((VarCharType) dataType.getLogicalType()).getLength()));\n\t\t\t\t} else {\n\t\t\t\t\treturn o -> new HiveVarchar((String) o, ((VarCharType) dataType.getLogicalType()).getLength());\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// TODO: handle decimal type\n\t\t}\n\n\t\t// TODO: handle complex types like struct, list, and map\n\n\t\tthrow new FlinkHiveUDFException(\n\t\t\tString.format(\"Flink doesn't support convert object conversion for %s yet\", inspector));\n\t}",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172 +\n 173 +\n 174 +\n 175 +\n 176 +\n 177 +\n 178 +\n 179 +\n 180 +\n 181 +\n 182 +\n 183 +\n 184 +\n 185 +\n 186 +\n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "\t/**\n\t * Get conversion for converting Flink object to Hive object from an ObjectInspector and the corresponding Flink DataType.\n\t */\n\tpublic static HiveObjectConversion getConversion(ObjectInspector inspector, DataType dataType) {\n\t\tif (inspector instanceof PrimitiveObjectInspector) {\n\t\t\tif (inspector instanceof BooleanObjectInspector ||\n\t\t\t\t\tinspector instanceof StringObjectInspector ||\n\t\t\t\t\tinspector instanceof ByteObjectInspector ||\n\t\t\t\t\tinspector instanceof ShortObjectInspector ||\n\t\t\t\t\tinspector instanceof IntObjectInspector ||\n\t\t\t\t\tinspector instanceof LongObjectInspector ||\n\t\t\t\t\tinspector instanceof FloatObjectInspector ||\n\t\t\t\t\tinspector instanceof DoubleObjectInspector ||\n\t\t\t\t\tinspector instanceof DateObjectInspector ||\n\t\t\t\t\tinspector instanceof TimestampObjectInspector ||\n\t\t\t\t\tinspector instanceof BinaryObjectInspector) {\n\t\t\t\treturn IdentityConversion.INSTANCE;\n\t\t\t} else if (inspector instanceof HiveCharObjectInspector) {\n\t\t\t\treturn o -> new HiveChar((String) o, ((CharType) dataType.getLogicalType()).getLength());\n\t\t\t} else if (inspector instanceof HiveVarcharObjectInspector) {\n\t\t\t\treturn o -> new HiveVarchar((String) o, ((VarCharType) dataType.getLogicalType()).getLength());\n\t\t\t}\n\n\t\t\t// TODO: handle decimal type\n\t\t}\n\n\t\t// TODO: handle complex types like struct, list, and map\n\n\t\tthrow new FlinkHiveUDFException(\n\t\t\tString.format(\"Flink doesn't support convert object conversion for %s yet\", inspector));\n\t}"
        ],
        [
            "HiveInspectors::toFlinkObject(ObjectInspector,Object)",
            " 275  \n 276  \n 277  \n 278  \n 279 -\n 280 -\n 281 -\n 282 -\n 283 -\n 284  \n 285  \n 286  \n 287  \n 288 -\n 289 -\n 290 -\n 291 -\n 292 -\n 293 -\n 294 -\n 295 -\n 296 -\n 297 -\n 298 -\n 299 -\n 300 -\n 301 -\n 302 -\n 303 -\n 304 -\n 305 -\n 306 -\n 307 -\n 308 -\n 309 -\n 310 -\n 311 -\n 312 -\n 313 -\n 314 -\n 315 -\n 316 -\n 317 -\n 318 -\n 319 -\n 320 -\n 321 -\n 322 -\n 323 -\n 324 -\n 325 -\n 326 -\n 327 -\n 328 -\n 329 -\n 330 -\n 331 -\n 332 -\n 333 -\n 334 -\n 335 -\n 336 -\n 337 -\n 338 -\n 339 -\n 340 -\n 341 -\n 342 -\n 343 -\n 344 -\n 345 -\n 346 -\n 347 -\n 348 -\n 349 -\n 350 -\n 351 -\n 352 -\n 353 -\n 354  \n 355  \n 356  \n 357 -\n 358 -\n 359 -\n 360  \n 361  \n 362  \n 363 -\n 364 -\n 365 -\n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  ",
            "\t/**\n\t * Converts a Hive object to Flink object with an ObjectInspector.\n\t */\n\tpublic static Object toFlinkObject(ObjectInspector inspector, Object data) {\n\t\tif (data == null) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (inspector instanceof VoidObjectInspector) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (inspector instanceof PrimitiveObjectInspector) {\n\t\t\tif (inspector instanceof BooleanObjectInspector) {\n\t\t\t\tBooleanObjectInspector oi = (BooleanObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.get(data) :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof StringObjectInspector) {\n\t\t\t\tStringObjectInspector oi = (StringObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.getPrimitiveWritableObject(data).toString() :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof ByteObjectInspector) {\n\t\t\t\tByteObjectInspector oi = (ByteObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.get(data) :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof ShortObjectInspector) {\n\t\t\t\tShortObjectInspector oi = (ShortObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.get(data) :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof IntObjectInspector) {\n\t\t\t\tIntObjectInspector oi = (IntObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.get(data) :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof LongObjectInspector) {\n\t\t\t\tLongObjectInspector oi = (LongObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.get(data) :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof FloatObjectInspector) {\n\t\t\t\tFloatObjectInspector oi = (FloatObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.get(data) :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof DoubleObjectInspector) {\n\t\t\t\tDoubleObjectInspector oi = (DoubleObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.get(data) :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof DateObjectInspector) {\n\t\t\t\tDateObjectInspector oi = (DateObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.getPrimitiveWritableObject(data).get() :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof TimestampObjectInspector) {\n\t\t\t\tTimestampObjectInspector oi = (TimestampObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.getPrimitiveWritableObject(data).getTimestamp() :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof BinaryObjectInspector) {\n\t\t\t\tBinaryObjectInspector oi = (BinaryObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.getPrimitiveWritableObject(data).getBytes() :\n\t\t\t\t\toi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof HiveCharObjectInspector) {\n\t\t\t\tHiveCharObjectInspector oi = (HiveCharObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.getPrimitiveWritableObject(data).getHiveChar().getValue() :\n\t\t\t\t\toi.getPrimitiveJavaObject(data).getValue();\n\t\t\t} else if (inspector instanceof HiveVarcharObjectInspector) {\n\t\t\t\tHiveVarcharObjectInspector oi = (HiveVarcharObjectInspector) inspector;\n\n\t\t\t\treturn oi.preferWritable() ?\n\t\t\t\t\toi.getPrimitiveWritableObject(data).getHiveVarchar().getValue() :\n\t\t\t\t\toi.getPrimitiveJavaObject(data).getValue();\n\t\t\t}\n\n\t\t\t// TODO: handle decimal type\n\t\t}\n\n\t\t// TODO: handle complex types like list and map\n\n\t\tif (inspector instanceof StandardStructObjectInspector) {\n\t\t\tStandardStructObjectInspector structInspector = (StandardStructObjectInspector) inspector;\n\n\t\t\tList<? extends StructField> fields = structInspector.getAllStructFieldRefs();\n\n\t\t\tRow row = new Row(fields.size());\n\t\t\tfor (int i = 0; i < row.getArity(); i++) {\n\t\t\t\trow.setField(\n\t\t\t\t\ti,\n\t\t\t\t\ttoFlinkObject(\n\t\t\t\t\t\tfields.get(i).getFieldObjectInspector(),\n\t\t\t\t\t\tstructInspector.getStructFieldData(data, fields.get(i)))\n\t\t\t\t);\n\t\t\t}\n\n\t\t\treturn row;\n\t\t}\n\n\t\tthrow new FlinkHiveUDFException(\n\t\t\tString.format(\"Unwrap does not support ObjectInspector '%s' yet\", inspector));\n\t}",
            " 198  \n 199  \n 200  \n 201  \n 202 +\n 203  \n 204  \n 205  \n 206  \n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214 +\n 215 +\n 216 +\n 217 +\n 218 +\n 219 +\n 220 +\n 221  \n 222  \n 223  \n 224 +\n 225  \n 226  \n 227  \n 228 +\n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "\t/**\n\t * Converts a Hive object to Flink object with an ObjectInspector.\n\t */\n\tpublic static Object toFlinkObject(ObjectInspector inspector, Object data) {\n\t\tif (data == null || inspector instanceof VoidObjectInspector) {\n\t\t\treturn null;\n\t\t}\n\n\t\tif (inspector instanceof PrimitiveObjectInspector) {\n\t\t\tif (inspector instanceof BooleanObjectInspector ||\n\t\t\t\t\tinspector instanceof StringObjectInspector ||\n\t\t\t\t\tinspector instanceof ByteObjectInspector ||\n\t\t\t\t\tinspector instanceof ShortObjectInspector ||\n\t\t\t\t\tinspector instanceof IntObjectInspector ||\n\t\t\t\t\tinspector instanceof LongObjectInspector ||\n\t\t\t\t\tinspector instanceof FloatObjectInspector ||\n\t\t\t\t\tinspector instanceof DoubleObjectInspector ||\n\t\t\t\t\tinspector instanceof DateObjectInspector ||\n\t\t\t\t\tinspector instanceof TimestampObjectInspector ||\n\t\t\t\t\tinspector instanceof BinaryObjectInspector) {\n\n\t\t\t\tPrimitiveObjectInspector poi = (PrimitiveObjectInspector) inspector;\n\t\t\t\treturn poi.getPrimitiveJavaObject(data);\n\t\t\t} else if (inspector instanceof HiveCharObjectInspector) {\n\t\t\t\tHiveCharObjectInspector oi = (HiveCharObjectInspector) inspector;\n\n\t\t\t\treturn oi.getPrimitiveJavaObject(data).getValue();\n\t\t\t} else if (inspector instanceof HiveVarcharObjectInspector) {\n\t\t\t\tHiveVarcharObjectInspector oi = (HiveVarcharObjectInspector) inspector;\n\n\t\t\t\treturn oi.getPrimitiveJavaObject(data).getValue();\n\t\t\t}\n\n\t\t\t// TODO: handle decimal type\n\t\t}\n\n\t\t// TODO: handle complex types like list and map\n\n\t\tif (inspector instanceof StandardStructObjectInspector) {\n\t\t\tStandardStructObjectInspector structInspector = (StandardStructObjectInspector) inspector;\n\n\t\t\tList<? extends StructField> fields = structInspector.getAllStructFieldRefs();\n\n\t\t\tRow row = new Row(fields.size());\n\t\t\tfor (int i = 0; i < row.getArity(); i++) {\n\t\t\t\trow.setField(\n\t\t\t\t\ti,\n\t\t\t\t\ttoFlinkObject(\n\t\t\t\t\t\tfields.get(i).getFieldObjectInspector(),\n\t\t\t\t\t\tstructInspector.getStructFieldData(data, fields.get(i)))\n\t\t\t\t);\n\t\t\t}\n\n\t\t\treturn row;\n\t\t}\n\n\t\tthrow new FlinkHiveUDFException(\n\t\t\tString.format(\"Unwrap does not support ObjectInspector '%s' yet\", inspector));\n\t}"
        ],
        [
            "HiveInspectors::getObjectInspector(Class)",
            " 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431 -\n 432  \n 433  \n 434 -\n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  ",
            "\tpublic static ObjectInspector getObjectInspector(Class clazz) {\n\t\tTypeInfo typeInfo;\n\n\t\tif (clazz.equals(String.class) || clazz.equals(Text.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.stringTypeInfo;\n\t\t} else if (clazz.equals(Boolean.class) || clazz.equals(BooleanWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.booleanTypeInfo;\n\t\t} else if (clazz.equals(Byte.class) || clazz.equals(ByteWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.byteTypeInfo;\n\t\t} else if (clazz.equals(Short.class) || clazz.equals(ShortWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.shortTypeInfo;\n\t\t} else if (clazz.equals(Integer.class) || clazz.equals(IntWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.intTypeInfo;\n\t\t} else if (clazz.equals(Long.class) || clazz.equals(LongWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.longTypeInfo;\n\t\t} else if (clazz.equals(Float.class) || clazz.equals(FloatWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.floatTypeInfo;\n\t\t} else if (clazz.equals(Double.class) || clazz.equals(DoubleWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.doubleTypeInfo;\n\t\t} else if (clazz.equals(Date.class) || clazz.equals(DateWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.dateTypeInfo;\n\t\t} else if (clazz.equals(Timestamp.class) || clazz.equals(TimestampWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.timestampTypeInfo;\n\t\t} else if (clazz.equals(byte[].class) || clazz.equals(BytesWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.binaryTypeInfo;\n\t\t} else if (clazz.equals(HiveChar.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.charTypeInfo;\n\t\t} else if (clazz.equals(HiveVarchar.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.varcharTypeInfo;\n\t\t} else {\n\t\t\tthrow new FlinkHiveUDFException(\n\t\t\t\tString.format(\"Class %s is not supported yet\", clazz.getName()));\n\t\t}\n\n\t\treturn getObjectInspector(typeInfo);\n\t}",
            " 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294 +\n 295  \n 296  \n 297 +\n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  ",
            "\tpublic static ObjectInspector getObjectInspector(Class clazz) {\n\t\tTypeInfo typeInfo;\n\n\t\tif (clazz.equals(String.class) || clazz.equals(Text.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.stringTypeInfo;\n\t\t} else if (clazz.equals(Boolean.class) || clazz.equals(BooleanWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.booleanTypeInfo;\n\t\t} else if (clazz.equals(Byte.class) || clazz.equals(ByteWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.byteTypeInfo;\n\t\t} else if (clazz.equals(Short.class) || clazz.equals(ShortWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.shortTypeInfo;\n\t\t} else if (clazz.equals(Integer.class) || clazz.equals(IntWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.intTypeInfo;\n\t\t} else if (clazz.equals(Long.class) || clazz.equals(LongWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.longTypeInfo;\n\t\t} else if (clazz.equals(Float.class) || clazz.equals(FloatWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.floatTypeInfo;\n\t\t} else if (clazz.equals(Double.class) || clazz.equals(DoubleWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.doubleTypeInfo;\n\t\t} else if (clazz.equals(Date.class) || clazz.equals(DateWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.dateTypeInfo;\n\t\t} else if (clazz.equals(Timestamp.class) || clazz.equals(TimestampWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.timestampTypeInfo;\n\t\t} else if (clazz.equals(byte[].class) || clazz.equals(BytesWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.binaryTypeInfo;\n\t\t} else if (clazz.equals(HiveChar.class) || clazz.equals(HiveCharWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.charTypeInfo;\n\t\t} else if (clazz.equals(HiveVarchar.class) || clazz.equals(HiveVarcharWritable.class)) {\n\n\t\t\ttypeInfo = TypeInfoFactory.varcharTypeInfo;\n\t\t} else {\n\t\t\tthrow new FlinkHiveUDFException(\n\t\t\t\tString.format(\"Class %s is not supported yet\", clazz.getName()));\n\t\t}\n\n\t\treturn getObjectInspector(typeInfo);\n\t}"
        ]
    ]
}