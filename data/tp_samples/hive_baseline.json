{
    "0a6d30b3e89d97028b3cd4174ec92e1f5a56d49f": [
        [
            "SortedDynPartitionOptimizer::SortedDynamicPartitionProc::process(Node,Stack,NodeProcessorCtx,Object)",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 -\n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS. If the operator tree already contains\n      // RS then ReduceSinkDeDuplication optimization should merge them\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n\n      LOG.info(\"Sorted dynamic partitioning optimization kicked in..\");\n\n      // if not dynamic partitioning then bail out\n      if (fsOp.getConf().getDynPartCtx() == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as dynamic partitioning context is null\");\n        return null;\n      }\n\n      // if list bucketing then bail out\n      ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();\n      if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty()\n          && !lbCtx.getSkewedColValues().isEmpty()) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as list bucketing is enabled\");\n        return null;\n      }\n\n      Table destTable = fsOp.getConf().getTable();\n      if (destTable == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as destination table is null\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      // if all dp columns got constant folded then disable this optimization\n      if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {\n        LOG.debug(\"Bailing out of sorted dynamic partition optimizer as all dynamic partition\" +\n            \" columns got constant folded (static partitioning)\");\n        return null;\n      }\n\n      // if RS is inserted by enforce bucketing or sorting, we need to remove it\n      // since ReduceSinkDeDuplication will not merge them to single RS.\n      // RS inserted by enforce bucketing/sorting will have bucketing column in\n      // reduce sink key whereas RS inserted by this optimization will have\n      // partition columns followed by bucket number followed by sort columns in\n      // the reduce sink key. Since both key columns are not prefix subset\n      // ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs.\n      // To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting.\n      if (!removeRSInsertedByEnforceBucketing(fsOp)) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as some partition columns \" +\n            \"got constant folded.\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      DynamicPartitionCtx dpCtx = fsOp.getConf().getDynPartCtx();\n      int numBuckets = destTable.getNumBuckets();\n\n      // if enforce bucketing/sorting is disabled numBuckets will not be set.\n      // set the number of buckets here to ensure creation of empty buckets\n      dpCtx.setNumBuckets(numBuckets);\n\n      // Get the positions for partition, bucket and sort columns\n      List<Integer> bucketPositions = getBucketPositions(destTable.getBucketCols(),\n          destTable.getCols());\n      List<Integer> sortPositions = null;\n      List<Integer> sortOrder = null;\n      if (fsOp.getConf().getWriteType() == AcidUtils.Operation.UPDATE ||\n          fsOp.getConf().getWriteType() == AcidUtils.Operation.DELETE) {\n        // When doing updates and deletes we always want to sort on the rowid because the ACID\n        // reader will expect this sort order when doing reads.  So\n        // ignore whatever comes from the table and enforce this sort order instead.\n        sortPositions = Arrays.asList(0);\n        sortOrder = Arrays.asList(1); // 1 means asc, could really use enum here in the thrift if\n      } else {\n        if (!destTable.getSortCols().isEmpty()) {\n          // Sort columns specified by table\n          sortPositions = getSortPositions(destTable.getSortCols(), destTable.getCols());\n          sortOrder = getSortOrders(destTable.getSortCols(), destTable.getCols());\n        } else {\n          // Infer sort columns from operator tree\n          sortPositions = Lists.newArrayList();\n          sortOrder = Lists.newArrayList();\n          inferSortPositions(fsParent, sortPositions, sortOrder);\n        }\n      }\n      List<Integer> sortNullOrder = new ArrayList<Integer>();\n      for (int order : sortOrder) {\n        sortNullOrder.add(order == 1 ? 0 : 1); // for asc, nulls first; for desc, nulls last\n      }\n      LOG.debug(\"Got sort order\");\n      for (int i : sortPositions) LOG.debug(\"sort position \" + i);\n      for (int i : sortOrder) LOG.debug(\"sort order \" + i);\n      for (int i : sortNullOrder) LOG.debug(\"sort null order \" + i);\n      List<Integer> partitionPositions = getPartitionPositions(dpCtx, fsParent.getSchema());\n      List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();\n      ArrayList<ExprNodeDesc> bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);\n\n      // update file sink descriptor\n      fsOp.getConf().setMultiFileSpray(false);\n      fsOp.getConf().setNumFiles(1);\n      fsOp.getConf().setTotalFiles(1);\n\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n\n      // Create ReduceSink operator\n      ReduceSinkOperator rsOp = getReduceSinkOp(partitionPositions, sortPositions, sortOrder, sortNullOrder,\n          allRSCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());\n\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (partitionPositions.contains(i) || sortPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      if (!bucketColumns.isEmpty()) {\n        descs.add(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, ReduceField.KEY.toString()+\".'\"+BUCKET_NUMBER_COL_NAME+\"'\", null, false));\n        colNames.add(\"'\"+BUCKET_NUMBER_COL_NAME+\"'\");\n        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo, selRS.getSignature().get(0).getTabAlias(), true, true);\n        selRS.getSignature().add(ci);\n        fsParent.getSchema().getSignature().add(ci);\n      }\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, rsOp);\n\n      // link SEL to FS\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(selOp);\n      selOp.getChildOperators().add(fsOp);\n\n      // Set if partition sorted or partition bucket sorted\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      if (bucketColumns.size() > 0) {\n        fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);\n      }\n\n      // update partition column info in FS descriptor\n      fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());\n\n      LOG.info(\"Inserted \" + rsOp.getOperatorId() + \" and \" + selOp.getOperatorId()\n          + \" as parent of \" + fsOp.getOperatorId() + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187 +\n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195 +\n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 +\n 208 +\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  ",
            "    @Override\n    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      // introduce RS and EX before FS. If the operator tree already contains\n      // RS then ReduceSinkDeDuplication optimization should merge them\n      FileSinkOperator fsOp = (FileSinkOperator) nd;\n\n      LOG.info(\"Sorted dynamic partitioning optimization kicked in..\");\n\n      // if not dynamic partitioning then bail out\n      if (fsOp.getConf().getDynPartCtx() == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as dynamic partitioning context is null\");\n        return null;\n      }\n\n      // if list bucketing then bail out\n      ListBucketingCtx lbCtx = fsOp.getConf().getLbCtx();\n      if (lbCtx != null && !lbCtx.getSkewedColNames().isEmpty()\n          && !lbCtx.getSkewedColValues().isEmpty()) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as list bucketing is enabled\");\n        return null;\n      }\n\n      Table destTable = fsOp.getConf().getTable();\n      if (destTable == null) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as destination table is null\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      Operator<? extends OperatorDesc> fsParent = fsOp.getParentOperators().get(0);\n      // if all dp columns got constant folded then disable this optimization\n      if (allStaticPartitions(fsParent, fsOp.getConf().getDynPartCtx())) {\n        LOG.debug(\"Bailing out of sorted dynamic partition optimizer as all dynamic partition\" +\n            \" columns got constant folded (static partitioning)\");\n        return null;\n      }\n\n      // if RS is inserted by enforce bucketing or sorting, we need to remove it\n      // since ReduceSinkDeDuplication will not merge them to single RS.\n      // RS inserted by enforce bucketing/sorting will have bucketing column in\n      // reduce sink key whereas RS inserted by this optimization will have\n      // partition columns followed by bucket number followed by sort columns in\n      // the reduce sink key. Since both key columns are not prefix subset\n      // ReduceSinkDeDuplication will not merge them together resulting in 2 MR jobs.\n      // To avoid that we will remove the RS (and EX) inserted by enforce bucketing/sorting.\n      if (!removeRSInsertedByEnforceBucketing(fsOp)) {\n        LOG.debug(\"Bailing out of sort dynamic partition optimization as some partition columns \" +\n            \"got constant folded.\");\n        return null;\n      }\n\n      // unlink connection between FS and its parent\n      fsParent = fsOp.getParentOperators().get(0);\n      fsParent.getChildOperators().clear();\n\n      DynamicPartitionCtx dpCtx = fsOp.getConf().getDynPartCtx();\n      int numBuckets = destTable.getNumBuckets();\n\n      // if enforce bucketing/sorting is disabled numBuckets will not be set.\n      // set the number of buckets here to ensure creation of empty buckets\n      dpCtx.setNumBuckets(numBuckets);\n\n      // Get the positions for partition, bucket and sort columns\n      List<Integer> bucketPositions = getBucketPositions(destTable.getBucketCols(),\n          destTable.getCols());\n      List<Integer> sortPositions = null;\n      List<Integer> sortOrder = null;\n      ArrayList<ExprNodeDesc> bucketColumns;\n      if (fsOp.getConf().getWriteType() == AcidUtils.Operation.UPDATE ||\n          fsOp.getConf().getWriteType() == AcidUtils.Operation.DELETE) {\n        // When doing updates and deletes we always want to sort on the rowid because the ACID\n        // reader will expect this sort order when doing reads.  So\n        // ignore whatever comes from the table and enforce this sort order instead.\n        sortPositions = Arrays.asList(0);\n        sortOrder = Arrays.asList(1); // 1 means asc, could really use enum here in the thrift if\n        bucketColumns = new ArrayList<>(); // Bucketing column is already present in ROW__ID, which is specially handled in ReduceSink\n      } else {\n        if (!destTable.getSortCols().isEmpty()) {\n          // Sort columns specified by table\n          sortPositions = getSortPositions(destTable.getSortCols(), destTable.getCols());\n          sortOrder = getSortOrders(destTable.getSortCols(), destTable.getCols());\n        } else {\n          // Infer sort columns from operator tree\n          sortPositions = Lists.newArrayList();\n          sortOrder = Lists.newArrayList();\n          inferSortPositions(fsParent, sortPositions, sortOrder);\n        }\n        List<ColumnInfo> colInfos = fsParent.getSchema().getSignature();\n        bucketColumns = getPositionsToExprNodes(bucketPositions, colInfos);\n      }\n      List<Integer> sortNullOrder = new ArrayList<Integer>();\n      for (int order : sortOrder) {\n        sortNullOrder.add(order == 1 ? 0 : 1); // for asc, nulls first; for desc, nulls last\n      }\n      LOG.debug(\"Got sort order\");\n      for (int i : sortPositions) LOG.debug(\"sort position \" + i);\n      for (int i : sortOrder) LOG.debug(\"sort order \" + i);\n      for (int i : sortNullOrder) LOG.debug(\"sort null order \" + i);\n      List<Integer> partitionPositions = getPartitionPositions(dpCtx, fsParent.getSchema());\n\n      // update file sink descriptor\n      fsOp.getConf().setMultiFileSpray(false);\n      fsOp.getConf().setNumFiles(1);\n      fsOp.getConf().setTotalFiles(1);\n\n      ArrayList<ColumnInfo> parentCols = Lists.newArrayList(fsParent.getSchema().getSignature());\n      ArrayList<ExprNodeDesc> allRSCols = Lists.newArrayList();\n      for (ColumnInfo ci : parentCols) {\n        allRSCols.add(new ExprNodeColumnDesc(ci));\n      }\n\n      // Create ReduceSink operator\n      ReduceSinkOperator rsOp = getReduceSinkOp(partitionPositions, sortPositions, sortOrder, sortNullOrder,\n          allRSCols, bucketColumns, numBuckets, fsParent, fsOp.getConf().getWriteType());\n\n      List<ExprNodeDesc> descs = new ArrayList<ExprNodeDesc>(allRSCols.size());\n      List<String> colNames = new ArrayList<String>();\n      String colName;\n      for (int i = 0; i < allRSCols.size(); i++) {\n        ExprNodeDesc col = allRSCols.get(i);\n        colName = col.getExprString();\n        colNames.add(colName);\n        if (partitionPositions.contains(i) || sortPositions.contains(i)) {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.KEY.toString()+\".\"+colName, null, false));\n        } else {\n          descs.add(new ExprNodeColumnDesc(col.getTypeInfo(), ReduceField.VALUE.toString()+\".\"+colName, null, false));\n        }\n      }\n      RowSchema selRS = new RowSchema(fsParent.getSchema());\n      if (!bucketColumns.isEmpty()) {\n        descs.add(new ExprNodeColumnDesc(TypeInfoFactory.stringTypeInfo, ReduceField.KEY.toString()+\".'\"+BUCKET_NUMBER_COL_NAME+\"'\", null, false));\n        colNames.add(\"'\"+BUCKET_NUMBER_COL_NAME+\"'\");\n        ColumnInfo ci = new ColumnInfo(BUCKET_NUMBER_COL_NAME, TypeInfoFactory.stringTypeInfo, selRS.getSignature().get(0).getTabAlias(), true, true);\n        selRS.getSignature().add(ci);\n        fsParent.getSchema().getSignature().add(ci);\n      }\n      // Create SelectDesc\n      SelectDesc selConf = new SelectDesc(descs, colNames);\n\n      // Create Select Operator\n      SelectOperator selOp = (SelectOperator) OperatorFactory.getAndMakeChild(\n              selConf, selRS, rsOp);\n\n      // link SEL to FS\n      fsOp.getParentOperators().clear();\n      fsOp.getParentOperators().add(selOp);\n      selOp.getChildOperators().add(fsOp);\n\n      // Set if partition sorted or partition bucket sorted\n      fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_SORTED);\n      if (bucketColumns.size() > 0) {\n        fsOp.getConf().setDpSortState(FileSinkDesc.DPSortState.PARTITION_BUCKET_SORTED);\n      }\n\n      // update partition column info in FS descriptor\n      fsOp.getConf().setPartitionCols( rsOp.getConf().getPartitionCols());\n\n      LOG.info(\"Inserted \" + rsOp.getOperatorId() + \" and \" + selOp.getOperatorId()\n          + \" as parent of \" + fsOp.getOperatorId() + \" and child of \" + fsParent.getOperatorId());\n\n      parseCtx.setReduceSinkAddedBySortedDynPartition(true);\n      return null;\n    }"
        ]
    ],
    "4b7f373e58a222cc2bd83ea28b916009d7ebf75b": [
        [
            "CliConfigs::MiniTezCliConfig::MiniTezCliConfig()",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  ",
            "    public MiniTezCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minitez.query.files\");\n        includesFrom(testConfigProps, \"minitez.query.files.shared\");\n        excludesFrom(testConfigProps, \"minillap.query.files\");\n        excludesFrom(testConfigProps, \"minillap.shared.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/tez\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init_tez.sql\");\n        setCleanupScript(\"q_test_cleanup_tez.sql\");\n\n        setHiveConfDir(\"data/conf/tez\");\n        setClusterType(MiniClusterType.tez);\n        setMetastoreType(MetastoreType.hbase);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  ",
            "    public MiniTezCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minitez.query.files\");\n        includesFrom(testConfigProps, \"minitez.query.files.shared\");\n        excludesFrom(testConfigProps, \"minillap.query.files\");\n        excludesFrom(testConfigProps, \"minillap.shared.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/tez\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init_tez.sql\");\n        setCleanupScript(\"q_test_cleanup_tez.sql\");\n\n        setHiveConfDir(\"data/conf/tez\");\n        setClusterType(MiniClusterType.tez);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ]
    ],
    "130617443bb05d79c18420c0c4e903a76da3651c": [
        [
            "HiveAlterHandler::alterTable(RawStore,Warehouse,String,String,Table,EnvironmentContext)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 -\n 167 -\n 168 -\n 169 -\n 170  \n 171  \n 172  \n 173  \n 174  \n 175 -\n 176  \n 177 -\n 178 -\n 179 -\n 180 -\n 181 -\n 182 -\n 183 -\n 184 -\n 185 -\n 186 -\n 187 -\n 188 -\n 189 -\n 190 -\n 191 -\n 192 -\n 193 -\n 194 -\n 195 -\n 196 -\n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205  \n 206 -\n 207 -\n 208 -\n 209 -\n 210 -\n 211 -\n 212 -\n 213 -\n 214 -\n 215 -\n 216 -\n 217 -\n 218 -\n 219 -\n 220 -\n 221 -\n 222 -\n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229 -\n 230  \n 231 -\n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  ",
            "  @Override\n  public void alterTable(RawStore msdb, Warehouse wh, String dbname,\n      String name, Table newt, EnvironmentContext environmentContext) throws InvalidOperationException, MetaException {\n    final boolean cascade = environmentContext != null\n        && environmentContext.isSetProperties()\n        && StatsSetupConst.TRUE.equals(environmentContext.getProperties().get(\n            StatsSetupConst.CASCADE));\n    if (newt == null) {\n      throw new InvalidOperationException(\"New table is invalid: \" + newt);\n    }\n\n    if (!MetaStoreUtils.validateName(newt.getTableName(), hiveConf)) {\n      throw new InvalidOperationException(newt.getTableName()\n          + \" is not a valid object name\");\n    }\n    String validate = MetaStoreUtils.validateTblColumns(newt.getSd().getCols());\n    if (validate != null) {\n      throw new InvalidOperationException(\"Invalid column \" + validate);\n    }\n\n    Path srcPath = null;\n    FileSystem srcFs = null;\n    Path destPath = null;\n    FileSystem destFs = null;\n\n    boolean success = false;\n    boolean moveData = false;\n    boolean rename = false;\n    Table oldt = null;\n    List<ObjectPair<Partition, String>> altps = new ArrayList<ObjectPair<Partition, String>>();\n\n    try {\n      msdb.openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n\n      // check if table with the new name already exists\n      if (!newt.getTableName().equalsIgnoreCase(name)\n          || !newt.getDbName().equalsIgnoreCase(dbname)) {\n        if (msdb.getTable(newt.getDbName(), newt.getTableName()) != null) {\n          throw new InvalidOperationException(\"new table \" + newt.getDbName()\n              + \".\" + newt.getTableName() + \" already exists\");\n        }\n        rename = true;\n      }\n\n      // get old table\n      oldt = msdb.getTable(dbname, name);\n      if (oldt == null) {\n        throw new InvalidOperationException(\"table \" + dbname + \".\" + name + \" doesn't exist\");\n      }\n\n      if (HiveConf.getBoolVar(hiveConf,\n            HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES,\n            false)) {\n        // Throws InvalidOperationException if the new column types are not\n        // compatible with the current column types.\n        MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(\n            oldt.getSd().getCols(), newt.getSd().getCols());\n      }\n\n      if (cascade) {\n        //Currently only column related changes can be cascaded in alter table\n        if(MetaStoreUtils.isCascadeNeededInAlterTable(oldt, newt)) {\n          List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n          for (Partition part : parts) {\n            List<FieldSchema> oldCols = part.getSd().getCols();\n            part.getSd().setCols(newt.getSd().getCols());\n            String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());\n            updatePartColumnStatsForAlterColumns(msdb, part, oldPartName, part.getValues(), oldCols, part);\n            msdb.alterPartition(dbname, name, part.getValues(), part);\n          }\n        } else {\n          LOG.warn(\"Alter table does not cascade changes to its partitions.\");\n        }\n      }\n\n      //check that partition keys have not changed, except for virtual views\n      //however, allow the partition comments to change\n      boolean partKeysPartiallyEqual = checkPartialPartKeysEqual(oldt.getPartitionKeys(),\n          newt.getPartitionKeys());\n\n      if(!oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())){\n        if (oldt.getPartitionKeys().size() != newt.getPartitionKeys().size()\n            || !partKeysPartiallyEqual) {\n          throw new InvalidOperationException(\n              \"partition keys can not be changed.\");\n        }\n      }\n\n      // if this alter is a rename, the table is not a virtual view, the user\n      // didn't change the default location (or new location is empty), and\n      // table is not an external table, that means user is asking metastore to\n      // move data to the new location corresponding to the new name\n      if (rename\n          && !oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())\n          && (oldt.getSd().getLocation().compareTo(newt.getSd().getLocation()) == 0\n            || StringUtils.isEmpty(newt.getSd().getLocation()))\n          && !MetaStoreUtils.isExternalTable(oldt)) {\n\n        srcPath = new Path(oldt.getSd().getLocation());\n        srcFs = wh.getFs(srcPath);\n\n        // that means user is asking metastore to move data to new location\n        // corresponding to the new name\n        // get new location\n        Database db = msdb.getDatabase(newt.getDbName());\n        Path databasePath = constructRenamedPath(wh.getDatabasePath(db), srcPath);\n        destPath = new Path(databasePath, newt.getTableName().toLowerCase());\n        destFs = wh.getFs(destPath);\n\n        newt.getSd().setLocation(destPath.toString());\n        moveData = true;\n\n        // check that destination does not exist otherwise we will be\n        // overwriting data\n        // check that src and dest are on the same file system\n        if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n          throw new InvalidOperationException(\"table new location \" + destPath\n              + \" is on a different file system than the old location \"\n              + srcPath + \". This operation is not supported\");\n        }\n        try {\n          srcFs.exists(srcPath); // check that src exists and also checks\n                                 // permissions necessary\n          if (destFs.exists(destPath)) {\n            throw new InvalidOperationException(\"New location for this table \"\n                + newt.getDbName() + \".\" + newt.getTableName()\n                + \" already exists : \" + destPath);\n          }\n        } catch (IOException e) {\n          throw new InvalidOperationException(\"Unable to access new location \"\n              + destPath + \" for table \" + newt.getDbName() + \".\"\n              + newt.getTableName());\n        }\n        String oldTblLocPath = srcPath.toUri().getPath();\n        String newTblLocPath = destPath.toUri().getPath();\n\n        // also the location field in partition\n        List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n        for (Partition part : parts) {\n          String oldPartLoc = part.getSd().getLocation();\n          if (oldPartLoc.contains(oldTblLocPath)) {\n            URI oldUri = new Path(oldPartLoc).toUri();\n            String newPath = oldUri.getPath().replace(oldTblLocPath, newTblLocPath);\n            Path newPartLocPath = new Path(oldUri.getScheme(), oldUri.getAuthority(), newPath);\n            altps.add(ObjectPair.create(part, part.getSd().getLocation()));\n            part.getSd().setLocation(newPartLocPath.toString());\n            String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());\n            try {\n              //existing partition column stats is no longer valid, remove them\n              msdb.deletePartitionColumnStatistics(dbname, name, oldPartName, part.getValues(), null);\n            } catch (InvalidInputException iie) {\n              throw new InvalidOperationException(\"Unable to update partition stats in table rename.\" + iie);\n            }\n            msdb.alterPartition(dbname, name, part.getValues(), part);\n          }\n        }\n      } else if (MetaStoreUtils.requireCalStats(hiveConf, null, null, newt, environmentContext) &&\n        (newt.getPartitionKeysSize() == 0)) {\n          Database db = msdb.getDatabase(newt.getDbName());\n          // Update table stats. For partitioned table, we update stats in\n          // alterPartition()\n          MetaStoreUtils.updateTableStatsFast(db, newt, wh, false, true, environmentContext);\n      }\n\n      alterTableUpdateTableColumnStats(msdb, oldt, newt);\n      // commit the changes\n      success = msdb.commitTransaction();\n    } catch (InvalidObjectException e) {\n      LOG.debug(\"Failed to get object from Metastore \", e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table.\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } catch (NoSuchObjectException e) {\n      LOG.debug(\"Object not found in metastore \", e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && moveData) {\n        // change the file name in hdfs\n        // check that src exists otherwise there is no need to copy the data\n        // rename the src to destination\n        try {\n          if (srcFs.exists(srcPath) && !srcFs.rename(srcPath, destPath)) {\n            throw new IOException(\"Renaming \" + srcPath + \" to \" + destPath + \" failed\");\n          }\n        } catch (IOException e) {\n          LOG.error(\"Alter Table operation for \" + dbname + \".\" + name + \" failed.\", e);\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterTable(newt.getDbName(), newt.getTableName(), oldt);\n            for (ObjectPair<Partition, String> pair : altps) {\n              Partition part = pair.getFirst();\n              part.getSd().setLocation(pair.getSecond());\n              msdb.alterPartition(newt.getDbName(), name, part.getValues(), part);\n            }\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            // we should log this for manual rollback by administrator\n            LOG.error(\"Reverting metadata by HDFS operation failure failed During HDFS operation failed\", e1);\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should be renamed to \" + Warehouse.getQualifiedName(oldt));\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should have path \" + srcPath);\n            for (ObjectPair<Partition, String> pair : altps) {\n              LOG.error(\"Partition \" + Warehouse.getQualifiedName(pair.getFirst()) +\n                  \" should have path \" + pair.getSecond());\n            }\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Alter Table operation for \" + dbname + \".\" + name +\n            \" failed to move data due to: '\" + getSimpleMessage(e) + \"' See hive log file for details.\");\n        }\n      }\n    }\n    if (!success) {\n      throw new MetaException(\"Committing the alter table transaction was not successful.\");\n    }\n  }",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172  \n 173  \n 174  \n 175  \n 176  \n 177 +\n 178 +\n 179 +\n 180 +\n 181 +\n 182 +\n 183  \n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193 +\n 194 +\n 195 +\n 196 +\n 197 +\n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208  \n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214 +\n 215 +\n 216 +\n 217 +\n 218 +\n 219 +\n 220 +\n 221 +\n 222 +\n 223 +\n 224 +\n 225 +\n 226 +\n 227 +\n 228 +\n 229 +\n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238 +\n 239 +\n 240 +\n 241 +\n 242 +\n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  ",
            "  @Override\n  public void alterTable(RawStore msdb, Warehouse wh, String dbname,\n      String name, Table newt, EnvironmentContext environmentContext) throws InvalidOperationException, MetaException {\n    final boolean cascade = environmentContext != null\n        && environmentContext.isSetProperties()\n        && StatsSetupConst.TRUE.equals(environmentContext.getProperties().get(\n            StatsSetupConst.CASCADE));\n    if (newt == null) {\n      throw new InvalidOperationException(\"New table is invalid: \" + newt);\n    }\n\n    if (!MetaStoreUtils.validateName(newt.getTableName(), hiveConf)) {\n      throw new InvalidOperationException(newt.getTableName()\n          + \" is not a valid object name\");\n    }\n    String validate = MetaStoreUtils.validateTblColumns(newt.getSd().getCols());\n    if (validate != null) {\n      throw new InvalidOperationException(\"Invalid column \" + validate);\n    }\n\n    Path srcPath = null;\n    FileSystem srcFs = null;\n    Path destPath = null;\n    FileSystem destFs = null;\n\n    boolean success = false;\n    boolean moveData = false;\n    boolean rename = false;\n    Table oldt = null;\n    List<ObjectPair<Partition, String>> altps = new ArrayList<ObjectPair<Partition, String>>();\n\n    try {\n      msdb.openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n\n      // check if table with the new name already exists\n      if (!newt.getTableName().equalsIgnoreCase(name)\n          || !newt.getDbName().equalsIgnoreCase(dbname)) {\n        if (msdb.getTable(newt.getDbName(), newt.getTableName()) != null) {\n          throw new InvalidOperationException(\"new table \" + newt.getDbName()\n              + \".\" + newt.getTableName() + \" already exists\");\n        }\n        rename = true;\n      }\n\n      // get old table\n      oldt = msdb.getTable(dbname, name);\n      if (oldt == null) {\n        throw new InvalidOperationException(\"table \" + dbname + \".\" + name + \" doesn't exist\");\n      }\n\n      if (HiveConf.getBoolVar(hiveConf,\n            HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES,\n            false)) {\n        // Throws InvalidOperationException if the new column types are not\n        // compatible with the current column types.\n        MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(\n            oldt.getSd().getCols(), newt.getSd().getCols());\n      }\n\n      if (cascade) {\n        //Currently only column related changes can be cascaded in alter table\n        if(MetaStoreUtils.isCascadeNeededInAlterTable(oldt, newt)) {\n          List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n          for (Partition part : parts) {\n            List<FieldSchema> oldCols = part.getSd().getCols();\n            part.getSd().setCols(newt.getSd().getCols());\n            String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());\n            updatePartColumnStatsForAlterColumns(msdb, part, oldPartName, part.getValues(), oldCols, part);\n            msdb.alterPartition(dbname, name, part.getValues(), part);\n          }\n        } else {\n          LOG.warn(\"Alter table does not cascade changes to its partitions.\");\n        }\n      }\n\n      //check that partition keys have not changed, except for virtual views\n      //however, allow the partition comments to change\n      boolean partKeysPartiallyEqual = checkPartialPartKeysEqual(oldt.getPartitionKeys(),\n          newt.getPartitionKeys());\n\n      if(!oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())){\n        if (oldt.getPartitionKeys().size() != newt.getPartitionKeys().size()\n            || !partKeysPartiallyEqual) {\n          throw new InvalidOperationException(\n              \"partition keys can not be changed.\");\n        }\n      }\n\n      // rename needs change the data location and move the data to the new location corresponding\n      // to the new name if:\n      // 1) the table is not a virtual view, and\n      // 2) the table is not an external table, and\n      // 3) the user didn't change the default location (or new location is empty), and\n      // 4) the table was not initially created with a specified location\n      if (rename\n          && !oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())\n          && (oldt.getSd().getLocation().compareTo(newt.getSd().getLocation()) == 0\n            || StringUtils.isEmpty(newt.getSd().getLocation()))\n          && !MetaStoreUtils.isExternalTable(oldt)) {\n        Database olddb = msdb.getDatabase(dbname);\n        // if a table was created in a user specified location using the DDL like\n        // create table tbl ... location ...., it should be treated like an external table\n        // in the table rename, its data location should not be changed. We can check\n        // if the table directory was created directly under its database directory to tell\n        // if it is such a table\n        srcPath = new Path(oldt.getSd().getLocation());\n        String oldtRelativePath = (new Path(olddb.getLocationUri()).toUri())\n            .relativize(srcPath.toUri()).toString();\n        boolean tableInSpecifiedLoc = !oldtRelativePath.equalsIgnoreCase(name)\n            && !oldtRelativePath.equalsIgnoreCase(name + Path.SEPARATOR);\n\n        if (!tableInSpecifiedLoc) {\n          srcFs = wh.getFs(srcPath);\n\n          // get new location\n          Database db = msdb.getDatabase(newt.getDbName());\n          Path databasePath = constructRenamedPath(wh.getDatabasePath(db), srcPath);\n          destPath = new Path(databasePath, newt.getTableName().toLowerCase());\n          destFs = wh.getFs(destPath);\n\n          newt.getSd().setLocation(destPath.toString());\n          moveData = true;\n\n          // check that destination does not exist otherwise we will be\n          // overwriting data\n          // check that src and dest are on the same file system\n          if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n            throw new InvalidOperationException(\"table new location \" + destPath\n                + \" is on a different file system than the old location \"\n                + srcPath + \". This operation is not supported\");\n          }\n          try {\n            srcFs.exists(srcPath); // check that src exists and also checks\n                                   // permissions necessary\n            if (destFs.exists(destPath)) {\n              throw new InvalidOperationException(\"New location for this table \"\n                  + newt.getDbName() + \".\" + newt.getTableName()\n                  + \" already exists : \" + destPath);\n            }\n          } catch (IOException e) {\n            throw new InvalidOperationException(\"Unable to access new location \"\n                + destPath + \" for table \" + newt.getDbName() + \".\"\n                + newt.getTableName());\n          }\n          String oldTblLocPath = srcPath.toUri().getPath();\n          String newTblLocPath = destPath.toUri().getPath();\n\n          // also the location field in partition\n          List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n          for (Partition part : parts) {\n            String oldPartLoc = part.getSd().getLocation();\n            if (oldPartLoc.contains(oldTblLocPath)) {\n              URI oldUri = new Path(oldPartLoc).toUri();\n              String newPath = oldUri.getPath().replace(oldTblLocPath, newTblLocPath);\n              Path newPartLocPath = new Path(oldUri.getScheme(), oldUri.getAuthority(), newPath);\n              altps.add(ObjectPair.create(part, part.getSd().getLocation()));\n              part.getSd().setLocation(newPartLocPath.toString());\n              String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());\n              try {\n                //existing partition column stats is no longer valid, remove them\n                msdb.deletePartitionColumnStatistics(dbname, name, oldPartName, part.getValues(), null);\n              } catch (InvalidInputException iie) {\n                throw new InvalidOperationException(\"Unable to update partition stats in table rename.\" + iie);\n              }\n              msdb.alterPartition(dbname, name, part.getValues(), part);\n            }\n          }\n        }\n      } else if (MetaStoreUtils.requireCalStats(hiveConf, null, null, newt, environmentContext) &&\n        (newt.getPartitionKeysSize() == 0)) {\n          Database db = msdb.getDatabase(newt.getDbName());\n          // Update table stats. For partitioned table, we update stats in\n          // alterPartition()\n          MetaStoreUtils.updateTableStatsFast(db, newt, wh, false, true, environmentContext);\n      }\n\n      alterTableUpdateTableColumnStats(msdb, oldt, newt);\n      // commit the changes\n      success = msdb.commitTransaction();\n    } catch (InvalidObjectException e) {\n      LOG.debug(\"Failed to get object from Metastore \", e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table.\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } catch (NoSuchObjectException e) {\n      LOG.debug(\"Object not found in metastore \", e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && moveData) {\n        // change the file name in hdfs\n        // check that src exists otherwise there is no need to copy the data\n        // rename the src to destination\n        try {\n          if (srcFs.exists(srcPath) && !srcFs.rename(srcPath, destPath)) {\n            throw new IOException(\"Renaming \" + srcPath + \" to \" + destPath + \" failed\");\n          }\n        } catch (IOException e) {\n          LOG.error(\"Alter Table operation for \" + dbname + \".\" + name + \" failed.\", e);\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterTable(newt.getDbName(), newt.getTableName(), oldt);\n            for (ObjectPair<Partition, String> pair : altps) {\n              Partition part = pair.getFirst();\n              part.getSd().setLocation(pair.getSecond());\n              msdb.alterPartition(newt.getDbName(), name, part.getValues(), part);\n            }\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            // we should log this for manual rollback by administrator\n            LOG.error(\"Reverting metadata by HDFS operation failure failed During HDFS operation failed\", e1);\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should be renamed to \" + Warehouse.getQualifiedName(oldt));\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should have path \" + srcPath);\n            for (ObjectPair<Partition, String> pair : altps) {\n              LOG.error(\"Partition \" + Warehouse.getQualifiedName(pair.getFirst()) +\n                  \" should have path \" + pair.getSecond());\n            }\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Alter Table operation for \" + dbname + \".\" + name +\n            \" failed to move data due to: '\" + getSimpleMessage(e) + \"' See hive log file for details.\");\n        }\n      }\n    }\n    if (!success) {\n      throw new MetaException(\"Committing the alter table transaction was not successful.\");\n    }\n  }"
        ],
        [
            "TestSemanticAnalysis::testAlterTableRename()",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 -\n 252  \n 253  \n 254  \n 255 -\n 256  \n 257  \n 258  ",
            "  @Test\n  public void testAlterTableRename() throws CommandNeedRetryException, TException {\n    hcatDriver.run(\"drop table oldname\");\n    hcatDriver.run(\"drop table newname\");\n    hcatDriver.run(\"create table oldname (a int)\");\n    Table tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, \"oldname\");\n    assertTrue(tbl.getSd().getLocation().contains(\"oldname\"));\n\n    hcatDriver.run(\"alter table oldname rename to newNAME\");\n    tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, \"newname\");\n    assertTrue(tbl.getSd().getLocation().contains(\"newname\"));\n\n    hcatDriver.run(\"drop table newname\");\n  }",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 +\n 252  \n 253  \n 254  \n 255 +\n 256 +\n 257 +\n 258 +\n 259 +\n 260 +\n 261  \n 262  \n 263  ",
            "  @Test\n  public void testAlterTableRename() throws CommandNeedRetryException, TException {\n    hcatDriver.run(\"drop table oldname\");\n    hcatDriver.run(\"drop table newname\");\n    hcatDriver.run(\"create table oldname (a int)\");\n    Table tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, \"oldname\");\n    assertTrue(\"The old table location is: \" + tbl.getSd().getLocation(), tbl.getSd().getLocation().contains(\"oldname\"));\n\n    hcatDriver.run(\"alter table oldname rename to newNAME\");\n    tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, \"newname\");\n    // since the oldname table is not under its database (See HIVE-15059), the renamed oldname table will keep\n    // its location after HIVE-14909. I changed to check the existence of the newname table and its name instead\n    // of verifying its location\n    // assertTrue(tbl.getSd().getLocation().contains(\"newname\"));\n    assertTrue(tbl != null);\n    assertTrue(tbl.getTableName().equalsIgnoreCase(\"newname\"));\n\n    hcatDriver.run(\"drop table newname\");\n  }"
        ]
    ],
    "27fb87cfcea241c2d7961baf68e84ce97f2dee7a": [
        [
            "ColumnStatsUpdateTask::constructColumnStatsFromInput()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 -\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244 -\n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  ",
            "  private ColumnStatistics constructColumnStatsFromInput()\n      throws SemanticException, MetaException {\n\n    String dbName = SessionState.get().getCurrentDatabase();\n    ColumnStatsDesc desc = work.getColStats();\n    String tableName = desc.getTableName();\n    String partName = work.getPartName();\n    List<String> colName = desc.getColName();\n    List<String> colType = desc.getColType();\n\n    ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n\n    // grammar prohibits more than 1 column so we are guaranteed to have only 1\n    // element in this lists.\n\n    statsObj.setColName(colName.get(0));\n\n    statsObj.setColType(colType.get(0));\n    \n    ColumnStatisticsData statsData = new ColumnStatisticsData();\n    \n    String columnType = colType.get(0);\n\n    if (columnType.equalsIgnoreCase(\"long\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNullsIsSet(false);\n      longStats.setNumDVsIsSet(false);\n      longStats.setLowValueIsSet(false);\n      longStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          longStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          longStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          longStats.setLowValue(Long.parseLong(value));\n        } else if (fName.equals(\"highValue\")) {\n          longStats.setHighValue(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n\n      }\n      statsData.setLongStats(longStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"double\")) {\n      DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n      doubleStats.setNumNullsIsSet(false);\n      doubleStats.setNumDVsIsSet(false);\n      doubleStats.setLowValueIsSet(false);\n      doubleStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          doubleStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          doubleStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          doubleStats.setLowValue(Double.parseDouble(value));\n        } else if (fName.equals(\"highValue\")) {\n          doubleStats.setHighValue(Double.parseDouble(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDoubleStats(doubleStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"string\")) {\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setMaxColLenIsSet(false);\n      stringStats.setAvgColLenIsSet(false);\n      stringStats.setNumNullsIsSet(false);\n      stringStats.setNumDVsIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          stringStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          stringStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"avgColLen\")) {\n          stringStats.setAvgColLen(Double.parseDouble(value));\n        } else if (fName.equals(\"maxColLen\")) {\n          stringStats.setMaxColLen(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setStringStats(stringStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"boolean\")) {\n      BooleanColumnStatsData booleanStats = new BooleanColumnStatsData();\n      booleanStats.setNumNullsIsSet(false);\n      booleanStats.setNumTruesIsSet(false);\n      booleanStats.setNumFalsesIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          booleanStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numTrues\")) {\n          booleanStats.setNumTrues(Long.parseLong(value));\n        } else if (fName.equals(\"numFalses\")) {\n          booleanStats.setNumFalses(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setBooleanStats(booleanStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNullsIsSet(false);\n      binaryStats.setAvgColLenIsSet(false);\n      binaryStats.setMaxColLenIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          binaryStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"avgColLen\")) {\n          binaryStats.setAvgColLen(Double.parseDouble(value));\n        } else if (fName.equals(\"maxColLen\")) {\n          binaryStats.setMaxColLen(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setBinaryStats(binaryStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"decimal\")) {\n      DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n      decimalStats.setNumNullsIsSet(false);\n      decimalStats.setNumDVsIsSet(false);\n      decimalStats.setLowValueIsSet(false);\n      decimalStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          decimalStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          decimalStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          BigDecimal d = new BigDecimal(value);\n          decimalStats.setLowValue(new Decimal(ByteBuffer.wrap(d\n              .unscaledValue().toByteArray()), (short) d.scale()));\n        } else if (fName.equals(\"highValue\")) {\n          BigDecimal d = new BigDecimal(value);\n          decimalStats.setHighValue(new Decimal(ByteBuffer.wrap(d\n              .unscaledValue().toByteArray()), (short) d.scale()));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDecimalStats(decimalStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"date\")) {\n      DateColumnStatsData dateStats = new DateColumnStatsData();\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          dateStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          dateStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          // Date high/low value is stored as long in stats DB, but allow users to set high/low\n          // value using either date format (yyyy-mm-dd) or numeric format (days since epoch)\n          dateStats.setLowValue(readDateValue(value));\n        } else if (fName.equals(\"highValue\")) {\n          dateStats.setHighValue(readDateValue(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDateStats(dateStats);\n      statsObj.setStatsData(statsData);\n    } else {\n      throw new SemanticException(\"Unsupported type\");\n    }\n    String [] names = Utilities.getDbTableName(dbName, tableName);\n    ColumnStatisticsDesc statsDesc = getColumnStatsDesc(names[0], names[1],\n        partName, partName == null);\n    ColumnStatistics colStat = new ColumnStatistics();\n    colStat.setStatsDesc(statsDesc);\n    colStat.addToStatsObj(statsObj);\n    return colStat;\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102 +\n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 +\n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247 +\n 248 +\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "  private ColumnStatistics constructColumnStatsFromInput()\n      throws SemanticException, MetaException {\n\n    String dbName = SessionState.get().getCurrentDatabase();\n    ColumnStatsDesc desc = work.getColStats();\n    String tableName = desc.getTableName();\n    String partName = work.getPartName();\n    List<String> colName = desc.getColName();\n    List<String> colType = desc.getColType();\n\n    ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n\n    // grammar prohibits more than 1 column so we are guaranteed to have only 1\n    // element in this lists.\n\n    statsObj.setColName(colName.get(0));\n\n    statsObj.setColType(colType.get(0));\n    \n    ColumnStatisticsData statsData = new ColumnStatisticsData();\n    \n    String columnType = colType.get(0);\n\n    if (columnType.equalsIgnoreCase(\"long\") || columnType.equalsIgnoreCase(\"tinyint\")\n            || columnType.equalsIgnoreCase(\"smallint\") || columnType.equalsIgnoreCase(\"int\")\n            || columnType.equalsIgnoreCase(\"bigint\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNullsIsSet(false);\n      longStats.setNumDVsIsSet(false);\n      longStats.setLowValueIsSet(false);\n      longStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          longStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          longStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          longStats.setLowValue(Long.parseLong(value));\n        } else if (fName.equals(\"highValue\")) {\n          longStats.setHighValue(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n\n      }\n      statsData.setLongStats(longStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"double\") || columnType.equalsIgnoreCase(\"float\")) {\n      DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n      doubleStats.setNumNullsIsSet(false);\n      doubleStats.setNumDVsIsSet(false);\n      doubleStats.setLowValueIsSet(false);\n      doubleStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          doubleStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          doubleStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          doubleStats.setLowValue(Double.parseDouble(value));\n        } else if (fName.equals(\"highValue\")) {\n          doubleStats.setHighValue(Double.parseDouble(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDoubleStats(doubleStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"string\") || columnType.toLowerCase().startsWith(\"char\")\n              || columnType.toLowerCase().startsWith(\"varchar\")) { //char(x),varchar(x) types\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setMaxColLenIsSet(false);\n      stringStats.setAvgColLenIsSet(false);\n      stringStats.setNumNullsIsSet(false);\n      stringStats.setNumDVsIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          stringStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          stringStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"avgColLen\")) {\n          stringStats.setAvgColLen(Double.parseDouble(value));\n        } else if (fName.equals(\"maxColLen\")) {\n          stringStats.setMaxColLen(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setStringStats(stringStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"boolean\")) {\n      BooleanColumnStatsData booleanStats = new BooleanColumnStatsData();\n      booleanStats.setNumNullsIsSet(false);\n      booleanStats.setNumTruesIsSet(false);\n      booleanStats.setNumFalsesIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          booleanStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numTrues\")) {\n          booleanStats.setNumTrues(Long.parseLong(value));\n        } else if (fName.equals(\"numFalses\")) {\n          booleanStats.setNumFalses(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setBooleanStats(booleanStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNullsIsSet(false);\n      binaryStats.setAvgColLenIsSet(false);\n      binaryStats.setMaxColLenIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          binaryStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"avgColLen\")) {\n          binaryStats.setAvgColLen(Double.parseDouble(value));\n        } else if (fName.equals(\"maxColLen\")) {\n          binaryStats.setMaxColLen(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setBinaryStats(binaryStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.toLowerCase().startsWith(\"decimal\")) { //decimal(a,b) type\n      DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n      decimalStats.setNumNullsIsSet(false);\n      decimalStats.setNumDVsIsSet(false);\n      decimalStats.setLowValueIsSet(false);\n      decimalStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          decimalStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          decimalStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          BigDecimal d = new BigDecimal(value);\n          decimalStats.setLowValue(new Decimal(ByteBuffer.wrap(d\n              .unscaledValue().toByteArray()), (short) d.scale()));\n        } else if (fName.equals(\"highValue\")) {\n          BigDecimal d = new BigDecimal(value);\n          decimalStats.setHighValue(new Decimal(ByteBuffer.wrap(d\n              .unscaledValue().toByteArray()), (short) d.scale()));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDecimalStats(decimalStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"date\")\n            || columnType.equalsIgnoreCase(\"timestamp\")) {\n      DateColumnStatsData dateStats = new DateColumnStatsData();\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          dateStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          dateStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          // Date high/low value is stored as long in stats DB, but allow users to set high/low\n          // value using either date format (yyyy-mm-dd) or numeric format (days since epoch)\n          dateStats.setLowValue(readDateValue(value));\n        } else if (fName.equals(\"highValue\")) {\n          dateStats.setHighValue(readDateValue(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDateStats(dateStats);\n      statsObj.setStatsData(statsData);\n    } else {\n      throw new SemanticException(\"Unsupported type\");\n    }\n    String [] names = Utilities.getDbTableName(dbName, tableName);\n    ColumnStatisticsDesc statsDesc = getColumnStatsDesc(names[0], names[1],\n        partName, partName == null);\n    ColumnStatistics colStat = new ColumnStatistics();\n    colStat.setStatsDesc(statsDesc);\n    colStat.addToStatsObj(statsObj);\n    return colStat;\n  }"
        ]
    ],
    "89efd238e357046e1daf2a402500c262c667d3ec": [
        [
            "LlapOptionsProcessor::LlapOptionsProcessor()",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  ",
            "  @SuppressWarnings(\"static-access\")\n  public LlapOptionsProcessor() {\n\n    // set the number of instances on which llap should run\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_INSTANCES).withLongOpt(OPTION_INSTANCES)\n        .withDescription(\"Specify the number of instances to run this on\").create('i'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_NAME).withLongOpt(OPTION_NAME)\n        .withDescription(\"Cluster name for YARN registry\").create('n'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_DIRECTORY).withLongOpt(OPTION_DIRECTORY)\n        .withDescription(\"Temp directory for jars etc.\").create('d'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_ARGS).withLongOpt(OPTION_ARGS)\n        .withDescription(\"java arguments to the llap instance\").create('a'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LOGLEVEL).withLongOpt(OPTION_LOGLEVEL)\n        .withDescription(\"log levels for the llap instance\").create('l'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LOGGER).withLongOpt(OPTION_LOGGER)\n        .withDescription(\n            \"logger for llap instance ([\" + LogHelpers.LLAP_LOGGER_NAME_RFA + \"], \" +\n                LogHelpers.LLAP_LOGGER_NAME_QUERY_ROUTING + \", \" + LogHelpers.LLAP_LOGGER_NAME_CONSOLE)\n        .create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_CHAOS_MONKEY).withLongOpt(OPTION_CHAOS_MONKEY)\n        .withDescription(\"chaosmonkey interval\").create('m'));\n\n    options.addOption(OptionBuilder.hasArg(false).withArgName(OPTION_SLIDER_DEFAULT_KEYTAB).withLongOpt(OPTION_SLIDER_DEFAULT_KEYTAB)\n        .withDescription(\"try to set default settings for Slider AM keytab; mostly for dev testing\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_KEYTAB_DIR).withLongOpt(OPTION_SLIDER_KEYTAB_DIR)\n        .withDescription(\"Slider AM keytab directory on HDFS (where the headless user keytab is stored by Slider keytab installation, e.g. .slider/keytabs/llap)\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_KEYTAB).withLongOpt(OPTION_SLIDER_KEYTAB)\n        .withDescription(\"Slider AM keytab file name inside \" + OPTION_SLIDER_KEYTAB_DIR).create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_PRINCIPAL).withLongOpt(OPTION_SLIDER_PRINCIPAL)\n        .withDescription(\"Slider AM principal; should be the user running the cluster, e.g. hive@EXAMPLE.COM\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_EXECUTORS).withLongOpt(OPTION_EXECUTORS)\n        .withDescription(\"executor per instance\").create('e'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_CACHE).withLongOpt(OPTION_CACHE)\n        .withDescription(\"cache size per instance\").create('c'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SIZE).withLongOpt(OPTION_SIZE)\n        .withDescription(\"container size per instance\").create('s'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_XMX).withLongOpt(OPTION_XMX)\n        .withDescription(\"working memory size\").create('w'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LLAP_QUEUE)\n        .withLongOpt(OPTION_LLAP_QUEUE)\n        .withDescription(\"The queue within which LLAP will be started\").create('q'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_OUTPUT_DIR)\n        .withLongOpt(OPTION_OUTPUT_DIR)\n        .withDescription(\"Output directory for the generated scripts\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_AUXJARS).withLongOpt(OPTION_AUXJARS)\n        .withDescription(\"additional jars to package (by default, JSON SerDe jar is packaged\"\n            + \" if available)\").create('j'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_AUXHBASE).withLongOpt(OPTION_AUXHBASE)\n        .withDescription(\"whether to package the HBase jars (true by default)\").create('h'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_JAVA_HOME).withLongOpt(OPTION_JAVA_HOME)\n        .withDescription(\n            \"Path to the JRE/JDK. This should be installed at the same location on all cluster nodes ($JAVA_HOME, java.home by default)\")\n        .create());\n\n    // -hiveconf x=y\n    options.addOption(OptionBuilder.withValueSeparator().hasArgs(2).withArgName(\"property=value\")\n        .withLongOpt(OPTION_HIVECONF)\n        .withDescription(\"Use value for given property. Overridden by explicit parameters\")\n        .create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(\"b\")\n        .withLongOpt(OPTION_SLIDER_AM_CONTAINER_MB)\n        .withDescription(\"The size of the slider AppMaster container in MB\").create('b'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_IO_THREADS)\n        .withLongOpt(OPTION_IO_THREADS).withDescription(\"executor per instance\").create('t'));\n\n    // [-H|--help]\n    options.addOption(new Option(\"H\", \"help\", false, \"Print help information\"));\n  }",
            " 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214 +\n 215 +\n 216 +\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  ",
            "  @SuppressWarnings(\"static-access\")\n  public LlapOptionsProcessor() {\n\n    // set the number of instances on which llap should run\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_INSTANCES).withLongOpt(OPTION_INSTANCES)\n        .withDescription(\"Specify the number of instances to run this on\").create('i'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_NAME).withLongOpt(OPTION_NAME)\n        .withDescription(\"Cluster name for YARN registry\").create('n'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_DIRECTORY).withLongOpt(OPTION_DIRECTORY)\n        .withDescription(\"Temp directory for jars etc.\").create('d'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_ARGS).withLongOpt(OPTION_ARGS)\n        .withDescription(\"java arguments to the llap instance\").create('a'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LOGLEVEL).withLongOpt(OPTION_LOGLEVEL)\n        .withDescription(\"log levels for the llap instance\").create('l'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LOGGER).withLongOpt(OPTION_LOGGER)\n        .withDescription(\n            \"logger for llap instance ([\" + LogHelpers.LLAP_LOGGER_NAME_RFA + \"], \" +\n                LogHelpers.LLAP_LOGGER_NAME_QUERY_ROUTING + \", \" + LogHelpers.LLAP_LOGGER_NAME_CONSOLE)\n        .create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_CHAOS_MONKEY).withLongOpt(OPTION_CHAOS_MONKEY)\n        .withDescription(\"chaosmonkey interval\").create('m'));\n\n    options.addOption(OptionBuilder.hasArg(false).withArgName(OPTION_SLIDER_DEFAULT_KEYTAB).withLongOpt(OPTION_SLIDER_DEFAULT_KEYTAB)\n        .withDescription(\"try to set default settings for Slider AM keytab; mostly for dev testing\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_KEYTAB_DIR).withLongOpt(OPTION_SLIDER_KEYTAB_DIR)\n        .withDescription(\"Slider AM keytab directory on HDFS (where the headless user keytab is stored by Slider keytab installation, e.g. .slider/keytabs/llap)\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_KEYTAB).withLongOpt(OPTION_SLIDER_KEYTAB)\n        .withDescription(\"Slider AM keytab file name inside \" + OPTION_SLIDER_KEYTAB_DIR).create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_PRINCIPAL).withLongOpt(OPTION_SLIDER_PRINCIPAL)\n        .withDescription(\"Slider AM principal; should be the user running the cluster, e.g. hive@EXAMPLE.COM\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_PLACEMENT).withLongOpt(OPTION_SLIDER_PLACEMENT)\n        .withDescription(\"Slider placement policy; see slider documentation at https://slider.incubator.apache.org/docs/placement.html.\"\n          + \" 4 means anti-affinity (the default; unnecessary if LLAP is going to take more than half of the YARN capacity of a node), 0 is normal.\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_EXECUTORS).withLongOpt(OPTION_EXECUTORS)\n        .withDescription(\"executor per instance\").create('e'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_CACHE).withLongOpt(OPTION_CACHE)\n        .withDescription(\"cache size per instance\").create('c'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SIZE).withLongOpt(OPTION_SIZE)\n        .withDescription(\"container size per instance\").create('s'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_XMX).withLongOpt(OPTION_XMX)\n        .withDescription(\"working memory size\").create('w'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LLAP_QUEUE)\n        .withLongOpt(OPTION_LLAP_QUEUE)\n        .withDescription(\"The queue within which LLAP will be started\").create('q'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_OUTPUT_DIR)\n        .withLongOpt(OPTION_OUTPUT_DIR)\n        .withDescription(\"Output directory for the generated scripts\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_AUXJARS).withLongOpt(OPTION_AUXJARS)\n        .withDescription(\"additional jars to package (by default, JSON SerDe jar is packaged\"\n            + \" if available)\").create('j'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_AUXHBASE).withLongOpt(OPTION_AUXHBASE)\n        .withDescription(\"whether to package the HBase jars (true by default)\").create('h'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_JAVA_HOME).withLongOpt(OPTION_JAVA_HOME)\n        .withDescription(\n            \"Path to the JRE/JDK. This should be installed at the same location on all cluster nodes ($JAVA_HOME, java.home by default)\")\n        .create());\n\n    // -hiveconf x=y\n    options.addOption(OptionBuilder.withValueSeparator().hasArgs(2).withArgName(\"property=value\")\n        .withLongOpt(OPTION_HIVECONF)\n        .withDescription(\"Use value for given property. Overridden by explicit parameters\")\n        .create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(\"b\")\n        .withLongOpt(OPTION_SLIDER_AM_CONTAINER_MB)\n        .withDescription(\"The size of the slider AppMaster container in MB\").create('b'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_IO_THREADS)\n        .withLongOpt(OPTION_IO_THREADS).withDescription(\"executor per instance\").create('t'));\n\n    // [-H|--help]\n    options.addOption(new Option(\"H\", \"help\", false, \"Print help information\"));\n  }"
        ]
    ],
    "fc08e3be585f6c152895e88c626012e9e73017a9": [
        [
            "GenericUDAFBloomFilter::GenericUDAFBloomFilterEvaluator::getExpectedEntries()",
            " 247  \n 248  \n 249 -\n 250  \n 251 -\n 252  ",
            "    public long getExpectedEntries() {\n      if (sourceOperator != null && sourceOperator.getStatistics() != null) {\n        return sourceOperator.getStatistics().getNumRows();\n      }\n      return -1;\n    }",
            " 251  \n 252 +\n 253  \n 254 +\n 255 +\n 256 +\n 257 +\n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269 +\n 270 +\n 271 +\n 272 +\n 273  \n 274 +\n 275 +\n 276  ",
            "    public long getExpectedEntries() {\n      long expectedEntries = -1;\n      if (sourceOperator != null && sourceOperator.getStatistics() != null) {\n        Statistics stats = sourceOperator.getStatistics();\n        expectedEntries = stats.getNumRows();\n\n        // Use NumDistinctValues if possible\n        switch (stats.getColumnStatsState()) {\n          case COMPLETE:\n          case PARTIAL:\n            // There should only be column stats for one column, use if that is the case.\n            List<ColStatistics> colStats = stats.getColumnStats();\n            if (colStats.size() == 1) {\n              long ndv = colStats.get(0).getCountDistint();\n              if (ndv > 0) {\n                expectedEntries = ndv;\n              }\n            }\n            break;\n          default:\n            break;\n        }\n      }\n\n      return expectedEntries;\n    }"
        ]
    ],
    "016afe0d69f3a90290e3a127149430ad6d4c603f": [
        [
            "GenVectorCode::generateFilterColumnBetweenDynamicValue(String)",
            "1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421 -\n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433 -\n1434  \n1435  \n1436  \n1437  \n1438  \n1439 -\n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453 -\n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  ",
            "  private void generateFilterColumnBetweenDynamicValue(String[] tdesc) throws Exception {\n    String operandType = tdesc[1];\n    String optionalNot = tdesc[2];\n\n    String className = \"Filter\" + getCamelCaseType(operandType) + \"Column\" +\n      (optionalNot.equals(\"!\") ? \"Not\" : \"\") + \"BetweenDynamicValue\";\n\n    String typeName = getCamelCaseType(operandType);\n    String defaultValue;\n    String vectorType;\n    String getPrimitiveMethod;\n    String getValueMethod;\n    String conversionMethod;\n\n    if (operandType.equals(\"long\")) {\n      defaultValue = \"0\";\n      vectorType = \"long\";\n      getPrimitiveMethod = \"getLong\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"double\")) {\n      defaultValue = \"0\";\n      vectorType = \"double\";\n      getPrimitiveMethod = \"getDouble\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"decimal\")) {\n      defaultValue = \"null\";\n      vectorType = \"HiveDecimal\";\n      getPrimitiveMethod = \"getHiveDecimal\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"string\")) {\n      defaultValue = \"null\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getString\";\n      getValueMethod = \".getBytes()\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"char\")) {\n      defaultValue = \"null\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getHiveChar\";\n      getValueMethod = \".getStrippedValue().getBytes()\";  // Does vectorization use stripped char values?\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"varchar\")) {\n      defaultValue = \"null\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getHiveVarchar\";\n      getValueMethod = \".getValue().getBytes()\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"date\")) {\n      defaultValue = \"0\";\n      vectorType = \"long\";\n      getPrimitiveMethod = \"getDate\";\n      getValueMethod = \"\";\n      conversionMethod = \"DateWritable.dateToDays\";\n      // Special case - Date requires its own specific BetweenDynamicValue class, but derives from FilterLongColumnBetween\n      typeName = \"Long\";\n    } else if (operandType.equals(\"timestamp\")) {\n      defaultValue = \"null\";\n      vectorType = \"Timestamp\";\n      getPrimitiveMethod = \"getTimestamp\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else {\n      throw new IllegalArgumentException(\"Type \" + operandType + \" not supported\");\n    }\n\n    // Read the template into a string, expand it, and write it.\n    File templateFile = new File(joinPath(this.expressionTemplateDirectory, tdesc[0] + \".txt\"));\n    String templateString = readFile(templateFile);\n    templateString = templateString.replaceAll(\"<ClassName>\", className);\n    templateString = templateString.replaceAll(\"<TypeName>\", typeName);\n    templateString = templateString.replaceAll(\"<DefaultValue>\", defaultValue);\n    templateString = templateString.replaceAll(\"<VectorType>\", vectorType);\n    templateString = templateString.replaceAll(\"<GetPrimitiveMethod>\", getPrimitiveMethod);\n    templateString = templateString.replaceAll(\"<GetValueMethod>\", getValueMethod);\n    templateString = templateString.replaceAll(\"<ConversionMethod>\", conversionMethod);\n\n    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,\n        className, templateString);\n  }",
            "1394  \n1395  \n1396  \n1397  \n1398  \n1399  \n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421 +\n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433 +\n1434  \n1435  \n1436  \n1437  \n1438  \n1439 +\n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453 +\n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  \n1463  \n1464  \n1465  \n1466  \n1467  \n1468  \n1469  \n1470  \n1471  \n1472  \n1473  \n1474  \n1475  ",
            "  private void generateFilterColumnBetweenDynamicValue(String[] tdesc) throws Exception {\n    String operandType = tdesc[1];\n    String optionalNot = tdesc[2];\n\n    String className = \"Filter\" + getCamelCaseType(operandType) + \"Column\" +\n      (optionalNot.equals(\"!\") ? \"Not\" : \"\") + \"BetweenDynamicValue\";\n\n    String typeName = getCamelCaseType(operandType);\n    String defaultValue;\n    String vectorType;\n    String getPrimitiveMethod;\n    String getValueMethod;\n    String conversionMethod;\n\n    if (operandType.equals(\"long\")) {\n      defaultValue = \"0\";\n      vectorType = \"long\";\n      getPrimitiveMethod = \"getLong\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"double\")) {\n      defaultValue = \"0\";\n      vectorType = \"double\";\n      getPrimitiveMethod = \"getDouble\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"decimal\")) {\n      defaultValue = \"HiveDecimal.ZERO\";\n      vectorType = \"HiveDecimal\";\n      getPrimitiveMethod = \"getHiveDecimal\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"string\")) {\n      defaultValue = \"null\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getString\";\n      getValueMethod = \".getBytes()\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"char\")) {\n      defaultValue = \"new HiveChar(\\\"\\\", 1)\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getHiveChar\";\n      getValueMethod = \".getStrippedValue().getBytes()\";  // Does vectorization use stripped char values?\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"varchar\")) {\n      defaultValue = \"new HiveVarchar(\\\"\\\", 1)\";\n      vectorType = \"byte[]\";\n      getPrimitiveMethod = \"getHiveVarchar\";\n      getValueMethod = \".getValue().getBytes()\";\n      conversionMethod = \"\";\n    } else if (operandType.equals(\"date\")) {\n      defaultValue = \"0\";\n      vectorType = \"long\";\n      getPrimitiveMethod = \"getDate\";\n      getValueMethod = \"\";\n      conversionMethod = \"DateWritable.dateToDays\";\n      // Special case - Date requires its own specific BetweenDynamicValue class, but derives from FilterLongColumnBetween\n      typeName = \"Long\";\n    } else if (operandType.equals(\"timestamp\")) {\n      defaultValue = \"new Timestamp(0)\";\n      vectorType = \"Timestamp\";\n      getPrimitiveMethod = \"getTimestamp\";\n      getValueMethod = \"\";\n      conversionMethod = \"\";\n    } else {\n      throw new IllegalArgumentException(\"Type \" + operandType + \" not supported\");\n    }\n\n    // Read the template into a string, expand it, and write it.\n    File templateFile = new File(joinPath(this.expressionTemplateDirectory, tdesc[0] + \".txt\"));\n    String templateString = readFile(templateFile);\n    templateString = templateString.replaceAll(\"<ClassName>\", className);\n    templateString = templateString.replaceAll(\"<TypeName>\", typeName);\n    templateString = templateString.replaceAll(\"<DefaultValue>\", defaultValue);\n    templateString = templateString.replaceAll(\"<VectorType>\", vectorType);\n    templateString = templateString.replaceAll(\"<GetPrimitiveMethod>\", getPrimitiveMethod);\n    templateString = templateString.replaceAll(\"<GetValueMethod>\", getValueMethod);\n    templateString = templateString.replaceAll(\"<ConversionMethod>\", conversionMethod);\n\n    writeFile(templateFile.lastModified(), expressionOutputDirectory, expressionClassesDirectory,\n        className, templateString);\n  }"
        ]
    ],
    "301ebbb99f91ba60c1922a1cb2b07e2977cbfe15": [
        [
            "AMReporter::AMNodeInfo::AMNodeInfo(LlapNodeId,String,Token,QueryIdentifier,RetryPolicy,long,SocketFactory,Configuration)",
            " 441 -\n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 -\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  ",
            "    public AMNodeInfo(LlapNodeId amNodeId, String user,\n                      Token<JobTokenIdentifier> jobToken,\n                      QueryIdentifier currentQueryIdentifier,\n                      RetryPolicy retryPolicy,\n                      long timeout,\n                      SocketFactory socketFactory,\n                      Configuration conf) {\n      this.user = user;\n      this.jobToken = jobToken;\n      this.queryIdentifier = currentQueryIdentifier;\n      this.retryPolicy = retryPolicy;\n      this.timeout = timeout;\n      this.socketFactory = socketFactory;\n      this.conf = conf;\n      this.amNodeId = amNodeId;\n    }",
            " 441 +\n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 +\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  ",
            "    public AMNodeInfo(LlapNodeId amNodeId, String umbilicalUser,\n                      Token<JobTokenIdentifier> jobToken,\n                      QueryIdentifier currentQueryIdentifier,\n                      RetryPolicy retryPolicy,\n                      long timeout,\n                      SocketFactory socketFactory,\n                      Configuration conf) {\n      this.umbilicalUser = umbilicalUser;\n      this.jobToken = jobToken;\n      this.queryIdentifier = currentQueryIdentifier;\n      this.retryPolicy = retryPolicy;\n      this.timeout = timeout;\n      this.socketFactory = socketFactory;\n      this.conf = conf;\n      this.amNodeId = amNodeId;\n    }"
        ],
        [
            "TaskRunnerCallable::TaskRunnerCallable(SubmitWorkRequestProto,QueryFragmentInfo,Configuration,ExecutionContext,Map,Credentials,long,AMReporter,ConfParams,LlapDaemonExecutorMetrics,KilledTaskHandler,FragmentCompletionHandler,HadoopShim,TezTaskAttemptID,SignableVertexSpec,TezEvent,UserGroupInformation,SchedulerFragmentCompletingListener,SocketFactory)",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "  @VisibleForTesting\n  public TaskRunnerCallable(SubmitWorkRequestProto request, QueryFragmentInfo fragmentInfo,\n                            Configuration conf, ExecutionContext executionContext, Map<String, String> envMap,\n                            Credentials credentials, long memoryAvailable, AMReporter amReporter, ConfParams confParams,\n                            LlapDaemonExecutorMetrics metrics, KilledTaskHandler killedTaskHandler,\n                            FragmentCompletionHandler fragmentCompleteHandler, HadoopShim tezHadoopShim,\n                            TezTaskAttemptID attemptId, SignableVertexSpec vertex, TezEvent initialEvent,\n                            UserGroupInformation fsTaskUgi, SchedulerFragmentCompletingListener completionListener,\n                            SocketFactory socketFactory) {\n    this.request = request;\n    this.fragmentInfo = fragmentInfo;\n    this.conf = conf;\n    this.executionContext = executionContext;\n    this.envMap = envMap;\n    this.objectRegistry = new ObjectRegistryImpl();\n    this.credentials = credentials;\n    this.memoryAvailable = memoryAvailable;\n    this.confParams = confParams;\n    this.jobToken = TokenCache.getSessionToken(credentials);\n    this.vertex = vertex;\n    this.taskSpec = Converters.getTaskSpecfromProto(\n        vertex, request.getFragmentNumber(), request.getAttemptNumber(), attemptId);\n    this.amReporter = amReporter;\n    // Register with the AMReporter when the callable is setup. Unregister once it starts running.\n    if (amReporter != null && jobToken != null) {\n      this.amReporter.registerTask(request.getAmHost(), request.getAmPort(),\n          vertex.getUser(), jobToken, fragmentInfo.getQueryInfo().getQueryIdentifier(), attemptId);\n    }\n    this.metrics = metrics;\n    this.requestId = taskSpec.getTaskAttemptID().toString();\n    threadNameSuffix = constructThreadNameSuffix(taskSpec.getTaskAttemptID());\n\n    this.queryId = ContainerRunnerImpl\n        .constructUniqueQueryId(vertex.getHiveQueryId(),\n            fragmentInfo.getQueryInfo().getDagIdentifier());\n    this.killedTaskHandler = killedTaskHandler;\n    this.fragmentCompletionHanler = fragmentCompleteHandler;\n    this.tezHadoopShim = tezHadoopShim;\n    this.initialEvent = initialEvent;\n    this.fsTaskUgi = fsTaskUgi;\n    this.completionListener = completionListener;\n    this.socketFactory = socketFactory;\n  }",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "  @VisibleForTesting\n  public TaskRunnerCallable(SubmitWorkRequestProto request, QueryFragmentInfo fragmentInfo,\n                            Configuration conf, ExecutionContext executionContext, Map<String, String> envMap,\n                            Credentials credentials, long memoryAvailable, AMReporter amReporter, ConfParams confParams,\n                            LlapDaemonExecutorMetrics metrics, KilledTaskHandler killedTaskHandler,\n                            FragmentCompletionHandler fragmentCompleteHandler, HadoopShim tezHadoopShim,\n                            TezTaskAttemptID attemptId, SignableVertexSpec vertex, TezEvent initialEvent,\n                            UserGroupInformation fsTaskUgi, SchedulerFragmentCompletingListener completionListener,\n                            SocketFactory socketFactory) {\n    this.request = request;\n    this.fragmentInfo = fragmentInfo;\n    this.conf = conf;\n    this.executionContext = executionContext;\n    this.envMap = envMap;\n    this.objectRegistry = new ObjectRegistryImpl();\n    this.credentials = credentials;\n    this.memoryAvailable = memoryAvailable;\n    this.confParams = confParams;\n    this.jobToken = TokenCache.getSessionToken(credentials);\n    this.vertex = vertex;\n    this.taskSpec = Converters.getTaskSpecfromProto(\n        vertex, request.getFragmentNumber(), request.getAttemptNumber(), attemptId);\n    this.amReporter = amReporter;\n    // Register with the AMReporter when the callable is setup. Unregister once it starts running.\n    if (amReporter != null && jobToken != null) {\n      this.amReporter.registerTask(request.getAmHost(), request.getAmPort(),\n          vertex.getTokenIdentifier(), jobToken, fragmentInfo.getQueryInfo().getQueryIdentifier(), attemptId);\n    }\n    this.metrics = metrics;\n    this.requestId = taskSpec.getTaskAttemptID().toString();\n    threadNameSuffix = constructThreadNameSuffix(taskSpec.getTaskAttemptID());\n\n    this.queryId = ContainerRunnerImpl\n        .constructUniqueQueryId(vertex.getHiveQueryId(),\n            fragmentInfo.getQueryInfo().getDagIdentifier());\n    this.killedTaskHandler = killedTaskHandler;\n    this.fragmentCompletionHanler = fragmentCompleteHandler;\n    this.tezHadoopShim = tezHadoopShim;\n    this.initialEvent = initialEvent;\n    this.fsTaskUgi = fsTaskUgi;\n    this.completionListener = completionListener;\n    this.socketFactory = socketFactory;\n  }"
        ],
        [
            "AMReporter::registerTask(String,int,String,Token,QueryIdentifier,TezTaskAttemptID)",
            " 197 -\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 -\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  public void registerTask(String amLocation, int port, String user,\n      Token<JobTokenIdentifier> jobToken, QueryIdentifier queryIdentifier,\n      TezTaskAttemptID attemptId) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\n          \"Registering for heartbeat: {}, queryIdentifier={}, attemptId={}\",\n          (amLocation + \":\" + port), queryIdentifier, attemptId);\n    }\n    AMNodeInfo amNodeInfo;\n\n    // Since we don't have an explicit AM end signal yet - we're going to create\n    // and discard AMNodeInfo instances per query.\n    synchronized (knownAppMasters) {\n      LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);\n      amNodeInfo = knownAppMasters.get(queryIdentifier);\n      if (amNodeInfo == null) {\n        amNodeInfo = new AMNodeInfo(amNodeId, user, jobToken, queryIdentifier,\n            retryPolicy, retryTimeout, socketFactory, conf);\n        knownAppMasters.put(queryIdentifier, amNodeInfo);\n        // Add to the queue only the first time this is registered, and on\n        // subsequent instances when it's taken off the queue.\n        amNodeInfo.setNextHeartbeatTime(System.currentTimeMillis() + heartbeatInterval);\n        pendingHeartbeatQueeu.add(amNodeInfo);\n        // AMNodeInfo will only be cleared when a queryComplete is received for this query, or\n        // when we detect a failure on the AM side (failure to heartbeat).\n        // A single queueLookupCallable is added here. We have to make sure one instance stays\n        // in the queue till the query completes.\n      }\n      amNodeInfo.addTaskAttempt(attemptId);\n    }\n  }",
            " 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  public void registerTask(String amLocation, int port, String umbilicalUser,\n      Token<JobTokenIdentifier> jobToken, QueryIdentifier queryIdentifier,\n      TezTaskAttemptID attemptId) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\n          \"Registering for heartbeat: {}, queryIdentifier={}, attemptId={}\",\n          (amLocation + \":\" + port), queryIdentifier, attemptId);\n    }\n    AMNodeInfo amNodeInfo;\n\n    // Since we don't have an explicit AM end signal yet - we're going to create\n    // and discard AMNodeInfo instances per query.\n    synchronized (knownAppMasters) {\n      LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);\n      amNodeInfo = knownAppMasters.get(queryIdentifier);\n      if (amNodeInfo == null) {\n        amNodeInfo = new AMNodeInfo(amNodeId, umbilicalUser, jobToken, queryIdentifier,\n            retryPolicy, retryTimeout, socketFactory, conf);\n        knownAppMasters.put(queryIdentifier, amNodeInfo);\n        // Add to the queue only the first time this is registered, and on\n        // subsequent instances when it's taken off the queue.\n        amNodeInfo.setNextHeartbeatTime(System.currentTimeMillis() + heartbeatInterval);\n        pendingHeartbeatQueeu.add(amNodeInfo);\n        // AMNodeInfo will only be cleared when a queryComplete is received for this query, or\n        // when we detect a failure on the AM side (failure to heartbeat).\n        // A single queueLookupCallable is added here. We have to make sure one instance stays\n        // in the queue till the query completes.\n      }\n      amNodeInfo.addTaskAttempt(attemptId);\n    }\n  }"
        ],
        [
            "ContainerRunnerImpl::KilledTaskHandlerImpl::taskKilled(String,int,String,Token,QueryIdentifier,TezTaskAttemptID)",
            " 510  \n 511 -\n 512  \n 513  \n 514 -\n 515  ",
            "    @Override\n    public void taskKilled(String amLocation, int port, String user,\n                           Token<JobTokenIdentifier> jobToken, QueryIdentifier queryIdentifier,\n                           TezTaskAttemptID taskAttemptId) {\n      amReporter.taskKilled(amLocation, port, user, jobToken, queryIdentifier, taskAttemptId);\n    }",
            " 510  \n 511 +\n 512  \n 513  \n 514 +\n 515  ",
            "    @Override\n    public void taskKilled(String amLocation, int port, String umbilicalUser,\n                           Token<JobTokenIdentifier> jobToken, QueryIdentifier queryIdentifier,\n                           TezTaskAttemptID taskAttemptId) {\n      amReporter.taskKilled(amLocation, port, umbilicalUser, jobToken, queryIdentifier, taskAttemptId);\n    }"
        ],
        [
            "TaskRunnerCallable::reportTaskKilled()",
            " 375  \n 376  \n 377  \n 378  \n 379  \n 380 -\n 381  \n 382  ",
            "  /**\n   * Inform the AM that this task has been killed.\n   */\n  public void reportTaskKilled() {\n    killedTaskHandler\n        .taskKilled(request.getAmHost(), request.getAmPort(), vertex.getUser(), jobToken,\n            fragmentInfo.getQueryInfo().getQueryIdentifier(), taskSpec.getTaskAttemptID());\n  }",
            " 375  \n 376  \n 377  \n 378  \n 379  \n 380 +\n 381  \n 382  ",
            "  /**\n   * Inform the AM that this task has been killed.\n   */\n  public void reportTaskKilled() {\n    killedTaskHandler\n        .taskKilled(request.getAmHost(), request.getAmPort(), vertex.getTokenIdentifier(), jobToken,\n            fragmentInfo.getQueryInfo().getQueryIdentifier(), taskSpec.getTaskAttemptID());\n  }"
        ],
        [
            "AMReporter::AMNodeInfo::getUmbilical()",
            " 458  \n 459  \n 460  \n 461  \n 462  \n 463 -\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  ",
            "    synchronized LlapTaskUmbilicalProtocol getUmbilical() throws IOException, InterruptedException {\n      if (umbilical == null) {\n        final InetSocketAddress address =\n            NetUtils.createSocketAddrForHost(amNodeId.getHostname(), amNodeId.getPort());\n        SecurityUtil.setTokenService(this.jobToken, address);\n        UserGroupInformation ugi = UserGroupInformation.createRemoteUser(user);\n        ugi.addToken(jobToken);\n        umbilical = ugi.doAs(new PrivilegedExceptionAction<LlapTaskUmbilicalProtocol>() {\n          @Override\n          public LlapTaskUmbilicalProtocol run() throws Exception {\n            return RPC\n                .getProxy(LlapTaskUmbilicalProtocol.class, LlapTaskUmbilicalProtocol.versionID,\n                    address, UserGroupInformation.getCurrentUser(), conf, socketFactory,\n                    (int) timeout);\n          }\n        });\n      }\n      return umbilical;\n    }",
            " 458  \n 459  \n 460  \n 461  \n 462  \n 463 +\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  ",
            "    synchronized LlapTaskUmbilicalProtocol getUmbilical() throws IOException, InterruptedException {\n      if (umbilical == null) {\n        final InetSocketAddress address =\n            NetUtils.createSocketAddrForHost(amNodeId.getHostname(), amNodeId.getPort());\n        SecurityUtil.setTokenService(this.jobToken, address);\n        UserGroupInformation ugi = UserGroupInformation.createRemoteUser(umbilicalUser);\n        ugi.addToken(jobToken);\n        umbilical = ugi.doAs(new PrivilegedExceptionAction<LlapTaskUmbilicalProtocol>() {\n          @Override\n          public LlapTaskUmbilicalProtocol run() throws Exception {\n            return RPC\n                .getProxy(LlapTaskUmbilicalProtocol.class, LlapTaskUmbilicalProtocol.versionID,\n                    address, UserGroupInformation.getCurrentUser(), conf, socketFactory,\n                    (int) timeout);\n          }\n        });\n      }\n      return umbilical;\n    }"
        ],
        [
            "AMReporter::taskKilled(String,int,String,Token,QueryIdentifier,TezTaskAttemptID)",
            " 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 -\n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "  public void taskKilled(String amLocation, int port, String user, Token<JobTokenIdentifier> jobToken,\n                         final QueryIdentifier queryIdentifier, final TezTaskAttemptID taskAttemptId) {\n    LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);\n    AMNodeInfo amNodeInfo;\n    synchronized (knownAppMasters) {\n      amNodeInfo = knownAppMasters.get(queryIdentifier);\n      if (amNodeInfo == null) {\n        amNodeInfo = new AMNodeInfo(amNodeId, user, jobToken, queryIdentifier, retryPolicy, retryTimeout, socketFactory,\n          conf);\n      }\n    }\n\n    // Even if the service hasn't started up. It's OK to make this invocation since this will\n    // only happen after the AtomicReference address has been populated. Not adding an additional check.\n    ListenableFuture<Void> future =\n        executor.submit(new KillTaskCallable(taskAttemptId, amNodeInfo));\n    Futures.addCallback(future, new FutureCallback<Void>() {\n      @Override\n      public void onSuccess(Void result) {\n        LOG.info(\"Sent taskKilled for {}\", taskAttemptId);\n      }\n\n      @Override\n      public void onFailure(Throwable t) {\n        LOG.warn(\"Failed to send taskKilled for {}. The attempt will likely time out.\",\n            taskAttemptId);\n      }\n    });\n  }",
            " 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 +\n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "  public void taskKilled(String amLocation, int port, String umbilicalUser, Token<JobTokenIdentifier> jobToken,\n                         final QueryIdentifier queryIdentifier, final TezTaskAttemptID taskAttemptId) {\n    LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);\n    AMNodeInfo amNodeInfo;\n    synchronized (knownAppMasters) {\n      amNodeInfo = knownAppMasters.get(queryIdentifier);\n      if (amNodeInfo == null) {\n        amNodeInfo = new AMNodeInfo(amNodeId, umbilicalUser, jobToken, queryIdentifier, retryPolicy, retryTimeout, socketFactory,\n          conf);\n      }\n    }\n\n    // Even if the service hasn't started up. It's OK to make this invocation since this will\n    // only happen after the AtomicReference address has been populated. Not adding an additional check.\n    ListenableFuture<Void> future =\n        executor.submit(new KillTaskCallable(taskAttemptId, amNodeInfo));\n    Futures.addCallback(future, new FutureCallback<Void>() {\n      @Override\n      public void onSuccess(Void result) {\n        LOG.info(\"Sent taskKilled for {}\", taskAttemptId);\n      }\n\n      @Override\n      public void onFailure(Throwable t) {\n        LOG.warn(\"Failed to send taskKilled for {}. The attempt will likely time out.\",\n            taskAttemptId);\n      }\n    });\n  }"
        ]
    ],
    "3c1dfe379ec1fcaa688b064561a4daca098ed2b8": [
        [
            "AvroSerDe::initialize(Configuration,Properties)",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131  \n 132  \n 133  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    // Reset member variables so we don't get in a half-constructed state\n    if (schema != null) {\n      LOG.debug(\"Resetting already initialized AvroSerDe\");\n    }\n\n    schema = null;\n    oi = null;\n    columnNames = null;\n    columnTypes = null;\n\n    final String columnNameProperty = properties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = properties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n    final String columnCommentProperty = properties.getProperty(LIST_COLUMN_COMMENTS,\"\");\n    final String columnNameDelimiter = properties.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? properties\n        .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);\n        \n    if (hasExternalSchema(properties)\n        || columnNameProperty == null || columnNameProperty.isEmpty()\n        || columnTypeProperty == null || columnTypeProperty.isEmpty()) {\n      schema = determineSchemaOrReturnErrorSchema(configuration, properties);\n    } else {\n      // Get column names and sort order\n      columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));\n      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n      schema = getSchemaFromCols(properties, columnNames, columnTypes, columnCommentProperty);\n      properties.setProperty(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName(), schema.toString());\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Avro schema is \" + schema);\n    }\n\n    if (configuration == null) {\n      LOG.debug(\"Configuration null, not inserting schema\");\n    } else {\n      configuration.set(\n          AvroSerdeUtils.AvroTableProperties.AVRO_SERDE_SCHEMA.getPropName(), schema.toString(false));\n    }\n\n    badSchema = schema.equals(SchemaResolutionProblem.SIGNAL_BAD_SCHEMA);\n\n    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(schema);\n    this.columnNames = aoig.getColumnNames();\n    this.columnTypes = aoig.getColumnTypes();\n    this.oi = aoig.getObjectInspector();\n  }",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111 +\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    // Reset member variables so we don't get in a half-constructed state\n    if (schema != null) {\n      LOG.debug(\"Resetting already initialized AvroSerDe\");\n    }\n\n    schema = null;\n    oi = null;\n    columnNames = null;\n    columnTypes = null;\n\n    final String columnNameProperty = properties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = properties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n    final String columnCommentProperty = properties.getProperty(LIST_COLUMN_COMMENTS,\"\");\n    final String columnNameDelimiter = properties.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? properties\n        .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);\n        \n    if (hasExternalSchema(properties)\n        || columnNameProperty == null || columnNameProperty.isEmpty()\n        || columnTypeProperty == null || columnTypeProperty.isEmpty()) {\n      schema = determineSchemaOrReturnErrorSchema(configuration, properties);\n    } else {\n      // Get column names and sort order\n      columnNames = StringInternUtils.internStringsInList(\n          Arrays.asList(columnNameProperty.split(columnNameDelimiter)));\n      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n      schema = getSchemaFromCols(properties, columnNames, columnTypes, columnCommentProperty);\n      properties.setProperty(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName(), schema.toString());\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Avro schema is \" + schema);\n    }\n\n    if (configuration == null) {\n      LOG.debug(\"Configuration null, not inserting schema\");\n    } else {\n      configuration.set(\n          AvroSerdeUtils.AvroTableProperties.AVRO_SERDE_SCHEMA.getPropName(), schema.toString(false));\n    }\n\n    badSchema = schema.equals(SchemaResolutionProblem.SIGNAL_BAD_SCHEMA);\n\n    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(schema);\n    this.columnNames = StringInternUtils.internStringsInList(aoig.getColumnNames());\n    this.columnTypes = aoig.getColumnTypes();\n    this.oi = aoig.getObjectInspector();\n  }"
        ],
        [
            "ObjectInspectorFactory::getStandardStructObjectInspector(List,List,List)",
            " 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321 -\n 322  \n 323  \n 324  \n 325 -\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  ",
            "  public static StandardStructObjectInspector getStandardStructObjectInspector(\n      List<String> structFieldNames,\n      List<ObjectInspector> structFieldObjectInspectors,\n      List<String> structComments) {\n    ArrayList<List<?>> signature = new ArrayList<List<?>>(3);\n    signature.add(structFieldNames);\n    signature.add(structFieldObjectInspectors);\n    if(structComments != null) {\n      signature.add(structComments);\n    }\n    StandardStructObjectInspector result = cachedStandardStructObjectInspector.get(signature);\n    if(result == null) {\n      result = new StandardStructObjectInspector(structFieldNames, structFieldObjectInspectors, structComments);\n      StandardStructObjectInspector prev =\n        cachedStandardStructObjectInspector.putIfAbsent(signature, result);\n      if (prev != null) {\n        result = prev;\n      }\n    }\n    return result;\n  }",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320 +\n 321  \n 322  \n 323 +\n 324 +\n 325  \n 326  \n 327  \n 328 +\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  ",
            "  public static StandardStructObjectInspector getStandardStructObjectInspector(\n      List<String> structFieldNames,\n      List<ObjectInspector> structFieldObjectInspectors,\n      List<String> structComments) {\n    ArrayList<List<?>> signature = new ArrayList<List<?>>(3);\n    StringInternUtils.internStringsInList(structFieldNames);\n    signature.add(structFieldNames);\n    signature.add(structFieldObjectInspectors);\n    if (structComments != null) {\n      StringInternUtils.internStringsInList(structComments);\n      signature.add(structComments);\n    }\n    StandardStructObjectInspector result = cachedStandardStructObjectInspector.get(signature);\n    if (result == null) {\n      result = new StandardStructObjectInspector(structFieldNames, structFieldObjectInspectors, structComments);\n      StandardStructObjectInspector prev =\n        cachedStandardStructObjectInspector.putIfAbsent(signature, result);\n      if (prev != null) {\n        result = prev;\n      }\n    }\n    return result;\n  }"
        ],
        [
            "ColumnInfo::setTypeName(String)",
            " 116  \n 117 -\n 118  ",
            "  public void setTypeName(String typeName) {\n    this.typeName = typeName;\n  }",
            " 117  \n 118 +\n 119  ",
            "  public void setTypeName(String typeName) {\n    this.typeName = StringInternUtils.internIfNotNull(typeName);\n  }"
        ],
        [
            "ColumnInfo::ColumnInfo(String,ObjectInspector,String,boolean,boolean)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  ",
            "  public ColumnInfo(String internalName, ObjectInspector objectInspector,\n      String tabAlias, boolean isVirtualCol, boolean isHiddenVirtualCol) {\n    this.internalName = internalName;\n    this.objectInspector = objectInspector;\n    this.tabAlias = tabAlias;\n    this.isVirtualCol = isVirtualCol;\n    this.isHiddenVirtualCol = isHiddenVirtualCol;\n    this.typeName = getType().getTypeName();\n  }",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101  ",
            "  public ColumnInfo(String internalName, ObjectInspector objectInspector,\n      String tabAlias, boolean isVirtualCol, boolean isHiddenVirtualCol) {\n    this.internalName = internalName;\n    this.objectInspector = objectInspector;\n    this.tabAlias = tabAlias;\n    this.isVirtualCol = isVirtualCol;\n    this.isHiddenVirtualCol = isHiddenVirtualCol;\n    setTypeName(getType().getTypeName());\n  }"
        ],
        [
            "StandardStructObjectInspector::MyField::MyField(int,String,ObjectInspector)",
            "  55  \n  56  \n  57  \n  58 -\n  59  \n  60  ",
            "    public MyField(int fieldID, String fieldName,\n        ObjectInspector fieldObjectInspector) {\n      this.fieldID = fieldID;\n      this.fieldName = fieldName.toLowerCase();\n      this.fieldObjectInspector = fieldObjectInspector;\n    }",
            "  55  \n  56  \n  57  \n  58 +\n  59  \n  60  ",
            "    public MyField(int fieldID, String fieldName,\n        ObjectInspector fieldObjectInspector) {\n      this.fieldID = fieldID;\n      this.fieldName = fieldName.toLowerCase().intern();\n      this.fieldObjectInspector = fieldObjectInspector;\n    }"
        ],
        [
            "ColumnInfo::setAlias(String)",
            " 162  \n 163 -\n 164  ",
            "  public void setAlias(String col_alias) {\n    alias = col_alias;\n  }",
            " 163  \n 164 +\n 165  ",
            "  public void setAlias(String col_alias) {\n    alias = StringInternUtils.internIfNotNull(col_alias);\n  }"
        ],
        [
            "StringInternUtils::internStringsInList(List)",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 -\n 111 -\n 112 -\n 113 -\n 114  \n 115  \n 116  ",
            "  /**\n   * This method interns all the strings in the given list in place. That is,\n   * it iterates over the list, replaces each element with the interned copy\n   * and eventually returns the same list.\n   */\n  public static List<String> internStringsInList(List<String> list) {\n    if (list != null) {\n      ListIterator<String> it = list.listIterator();\n      while (it.hasNext()) {\n        it.set(it.next().intern());\n      }\n    }\n    return list;\n  }",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121 +\n 122  \n 123  \n 124  ",
            "  /**\n   * This method interns all the strings in the given list in place. That is,\n   * it iterates over the list, replaces each element with the interned copy\n   * and eventually returns the same list.\n   *\n   * Note that the provided List implementation should return an iterator\n   * (via list.listIterator()) method, and that iterator should implement\n   * the set(Object) method. That's what all List implementations in the JDK\n   * provide. However, if some custom List implementation doesn't have this\n   * functionality, this method will return without interning its elements.\n   */\n  public static List<String> internStringsInList(List<String> list) {\n    if (list != null) {\n      try {\n        ListIterator<String> it = list.listIterator();\n        while (it.hasNext()) {\n          it.set(it.next().intern());\n        }\n      } catch (UnsupportedOperationException e) { } // set() not implemented - ignore\n    }\n    return list;\n  }"
        ],
        [
            "LineageInfo::Dependency::setExpr(String)",
            " 402  \n 403  \n 404  \n 405  \n 406 -\n 407  ",
            "    /**\n     * @param expr the expr to set\n     */\n    public void setExpr(String expr) {\n      this.expr = expr;\n    }",
            " 403  \n 404  \n 405  \n 406  \n 407 +\n 408  ",
            "    /**\n     * @param expr the expr to set\n     */\n    public void setExpr(String expr) {\n      this.expr = StringInternUtils.internIfNotNull(expr);\n    }"
        ],
        [
            "TableDesc::setProperties(Properties)",
            " 131  \n 132  \n 133  ",
            "  public void setProperties(final Properties properties) {\n    this.properties = properties;\n  }",
            " 132  \n 133 +\n 134  \n 135  ",
            "  public void setProperties(final Properties properties) {\n    StringInternUtils.internValuesInMap((Map) properties);\n    this.properties = properties;\n  }"
        ],
        [
            "ExprNodeConstantDesc::setValue(Object)",
            "  78  \n  79  \n  80  \n  81  ",
            "  public void setValue(Object value) {\n    // Kryo setter\n    this.value = value;\n  }",
            "  79  \n  80  \n  81 +\n  82 +\n  83 +\n  84  \n  85  ",
            "  public void setValue(Object value) {\n    // Kryo setter\n    if (value instanceof String) {\n      value = StringInternUtils.internIfNotNull((String) value);\n    }\n    this.value = value;\n  }"
        ],
        [
            "ExprNodeConstantDesc::ExprNodeConstantDesc(TypeInfo,Object)",
            "  68  \n  69  \n  70 -\n  71  ",
            "  public ExprNodeConstantDesc(TypeInfo typeInfo, Object value) {\n    super(typeInfo);\n    this.value = value;\n  }",
            "  69  \n  70  \n  71 +\n  72  ",
            "  public ExprNodeConstantDesc(TypeInfo typeInfo, Object value) {\n    super(typeInfo);\n    setValue(value);\n  }"
        ],
        [
            "TableDesc::TableDesc(Class,Class,Properties)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 -\n  71  ",
            "  /**\n   * @param inputFormatClass\n   * @param outputFormatClass\n   * @param properties must contain serde class name associate with this table.\n   */\n  public TableDesc(\n      final Class<? extends InputFormat> inputFormatClass,\n      final Class<?> outputFormatClass, final Properties properties) {\n    this.inputFileFormatClass = inputFormatClass;\n    outputFileFormatClass = HiveFileFormatUtils\n        .getOutputFormatSubstitute(outputFormatClass);\n    this.properties = properties;\n  }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72  ",
            "  /**\n   * @param inputFormatClass\n   * @param outputFormatClass\n   * @param properties must contain serde class name associate with this table.\n   */\n  public TableDesc(\n      final Class<? extends InputFormat> inputFormatClass,\n      final Class<?> outputFormatClass, final Properties properties) {\n    this.inputFileFormatClass = inputFormatClass;\n    outputFileFormatClass = HiveFileFormatUtils\n        .getOutputFormatSubstitute(outputFormatClass);\n    setProperties(properties);\n  }"
        ]
    ],
    "f0812efdb82cb42a50bfe390413f6413d0ed5ab2": [
        [
            "TestSchemaTool::testValidateSchemaTables()",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "  /**\n   * Test to validate that all tables exist in the HMS metastore.\n   * @throws Exception\n   */\n  public void testValidateSchemaTables() throws Exception {\n    schemaTool.doInit(\"2.0.0\");\n\n    boolean isValid = (boolean)schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n\n    // upgrade to 2.2.0 schema and re-validate\n    schemaTool.doUpgrade(\"2.2.0\");\n    isValid = (boolean)schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n\n    // Simulate a missing table scenario by renaming a couple of tables\n    String[] scripts = new String[] {\n        \"RENAME TABLE SEQUENCE_TABLE to SEQUENCE_TABLE_RENAMED\",\n        \"RENAME TABLE NUCLEUS_TABLES to NUCLEUS_TABLES_RENAMED\"\n    };\n\n    File scriptFile = generateTestScript(scripts);\n    schemaTool.runBeeLine(scriptFile.getPath());\n    isValid = schemaTool.validateSchemaTables(conn);\n    assertFalse(isValid);\n\n    // Restored the renamed tables\n    scripts = new String[] {\n        \"RENAME TABLE SEQUENCE_TABLE_RENAMED to SEQUENCE_TABLE\",\n        \"RENAME TABLE NUCLEUS_TABLES_RENAMED to NUCLEUS_TABLES\"\n    };\n\n    scriptFile = generateTestScript(scripts);\n    schemaTool.runBeeLine(scriptFile.getPath());\n    isValid = schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n   }",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "  /**\n   * Test to validate that all tables exist in the HMS metastore.\n   * @throws Exception\n   */\n  public void testValidateSchemaTables() throws Exception {\n    schemaTool.doInit(\"2.0.0\");\n\n    boolean isValid = (boolean)schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n\n    // upgrade from 2.0.0 schema and re-validate\n    schemaTool.doUpgrade(\"2.0.0\");\n    isValid = (boolean)schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n\n    // Simulate a missing table scenario by renaming a couple of tables\n    String[] scripts = new String[] {\n        \"RENAME TABLE SEQUENCE_TABLE to SEQUENCE_TABLE_RENAMED\",\n        \"RENAME TABLE NUCLEUS_TABLES to NUCLEUS_TABLES_RENAMED\"\n    };\n\n    File scriptFile = generateTestScript(scripts);\n    schemaTool.runBeeLine(scriptFile.getPath());\n    isValid = schemaTool.validateSchemaTables(conn);\n    assertFalse(isValid);\n\n    // Restored the renamed tables\n    scripts = new String[] {\n        \"RENAME TABLE SEQUENCE_TABLE_RENAMED to SEQUENCE_TABLE\",\n        \"RENAME TABLE NUCLEUS_TABLES_RENAMED to NUCLEUS_TABLES\"\n    };\n\n    scriptFile = generateTestScript(scripts);\n    schemaTool.runBeeLine(scriptFile.getPath());\n    isValid = schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n   }"
        ]
    ],
    "40fe0d7e03a03bf6082ff00f17322756e6f00ea9": [
        [
            "TezJobMonitor::monitorExecution()",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207 -\n 208  \n 209 -\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  ",
            "  public int monitorExecution() {\n    boolean done = false;\n    boolean success = false;\n    int failedCounter = 0;\n    int rc = 0;\n    DAGStatus status = null;\n    Map<String, Progress> vertexProgressMap = null;\n\n\n    long monitorStartTime = System.currentTimeMillis();\n    synchronized (shutdownList) {\n      shutdownList.add(dagClient);\n    }\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_RUN_DAG);\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_SUBMIT_TO_RUNNING);\n    DAGStatus.State lastState = null;\n    boolean running = false;\n\n    while (true) {\n\n      try {\n        if (context != null) {\n          context.checkHeartbeaterLockException();\n        }\n\n        status = dagClient.getDAGStatus(new HashSet<StatusGetOpts>(), CHECK_INTERVAL);\n        vertexProgressMap = status.getVertexProgress();\n        DAGStatus.State state = status.getState();\n\n        if (state != lastState || state == RUNNING) {\n          lastState = state;\n\n          switch (state) {\n            case SUBMITTED:\n              console.printInfo(\"Status: Submitted\");\n              break;\n            case INITING:\n              console.printInfo(\"Status: Initializing\");\n              this.executionStartTime = System.currentTimeMillis();\n              break;\n            case RUNNING:\n              if (!running) {\n                perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_SUBMIT_TO_RUNNING);\n                console.printInfo(\"Status: Running (\" + dagClient.getExecutionContext() + \")\\n\");\n                this.executionStartTime = System.currentTimeMillis();\n                running = true;\n              }\n              updateFunction.update(status, vertexProgressMap);\n              break;\n            case SUCCEEDED:\n              if (!running) {\n                this.executionStartTime = monitorStartTime;\n              }\n              updateFunction.update(status, vertexProgressMap);\n              success = true;\n              running = false;\n              done = true;\n              break;\n            case KILLED:\n              if (!running) {\n                this.executionStartTime = monitorStartTime;\n              }\n              updateFunction.update(status, vertexProgressMap);\n              console.printInfo(\"Status: Killed\");\n              running = false;\n              done = true;\n              rc = 1;\n              break;\n            case FAILED:\n            case ERROR:\n              if (!running) {\n                this.executionStartTime = monitorStartTime;\n              }\n              updateFunction.update(status, vertexProgressMap);\n              console.printError(\"Status: Failed\");\n              running = false;\n              done = true;\n              rc = 2;\n              break;\n          }\n        }\n      } catch (Exception e) {\n        console.printInfo(\"Exception: \" + e.getMessage());\n        boolean isInterrupted = hasInterruptedException(e);\n        if (isInterrupted || (++failedCounter % MAX_RETRY_INTERVAL / CHECK_INTERVAL == 0)) {\n          try {\n            console.printInfo(\"Killing DAG...\");\n            dagClient.tryKillDAG();\n          } catch (IOException | TezException tezException) {\n            // best effort\n          }\n          console\n              .printError(\"Execution has failed. stack trace: \" + ExceptionUtils.getStackTrace(e));\n          rc = 1;\n          done = true;\n        } else {\n          console.printInfo(\"Retrying...\");\n        }\n      } finally {\n        if (done) {\n          if (rc != 0 && status != null) {\n            for (String diag : status.getDiagnostics()) {\n              console.printError(diag);\n              diagnostics.append(diag);\n            }\n          }\n          synchronized (shutdownList) {\n            shutdownList.remove(dagClient);\n          }\n          break;\n        }\n      }\n    }\n\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_RUN_DAG);\n    printSummary(success, vertexProgressMap);\n    return rc;\n  }",
            " 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155 +\n 156  \n 157  \n 158  \n 159 +\n 160 +\n 161 +\n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179 +\n 180 +\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220 +\n 221 +\n 222 +\n 223 +\n 224 +\n 225  \n 226 +\n 227 +\n 228 +\n 229 +\n 230 +\n 231 +\n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  ",
            "  public int monitorExecution() {\n    boolean done = false;\n    boolean success = false;\n    int failedCounter = 0;\n    final StopWatch failureTimer = new StopWatch();\n    int rc = 0;\n    DAGStatus status = null;\n    Map<String, Progress> vertexProgressMap = null;\n\n\n    long monitorStartTime = System.currentTimeMillis();\n    synchronized (shutdownList) {\n      shutdownList.add(dagClient);\n    }\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_RUN_DAG);\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_SUBMIT_TO_RUNNING);\n    DAGStatus.State lastState = null;\n    boolean running = false;\n\n    int checkInterval = MIN_CHECK_INTERVAL;\n    while (true) {\n\n      try {\n        if (context != null) {\n          context.checkHeartbeaterLockException();\n        }\n\n        status = dagClient.getDAGStatus(new HashSet<StatusGetOpts>(), checkInterval);\n        vertexProgressMap = status.getVertexProgress();\n        DAGStatus.State state = status.getState();\n\n        failedCounter = 0; // AM is responsive again (recovery?)\n        failureTimer.reset();\n\n        if (state != lastState || state == RUNNING) {\n          lastState = state;\n\n          switch (state) {\n            case SUBMITTED:\n              console.printInfo(\"Status: Submitted\");\n              break;\n            case INITING:\n              console.printInfo(\"Status: Initializing\");\n              this.executionStartTime = System.currentTimeMillis();\n              break;\n            case RUNNING:\n              if (!running) {\n                perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_SUBMIT_TO_RUNNING);\n                console.printInfo(\"Status: Running (\" + dagClient.getExecutionContext() + \")\\n\");\n                this.executionStartTime = System.currentTimeMillis();\n                running = true;\n                // from running -> failed/succeeded, the AM breaks out of timeouts\n                checkInterval = MAX_CHECK_INTERVAL;\n              }\n              updateFunction.update(status, vertexProgressMap);\n              break;\n            case SUCCEEDED:\n              if (!running) {\n                this.executionStartTime = monitorStartTime;\n              }\n              updateFunction.update(status, vertexProgressMap);\n              success = true;\n              running = false;\n              done = true;\n              break;\n            case KILLED:\n              if (!running) {\n                this.executionStartTime = monitorStartTime;\n              }\n              updateFunction.update(status, vertexProgressMap);\n              console.printInfo(\"Status: Killed\");\n              running = false;\n              done = true;\n              rc = 1;\n              break;\n            case FAILED:\n            case ERROR:\n              if (!running) {\n                this.executionStartTime = monitorStartTime;\n              }\n              updateFunction.update(status, vertexProgressMap);\n              console.printError(\"Status: Failed\");\n              running = false;\n              done = true;\n              rc = 2;\n              break;\n          }\n        }\n      } catch (Exception e) {\n        console.printInfo(\"Exception: \" + e.getMessage());\n        boolean isInterrupted = hasInterruptedException(e);\n        if (failedCounter == 0) {\n          failureTimer.reset();\n          failureTimer.start();\n        }\n        if (isInterrupted\n            || (++failedCounter >= MAX_RETRY_FAILURES && failureTimer.now(TimeUnit.MILLISECONDS) > MAX_RETRY_INTERVAL)) {\n          try {\n            if (isInterrupted) {\n              console.printInfo(\"Killing DAG...\");\n            } else {\n              console.printInfo(String.format(\"Killing DAG... after %d seconds\",\n                  failureTimer.now(TimeUnit.SECONDS)));\n            }\n            dagClient.tryKillDAG();\n          } catch (IOException | TezException tezException) {\n            // best effort\n          }\n          console\n              .printError(\"Execution has failed. stack trace: \" + ExceptionUtils.getStackTrace(e));\n          rc = 1;\n          done = true;\n        } else {\n          console.printInfo(\"Retrying...\");\n        }\n      } finally {\n        if (done) {\n          if (rc != 0 && status != null) {\n            for (String diag : status.getDiagnostics()) {\n              console.printError(diag);\n              diagnostics.append(diag);\n            }\n          }\n          synchronized (shutdownList) {\n            shutdownList.remove(dagClient);\n          }\n          break;\n        }\n      }\n    }\n\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_RUN_DAG);\n    printSummary(success, vertexProgressMap);\n    return rc;\n  }"
        ]
    ],
    "952fe6e17e8418515caf33de96e33ff16711265f": [
        [
            "TestSchemaTool::validateMetastoreDbPropertiesTable()",
            " 690  \n 691  \n 692  \n 693  \n 694  \n 695 -\n 696 -\n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  ",
            "  private void validateMetastoreDbPropertiesTable() throws HiveMetaException, IOException {\n    boolean isValid = (boolean) schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n    // adding same property key twice should throw unique key constraint violation exception\n    String[] scripts = new String[] {\n        \"insert into METASTORE_DB_PROPERTIES values (1, 'guid', 'test-uuid-1', 'dummy uuid 1')\",\n        \"insert into METASTORE_DB_PROPERTIES values (2, 'guid', 'test-uuid-2', 'dummy uuid 2')\", };\n    File scriptFile = generateTestScript(scripts);\n    Exception ex = null;\n    try {\n      schemaTool.runBeeLine(scriptFile.getPath());\n    } catch (Exception iox) {\n      ex = iox;\n    }\n    assertTrue(ex != null && ex instanceof IOException);\n  }",
            " 690  \n 691  \n 692  \n 693  \n 694  \n 695 +\n 696 +\n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  ",
            "  private void validateMetastoreDbPropertiesTable() throws HiveMetaException, IOException {\n    boolean isValid = (boolean) schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n    // adding same property key twice should throw unique key constraint violation exception\n    String[] scripts = new String[] {\n        \"insert into METASTORE_DB_PROPERTIES values ('guid', 'test-uuid-1', 'dummy uuid 1')\",\n        \"insert into METASTORE_DB_PROPERTIES values ('guid', 'test-uuid-2', 'dummy uuid 2')\", };\n    File scriptFile = generateTestScript(scripts);\n    Exception ex = null;\n    try {\n      schemaTool.runBeeLine(scriptFile.getPath());\n    } catch (Exception iox) {\n      ex = iox;\n    }\n    assertTrue(ex != null && ex instanceof IOException);\n  }"
        ]
    ],
    "314fe44e5c45fdf1ea2a3edbe9bb9eedda5c12fd": [
        [
            "HiveSchemaTool::validateSchemaTables(Connection)",
            " 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747 -\n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  ",
            "  boolean validateSchemaTables(Connection conn) throws HiveMetaException {\n    String version            = null;\n    ResultSet rs              = null;\n    DatabaseMetaData metadata = null;\n    List<String> dbTables     = new ArrayList<String>();\n    List<String> schemaTables = new ArrayList<String>();\n    List<String> subScripts   = new ArrayList<String>();\n    Connection hmsConn        = getConnectionToMetastore(false);\n\n    System.out.println(\"Validating metastore schema tables\");\n    try {\n      version = metaStoreSchemaInfo.getMetaStoreSchemaVersion(getConnectionInfo(false));\n    } catch (HiveMetaException he) {\n      System.err.println(\"Failed to determine schema version from Hive Metastore DB. \" + he.getMessage());\n      System.out.println(\"Failed in schema table validation.\");\n      LOG.debug(\"Failed to determine schema version from Hive Metastore DB,\" + he.getMessage());\n      return false;\n    }\n\n    // re-open the hms connection\n    hmsConn = getConnectionToMetastore(false);\n\n    LOG.debug(\"Validating tables in the schema for version \" + version);\n    try {\n      metadata       = conn.getMetaData();\n      String[] types = {\"TABLE\"};\n      rs             = metadata.getTables(null, null, \"%\", types);\n      String table   = null;\n\n      while (rs.next()) {\n        table = rs.getString(\"TABLE_NAME\");\n        dbTables.add(table.toLowerCase());\n        LOG.debug(\"Found table \" + table + \" in HMS dbstore\");\n      }\n    } catch (SQLException e) {\n      throw new HiveMetaException(\"Failed to retrieve schema tables from Hive Metastore DB,\" + e.getMessage());\n    } finally {\n      if (rs != null) {\n        try {\n          rs.close();\n        } catch (SQLException e) {\n          throw new HiveMetaException(\"Failed to close resultset\", e);\n        }\n      }\n    }\n\n    // parse the schema file to determine the tables that are expected to exist\n    // we are using oracle schema because it is simpler to parse, no quotes or backticks etc\n    String baseDir    = new File(metaStoreSchemaInfo.getMetaStoreScriptDir()).getParent();\n    String schemaFile = new File(metaStoreSchemaInfo.getMetaStoreScriptDir(),\n        metaStoreSchemaInfo.generateInitFileName(version)).getPath();\n    try {\n      LOG.debug(\"Parsing schema script \" + schemaFile);\n      subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      while (subScripts.size() > 0) {\n        schemaFile = baseDir + \"/\" + dbType + \"/\" + subScripts.remove(0);\n        LOG.debug(\"Parsing subscript \" + schemaFile);\n        subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      }\n    } catch (Exception e) {\n      System.err.println(\"Exception in parsing schema file. Cause:\" + e.getMessage());\n      System.out.println(\"Failed in schema table validation.\");\n      return false;\n    }\n\n    LOG.debug(\"Schema tables:[ \" + Arrays.toString(schemaTables.toArray()) + \" ]\");\n    LOG.debug(\"DB tables:[ \" + Arrays.toString(dbTables.toArray()) + \" ]\");\n    // now diff the lists\n    schemaTables.removeAll(dbTables);\n    if (schemaTables.size() > 0) {\n      Collections.sort(schemaTables);\n      System.err.println(\"Table(s) [ \" + Arrays.toString(schemaTables.toArray())\n          + \" ] are missing from the metastore database schema.\");\n      System.out.println(\"Failed in schema table validation\");\n      return false;\n    } else {\n      System.out.println(\"Succeeded in schema table validation.\");\n      return true;\n    }\n  }",
            " 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747 +\n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  ",
            "  boolean validateSchemaTables(Connection conn) throws HiveMetaException {\n    String version            = null;\n    ResultSet rs              = null;\n    DatabaseMetaData metadata = null;\n    List<String> dbTables     = new ArrayList<String>();\n    List<String> schemaTables = new ArrayList<String>();\n    List<String> subScripts   = new ArrayList<String>();\n    Connection hmsConn        = getConnectionToMetastore(false);\n\n    System.out.println(\"Validating metastore schema tables\");\n    try {\n      version = metaStoreSchemaInfo.getMetaStoreSchemaVersion(getConnectionInfo(false));\n    } catch (HiveMetaException he) {\n      System.err.println(\"Failed to determine schema version from Hive Metastore DB. \" + he.getMessage());\n      System.out.println(\"Failed in schema table validation.\");\n      LOG.debug(\"Failed to determine schema version from Hive Metastore DB,\" + he.getMessage());\n      return false;\n    }\n\n    // re-open the hms connection\n    hmsConn = getConnectionToMetastore(false);\n\n    LOG.debug(\"Validating tables in the schema for version \" + version);\n    try {\n      metadata       = conn.getMetaData();\n      String[] types = {\"TABLE\"};\n      rs             = metadata.getTables(null, hmsConn.getSchema(), \"%\", types);\n      String table   = null;\n\n      while (rs.next()) {\n        table = rs.getString(\"TABLE_NAME\");\n        dbTables.add(table.toLowerCase());\n        LOG.debug(\"Found table \" + table + \" in HMS dbstore\");\n      }\n    } catch (SQLException e) {\n      throw new HiveMetaException(\"Failed to retrieve schema tables from Hive Metastore DB,\" + e.getMessage());\n    } finally {\n      if (rs != null) {\n        try {\n          rs.close();\n        } catch (SQLException e) {\n          throw new HiveMetaException(\"Failed to close resultset\", e);\n        }\n      }\n    }\n\n    // parse the schema file to determine the tables that are expected to exist\n    // we are using oracle schema because it is simpler to parse, no quotes or backticks etc\n    String baseDir    = new File(metaStoreSchemaInfo.getMetaStoreScriptDir()).getParent();\n    String schemaFile = new File(metaStoreSchemaInfo.getMetaStoreScriptDir(),\n        metaStoreSchemaInfo.generateInitFileName(version)).getPath();\n    try {\n      LOG.debug(\"Parsing schema script \" + schemaFile);\n      subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      while (subScripts.size() > 0) {\n        schemaFile = baseDir + \"/\" + dbType + \"/\" + subScripts.remove(0);\n        LOG.debug(\"Parsing subscript \" + schemaFile);\n        subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      }\n    } catch (Exception e) {\n      System.err.println(\"Exception in parsing schema file. Cause:\" + e.getMessage());\n      System.out.println(\"Failed in schema table validation.\");\n      return false;\n    }\n\n    LOG.debug(\"Schema tables:[ \" + Arrays.toString(schemaTables.toArray()) + \" ]\");\n    LOG.debug(\"DB tables:[ \" + Arrays.toString(dbTables.toArray()) + \" ]\");\n    // now diff the lists\n    schemaTables.removeAll(dbTables);\n    if (schemaTables.size() > 0) {\n      Collections.sort(schemaTables);\n      System.err.println(\"Table(s) [ \" + Arrays.toString(schemaTables.toArray())\n          + \" ] are missing from the metastore database schema.\");\n      System.out.println(\"Failed in schema table validation\");\n      return false;\n    } else {\n      System.out.println(\"Succeeded in schema table validation.\");\n      return true;\n    }\n  }"
        ]
    ],
    "71f52d8ad512904b3f2c4f04fe39a33f2834f1f2": [
        [
            "GenSparkUtils::createMapWork(GenSparkProcContext,Operator,SparkWork,PrunedPartitionList,boolean)",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 -\n 161 -\n 162  \n 163 -\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "  public MapWork createMapWork(GenSparkProcContext context, Operator<?> root,\n      SparkWork sparkWork, PrunedPartitionList partitions, boolean deferSetup) throws SemanticException {\n    Preconditions.checkArgument(root.getParentOperators().isEmpty(),\n        \"AssertionError: expected root.getParentOperators() to be empty\");\n    MapWork mapWork = new MapWork(\"Map \" + (++sequenceNumber));\n    LOG.debug(\"Adding map work (\" + mapWork.getName() + \") for \" + root);\n\n    // map work starts with table scan operators\n    Preconditions.checkArgument(root instanceof TableScanOperator,\n      \"AssertionError: expected root to be an instance of TableScanOperator, but was \"\n      + root.getClass().getName());\n    String alias = ((TableScanOperator) root).getConf().getAlias();\n\n    if (!deferSetup) {\n      setupMapWork(mapWork, context, partitions,(TableScanOperator) root, alias);\n    }\n\n    // add new item to the Spark work\n    sparkWork.add(mapWork);\n\n    return mapWork;\n  }",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160 +\n 161 +\n 162 +\n 163 +\n 164 +\n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172  \n 173 +\n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  ",
            "  public MapWork createMapWork(GenSparkProcContext context, Operator<?> root,\n      SparkWork sparkWork, PrunedPartitionList partitions, boolean deferSetup) throws SemanticException {\n    Preconditions.checkArgument(root.getParentOperators().isEmpty(),\n        \"AssertionError: expected root.getParentOperators() to be empty\");\n    MapWork mapWork = new MapWork(\"Map \" + (++sequenceNumber));\n    LOG.debug(\"Adding map work (\" + mapWork.getName() + \") for \" + root);\n\n    // map work starts with table scan operators\n    Preconditions.checkArgument(root instanceof TableScanOperator,\n      \"AssertionError: expected root to be an instance of TableScanOperator, but was \"\n      + root.getClass().getName());\n    String alias_id = null;\n    if (context.parseContext != null && context.parseContext.getTopOps() != null) {\n      for (String currentAliasID : context.parseContext.getTopOps().keySet()) {\n        Operator<? extends OperatorDesc> currOp = context.parseContext.getTopOps().get(currentAliasID);\n        if (currOp == root) {\n          alias_id = currentAliasID;\n          break;\n        }\n      }\n    }\n    if (alias_id == null)\n      alias_id = ((TableScanOperator) root).getConf().getAlias();\n    if (!deferSetup) {\n      setupMapWork(mapWork, context, partitions,(TableScanOperator) root, alias_id);\n    }\n\n    // add new item to the Spark work\n    sparkWork.add(mapWork);\n\n    return mapWork;\n  }"
        ],
        [
            "GenSparkUtils::setupMapWork(MapWork,GenSparkProcContext,PrunedPartitionList,TableScanOperator,String)",
            " 173  \n 174  \n 175 -\n 176  \n 177  \n 178 -\n 179  ",
            "  protected void setupMapWork(MapWork mapWork, GenSparkProcContext context,\n      PrunedPartitionList partitions, TableScanOperator root,\n      String alias) throws SemanticException {\n    // All the setup is done in GenMapRedUtils\n    GenMapRedUtils.setMapWork(mapWork, context.parseContext,\n        context.inputs, partitions, root, alias, context.conf, false);\n  }",
            " 183  \n 184  \n 185 +\n 186  \n 187  \n 188 +\n 189  ",
            "  protected void setupMapWork(MapWork mapWork, GenSparkProcContext context,\n      PrunedPartitionList partitions, TableScanOperator root,\n      String alias_id) throws SemanticException {\n    // All the setup is done in GenMapRedUtils\n    GenMapRedUtils.setMapWork(mapWork, context.parseContext,\n        context.inputs, partitions, root, alias_id, context.conf, false);\n  }"
        ]
    ],
    "a988c159cc3e9f43211432eb035454ee6143b219": [
        [
            "JoinDesc::getColumnExprMapForExplain()",
            " 381  \n 382  \n 383 -\n 384  \n 385 -\n 386  \n 387 -\n 388  \n 389 -\n 390 -\n 391  \n 392  \n 393  ",
            "  @Override\n  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, ExprNodeDesc> getColumnExprMapForExplain() {\n    if(this.reversedExprs == null) {\n      return this.colExprMap;\n    }\n    Map<String, ExprNodeDesc> explainColMap = new HashMap<>();\n    for(String col:this.colExprMap.keySet()){\n      String taggedCol = this.reversedExprs.get(col) + \" \" + col;\n      explainColMap.put(taggedCol, this.colExprMap.get(col));\n    }\n    return explainColMap;\n  }",
            " 381  \n 382  \n 383 +\n 384  \n 385 +\n 386  \n 387 +\n 388  \n 389 +\n 390 +\n 391  \n 392  \n 393  ",
            "  @Override\n  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, String> getColumnExprMapForExplain() {\n    if(this.reversedExprs == null) {\n      return super.getColumnExprMapForExplain();\n    }\n    Map<String, String> explainColMap = new HashMap<>();\n    for(String col:this.colExprMap.keySet()){\n      String taggedCol = this.reversedExprs.get(col) + \":\" + this.colExprMap.get(col);\n      explainColMap.put(col, taggedCol);\n    }\n    return explainColMap;\n  }"
        ],
        [
            "AbstractOperatorDesc::getColumnExprMapForExplain()",
            " 144  \n 145 -\n 146 -\n 147  ",
            "  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, ExprNodeDesc> getColumnExprMapForExplain() {\n    return this.colExprMap;\n  }",
            " 145  \n 146 +\n 147 +\n 148 +\n 149 +\n 150 +\n 151 +\n 152  ",
            "  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, String> getColumnExprMapForExplain() {\n    Map<String, String> colExprMapForExplain = new HashMap<>();\n    for(String col:this.colExprMap.keySet()) {\n      colExprMapForExplain.put(col, this.colExprMap.get(col).toString());\n    }\n    return colExprMapForExplain;\n  }"
        ]
    ],
    "d62a038a1ca84037b7b17ce989954a93d1a76a25": [
        [
            "JoinDesc::getColumnExprMapForExplain()",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 -\n 390  \n 391  \n 392  \n 393  ",
            "  @Override\n  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, String> getColumnExprMapForExplain() {\n    if(this.reversedExprs == null) {\n      return super.getColumnExprMapForExplain();\n    }\n    Map<String, String> explainColMap = new HashMap<>();\n    for(String col:this.colExprMap.keySet()){\n      String taggedCol = this.reversedExprs.get(col) + \":\" + this.colExprMap.get(col);\n      explainColMap.put(col, taggedCol);\n    }\n    return explainColMap;\n  }",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 +\n 390 +\n 391  \n 392  \n 393  \n 394  ",
            "  @Override\n  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, String> getColumnExprMapForExplain() {\n    if(this.reversedExprs == null) {\n      return super.getColumnExprMapForExplain();\n    }\n    Map<String, String> explainColMap = new HashMap<>();\n    for(String col:this.colExprMap.keySet()){\n      String taggedCol = this.reversedExprs.get(col) + \":\"\n          + this.colExprMap.get(col).getExprString();\n      explainColMap.put(col, taggedCol);\n    }\n    return explainColMap;\n  }"
        ]
    ],
    "8432dd8d77b3c36f0e269ad8a0d2b20a1574b477": [
        [
            "CacheChunk::init(MemoryBuffer,long,long)",
            "  42  \n  43  \n  44  \n  45  \n  46  ",
            "  public void init(MemoryBuffer buffer, long offset, long end) {\n    this.buffer = buffer;\n    this.offset = offset;\n    this.end = end;\n  }",
            "  42  \n  43  \n  44  \n  45  \n  46 +\n  47  ",
            "  public void init(MemoryBuffer buffer, long offset, long end) {\n    this.buffer = buffer;\n    this.offset = offset;\n    this.end = end;\n    this.next = this.prev = null; // Just in case.\n  }"
        ],
        [
            "OrcEncodedDataReader::DataWrapperForOrc::getFileData(Object,DiskRangeList,long,DiskRangeListFactory,BooleanRef)",
            " 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837 -\n 838  \n 839  \n 840 -\n 841 -\n 842  ",
            "    @Override\n    public DiskRangeList getFileData(Object fileKey, DiskRangeList range,\n        long baseOffset, DiskRangeListFactory factory, BooleanRef gotAllData) {\n      DiskRangeList result = lowLevelCache.getFileData(\n          fileKey, range, baseOffset, factory, counters, gotAllData);\n      if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {\n        LlapIoImpl.ORC_LOGGER.trace(\"Disk ranges after data cache (file \" + fileKey +\n            \", base offset \" + baseOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(range));\n      }\n      if (gotAllData.value) return result;\n      return (metadataCache == null) ? range\n          : metadataCache.getIncompleteCbs(fileKey, range, baseOffset, factory, gotAllData);\n    }",
            " 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837 +\n 838  \n 839  \n 840 +\n 841 +\n 842  ",
            "    @Override\n    public DiskRangeList getFileData(Object fileKey, DiskRangeList range,\n        long baseOffset, DiskRangeListFactory factory, BooleanRef gotAllData) {\n      DiskRangeList result = lowLevelCache.getFileData(\n          fileKey, range, baseOffset, factory, counters, gotAllData);\n      if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {\n        LlapIoImpl.ORC_LOGGER.trace(\"Disk ranges after data cache (file \" + fileKey +\n            \", base offset \" + baseOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(result));\n      }\n      if (gotAllData.value) return result;\n      return (metadataCache == null) ? result\n          : metadataCache.getIncompleteCbs(fileKey, result, baseOffset, factory, gotAllData);\n    }"
        ]
    ],
    "bb2f25c1a189b031a9601cb00a3dc2f5d6f5ac4a": [
        [
            "SecretManager::createLlapZkConf(Configuration,String,String,String)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 -\n 179  \n 180  \n 181  \n 182  \n 183 -\n 184  \n 185  \n 186 -\n 187  \n 188 -\n 189 -\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  ",
            "  private static LlapZkConf createLlapZkConf(\n      Configuration conf, String llapPrincipal, String llapKeytab, String clusterId) {\n     String principal = HiveConf.getVar(conf, ConfVars.LLAP_ZKSM_KERBEROS_PRINCIPAL, llapPrincipal);\n     String keyTab = HiveConf.getVar(conf, ConfVars.LLAP_ZKSM_KERBEROS_KEYTAB_FILE, llapKeytab);\n     // Override the default delegation token lifetime for LLAP.\n     // Also set all the necessary ZK settings to defaults and LLAP configs, if not set.\n     final Configuration zkConf = new Configuration(conf);\n    long tokenLifetime = HiveConf.getTimeVar(\n        conf, ConfVars.LLAP_DELEGATION_TOKEN_LIFETIME, TimeUnit.SECONDS);\n    zkConf.setLong(DelegationTokenManager.MAX_LIFETIME, tokenLifetime);\n    zkConf.setLong(DelegationTokenManager.RENEW_INTERVAL, tokenLifetime);\n    try {\n      zkConf.set(SecretManager.ZK_DTSM_ZK_KERBEROS_PRINCIPAL,\n          SecurityUtil.getServerPrincipal(principal, \"0.0.0.0\"));\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    zkConf.set(SecretManager.ZK_DTSM_ZK_KERBEROS_KEYTAB, keyTab);\n    String zkPath = \"zkdtsm_\" + clusterId;\n    LOG.info(\"Using {} as ZK secret manager path\", zkPath);\n    zkConf.set(SecretManager.ZK_DTSM_ZNODE_WORKING_PATH, zkPath);\n    // Hardcode SASL here. ZKDTSM only supports none or sasl and we never want none.\n    zkConf.set(SecretManager.ZK_DTSM_ZK_AUTH_TYPE, \"sasl\");\n    setZkConfIfNotSet(zkConf, SecretManager.ZK_DTSM_ZK_CONNECTION_STRING,\n        HiveConf.getVar(zkConf, ConfVars.LLAP_ZKSM_ZK_CONNECTION_STRING));\n\n    UserGroupInformation zkUgi = null;\n    try {\n      zkUgi = LlapUtil.loginWithKerberos(principal, keyTab);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return new LlapZkConf(zkConf, zkUgi);\n  }",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  \n 180  \n 181  \n 182  \n 183 +\n 184  \n 185  \n 186 +\n 187  \n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193 +\n 194 +\n 195 +\n 196 +\n 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  ",
            "  private static LlapZkConf createLlapZkConf(\n      Configuration conf, String llapPrincipal, String llapKeytab, String clusterId) {\n     String principal = HiveConf.getVar(conf, ConfVars.LLAP_ZKSM_KERBEROS_PRINCIPAL, llapPrincipal);\n     String keyTab = HiveConf.getVar(conf, ConfVars.LLAP_ZKSM_KERBEROS_KEYTAB_FILE, llapKeytab);\n     // Override the default delegation token lifetime for LLAP.\n     // Also set all the necessary ZK settings to defaults and LLAP configs, if not set.\n     final Configuration zkConf = new Configuration(conf);\n    long tokenLifetime = HiveConf.getTimeVar(\n        conf, ConfVars.LLAP_DELEGATION_TOKEN_LIFETIME, TimeUnit.SECONDS);\n    zkConf.setLong(DelegationTokenManager.MAX_LIFETIME, tokenLifetime);\n    zkConf.setLong(DelegationTokenManager.RENEW_INTERVAL, tokenLifetime);\n    try {\n      zkConf.set(ZK_DTSM_ZK_KERBEROS_PRINCIPAL,\n          SecurityUtil.getServerPrincipal(principal, \"0.0.0.0\"));\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    zkConf.set(ZK_DTSM_ZK_KERBEROS_KEYTAB, keyTab);\n    String zkPath = \"zkdtsm_\" + clusterId;\n    LOG.info(\"Using {} as ZK secret manager path\", zkPath);\n    zkConf.set(ZK_DTSM_ZNODE_WORKING_PATH, zkPath);\n    // Hardcode SASL here. ZKDTSM only supports none or sasl and we never want none.\n    zkConf.set(ZK_DTSM_ZK_AUTH_TYPE, \"sasl\");\n    long sessionTimeoutMs = HiveConf.getTimeVar(\n        zkConf, ConfVars.LLAP_ZKSM_ZK_SESSION_TIMEOUT, TimeUnit.MILLISECONDS);\n    long newRetryCount =\n        (ZK_DTSM_ZK_NUM_RETRIES_DEFAULT * sessionTimeoutMs) / ZK_DTSM_ZK_SESSION_TIMEOUT_DEFAULT;\n    long connTimeoutMs = Math.max(sessionTimeoutMs, ZK_DTSM_ZK_CONNECTION_TIMEOUT_DEFAULT);\n    zkConf.set(ZK_DTSM_ZK_SESSION_TIMEOUT, Long.toString(sessionTimeoutMs));\n    zkConf.set(ZK_DTSM_ZK_CONNECTION_TIMEOUT, Long.toString(connTimeoutMs));\n    zkConf.set(ZK_DTSM_ZK_NUM_RETRIES, Long.toString(newRetryCount));\n    setZkConfIfNotSet(zkConf, ZK_DTSM_ZK_CONNECTION_STRING,\n        HiveConf.getVar(zkConf, ConfVars.LLAP_ZKSM_ZK_CONNECTION_STRING));\n\n    UserGroupInformation zkUgi = null;\n    try {\n      zkUgi = LlapUtil.loginWithKerberos(principal, keyTab);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return new LlapZkConf(zkConf, zkUgi);\n  }"
        ],
        [
            "SecretManager::createSecretManager(Configuration,String)",
            " 201  \n 202  \n 203  \n 204 -\n 205  ",
            "  public static SecretManager createSecretManager(Configuration conf, String clusterId) {\n    String llapPrincipal = HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_PRINCIPAL),\n        llapKeytab = HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_KEYTAB_FILE);\n    return SecretManager.createSecretManager(conf, llapPrincipal, llapKeytab, clusterId);\n  }",
            " 209  \n 210  \n 211  \n 212 +\n 213  ",
            "  public static SecretManager createSecretManager(Configuration conf, String clusterId) {\n    String llapPrincipal = HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_PRINCIPAL),\n        llapKeytab = HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_KEYTAB_FILE);\n    return createSecretManager(conf, llapPrincipal, llapKeytab, clusterId);\n  }"
        ]
    ],
    "dd04a92f7178df2ef26ca0b76bd2985c34a7dbaf": [
        [
            "GenericUDAFVarianceSample::GenericUDAFVarianceSampleEvaluator::terminate(AggregationBuffer)",
            "  77  \n  78  \n  79  \n  80  \n  81 -\n  82  \n  83  \n  84 -\n  85 -\n  86 -\n  87 -\n  88 -\n  89  \n  90  \n  91  ",
            "    @Override\n    public Object terminate(AggregationBuffer agg) throws HiveException {\n      StdAgg myagg = (StdAgg) agg;\n\n      if (myagg.count == 0) { // SQL standard - return null for zero elements\n        return null;\n      } else {\n        if (myagg.count > 1) {\n          getResult().set(myagg.variance / (myagg.count - 1));\n        } else { // for one element the variance is always 0\n          getResult().set(0);\n        }\n        return getResult();\n      }\n    }",
            "  80  \n  81  \n  82  \n  83  \n  84 +\n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  ",
            "    @Override\n    public Object terminate(AggregationBuffer agg) throws HiveException {\n      StdAgg myagg = (StdAgg) agg;\n\n      if (myagg.count <= 1) {\n        return null;\n      } else {\n        getResult().set(myagg.variance / (myagg.count - 1));\n        return getResult();\n      }\n    }"
        ],
        [
            "GenericUDAFStdSample::GenericUDAFStdSampleEvaluator::terminate(AggregationBuffer)",
            "  77  \n  78  \n  79  \n  80  \n  81 -\n  82  \n  83  \n  84 -\n  85 -\n  86 -\n  87 -\n  88 -\n  89  \n  90  \n  91  ",
            "    @Override\n    public Object terminate(AggregationBuffer agg) throws HiveException {\n      StdAgg myagg = (StdAgg) agg;\n\n      if (myagg.count == 0) { // SQL standard - return null for zero elements\n        return null;\n      } else {\n        if (myagg.count > 1) {\n          getResult().set(Math.sqrt(myagg.variance / (myagg.count - 1)));\n        } else { // for one element the variance is always 0\n          getResult().set(0);\n        }\n        return getResult();\n      }\n    }",
            "  80  \n  81  \n  82  \n  83  \n  84 +\n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  ",
            "    @Override\n    public Object terminate(AggregationBuffer agg) throws HiveException {\n      StdAgg myagg = (StdAgg) agg;\n\n      if (myagg.count <= 1) { // SQL standard - return null for zero or one elements\n        return null;\n      } else {\n        getResult().set(Math.sqrt(myagg.variance / (myagg.count - 1)));\n        return getResult();\n      }\n    }"
        ]
    ],
    "4a33ec8fcae5f7d18105ef62e33150db6e853af5": [
        [
            "HiveConf::StrictChecks::checkNoLimit(Configuration)",
            "4992  \n4993 -\n4994  ",
            "    public static String checkNoLimit(Configuration conf) {\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY) ? null : NO_LIMIT_MSG;\n    }",
            "4995  \n4996 +\n4997  ",
            "    public static String checkNoLimit(Configuration conf) {\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_ORDERBY_NO_LIMIT) ? null : NO_LIMIT_MSG;\n    }"
        ],
        [
            "HiveConf::StrictChecks::makeMessage(String,ConfVars)",
            "4985  \n4986  \n4987 -\n4988 -\n4989 -\n4990  ",
            "    private static String makeMessage(String what, ConfVars setting) {\n      return what + \" are disabled for safety reasons. If you know what you are doing, please set \"\n          + setting.varname + \" to false and that \" + ConfVars.HIVEMAPREDMODE.varname + \" is not\"\n          + \" set to 'strict' to proceed. Note that if you may get errors or incorrect results if\"\n          + \" you make a mistake while using some of the unsafe features.\";\n    }",
            "4988  \n4989  \n4990 +\n4991 +\n4992 +\n4993  ",
            "    private static String makeMessage(String what, ConfVars setting) {\n      return what + \" are disabled for safety reasons. If you know what you are doing, please set \"\n          + setting.varname + \" to false and make sure that \" + ConfVars.HIVEMAPREDMODE.varname +\n              \" is not set to 'strict' to proceed. Note that you may get errors or incorrect \" +\n              \"results if you make a mistake while using some of the unsafe features.\";\n    }"
        ],
        [
            "HiveConf::StrictChecks::checkNoPartitionFilter(Configuration)",
            "4996  \n4997 -\n4998  \n4999  ",
            "    public static String checkNoPartitionFilter(Configuration conf) {\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY)\n          ? null : NO_PARTITIONLESS_MSG;\n    }",
            "4999  \n5000 +\n5001  \n5002  ",
            "    public static String checkNoPartitionFilter(Configuration conf) {\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_NO_PARTITION_FILTER)\n          ? null : NO_PARTITIONLESS_MSG;\n    }"
        ]
    ],
    "b7b3f881f4840430378a27131278876a34368995": [
        [
            "CliConfigs::MiniDruidCliConfig::MiniDruidCliConfig()",
            " 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 -\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  ",
            "    public MiniDruidCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"druid.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/druid\");\n        setLogDir(\"itests/qtest/target/tmp/log\");\n\n        setInitScript(\"q_test_druid_init.sql\");\n        setCleanupScript(\"q_test_cleanup_druid.sql\");\n        setHiveConfDir(\"\");\n        setClusterType(MiniClusterType.druid);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180 +\n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  ",
            "    public MiniDruidCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"druid.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/druid\");\n        setLogDir(\"itests/qtest/target/tmp/log\");\n\n        setInitScript(\"q_test_druid_init.sql\");\n        setCleanupScript(\"q_test_cleanup_druid.sql\");\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.druid);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ],
        [
            "QTestUtil::setupMiniCluster(HadoopShims,String)",
            " 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671 -\n 672 -\n 673 -\n 674 -\n 675 -\n 676 -\n 677 -\n 678 -\n 679 -\n 680 -\n 681 -\n 682 -\n 683  \n 684  ",
            "  private void setupMiniCluster(HadoopShims shims, String confDir) throws\n      IOException {\n\n    String uriString = fs.getUri().toString();\n\n    if (clusterType.getCoreClusterType() == CoreClusterType.TEZ) {\n      if (confDir != null && !confDir.isEmpty()) {\n        conf.addResource(new URL(\"file://\" + new File(confDir).toURI().getPath()\n            + \"/tez-site.xml\"));\n      }\n      int numTrackers = 2;\n      if (EnumSet.of(MiniClusterType.llap, MiniClusterType.llap_local).contains(clusterType)) {\n        llapCluster = LlapItUtils.startAndGetMiniLlapCluster(conf, setup.zooKeeperCluster, confDir);\n      } else {\n      }\n      if (EnumSet.of(MiniClusterType.llap_local, MiniClusterType.tez_local).contains(clusterType)) {\n        mr = shims.getLocalMiniTezCluster(conf, clusterType == MiniClusterType.llap_local);\n      } else {\n        mr = shims.getMiniTezCluster(conf, numTrackers, uriString,\n            EnumSet.of(MiniClusterType.llap, MiniClusterType.llap_local).contains(clusterType));\n      }\n    } else if (clusterType == MiniClusterType.miniSparkOnYarn) {\n      mr = shims.getMiniSparkCluster(conf, 2, uriString, 1);\n    } else if (clusterType == MiniClusterType.mr) {\n      mr = shims.getMiniMrCluster(conf, 2, uriString, 1);\n    } else if (clusterType == MiniClusterType.druid) {\n      final String tempDir = System.getProperty(\"test.tmp.dir\");\n      druidCluster = new MiniDruidCluster(\"mini-druid\",\n              getLogDirectory(),\n              tempDir,\n              setup.zkPort,\n              Utilities.jarFinderGetJar(MiniDruidCluster.class)\n      );\n      druidCluster.init(conf);\n      final Path druidDeepStorage = fs.makeQualified(new Path(druidCluster.getDeepStorageDir()));\n      fs.mkdirs(druidDeepStorage);\n      druidCluster.start();\n    }\n  }",
            " 645  \n 646  \n 647  \n 648  \n 649  \n 650 +\n 651 +\n 652 +\n 653 +\n 654 +\n 655 +\n 656 +\n 657 +\n 658 +\n 659 +\n 660 +\n 661 +\n 662 +\n 663 +\n 664 +\n 665 +\n 666 +\n 667 +\n 668 +\n 669 +\n 670 +\n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  ",
            "  private void setupMiniCluster(HadoopShims shims, String confDir) throws\n      IOException {\n\n    String uriString = fs.getUri().toString();\n\n    if (clusterType == MiniClusterType.druid) {\n      final String tempDir = System.getProperty(\"test.tmp.dir\");\n      druidCluster = new MiniDruidCluster(\"mini-druid\",\n          getLogDirectory(),\n          tempDir,\n          setup.zkPort,\n          Utilities.jarFinderGetJar(MiniDruidCluster.class)\n      );\n      final Path druidDeepStorage = fs.makeQualified(new Path(druidCluster.getDeepStorageDir()));\n      fs.mkdirs(druidDeepStorage);\n      conf.set(\"hive.druid.storage.storageDirectory\", druidDeepStorage.toUri().getPath());\n      conf.set(\"hive.druid.metadata.db.type\", \"derby\");\n      conf.set(\"hive.druid.metadata.uri\", druidCluster.getMetadataURI());\n      final Path scratchDir = fs\n          .makeQualified(new Path(System.getProperty(\"test.tmp.dir\"), \"druidStagingDir\"));\n      fs.mkdirs(scratchDir);\n      conf.set(\"hive.druid.working.directory\", scratchDir.toUri().getPath());\n      druidCluster.init(conf);\n      druidCluster.start();\n    }\n\n    if (clusterType.getCoreClusterType() == CoreClusterType.TEZ) {\n      if (confDir != null && !confDir.isEmpty()) {\n        conf.addResource(new URL(\"file://\" + new File(confDir).toURI().getPath()\n            + \"/tez-site.xml\"));\n      }\n      int numTrackers = 2;\n      if (EnumSet.of(MiniClusterType.llap, MiniClusterType.llap_local).contains(clusterType)) {\n        llapCluster = LlapItUtils.startAndGetMiniLlapCluster(conf, setup.zooKeeperCluster, confDir);\n      } else {\n      }\n      if (EnumSet.of(MiniClusterType.llap_local, MiniClusterType.tez_local).contains(clusterType)) {\n        mr = shims.getLocalMiniTezCluster(conf, clusterType == MiniClusterType.llap_local);\n      } else {\n        mr = shims.getMiniTezCluster(conf, numTrackers, uriString,\n            EnumSet.of(MiniClusterType.llap, MiniClusterType.llap_local).contains(clusterType));\n      }\n    } else if (clusterType == MiniClusterType.miniSparkOnYarn) {\n      mr = shims.getMiniSparkCluster(conf, 2, uriString, 1);\n    } else if (clusterType == MiniClusterType.mr) {\n      mr = shims.getMiniMrCluster(conf, 2, uriString, 1);\n    }\n  }"
        ]
    ],
    "9e410b29628e0c517086ed2f42cf5b9f4a852397": [
        [
            "ASTConverter::convertOrderLimitToASTNode(HiveSortLimit)",
            " 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 -\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  ",
            "  private void convertOrderLimitToASTNode(HiveSortLimit order) {\n    if (order != null) {\n      HiveSortLimit hiveSortLimit = order;\n      if (!hiveSortLimit.getCollation().getFieldCollations().isEmpty()) {\n        // 1 Add order by token\n        ASTNode orderAst = ASTBuilder.createAST(HiveParser.TOK_ORDERBY, \"TOK_ORDERBY\");\n\n        schema = new Schema(hiveSortLimit);\n        Map<Integer, RexNode> obRefToCallMap = hiveSortLimit.getInputRefToCallMap();\n        RexNode obExpr;\n        ASTNode astCol;\n        for (RelFieldCollation c : hiveSortLimit.getCollation().getFieldCollations()) {\n\n          // 2 Add Direction token\n          ASTNode directionAST = c.getDirection() == RelFieldCollation.Direction.ASCENDING ? ASTBuilder\n              .createAST(HiveParser.TOK_TABSORTCOLNAMEASC, \"TOK_TABSORTCOLNAMEASC\") : ASTBuilder\n              .createAST(HiveParser.TOK_TABSORTCOLNAMEDESC, \"TOK_TABSORTCOLNAMEDESC\");\n          ASTNode nullDirectionAST;\n          // Null direction\n          if (c.nullDirection == RelFieldCollation.NullDirection.FIRST) {\n            nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_FIRST, \"TOK_NULLS_FIRST\");\n            directionAST.addChild(nullDirectionAST);\n          } else if (c.nullDirection == RelFieldCollation.NullDirection.LAST) {\n            nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_LAST, \"TOK_NULLS_LAST\");\n            directionAST.addChild(nullDirectionAST);\n          } else {\n            // Default\n            if (c.getDirection() == RelFieldCollation.Direction.ASCENDING) {\n              nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_FIRST, \"TOK_NULLS_FIRST\");\n              directionAST.addChild(nullDirectionAST);\n            } else {\n              nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_LAST, \"TOK_NULLS_LAST\");\n              directionAST.addChild(nullDirectionAST);\n            }\n          }\n\n          // 3 Convert OB expr (OB Expr is usually an input ref except for top\n          // level OB; top level OB will have RexCall kept in a map.)\n          obExpr = null;\n          if (obRefToCallMap != null)\n            obExpr = obRefToCallMap.get(c.getFieldIndex());\n\n          if (obExpr != null) {\n            astCol = obExpr.accept(new RexVisitor(schema));\n          } else {\n            ColumnInfo cI = schema.get(c.getFieldIndex());\n            /*\n             * The RowResolver setup for Select drops Table associations. So\n             * setup ASTNode on unqualified name.\n             */\n            astCol = ASTBuilder.unqualifiedName(cI.column);\n          }\n\n          // 4 buildup the ob expr AST\n          nullDirectionAST.addChild(astCol);\n          orderAst.addChild(directionAST);\n        }\n        hiveAST.order = orderAst;\n      }\n\n      RexNode offsetExpr = hiveSortLimit.getOffsetExpr();\n      RexNode fetchExpr = hiveSortLimit.getFetchExpr();\n      if (fetchExpr != null) {\n        Object offset = (offsetExpr == null) ?\n            new Integer(0) : ((RexLiteral) offsetExpr).getValue2();\n        Object fetch = ((RexLiteral) fetchExpr).getValue2();\n        hiveAST.limit = ASTBuilder.limit(offset, fetch);\n      }\n    }\n  }",
            " 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 +\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  ",
            "  private void convertOrderLimitToASTNode(HiveSortLimit order) {\n    if (order != null) {\n      HiveSortLimit hiveSortLimit = order;\n      if (!hiveSortLimit.getCollation().getFieldCollations().isEmpty()) {\n        // 1 Add order by token\n        ASTNode orderAst = ASTBuilder.createAST(HiveParser.TOK_ORDERBY, \"TOK_ORDERBY\");\n\n        schema = new Schema(hiveSortLimit);\n        Map<Integer, RexNode> obRefToCallMap = hiveSortLimit.getInputRefToCallMap();\n        RexNode obExpr;\n        ASTNode astCol;\n        for (RelFieldCollation c : hiveSortLimit.getCollation().getFieldCollations()) {\n\n          // 2 Add Direction token\n          ASTNode directionAST = c.getDirection() == RelFieldCollation.Direction.ASCENDING ? ASTBuilder\n              .createAST(HiveParser.TOK_TABSORTCOLNAMEASC, \"TOK_TABSORTCOLNAMEASC\") : ASTBuilder\n              .createAST(HiveParser.TOK_TABSORTCOLNAMEDESC, \"TOK_TABSORTCOLNAMEDESC\");\n          ASTNode nullDirectionAST;\n          // Null direction\n          if (c.nullDirection == RelFieldCollation.NullDirection.FIRST) {\n            nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_FIRST, \"TOK_NULLS_FIRST\");\n            directionAST.addChild(nullDirectionAST);\n          } else if (c.nullDirection == RelFieldCollation.NullDirection.LAST) {\n            nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_LAST, \"TOK_NULLS_LAST\");\n            directionAST.addChild(nullDirectionAST);\n          } else {\n            // Default\n            if (c.getDirection() == RelFieldCollation.Direction.ASCENDING) {\n              nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_FIRST, \"TOK_NULLS_FIRST\");\n              directionAST.addChild(nullDirectionAST);\n            } else {\n              nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_LAST, \"TOK_NULLS_LAST\");\n              directionAST.addChild(nullDirectionAST);\n            }\n          }\n\n          // 3 Convert OB expr (OB Expr is usually an input ref except for top\n          // level OB; top level OB will have RexCall kept in a map.)\n          obExpr = null;\n          if (obRefToCallMap != null)\n            obExpr = obRefToCallMap.get(c.getFieldIndex());\n\n          if (obExpr != null) {\n            astCol = obExpr.accept(new RexVisitor(schema, false, order.getCluster().getRexBuilder()));\n          } else {\n            ColumnInfo cI = schema.get(c.getFieldIndex());\n            /*\n             * The RowResolver setup for Select drops Table associations. So\n             * setup ASTNode on unqualified name.\n             */\n            astCol = ASTBuilder.unqualifiedName(cI.column);\n          }\n\n          // 4 buildup the ob expr AST\n          nullDirectionAST.addChild(astCol);\n          orderAst.addChild(directionAST);\n        }\n        hiveAST.order = orderAst;\n      }\n\n      RexNode offsetExpr = hiveSortLimit.getOffsetExpr();\n      RexNode fetchExpr = hiveSortLimit.getFetchExpr();\n      if (fetchExpr != null) {\n        Object offset = (offsetExpr == null) ?\n            new Integer(0) : ((RexLiteral) offsetExpr).getValue2();\n        Object fetch = ((RexLiteral) fetchExpr).getValue2();\n        hiveAST.limit = ASTBuilder.limit(offset, fetch);\n      }\n    }\n  }"
        ],
        [
            "ASTConverter::RexVisitor::visitCall(RexCall)",
            " 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673 -\n 674  \n 675  \n 676  \n 677 -\n 678  \n 679  \n 680  \n 681 -\n 682 -\n 683 -\n 684  \n 685  \n 686  \n 687  \n 688  \n 689 -\n 690 -\n 691 -\n 692 -\n 693 -\n 694 -\n 695 -\n 696 -\n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  ",
            "    @Override\n    public ASTNode visitCall(RexCall call) {\n      if (!deep) {\n        return null;\n      }\n\n      SqlOperator op = call.getOperator();\n      List<ASTNode> astNodeLst = new LinkedList<ASTNode>();\n      if (op.kind == SqlKind.CAST) {\n        HiveToken ht = TypeConverter.hiveToken(call.getType());\n        ASTBuilder astBldr = ASTBuilder.construct(ht.type, ht.text);\n        if (ht.args != null) {\n          for (String castArg : ht.args)\n            astBldr.add(HiveParser.Identifier, castArg);\n        }\n        astNodeLst.add(astBldr.node());\n      }\n\n      if (op.kind == SqlKind.EXTRACT) {\n        // Extract on date: special handling since function in Hive does\n        // include <time_unit>. Observe that <time_unit> information\n        // is implicit in the function name, thus translation will\n        // proceed correctly if we just ignore the <time_unit>\n        astNodeLst.add(call.operands.get(1).accept(this));\n      } else if (op.kind == SqlKind.FLOOR &&\n              call.operands.size() == 2) {\n        // Floor on date: special handling since function in Hive does\n        // include <time_unit>. Observe that <time_unit> information\n        // is implicit in the function name, thus translation will\n        // proceed correctly if we just ignore the <time_unit>\n        astNodeLst.add(call.operands.get(0).accept(this));\n      } else {\n        for (RexNode operand : call.operands) {\n          astNodeLst.add(operand.accept(this));\n        }\n      }\n\n      if (isFlat(call)) {\n        return SqlFunctionConverter.buildAST(op, astNodeLst, 0);\n      } else {\n        return SqlFunctionConverter.buildAST(op, astNodeLst);\n      }\n    }",
            " 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673 +\n 674 +\n 675 +\n 676 +\n 677 +\n 678 +\n 679 +\n 680 +\n 681 +\n 682 +\n 683 +\n 684 +\n 685 +\n 686 +\n 687 +\n 688 +\n 689 +\n 690 +\n 691  \n 692  \n 693  \n 694 +\n 695  \n 696 +\n 697  \n 698  \n 699 +\n 700 +\n 701 +\n 702 +\n 703 +\n 704  \n 705  \n 706  \n 707  \n 708  \n 709 +\n 710 +\n 711 +\n 712 +\n 713 +\n 714 +\n 715 +\n 716 +\n 717 +\n 718 +\n 719 +\n 720 +\n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  ",
            "    @Override\n    public ASTNode visitCall(RexCall call) {\n      if (!deep) {\n        return null;\n      }\n\n      SqlOperator op = call.getOperator();\n      List<ASTNode> astNodeLst = new LinkedList<ASTNode>();\n      switch (op.kind) {\n      case EQUALS:\n      case NOT_EQUALS:\n      case LESS_THAN:\n      case GREATER_THAN:\n      case LESS_THAN_OR_EQUAL:\n      case GREATER_THAN_OR_EQUAL:\n        if (rexBuilder != null && RexUtil.isReferenceOrAccess(call.operands.get(1), true) &&\n            RexUtil.isLiteral(call.operands.get(0), true)) {\n          // Swap to get reference on the left side\n          return visitCall((RexCall) RexUtil.invert(rexBuilder, call));\n        } else {\n          for (RexNode operand : call.operands) {\n            astNodeLst.add(operand.accept(this));\n          }\n        }\n        break;\n      case CAST:\n        HiveToken ht = TypeConverter.hiveToken(call.getType());\n        ASTBuilder astBldr = ASTBuilder.construct(ht.type, ht.text);\n        if (ht.args != null) {\n          for (String castArg : ht.args) {\n            astBldr.add(HiveParser.Identifier, castArg);\n          }\n        }\n        astNodeLst.add(astBldr.node());\n        for (RexNode operand : call.operands) {\n          astNodeLst.add(operand.accept(this));\n        }\n        break;\n      case EXTRACT:\n        // Extract on date: special handling since function in Hive does\n        // include <time_unit>. Observe that <time_unit> information\n        // is implicit in the function name, thus translation will\n        // proceed correctly if we just ignore the <time_unit>\n        astNodeLst.add(call.operands.get(1).accept(this));\n        break;\n      case FLOOR:\n        if (call.operands.size() == 2) {\n          // Floor on date: special handling since function in Hive does\n          // include <time_unit>. Observe that <time_unit> information\n          // is implicit in the function name, thus translation will\n          // proceed correctly if we just ignore the <time_unit>\n          astNodeLst.add(call.operands.get(0).accept(this));\n          break;\n        }\n        // fall-through\n      default:\n        for (RexNode operand : call.operands) {\n          astNodeLst.add(operand.accept(this));\n        }\n      }\n\n      if (isFlat(call)) {\n        return SqlFunctionConverter.buildAST(op, astNodeLst, 0);\n      } else {\n        return SqlFunctionConverter.buildAST(op, astNodeLst);\n      }\n    }"
        ],
        [
            "ASTConverter::Schema::Schema(Schema,Aggregate)",
            " 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787 -\n 788  \n 789  \n 790  \n 791  ",
            "    Schema(Schema src, Aggregate gBy) {\n      for (int i : gBy.getGroupSet()) {\n        ColumnInfo cI = src.get(i);\n        add(cI);\n      }\n      List<AggregateCall> aggs = gBy.getAggCallList();\n      for (AggregateCall agg : aggs) {\n        if (agg.getAggregation() == HiveGroupingID.INSTANCE) {\n          add(new ColumnInfo(null,VirtualColumn.GROUPINGID.getName()));\n          continue;\n        }\n        int argCount = agg.getArgList().size();\n        ASTBuilder b = agg.isDistinct() ? ASTBuilder.construct(HiveParser.TOK_FUNCTIONDI,\n            \"TOK_FUNCTIONDI\") : argCount == 0 ? ASTBuilder.construct(HiveParser.TOK_FUNCTIONSTAR,\n            \"TOK_FUNCTIONSTAR\") : ASTBuilder.construct(HiveParser.TOK_FUNCTION, \"TOK_FUNCTION\");\n        b.add(HiveParser.Identifier, agg.getAggregation().getName());\n        for (int i : agg.getArgList()) {\n          RexInputRef iRef = new RexInputRef(i, gBy.getCluster().getTypeFactory()\n              .createSqlType(SqlTypeName.ANY));\n          b.add(iRef.accept(new RexVisitor(src)));\n        }\n        add(new ColumnInfo(null, b.node()));\n      }\n    }",
            " 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811 +\n 812  \n 813  \n 814  \n 815  ",
            "    Schema(Schema src, Aggregate gBy) {\n      for (int i : gBy.getGroupSet()) {\n        ColumnInfo cI = src.get(i);\n        add(cI);\n      }\n      List<AggregateCall> aggs = gBy.getAggCallList();\n      for (AggregateCall agg : aggs) {\n        if (agg.getAggregation() == HiveGroupingID.INSTANCE) {\n          add(new ColumnInfo(null,VirtualColumn.GROUPINGID.getName()));\n          continue;\n        }\n        int argCount = agg.getArgList().size();\n        ASTBuilder b = agg.isDistinct() ? ASTBuilder.construct(HiveParser.TOK_FUNCTIONDI,\n            \"TOK_FUNCTIONDI\") : argCount == 0 ? ASTBuilder.construct(HiveParser.TOK_FUNCTIONSTAR,\n            \"TOK_FUNCTIONSTAR\") : ASTBuilder.construct(HiveParser.TOK_FUNCTION, \"TOK_FUNCTION\");\n        b.add(HiveParser.Identifier, agg.getAggregation().getName());\n        for (int i : agg.getArgList()) {\n          RexInputRef iRef = new RexInputRef(i, gBy.getCluster().getTypeFactory()\n              .createSqlType(SqlTypeName.ANY));\n          b.add(iRef.accept(new RexVisitor(src, false, gBy.getCluster().getRexBuilder())));\n        }\n        add(new ColumnInfo(null, b.node()));\n      }\n    }"
        ],
        [
            "ASTConverter::convertSource(RelNode)",
            " 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356 -\n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  ",
            "  private QueryBlockInfo convertSource(RelNode r) throws CalciteSemanticException {\n    Schema s = null;\n    ASTNode ast = null;\n\n    if (r instanceof TableScan) {\n      TableScan f = (TableScan) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof DruidQuery) {\n      DruidQuery f = (DruidQuery) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof Join) {\n      Join join = (Join) r;\n      QueryBlockInfo left = convertSource(join.getLeft());\n      QueryBlockInfo right = convertSource(join.getRight());\n      s = new Schema(left.schema, right.schema);\n      ASTNode cond = join.getCondition().accept(new RexVisitor(s));\n      boolean semiJoin = join instanceof SemiJoin;\n      if (join.getRight() instanceof Join && !semiJoin) {\n          // should not be done for semijoin since it will change the semantics\n        // Invert join inputs; this is done because otherwise the SemanticAnalyzer\n        // methods to merge joins will not kick in\n        JoinRelType type;\n        if (join.getJoinType() == JoinRelType.LEFT) {\n          type = JoinRelType.RIGHT;\n        } else if (join.getJoinType() == JoinRelType.RIGHT) {\n          type = JoinRelType.LEFT;\n        } else {\n          type = join.getJoinType();\n        }\n        ast = ASTBuilder.join(right.ast, left.ast, type, cond, semiJoin);\n      } else {\n        ast = ASTBuilder.join(left.ast, right.ast, join.getJoinType(), cond, semiJoin);\n      }\n      if (semiJoin) {\n        s = left.schema;\n      }\n    } else if (r instanceof Union) {\n      Union u = ((Union) r);\n      ASTNode left = new ASTConverter(((Union) r).getInput(0), this.derivedTableCount).convert();\n      for (int ind = 1; ind < u.getInputs().size(); ind++) {\n        left = getUnionAllAST(left, new ASTConverter(((Union) r).getInput(ind),\n            this.derivedTableCount).convert());\n        String sqAlias = nextAlias();\n        ast = ASTBuilder.subQuery(left, sqAlias);\n        s = new Schema((Union) r, sqAlias);\n      }\n    } else {\n      ASTConverter src = new ASTConverter(r, this.derivedTableCount);\n      ASTNode srcAST = src.convert();\n      String sqAlias = nextAlias();\n      s = src.getRowSchema(sqAlias);\n      ast = ASTBuilder.subQuery(srcAST, sqAlias);\n    }\n    return new QueryBlockInfo(s, ast);\n  }",
            " 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356 +\n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  ",
            "  private QueryBlockInfo convertSource(RelNode r) throws CalciteSemanticException {\n    Schema s = null;\n    ASTNode ast = null;\n\n    if (r instanceof TableScan) {\n      TableScan f = (TableScan) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof DruidQuery) {\n      DruidQuery f = (DruidQuery) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof Join) {\n      Join join = (Join) r;\n      QueryBlockInfo left = convertSource(join.getLeft());\n      QueryBlockInfo right = convertSource(join.getRight());\n      s = new Schema(left.schema, right.schema);\n      ASTNode cond = join.getCondition().accept(new RexVisitor(s, false, r.getCluster().getRexBuilder()));\n      boolean semiJoin = join instanceof SemiJoin;\n      if (join.getRight() instanceof Join && !semiJoin) {\n          // should not be done for semijoin since it will change the semantics\n        // Invert join inputs; this is done because otherwise the SemanticAnalyzer\n        // methods to merge joins will not kick in\n        JoinRelType type;\n        if (join.getJoinType() == JoinRelType.LEFT) {\n          type = JoinRelType.RIGHT;\n        } else if (join.getJoinType() == JoinRelType.RIGHT) {\n          type = JoinRelType.LEFT;\n        } else {\n          type = join.getJoinType();\n        }\n        ast = ASTBuilder.join(right.ast, left.ast, type, cond, semiJoin);\n      } else {\n        ast = ASTBuilder.join(left.ast, right.ast, join.getJoinType(), cond, semiJoin);\n      }\n      if (semiJoin) {\n        s = left.schema;\n      }\n    } else if (r instanceof Union) {\n      Union u = ((Union) r);\n      ASTNode left = new ASTConverter(((Union) r).getInput(0), this.derivedTableCount).convert();\n      for (int ind = 1; ind < u.getInputs().size(); ind++) {\n        left = getUnionAllAST(left, new ASTConverter(((Union) r).getInput(ind),\n            this.derivedTableCount).convert());\n        String sqAlias = nextAlias();\n        ast = ASTBuilder.subQuery(left, sqAlias);\n        s = new Schema((Union) r, sqAlias);\n      }\n    } else {\n      ASTConverter src = new ASTConverter(r, this.derivedTableCount);\n      ASTNode srcAST = src.convert();\n      String sqAlias = nextAlias();\n      s = src.getRowSchema(sqAlias);\n      ast = ASTBuilder.subQuery(srcAST, sqAlias);\n    }\n    return new QueryBlockInfo(s, ast);\n  }"
        ],
        [
            "ASTConverter::convert()",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 -\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 -\n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176 -\n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 -\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "  private ASTNode convert() throws CalciteSemanticException {\n    /*\n     * 1. Walk RelNode Graph; note from, where, gBy.. nodes.\n     */\n    new QBVisitor().go(root);\n\n    /*\n     * 2. convert from node.\n     */\n    QueryBlockInfo qb = convertSource(from);\n    schema = qb.schema;\n    hiveAST.from = ASTBuilder.construct(HiveParser.TOK_FROM, \"TOK_FROM\").add(qb.ast).node();\n\n    /*\n     * 3. convert filterNode\n     */\n    if (where != null) {\n      ASTNode cond = where.getCondition().accept(new RexVisitor(schema));\n      hiveAST.where = ASTBuilder.where(cond);\n    }\n\n    /*\n     * 4. GBy\n     */\n    if (groupBy != null) {\n      ASTBuilder b;\n      boolean groupingSetsExpression = false;\n      Group aggregateType = groupBy.getGroupType();\n      switch (aggregateType) {\n        case SIMPLE:\n          b = ASTBuilder.construct(HiveParser.TOK_GROUPBY, \"TOK_GROUPBY\");\n          break;\n        case ROLLUP:\n          b = ASTBuilder.construct(HiveParser.TOK_ROLLUP_GROUPBY, \"TOK_ROLLUP_GROUPBY\");\n          break;\n        case CUBE:\n          b = ASTBuilder.construct(HiveParser.TOK_CUBE_GROUPBY, \"TOK_CUBE_GROUPBY\");\n          break;\n        case OTHER:\n          b = ASTBuilder.construct(HiveParser.TOK_GROUPING_SETS, \"TOK_GROUPING_SETS\");\n          groupingSetsExpression = true;\n          break;\n        default:\n          throw new CalciteSemanticException(\"Group type not recognized\");\n      }\n\n      HiveAggregate hiveAgg = (HiveAggregate) groupBy;\n      for (int pos : hiveAgg.getAggregateColumnsOrder()) {\n        RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n            groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n        b.add(iRef.accept(new RexVisitor(schema)));\n      }\n      for (int pos = 0; pos < groupBy.getGroupCount(); pos++) {\n        if (!hiveAgg.getAggregateColumnsOrder().contains(pos)) {\n          RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n              groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n          b.add(iRef.accept(new RexVisitor(schema)));\n        }\n      }\n\n      //Grouping sets expressions\n      if(groupingSetsExpression) {\n        for(ImmutableBitSet groupSet: groupBy.getGroupSets()) {\n          ASTBuilder expression = ASTBuilder.construct(\n                  HiveParser.TOK_GROUPING_SETS_EXPRESSION, \"TOK_GROUPING_SETS_EXPRESSION\");\n          for (int i : groupSet) {\n            RexInputRef iRef = new RexInputRef(i, groupBy.getCluster().getTypeFactory()\n                .createSqlType(SqlTypeName.ANY));\n            expression.add(iRef.accept(new RexVisitor(schema)));\n          }\n          b.add(expression);\n        }\n      }\n\n      if (!groupBy.getGroupSet().isEmpty()) {\n        hiveAST.groupBy = b.node();\n      }\n\n      schema = new Schema(schema, groupBy);\n    }\n\n    /*\n     * 5. Having\n     */\n    if (having != null) {\n      ASTNode cond = having.getCondition().accept(new RexVisitor(schema));\n      hiveAST.having = ASTBuilder.having(cond);\n    }\n\n    /*\n     * 6. Project\n     */\n    ASTBuilder b = ASTBuilder.construct(HiveParser.TOK_SELECT, \"TOK_SELECT\");\n\n    if (select instanceof Project) {\n      List<RexNode> childExps = ((Project) select).getChildExps();\n      if (childExps.isEmpty()) {\n        RexLiteral r = select.getCluster().getRexBuilder().makeExactLiteral(new BigDecimal(1));\n        ASTNode selectExpr = ASTBuilder.selectExpr(ASTBuilder.literal(r), \"1\");\n        b.add(selectExpr);\n      } else {\n        int i = 0;\n\n        for (RexNode r : childExps) {\n          ASTNode expr = r.accept(new RexVisitor(schema, r instanceof RexLiteral,\n              select.getCluster().getRexBuilder()));\n          String alias = select.getRowType().getFieldNames().get(i++);\n          ASTNode selectExpr = ASTBuilder.selectExpr(expr, alias);\n          b.add(selectExpr);\n        }\n      }\n      hiveAST.select = b.node();\n    } else {\n      // select is UDTF\n      HiveTableFunctionScan udtf = (HiveTableFunctionScan) select;\n      List<ASTNode> children = new ArrayList<>();\n      RexCall call = (RexCall) udtf.getCall();\n      for (RexNode r : call.getOperands()) {\n        ASTNode expr = r.accept(new RexVisitor(schema, r instanceof RexLiteral,\n            select.getCluster().getRexBuilder()));\n        children.add(expr);\n      }\n      ASTBuilder sel = ASTBuilder.construct(HiveParser.TOK_SELEXPR, \"TOK_SELEXPR\");\n      ASTNode function = buildUDTFAST(call.getOperator().getName(), children);\n      sel.add(function);\n      for (String alias : udtf.getRowType().getFieldNames()) {\n        sel.add(HiveParser.Identifier, alias);\n      }\n      b.add(sel);\n      hiveAST.select = b.node();\n    }\n\n    /*\n     * 7. Order Use in Order By from the block above. RelNode has no pointer to\n     * parent hence we need to go top down; but OB at each block really belong\n     * to its src/from. Hence the need to pass in sort for each block from\n     * its parent.\n     * 8. Limit\n     */\n    convertOrderLimitToASTNode((HiveSortLimit) orderLimit);\n\n    return hiveAST.getAST();\n  }",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176 +\n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 +\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "  private ASTNode convert() throws CalciteSemanticException {\n    /*\n     * 1. Walk RelNode Graph; note from, where, gBy.. nodes.\n     */\n    new QBVisitor().go(root);\n\n    /*\n     * 2. convert from node.\n     */\n    QueryBlockInfo qb = convertSource(from);\n    schema = qb.schema;\n    hiveAST.from = ASTBuilder.construct(HiveParser.TOK_FROM, \"TOK_FROM\").add(qb.ast).node();\n\n    /*\n     * 3. convert filterNode\n     */\n    if (where != null) {\n      ASTNode cond = where.getCondition().accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder()));\n      hiveAST.where = ASTBuilder.where(cond);\n    }\n\n    /*\n     * 4. GBy\n     */\n    if (groupBy != null) {\n      ASTBuilder b;\n      boolean groupingSetsExpression = false;\n      Group aggregateType = groupBy.getGroupType();\n      switch (aggregateType) {\n        case SIMPLE:\n          b = ASTBuilder.construct(HiveParser.TOK_GROUPBY, \"TOK_GROUPBY\");\n          break;\n        case ROLLUP:\n          b = ASTBuilder.construct(HiveParser.TOK_ROLLUP_GROUPBY, \"TOK_ROLLUP_GROUPBY\");\n          break;\n        case CUBE:\n          b = ASTBuilder.construct(HiveParser.TOK_CUBE_GROUPBY, \"TOK_CUBE_GROUPBY\");\n          break;\n        case OTHER:\n          b = ASTBuilder.construct(HiveParser.TOK_GROUPING_SETS, \"TOK_GROUPING_SETS\");\n          groupingSetsExpression = true;\n          break;\n        default:\n          throw new CalciteSemanticException(\"Group type not recognized\");\n      }\n\n      HiveAggregate hiveAgg = (HiveAggregate) groupBy;\n      for (int pos : hiveAgg.getAggregateColumnsOrder()) {\n        RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n            groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n        b.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n      }\n      for (int pos = 0; pos < groupBy.getGroupCount(); pos++) {\n        if (!hiveAgg.getAggregateColumnsOrder().contains(pos)) {\n          RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n              groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n          b.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n        }\n      }\n\n      //Grouping sets expressions\n      if(groupingSetsExpression) {\n        for(ImmutableBitSet groupSet: groupBy.getGroupSets()) {\n          ASTBuilder expression = ASTBuilder.construct(\n                  HiveParser.TOK_GROUPING_SETS_EXPRESSION, \"TOK_GROUPING_SETS_EXPRESSION\");\n          for (int i : groupSet) {\n            RexInputRef iRef = new RexInputRef(i, groupBy.getCluster().getTypeFactory()\n                .createSqlType(SqlTypeName.ANY));\n            expression.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n          }\n          b.add(expression);\n        }\n      }\n\n      if (!groupBy.getGroupSet().isEmpty()) {\n        hiveAST.groupBy = b.node();\n      }\n\n      schema = new Schema(schema, groupBy);\n    }\n\n    /*\n     * 5. Having\n     */\n    if (having != null) {\n      ASTNode cond = having.getCondition().accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder()));\n      hiveAST.having = ASTBuilder.having(cond);\n    }\n\n    /*\n     * 6. Project\n     */\n    ASTBuilder b = ASTBuilder.construct(HiveParser.TOK_SELECT, \"TOK_SELECT\");\n\n    if (select instanceof Project) {\n      List<RexNode> childExps = ((Project) select).getChildExps();\n      if (childExps.isEmpty()) {\n        RexLiteral r = select.getCluster().getRexBuilder().makeExactLiteral(new BigDecimal(1));\n        ASTNode selectExpr = ASTBuilder.selectExpr(ASTBuilder.literal(r), \"1\");\n        b.add(selectExpr);\n      } else {\n        int i = 0;\n\n        for (RexNode r : childExps) {\n          ASTNode expr = r.accept(new RexVisitor(schema, r instanceof RexLiteral,\n              select.getCluster().getRexBuilder()));\n          String alias = select.getRowType().getFieldNames().get(i++);\n          ASTNode selectExpr = ASTBuilder.selectExpr(expr, alias);\n          b.add(selectExpr);\n        }\n      }\n      hiveAST.select = b.node();\n    } else {\n      // select is UDTF\n      HiveTableFunctionScan udtf = (HiveTableFunctionScan) select;\n      List<ASTNode> children = new ArrayList<>();\n      RexCall call = (RexCall) udtf.getCall();\n      for (RexNode r : call.getOperands()) {\n        ASTNode expr = r.accept(new RexVisitor(schema, r instanceof RexLiteral,\n            select.getCluster().getRexBuilder()));\n        children.add(expr);\n      }\n      ASTBuilder sel = ASTBuilder.construct(HiveParser.TOK_SELEXPR, \"TOK_SELEXPR\");\n      ASTNode function = buildUDTFAST(call.getOperator().getName(), children);\n      sel.add(function);\n      for (String alias : udtf.getRowType().getFieldNames()) {\n        sel.add(HiveParser.Identifier, alias);\n      }\n      b.add(sel);\n      hiveAST.select = b.node();\n    }\n\n    /*\n     * 7. Order Use in Order By from the block above. RelNode has no pointer to\n     * parent hence we need to go top down; but OB at each block really belong\n     * to its src/from. Hence the need to pass in sort for each block from\n     * its parent.\n     * 8. Limit\n     */\n    convertOrderLimitToASTNode((HiveSortLimit) orderLimit);\n\n    return hiveAST.getAST();\n  }"
        ]
    ],
    "14bb998cfdac46f985c6d383df634cd569abc65b": [
        [
            "TestTriggersMoveWorkloadManager::testTriggerMoveBackKill()",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169 -\n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  ",
            "  @Test(timeout = 60000)\n  public void testTriggerMoveBackKill() throws Exception {\n    Expression moveExpression1 = ExpressionFactory.fromString(\"HDFS_BYTES_READ > 100\");\n    Expression moveExpression2 = ExpressionFactory.fromString(\"SHUFFLE_BYTES > 200\");\n    Expression killExpression = ExpressionFactory.fromString(\"EXECUTION_TIME > 2000\");\n    Trigger moveTrigger1 = new ExecutionTrigger(\"move_big_read\", moveExpression1,\n      new Action(Action.Type.MOVE_TO_POOL, \"ETL\"));\n    Trigger moveTrigger2 = new ExecutionTrigger(\"move_high\", moveExpression2,\n      new Action(Action.Type.MOVE_TO_POOL, \"BI\"));\n    Trigger killTrigger = new ExecutionTrigger(\"slow_query_kill\", killExpression,\n      new Action(Action.Type.KILL_QUERY));\n    setupTriggers(Lists.newArrayList(moveTrigger1, killTrigger), Lists.newArrayList(moveTrigger2));\n    String query = \"select sleep(t1.under_col, 1), t1.value from \" + tableName + \" t1 join \" + tableName +\n      \" t2 on t1.under_col>=t2.under_col\";\n    List<String> setCmds = new ArrayList<>();\n    setCmds.add(\"set hive.tez.session.events.print.summary=json\");\n    setCmds.add(\"set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    setCmds.add(\"set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    List<String> errCaptureExpect = new ArrayList<>();\n    errCaptureExpect.add(\"Workload Manager Events Summary\");\n    errCaptureExpect.add(\"Event: GET Pool: BI Cluster %: 80.00\");\n    errCaptureExpect.add(\"Event: MOVE Pool: ETL Cluster %: 20.00\");\n    errCaptureExpect.add(\"Event: MOVE Pool: BI Cluster %: 80.00\");\n    errCaptureExpect.add(\"Event: KILL Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"Event: RETURN Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"GET\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"MOVE\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"MOVE\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"KILL\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"RETURN\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"move_big_read\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"slow_query_kill\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"move_high\\\"\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + moveTrigger1 + \" violated\");\n    // violation in ETL queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + moveTrigger2 + \" violated\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + killTrigger + \" violated\");\n    errCaptureExpect.add(\"\\\"subscribedCounters\\\" : [ \\\"HDFS_BYTES_READ\\\", \\\"EXECUTION_TIME\\\", \\\"SHUFFLE_BYTES\\\" ]\");\n    runQueryWithTrigger(query, setCmds, killTrigger + \" violated\", errCaptureExpect);\n  }",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172 +\n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "  @Test(timeout = 60000)\n  public void testTriggerMoveBackKill() throws Exception {\n    Expression moveExpression1 = ExpressionFactory.fromString(\"HDFS_BYTES_READ > 100\");\n    Expression moveExpression2 = ExpressionFactory.fromString(\"SHUFFLE_BYTES > 200\");\n    Expression killExpression = ExpressionFactory.fromString(\"EXECUTION_TIME > 2000\");\n    Trigger moveTrigger1 = new ExecutionTrigger(\"move_big_read\", moveExpression1,\n      new Action(Action.Type.MOVE_TO_POOL, \"ETL\"));\n    Trigger moveTrigger2 = new ExecutionTrigger(\"move_high\", moveExpression2,\n      new Action(Action.Type.MOVE_TO_POOL, \"BI\"));\n    Trigger killTrigger = new ExecutionTrigger(\"slow_query_kill\", killExpression,\n      new Action(Action.Type.KILL_QUERY));\n    setupTriggers(Lists.newArrayList(moveTrigger1, killTrigger), Lists.newArrayList(moveTrigger2));\n    String query = \"select sleep(t1.under_col, 1), t1.value from \" + tableName + \" t1 join \" + tableName +\n      \" t2 on t1.under_col>=t2.under_col\";\n    List<String> setCmds = new ArrayList<>();\n    setCmds.add(\"set hive.tez.session.events.print.summary=json\");\n    setCmds.add(\"set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    setCmds.add(\"set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    List<String> errCaptureExpect = new ArrayList<>();\n    errCaptureExpect.add(\"Workload Manager Events Summary\");\n    errCaptureExpect.add(\"Event: GET Pool: BI\");\n    // HIVE-19061 introduces UPDATE event which will capture changes to allocation % after GET\n    errCaptureExpect.add(\"Event: MOVE Pool: ETL Cluster %: 20.00\");\n    errCaptureExpect.add(\"Event: MOVE Pool: BI Cluster %: 80.00\");\n    errCaptureExpect.add(\"Event: KILL Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"Event: RETURN Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"GET\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"MOVE\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"MOVE\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"KILL\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"RETURN\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"move_big_read\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"slow_query_kill\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"move_high\\\"\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + moveTrigger1 + \" violated\");\n    // violation in ETL queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + moveTrigger2 + \" violated\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + killTrigger + \" violated\");\n    errCaptureExpect.add(\"\\\"subscribedCounters\\\" : [ \\\"HDFS_BYTES_READ\\\", \\\"EXECUTION_TIME\\\", \\\"SHUFFLE_BYTES\\\" ]\");\n    runQueryWithTrigger(query, setCmds, killTrigger + \" violated\", errCaptureExpect);\n  }"
        ],
        [
            "TestTriggersMoveWorkloadManager::testTriggerMoveEscapeKill()",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135 -\n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  ",
            "  @Test(timeout = 60000)\n  public void testTriggerMoveEscapeKill() throws Exception {\n    Expression moveExpression = ExpressionFactory.fromString(\"HDFS_BYTES_READ > 100\");\n    Expression killExpression = ExpressionFactory.fromString(\"EXECUTION_TIME > 5000\");\n    Trigger moveTrigger = new ExecutionTrigger(\"move_big_read\", moveExpression,\n      new Action(Action.Type.MOVE_TO_POOL, \"ETL\"));\n    Trigger killTrigger = new ExecutionTrigger(\"slow_query_kill\", killExpression,\n      new Action(Action.Type.KILL_QUERY));\n    setupTriggers(Lists.newArrayList(moveTrigger, killTrigger), Lists.newArrayList());\n    String query = \"select sleep(t1.under_col, 1), t1.value from \" + tableName + \" t1 join \" + tableName +\n      \" t2 on t1.under_col==t2.under_col\";\n    List<String> setCmds = new ArrayList<>();\n    setCmds.add(\"set hive.tez.session.events.print.summary=json\");\n    setCmds.add(\"set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    setCmds.add(\"set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    List<String> errCaptureExpect = new ArrayList<>();\n    errCaptureExpect.add(\"Workload Manager Events Summary\");\n    errCaptureExpect.add(\"Event: GET Pool: BI Cluster %: 80.00\");\n    errCaptureExpect.add(\"Event: MOVE Pool: ETL Cluster %: 20.00\");\n    errCaptureExpect.add(\"Event: RETURN Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"GET\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"MOVE\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"RETURN\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"move_big_read\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"slow_query_kill\\\"\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + moveTrigger + \" violated\");\n    errCaptureExpect.add(\"\\\"subscribedCounters\\\" : [ \\\"HDFS_BYTES_READ\\\", \\\"EXECUTION_TIME\\\" ]\");\n    runQueryWithTrigger(query, setCmds, null, errCaptureExpect);\n  }",
            " 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 +\n 137 +\n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "  @Test(timeout = 60000)\n  public void testTriggerMoveEscapeKill() throws Exception {\n    Expression moveExpression = ExpressionFactory.fromString(\"HDFS_BYTES_READ > 100\");\n    Expression killExpression = ExpressionFactory.fromString(\"EXECUTION_TIME > 5000\");\n    Trigger moveTrigger = new ExecutionTrigger(\"move_big_read\", moveExpression,\n      new Action(Action.Type.MOVE_TO_POOL, \"ETL\"));\n    Trigger killTrigger = new ExecutionTrigger(\"slow_query_kill\", killExpression,\n      new Action(Action.Type.KILL_QUERY));\n    setupTriggers(Lists.newArrayList(moveTrigger, killTrigger), Lists.newArrayList());\n    String query = \"select sleep(t1.under_col, 1), t1.value from \" + tableName + \" t1 join \" + tableName +\n      \" t2 on t1.under_col==t2.under_col\";\n    List<String> setCmds = new ArrayList<>();\n    setCmds.add(\"set hive.tez.session.events.print.summary=json\");\n    setCmds.add(\"set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    setCmds.add(\"set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    List<String> errCaptureExpect = new ArrayList<>();\n    errCaptureExpect.add(\"Workload Manager Events Summary\");\n    errCaptureExpect.add(\"Event: GET Pool: BI\");\n    // HIVE-19061 introduces UPDATE event which will capture changes to allocation % after GET\n    errCaptureExpect.add(\"Event: MOVE Pool: ETL Cluster %: 20.00\");\n    errCaptureExpect.add(\"Event: RETURN Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"GET\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"MOVE\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"RETURN\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"move_big_read\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"slow_query_kill\\\"\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + moveTrigger + \" violated\");\n    errCaptureExpect.add(\"\\\"subscribedCounters\\\" : [ \\\"HDFS_BYTES_READ\\\", \\\"EXECUTION_TIME\\\" ]\");\n    runQueryWithTrigger(query, setCmds, null, errCaptureExpect);\n  }"
        ],
        [
            "TestTriggersMoveWorkloadManager::testTriggerMoveConflictKill()",
            " 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246 -\n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  ",
            "  @Test(timeout = 60000)\n  public void testTriggerMoveConflictKill() throws Exception {\n    Expression moveExpression = ExpressionFactory.fromString(\"HDFS_BYTES_READ > 100\");\n    Expression killExpression = ExpressionFactory.fromString(\"HDFS_BYTES_READ > 100\");\n    Trigger moveTrigger = new ExecutionTrigger(\"move_big_read\", moveExpression,\n      new Action(Action.Type.MOVE_TO_POOL, \"ETL\"));\n    Trigger killTrigger = new ExecutionTrigger(\"kill_big_read\", killExpression,\n      new Action(Action.Type.KILL_QUERY));\n    setupTriggers(Lists.newArrayList(moveTrigger, killTrigger), Lists.newArrayList());\n    String query = \"select sleep(t1.under_col, 5), t1.value from \" + tableName + \" t1 join \" + tableName +\n      \" t2 on t1.under_col>=t2.under_col\";\n    List<String> setCmds = new ArrayList<>();\n    setCmds.add(\"set hive.tez.session.events.print.summary=json\");\n    setCmds.add(\"set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    setCmds.add(\"set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    List<String> errCaptureExpect = new ArrayList<>();\n    errCaptureExpect.add(\"Workload Manager Events Summary\");\n    errCaptureExpect.add(\"Event: GET Pool: BI Cluster %: 80.00\");\n    errCaptureExpect.add(\"Event: KILL Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"Event: RETURN Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"GET\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"KILL\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"RETURN\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"move_big_read\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"kill_big_read\\\"\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + killTrigger + \" violated\");\n    errCaptureExpect.add(\"\\\"subscribedCounters\\\" : [ \\\"HDFS_BYTES_READ\\\" ]\");\n    runQueryWithTrigger(query, setCmds, killTrigger + \" violated\", errCaptureExpect);\n  }",
            " 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249 +\n 250 +\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  ",
            "  @Test(timeout = 60000)\n  public void testTriggerMoveConflictKill() throws Exception {\n    Expression moveExpression = ExpressionFactory.fromString(\"HDFS_BYTES_READ > 100\");\n    Expression killExpression = ExpressionFactory.fromString(\"HDFS_BYTES_READ > 100\");\n    Trigger moveTrigger = new ExecutionTrigger(\"move_big_read\", moveExpression,\n      new Action(Action.Type.MOVE_TO_POOL, \"ETL\"));\n    Trigger killTrigger = new ExecutionTrigger(\"kill_big_read\", killExpression,\n      new Action(Action.Type.KILL_QUERY));\n    setupTriggers(Lists.newArrayList(moveTrigger, killTrigger), Lists.newArrayList());\n    String query = \"select sleep(t1.under_col, 5), t1.value from \" + tableName + \" t1 join \" + tableName +\n      \" t2 on t1.under_col>=t2.under_col\";\n    List<String> setCmds = new ArrayList<>();\n    setCmds.add(\"set hive.tez.session.events.print.summary=json\");\n    setCmds.add(\"set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    setCmds.add(\"set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    List<String> errCaptureExpect = new ArrayList<>();\n    errCaptureExpect.add(\"Workload Manager Events Summary\");\n    errCaptureExpect.add(\"Event: GET Pool: BI\");\n    // HIVE-19061 introduces UPDATE event which will capture changes to allocation % after GET\n    errCaptureExpect.add(\"Event: KILL Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"Event: RETURN Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"GET\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"KILL\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"RETURN\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"move_big_read\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"kill_big_read\\\"\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + killTrigger + \" violated\");\n    errCaptureExpect.add(\"\\\"subscribedCounters\\\" : [ \\\"HDFS_BYTES_READ\\\" ]\");\n    runQueryWithTrigger(query, setCmds, killTrigger + \" violated\", errCaptureExpect);\n  }"
        ],
        [
            "TestTriggersMoveWorkloadManager::testTriggerMoveAndKill()",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 -\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "  @Test(timeout = 60000)\n  public void testTriggerMoveAndKill() throws Exception {\n    Expression moveExpression = ExpressionFactory.fromString(\"EXECUTION_TIME > 1sec\");\n    Expression killExpression = ExpressionFactory.fromString(\"EXECUTION_TIME > 5000ms\");\n    Trigger moveTrigger = new ExecutionTrigger(\"slow_query_move\", moveExpression,\n      new Action(Action.Type.MOVE_TO_POOL, \"ETL\"));\n    Trigger killTrigger = new ExecutionTrigger(\"slow_query_kill\", killExpression,\n      new Action(Action.Type.KILL_QUERY));\n    setupTriggers(Lists.newArrayList(moveTrigger), Lists.newArrayList(killTrigger));\n    String query = \"select sleep(t1.under_col, 5), t1.value from \" + tableName + \" t1 join \" + tableName +\n      \" t2 on t1.under_col>=t2.under_col\";\n    List<String> setCmds = new ArrayList<>();\n    setCmds.add(\"set hive.tez.session.events.print.summary=json\");\n    setCmds.add(\"set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    setCmds.add(\"set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    List<String> errCaptureExpect = new ArrayList<>();\n    errCaptureExpect.add(\"Workload Manager Events Summary\");\n    errCaptureExpect.add(\"Event: GET Pool: BI Cluster %: 80.00\");\n    errCaptureExpect.add(\"Event: MOVE Pool: ETL Cluster %: 20.00\");\n    errCaptureExpect.add(\"Event: KILL Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"Event: RETURN Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"GET\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"MOVE\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"KILL\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"RETURN\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"slow_query_move\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"slow_query_kill\\\"\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + moveTrigger + \" violated\");\n    // violation in ETL queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + killTrigger + \" violated\");\n    errCaptureExpect.add(\"\\\"subscribedCounters\\\" : [ \\\"EXECUTION_TIME\\\" ]\");\n    runQueryWithTrigger(query, setCmds, killTrigger + \" violated\", errCaptureExpect);\n  }",
            "  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101 +\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "  @Test(timeout = 60000)\n  public void testTriggerMoveAndKill() throws Exception {\n    Expression moveExpression = ExpressionFactory.fromString(\"EXECUTION_TIME > 1sec\");\n    Expression killExpression = ExpressionFactory.fromString(\"EXECUTION_TIME > 5000ms\");\n    Trigger moveTrigger = new ExecutionTrigger(\"slow_query_move\", moveExpression,\n      new Action(Action.Type.MOVE_TO_POOL, \"ETL\"));\n    Trigger killTrigger = new ExecutionTrigger(\"slow_query_kill\", killExpression,\n      new Action(Action.Type.KILL_QUERY));\n    setupTriggers(Lists.newArrayList(moveTrigger), Lists.newArrayList(killTrigger));\n    String query = \"select sleep(t1.under_col, 5), t1.value from \" + tableName + \" t1 join \" + tableName +\n      \" t2 on t1.under_col>=t2.under_col\";\n    List<String> setCmds = new ArrayList<>();\n    setCmds.add(\"set hive.tez.session.events.print.summary=json\");\n    setCmds.add(\"set hive.exec.post.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    setCmds.add(\"set hive.exec.failure.hooks=org.apache.hadoop.hive.ql.hooks.PostExecWMEventsSummaryPrinter\");\n    List<String> errCaptureExpect = new ArrayList<>();\n    errCaptureExpect.add(\"Workload Manager Events Summary\");\n    errCaptureExpect.add(\"Event: GET Pool: BI\");\n    // HIVE-19061 introduces UPDATE event which will capture changes to allocation % after GET\n    errCaptureExpect.add(\"Event: MOVE Pool: ETL Cluster %: 20.00\");\n    errCaptureExpect.add(\"Event: KILL Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"Event: RETURN Pool: null Cluster %: 0.00\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"GET\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"MOVE\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"KILL\\\"\");\n    errCaptureExpect.add(\"\\\"eventType\\\" : \\\"RETURN\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"slow_query_move\\\"\");\n    errCaptureExpect.add(\"\\\"name\\\" : \\\"slow_query_kill\\\"\");\n    // violation in BI queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + moveTrigger + \" violated\");\n    // violation in ETL queue\n    errCaptureExpect.add(\"\\\"violationMsg\\\" : \\\"Trigger \" + killTrigger + \" violated\");\n    errCaptureExpect.add(\"\\\"subscribedCounters\\\" : [ \\\"EXECUTION_TIME\\\" ]\");\n    runQueryWithTrigger(query, setCmds, killTrigger + \" violated\", errCaptureExpect);\n  }"
        ]
    ],
    "803e1e0b89140350c40a7ec5c42f217532550495": [
        [
            "SparkClientImpl::startDriver(RpcServer,String,String)",
            " 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338 -\n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379 -\n 380 -\n 381 -\n 382 -\n 383 -\n 384 -\n 385 -\n 386 -\n 387 -\n 388 -\n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  ",
            "  private Thread startDriver(final RpcServer rpcServer, final String clientId, final String secret)\n      throws IOException {\n    Runnable runnable;\n    final String serverAddress = rpcServer.getAddress();\n    final String serverPort = String.valueOf(rpcServer.getPort());\n\n    if (conf.containsKey(SparkClientFactory.CONF_KEY_IN_PROCESS)) {\n      // Mostly for testing things quickly. Do not do this in production.\n      // when invoked in-process it inherits the environment variables of the parent\n      LOG.warn(\"!!!! Running remote driver in-process. !!!!\");\n      runnable = new Runnable() {\n        @Override\n        public void run() {\n          List<String> args = Lists.newArrayList();\n          args.add(\"--remote-host\");\n          args.add(serverAddress);\n          args.add(\"--remote-port\");\n          args.add(serverPort);\n          args.add(\"--client-id\");\n          args.add(clientId);\n          args.add(\"--secret\");\n          args.add(secret);\n\n          for (Map.Entry<String, String> e : conf.entrySet()) {\n            args.add(\"--conf\");\n            args.add(String.format(\"%s=%s\", e.getKey(), conf.get(e.getKey())));\n          }\n          try {\n            RemoteDriver.main(args.toArray(new String[args.size()]));\n          } catch (Exception e) {\n            LOG.error(\"Error running driver.\", e);\n          }\n        }\n      };\n    } else {\n      // If a Spark installation is provided, use the spark-submit script. Otherwise, call the\n      // SparkSubmit class directly, which has some caveats (like having to provide a proper\n      // version of Guava on the classpath depending on the deploy mode).\n      String sparkHome = Strings.emptyToNull(conf.get(SPARK_HOME_KEY));\n      if (sparkHome == null) {\n        sparkHome = Strings.emptyToNull(System.getenv(SPARK_HOME_ENV));\n      }\n      if (sparkHome == null) {\n        sparkHome = Strings.emptyToNull(System.getProperty(SPARK_HOME_KEY));\n      }\n      String sparkLogDir = conf.get(\"hive.spark.log.dir\");\n      if (sparkLogDir == null) {\n        if (sparkHome == null) {\n          sparkLogDir = \"./target/\";\n        } else {\n          sparkLogDir = sparkHome + \"/logs/\";\n        }\n      }\n\n      String osxTestOpts = \"\";\n      if (Strings.nullToEmpty(System.getProperty(\"os.name\")).toLowerCase().contains(\"mac\")) {\n        osxTestOpts = Strings.nullToEmpty(System.getenv(OSX_TEST_OPTS));\n      }\n\n      String driverJavaOpts = Joiner.on(\" \").skipNulls().join(\n          \"-Dhive.spark.log.dir=\" + sparkLogDir, osxTestOpts, conf.get(DRIVER_OPTS_KEY));\n      String executorJavaOpts = Joiner.on(\" \").skipNulls().join(\n          \"-Dhive.spark.log.dir=\" + sparkLogDir, osxTestOpts, conf.get(EXECUTOR_OPTS_KEY));\n\n      // Create a file with all the job properties to be read by spark-submit. Change the\n      // file's permissions so that only the owner can read it. This avoid having the\n      // connection secret show up in the child process's command line.\n      File properties = File.createTempFile(\"spark-submit.\", \".properties\");\n      if (!properties.setReadable(false) || !properties.setReadable(true, true)) {\n        throw new IOException(\"Cannot change permissions of job properties file.\");\n      }\n      properties.deleteOnExit();\n\n      Properties allProps = new Properties();\n      // first load the defaults from spark-defaults.conf if available\n      try {\n        URL sparkDefaultsUrl = Thread.currentThread().getContextClassLoader().getResource(\"spark-defaults.conf\");\n        if (sparkDefaultsUrl != null) {\n          LOG.info(\"Loading spark defaults: \" + sparkDefaultsUrl);\n          allProps.load(new ByteArrayInputStream(Resources.toByteArray(sparkDefaultsUrl)));\n        }\n      } catch (Exception e) {\n        String msg = \"Exception trying to load spark-defaults.conf: \" + e;\n        throw new IOException(msg, e);\n      }\n      // then load the SparkClientImpl config\n      for (Map.Entry<String, String> e : conf.entrySet()) {\n        allProps.put(e.getKey(), conf.get(e.getKey()));\n      }\n      allProps.put(SparkClientFactory.CONF_CLIENT_ID, clientId);\n      allProps.put(SparkClientFactory.CONF_KEY_SECRET, secret);\n      allProps.put(DRIVER_OPTS_KEY, driverJavaOpts);\n      allProps.put(EXECUTOR_OPTS_KEY, executorJavaOpts);\n\n      String isTesting = conf.get(\"spark.testing\");\n      if (isTesting != null && isTesting.equalsIgnoreCase(\"true\")) {\n        String hiveHadoopTestClasspath = Strings.nullToEmpty(System.getenv(\"HIVE_HADOOP_TEST_CLASSPATH\"));\n        if (!hiveHadoopTestClasspath.isEmpty()) {\n          String extraDriverClasspath = Strings.nullToEmpty((String)allProps.get(DRIVER_EXTRA_CLASSPATH));\n          if (extraDriverClasspath.isEmpty()) {\n            allProps.put(DRIVER_EXTRA_CLASSPATH, hiveHadoopTestClasspath);\n          } else {\n            extraDriverClasspath = extraDriverClasspath.endsWith(File.pathSeparator) ? extraDriverClasspath : extraDriverClasspath + File.pathSeparator;\n            allProps.put(DRIVER_EXTRA_CLASSPATH, extraDriverClasspath + hiveHadoopTestClasspath);\n          }\n\n          String extraExecutorClasspath = Strings.nullToEmpty((String)allProps.get(EXECUTOR_EXTRA_CLASSPATH));\n          if (extraExecutorClasspath.isEmpty()) {\n            allProps.put(EXECUTOR_EXTRA_CLASSPATH, hiveHadoopTestClasspath);\n          } else {\n            extraExecutorClasspath = extraExecutorClasspath.endsWith(File.pathSeparator) ? extraExecutorClasspath : extraExecutorClasspath + File.pathSeparator;\n            allProps.put(EXECUTOR_EXTRA_CLASSPATH, extraExecutorClasspath + hiveHadoopTestClasspath);\n          }\n        }\n      }\n\n      Writer writer = new OutputStreamWriter(new FileOutputStream(properties), Charsets.UTF_8);\n      try {\n        allProps.store(writer, \"Spark Context configuration\");\n      } finally {\n        writer.close();\n      }\n\n      // Define how to pass options to the child process. If launching in client (or local)\n      // mode, the driver options need to be passed directly on the command line. Otherwise,\n      // SparkSubmit will take care of that for us.\n      String master = conf.get(\"spark.master\");\n      Preconditions.checkArgument(master != null, \"spark.master is not defined.\");\n      String deployMode = conf.get(\"spark.submit.deployMode\");\n\n      List<String> argv = Lists.newArrayList();\n\n      if (sparkHome != null) {\n        argv.add(new File(sparkHome, \"bin/spark-submit\").getAbsolutePath());\n      } else {\n        LOG.info(\"No spark.home provided, calling SparkSubmit directly.\");\n        argv.add(new File(System.getProperty(\"java.home\"), \"bin/java\").getAbsolutePath());\n\n        if (master.startsWith(\"local\") || master.startsWith(\"mesos\") ||\n            SparkClientUtilities.isYarnClientMode(master, deployMode) ||\n            master.startsWith(\"spark\")) {\n          String mem = conf.get(\"spark.driver.memory\");\n          if (mem != null) {\n            argv.add(\"-Xms\" + mem);\n            argv.add(\"-Xmx\" + mem);\n          }\n\n          String cp = conf.get(\"spark.driver.extraClassPath\");\n          if (cp != null) {\n            argv.add(\"-classpath\");\n            argv.add(cp);\n          }\n\n          String libPath = conf.get(\"spark.driver.extraLibPath\");\n          if (libPath != null) {\n            argv.add(\"-Djava.library.path=\" + libPath);\n          }\n\n          String extra = conf.get(DRIVER_OPTS_KEY);\n          if (extra != null) {\n            for (String opt : extra.split(\"[ ]\")) {\n              if (!opt.trim().isEmpty()) {\n                argv.add(opt.trim());\n              }\n            }\n          }\n        }\n\n        argv.add(\"org.apache.spark.deploy.SparkSubmit\");\n      }\n\n      if (\"kerberos\".equals(hiveConf.get(HADOOP_SECURITY_AUTHENTICATION))) {\n          String principal = SecurityUtil.getServerPrincipal(hiveConf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL),\n              \"0.0.0.0\");\n          String keyTabFile = hiveConf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB);\n          argv.add(\"--principal\");\n          argv.add(principal);\n          argv.add(\"--keytab\");\n          argv.add(keyTabFile);\n      }\n\n      if (SparkClientUtilities.isYarnClusterMode(master, deployMode)) {\n        String executorCores = conf.get(\"spark.executor.cores\");\n        if (executorCores != null) {\n          argv.add(\"--executor-cores\");\n          argv.add(executorCores);\n        }\n\n        String executorMemory = conf.get(\"spark.executor.memory\");\n        if (executorMemory != null) {\n          argv.add(\"--executor-memory\");\n          argv.add(executorMemory);\n        }\n\n        String numOfExecutors = conf.get(\"spark.executor.instances\");\n        if (numOfExecutors != null) {\n          argv.add(\"--num-executors\");\n          argv.add(numOfExecutors);\n        }\n      }\n      if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS)) {\n        try {\n          String currentUser = Utils.getUGI().getShortUserName();\n          // do not do impersonation in CLI mode\n          if (!currentUser.equals(System.getProperty(\"user.name\"))) {\n            LOG.info(\"Attempting impersonation of \" + currentUser);\n            argv.add(\"--proxy-user\");\n            argv.add(currentUser);\n          }\n        } catch (Exception e) {\n          String msg = \"Cannot obtain username: \" + e;\n          throw new IllegalStateException(msg, e);\n        }\n      }\n\n      argv.add(\"--properties-file\");\n      argv.add(properties.getAbsolutePath());\n      argv.add(\"--class\");\n      argv.add(RemoteDriver.class.getName());\n\n      String jar = \"spark-internal\";\n      if (SparkContext.jarOfClass(this.getClass()).isDefined()) {\n        jar = SparkContext.jarOfClass(this.getClass()).get();\n      }\n      argv.add(jar);\n\n      argv.add(\"--remote-host\");\n      argv.add(serverAddress);\n      argv.add(\"--remote-port\");\n      argv.add(serverPort);\n\n      //hive.spark.* keys are passed down to the RemoteDriver via --conf,\n      //as --properties-file contains the spark.* keys that are meant for SparkConf object.\n      for (String hiveSparkConfKey : RpcConfiguration.HIVE_SPARK_RSC_CONFIGS) {\n        String value = RpcConfiguration.getValue(hiveConf, hiveSparkConfKey);\n        argv.add(\"--conf\");\n        argv.add(String.format(\"%s=%s\", hiveSparkConfKey, value));\n      }\n\n      String cmd = Joiner.on(\" \").join(argv);\n      LOG.info(\"Running client driver with argv: {}\", cmd);\n      ProcessBuilder pb = new ProcessBuilder(\"sh\", \"-c\", cmd);\n\n      // Prevent hive configurations from being visible in Spark.\n      pb.environment().remove(\"HIVE_HOME\");\n      pb.environment().remove(\"HIVE_CONF_DIR\");\n      // Add credential provider password to the child process's environment\n      // In case of Spark the credential provider location is provided in the jobConf when the job is submitted\n      String password = getSparkJobCredentialProviderPassword();\n      if(password != null) {\n        pb.environment().put(Constants.HADOOP_CREDENTIAL_PASSWORD_ENVVAR, password);\n      }\n      if (isTesting != null) {\n        pb.environment().put(\"SPARK_TESTING\", isTesting);\n      }\n\n      final Process child = pb.start();\n      int childId = childIdGenerator.incrementAndGet();\n      final List<String> childErrorLog = new ArrayList<String>();\n      redirect(\"stdout-redir-\" + childId, new Redirector(child.getInputStream()));\n      redirect(\"stderr-redir-\" + childId, new Redirector(child.getErrorStream(), childErrorLog));\n\n      runnable = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            int exitCode = child.waitFor();\n            if (exitCode != 0) {\n              StringBuilder errStr = new StringBuilder();\n              for (String s : childErrorLog) {\n                errStr.append(s);\n                errStr.append('\\n');\n              }\n\n              rpcServer.cancelClient(clientId,\n                  \"Child process exited before connecting back with error log \" + errStr.toString());\n              LOG.warn(\"Child process exited with code {}\", exitCode);\n            }\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Waiting thread interrupted, killing child process.\");\n            Thread.interrupted();\n            child.destroy();\n          } catch (Exception e) {\n            LOG.warn(\"Exception while waiting for child process.\", e);\n          }\n        }\n      };\n    }\n\n    Thread thread = new Thread(runnable);\n    thread.setDaemon(true);\n    thread.setName(\"Driver\");\n    thread.start();\n    return thread;\n  }",
            " 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338 +\n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398 +\n 399 +\n 400 +\n 401 +\n 402 +\n 403 +\n 404 +\n 405 +\n 406 +\n 407 +\n 408 +\n 409 +\n 410 +\n 411 +\n 412 +\n 413 +\n 414 +\n 415 +\n 416 +\n 417 +\n 418 +\n 419 +\n 420 +\n 421 +\n 422 +\n 423 +\n 424 +\n 425 +\n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  ",
            "  private Thread startDriver(final RpcServer rpcServer, final String clientId, final String secret)\n      throws IOException {\n    Runnable runnable;\n    final String serverAddress = rpcServer.getAddress();\n    final String serverPort = String.valueOf(rpcServer.getPort());\n\n    if (conf.containsKey(SparkClientFactory.CONF_KEY_IN_PROCESS)) {\n      // Mostly for testing things quickly. Do not do this in production.\n      // when invoked in-process it inherits the environment variables of the parent\n      LOG.warn(\"!!!! Running remote driver in-process. !!!!\");\n      runnable = new Runnable() {\n        @Override\n        public void run() {\n          List<String> args = Lists.newArrayList();\n          args.add(\"--remote-host\");\n          args.add(serverAddress);\n          args.add(\"--remote-port\");\n          args.add(serverPort);\n          args.add(\"--client-id\");\n          args.add(clientId);\n          args.add(\"--secret\");\n          args.add(secret);\n\n          for (Map.Entry<String, String> e : conf.entrySet()) {\n            args.add(\"--conf\");\n            args.add(String.format(\"%s=%s\", e.getKey(), conf.get(e.getKey())));\n          }\n          try {\n            RemoteDriver.main(args.toArray(new String[args.size()]));\n          } catch (Exception e) {\n            LOG.error(\"Error running driver.\", e);\n          }\n        }\n      };\n    } else {\n      // If a Spark installation is provided, use the spark-submit script. Otherwise, call the\n      // SparkSubmit class directly, which has some caveats (like having to provide a proper\n      // version of Guava on the classpath depending on the deploy mode).\n      String sparkHome = Strings.emptyToNull(conf.get(SPARK_HOME_KEY));\n      if (sparkHome == null) {\n        sparkHome = Strings.emptyToNull(System.getenv(SPARK_HOME_ENV));\n      }\n      if (sparkHome == null) {\n        sparkHome = Strings.emptyToNull(System.getProperty(SPARK_HOME_KEY));\n      }\n      String sparkLogDir = conf.get(\"hive.spark.log.dir\");\n      if (sparkLogDir == null) {\n        if (sparkHome == null) {\n          sparkLogDir = \"./target/\";\n        } else {\n          sparkLogDir = sparkHome + \"/logs/\";\n        }\n      }\n\n      String osxTestOpts = \"\";\n      if (Strings.nullToEmpty(System.getProperty(\"os.name\")).toLowerCase().contains(\"mac\")) {\n        osxTestOpts = Strings.nullToEmpty(System.getenv(OSX_TEST_OPTS));\n      }\n\n      String driverJavaOpts = Joiner.on(\" \").skipNulls().join(\n          \"-Dhive.spark.log.dir=\" + sparkLogDir, osxTestOpts, conf.get(DRIVER_OPTS_KEY));\n      String executorJavaOpts = Joiner.on(\" \").skipNulls().join(\n          \"-Dhive.spark.log.dir=\" + sparkLogDir, osxTestOpts, conf.get(EXECUTOR_OPTS_KEY));\n\n      // Create a file with all the job properties to be read by spark-submit. Change the\n      // file's permissions so that only the owner can read it. This avoid having the\n      // connection secret show up in the child process's command line.\n      File properties = File.createTempFile(\"spark-submit.\", \".properties\");\n      if (!properties.setReadable(false) || !properties.setReadable(true, true)) {\n        throw new IOException(\"Cannot change permissions of job properties file.\");\n      }\n      properties.deleteOnExit();\n\n      Properties allProps = new Properties();\n      // first load the defaults from spark-defaults.conf if available\n      try {\n        URL sparkDefaultsUrl = Thread.currentThread().getContextClassLoader().getResource(\"spark-defaults.conf\");\n        if (sparkDefaultsUrl != null) {\n          LOG.info(\"Loading spark defaults: \" + sparkDefaultsUrl);\n          allProps.load(new ByteArrayInputStream(Resources.toByteArray(sparkDefaultsUrl)));\n        }\n      } catch (Exception e) {\n        String msg = \"Exception trying to load spark-defaults.conf: \" + e;\n        throw new IOException(msg, e);\n      }\n      // then load the SparkClientImpl config\n      for (Map.Entry<String, String> e : conf.entrySet()) {\n        allProps.put(e.getKey(), conf.get(e.getKey()));\n      }\n      allProps.put(SparkClientFactory.CONF_CLIENT_ID, clientId);\n      allProps.put(SparkClientFactory.CONF_KEY_SECRET, secret);\n      allProps.put(DRIVER_OPTS_KEY, driverJavaOpts);\n      allProps.put(EXECUTOR_OPTS_KEY, executorJavaOpts);\n\n      String isTesting = conf.get(\"spark.testing\");\n      if (isTesting != null && isTesting.equalsIgnoreCase(\"true\")) {\n        String hiveHadoopTestClasspath = Strings.nullToEmpty(System.getenv(\"HIVE_HADOOP_TEST_CLASSPATH\"));\n        if (!hiveHadoopTestClasspath.isEmpty()) {\n          String extraDriverClasspath = Strings.nullToEmpty((String)allProps.get(DRIVER_EXTRA_CLASSPATH));\n          if (extraDriverClasspath.isEmpty()) {\n            allProps.put(DRIVER_EXTRA_CLASSPATH, hiveHadoopTestClasspath);\n          } else {\n            extraDriverClasspath = extraDriverClasspath.endsWith(File.pathSeparator) ? extraDriverClasspath : extraDriverClasspath + File.pathSeparator;\n            allProps.put(DRIVER_EXTRA_CLASSPATH, extraDriverClasspath + hiveHadoopTestClasspath);\n          }\n\n          String extraExecutorClasspath = Strings.nullToEmpty((String)allProps.get(EXECUTOR_EXTRA_CLASSPATH));\n          if (extraExecutorClasspath.isEmpty()) {\n            allProps.put(EXECUTOR_EXTRA_CLASSPATH, hiveHadoopTestClasspath);\n          } else {\n            extraExecutorClasspath = extraExecutorClasspath.endsWith(File.pathSeparator) ? extraExecutorClasspath : extraExecutorClasspath + File.pathSeparator;\n            allProps.put(EXECUTOR_EXTRA_CLASSPATH, extraExecutorClasspath + hiveHadoopTestClasspath);\n          }\n        }\n      }\n\n      Writer writer = new OutputStreamWriter(new FileOutputStream(properties), Charsets.UTF_8);\n      try {\n        allProps.store(writer, \"Spark Context configuration\");\n      } finally {\n        writer.close();\n      }\n\n      // Define how to pass options to the child process. If launching in client (or local)\n      // mode, the driver options need to be passed directly on the command line. Otherwise,\n      // SparkSubmit will take care of that for us.\n      String master = conf.get(\"spark.master\");\n      Preconditions.checkArgument(master != null, \"spark.master is not defined.\");\n      String deployMode = conf.get(\"spark.submit.deployMode\");\n\n      List<String> argv = Lists.newLinkedList();\n\n      if (sparkHome != null) {\n        argv.add(new File(sparkHome, \"bin/spark-submit\").getAbsolutePath());\n      } else {\n        LOG.info(\"No spark.home provided, calling SparkSubmit directly.\");\n        argv.add(new File(System.getProperty(\"java.home\"), \"bin/java\").getAbsolutePath());\n\n        if (master.startsWith(\"local\") || master.startsWith(\"mesos\") ||\n            SparkClientUtilities.isYarnClientMode(master, deployMode) ||\n            master.startsWith(\"spark\")) {\n          String mem = conf.get(\"spark.driver.memory\");\n          if (mem != null) {\n            argv.add(\"-Xms\" + mem);\n            argv.add(\"-Xmx\" + mem);\n          }\n\n          String cp = conf.get(\"spark.driver.extraClassPath\");\n          if (cp != null) {\n            argv.add(\"-classpath\");\n            argv.add(cp);\n          }\n\n          String libPath = conf.get(\"spark.driver.extraLibPath\");\n          if (libPath != null) {\n            argv.add(\"-Djava.library.path=\" + libPath);\n          }\n\n          String extra = conf.get(DRIVER_OPTS_KEY);\n          if (extra != null) {\n            for (String opt : extra.split(\"[ ]\")) {\n              if (!opt.trim().isEmpty()) {\n                argv.add(opt.trim());\n              }\n            }\n          }\n        }\n\n        argv.add(\"org.apache.spark.deploy.SparkSubmit\");\n      }\n\n      if (SparkClientUtilities.isYarnClusterMode(master, deployMode)) {\n        String executorCores = conf.get(\"spark.executor.cores\");\n        if (executorCores != null) {\n          argv.add(\"--executor-cores\");\n          argv.add(executorCores);\n        }\n\n        String executorMemory = conf.get(\"spark.executor.memory\");\n        if (executorMemory != null) {\n          argv.add(\"--executor-memory\");\n          argv.add(executorMemory);\n        }\n\n        String numOfExecutors = conf.get(\"spark.executor.instances\");\n        if (numOfExecutors != null) {\n          argv.add(\"--num-executors\");\n          argv.add(numOfExecutors);\n        }\n      }\n      // The options --principal/--keypad do not work with --proxy-user in spark-submit.sh\n      // (see HIVE-15485, SPARK-5493, SPARK-19143), so Hive could only support doAs or\n      // delegation token renewal, but not both. Since doAs is a more common case, if both\n      // are needed, we choose to favor doAs. So when doAs is enabled, we use kinit command,\n      // otherwise, we pass the principal/keypad to spark to support the token renewal for\n      // long-running application.\n      if (\"kerberos\".equals(hiveConf.get(HADOOP_SECURITY_AUTHENTICATION))) {\n        String principal = SecurityUtil.getServerPrincipal(hiveConf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL),\n            \"0.0.0.0\");\n        String keyTabFile = hiveConf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB);\n        if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS)) {\n          List<String> kinitArgv = Lists.newLinkedList();\n          kinitArgv.add(\"kinit\");\n          kinitArgv.add(principal);\n          kinitArgv.add(\"-k\");\n          kinitArgv.add(\"-t\");\n          kinitArgv.add(keyTabFile + \";\");\n          kinitArgv.addAll(argv);\n          argv = kinitArgv;\n        } else {\n          // if doAs is not enabled, we pass the principal/keypad to spark-submit in order to\n          // support the possible delegation token renewal in Spark\n          argv.add(\"--principal\");\n          argv.add(principal);\n          argv.add(\"--keytab\");\n          argv.add(keyTabFile);\n        }\n      }\n      if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS)) {\n        try {\n          String currentUser = Utils.getUGI().getShortUserName();\n          // do not do impersonation in CLI mode\n          if (!currentUser.equals(System.getProperty(\"user.name\"))) {\n            LOG.info(\"Attempting impersonation of \" + currentUser);\n            argv.add(\"--proxy-user\");\n            argv.add(currentUser);\n          }\n        } catch (Exception e) {\n          String msg = \"Cannot obtain username: \" + e;\n          throw new IllegalStateException(msg, e);\n        }\n      }\n\n      argv.add(\"--properties-file\");\n      argv.add(properties.getAbsolutePath());\n      argv.add(\"--class\");\n      argv.add(RemoteDriver.class.getName());\n\n      String jar = \"spark-internal\";\n      if (SparkContext.jarOfClass(this.getClass()).isDefined()) {\n        jar = SparkContext.jarOfClass(this.getClass()).get();\n      }\n      argv.add(jar);\n\n      argv.add(\"--remote-host\");\n      argv.add(serverAddress);\n      argv.add(\"--remote-port\");\n      argv.add(serverPort);\n\n      //hive.spark.* keys are passed down to the RemoteDriver via --conf,\n      //as --properties-file contains the spark.* keys that are meant for SparkConf object.\n      for (String hiveSparkConfKey : RpcConfiguration.HIVE_SPARK_RSC_CONFIGS) {\n        String value = RpcConfiguration.getValue(hiveConf, hiveSparkConfKey);\n        argv.add(\"--conf\");\n        argv.add(String.format(\"%s=%s\", hiveSparkConfKey, value));\n      }\n\n      String cmd = Joiner.on(\" \").join(argv);\n      LOG.info(\"Running client driver with argv: {}\", cmd);\n      ProcessBuilder pb = new ProcessBuilder(\"sh\", \"-c\", cmd);\n\n      // Prevent hive configurations from being visible in Spark.\n      pb.environment().remove(\"HIVE_HOME\");\n      pb.environment().remove(\"HIVE_CONF_DIR\");\n      // Add credential provider password to the child process's environment\n      // In case of Spark the credential provider location is provided in the jobConf when the job is submitted\n      String password = getSparkJobCredentialProviderPassword();\n      if(password != null) {\n        pb.environment().put(Constants.HADOOP_CREDENTIAL_PASSWORD_ENVVAR, password);\n      }\n      if (isTesting != null) {\n        pb.environment().put(\"SPARK_TESTING\", isTesting);\n      }\n\n      final Process child = pb.start();\n      int childId = childIdGenerator.incrementAndGet();\n      final List<String> childErrorLog = new ArrayList<String>();\n      redirect(\"stdout-redir-\" + childId, new Redirector(child.getInputStream()));\n      redirect(\"stderr-redir-\" + childId, new Redirector(child.getErrorStream(), childErrorLog));\n\n      runnable = new Runnable() {\n        @Override\n        public void run() {\n          try {\n            int exitCode = child.waitFor();\n            if (exitCode != 0) {\n              StringBuilder errStr = new StringBuilder();\n              for (String s : childErrorLog) {\n                errStr.append(s);\n                errStr.append('\\n');\n              }\n\n              rpcServer.cancelClient(clientId,\n                  \"Child process exited before connecting back with error log \" + errStr.toString());\n              LOG.warn(\"Child process exited with code {}\", exitCode);\n            }\n          } catch (InterruptedException ie) {\n            LOG.warn(\"Waiting thread interrupted, killing child process.\");\n            Thread.interrupted();\n            child.destroy();\n          } catch (Exception e) {\n            LOG.warn(\"Exception while waiting for child process.\", e);\n          }\n        }\n      };\n    }\n\n    Thread thread = new Thread(runnable);\n    thread.setDaemon(true);\n    thread.setName(\"Driver\");\n    thread.start();\n    return thread;\n  }"
        ]
    ],
    "0d787cbc055eb237bcccd5fdbc144fb6b1d7d4ca": [
        [
            "HiveRelMdPredicates::JoinConditionBasedPredicateInference::ExprsItr::hasNext()",
            " 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645 -\n 646  ",
            "      public boolean hasNext() {\n        if (firstCall) {\n          initializeMapping();\n          firstCall = false;\n        } else {\n          computeNextMapping(iterationIdx.length - 1);\n        }\n        return nextMapping != null;\n      }",
            " 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645 +\n 646  ",
            "      public boolean hasNext() {\n        if (firstCall) {\n          initializeMapping();\n          firstCall = false;\n        } else {\n          computeNextMapping(iterationIdx.length - 1);\n        }\n          return nextMapping != null;\n      }"
        ],
        [
            "HiveRelMdPredicates::JoinConditionBasedPredicateInference::ExprsItr::computeNextMapping(int)",
            " 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662 -\n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  ",
            "      private void computeNextMapping(int level) {\n        int t = columnSets[level].nextSetBit(iterationIdx[level]);\n        if (t < 0) {\n          if (level == 0) {\n            nextMapping = null;\n          } else {\n            iterationIdx[level] = 0;\n            computeNextMapping(level - 1);\n          }\n        } else {\n          nextMapping.set(columns[level], t);\n          iterationIdx[level] = t + 1;\n        }\n      }",
            " 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662 +\n 663 +\n 664 +\n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  ",
            "      private void computeNextMapping(int level) {\n        int t = columnSets[level].nextSetBit(iterationIdx[level]);\n        if (t < 0) {\n          if (level == 0) {\n            nextMapping = null;\n          } else {\n            int tmp = columnSets[level].nextSetBit(0);\n            nextMapping.set(columns[level], tmp);\n            iterationIdx[level] = tmp + 1;\n            computeNextMapping(level - 1);\n          }\n        } else {\n          nextMapping.set(columns[level], t);\n          iterationIdx[level] = t + 1;\n        }\n      }"
        ]
    ],
    "1d83625385d466b18018201fb89e3541f2fc46b6": [
        [
            "CliConfigs::MiniDruidKafkaCliConfig::MiniDruidKafkaCliConfig()",
            " 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "    public MiniDruidKafkaCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"druid.kafka.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/druid\");\n        setLogDir(\"itests/qtest/target/tmp/log\");\n\n        setInitScript(\"q_test_druid_init.sql\");\n        setCleanupScript(\"q_test_cleanup_druid.sql\");\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.druidKafka);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201 +\n 202 +\n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  ",
            "    public MiniDruidKafkaCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"druid.kafka.query.files\");\n\n        excludeQuery(\"druidkafkamini_basic.q\"); // Disabled in HIVE-19509\n\n        setResultsDir(\"ql/src/test/results/clientpositive/druid\");\n        setLogDir(\"itests/qtest/target/tmp/log\");\n\n        setInitScript(\"q_test_druid_init.sql\");\n        setCleanupScript(\"q_test_cleanup_druid.sql\");\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.druidKafka);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ],
        [
            "CliConfigs::NegativeCliConfig::NegativeCliConfig()",
            " 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372 -\n 373 -\n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  ",
            "    public NegativeCliConfig() {\n      super(CoreNegativeCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientnegative\");\n\n        excludesFrom(testConfigProps, \"minimr.query.negative.files\");\n        excludesFrom(testConfigProps, \"spark.only.query.negative.files\");\n        excludeQuery(\"authorization_uri_import.q\");\n        excludeQuery(\"merge_negative_5.q\");\n        excludeQuery(\"mm_concatenate.q\");\n\n        setResultsDir(\"ql/src/test/results/clientnegative\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientnegative\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"\");\n        setClusterType(MiniClusterType.none);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374 +\n 375 +\n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  ",
            "    public NegativeCliConfig() {\n      super(CoreNegativeCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientnegative\");\n\n        excludesFrom(testConfigProps, \"minimr.query.negative.files\");\n        excludesFrom(testConfigProps, \"spark.only.query.negative.files\");\n        excludeQuery(\"authorization_uri_import.q\");\n        excludeQuery(\"merge_negative_5.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"mm_concatenate.q\"); // Disabled in HIVE-19509\n\n        setResultsDir(\"ql/src/test/results/clientnegative\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientnegative\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"\");\n        setClusterType(MiniClusterType.none);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ],
        [
            "CliConfigs::MiniLlapLocalCliConfig::MiniLlapLocalCliConfig()",
            " 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 -\n 226 -\n 227 -\n 228 -\n 229 -\n 230 -\n 231 -\n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "    public MiniLlapLocalCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minillaplocal.query.files\");\n        includesFrom(testConfigProps, \"minillaplocal.shared.query.files\");\n        excludeQuery(\"bucket_map_join_tez1.q\");\n        excludeQuery(\"special_character_in_tabnames_1.q\");\n        excludeQuery(\"sysdb.q\");\n        excludeQuery(\"tez_smb_1.q\");\n        excludeQuery(\"union_fast_stats.q\");\n        excludeQuery(\"schema_evol_orc_acidvec_part.q\");\n        excludeQuery(\"schema_evol_orc_vec_part_llap_io.q\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/llap\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.llap_local);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.local);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 +\n 228 +\n 229 +\n 230 +\n 231 +\n 232 +\n 233 +\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  ",
            "    public MiniLlapLocalCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minillaplocal.query.files\");\n        includesFrom(testConfigProps, \"minillaplocal.shared.query.files\");\n        excludeQuery(\"bucket_map_join_tez1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"special_character_in_tabnames_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"sysdb.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"tez_smb_1.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"union_fast_stats.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_acidvec_part.q\"); // Disabled in HIVE-19509\n        excludeQuery(\"schema_evol_orc_vec_part_llap_io.q\"); // Disabled in HIVE-19509\n\n        setResultsDir(\"ql/src/test/results/clientpositive/llap\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"data/conf/llap\");\n        setClusterType(MiniClusterType.llap_local);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.local);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ]
    ],
    "57ae3aca05d21628df620f33a9f03966f33c8d7b": [
        [
            "SparkClientFactory::stop()",
            "  63  \n  64 -\n  65  \n  66 -\n  67 -\n  68  \n  69  ",
            "  /** Stops the SparkClient library. */\n  public static synchronized void stop() {\n    if (server != null) {\n      server.close();\n      server = null;\n    }\n  }",
            "  64  \n  65 +\n  66  \n  67 +\n  68 +\n  69 +\n  70 +\n  71 +\n  72 +\n  73  \n  74  ",
            "  /** Stops the SparkClient library. */\n  public static void stop() {\n    if (server != null) {\n      synchronized (stopLock) {\n        if (server != null) {\n          server.close();\n          server = null;\n        }\n      }\n    }\n  }"
        ]
    ],
    "95bdb2b1afd7cb4b7680e70a7f3175b9740f5d06": [
        [
            "TestJdbcWithDBTokenStore::beforeTest()",
            "  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  ",
            "  @BeforeClass\n  public static void beforeTest() throws Exception {\n    Class.forName(MiniHS2.getJdbcDriverName());\n    confOverlay.put(ConfVars.HIVE_SERVER2_SESSION_HOOK.varname,\n        SessionHookTest.class.getName());\n\n    miniHiveKdc = new MiniHiveKdc();\n    HiveConf hiveConf = new HiveConf();\n    //using old config value tests backwards compatibility\n    hiveConf.setVar(ConfVars.METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_CLS, \"org.apache.hadoop.hive.thrift.DBTokenStore\");\n    miniHS2 = MiniHiveKdc.getMiniHS2WithKerbWithRemoteHMSWithKerb(miniHiveKdc, hiveConf);\n    miniHS2.start(confOverlay);\n    String metastorePrincipal = miniHS2.getConfProperty(ConfVars.METASTORE_KERBEROS_PRINCIPAL.varname);\n    String hs2Principal = miniHS2.getConfProperty(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL.varname);\n    String hs2KeyTab = miniHS2.getConfProperty(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB.varname);\n    System.out.println(\"HS2 principal : \" + hs2Principal + \" HS2 keytab : \" + hs2KeyTab + \" Metastore principal : \" + metastorePrincipal);\n  }",
            "  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 +\n  51 +\n  52 +\n  53 +\n  54 +\n  55 +\n  56 +\n  57 +\n  58 +\n  59 +\n  60 +\n  61 +\n  62  ",
            "  @BeforeClass\n  public static void beforeTest() throws Exception {\n    Class.forName(MiniHS2.getJdbcDriverName());\n    confOverlay.put(ConfVars.HIVE_SERVER2_SESSION_HOOK.varname,\n        SessionHookTest.class.getName());\n\n    miniHiveKdc = new MiniHiveKdc();\n    HiveConf hiveConf = new HiveConf();\n    //using old config value tests backwards compatibility\n    hiveConf.setVar(ConfVars.METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_CLS, \"org.apache.hadoop.hive.thrift.DBTokenStore\");\n    miniHS2 = MiniHiveKdc.getMiniHS2WithKerbWithRemoteHMSWithKerb(miniHiveKdc, hiveConf);\n    miniHS2.start(confOverlay);\n    String metastorePrincipal = miniHS2.getConfProperty(ConfVars.METASTORE_KERBEROS_PRINCIPAL.varname);\n    String hs2Principal = miniHS2.getConfProperty(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL.varname);\n    String hs2KeyTab = miniHS2.getConfProperty(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB.varname);\n    System.out.println(\"HS2 principal : \" + hs2Principal + \" HS2 keytab : \" + hs2KeyTab + \" Metastore principal : \" + metastorePrincipal);\n    System.setProperty(HiveConf.ConfVars.METASTOREWAREHOUSE.varname,\n        MetastoreConf.getVar(hiveConf, MetastoreConf.ConfVars.WAREHOUSE));\n    System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.varname,\n        MetastoreConf.getVar(hiveConf, MetastoreConf.ConfVars.CONNECT_URL_KEY));\n    // Before this patch, the Embedded MetaStore was used here not the one started by the MiniHS2\n    // The below 3 lines would change the tests to use the Remote MetaStore, but it will cause a\n    // failure. By removing the thrift MetaStore uris, the tests are passing again.\n    // I think this is an valid problem here, but not really sure about the\n    // tests original intention, so keep everything as it was originally.\n//    System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname,\n//        MetastoreConf.getVar(hiveConf, MetastoreConf.ConfVars.THRIFT_URIS));\n//    Thread.sleep(2000);\n  }"
        ],
        [
            "TestJdbcWithDBTokenStoreNoDoAs::beforeTest()",
            "  33  \n  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  ",
            "  @BeforeClass\n  public static void beforeTest() throws Exception {\n    Class.forName(MiniHS2.getJdbcDriverName());\n    confOverlay.put(ConfVars.HIVE_SERVER2_SESSION_HOOK.varname,\n        SessionHookTest.class.getName());\n\n    miniHiveKdc = new MiniHiveKdc();\n    HiveConf hiveConf = new HiveConf();\n    hiveConf.setVar(ConfVars.METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_CLS, \"org.apache.hadoop.hive.thrift.DBTokenStore\");\n    hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);\n    miniHS2 = MiniHiveKdc.getMiniHS2WithKerbWithRemoteHMS(miniHiveKdc, hiveConf);\n    miniHS2.start(confOverlay);\n    String metastorePrincipal = miniHS2.getConfProperty(ConfVars.METASTORE_KERBEROS_PRINCIPAL.varname);\n    String hs2Principal = miniHS2.getConfProperty(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL.varname);\n    String hs2KeyTab = miniHS2.getConfProperty(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB.varname);\n    System.out.println(\"HS2 principal : \" + hs2Principal + \" HS2 keytab : \" + hs2KeyTab + \" Metastore principal : \" + metastorePrincipal);\n  }",
            "  34  \n  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50 +\n  51 +\n  52 +\n  53 +\n  54 +\n  55 +\n  56 +\n  57  ",
            "  @BeforeClass\n  public static void beforeTest() throws Exception {\n    Class.forName(MiniHS2.getJdbcDriverName());\n    confOverlay.put(ConfVars.HIVE_SERVER2_SESSION_HOOK.varname,\n        SessionHookTest.class.getName());\n\n    miniHiveKdc = new MiniHiveKdc();\n    HiveConf hiveConf = new HiveConf();\n    hiveConf.setVar(ConfVars.METASTORE_CLUSTER_DELEGATION_TOKEN_STORE_CLS, \"org.apache.hadoop.hive.thrift.DBTokenStore\");\n    hiveConf.setBoolVar(ConfVars.HIVE_SERVER2_ENABLE_DOAS, false);\n    miniHS2 = MiniHiveKdc.getMiniHS2WithKerbWithRemoteHMS(miniHiveKdc, hiveConf);\n    miniHS2.start(confOverlay);\n    String metastorePrincipal = miniHS2.getConfProperty(ConfVars.METASTORE_KERBEROS_PRINCIPAL.varname);\n    String hs2Principal = miniHS2.getConfProperty(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL.varname);\n    String hs2KeyTab = miniHS2.getConfProperty(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB.varname);\n    System.out.println(\"HS2 principal : \" + hs2Principal + \" HS2 keytab : \" + hs2KeyTab + \" Metastore principal : \" + metastorePrincipal);\n    System.setProperty(HiveConf.ConfVars.METASTOREWAREHOUSE.varname,\n        MetastoreConf.getVar(hiveConf, MetastoreConf.ConfVars.WAREHOUSE));\n    System.setProperty(HiveConf.ConfVars.METASTORECONNECTURLKEY.varname,\n        MetastoreConf.getVar(hiveConf, MetastoreConf.ConfVars.CONNECT_URL_KEY));\n    System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname,\n        MetastoreConf.getVar(hiveConf, MetastoreConf.ConfVars.THRIFT_URIS));\n    Thread.sleep(2000);\n  }"
        ]
    ],
    "1a2e378f36a1a02d68ff1647f856f1e223276da0": [
        [
            "DDLSemanticAnalyzer::buildTriggerExpression(ASTNode)",
            "1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100 -\n1101  \n1102  \n1103  \n1104  \n1105  ",
            "  private String buildTriggerExpression(ASTNode ast) throws SemanticException {\n    if (ast.getType() != HiveParser.TOK_TRIGGER_EXPRESSION || ast.getChildCount() == 0) {\n      throw new SemanticException(\"Invalid trigger expression.\");\n    }\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < ast.getChildCount(); ++i) {\n      builder.append(stripQuotes(ast.getChild(i).getText()));\n      builder.append(' ');\n    }\n    builder.deleteCharAt(builder.length() - 1);\n    return builder.toString();\n  }",
            "1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100 +\n1101  \n1102  \n1103  \n1104  \n1105  ",
            "  private String buildTriggerExpression(ASTNode ast) throws SemanticException {\n    if (ast.getType() != HiveParser.TOK_TRIGGER_EXPRESSION || ast.getChildCount() == 0) {\n      throw new SemanticException(\"Invalid trigger expression.\");\n    }\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < ast.getChildCount(); ++i) {\n      builder.append(ast.getChild(i).getText()); // Don't strip quotes.\n      builder.append(' ');\n    }\n    builder.deleteCharAt(builder.length() - 1);\n    return builder.toString();\n  }"
        ]
    ],
    "57f40f71ff9275458e743aad0ca460dff2291a4a": [
        [
            "TestSchemaToolCatalogOps::removeDb()",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  @AfterClass\n  public static void removeDb() throws Exception {\n    File metaStoreDir = new File(testMetastoreDB);\n    if (metaStoreDir.exists()) {\n      FileUtils.forceDeleteOnExit(metaStoreDir);\n    }\n  }",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 +\n  96 +\n  97  ",
            "  @AfterClass\n  public static void removeDb() throws Exception {\n    File metaStoreDir = new File(testMetastoreDB);\n    if (metaStoreDir.exists()) {\n      FileUtils.forceDeleteOnExit(metaStoreDir);\n    }\n    System.setOut(outStream);\n    System.setErr(errStream);\n  }"
        ],
        [
            "TestSchemaToolCatalogOps::initDb()",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "  @BeforeClass\n  public static void initDb() throws HiveMetaException, IOException {\n    conf = new HiveConf();\n    MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.AUTO_CREATE_ALL, false);\n    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.HMS_HANDLER_ATTEMPTS, 1);\n    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.THRIFT_CONNECTION_RETRIES, 1);\n    testMetastoreDB = System.getProperty(\"java.io.tmpdir\") +\n        File.separator + \"testschematoolcatopsdb\";\n    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.CONNECT_URL_KEY,\n        \"jdbc:derby:\" + testMetastoreDB + \";create=true\");\n    schemaTool = new MetastoreSchemaTool();\n    schemaTool.init(System.getProperty(\"test.tmp.dir\", \"target/tmp\"),\n        new String[]{\"-dbType\", \"derby\", \"--info\"}, null, conf);\n\n    String userName = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.CONNECTION_USER_NAME);\n    String passWord = MetastoreConf.getPassword(conf, MetastoreConf.ConfVars.PWD);\n    schemaTool.setUserName(userName);\n    schemaTool.setPassWord(passWord);\n\n    argsBase = \"-dbType derby -userName \" + userName + \" -passWord \" + passWord + \" \";\n    execute(new SchemaToolTaskInit(), \"-initSchema\"); // Pre-install the database so all the tables are there.\n  }",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83 +\n  84  \n  85  \n  86  \n  87  ",
            "  @BeforeClass\n  public static void initDb() throws HiveMetaException, IOException {\n    conf = new HiveConf();\n    MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.AUTO_CREATE_ALL, false);\n    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.HMS_HANDLER_ATTEMPTS, 1);\n    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.THRIFT_CONNECTION_RETRIES, 1);\n    testMetastoreDB = System.getProperty(\"java.io.tmpdir\") +\n        File.separator + \"testschematoolcatopsdb\";\n    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.CONNECT_URL_KEY,\n        \"jdbc:derby:\" + testMetastoreDB + \";create=true\");\n    schemaTool = new MetastoreSchemaTool();\n    schemaTool.init(System.getProperty(\"test.tmp.dir\", \"target/tmp\"),\n        new String[]{\"-dbType\", \"derby\", \"--info\"}, null, conf);\n\n    String userName = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.CONNECTION_USER_NAME);\n    String passWord = MetastoreConf.getPassword(conf, MetastoreConf.ConfVars.PWD);\n    schemaTool.setUserName(userName);\n    schemaTool.setPassWord(passWord);\n    errStream = System.err;\n    outStream = System.out;\n\n    argsBase = \"-dbType derby -userName \" + userName + \" -passWord \" + passWord + \" \";\n    execute(new SchemaToolTaskInit(), \"-initSchema\"); // Pre-install the database so all the tables are there.\n  }"
        ]
    ],
    "be1130d567bd3b075fa3364215bb561e221506ed": [
        [
            "DDLTask::createView(Hive,CreateViewDesc)",
            "5132  \n5133  \n5134  \n5135  \n5136  \n5137  \n5138  \n5139  \n5140  \n5141  \n5142  \n5143  \n5144  \n5145  \n5146  \n5147  \n5148  \n5149  \n5150  \n5151  \n5152  \n5153  \n5154  \n5155  \n5156  \n5157  \n5158 -\n5159  \n5160  \n5161  \n5162  \n5163  \n5164  \n5165  \n5166  \n5167  \n5168  \n5169  \n5170  \n5171  \n5172  \n5173  \n5174  \n5175  \n5176  \n5177  \n5178  \n5179  \n5180  \n5181  \n5182  \n5183  \n5184  \n5185  \n5186  \n5187  \n5188  \n5189  \n5190  \n5191  \n5192  \n5193  \n5194  \n5195  \n5196  \n5197  \n5198  \n5199  \n5200  \n5201  \n5202  \n5203  \n5204  \n5205  \n5206  ",
            "  /**\n   * Create a new view.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtView\n   *          This is the view we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createView(Hive db, CreateViewDesc crtView) throws HiveException {\n    Table oldview = db.getTable(crtView.getViewName(), false);\n    if (oldview != null) {\n      // Check whether we are replicating\n      if (crtView.getReplicationSpec().isInReplicationScope()) {\n        // if this is a replication spec, then replace-mode semantics might apply.\n        if (crtView.getReplicationSpec().allowEventReplacementInto(oldview.getParameters())){\n          crtView.setReplace(true); // we replace existing view.\n        } else {\n          LOG.debug(\"DDLTask: Create View is skipped as view {} is newer than update\",\n              crtView.getViewName()); // no replacement, the existing table state is newer than our update.\n          return 0;\n        }\n      }\n\n      if (!crtView.isReplace()) {\n        // View already exists, thus we should be replacing\n        throw new HiveException(ErrorMsg.TABLE_ALREADY_EXISTS.getMsg(crtView.getViewName()));\n      }\n\n      // It should not be a materialized view\n      assert !crtView.isMaterialized();\n\n      // replace existing view\n      // remove the existing partition columns from the field schema\n      oldview.setViewOriginalText(crtView.getViewOriginalText());\n      oldview.setViewExpandedText(crtView.getViewExpandedText());\n      oldview.setFields(crtView.getSchema());\n      if (crtView.getComment() != null) {\n        oldview.setProperty(\"comment\", crtView.getComment());\n      }\n      if (crtView.getTblProps() != null) {\n        oldview.getTTable().getParameters().putAll(crtView.getTblProps());\n      }\n      oldview.setPartCols(crtView.getPartCols());\n      if (crtView.getInputFormat() != null) {\n        oldview.setInputFormatClass(crtView.getInputFormat());\n      }\n      if (crtView.getOutputFormat() != null) {\n        oldview.setOutputFormatClass(crtView.getOutputFormat());\n      }\n      oldview.checkValidity(null);\n      db.alterTable(crtView.getViewName(), oldview, false, null, true);\n      addIfAbsentByName(new WriteEntity(oldview, WriteEntity.WriteType.DDL_NO_LOCK));\n    } else {\n      // We create new view\n      Table tbl = crtView.toTable(conf);\n      // We set the signature for the view if it is a materialized view\n      if (tbl.isMaterializedView()) {\n        CreationMetadata cm =\n            new CreationMetadata(MetaStoreUtils.getDefaultCatalog(conf), tbl.getDbName(),\n                tbl.getTableName(), ImmutableSet.copyOf(crtView.getTablesUsed()));\n        cm.setValidTxnList(conf.get(ValidTxnWriteIdList.VALID_TABLES_WRITEIDS_KEY));\n        tbl.getTTable().setCreationMetadata(cm);\n      }\n      db.createTable(tbl, crtView.getIfNotExists());\n      addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n\n      //set lineage info\n      DataContainer dc = new DataContainer(tbl.getTTable());\n      queryState.getLineageState().setLineage(new Path(crtView.getViewName()), dc, tbl.getCols());\n    }\n    return 0;\n  }",
            "5132  \n5133  \n5134  \n5135  \n5136  \n5137  \n5138  \n5139  \n5140  \n5141  \n5142  \n5143  \n5144  \n5145  \n5146  \n5147  \n5148  \n5149  \n5150  \n5151  \n5152  \n5153  \n5154  \n5155  \n5156  \n5157  \n5158 +\n5159  \n5160  \n5161  \n5162  \n5163  \n5164  \n5165  \n5166  \n5167  \n5168  \n5169  \n5170  \n5171  \n5172  \n5173  \n5174  \n5175  \n5176  \n5177  \n5178  \n5179  \n5180  \n5181  \n5182  \n5183  \n5184  \n5185  \n5186  \n5187  \n5188  \n5189  \n5190  \n5191  \n5192  \n5193  \n5194  \n5195  \n5196  \n5197  \n5198  \n5199  \n5200  \n5201  \n5202  \n5203  \n5204  \n5205  \n5206  ",
            "  /**\n   * Create a new view.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtView\n   *          This is the view we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createView(Hive db, CreateViewDesc crtView) throws HiveException {\n    Table oldview = db.getTable(crtView.getViewName(), false);\n    if (oldview != null) {\n      // Check whether we are replicating\n      if (crtView.getReplicationSpec().isInReplicationScope()) {\n        // if this is a replication spec, then replace-mode semantics might apply.\n        if (crtView.getReplicationSpec().allowEventReplacementInto(oldview.getParameters())){\n          crtView.setReplace(true); // we replace existing view.\n        } else {\n          LOG.debug(\"DDLTask: Create View is skipped as view {} is newer than update\",\n              crtView.getViewName()); // no replacement, the existing table state is newer than our update.\n          return 0;\n        }\n      }\n\n      if (!crtView.isReplace() && !crtView.getIfNotExists()) {\n        // View already exists, thus we should be replacing\n        throw new HiveException(ErrorMsg.TABLE_ALREADY_EXISTS.getMsg(crtView.getViewName()));\n      }\n\n      // It should not be a materialized view\n      assert !crtView.isMaterialized();\n\n      // replace existing view\n      // remove the existing partition columns from the field schema\n      oldview.setViewOriginalText(crtView.getViewOriginalText());\n      oldview.setViewExpandedText(crtView.getViewExpandedText());\n      oldview.setFields(crtView.getSchema());\n      if (crtView.getComment() != null) {\n        oldview.setProperty(\"comment\", crtView.getComment());\n      }\n      if (crtView.getTblProps() != null) {\n        oldview.getTTable().getParameters().putAll(crtView.getTblProps());\n      }\n      oldview.setPartCols(crtView.getPartCols());\n      if (crtView.getInputFormat() != null) {\n        oldview.setInputFormatClass(crtView.getInputFormat());\n      }\n      if (crtView.getOutputFormat() != null) {\n        oldview.setOutputFormatClass(crtView.getOutputFormat());\n      }\n      oldview.checkValidity(null);\n      db.alterTable(crtView.getViewName(), oldview, false, null, true);\n      addIfAbsentByName(new WriteEntity(oldview, WriteEntity.WriteType.DDL_NO_LOCK));\n    } else {\n      // We create new view\n      Table tbl = crtView.toTable(conf);\n      // We set the signature for the view if it is a materialized view\n      if (tbl.isMaterializedView()) {\n        CreationMetadata cm =\n            new CreationMetadata(MetaStoreUtils.getDefaultCatalog(conf), tbl.getDbName(),\n                tbl.getTableName(), ImmutableSet.copyOf(crtView.getTablesUsed()));\n        cm.setValidTxnList(conf.get(ValidTxnWriteIdList.VALID_TABLES_WRITEIDS_KEY));\n        tbl.getTTable().setCreationMetadata(cm);\n      }\n      db.createTable(tbl, crtView.getIfNotExists());\n      addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n\n      //set lineage info\n      DataContainer dc = new DataContainer(tbl.getTTable());\n      queryState.getLineageState().setLineage(new Path(crtView.getViewName()), dc, tbl.getCols());\n    }\n    return 0;\n  }"
        ]
    ],
    "0f7163fad61745276f5b8e1eaa797ba297810780": [
        [
            "CalcitePlanner::createPlanner(HiveConf,Set,Set)",
            " 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415 -\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  ",
            "  private static RelOptPlanner createPlanner(\n      HiveConf conf, Set<RelNode> corrScalarRexSQWithAgg, Set<RelNode> scalarAggNoGbyNoWin) {\n    final Double maxSplitSize = (double) HiveConf.getLongVar(\n            conf, HiveConf.ConfVars.MAPREDMAXSPLITSIZE);\n    final Double maxMemory = (double) HiveConf.getLongVar(\n            conf, HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);\n    HiveAlgorithmsConf algorithmsConf = new HiveAlgorithmsConf(maxSplitSize, maxMemory);\n    HiveRulesRegistry registry = new HiveRulesRegistry();\n    Properties calciteConfigProperties = new Properties();\n    calciteConfigProperties.setProperty(\n        CalciteConnectionProperty.TIME_ZONE.camelName(),\n        conf.getLocalTimeZone().getId());\n    calciteConfigProperties.setProperty(\n        CalciteConnectionProperty.MATERIALIZATIONS_ENABLED.camelName(),\n        Boolean.FALSE.toString());\n    CalciteConnectionConfig calciteConfig = new CalciteConnectionConfigImpl(calciteConfigProperties);\n    boolean isCorrelatedColumns = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_CORRELATED_MULTI_KEY_JOINS);\n    boolean heuristicMaterializationStrategy = HiveConf.getVar(conf,\n        HiveConf.ConfVars.HIVE_MATERIALIZED_VIEW_REWRITING_SELECTION_STRATEGY).equals(\"heuristic\");\n    HivePlannerContext confContext = new HivePlannerContext(algorithmsConf, registry, calciteConfig,\n        corrScalarRexSQWithAgg, scalarAggNoGbyNoWin,\n        new HiveConfPlannerContext(isCorrelatedColumns, heuristicMaterializationStrategy));\n    return HiveVolcanoPlanner.createPlanner(confContext);\n  }",
            " 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415 +\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  ",
            "  private static RelOptPlanner createPlanner(\n      HiveConf conf, Set<RelNode> corrScalarRexSQWithAgg, Set<RelNode> scalarAggNoGbyNoWin) {\n    final Double maxSplitSize = (double) HiveConf.getLongVar(\n            conf, HiveConf.ConfVars.MAPREDMAXSPLITSIZE);\n    final Double maxMemory = (double) HiveConf.getLongVar(\n            conf, HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);\n    HiveAlgorithmsConf algorithmsConf = new HiveAlgorithmsConf(maxSplitSize, maxMemory);\n    HiveRulesRegistry registry = new HiveRulesRegistry();\n    Properties calciteConfigProperties = new Properties();\n    calciteConfigProperties.setProperty(\n        CalciteConnectionProperty.TIME_ZONE.camelName(),\n        conf.getLocalTimeZone().getId());\n    calciteConfigProperties.setProperty(\n        CalciteConnectionProperty.MATERIALIZATIONS_ENABLED.camelName(),\n        Boolean.FALSE.toString());\n    CalciteConnectionConfig calciteConfig = new CalciteConnectionConfigImpl(calciteConfigProperties);\n    boolean isCorrelatedColumns = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CBO_STATS_CORRELATED_MULTI_KEY_JOINS);\n    boolean heuristicMaterializationStrategy = HiveConf.getVar(conf,\n        HiveConf.ConfVars.HIVE_MATERIALIZED_VIEW_REWRITING_SELECTION_STRATEGY).equals(\"heuristic\");\n    HivePlannerContext confContext = new HivePlannerContext(algorithmsConf, registry, calciteConfig,\n        corrScalarRexSQWithAgg, scalarAggNoGbyNoWin,\n        new HiveConfPlannerContext(isCorrelatedColumns, heuristicMaterializationStrategy));\n    return HiveVolcanoPlanner.createPlanner(confContext);\n  }"
        ]
    ],
    "37120b87777c40baf44040fc7778d129cd8a5824": [
        [
            "ColumnType::getTypeName(String)",
            " 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  ",
            "  /**\n   * Given a type string return the type name.  For example, passing in the type string\n   * <tt>varchar(256)</tt> will return <tt>varchar</tt>.\n   * @param typeString Type string\n   * @return type name, guaranteed to be in lower case\n   */\n  public static String getTypeName(String typeString) {\n    if (typeString == null) return null;\n    String protoType = typeString.toLowerCase().split(\"\\\\W\")[0];\n    String realType = alternateTypeNames.get(protoType);\n    return realType == null ? protoType : realType;\n  }",
            " 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211 +\n 212 +\n 213 +\n 214  \n 215  \n 216  ",
            "  /**\n   * Given a type string return the type name.  For example, passing in the type string\n   * <tt>varchar(256)</tt> will return <tt>varchar</tt>.\n   * @param typeString Type string\n   * @return type name, guaranteed to be in lower case\n   */\n  public static String getTypeName(String typeString) {\n    if (typeString == null) return null;\n    String protoType = typeString.toLowerCase().split(\"\\\\W\")[0];\n    if (decoratedTypeNames.contains(protoType)) {\n      return protoType;\n    }\n    String realType = alternateTypeNames.get(protoType);\n    return realType == null ? protoType : realType;\n  }"
        ],
        [
            "ColumnType::areColTypesCompatible(String,String)",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220 -\n 221 -\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  ",
            "  public static boolean areColTypesCompatible(String from, String to) {\n    if (from.equals(to)) return true;\n\n    if (PrimitiveTypes.contains(from) && PrimitiveTypes.contains(to)) {\n      // They aren't the same, but we may be able to do a cast\n\n      // If they are both types of strings, that should be fine\n      if (StringTypes.contains(from) && StringTypes.contains(to)) return true;\n\n      // If both are numeric, make sure the new type is larger than the old.\n      if (NumericTypes.contains(from) && NumericTypes.contains(to)) {\n        return NumericCastOrder.get(from) < NumericCastOrder.get(to);\n      }\n\n      // Allow string to double conversion\n      if (StringTypes.contains(from) && to.equals(DOUBLE_TYPE_NAME)) return true;\n\n      // Void can go to anything\n      if (from.equals(VOID_TYPE_NAME)) return true;\n\n      // Allow date to string casts.  NOTE: I suspect this is the reverse of what we actually\n      // want, but it matches the code in o.a.h.h.serde2.typeinfo.TypeInfoUtils.  I can't see how\n      // users would be altering date columns into string columns.  The other I easily see since\n      // Hive did not originally support datetime types.  Also, the comment in the Hive code\n      // says string to date, even though the code does the opposite.  But for now I'm keeping\n      // this as is so the functionality matches.\n      if (DateTimeTypes.contains(from) && StringTypes.contains(to)) return true;\n\n      // Allow numeric to string\n      if (NumericTypes.contains(from) && StringTypes.contains(to)) return true;\n\n    }\n    return false;\n  }",
            " 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232 +\n 233 +\n 234 +\n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  ",
            "  public static boolean areColTypesCompatible(String from, String to) {\n    if (from.equals(to)) return true;\n\n    if (PrimitiveTypes.contains(from) && PrimitiveTypes.contains(to)) {\n      // They aren't the same, but we may be able to do a cast\n\n      // If they are both types of strings, that should be fine\n      if (StringTypes.contains(from) && StringTypes.contains(to)) return true;\n\n      // If both are numeric, make sure the new type is larger than the old.\n      if (NumericTypes.contains(from) && NumericTypes.contains(to)) {\n        return NumericCastOrder.get(from) < NumericCastOrder.get(to);\n      }\n\n      // Allow string to double/decimal conversion\n      if (StringTypes.contains(from) &&\n          (to.equals(DOUBLE_TYPE_NAME) || to.equals(DECIMAL_TYPE_NAME))) return true;\n\n      // Void can go to anything\n      if (from.equals(VOID_TYPE_NAME)) return true;\n\n      // Allow date to string casts.  NOTE: I suspect this is the reverse of what we actually\n      // want, but it matches the code in o.a.h.h.serde2.typeinfo.TypeInfoUtils.  I can't see how\n      // users would be altering date columns into string columns.  The other I easily see since\n      // Hive did not originally support datetime types.  Also, the comment in the Hive code\n      // says string to date, even though the code does the opposite.  But for now I'm keeping\n      // this as is so the functionality matches.\n      if (DateTimeTypes.contains(from) && StringTypes.contains(to)) return true;\n\n      // Allow numeric to string\n      if (NumericTypes.contains(from) && StringTypes.contains(to)) return true;\n\n    }\n    return false;\n  }"
        ]
    ],
    "83d1fd23a22ee96dd2c464d67303eadf31194454": [
        [
            "RelOptHiveTable::isNonNullableKey(ImmutableBitSet)",
            " 223  \n 224  \n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  ",
            "  public boolean isNonNullableKey(ImmutableBitSet columns) {\n    for (ImmutableBitSet key : nonNullablekeys) {\n      if (key.contains(columns)) {\n        return true;\n      }\n    }\n    return false;\n  }",
            " 223  \n 224  \n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  ",
            "  public boolean isNonNullableKey(ImmutableBitSet columns) {\n    for (ImmutableBitSet key : nonNullablekeys) {\n      if (columns.contains(key)) {\n        return true;\n      }\n    }\n    return false;\n  }"
        ]
    ],
    "d7fbd639ff0188b0e5d22ada50795b1a0784a0be": [
        [
            "HashCodeUtil::calculateBytesHashCode(byte,int,int)",
            "  50  \n  51  \n  52  ",
            "  public static int calculateBytesHashCode(byte[] keyBytes, int keyStart, int keyLength) {\n    return murmurHash(keyBytes, keyStart, keyLength);\n  }",
            "  50  \n  51  \n  52 +\n  53  \n  54  ",
            "  @Deprecated\n  public static int calculateBytesHashCode(byte[] keyBytes, int keyStart, int keyLength) {\n    // Don't use this for ReduceSinkOperators\n    return murmurHash(keyBytes, keyStart, keyLength);\n  }"
        ],
        [
            "VectorReduceSinkUniformHashOperator::initializeOp(Configuration)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91  \n  92  \n  93  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n\n    Preconditions.checkState(!isEmptyKey);\n    // Create all nulls key.\n    try {\n      Output nullKeyOutput = new Output();\n      keyBinarySortableSerializeWrite.set(nullKeyOutput);\n      for (int i = 0; i < reduceSinkKeyColumnMap.length; i++) {\n        keyBinarySortableSerializeWrite.writeNull();\n      }\n      int nullBytesLength = nullKeyOutput.getLength();\n      nullBytes = new byte[nullBytesLength];\n      System.arraycopy(nullKeyOutput.getData(), 0, nullBytes, 0, nullBytesLength);\n      nullKeyHashCode = HashCodeUtil.calculateBytesHashCode(nullBytes, 0, nullBytesLength);\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  \n  91  \n  92  \n  93  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n\n    Preconditions.checkState(!isEmptyKey);\n    // Create all nulls key.\n    try {\n      Output nullKeyOutput = new Output();\n      keyBinarySortableSerializeWrite.set(nullKeyOutput);\n      for (int i = 0; i < reduceSinkKeyColumnMap.length; i++) {\n        keyBinarySortableSerializeWrite.writeNull();\n      }\n      int nullBytesLength = nullKeyOutput.getLength();\n      nullBytes = new byte[nullBytesLength];\n      System.arraycopy(nullKeyOutput.getData(), 0, nullBytes, 0, nullBytesLength);\n      nullKeyHashCode = Murmur3.hash32(nullBytes, 0, nullBytesLength, 0);\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }"
        ]
    ],
    "b8afcc3f7c0b47b1a83ce258da1091f1dbcdb10f": [
        [
            "TestInputOutputFormat::testACIDReaderNoFooterSerializeWithDeltas()",
            "3621  \n3622  \n3623  \n3624  \n3625  \n3626  \n3627  \n3628  \n3629  \n3630  \n3631  \n3632  \n3633  \n3634  \n3635  \n3636  \n3637  \n3638  \n3639  \n3640  \n3641  \n3642  \n3643  \n3644  \n3645  \n3646  \n3647  \n3648  \n3649  \n3650  \n3651  \n3652  \n3653  \n3654  \n3655  \n3656  \n3657  \n3658  \n3659  \n3660  \n3661  \n3662  \n3663  \n3664  \n3665  \n3666  \n3667  \n3668  \n3669  \n3670  \n3671  \n3672  \n3673  \n3674  \n3675  \n3676  \n3677  \n3678  \n3679  \n3680  \n3681  \n3682  \n3683  \n3684  \n3685  \n3686  \n3687  \n3688  \n3689  \n3690 -\n3691  \n3692  \n3693  \n3694  ",
            "  @Test\n  public void testACIDReaderNoFooterSerializeWithDeltas() throws Exception {\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    FileSystem fs = FileSystem.get(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable7\");\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, MyRow.getColumnNamesProperty());\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, MyRow.getColumnTypesProperty());\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    AcidOutputFormat.Options options = new AcidOutputFormat.Options(conf).bucket(1).minimumWriteId(1)\n        .maximumWriteId(1).inspector(inspector).finalDestination(mockPath);\n    OrcOutputFormat of = new OrcOutputFormat();\n    RecordUpdater ru = of.getRecordUpdater(mockPath, options);\n    for (int i = 0; i < 10; ++i) {\n      ru.insert(options.getMinimumWriteId(), new MyRow(i, 2 * i));\n    }\n    ru.close(false);//this deletes the side file\n\n    //set up props for read\n    conf.setBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, true);\n    AcidUtils.setAcidOperationalProperties(conf, true, null);\n    conf.set(ValidTxnList.VALID_TXNS_KEY,\n        new ValidReadTxnList(new long[0], new BitSet(), 1000, Long.MAX_VALUE).writeToString());\n\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertFalse(\"No footer serialize test for ACID reader, hasFooter is not expected in\" +\n        \" orc splits.\", ((OrcSplit) split).hasFooter());\n      orcInputFormat.getRecordReader(split, conf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    assertEquals(12, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }",
            "3609  \n3610  \n3611  \n3612  \n3613  \n3614  \n3615  \n3616  \n3617  \n3618  \n3619  \n3620  \n3621  \n3622  \n3623  \n3624  \n3625  \n3626  \n3627  \n3628  \n3629  \n3630  \n3631  \n3632  \n3633  \n3634  \n3635  \n3636  \n3637  \n3638  \n3639  \n3640  \n3641  \n3642  \n3643  \n3644  \n3645  \n3646  \n3647  \n3648  \n3649  \n3650  \n3651  \n3652  \n3653  \n3654  \n3655  \n3656  \n3657  \n3658  \n3659  \n3660  \n3661  \n3662  \n3663  \n3664  \n3665  \n3666  \n3667  \n3668  \n3669  \n3670  \n3671  \n3672  \n3673  \n3674  \n3675  \n3676  \n3677  \n3678 +\n3679  \n3680  \n3681  \n3682  ",
            "  @Test\n  public void testACIDReaderNoFooterSerializeWithDeltas() throws Exception {\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    FileSystem fs = FileSystem.get(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable7\");\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, MyRow.getColumnNamesProperty());\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, MyRow.getColumnTypesProperty());\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    AcidOutputFormat.Options options = new AcidOutputFormat.Options(conf).bucket(1).minimumWriteId(1)\n        .maximumWriteId(1).inspector(inspector).finalDestination(mockPath);\n    OrcOutputFormat of = new OrcOutputFormat();\n    RecordUpdater ru = of.getRecordUpdater(mockPath, options);\n    for (int i = 0; i < 10; ++i) {\n      ru.insert(options.getMinimumWriteId(), new MyRow(i, 2 * i));\n    }\n    ru.close(false);//this deletes the side file\n\n    //set up props for read\n    conf.setBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, true);\n    AcidUtils.setAcidOperationalProperties(conf, true, null);\n    conf.set(ValidTxnList.VALID_TXNS_KEY,\n        new ValidReadTxnList(new long[0], new BitSet(), 1000, Long.MAX_VALUE).writeToString());\n\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertFalse(\"No footer serialize test for ACID reader, hasFooter is not expected in\" +\n        \" orc splits.\", ((OrcSplit) split).hasFooter());\n      orcInputFormat.getRecordReader(split, conf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    assertEquals(10, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }"
        ],
        [
            "TestInputOutputFormat::testNonVectorReaderNoFooterSerialize()",
            "3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212  \n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233 -\n3234 -\n3235 -\n3236 -\n3237 -\n3238  \n3239  \n3240  \n3241  ",
            "  @Test\n  public void testNonVectorReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable1\");\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for non-vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, conf, null);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read footer - split 1 => mock:/mocktable1/0_0\n    // call-2: open to read data - split 1 => mock:/mocktable1/0_0\n    // call-3: open to read footer - split 2 => mock:/mocktable1/0_1\n    // call-4: open to read data - split 2 => mock:/mocktable1/0_1\n    assertEquals(4, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }",
            "3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212  \n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233 +\n3234 +\n3235 +\n3236  \n3237  \n3238  \n3239  ",
            "  @Test\n  public void testNonVectorReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable1\");\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for non-vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, conf, null);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read - split 1 => mock:/mocktable1/0_0\n    // call-2: open to read - split 2 => mock:/mocktable1/0_1\n    assertEquals(2, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }"
        ],
        [
            "TestInputOutputFormat::testACIDReaderNoFooterSerialize()",
            "3461  \n3462  \n3463  \n3464  \n3465  \n3466  \n3467  \n3468  \n3469  \n3470  \n3471  \n3472  \n3473  \n3474  \n3475  \n3476  \n3477  \n3478  \n3479  \n3480  \n3481  \n3482  \n3483  \n3484  \n3485  \n3486  \n3487  \n3488  \n3489  \n3490  \n3491  \n3492  \n3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503  \n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514  \n3515  \n3516  \n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  \n3524  \n3525  \n3526  \n3527  \n3528 -\n3529 -\n3530 -\n3531 -\n3532 -\n3533 -\n3534 -\n3535 -\n3536 -\n3537  \n3538  \n3539  \n3540  ",
            "  @Test\n  public void testACIDReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable5\");\n    conf.set(ConfVars.HIVE_TRANSACTIONAL_TABLE_SCAN.varname, \"true\");\n    conf.setBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, true);\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, MyRow.getColumnNamesProperty());\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, MyRow.getColumnTypesProperty());\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      assertTrue(split.toString().contains(\"isOriginal=true\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for non-vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, conf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read footer - split 1 => mock:/mocktable5/0_0\n    // call-2: open to read data - split 1 => mock:/mocktable5/0_0\n    // call-3: getAcidState - split 1 => mock:/mocktable5 (to compute offset for original read)\n    // call-4: open to read footer - split 2 => mock:/mocktable5/0_1\n    // call-5: open to read data - split 2 => mock:/mocktable5/0_1\n    // call-6: getAcidState - split 2 => mock:/mocktable5 (to compute offset for original read)\n    // call-7: open to read footer - split 2 => mock:/mocktable5/0_0 (to get row count)\n    // call-8: file status - split 2 => mock:/mocktable5/0_0\n    assertEquals(12, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }",
            "3457  \n3458  \n3459  \n3460  \n3461  \n3462  \n3463  \n3464  \n3465  \n3466  \n3467  \n3468  \n3469  \n3470  \n3471  \n3472  \n3473  \n3474  \n3475  \n3476  \n3477  \n3478  \n3479  \n3480  \n3481  \n3482  \n3483  \n3484  \n3485  \n3486  \n3487  \n3488  \n3489  \n3490  \n3491  \n3492  \n3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503  \n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514  \n3515  \n3516  \n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  \n3524 +\n3525  \n3526  \n3527  \n3528  ",
            "  @Test\n  public void testACIDReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable5\");\n    conf.set(ConfVars.HIVE_TRANSACTIONAL_TABLE_SCAN.varname, \"true\");\n    conf.setBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, true);\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, MyRow.getColumnNamesProperty());\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, MyRow.getColumnTypesProperty());\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      assertTrue(split.toString().contains(\"isOriginal=true\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for non-vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, conf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    assertEquals(10, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }"
        ],
        [
            "TestInputOutputFormat::testVectorReaderNoFooterSerialize()",
            "3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331  \n3332  \n3333  \n3334  \n3335  \n3336  \n3337  \n3338  \n3339  \n3340  \n3341  \n3342  \n3343  \n3344  \n3345  \n3346  \n3347  \n3348  \n3349  \n3350  \n3351  \n3352  \n3353  \n3354  \n3355  \n3356  \n3357  \n3358  \n3359  \n3360  \n3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378 -\n3379 -\n3380 -\n3381 -\n3382 -\n3383  \n3384  \n3385  \n3386  ",
            "  @Test\n  public void testVectorReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable3\");\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    JobConf jobConf = createMockExecutionEnvironment(workDir, new Path(\"mock:///\"),\n        \"mocktable3\", inspector, true, 0);\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, jobConf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read footer - split 1 => mock:/mocktable3/0_0\n    // call-2: open to read data - split 1 => mock:/mocktable3/0_0\n    // call-3: open to read footer - split 2 => mock:/mocktable3/0_1\n    // call-4: open to read data - split 2 => mock:/mocktable3/0_1\n    assertEquals(4, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }",
            "3311  \n3312  \n3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331  \n3332  \n3333  \n3334  \n3335  \n3336  \n3337  \n3338  \n3339  \n3340  \n3341  \n3342  \n3343  \n3344  \n3345  \n3346  \n3347  \n3348  \n3349  \n3350  \n3351  \n3352  \n3353  \n3354  \n3355  \n3356  \n3357  \n3358  \n3359  \n3360  \n3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376 +\n3377 +\n3378 +\n3379  \n3380  \n3381  \n3382  ",
            "  @Test\n  public void testVectorReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable3\");\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    JobConf jobConf = createMockExecutionEnvironment(workDir, new Path(\"mock:///\"),\n        \"mocktable3\", inspector, true, 0);\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, jobConf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read - split 1 => mock:/mocktable3/0_0\n    // call-2: open to read - split 2 => mock:/mocktable3/0_1\n    assertEquals(2, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }"
        ]
    ]
}