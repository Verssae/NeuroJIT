{
    "9feed2f61aeb91d34b8c4231ca1d6149d81c8a02": [
        [
            "SessionState::start(SessionState,boolean,LogHelper)",
            " 550 -\n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  ",
            "  private static void start(SessionState startSs, boolean isAsync, LogHelper console) {\n    setCurrentSessionState(startSs);\n\n    if (startSs.hiveHist == null){\n      if (startSs.getConf().getBoolVar(HiveConf.ConfVars.HIVE_SESSION_HISTORY_ENABLED)) {\n        startSs.hiveHist = new HiveHistoryImpl(startSs);\n      } else {\n        // Hive history is disabled, create a no-op proxy\n        startSs.hiveHist = HiveHistoryProxyHandler.getNoOpHiveHistoryProxy();\n      }\n    }\n\n    // Get the following out of the way when you start the session these take a\n    // while and should be done when we start up.\n    try {\n      UserGroupInformation sessionUGI = Utils.getUGI();\n      FileSystem.get(startSs.sessionConf);\n\n      // Create scratch dirs for this session\n      startSs.createSessionDirs(sessionUGI.getShortUserName());\n\n      // Set temp file containing results to be sent to HiveClient\n      if (startSs.getTmpOutputFile() == null) {\n        try {\n          startSs.setTmpOutputFile(createTempFile(startSs.getConf()));\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n\n      // Set temp file containing error output to be sent to client\n      if (startSs.getTmpErrOutputFile() == null) {\n        try {\n          startSs.setTmpErrOutputFile(createTempFile(startSs.getConf()));\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (Exception e) {\n      // Catch-all due to some exec time dependencies on session state\n      // that would cause ClassNoFoundException otherwise\n      throw new RuntimeException(e);\n    }\n\n    String engine = HiveConf.getVar(startSs.getConf(), HiveConf.ConfVars.HIVE_EXECUTION_ENGINE);\n    if (!engine.equals(\"tez\") || startSs.isHiveServerQuery) return;\n\n    try {\n      if (startSs.tezSessionState == null) {\n        startSs.setTezSession(new TezSessionState(startSs.getSessionId()));\n      }\n      if (startSs.tezSessionState.isOpen()) {\n        return;\n      }\n      if (startSs.tezSessionState.isOpening()) {\n        if (!isAsync) {\n          startSs.tezSessionState.endOpen();\n        }\n        return;\n      }\n      // Neither open nor opening.\n      if (!isAsync) {\n        startSs.tezSessionState.open(startSs.sessionConf); // should use conf on session start-up\n      } else {\n        startSs.tezSessionState.beginOpen(startSs.sessionConf, null, console);\n      }\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }",
            " 554 +\n 555  \n 556  \n 557 +\n 558 +\n 559 +\n 560 +\n 561 +\n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  ",
            "  synchronized private static void start(SessionState startSs, boolean isAsync, LogHelper console) {\n    setCurrentSessionState(startSs);\n\n    if (startSs.isStarted) {\n      return;\n    }\n    startSs.isStarted = true;\n\n    if (startSs.hiveHist == null){\n      if (startSs.getConf().getBoolVar(HiveConf.ConfVars.HIVE_SESSION_HISTORY_ENABLED)) {\n        startSs.hiveHist = new HiveHistoryImpl(startSs);\n      } else {\n        // Hive history is disabled, create a no-op proxy\n        startSs.hiveHist = HiveHistoryProxyHandler.getNoOpHiveHistoryProxy();\n      }\n    }\n\n    // Get the following out of the way when you start the session these take a\n    // while and should be done when we start up.\n    try {\n      UserGroupInformation sessionUGI = Utils.getUGI();\n      FileSystem.get(startSs.sessionConf);\n\n      // Create scratch dirs for this session\n      startSs.createSessionDirs(sessionUGI.getShortUserName());\n\n      // Set temp file containing results to be sent to HiveClient\n      if (startSs.getTmpOutputFile() == null) {\n        try {\n          startSs.setTmpOutputFile(createTempFile(startSs.getConf()));\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n\n      // Set temp file containing error output to be sent to client\n      if (startSs.getTmpErrOutputFile() == null) {\n        try {\n          startSs.setTmpErrOutputFile(createTempFile(startSs.getConf()));\n        } catch (IOException e) {\n          throw new RuntimeException(e);\n        }\n      }\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (Exception e) {\n      // Catch-all due to some exec time dependencies on session state\n      // that would cause ClassNoFoundException otherwise\n      throw new RuntimeException(e);\n    }\n\n    String engine = HiveConf.getVar(startSs.getConf(), HiveConf.ConfVars.HIVE_EXECUTION_ENGINE);\n    if (!engine.equals(\"tez\") || startSs.isHiveServerQuery) return;\n\n    try {\n      if (startSs.tezSessionState == null) {\n        startSs.setTezSession(new TezSessionState(startSs.getSessionId()));\n      }\n      if (startSs.tezSessionState.isOpen()) {\n        return;\n      }\n      if (startSs.tezSessionState.isOpening()) {\n        if (!isAsync) {\n          startSs.tezSessionState.endOpen();\n        }\n        return;\n      }\n      // Neither open nor opening.\n      if (!isAsync) {\n        startSs.tezSessionState.open(startSs.sessionConf); // should use conf on session start-up\n      } else {\n        startSs.tezSessionState.beginOpen(startSs.sessionConf, null, console);\n      }\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }"
        ]
    ],
    "4b7f373e58a222cc2bd83ea28b916009d7ebf75b": [
        [
            "CliConfigs::MiniTezCliConfig::MiniTezCliConfig()",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  ",
            "    public MiniTezCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minitez.query.files\");\n        includesFrom(testConfigProps, \"minitez.query.files.shared\");\n        excludesFrom(testConfigProps, \"minillap.query.files\");\n        excludesFrom(testConfigProps, \"minillap.shared.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/tez\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init_tez.sql\");\n        setCleanupScript(\"q_test_cleanup_tez.sql\");\n\n        setHiveConfDir(\"data/conf/tez\");\n        setClusterType(MiniClusterType.tez);\n        setMetastoreType(MetastoreType.hbase);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  ",
            "    public MiniTezCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"minitez.query.files\");\n        includesFrom(testConfigProps, \"minitez.query.files.shared\");\n        excludesFrom(testConfigProps, \"minillap.query.files\");\n        excludesFrom(testConfigProps, \"minillap.shared.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/tez\");\n        setLogDir(\"itests/qtest/target/qfile-results/clientpositive\");\n\n        setInitScript(\"q_test_init_tez.sql\");\n        setCleanupScript(\"q_test_cleanup_tez.sql\");\n\n        setHiveConfDir(\"data/conf/tez\");\n        setClusterType(MiniClusterType.tez);\n        setMetastoreType(MetastoreType.sql);\n        setFsType(QTestUtil.FsType.hdfs);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ]
    ],
    "130617443bb05d79c18420c0c4e903a76da3651c": [
        [
            "HiveAlterHandler::alterTable(RawStore,Warehouse,String,String,Table,EnvironmentContext)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 -\n 167 -\n 168 -\n 169 -\n 170  \n 171  \n 172  \n 173  \n 174  \n 175 -\n 176  \n 177 -\n 178 -\n 179 -\n 180 -\n 181 -\n 182 -\n 183 -\n 184 -\n 185 -\n 186 -\n 187 -\n 188 -\n 189 -\n 190 -\n 191 -\n 192 -\n 193 -\n 194 -\n 195 -\n 196 -\n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202 -\n 203 -\n 204 -\n 205  \n 206 -\n 207 -\n 208 -\n 209 -\n 210 -\n 211 -\n 212 -\n 213 -\n 214 -\n 215 -\n 216 -\n 217 -\n 218 -\n 219 -\n 220 -\n 221 -\n 222 -\n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229 -\n 230  \n 231 -\n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  ",
            "  @Override\n  public void alterTable(RawStore msdb, Warehouse wh, String dbname,\n      String name, Table newt, EnvironmentContext environmentContext) throws InvalidOperationException, MetaException {\n    final boolean cascade = environmentContext != null\n        && environmentContext.isSetProperties()\n        && StatsSetupConst.TRUE.equals(environmentContext.getProperties().get(\n            StatsSetupConst.CASCADE));\n    if (newt == null) {\n      throw new InvalidOperationException(\"New table is invalid: \" + newt);\n    }\n\n    if (!MetaStoreUtils.validateName(newt.getTableName(), hiveConf)) {\n      throw new InvalidOperationException(newt.getTableName()\n          + \" is not a valid object name\");\n    }\n    String validate = MetaStoreUtils.validateTblColumns(newt.getSd().getCols());\n    if (validate != null) {\n      throw new InvalidOperationException(\"Invalid column \" + validate);\n    }\n\n    Path srcPath = null;\n    FileSystem srcFs = null;\n    Path destPath = null;\n    FileSystem destFs = null;\n\n    boolean success = false;\n    boolean moveData = false;\n    boolean rename = false;\n    Table oldt = null;\n    List<ObjectPair<Partition, String>> altps = new ArrayList<ObjectPair<Partition, String>>();\n\n    try {\n      msdb.openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n\n      // check if table with the new name already exists\n      if (!newt.getTableName().equalsIgnoreCase(name)\n          || !newt.getDbName().equalsIgnoreCase(dbname)) {\n        if (msdb.getTable(newt.getDbName(), newt.getTableName()) != null) {\n          throw new InvalidOperationException(\"new table \" + newt.getDbName()\n              + \".\" + newt.getTableName() + \" already exists\");\n        }\n        rename = true;\n      }\n\n      // get old table\n      oldt = msdb.getTable(dbname, name);\n      if (oldt == null) {\n        throw new InvalidOperationException(\"table \" + dbname + \".\" + name + \" doesn't exist\");\n      }\n\n      if (HiveConf.getBoolVar(hiveConf,\n            HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES,\n            false)) {\n        // Throws InvalidOperationException if the new column types are not\n        // compatible with the current column types.\n        MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(\n            oldt.getSd().getCols(), newt.getSd().getCols());\n      }\n\n      if (cascade) {\n        //Currently only column related changes can be cascaded in alter table\n        if(MetaStoreUtils.isCascadeNeededInAlterTable(oldt, newt)) {\n          List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n          for (Partition part : parts) {\n            List<FieldSchema> oldCols = part.getSd().getCols();\n            part.getSd().setCols(newt.getSd().getCols());\n            String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());\n            updatePartColumnStatsForAlterColumns(msdb, part, oldPartName, part.getValues(), oldCols, part);\n            msdb.alterPartition(dbname, name, part.getValues(), part);\n          }\n        } else {\n          LOG.warn(\"Alter table does not cascade changes to its partitions.\");\n        }\n      }\n\n      //check that partition keys have not changed, except for virtual views\n      //however, allow the partition comments to change\n      boolean partKeysPartiallyEqual = checkPartialPartKeysEqual(oldt.getPartitionKeys(),\n          newt.getPartitionKeys());\n\n      if(!oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())){\n        if (oldt.getPartitionKeys().size() != newt.getPartitionKeys().size()\n            || !partKeysPartiallyEqual) {\n          throw new InvalidOperationException(\n              \"partition keys can not be changed.\");\n        }\n      }\n\n      // if this alter is a rename, the table is not a virtual view, the user\n      // didn't change the default location (or new location is empty), and\n      // table is not an external table, that means user is asking metastore to\n      // move data to the new location corresponding to the new name\n      if (rename\n          && !oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())\n          && (oldt.getSd().getLocation().compareTo(newt.getSd().getLocation()) == 0\n            || StringUtils.isEmpty(newt.getSd().getLocation()))\n          && !MetaStoreUtils.isExternalTable(oldt)) {\n\n        srcPath = new Path(oldt.getSd().getLocation());\n        srcFs = wh.getFs(srcPath);\n\n        // that means user is asking metastore to move data to new location\n        // corresponding to the new name\n        // get new location\n        Database db = msdb.getDatabase(newt.getDbName());\n        Path databasePath = constructRenamedPath(wh.getDatabasePath(db), srcPath);\n        destPath = new Path(databasePath, newt.getTableName().toLowerCase());\n        destFs = wh.getFs(destPath);\n\n        newt.getSd().setLocation(destPath.toString());\n        moveData = true;\n\n        // check that destination does not exist otherwise we will be\n        // overwriting data\n        // check that src and dest are on the same file system\n        if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n          throw new InvalidOperationException(\"table new location \" + destPath\n              + \" is on a different file system than the old location \"\n              + srcPath + \". This operation is not supported\");\n        }\n        try {\n          srcFs.exists(srcPath); // check that src exists and also checks\n                                 // permissions necessary\n          if (destFs.exists(destPath)) {\n            throw new InvalidOperationException(\"New location for this table \"\n                + newt.getDbName() + \".\" + newt.getTableName()\n                + \" already exists : \" + destPath);\n          }\n        } catch (IOException e) {\n          throw new InvalidOperationException(\"Unable to access new location \"\n              + destPath + \" for table \" + newt.getDbName() + \".\"\n              + newt.getTableName());\n        }\n        String oldTblLocPath = srcPath.toUri().getPath();\n        String newTblLocPath = destPath.toUri().getPath();\n\n        // also the location field in partition\n        List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n        for (Partition part : parts) {\n          String oldPartLoc = part.getSd().getLocation();\n          if (oldPartLoc.contains(oldTblLocPath)) {\n            URI oldUri = new Path(oldPartLoc).toUri();\n            String newPath = oldUri.getPath().replace(oldTblLocPath, newTblLocPath);\n            Path newPartLocPath = new Path(oldUri.getScheme(), oldUri.getAuthority(), newPath);\n            altps.add(ObjectPair.create(part, part.getSd().getLocation()));\n            part.getSd().setLocation(newPartLocPath.toString());\n            String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());\n            try {\n              //existing partition column stats is no longer valid, remove them\n              msdb.deletePartitionColumnStatistics(dbname, name, oldPartName, part.getValues(), null);\n            } catch (InvalidInputException iie) {\n              throw new InvalidOperationException(\"Unable to update partition stats in table rename.\" + iie);\n            }\n            msdb.alterPartition(dbname, name, part.getValues(), part);\n          }\n        }\n      } else if (MetaStoreUtils.requireCalStats(hiveConf, null, null, newt, environmentContext) &&\n        (newt.getPartitionKeysSize() == 0)) {\n          Database db = msdb.getDatabase(newt.getDbName());\n          // Update table stats. For partitioned table, we update stats in\n          // alterPartition()\n          MetaStoreUtils.updateTableStatsFast(db, newt, wh, false, true, environmentContext);\n      }\n\n      alterTableUpdateTableColumnStats(msdb, oldt, newt);\n      // commit the changes\n      success = msdb.commitTransaction();\n    } catch (InvalidObjectException e) {\n      LOG.debug(\"Failed to get object from Metastore \", e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table.\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } catch (NoSuchObjectException e) {\n      LOG.debug(\"Object not found in metastore \", e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && moveData) {\n        // change the file name in hdfs\n        // check that src exists otherwise there is no need to copy the data\n        // rename the src to destination\n        try {\n          if (srcFs.exists(srcPath) && !srcFs.rename(srcPath, destPath)) {\n            throw new IOException(\"Renaming \" + srcPath + \" to \" + destPath + \" failed\");\n          }\n        } catch (IOException e) {\n          LOG.error(\"Alter Table operation for \" + dbname + \".\" + name + \" failed.\", e);\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterTable(newt.getDbName(), newt.getTableName(), oldt);\n            for (ObjectPair<Partition, String> pair : altps) {\n              Partition part = pair.getFirst();\n              part.getSd().setLocation(pair.getSecond());\n              msdb.alterPartition(newt.getDbName(), name, part.getValues(), part);\n            }\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            // we should log this for manual rollback by administrator\n            LOG.error(\"Reverting metadata by HDFS operation failure failed During HDFS operation failed\", e1);\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should be renamed to \" + Warehouse.getQualifiedName(oldt));\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should have path \" + srcPath);\n            for (ObjectPair<Partition, String> pair : altps) {\n              LOG.error(\"Partition \" + Warehouse.getQualifiedName(pair.getFirst()) +\n                  \" should have path \" + pair.getSecond());\n            }\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Alter Table operation for \" + dbname + \".\" + name +\n            \" failed to move data due to: '\" + getSimpleMessage(e) + \"' See hive log file for details.\");\n        }\n      }\n    }\n    if (!success) {\n      throw new MetaException(\"Committing the alter table transaction was not successful.\");\n    }\n  }",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172  \n 173  \n 174  \n 175  \n 176  \n 177 +\n 178 +\n 179 +\n 180 +\n 181 +\n 182 +\n 183  \n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193 +\n 194 +\n 195 +\n 196 +\n 197 +\n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208  \n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214 +\n 215 +\n 216 +\n 217 +\n 218 +\n 219 +\n 220 +\n 221 +\n 222 +\n 223 +\n 224 +\n 225 +\n 226 +\n 227 +\n 228 +\n 229 +\n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238 +\n 239 +\n 240 +\n 241 +\n 242 +\n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  ",
            "  @Override\n  public void alterTable(RawStore msdb, Warehouse wh, String dbname,\n      String name, Table newt, EnvironmentContext environmentContext) throws InvalidOperationException, MetaException {\n    final boolean cascade = environmentContext != null\n        && environmentContext.isSetProperties()\n        && StatsSetupConst.TRUE.equals(environmentContext.getProperties().get(\n            StatsSetupConst.CASCADE));\n    if (newt == null) {\n      throw new InvalidOperationException(\"New table is invalid: \" + newt);\n    }\n\n    if (!MetaStoreUtils.validateName(newt.getTableName(), hiveConf)) {\n      throw new InvalidOperationException(newt.getTableName()\n          + \" is not a valid object name\");\n    }\n    String validate = MetaStoreUtils.validateTblColumns(newt.getSd().getCols());\n    if (validate != null) {\n      throw new InvalidOperationException(\"Invalid column \" + validate);\n    }\n\n    Path srcPath = null;\n    FileSystem srcFs = null;\n    Path destPath = null;\n    FileSystem destFs = null;\n\n    boolean success = false;\n    boolean moveData = false;\n    boolean rename = false;\n    Table oldt = null;\n    List<ObjectPair<Partition, String>> altps = new ArrayList<ObjectPair<Partition, String>>();\n\n    try {\n      msdb.openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n\n      // check if table with the new name already exists\n      if (!newt.getTableName().equalsIgnoreCase(name)\n          || !newt.getDbName().equalsIgnoreCase(dbname)) {\n        if (msdb.getTable(newt.getDbName(), newt.getTableName()) != null) {\n          throw new InvalidOperationException(\"new table \" + newt.getDbName()\n              + \".\" + newt.getTableName() + \" already exists\");\n        }\n        rename = true;\n      }\n\n      // get old table\n      oldt = msdb.getTable(dbname, name);\n      if (oldt == null) {\n        throw new InvalidOperationException(\"table \" + dbname + \".\" + name + \" doesn't exist\");\n      }\n\n      if (HiveConf.getBoolVar(hiveConf,\n            HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES,\n            false)) {\n        // Throws InvalidOperationException if the new column types are not\n        // compatible with the current column types.\n        MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(\n            oldt.getSd().getCols(), newt.getSd().getCols());\n      }\n\n      if (cascade) {\n        //Currently only column related changes can be cascaded in alter table\n        if(MetaStoreUtils.isCascadeNeededInAlterTable(oldt, newt)) {\n          List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n          for (Partition part : parts) {\n            List<FieldSchema> oldCols = part.getSd().getCols();\n            part.getSd().setCols(newt.getSd().getCols());\n            String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());\n            updatePartColumnStatsForAlterColumns(msdb, part, oldPartName, part.getValues(), oldCols, part);\n            msdb.alterPartition(dbname, name, part.getValues(), part);\n          }\n        } else {\n          LOG.warn(\"Alter table does not cascade changes to its partitions.\");\n        }\n      }\n\n      //check that partition keys have not changed, except for virtual views\n      //however, allow the partition comments to change\n      boolean partKeysPartiallyEqual = checkPartialPartKeysEqual(oldt.getPartitionKeys(),\n          newt.getPartitionKeys());\n\n      if(!oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())){\n        if (oldt.getPartitionKeys().size() != newt.getPartitionKeys().size()\n            || !partKeysPartiallyEqual) {\n          throw new InvalidOperationException(\n              \"partition keys can not be changed.\");\n        }\n      }\n\n      // rename needs change the data location and move the data to the new location corresponding\n      // to the new name if:\n      // 1) the table is not a virtual view, and\n      // 2) the table is not an external table, and\n      // 3) the user didn't change the default location (or new location is empty), and\n      // 4) the table was not initially created with a specified location\n      if (rename\n          && !oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())\n          && (oldt.getSd().getLocation().compareTo(newt.getSd().getLocation()) == 0\n            || StringUtils.isEmpty(newt.getSd().getLocation()))\n          && !MetaStoreUtils.isExternalTable(oldt)) {\n        Database olddb = msdb.getDatabase(dbname);\n        // if a table was created in a user specified location using the DDL like\n        // create table tbl ... location ...., it should be treated like an external table\n        // in the table rename, its data location should not be changed. We can check\n        // if the table directory was created directly under its database directory to tell\n        // if it is such a table\n        srcPath = new Path(oldt.getSd().getLocation());\n        String oldtRelativePath = (new Path(olddb.getLocationUri()).toUri())\n            .relativize(srcPath.toUri()).toString();\n        boolean tableInSpecifiedLoc = !oldtRelativePath.equalsIgnoreCase(name)\n            && !oldtRelativePath.equalsIgnoreCase(name + Path.SEPARATOR);\n\n        if (!tableInSpecifiedLoc) {\n          srcFs = wh.getFs(srcPath);\n\n          // get new location\n          Database db = msdb.getDatabase(newt.getDbName());\n          Path databasePath = constructRenamedPath(wh.getDatabasePath(db), srcPath);\n          destPath = new Path(databasePath, newt.getTableName().toLowerCase());\n          destFs = wh.getFs(destPath);\n\n          newt.getSd().setLocation(destPath.toString());\n          moveData = true;\n\n          // check that destination does not exist otherwise we will be\n          // overwriting data\n          // check that src and dest are on the same file system\n          if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n            throw new InvalidOperationException(\"table new location \" + destPath\n                + \" is on a different file system than the old location \"\n                + srcPath + \". This operation is not supported\");\n          }\n          try {\n            srcFs.exists(srcPath); // check that src exists and also checks\n                                   // permissions necessary\n            if (destFs.exists(destPath)) {\n              throw new InvalidOperationException(\"New location for this table \"\n                  + newt.getDbName() + \".\" + newt.getTableName()\n                  + \" already exists : \" + destPath);\n            }\n          } catch (IOException e) {\n            throw new InvalidOperationException(\"Unable to access new location \"\n                + destPath + \" for table \" + newt.getDbName() + \".\"\n                + newt.getTableName());\n          }\n          String oldTblLocPath = srcPath.toUri().getPath();\n          String newTblLocPath = destPath.toUri().getPath();\n\n          // also the location field in partition\n          List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n          for (Partition part : parts) {\n            String oldPartLoc = part.getSd().getLocation();\n            if (oldPartLoc.contains(oldTblLocPath)) {\n              URI oldUri = new Path(oldPartLoc).toUri();\n              String newPath = oldUri.getPath().replace(oldTblLocPath, newTblLocPath);\n              Path newPartLocPath = new Path(oldUri.getScheme(), oldUri.getAuthority(), newPath);\n              altps.add(ObjectPair.create(part, part.getSd().getLocation()));\n              part.getSd().setLocation(newPartLocPath.toString());\n              String oldPartName = Warehouse.makePartName(oldt.getPartitionKeys(), part.getValues());\n              try {\n                //existing partition column stats is no longer valid, remove them\n                msdb.deletePartitionColumnStatistics(dbname, name, oldPartName, part.getValues(), null);\n              } catch (InvalidInputException iie) {\n                throw new InvalidOperationException(\"Unable to update partition stats in table rename.\" + iie);\n              }\n              msdb.alterPartition(dbname, name, part.getValues(), part);\n            }\n          }\n        }\n      } else if (MetaStoreUtils.requireCalStats(hiveConf, null, null, newt, environmentContext) &&\n        (newt.getPartitionKeysSize() == 0)) {\n          Database db = msdb.getDatabase(newt.getDbName());\n          // Update table stats. For partitioned table, we update stats in\n          // alterPartition()\n          MetaStoreUtils.updateTableStatsFast(db, newt, wh, false, true, environmentContext);\n      }\n\n      alterTableUpdateTableColumnStats(msdb, oldt, newt);\n      // commit the changes\n      success = msdb.commitTransaction();\n    } catch (InvalidObjectException e) {\n      LOG.debug(\"Failed to get object from Metastore \", e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table.\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } catch (NoSuchObjectException e) {\n      LOG.debug(\"Object not found in metastore \", e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && moveData) {\n        // change the file name in hdfs\n        // check that src exists otherwise there is no need to copy the data\n        // rename the src to destination\n        try {\n          if (srcFs.exists(srcPath) && !srcFs.rename(srcPath, destPath)) {\n            throw new IOException(\"Renaming \" + srcPath + \" to \" + destPath + \" failed\");\n          }\n        } catch (IOException e) {\n          LOG.error(\"Alter Table operation for \" + dbname + \".\" + name + \" failed.\", e);\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterTable(newt.getDbName(), newt.getTableName(), oldt);\n            for (ObjectPair<Partition, String> pair : altps) {\n              Partition part = pair.getFirst();\n              part.getSd().setLocation(pair.getSecond());\n              msdb.alterPartition(newt.getDbName(), name, part.getValues(), part);\n            }\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            // we should log this for manual rollback by administrator\n            LOG.error(\"Reverting metadata by HDFS operation failure failed During HDFS operation failed\", e1);\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should be renamed to \" + Warehouse.getQualifiedName(oldt));\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should have path \" + srcPath);\n            for (ObjectPair<Partition, String> pair : altps) {\n              LOG.error(\"Partition \" + Warehouse.getQualifiedName(pair.getFirst()) +\n                  \" should have path \" + pair.getSecond());\n            }\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Alter Table operation for \" + dbname + \".\" + name +\n            \" failed to move data due to: '\" + getSimpleMessage(e) + \"' See hive log file for details.\");\n        }\n      }\n    }\n    if (!success) {\n      throw new MetaException(\"Committing the alter table transaction was not successful.\");\n    }\n  }"
        ],
        [
            "TestSemanticAnalysis::testAlterTableRename()",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 -\n 252  \n 253  \n 254  \n 255 -\n 256  \n 257  \n 258  ",
            "  @Test\n  public void testAlterTableRename() throws CommandNeedRetryException, TException {\n    hcatDriver.run(\"drop table oldname\");\n    hcatDriver.run(\"drop table newname\");\n    hcatDriver.run(\"create table oldname (a int)\");\n    Table tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, \"oldname\");\n    assertTrue(tbl.getSd().getLocation().contains(\"oldname\"));\n\n    hcatDriver.run(\"alter table oldname rename to newNAME\");\n    tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, \"newname\");\n    assertTrue(tbl.getSd().getLocation().contains(\"newname\"));\n\n    hcatDriver.run(\"drop table newname\");\n  }",
            " 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251 +\n 252  \n 253  \n 254  \n 255 +\n 256 +\n 257 +\n 258 +\n 259 +\n 260 +\n 261  \n 262  \n 263  ",
            "  @Test\n  public void testAlterTableRename() throws CommandNeedRetryException, TException {\n    hcatDriver.run(\"drop table oldname\");\n    hcatDriver.run(\"drop table newname\");\n    hcatDriver.run(\"create table oldname (a int)\");\n    Table tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, \"oldname\");\n    assertTrue(\"The old table location is: \" + tbl.getSd().getLocation(), tbl.getSd().getLocation().contains(\"oldname\"));\n\n    hcatDriver.run(\"alter table oldname rename to newNAME\");\n    tbl = client.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, \"newname\");\n    // since the oldname table is not under its database (See HIVE-15059), the renamed oldname table will keep\n    // its location after HIVE-14909. I changed to check the existence of the newname table and its name instead\n    // of verifying its location\n    // assertTrue(tbl.getSd().getLocation().contains(\"newname\"));\n    assertTrue(tbl != null);\n    assertTrue(tbl.getTableName().equalsIgnoreCase(\"newname\"));\n\n    hcatDriver.run(\"drop table newname\");\n  }"
        ]
    ],
    "27fb87cfcea241c2d7961baf68e84ce97f2dee7a": [
        [
            "ColumnStatsUpdateTask::constructColumnStatsFromInput()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 -\n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150 -\n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244 -\n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  ",
            "  private ColumnStatistics constructColumnStatsFromInput()\n      throws SemanticException, MetaException {\n\n    String dbName = SessionState.get().getCurrentDatabase();\n    ColumnStatsDesc desc = work.getColStats();\n    String tableName = desc.getTableName();\n    String partName = work.getPartName();\n    List<String> colName = desc.getColName();\n    List<String> colType = desc.getColType();\n\n    ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n\n    // grammar prohibits more than 1 column so we are guaranteed to have only 1\n    // element in this lists.\n\n    statsObj.setColName(colName.get(0));\n\n    statsObj.setColType(colType.get(0));\n    \n    ColumnStatisticsData statsData = new ColumnStatisticsData();\n    \n    String columnType = colType.get(0);\n\n    if (columnType.equalsIgnoreCase(\"long\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNullsIsSet(false);\n      longStats.setNumDVsIsSet(false);\n      longStats.setLowValueIsSet(false);\n      longStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          longStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          longStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          longStats.setLowValue(Long.parseLong(value));\n        } else if (fName.equals(\"highValue\")) {\n          longStats.setHighValue(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n\n      }\n      statsData.setLongStats(longStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"double\")) {\n      DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n      doubleStats.setNumNullsIsSet(false);\n      doubleStats.setNumDVsIsSet(false);\n      doubleStats.setLowValueIsSet(false);\n      doubleStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          doubleStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          doubleStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          doubleStats.setLowValue(Double.parseDouble(value));\n        } else if (fName.equals(\"highValue\")) {\n          doubleStats.setHighValue(Double.parseDouble(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDoubleStats(doubleStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"string\")) {\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setMaxColLenIsSet(false);\n      stringStats.setAvgColLenIsSet(false);\n      stringStats.setNumNullsIsSet(false);\n      stringStats.setNumDVsIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          stringStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          stringStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"avgColLen\")) {\n          stringStats.setAvgColLen(Double.parseDouble(value));\n        } else if (fName.equals(\"maxColLen\")) {\n          stringStats.setMaxColLen(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setStringStats(stringStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"boolean\")) {\n      BooleanColumnStatsData booleanStats = new BooleanColumnStatsData();\n      booleanStats.setNumNullsIsSet(false);\n      booleanStats.setNumTruesIsSet(false);\n      booleanStats.setNumFalsesIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          booleanStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numTrues\")) {\n          booleanStats.setNumTrues(Long.parseLong(value));\n        } else if (fName.equals(\"numFalses\")) {\n          booleanStats.setNumFalses(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setBooleanStats(booleanStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNullsIsSet(false);\n      binaryStats.setAvgColLenIsSet(false);\n      binaryStats.setMaxColLenIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          binaryStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"avgColLen\")) {\n          binaryStats.setAvgColLen(Double.parseDouble(value));\n        } else if (fName.equals(\"maxColLen\")) {\n          binaryStats.setMaxColLen(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setBinaryStats(binaryStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"decimal\")) {\n      DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n      decimalStats.setNumNullsIsSet(false);\n      decimalStats.setNumDVsIsSet(false);\n      decimalStats.setLowValueIsSet(false);\n      decimalStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          decimalStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          decimalStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          BigDecimal d = new BigDecimal(value);\n          decimalStats.setLowValue(new Decimal(ByteBuffer.wrap(d\n              .unscaledValue().toByteArray()), (short) d.scale()));\n        } else if (fName.equals(\"highValue\")) {\n          BigDecimal d = new BigDecimal(value);\n          decimalStats.setHighValue(new Decimal(ByteBuffer.wrap(d\n              .unscaledValue().toByteArray()), (short) d.scale()));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDecimalStats(decimalStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"date\")) {\n      DateColumnStatsData dateStats = new DateColumnStatsData();\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          dateStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          dateStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          // Date high/low value is stored as long in stats DB, but allow users to set high/low\n          // value using either date format (yyyy-mm-dd) or numeric format (days since epoch)\n          dateStats.setLowValue(readDateValue(value));\n        } else if (fName.equals(\"highValue\")) {\n          dateStats.setHighValue(readDateValue(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDateStats(dateStats);\n      statsObj.setStatsData(statsData);\n    } else {\n      throw new SemanticException(\"Unsupported type\");\n    }\n    String [] names = Utilities.getDbTableName(dbName, tableName);\n    ColumnStatisticsDesc statsDesc = getColumnStatsDesc(names[0], names[1],\n        partName, partName == null);\n    ColumnStatistics colStat = new ColumnStatistics();\n    colStat.setStatsDesc(statsDesc);\n    colStat.addToStatsObj(statsObj);\n    return colStat;\n  }",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101 +\n 102 +\n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152 +\n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247 +\n 248 +\n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  ",
            "  private ColumnStatistics constructColumnStatsFromInput()\n      throws SemanticException, MetaException {\n\n    String dbName = SessionState.get().getCurrentDatabase();\n    ColumnStatsDesc desc = work.getColStats();\n    String tableName = desc.getTableName();\n    String partName = work.getPartName();\n    List<String> colName = desc.getColName();\n    List<String> colType = desc.getColType();\n\n    ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n\n    // grammar prohibits more than 1 column so we are guaranteed to have only 1\n    // element in this lists.\n\n    statsObj.setColName(colName.get(0));\n\n    statsObj.setColType(colType.get(0));\n    \n    ColumnStatisticsData statsData = new ColumnStatisticsData();\n    \n    String columnType = colType.get(0);\n\n    if (columnType.equalsIgnoreCase(\"long\") || columnType.equalsIgnoreCase(\"tinyint\")\n            || columnType.equalsIgnoreCase(\"smallint\") || columnType.equalsIgnoreCase(\"int\")\n            || columnType.equalsIgnoreCase(\"bigint\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNullsIsSet(false);\n      longStats.setNumDVsIsSet(false);\n      longStats.setLowValueIsSet(false);\n      longStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          longStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          longStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          longStats.setLowValue(Long.parseLong(value));\n        } else if (fName.equals(\"highValue\")) {\n          longStats.setHighValue(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n\n      }\n      statsData.setLongStats(longStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"double\") || columnType.equalsIgnoreCase(\"float\")) {\n      DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n      doubleStats.setNumNullsIsSet(false);\n      doubleStats.setNumDVsIsSet(false);\n      doubleStats.setLowValueIsSet(false);\n      doubleStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          doubleStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          doubleStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          doubleStats.setLowValue(Double.parseDouble(value));\n        } else if (fName.equals(\"highValue\")) {\n          doubleStats.setHighValue(Double.parseDouble(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDoubleStats(doubleStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"string\") || columnType.toLowerCase().startsWith(\"char\")\n              || columnType.toLowerCase().startsWith(\"varchar\")) { //char(x),varchar(x) types\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setMaxColLenIsSet(false);\n      stringStats.setAvgColLenIsSet(false);\n      stringStats.setNumNullsIsSet(false);\n      stringStats.setNumDVsIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          stringStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          stringStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"avgColLen\")) {\n          stringStats.setAvgColLen(Double.parseDouble(value));\n        } else if (fName.equals(\"maxColLen\")) {\n          stringStats.setMaxColLen(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setStringStats(stringStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"boolean\")) {\n      BooleanColumnStatsData booleanStats = new BooleanColumnStatsData();\n      booleanStats.setNumNullsIsSet(false);\n      booleanStats.setNumTruesIsSet(false);\n      booleanStats.setNumFalsesIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          booleanStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numTrues\")) {\n          booleanStats.setNumTrues(Long.parseLong(value));\n        } else if (fName.equals(\"numFalses\")) {\n          booleanStats.setNumFalses(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setBooleanStats(booleanStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNullsIsSet(false);\n      binaryStats.setAvgColLenIsSet(false);\n      binaryStats.setMaxColLenIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          binaryStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"avgColLen\")) {\n          binaryStats.setAvgColLen(Double.parseDouble(value));\n        } else if (fName.equals(\"maxColLen\")) {\n          binaryStats.setMaxColLen(Long.parseLong(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setBinaryStats(binaryStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.toLowerCase().startsWith(\"decimal\")) { //decimal(a,b) type\n      DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n      decimalStats.setNumNullsIsSet(false);\n      decimalStats.setNumDVsIsSet(false);\n      decimalStats.setLowValueIsSet(false);\n      decimalStats.setHighValueIsSet(false);\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          decimalStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          decimalStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          BigDecimal d = new BigDecimal(value);\n          decimalStats.setLowValue(new Decimal(ByteBuffer.wrap(d\n              .unscaledValue().toByteArray()), (short) d.scale()));\n        } else if (fName.equals(\"highValue\")) {\n          BigDecimal d = new BigDecimal(value);\n          decimalStats.setHighValue(new Decimal(ByteBuffer.wrap(d\n              .unscaledValue().toByteArray()), (short) d.scale()));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDecimalStats(decimalStats);\n      statsObj.setStatsData(statsData);\n    } else if (columnType.equalsIgnoreCase(\"date\")\n            || columnType.equalsIgnoreCase(\"timestamp\")) {\n      DateColumnStatsData dateStats = new DateColumnStatsData();\n      Map<String, String> mapProp = work.getMapProp();\n      for (Entry<String, String> entry : mapProp.entrySet()) {\n        String fName = entry.getKey();\n        String value = entry.getValue();\n        if (fName.equals(\"numNulls\")) {\n          dateStats.setNumNulls(Long.parseLong(value));\n        } else if (fName.equals(\"numDVs\")) {\n          dateStats.setNumDVs(Long.parseLong(value));\n        } else if (fName.equals(\"lowValue\")) {\n          // Date high/low value is stored as long in stats DB, but allow users to set high/low\n          // value using either date format (yyyy-mm-dd) or numeric format (days since epoch)\n          dateStats.setLowValue(readDateValue(value));\n        } else if (fName.equals(\"highValue\")) {\n          dateStats.setHighValue(readDateValue(value));\n        } else {\n          throw new SemanticException(\"Unknown stat\");\n        }\n      }\n      statsData.setDateStats(dateStats);\n      statsObj.setStatsData(statsData);\n    } else {\n      throw new SemanticException(\"Unsupported type\");\n    }\n    String [] names = Utilities.getDbTableName(dbName, tableName);\n    ColumnStatisticsDesc statsDesc = getColumnStatsDesc(names[0], names[1],\n        partName, partName == null);\n    ColumnStatistics colStat = new ColumnStatistics();\n    colStat.setStatsDesc(statsDesc);\n    colStat.addToStatsObj(statsObj);\n    return colStat;\n  }"
        ]
    ],
    "89efd238e357046e1daf2a402500c262c667d3ec": [
        [
            "LlapOptionsProcessor::LlapOptionsProcessor()",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  ",
            "  @SuppressWarnings(\"static-access\")\n  public LlapOptionsProcessor() {\n\n    // set the number of instances on which llap should run\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_INSTANCES).withLongOpt(OPTION_INSTANCES)\n        .withDescription(\"Specify the number of instances to run this on\").create('i'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_NAME).withLongOpt(OPTION_NAME)\n        .withDescription(\"Cluster name for YARN registry\").create('n'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_DIRECTORY).withLongOpt(OPTION_DIRECTORY)\n        .withDescription(\"Temp directory for jars etc.\").create('d'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_ARGS).withLongOpt(OPTION_ARGS)\n        .withDescription(\"java arguments to the llap instance\").create('a'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LOGLEVEL).withLongOpt(OPTION_LOGLEVEL)\n        .withDescription(\"log levels for the llap instance\").create('l'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LOGGER).withLongOpt(OPTION_LOGGER)\n        .withDescription(\n            \"logger for llap instance ([\" + LogHelpers.LLAP_LOGGER_NAME_RFA + \"], \" +\n                LogHelpers.LLAP_LOGGER_NAME_QUERY_ROUTING + \", \" + LogHelpers.LLAP_LOGGER_NAME_CONSOLE)\n        .create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_CHAOS_MONKEY).withLongOpt(OPTION_CHAOS_MONKEY)\n        .withDescription(\"chaosmonkey interval\").create('m'));\n\n    options.addOption(OptionBuilder.hasArg(false).withArgName(OPTION_SLIDER_DEFAULT_KEYTAB).withLongOpt(OPTION_SLIDER_DEFAULT_KEYTAB)\n        .withDescription(\"try to set default settings for Slider AM keytab; mostly for dev testing\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_KEYTAB_DIR).withLongOpt(OPTION_SLIDER_KEYTAB_DIR)\n        .withDescription(\"Slider AM keytab directory on HDFS (where the headless user keytab is stored by Slider keytab installation, e.g. .slider/keytabs/llap)\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_KEYTAB).withLongOpt(OPTION_SLIDER_KEYTAB)\n        .withDescription(\"Slider AM keytab file name inside \" + OPTION_SLIDER_KEYTAB_DIR).create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_PRINCIPAL).withLongOpt(OPTION_SLIDER_PRINCIPAL)\n        .withDescription(\"Slider AM principal; should be the user running the cluster, e.g. hive@EXAMPLE.COM\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_EXECUTORS).withLongOpt(OPTION_EXECUTORS)\n        .withDescription(\"executor per instance\").create('e'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_CACHE).withLongOpt(OPTION_CACHE)\n        .withDescription(\"cache size per instance\").create('c'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SIZE).withLongOpt(OPTION_SIZE)\n        .withDescription(\"container size per instance\").create('s'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_XMX).withLongOpt(OPTION_XMX)\n        .withDescription(\"working memory size\").create('w'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LLAP_QUEUE)\n        .withLongOpt(OPTION_LLAP_QUEUE)\n        .withDescription(\"The queue within which LLAP will be started\").create('q'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_OUTPUT_DIR)\n        .withLongOpt(OPTION_OUTPUT_DIR)\n        .withDescription(\"Output directory for the generated scripts\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_AUXJARS).withLongOpt(OPTION_AUXJARS)\n        .withDescription(\"additional jars to package (by default, JSON SerDe jar is packaged\"\n            + \" if available)\").create('j'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_AUXHBASE).withLongOpt(OPTION_AUXHBASE)\n        .withDescription(\"whether to package the HBase jars (true by default)\").create('h'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_JAVA_HOME).withLongOpt(OPTION_JAVA_HOME)\n        .withDescription(\n            \"Path to the JRE/JDK. This should be installed at the same location on all cluster nodes ($JAVA_HOME, java.home by default)\")\n        .create());\n\n    // -hiveconf x=y\n    options.addOption(OptionBuilder.withValueSeparator().hasArgs(2).withArgName(\"property=value\")\n        .withLongOpt(OPTION_HIVECONF)\n        .withDescription(\"Use value for given property. Overridden by explicit parameters\")\n        .create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(\"b\")\n        .withLongOpt(OPTION_SLIDER_AM_CONTAINER_MB)\n        .withDescription(\"The size of the slider AppMaster container in MB\").create('b'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_IO_THREADS)\n        .withLongOpt(OPTION_IO_THREADS).withDescription(\"executor per instance\").create('t'));\n\n    // [-H|--help]\n    options.addOption(new Option(\"H\", \"help\", false, \"Print help information\"));\n  }",
            " 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214 +\n 215 +\n 216 +\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  ",
            "  @SuppressWarnings(\"static-access\")\n  public LlapOptionsProcessor() {\n\n    // set the number of instances on which llap should run\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_INSTANCES).withLongOpt(OPTION_INSTANCES)\n        .withDescription(\"Specify the number of instances to run this on\").create('i'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_NAME).withLongOpt(OPTION_NAME)\n        .withDescription(\"Cluster name for YARN registry\").create('n'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_DIRECTORY).withLongOpt(OPTION_DIRECTORY)\n        .withDescription(\"Temp directory for jars etc.\").create('d'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_ARGS).withLongOpt(OPTION_ARGS)\n        .withDescription(\"java arguments to the llap instance\").create('a'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LOGLEVEL).withLongOpt(OPTION_LOGLEVEL)\n        .withDescription(\"log levels for the llap instance\").create('l'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LOGGER).withLongOpt(OPTION_LOGGER)\n        .withDescription(\n            \"logger for llap instance ([\" + LogHelpers.LLAP_LOGGER_NAME_RFA + \"], \" +\n                LogHelpers.LLAP_LOGGER_NAME_QUERY_ROUTING + \", \" + LogHelpers.LLAP_LOGGER_NAME_CONSOLE)\n        .create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_CHAOS_MONKEY).withLongOpt(OPTION_CHAOS_MONKEY)\n        .withDescription(\"chaosmonkey interval\").create('m'));\n\n    options.addOption(OptionBuilder.hasArg(false).withArgName(OPTION_SLIDER_DEFAULT_KEYTAB).withLongOpt(OPTION_SLIDER_DEFAULT_KEYTAB)\n        .withDescription(\"try to set default settings for Slider AM keytab; mostly for dev testing\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_KEYTAB_DIR).withLongOpt(OPTION_SLIDER_KEYTAB_DIR)\n        .withDescription(\"Slider AM keytab directory on HDFS (where the headless user keytab is stored by Slider keytab installation, e.g. .slider/keytabs/llap)\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_KEYTAB).withLongOpt(OPTION_SLIDER_KEYTAB)\n        .withDescription(\"Slider AM keytab file name inside \" + OPTION_SLIDER_KEYTAB_DIR).create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_PRINCIPAL).withLongOpt(OPTION_SLIDER_PRINCIPAL)\n        .withDescription(\"Slider AM principal; should be the user running the cluster, e.g. hive@EXAMPLE.COM\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SLIDER_PLACEMENT).withLongOpt(OPTION_SLIDER_PLACEMENT)\n        .withDescription(\"Slider placement policy; see slider documentation at https://slider.incubator.apache.org/docs/placement.html.\"\n          + \" 4 means anti-affinity (the default; unnecessary if LLAP is going to take more than half of the YARN capacity of a node), 0 is normal.\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_EXECUTORS).withLongOpt(OPTION_EXECUTORS)\n        .withDescription(\"executor per instance\").create('e'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_CACHE).withLongOpt(OPTION_CACHE)\n        .withDescription(\"cache size per instance\").create('c'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_SIZE).withLongOpt(OPTION_SIZE)\n        .withDescription(\"container size per instance\").create('s'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_XMX).withLongOpt(OPTION_XMX)\n        .withDescription(\"working memory size\").create('w'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_LLAP_QUEUE)\n        .withLongOpt(OPTION_LLAP_QUEUE)\n        .withDescription(\"The queue within which LLAP will be started\").create('q'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_OUTPUT_DIR)\n        .withLongOpt(OPTION_OUTPUT_DIR)\n        .withDescription(\"Output directory for the generated scripts\").create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_AUXJARS).withLongOpt(OPTION_AUXJARS)\n        .withDescription(\"additional jars to package (by default, JSON SerDe jar is packaged\"\n            + \" if available)\").create('j'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_AUXHBASE).withLongOpt(OPTION_AUXHBASE)\n        .withDescription(\"whether to package the HBase jars (true by default)\").create('h'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_JAVA_HOME).withLongOpt(OPTION_JAVA_HOME)\n        .withDescription(\n            \"Path to the JRE/JDK. This should be installed at the same location on all cluster nodes ($JAVA_HOME, java.home by default)\")\n        .create());\n\n    // -hiveconf x=y\n    options.addOption(OptionBuilder.withValueSeparator().hasArgs(2).withArgName(\"property=value\")\n        .withLongOpt(OPTION_HIVECONF)\n        .withDescription(\"Use value for given property. Overridden by explicit parameters\")\n        .create());\n\n    options.addOption(OptionBuilder.hasArg().withArgName(\"b\")\n        .withLongOpt(OPTION_SLIDER_AM_CONTAINER_MB)\n        .withDescription(\"The size of the slider AppMaster container in MB\").create('b'));\n\n    options.addOption(OptionBuilder.hasArg().withArgName(OPTION_IO_THREADS)\n        .withLongOpt(OPTION_IO_THREADS).withDescription(\"executor per instance\").create('t'));\n\n    // [-H|--help]\n    options.addOption(new Option(\"H\", \"help\", false, \"Print help information\"));\n  }"
        ]
    ],
    "858ce8c22b65057e8cb3d9d87cb9b9dfb0f8666c": [
        [
            "CliConfigs::SparkOnYarnCliConfig::SparkOnYarnCliConfig()",
            " 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  ",
            "    public SparkOnYarnCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"miniSparkOnYarn.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/spark\");\n        setLogDir(\"itests/qtest-spark/target/qfile-results/clientpositive/spark\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"data/conf/spark/yarn-client\");\n        setClusterType(MiniClusterType.miniSparkOnYarn);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }",
            " 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482 +\n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  ",
            "    public SparkOnYarnCliConfig() {\n      super(CoreCliDriver.class);\n      try {\n        setQueryDir(\"ql/src/test/queries/clientpositive\");\n\n        includesFrom(testConfigProps, \"miniSparkOnYarn.query.files\");\n        includesFrom(testConfigProps, \"spark.only.query.files\");\n\n        setResultsDir(\"ql/src/test/results/clientpositive/spark\");\n        setLogDir(\"itests/qtest-spark/target/qfile-results/clientpositive/spark\");\n\n        setInitScript(\"q_test_init.sql\");\n        setCleanupScript(\"q_test_cleanup.sql\");\n\n        setHiveConfDir(\"data/conf/spark/yarn-client\");\n        setClusterType(MiniClusterType.miniSparkOnYarn);\n      } catch (Exception e) {\n        throw new RuntimeException(\"can't construct cliconfig\", e);\n      }\n    }"
        ],
        [
            "Vectorizer::vectorizeOperator(Operator,VectorizationContext,boolean)",
            "2436  \n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444  \n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504  \n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  ",
            "  Operator<? extends OperatorDesc> vectorizeOperator(Operator<? extends OperatorDesc> op,\n      VectorizationContext vContext, boolean isTez) throws HiveException {\n    Operator<? extends OperatorDesc> vectorOp = null;\n\n    switch (op.getType()) {\n      case MAPJOIN:\n        {\n          MapJoinDesc desc = (MapJoinDesc) op.getConf();\n          boolean specialize = canSpecializeMapJoin(op, desc, isTez || isSpark);\n\n          if (!specialize) {\n\n            Class<? extends Operator<?>> opClass = null;\n            if (op instanceof MapJoinOperator) {\n\n              // *NON-NATIVE* vector map differences for LEFT OUTER JOIN and Filtered...\n\n              List<ExprNodeDesc> bigTableFilters = desc.getFilters().get((byte) desc.getPosBigTable());\n              boolean isOuterAndFiltered = (!desc.isNoOuterJoin() && bigTableFilters.size() > 0);\n              if (!isOuterAndFiltered) {\n                opClass = VectorMapJoinOperator.class;\n              } else {\n                opClass = VectorMapJoinOuterFilteredOperator.class;\n              }\n            } else if (op instanceof SMBMapJoinOperator) {\n              opClass = VectorSMBMapJoinOperator.class;\n            }\n\n            vectorOp = OperatorFactory.getVectorOperator(\n                opClass, op.getCompilationOpContext(), op.getConf(), vContext);\n\n          } else {\n\n            // TEMPORARY Until Native Vector Map Join with Hybrid passes tests...\n            // HiveConf.setBoolVar(physicalContext.getConf(),\n            //    HiveConf.ConfVars.HIVEUSEHYBRIDGRACEHASHJOIN, false);\n\n            vectorOp = specializeMapJoinOperator(op, vContext, desc);\n          }\n        }\n        break;\n\n      case REDUCESINK:\n        {\n          VectorReduceSinkInfo vectorReduceSinkInfo = new VectorReduceSinkInfo();\n          ReduceSinkDesc desc = (ReduceSinkDesc) op.getConf();\n          boolean specialize = canSpecializeReduceSink(desc, isTez, vContext, vectorReduceSinkInfo);\n\n          if (!specialize) {\n\n            vectorOp = OperatorFactory.getVectorOperator(\n                op.getCompilationOpContext(), op.getConf(), vContext);\n\n          } else {\n\n            vectorOp = specializeReduceSinkOperator(op, vContext, desc, vectorReduceSinkInfo);\n\n          }\n        }\n        break;\n      case GROUPBY:\n      case FILTER:\n      case SELECT:\n      case FILESINK:\n      case LIMIT:\n      case EXTRACT:\n      case EVENT:\n      case HASHTABLESINK:\n        vectorOp = OperatorFactory.getVectorOperator(\n            op.getCompilationOpContext(), op.getConf(), vContext);\n        break;\n      default:\n        vectorOp = op;\n        break;\n    }\n\n    LOG.debug(\"vectorizeOperator \" + (vectorOp == null ? \"NULL\" : vectorOp.getClass().getName()));\n    LOG.debug(\"vectorizeOperator \" + (vectorOp == null || vectorOp.getConf() == null ? \"NULL\" : vectorOp.getConf().getClass().getName()));\n\n    if (vectorOp != op) {\n      fixupParentChildOperators(op, vectorOp);\n      ((AbstractOperatorDesc) vectorOp.getConf()).setVectorMode(true);\n    }\n    return vectorOp;\n  }",
            "2436  \n2437  \n2438  \n2439  \n2440  \n2441  \n2442  \n2443  \n2444  \n2445  \n2446  \n2447  \n2448  \n2449  \n2450  \n2451  \n2452  \n2453  \n2454  \n2455  \n2456  \n2457  \n2458  \n2459  \n2460  \n2461  \n2462  \n2463  \n2464  \n2465  \n2466  \n2467  \n2468  \n2469  \n2470  \n2471  \n2472  \n2473  \n2474  \n2475  \n2476  \n2477  \n2478  \n2479  \n2480  \n2481  \n2482  \n2483  \n2484  \n2485  \n2486  \n2487  \n2488  \n2489  \n2490  \n2491  \n2492  \n2493  \n2494  \n2495  \n2496  \n2497  \n2498  \n2499  \n2500  \n2501  \n2502  \n2503  \n2504 +\n2505  \n2506  \n2507  \n2508  \n2509  \n2510  \n2511  \n2512  \n2513  \n2514  \n2515  \n2516  \n2517  \n2518  \n2519  \n2520  \n2521  ",
            "  Operator<? extends OperatorDesc> vectorizeOperator(Operator<? extends OperatorDesc> op,\n      VectorizationContext vContext, boolean isTez) throws HiveException {\n    Operator<? extends OperatorDesc> vectorOp = null;\n\n    switch (op.getType()) {\n      case MAPJOIN:\n        {\n          MapJoinDesc desc = (MapJoinDesc) op.getConf();\n          boolean specialize = canSpecializeMapJoin(op, desc, isTez || isSpark);\n\n          if (!specialize) {\n\n            Class<? extends Operator<?>> opClass = null;\n            if (op instanceof MapJoinOperator) {\n\n              // *NON-NATIVE* vector map differences for LEFT OUTER JOIN and Filtered...\n\n              List<ExprNodeDesc> bigTableFilters = desc.getFilters().get((byte) desc.getPosBigTable());\n              boolean isOuterAndFiltered = (!desc.isNoOuterJoin() && bigTableFilters.size() > 0);\n              if (!isOuterAndFiltered) {\n                opClass = VectorMapJoinOperator.class;\n              } else {\n                opClass = VectorMapJoinOuterFilteredOperator.class;\n              }\n            } else if (op instanceof SMBMapJoinOperator) {\n              opClass = VectorSMBMapJoinOperator.class;\n            }\n\n            vectorOp = OperatorFactory.getVectorOperator(\n                opClass, op.getCompilationOpContext(), op.getConf(), vContext);\n\n          } else {\n\n            // TEMPORARY Until Native Vector Map Join with Hybrid passes tests...\n            // HiveConf.setBoolVar(physicalContext.getConf(),\n            //    HiveConf.ConfVars.HIVEUSEHYBRIDGRACEHASHJOIN, false);\n\n            vectorOp = specializeMapJoinOperator(op, vContext, desc);\n          }\n        }\n        break;\n\n      case REDUCESINK:\n        {\n          VectorReduceSinkInfo vectorReduceSinkInfo = new VectorReduceSinkInfo();\n          ReduceSinkDesc desc = (ReduceSinkDesc) op.getConf();\n          boolean specialize = canSpecializeReduceSink(desc, isTez, vContext, vectorReduceSinkInfo);\n\n          if (!specialize) {\n\n            vectorOp = OperatorFactory.getVectorOperator(\n                op.getCompilationOpContext(), op.getConf(), vContext);\n\n          } else {\n\n            vectorOp = specializeReduceSinkOperator(op, vContext, desc, vectorReduceSinkInfo);\n\n          }\n        }\n        break;\n      case GROUPBY:\n      case FILTER:\n      case SELECT:\n      case FILESINK:\n      case LIMIT:\n      case EXTRACT:\n      case EVENT:\n      case HASHTABLESINK:\n      case SPARKPRUNINGSINK:\n        vectorOp = OperatorFactory.getVectorOperator(\n            op.getCompilationOpContext(), op.getConf(), vContext);\n        break;\n      default:\n        vectorOp = op;\n        break;\n    }\n\n    LOG.debug(\"vectorizeOperator \" + (vectorOp == null ? \"NULL\" : vectorOp.getClass().getName()));\n    LOG.debug(\"vectorizeOperator \" + (vectorOp == null || vectorOp.getConf() == null ? \"NULL\" : vectorOp.getConf().getClass().getName()));\n\n    if (vectorOp != op) {\n      fixupParentChildOperators(op, vectorOp);\n      ((AbstractOperatorDesc) vectorOp.getConf()).setVectorMode(true);\n    }\n    return vectorOp;\n  }"
        ]
    ],
    "fc08e3be585f6c152895e88c626012e9e73017a9": [
        [
            "GenericUDAFBloomFilter::GenericUDAFBloomFilterEvaluator::getExpectedEntries()",
            " 247  \n 248  \n 249 -\n 250  \n 251 -\n 252  ",
            "    public long getExpectedEntries() {\n      if (sourceOperator != null && sourceOperator.getStatistics() != null) {\n        return sourceOperator.getStatistics().getNumRows();\n      }\n      return -1;\n    }",
            " 251  \n 252 +\n 253  \n 254 +\n 255 +\n 256 +\n 257 +\n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267 +\n 268 +\n 269 +\n 270 +\n 271 +\n 272 +\n 273  \n 274 +\n 275 +\n 276  ",
            "    public long getExpectedEntries() {\n      long expectedEntries = -1;\n      if (sourceOperator != null && sourceOperator.getStatistics() != null) {\n        Statistics stats = sourceOperator.getStatistics();\n        expectedEntries = stats.getNumRows();\n\n        // Use NumDistinctValues if possible\n        switch (stats.getColumnStatsState()) {\n          case COMPLETE:\n          case PARTIAL:\n            // There should only be column stats for one column, use if that is the case.\n            List<ColStatistics> colStats = stats.getColumnStats();\n            if (colStats.size() == 1) {\n              long ndv = colStats.get(0).getCountDistint();\n              if (ndv > 0) {\n                expectedEntries = ndv;\n              }\n            }\n            break;\n          default:\n            break;\n        }\n      }\n\n      return expectedEntries;\n    }"
        ]
    ],
    "dc0938c42f6c9a42adb3fcbb391fb759a3bb0072": [
        [
            "Utils::getSplitLocationProvider(Configuration,Logger)",
            "  34  \n  35  \n  36  \n  37 -\n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  ",
            "  public static SplitLocationProvider getSplitLocationProvider(Configuration conf, Logger LOG) throws\n      IOException {\n    boolean useCustomLocations =\n        HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS);\n    SplitLocationProvider splitLocationProvider;\n    LOG.info(\"SplitGenerator using llap affinitized locations: \" + useCustomLocations);\n    if (useCustomLocations) {\n      LlapRegistryService serviceRegistry = LlapRegistryService.getClient(conf);\n      LOG.info(\"Using LLAP instance \" + serviceRegistry.getApplicationId());\n\n      Collection<ServiceInstance> serviceInstances =\n          serviceRegistry.getInstances().getAllInstancesOrdered(true);\n      ArrayList<String> locations = new ArrayList<>(serviceInstances.size());\n      for (ServiceInstance serviceInstance : serviceInstances) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding \" + serviceInstance.getWorkerIdentity() + \" with hostname=\" +\n              serviceInstance.getHost() + \" to list for split locations\");\n        }\n        locations.add(serviceInstance.getHost());\n      }\n      splitLocationProvider = new HostAffinitySplitLocationProvider(locations);\n    } else {\n      splitLocationProvider = new SplitLocationProvider() {\n        @Override\n        public String[] getLocations(InputSplit split) throws IOException {\n          if (split == null) {\n            return null;\n          }\n          String[] locations = split.getLocations();\n          if (locations != null && locations.length == 1) {\n            if (\"localhost\".equals(locations[0])) {\n              return ArrayUtils.EMPTY_STRING_ARRAY;\n            }\n          }\n          return locations;\n        }\n      };\n    }\n    return splitLocationProvider;\n  }",
            "  35  \n  36  \n  37  \n  38 +\n  39 +\n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48 +\n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  ",
            "  public static SplitLocationProvider getSplitLocationProvider(Configuration conf, Logger LOG) throws\n      IOException {\n    boolean useCustomLocations =\n        HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_MODE).equals(\"llap\")\n        && HiveConf.getBoolVar(conf, HiveConf.ConfVars.LLAP_CLIENT_CONSISTENT_SPLITS);\n    SplitLocationProvider splitLocationProvider;\n    LOG.info(\"SplitGenerator using llap affinitized locations: \" + useCustomLocations);\n    if (useCustomLocations) {\n      LlapRegistryService serviceRegistry = LlapRegistryService.getClient(conf);\n      LOG.info(\"Using LLAP instance \" + serviceRegistry.getApplicationId());\n\n      Collection<ServiceInstance> serviceInstances =\n          serviceRegistry.getInstances().getAllInstancesOrdered(true);\n      Preconditions.checkArgument(!serviceInstances.isEmpty(),\n          \"No running LLAP daemons! Please check LLAP service status and zookeeper configuration\");\n      ArrayList<String> locations = new ArrayList<>(serviceInstances.size());\n      for (ServiceInstance serviceInstance : serviceInstances) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding \" + serviceInstance.getWorkerIdentity() + \" with hostname=\" +\n              serviceInstance.getHost() + \" to list for split locations\");\n        }\n        locations.add(serviceInstance.getHost());\n      }\n      splitLocationProvider = new HostAffinitySplitLocationProvider(locations);\n    } else {\n      splitLocationProvider = new SplitLocationProvider() {\n        @Override\n        public String[] getLocations(InputSplit split) throws IOException {\n          if (split == null) {\n            return null;\n          }\n          String[] locations = split.getLocations();\n          if (locations != null && locations.length == 1) {\n            if (\"localhost\".equals(locations[0])) {\n              return ArrayUtils.EMPTY_STRING_ARRAY;\n            }\n          }\n          return locations;\n        }\n      };\n    }\n    return splitLocationProvider;\n  }"
        ]
    ],
    "301ebbb99f91ba60c1922a1cb2b07e2977cbfe15": [
        [
            "TaskRunnerCallable::TaskRunnerCallable(SubmitWorkRequestProto,QueryFragmentInfo,Configuration,ExecutionContext,Map,Credentials,long,AMReporter,ConfParams,LlapDaemonExecutorMetrics,KilledTaskHandler,FragmentCompletionHandler,HadoopShim,TezTaskAttemptID,SignableVertexSpec,TezEvent,UserGroupInformation,SchedulerFragmentCompletingListener,SocketFactory)",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 -\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "  @VisibleForTesting\n  public TaskRunnerCallable(SubmitWorkRequestProto request, QueryFragmentInfo fragmentInfo,\n                            Configuration conf, ExecutionContext executionContext, Map<String, String> envMap,\n                            Credentials credentials, long memoryAvailable, AMReporter amReporter, ConfParams confParams,\n                            LlapDaemonExecutorMetrics metrics, KilledTaskHandler killedTaskHandler,\n                            FragmentCompletionHandler fragmentCompleteHandler, HadoopShim tezHadoopShim,\n                            TezTaskAttemptID attemptId, SignableVertexSpec vertex, TezEvent initialEvent,\n                            UserGroupInformation fsTaskUgi, SchedulerFragmentCompletingListener completionListener,\n                            SocketFactory socketFactory) {\n    this.request = request;\n    this.fragmentInfo = fragmentInfo;\n    this.conf = conf;\n    this.executionContext = executionContext;\n    this.envMap = envMap;\n    this.objectRegistry = new ObjectRegistryImpl();\n    this.credentials = credentials;\n    this.memoryAvailable = memoryAvailable;\n    this.confParams = confParams;\n    this.jobToken = TokenCache.getSessionToken(credentials);\n    this.vertex = vertex;\n    this.taskSpec = Converters.getTaskSpecfromProto(\n        vertex, request.getFragmentNumber(), request.getAttemptNumber(), attemptId);\n    this.amReporter = amReporter;\n    // Register with the AMReporter when the callable is setup. Unregister once it starts running.\n    if (amReporter != null && jobToken != null) {\n      this.amReporter.registerTask(request.getAmHost(), request.getAmPort(),\n          vertex.getUser(), jobToken, fragmentInfo.getQueryInfo().getQueryIdentifier(), attemptId);\n    }\n    this.metrics = metrics;\n    this.requestId = taskSpec.getTaskAttemptID().toString();\n    threadNameSuffix = constructThreadNameSuffix(taskSpec.getTaskAttemptID());\n\n    this.queryId = ContainerRunnerImpl\n        .constructUniqueQueryId(vertex.getHiveQueryId(),\n            fragmentInfo.getQueryInfo().getDagIdentifier());\n    this.killedTaskHandler = killedTaskHandler;\n    this.fragmentCompletionHanler = fragmentCompleteHandler;\n    this.tezHadoopShim = tezHadoopShim;\n    this.initialEvent = initialEvent;\n    this.fsTaskUgi = fsTaskUgi;\n    this.completionListener = completionListener;\n    this.socketFactory = socketFactory;\n  }",
            " 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153 +\n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  ",
            "  @VisibleForTesting\n  public TaskRunnerCallable(SubmitWorkRequestProto request, QueryFragmentInfo fragmentInfo,\n                            Configuration conf, ExecutionContext executionContext, Map<String, String> envMap,\n                            Credentials credentials, long memoryAvailable, AMReporter amReporter, ConfParams confParams,\n                            LlapDaemonExecutorMetrics metrics, KilledTaskHandler killedTaskHandler,\n                            FragmentCompletionHandler fragmentCompleteHandler, HadoopShim tezHadoopShim,\n                            TezTaskAttemptID attemptId, SignableVertexSpec vertex, TezEvent initialEvent,\n                            UserGroupInformation fsTaskUgi, SchedulerFragmentCompletingListener completionListener,\n                            SocketFactory socketFactory) {\n    this.request = request;\n    this.fragmentInfo = fragmentInfo;\n    this.conf = conf;\n    this.executionContext = executionContext;\n    this.envMap = envMap;\n    this.objectRegistry = new ObjectRegistryImpl();\n    this.credentials = credentials;\n    this.memoryAvailable = memoryAvailable;\n    this.confParams = confParams;\n    this.jobToken = TokenCache.getSessionToken(credentials);\n    this.vertex = vertex;\n    this.taskSpec = Converters.getTaskSpecfromProto(\n        vertex, request.getFragmentNumber(), request.getAttemptNumber(), attemptId);\n    this.amReporter = amReporter;\n    // Register with the AMReporter when the callable is setup. Unregister once it starts running.\n    if (amReporter != null && jobToken != null) {\n      this.amReporter.registerTask(request.getAmHost(), request.getAmPort(),\n          vertex.getTokenIdentifier(), jobToken, fragmentInfo.getQueryInfo().getQueryIdentifier(), attemptId);\n    }\n    this.metrics = metrics;\n    this.requestId = taskSpec.getTaskAttemptID().toString();\n    threadNameSuffix = constructThreadNameSuffix(taskSpec.getTaskAttemptID());\n\n    this.queryId = ContainerRunnerImpl\n        .constructUniqueQueryId(vertex.getHiveQueryId(),\n            fragmentInfo.getQueryInfo().getDagIdentifier());\n    this.killedTaskHandler = killedTaskHandler;\n    this.fragmentCompletionHanler = fragmentCompleteHandler;\n    this.tezHadoopShim = tezHadoopShim;\n    this.initialEvent = initialEvent;\n    this.fsTaskUgi = fsTaskUgi;\n    this.completionListener = completionListener;\n    this.socketFactory = socketFactory;\n  }"
        ],
        [
            "ContainerRunnerImpl::KilledTaskHandlerImpl::taskKilled(String,int,String,Token,QueryIdentifier,TezTaskAttemptID)",
            " 510  \n 511 -\n 512  \n 513  \n 514 -\n 515  ",
            "    @Override\n    public void taskKilled(String amLocation, int port, String user,\n                           Token<JobTokenIdentifier> jobToken, QueryIdentifier queryIdentifier,\n                           TezTaskAttemptID taskAttemptId) {\n      amReporter.taskKilled(amLocation, port, user, jobToken, queryIdentifier, taskAttemptId);\n    }",
            " 510  \n 511 +\n 512  \n 513  \n 514 +\n 515  ",
            "    @Override\n    public void taskKilled(String amLocation, int port, String umbilicalUser,\n                           Token<JobTokenIdentifier> jobToken, QueryIdentifier queryIdentifier,\n                           TezTaskAttemptID taskAttemptId) {\n      amReporter.taskKilled(amLocation, port, umbilicalUser, jobToken, queryIdentifier, taskAttemptId);\n    }"
        ],
        [
            "TaskRunnerCallable::reportTaskKilled()",
            " 375  \n 376  \n 377  \n 378  \n 379  \n 380 -\n 381  \n 382  ",
            "  /**\n   * Inform the AM that this task has been killed.\n   */\n  public void reportTaskKilled() {\n    killedTaskHandler\n        .taskKilled(request.getAmHost(), request.getAmPort(), vertex.getUser(), jobToken,\n            fragmentInfo.getQueryInfo().getQueryIdentifier(), taskSpec.getTaskAttemptID());\n  }",
            " 375  \n 376  \n 377  \n 378  \n 379  \n 380 +\n 381  \n 382  ",
            "  /**\n   * Inform the AM that this task has been killed.\n   */\n  public void reportTaskKilled() {\n    killedTaskHandler\n        .taskKilled(request.getAmHost(), request.getAmPort(), vertex.getTokenIdentifier(), jobToken,\n            fragmentInfo.getQueryInfo().getQueryIdentifier(), taskSpec.getTaskAttemptID());\n  }"
        ],
        [
            "AMReporter::AMNodeInfo::AMNodeInfo(LlapNodeId,String,Token,QueryIdentifier,RetryPolicy,long,SocketFactory,Configuration)",
            " 441 -\n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 -\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  ",
            "    public AMNodeInfo(LlapNodeId amNodeId, String user,\n                      Token<JobTokenIdentifier> jobToken,\n                      QueryIdentifier currentQueryIdentifier,\n                      RetryPolicy retryPolicy,\n                      long timeout,\n                      SocketFactory socketFactory,\n                      Configuration conf) {\n      this.user = user;\n      this.jobToken = jobToken;\n      this.queryIdentifier = currentQueryIdentifier;\n      this.retryPolicy = retryPolicy;\n      this.timeout = timeout;\n      this.socketFactory = socketFactory;\n      this.conf = conf;\n      this.amNodeId = amNodeId;\n    }",
            " 441 +\n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448 +\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  ",
            "    public AMNodeInfo(LlapNodeId amNodeId, String umbilicalUser,\n                      Token<JobTokenIdentifier> jobToken,\n                      QueryIdentifier currentQueryIdentifier,\n                      RetryPolicy retryPolicy,\n                      long timeout,\n                      SocketFactory socketFactory,\n                      Configuration conf) {\n      this.umbilicalUser = umbilicalUser;\n      this.jobToken = jobToken;\n      this.queryIdentifier = currentQueryIdentifier;\n      this.retryPolicy = retryPolicy;\n      this.timeout = timeout;\n      this.socketFactory = socketFactory;\n      this.conf = conf;\n      this.amNodeId = amNodeId;\n    }"
        ],
        [
            "AMReporter::taskKilled(String,int,String,Token,QueryIdentifier,TezTaskAttemptID)",
            " 247 -\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 -\n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "  public void taskKilled(String amLocation, int port, String user, Token<JobTokenIdentifier> jobToken,\n                         final QueryIdentifier queryIdentifier, final TezTaskAttemptID taskAttemptId) {\n    LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);\n    AMNodeInfo amNodeInfo;\n    synchronized (knownAppMasters) {\n      amNodeInfo = knownAppMasters.get(queryIdentifier);\n      if (amNodeInfo == null) {\n        amNodeInfo = new AMNodeInfo(amNodeId, user, jobToken, queryIdentifier, retryPolicy, retryTimeout, socketFactory,\n          conf);\n      }\n    }\n\n    // Even if the service hasn't started up. It's OK to make this invocation since this will\n    // only happen after the AtomicReference address has been populated. Not adding an additional check.\n    ListenableFuture<Void> future =\n        executor.submit(new KillTaskCallable(taskAttemptId, amNodeInfo));\n    Futures.addCallback(future, new FutureCallback<Void>() {\n      @Override\n      public void onSuccess(Void result) {\n        LOG.info(\"Sent taskKilled for {}\", taskAttemptId);\n      }\n\n      @Override\n      public void onFailure(Throwable t) {\n        LOG.warn(\"Failed to send taskKilled for {}. The attempt will likely time out.\",\n            taskAttemptId);\n      }\n    });\n  }",
            " 247 +\n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254 +\n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  ",
            "  public void taskKilled(String amLocation, int port, String umbilicalUser, Token<JobTokenIdentifier> jobToken,\n                         final QueryIdentifier queryIdentifier, final TezTaskAttemptID taskAttemptId) {\n    LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);\n    AMNodeInfo amNodeInfo;\n    synchronized (knownAppMasters) {\n      amNodeInfo = knownAppMasters.get(queryIdentifier);\n      if (amNodeInfo == null) {\n        amNodeInfo = new AMNodeInfo(amNodeId, umbilicalUser, jobToken, queryIdentifier, retryPolicy, retryTimeout, socketFactory,\n          conf);\n      }\n    }\n\n    // Even if the service hasn't started up. It's OK to make this invocation since this will\n    // only happen after the AtomicReference address has been populated. Not adding an additional check.\n    ListenableFuture<Void> future =\n        executor.submit(new KillTaskCallable(taskAttemptId, amNodeInfo));\n    Futures.addCallback(future, new FutureCallback<Void>() {\n      @Override\n      public void onSuccess(Void result) {\n        LOG.info(\"Sent taskKilled for {}\", taskAttemptId);\n      }\n\n      @Override\n      public void onFailure(Throwable t) {\n        LOG.warn(\"Failed to send taskKilled for {}. The attempt will likely time out.\",\n            taskAttemptId);\n      }\n    });\n  }"
        ],
        [
            "AMReporter::registerTask(String,int,String,Token,QueryIdentifier,TezTaskAttemptID)",
            " 197 -\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 -\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  public void registerTask(String amLocation, int port, String user,\n      Token<JobTokenIdentifier> jobToken, QueryIdentifier queryIdentifier,\n      TezTaskAttemptID attemptId) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\n          \"Registering for heartbeat: {}, queryIdentifier={}, attemptId={}\",\n          (amLocation + \":\" + port), queryIdentifier, attemptId);\n    }\n    AMNodeInfo amNodeInfo;\n\n    // Since we don't have an explicit AM end signal yet - we're going to create\n    // and discard AMNodeInfo instances per query.\n    synchronized (knownAppMasters) {\n      LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);\n      amNodeInfo = knownAppMasters.get(queryIdentifier);\n      if (amNodeInfo == null) {\n        amNodeInfo = new AMNodeInfo(amNodeId, user, jobToken, queryIdentifier,\n            retryPolicy, retryTimeout, socketFactory, conf);\n        knownAppMasters.put(queryIdentifier, amNodeInfo);\n        // Add to the queue only the first time this is registered, and on\n        // subsequent instances when it's taken off the queue.\n        amNodeInfo.setNextHeartbeatTime(System.currentTimeMillis() + heartbeatInterval);\n        pendingHeartbeatQueeu.add(amNodeInfo);\n        // AMNodeInfo will only be cleared when a queryComplete is received for this query, or\n        // when we detect a failure on the AM side (failure to heartbeat).\n        // A single queueLookupCallable is added here. We have to make sure one instance stays\n        // in the queue till the query completes.\n      }\n      amNodeInfo.addTaskAttempt(attemptId);\n    }\n  }",
            " 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "  public void registerTask(String amLocation, int port, String umbilicalUser,\n      Token<JobTokenIdentifier> jobToken, QueryIdentifier queryIdentifier,\n      TezTaskAttemptID attemptId) {\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\n          \"Registering for heartbeat: {}, queryIdentifier={}, attemptId={}\",\n          (amLocation + \":\" + port), queryIdentifier, attemptId);\n    }\n    AMNodeInfo amNodeInfo;\n\n    // Since we don't have an explicit AM end signal yet - we're going to create\n    // and discard AMNodeInfo instances per query.\n    synchronized (knownAppMasters) {\n      LlapNodeId amNodeId = LlapNodeId.getInstance(amLocation, port);\n      amNodeInfo = knownAppMasters.get(queryIdentifier);\n      if (amNodeInfo == null) {\n        amNodeInfo = new AMNodeInfo(amNodeId, umbilicalUser, jobToken, queryIdentifier,\n            retryPolicy, retryTimeout, socketFactory, conf);\n        knownAppMasters.put(queryIdentifier, amNodeInfo);\n        // Add to the queue only the first time this is registered, and on\n        // subsequent instances when it's taken off the queue.\n        amNodeInfo.setNextHeartbeatTime(System.currentTimeMillis() + heartbeatInterval);\n        pendingHeartbeatQueeu.add(amNodeInfo);\n        // AMNodeInfo will only be cleared when a queryComplete is received for this query, or\n        // when we detect a failure on the AM side (failure to heartbeat).\n        // A single queueLookupCallable is added here. We have to make sure one instance stays\n        // in the queue till the query completes.\n      }\n      amNodeInfo.addTaskAttempt(attemptId);\n    }\n  }"
        ],
        [
            "AMReporter::AMNodeInfo::getUmbilical()",
            " 458  \n 459  \n 460  \n 461  \n 462  \n 463 -\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  ",
            "    synchronized LlapTaskUmbilicalProtocol getUmbilical() throws IOException, InterruptedException {\n      if (umbilical == null) {\n        final InetSocketAddress address =\n            NetUtils.createSocketAddrForHost(amNodeId.getHostname(), amNodeId.getPort());\n        SecurityUtil.setTokenService(this.jobToken, address);\n        UserGroupInformation ugi = UserGroupInformation.createRemoteUser(user);\n        ugi.addToken(jobToken);\n        umbilical = ugi.doAs(new PrivilegedExceptionAction<LlapTaskUmbilicalProtocol>() {\n          @Override\n          public LlapTaskUmbilicalProtocol run() throws Exception {\n            return RPC\n                .getProxy(LlapTaskUmbilicalProtocol.class, LlapTaskUmbilicalProtocol.versionID,\n                    address, UserGroupInformation.getCurrentUser(), conf, socketFactory,\n                    (int) timeout);\n          }\n        });\n      }\n      return umbilical;\n    }",
            " 458  \n 459  \n 460  \n 461  \n 462  \n 463 +\n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  ",
            "    synchronized LlapTaskUmbilicalProtocol getUmbilical() throws IOException, InterruptedException {\n      if (umbilical == null) {\n        final InetSocketAddress address =\n            NetUtils.createSocketAddrForHost(amNodeId.getHostname(), amNodeId.getPort());\n        SecurityUtil.setTokenService(this.jobToken, address);\n        UserGroupInformation ugi = UserGroupInformation.createRemoteUser(umbilicalUser);\n        ugi.addToken(jobToken);\n        umbilical = ugi.doAs(new PrivilegedExceptionAction<LlapTaskUmbilicalProtocol>() {\n          @Override\n          public LlapTaskUmbilicalProtocol run() throws Exception {\n            return RPC\n                .getProxy(LlapTaskUmbilicalProtocol.class, LlapTaskUmbilicalProtocol.versionID,\n                    address, UserGroupInformation.getCurrentUser(), conf, socketFactory,\n                    (int) timeout);\n          }\n        });\n      }\n      return umbilical;\n    }"
        ]
    ],
    "3c1dfe379ec1fcaa688b064561a4daca098ed2b8": [
        [
            "ExprNodeConstantDesc::ExprNodeConstantDesc(TypeInfo,Object)",
            "  68  \n  69  \n  70 -\n  71  ",
            "  public ExprNodeConstantDesc(TypeInfo typeInfo, Object value) {\n    super(typeInfo);\n    this.value = value;\n  }",
            "  69  \n  70  \n  71 +\n  72  ",
            "  public ExprNodeConstantDesc(TypeInfo typeInfo, Object value) {\n    super(typeInfo);\n    setValue(value);\n  }"
        ],
        [
            "StandardStructObjectInspector::MyField::MyField(int,String,ObjectInspector)",
            "  55  \n  56  \n  57  \n  58 -\n  59  \n  60  ",
            "    public MyField(int fieldID, String fieldName,\n        ObjectInspector fieldObjectInspector) {\n      this.fieldID = fieldID;\n      this.fieldName = fieldName.toLowerCase();\n      this.fieldObjectInspector = fieldObjectInspector;\n    }",
            "  55  \n  56  \n  57  \n  58 +\n  59  \n  60  ",
            "    public MyField(int fieldID, String fieldName,\n        ObjectInspector fieldObjectInspector) {\n      this.fieldID = fieldID;\n      this.fieldName = fieldName.toLowerCase().intern();\n      this.fieldObjectInspector = fieldObjectInspector;\n    }"
        ],
        [
            "AvroSerDe::initialize(Configuration,Properties)",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130 -\n 131  \n 132  \n 133  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    // Reset member variables so we don't get in a half-constructed state\n    if (schema != null) {\n      LOG.debug(\"Resetting already initialized AvroSerDe\");\n    }\n\n    schema = null;\n    oi = null;\n    columnNames = null;\n    columnTypes = null;\n\n    final String columnNameProperty = properties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = properties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n    final String columnCommentProperty = properties.getProperty(LIST_COLUMN_COMMENTS,\"\");\n    final String columnNameDelimiter = properties.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? properties\n        .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);\n        \n    if (hasExternalSchema(properties)\n        || columnNameProperty == null || columnNameProperty.isEmpty()\n        || columnTypeProperty == null || columnTypeProperty.isEmpty()) {\n      schema = determineSchemaOrReturnErrorSchema(configuration, properties);\n    } else {\n      // Get column names and sort order\n      columnNames = Arrays.asList(columnNameProperty.split(columnNameDelimiter));\n      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n      schema = getSchemaFromCols(properties, columnNames, columnTypes, columnCommentProperty);\n      properties.setProperty(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName(), schema.toString());\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Avro schema is \" + schema);\n    }\n\n    if (configuration == null) {\n      LOG.debug(\"Configuration null, not inserting schema\");\n    } else {\n      configuration.set(\n          AvroSerdeUtils.AvroTableProperties.AVRO_SERDE_SCHEMA.getPropName(), schema.toString(false));\n    }\n\n    badSchema = schema.equals(SchemaResolutionProblem.SIGNAL_BAD_SCHEMA);\n\n    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(schema);\n    this.columnNames = aoig.getColumnNames();\n    this.columnTypes = aoig.getColumnTypes();\n    this.oi = aoig.getObjectInspector();\n  }",
            "  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111 +\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  ",
            "  @Override\n  public void initialize(Configuration configuration, Properties properties) throws SerDeException {\n    // Reset member variables so we don't get in a half-constructed state\n    if (schema != null) {\n      LOG.debug(\"Resetting already initialized AvroSerDe\");\n    }\n\n    schema = null;\n    oi = null;\n    columnNames = null;\n    columnTypes = null;\n\n    final String columnNameProperty = properties.getProperty(serdeConstants.LIST_COLUMNS);\n    final String columnTypeProperty = properties.getProperty(serdeConstants.LIST_COLUMN_TYPES);\n    final String columnCommentProperty = properties.getProperty(LIST_COLUMN_COMMENTS,\"\");\n    final String columnNameDelimiter = properties.containsKey(serdeConstants.COLUMN_NAME_DELIMITER) ? properties\n        .getProperty(serdeConstants.COLUMN_NAME_DELIMITER) : String.valueOf(SerDeUtils.COMMA);\n        \n    if (hasExternalSchema(properties)\n        || columnNameProperty == null || columnNameProperty.isEmpty()\n        || columnTypeProperty == null || columnTypeProperty.isEmpty()) {\n      schema = determineSchemaOrReturnErrorSchema(configuration, properties);\n    } else {\n      // Get column names and sort order\n      columnNames = StringInternUtils.internStringsInList(\n          Arrays.asList(columnNameProperty.split(columnNameDelimiter)));\n      columnTypes = TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n\n      schema = getSchemaFromCols(properties, columnNames, columnTypes, columnCommentProperty);\n      properties.setProperty(AvroSerdeUtils.AvroTableProperties.SCHEMA_LITERAL.getPropName(), schema.toString());\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Avro schema is \" + schema);\n    }\n\n    if (configuration == null) {\n      LOG.debug(\"Configuration null, not inserting schema\");\n    } else {\n      configuration.set(\n          AvroSerdeUtils.AvroTableProperties.AVRO_SERDE_SCHEMA.getPropName(), schema.toString(false));\n    }\n\n    badSchema = schema.equals(SchemaResolutionProblem.SIGNAL_BAD_SCHEMA);\n\n    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(schema);\n    this.columnNames = StringInternUtils.internStringsInList(aoig.getColumnNames());\n    this.columnTypes = aoig.getColumnTypes();\n    this.oi = aoig.getObjectInspector();\n  }"
        ],
        [
            "StringInternUtils::internStringsInList(List)",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 -\n 111 -\n 112 -\n 113 -\n 114  \n 115  \n 116  ",
            "  /**\n   * This method interns all the strings in the given list in place. That is,\n   * it iterates over the list, replaces each element with the interned copy\n   * and eventually returns the same list.\n   */\n  public static List<String> internStringsInList(List<String> list) {\n    if (list != null) {\n      ListIterator<String> it = list.listIterator();\n      while (it.hasNext()) {\n        it.set(it.next().intern());\n      }\n    }\n    return list;\n  }",
            " 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121 +\n 122  \n 123  \n 124  ",
            "  /**\n   * This method interns all the strings in the given list in place. That is,\n   * it iterates over the list, replaces each element with the interned copy\n   * and eventually returns the same list.\n   *\n   * Note that the provided List implementation should return an iterator\n   * (via list.listIterator()) method, and that iterator should implement\n   * the set(Object) method. That's what all List implementations in the JDK\n   * provide. However, if some custom List implementation doesn't have this\n   * functionality, this method will return without interning its elements.\n   */\n  public static List<String> internStringsInList(List<String> list) {\n    if (list != null) {\n      try {\n        ListIterator<String> it = list.listIterator();\n        while (it.hasNext()) {\n          it.set(it.next().intern());\n        }\n      } catch (UnsupportedOperationException e) { } // set() not implemented - ignore\n    }\n    return list;\n  }"
        ],
        [
            "ExprNodeConstantDesc::setValue(Object)",
            "  78  \n  79  \n  80  \n  81  ",
            "  public void setValue(Object value) {\n    // Kryo setter\n    this.value = value;\n  }",
            "  79  \n  80  \n  81 +\n  82 +\n  83 +\n  84  \n  85  ",
            "  public void setValue(Object value) {\n    // Kryo setter\n    if (value instanceof String) {\n      value = StringInternUtils.internIfNotNull((String) value);\n    }\n    this.value = value;\n  }"
        ],
        [
            "ColumnInfo::setTypeName(String)",
            " 116  \n 117 -\n 118  ",
            "  public void setTypeName(String typeName) {\n    this.typeName = typeName;\n  }",
            " 117  \n 118 +\n 119  ",
            "  public void setTypeName(String typeName) {\n    this.typeName = StringInternUtils.internIfNotNull(typeName);\n  }"
        ],
        [
            "ObjectInspectorFactory::getStandardStructObjectInspector(List,List,List)",
            " 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321 -\n 322  \n 323  \n 324  \n 325 -\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  ",
            "  public static StandardStructObjectInspector getStandardStructObjectInspector(\n      List<String> structFieldNames,\n      List<ObjectInspector> structFieldObjectInspectors,\n      List<String> structComments) {\n    ArrayList<List<?>> signature = new ArrayList<List<?>>(3);\n    signature.add(structFieldNames);\n    signature.add(structFieldObjectInspectors);\n    if(structComments != null) {\n      signature.add(structComments);\n    }\n    StandardStructObjectInspector result = cachedStandardStructObjectInspector.get(signature);\n    if(result == null) {\n      result = new StandardStructObjectInspector(structFieldNames, structFieldObjectInspectors, structComments);\n      StandardStructObjectInspector prev =\n        cachedStandardStructObjectInspector.putIfAbsent(signature, result);\n      if (prev != null) {\n        result = prev;\n      }\n    }\n    return result;\n  }",
            " 315  \n 316  \n 317  \n 318  \n 319  \n 320 +\n 321  \n 322  \n 323 +\n 324 +\n 325  \n 326  \n 327  \n 328 +\n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  ",
            "  public static StandardStructObjectInspector getStandardStructObjectInspector(\n      List<String> structFieldNames,\n      List<ObjectInspector> structFieldObjectInspectors,\n      List<String> structComments) {\n    ArrayList<List<?>> signature = new ArrayList<List<?>>(3);\n    StringInternUtils.internStringsInList(structFieldNames);\n    signature.add(structFieldNames);\n    signature.add(structFieldObjectInspectors);\n    if (structComments != null) {\n      StringInternUtils.internStringsInList(structComments);\n      signature.add(structComments);\n    }\n    StandardStructObjectInspector result = cachedStandardStructObjectInspector.get(signature);\n    if (result == null) {\n      result = new StandardStructObjectInspector(structFieldNames, structFieldObjectInspectors, structComments);\n      StandardStructObjectInspector prev =\n        cachedStandardStructObjectInspector.putIfAbsent(signature, result);\n      if (prev != null) {\n        result = prev;\n      }\n    }\n    return result;\n  }"
        ],
        [
            "LineageInfo::Dependency::setExpr(String)",
            " 402  \n 403  \n 404  \n 405  \n 406 -\n 407  ",
            "    /**\n     * @param expr the expr to set\n     */\n    public void setExpr(String expr) {\n      this.expr = expr;\n    }",
            " 403  \n 404  \n 405  \n 406  \n 407 +\n 408  ",
            "    /**\n     * @param expr the expr to set\n     */\n    public void setExpr(String expr) {\n      this.expr = StringInternUtils.internIfNotNull(expr);\n    }"
        ],
        [
            "TableDesc::setProperties(Properties)",
            " 131  \n 132  \n 133  ",
            "  public void setProperties(final Properties properties) {\n    this.properties = properties;\n  }",
            " 132  \n 133 +\n 134  \n 135  ",
            "  public void setProperties(final Properties properties) {\n    StringInternUtils.internValuesInMap((Map) properties);\n    this.properties = properties;\n  }"
        ],
        [
            "TableDesc::TableDesc(Class,Class,Properties)",
            "  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70 -\n  71  ",
            "  /**\n   * @param inputFormatClass\n   * @param outputFormatClass\n   * @param properties must contain serde class name associate with this table.\n   */\n  public TableDesc(\n      final Class<? extends InputFormat> inputFormatClass,\n      final Class<?> outputFormatClass, final Properties properties) {\n    this.inputFileFormatClass = inputFormatClass;\n    outputFileFormatClass = HiveFileFormatUtils\n        .getOutputFormatSubstitute(outputFormatClass);\n    this.properties = properties;\n  }",
            "  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71 +\n  72  ",
            "  /**\n   * @param inputFormatClass\n   * @param outputFormatClass\n   * @param properties must contain serde class name associate with this table.\n   */\n  public TableDesc(\n      final Class<? extends InputFormat> inputFormatClass,\n      final Class<?> outputFormatClass, final Properties properties) {\n    this.inputFileFormatClass = inputFormatClass;\n    outputFileFormatClass = HiveFileFormatUtils\n        .getOutputFormatSubstitute(outputFormatClass);\n    setProperties(properties);\n  }"
        ],
        [
            "ColumnInfo::setAlias(String)",
            " 162  \n 163 -\n 164  ",
            "  public void setAlias(String col_alias) {\n    alias = col_alias;\n  }",
            " 163  \n 164 +\n 165  ",
            "  public void setAlias(String col_alias) {\n    alias = StringInternUtils.internIfNotNull(col_alias);\n  }"
        ],
        [
            "ColumnInfo::ColumnInfo(String,ObjectInspector,String,boolean,boolean)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  ",
            "  public ColumnInfo(String internalName, ObjectInspector objectInspector,\n      String tabAlias, boolean isVirtualCol, boolean isHiddenVirtualCol) {\n    this.internalName = internalName;\n    this.objectInspector = objectInspector;\n    this.tabAlias = tabAlias;\n    this.isVirtualCol = isVirtualCol;\n    this.isHiddenVirtualCol = isHiddenVirtualCol;\n    this.typeName = getType().getTypeName();\n  }",
            "  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101  ",
            "  public ColumnInfo(String internalName, ObjectInspector objectInspector,\n      String tabAlias, boolean isVirtualCol, boolean isHiddenVirtualCol) {\n    this.internalName = internalName;\n    this.objectInspector = objectInspector;\n    this.tabAlias = tabAlias;\n    this.isVirtualCol = isVirtualCol;\n    this.isHiddenVirtualCol = isHiddenVirtualCol;\n    setTypeName(getType().getTypeName());\n  }"
        ]
    ],
    "f0812efdb82cb42a50bfe390413f6413d0ed5ab2": [
        [
            "TestSchemaTool::testValidateSchemaTables()",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129 -\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "  /**\n   * Test to validate that all tables exist in the HMS metastore.\n   * @throws Exception\n   */\n  public void testValidateSchemaTables() throws Exception {\n    schemaTool.doInit(\"2.0.0\");\n\n    boolean isValid = (boolean)schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n\n    // upgrade to 2.2.0 schema and re-validate\n    schemaTool.doUpgrade(\"2.2.0\");\n    isValid = (boolean)schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n\n    // Simulate a missing table scenario by renaming a couple of tables\n    String[] scripts = new String[] {\n        \"RENAME TABLE SEQUENCE_TABLE to SEQUENCE_TABLE_RENAMED\",\n        \"RENAME TABLE NUCLEUS_TABLES to NUCLEUS_TABLES_RENAMED\"\n    };\n\n    File scriptFile = generateTestScript(scripts);\n    schemaTool.runBeeLine(scriptFile.getPath());\n    isValid = schemaTool.validateSchemaTables(conn);\n    assertFalse(isValid);\n\n    // Restored the renamed tables\n    scripts = new String[] {\n        \"RENAME TABLE SEQUENCE_TABLE_RENAMED to SEQUENCE_TABLE\",\n        \"RENAME TABLE NUCLEUS_TABLES_RENAMED to NUCLEUS_TABLES\"\n    };\n\n    scriptFile = generateTestScript(scripts);\n    schemaTool.runBeeLine(scriptFile.getPath());\n    isValid = schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n   }",
            " 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129 +\n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "  /**\n   * Test to validate that all tables exist in the HMS metastore.\n   * @throws Exception\n   */\n  public void testValidateSchemaTables() throws Exception {\n    schemaTool.doInit(\"2.0.0\");\n\n    boolean isValid = (boolean)schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n\n    // upgrade from 2.0.0 schema and re-validate\n    schemaTool.doUpgrade(\"2.0.0\");\n    isValid = (boolean)schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n\n    // Simulate a missing table scenario by renaming a couple of tables\n    String[] scripts = new String[] {\n        \"RENAME TABLE SEQUENCE_TABLE to SEQUENCE_TABLE_RENAMED\",\n        \"RENAME TABLE NUCLEUS_TABLES to NUCLEUS_TABLES_RENAMED\"\n    };\n\n    File scriptFile = generateTestScript(scripts);\n    schemaTool.runBeeLine(scriptFile.getPath());\n    isValid = schemaTool.validateSchemaTables(conn);\n    assertFalse(isValid);\n\n    // Restored the renamed tables\n    scripts = new String[] {\n        \"RENAME TABLE SEQUENCE_TABLE_RENAMED to SEQUENCE_TABLE\",\n        \"RENAME TABLE NUCLEUS_TABLES_RENAMED to NUCLEUS_TABLES\"\n    };\n\n    scriptFile = generateTestScript(scripts);\n    schemaTool.runBeeLine(scriptFile.getPath());\n    isValid = schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n   }"
        ]
    ],
    "13d3bebd251f9406336c163286c959e899aee96c": [
        [
            "SqlFunctionConverter::StaticBlockBuilder::StaticBlockBuilder()",
            " 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  ",
            "    StaticBlockBuilder() {\n      registerFunction(\"+\", SqlStdOperatorTable.PLUS, hToken(HiveParser.PLUS, \"+\"));\n      registerFunction(\"-\", SqlStdOperatorTable.MINUS, hToken(HiveParser.MINUS, \"-\"));\n      registerFunction(\"*\", SqlStdOperatorTable.MULTIPLY, hToken(HiveParser.STAR, \"*\"));\n      registerFunction(\"/\", SqlStdOperatorTable.DIVIDE, hToken(HiveParser.DIVIDE, \"/\"));\n      registerFunction(\"%\", SqlStdOperatorTable.MOD, hToken(HiveParser.Identifier, \"%\"));\n      registerFunction(\"and\", SqlStdOperatorTable.AND, hToken(HiveParser.KW_AND, \"and\"));\n      registerFunction(\"or\", SqlStdOperatorTable.OR, hToken(HiveParser.KW_OR, \"or\"));\n      registerFunction(\"=\", SqlStdOperatorTable.EQUALS, hToken(HiveParser.EQUAL, \"=\"));\n      registerDuplicateFunction(\"==\", SqlStdOperatorTable.EQUALS, hToken(HiveParser.EQUAL, \"=\"));\n      registerFunction(\"<\", SqlStdOperatorTable.LESS_THAN, hToken(HiveParser.LESSTHAN, \"<\"));\n      registerFunction(\"<=\", SqlStdOperatorTable.LESS_THAN_OR_EQUAL,\n          hToken(HiveParser.LESSTHANOREQUALTO, \"<=\"));\n      registerFunction(\">\", SqlStdOperatorTable.GREATER_THAN, hToken(HiveParser.GREATERTHAN, \">\"));\n      registerFunction(\">=\", SqlStdOperatorTable.GREATER_THAN_OR_EQUAL,\n          hToken(HiveParser.GREATERTHANOREQUALTO, \">=\"));\n      registerFunction(\"not\", SqlStdOperatorTable.NOT, hToken(HiveParser.KW_NOT, \"not\"));\n      registerDuplicateFunction(\"!\", SqlStdOperatorTable.NOT, hToken(HiveParser.KW_NOT, \"not\"));\n      registerFunction(\"<>\", SqlStdOperatorTable.NOT_EQUALS, hToken(HiveParser.NOTEQUAL, \"<>\"));\n      registerDuplicateFunction(\"!=\", SqlStdOperatorTable.NOT_EQUALS, hToken(HiveParser.NOTEQUAL, \"<>\"));\n      registerFunction(\"in\", HiveIn.INSTANCE, hToken(HiveParser.Identifier, \"in\"));\n      registerFunction(\"between\", HiveBetween.INSTANCE, hToken(HiveParser.Identifier, \"between\"));\n      registerFunction(\"struct\", SqlStdOperatorTable.ROW, hToken(HiveParser.Identifier, \"struct\"));\n      registerFunction(\"isnotnull\", SqlStdOperatorTable.IS_NOT_NULL, hToken(HiveParser.TOK_ISNOTNULL, \"TOK_ISNOTNULL\"));\n      registerFunction(\"isnull\", SqlStdOperatorTable.IS_NULL, hToken(HiveParser.TOK_ISNULL, \"TOK_ISNULL\"));\n      registerFunction(\"when\", SqlStdOperatorTable.CASE, hToken(HiveParser.Identifier, \"when\"));\n      registerDuplicateFunction(\"case\", SqlStdOperatorTable.CASE, hToken(HiveParser.Identifier, \"when\"));\n      // timebased\n      registerFunction(\"year\", HiveExtractDate.YEAR,\n          hToken(HiveParser.Identifier, \"year\"));\n      registerFunction(\"quarter\", HiveExtractDate.QUARTER,\n          hToken(HiveParser.Identifier, \"quarter\"));\n      registerFunction(\"month\", HiveExtractDate.MONTH,\n          hToken(HiveParser.Identifier, \"month\"));\n      registerFunction(\"weekofyear\", HiveExtractDate.WEEK,\n          hToken(HiveParser.Identifier, \"weekofyear\"));\n      registerFunction(\"day\", HiveExtractDate.DAY,\n          hToken(HiveParser.Identifier, \"day\"));\n      registerFunction(\"hour\", HiveExtractDate.HOUR,\n          hToken(HiveParser.Identifier, \"hour\"));\n      registerFunction(\"minute\", HiveExtractDate.MINUTE,\n          hToken(HiveParser.Identifier, \"minute\"));\n      registerFunction(\"second\", HiveExtractDate.SECOND,\n          hToken(HiveParser.Identifier, \"second\"));\n      registerFunction(\"floor_year\", HiveFloorDate.YEAR,\n          hToken(HiveParser.Identifier, \"floor_year\"));\n      registerFunction(\"floor_quarter\", HiveFloorDate.QUARTER,\n          hToken(HiveParser.Identifier, \"floor_quarter\"));\n      registerFunction(\"floor_month\", HiveFloorDate.MONTH,\n          hToken(HiveParser.Identifier, \"floor_month\"));\n      registerFunction(\"floor_week\", HiveFloorDate.WEEK,\n          hToken(HiveParser.Identifier, \"floor_week\"));\n      registerFunction(\"floor_day\", HiveFloorDate.DAY,\n          hToken(HiveParser.Identifier, \"floor_day\"));\n      registerFunction(\"floor_hour\", HiveFloorDate.HOUR,\n          hToken(HiveParser.Identifier, \"floor_hour\"));\n      registerFunction(\"floor_minute\", HiveFloorDate.MINUTE,\n          hToken(HiveParser.Identifier, \"floor_minute\"));\n      registerFunction(\"floor_second\", HiveFloorDate.SECOND,\n          hToken(HiveParser.Identifier, \"floor_second\"));\n    }",
            " 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355 +\n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  ",
            "    StaticBlockBuilder() {\n      registerFunction(\"+\", SqlStdOperatorTable.PLUS, hToken(HiveParser.PLUS, \"+\"));\n      registerFunction(\"-\", SqlStdOperatorTable.MINUS, hToken(HiveParser.MINUS, \"-\"));\n      registerFunction(\"*\", SqlStdOperatorTable.MULTIPLY, hToken(HiveParser.STAR, \"*\"));\n      registerFunction(\"/\", SqlStdOperatorTable.DIVIDE, hToken(HiveParser.DIVIDE, \"/\"));\n      registerFunction(\"%\", SqlStdOperatorTable.MOD, hToken(HiveParser.Identifier, \"%\"));\n      registerFunction(\"and\", SqlStdOperatorTable.AND, hToken(HiveParser.KW_AND, \"and\"));\n      registerFunction(\"or\", SqlStdOperatorTable.OR, hToken(HiveParser.KW_OR, \"or\"));\n      registerFunction(\"=\", SqlStdOperatorTable.EQUALS, hToken(HiveParser.EQUAL, \"=\"));\n      registerDuplicateFunction(\"==\", SqlStdOperatorTable.EQUALS, hToken(HiveParser.EQUAL, \"=\"));\n      registerFunction(\"<\", SqlStdOperatorTable.LESS_THAN, hToken(HiveParser.LESSTHAN, \"<\"));\n      registerFunction(\"<=\", SqlStdOperatorTable.LESS_THAN_OR_EQUAL,\n          hToken(HiveParser.LESSTHANOREQUALTO, \"<=\"));\n      registerFunction(\">\", SqlStdOperatorTable.GREATER_THAN, hToken(HiveParser.GREATERTHAN, \">\"));\n      registerFunction(\">=\", SqlStdOperatorTable.GREATER_THAN_OR_EQUAL,\n          hToken(HiveParser.GREATERTHANOREQUALTO, \">=\"));\n      registerFunction(\"not\", SqlStdOperatorTable.NOT, hToken(HiveParser.KW_NOT, \"not\"));\n      registerDuplicateFunction(\"!\", SqlStdOperatorTable.NOT, hToken(HiveParser.KW_NOT, \"not\"));\n      registerFunction(\"<>\", SqlStdOperatorTable.NOT_EQUALS, hToken(HiveParser.NOTEQUAL, \"<>\"));\n      registerDuplicateFunction(\"!=\", SqlStdOperatorTable.NOT_EQUALS, hToken(HiveParser.NOTEQUAL, \"<>\"));\n      registerFunction(\"in\", HiveIn.INSTANCE, hToken(HiveParser.Identifier, \"in\"));\n      registerFunction(\"between\", HiveBetween.INSTANCE, hToken(HiveParser.Identifier, \"between\"));\n      registerFunction(\"struct\", SqlStdOperatorTable.ROW, hToken(HiveParser.Identifier, \"struct\"));\n      registerFunction(\"isnotnull\", SqlStdOperatorTable.IS_NOT_NULL, hToken(HiveParser.TOK_ISNOTNULL, \"TOK_ISNOTNULL\"));\n      registerFunction(\"isnull\", SqlStdOperatorTable.IS_NULL, hToken(HiveParser.TOK_ISNULL, \"TOK_ISNULL\"));\n      registerFunction(\"is not distinct from\", SqlStdOperatorTable.IS_NOT_DISTINCT_FROM, hToken(HiveParser.EQUAL_NS, \"<=>\"));\n      registerFunction(\"when\", SqlStdOperatorTable.CASE, hToken(HiveParser.Identifier, \"when\"));\n      registerDuplicateFunction(\"case\", SqlStdOperatorTable.CASE, hToken(HiveParser.Identifier, \"when\"));\n      // timebased\n      registerFunction(\"year\", HiveExtractDate.YEAR,\n          hToken(HiveParser.Identifier, \"year\"));\n      registerFunction(\"quarter\", HiveExtractDate.QUARTER,\n          hToken(HiveParser.Identifier, \"quarter\"));\n      registerFunction(\"month\", HiveExtractDate.MONTH,\n          hToken(HiveParser.Identifier, \"month\"));\n      registerFunction(\"weekofyear\", HiveExtractDate.WEEK,\n          hToken(HiveParser.Identifier, \"weekofyear\"));\n      registerFunction(\"day\", HiveExtractDate.DAY,\n          hToken(HiveParser.Identifier, \"day\"));\n      registerFunction(\"hour\", HiveExtractDate.HOUR,\n          hToken(HiveParser.Identifier, \"hour\"));\n      registerFunction(\"minute\", HiveExtractDate.MINUTE,\n          hToken(HiveParser.Identifier, \"minute\"));\n      registerFunction(\"second\", HiveExtractDate.SECOND,\n          hToken(HiveParser.Identifier, \"second\"));\n      registerFunction(\"floor_year\", HiveFloorDate.YEAR,\n          hToken(HiveParser.Identifier, \"floor_year\"));\n      registerFunction(\"floor_quarter\", HiveFloorDate.QUARTER,\n          hToken(HiveParser.Identifier, \"floor_quarter\"));\n      registerFunction(\"floor_month\", HiveFloorDate.MONTH,\n          hToken(HiveParser.Identifier, \"floor_month\"));\n      registerFunction(\"floor_week\", HiveFloorDate.WEEK,\n          hToken(HiveParser.Identifier, \"floor_week\"));\n      registerFunction(\"floor_day\", HiveFloorDate.DAY,\n          hToken(HiveParser.Identifier, \"floor_day\"));\n      registerFunction(\"floor_hour\", HiveFloorDate.HOUR,\n          hToken(HiveParser.Identifier, \"floor_hour\"));\n      registerFunction(\"floor_minute\", HiveFloorDate.MINUTE,\n          hToken(HiveParser.Identifier, \"floor_minute\"));\n      registerFunction(\"floor_second\", HiveFloorDate.SECOND,\n          hToken(HiveParser.Identifier, \"floor_second\"));\n    }"
        ]
    ],
    "bb2f25c1a189b031a9601cb00a3dc2f5d6f5ac4a": [
        [
            "SecretManager::createSecretManager(Configuration,String)",
            " 201  \n 202  \n 203  \n 204 -\n 205  ",
            "  public static SecretManager createSecretManager(Configuration conf, String clusterId) {\n    String llapPrincipal = HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_PRINCIPAL),\n        llapKeytab = HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_KEYTAB_FILE);\n    return SecretManager.createSecretManager(conf, llapPrincipal, llapKeytab, clusterId);\n  }",
            " 209  \n 210  \n 211  \n 212 +\n 213  ",
            "  public static SecretManager createSecretManager(Configuration conf, String clusterId) {\n    String llapPrincipal = HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_PRINCIPAL),\n        llapKeytab = HiveConf.getVar(conf, ConfVars.LLAP_KERBEROS_KEYTAB_FILE);\n    return createSecretManager(conf, llapPrincipal, llapKeytab, clusterId);\n  }"
        ],
        [
            "SecretManager::createLlapZkConf(Configuration,String,String,String)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 -\n 179  \n 180  \n 181  \n 182  \n 183 -\n 184  \n 185  \n 186 -\n 187  \n 188 -\n 189 -\n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  ",
            "  private static LlapZkConf createLlapZkConf(\n      Configuration conf, String llapPrincipal, String llapKeytab, String clusterId) {\n     String principal = HiveConf.getVar(conf, ConfVars.LLAP_ZKSM_KERBEROS_PRINCIPAL, llapPrincipal);\n     String keyTab = HiveConf.getVar(conf, ConfVars.LLAP_ZKSM_KERBEROS_KEYTAB_FILE, llapKeytab);\n     // Override the default delegation token lifetime for LLAP.\n     // Also set all the necessary ZK settings to defaults and LLAP configs, if not set.\n     final Configuration zkConf = new Configuration(conf);\n    long tokenLifetime = HiveConf.getTimeVar(\n        conf, ConfVars.LLAP_DELEGATION_TOKEN_LIFETIME, TimeUnit.SECONDS);\n    zkConf.setLong(DelegationTokenManager.MAX_LIFETIME, tokenLifetime);\n    zkConf.setLong(DelegationTokenManager.RENEW_INTERVAL, tokenLifetime);\n    try {\n      zkConf.set(SecretManager.ZK_DTSM_ZK_KERBEROS_PRINCIPAL,\n          SecurityUtil.getServerPrincipal(principal, \"0.0.0.0\"));\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    zkConf.set(SecretManager.ZK_DTSM_ZK_KERBEROS_KEYTAB, keyTab);\n    String zkPath = \"zkdtsm_\" + clusterId;\n    LOG.info(\"Using {} as ZK secret manager path\", zkPath);\n    zkConf.set(SecretManager.ZK_DTSM_ZNODE_WORKING_PATH, zkPath);\n    // Hardcode SASL here. ZKDTSM only supports none or sasl and we never want none.\n    zkConf.set(SecretManager.ZK_DTSM_ZK_AUTH_TYPE, \"sasl\");\n    setZkConfIfNotSet(zkConf, SecretManager.ZK_DTSM_ZK_CONNECTION_STRING,\n        HiveConf.getVar(zkConf, ConfVars.LLAP_ZKSM_ZK_CONNECTION_STRING));\n\n    UserGroupInformation zkUgi = null;\n    try {\n      zkUgi = LlapUtil.loginWithKerberos(principal, keyTab);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return new LlapZkConf(zkConf, zkUgi);\n  }",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  \n 180  \n 181  \n 182  \n 183 +\n 184  \n 185  \n 186 +\n 187  \n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193 +\n 194 +\n 195 +\n 196 +\n 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  ",
            "  private static LlapZkConf createLlapZkConf(\n      Configuration conf, String llapPrincipal, String llapKeytab, String clusterId) {\n     String principal = HiveConf.getVar(conf, ConfVars.LLAP_ZKSM_KERBEROS_PRINCIPAL, llapPrincipal);\n     String keyTab = HiveConf.getVar(conf, ConfVars.LLAP_ZKSM_KERBEROS_KEYTAB_FILE, llapKeytab);\n     // Override the default delegation token lifetime for LLAP.\n     // Also set all the necessary ZK settings to defaults and LLAP configs, if not set.\n     final Configuration zkConf = new Configuration(conf);\n    long tokenLifetime = HiveConf.getTimeVar(\n        conf, ConfVars.LLAP_DELEGATION_TOKEN_LIFETIME, TimeUnit.SECONDS);\n    zkConf.setLong(DelegationTokenManager.MAX_LIFETIME, tokenLifetime);\n    zkConf.setLong(DelegationTokenManager.RENEW_INTERVAL, tokenLifetime);\n    try {\n      zkConf.set(ZK_DTSM_ZK_KERBEROS_PRINCIPAL,\n          SecurityUtil.getServerPrincipal(principal, \"0.0.0.0\"));\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    zkConf.set(ZK_DTSM_ZK_KERBEROS_KEYTAB, keyTab);\n    String zkPath = \"zkdtsm_\" + clusterId;\n    LOG.info(\"Using {} as ZK secret manager path\", zkPath);\n    zkConf.set(ZK_DTSM_ZNODE_WORKING_PATH, zkPath);\n    // Hardcode SASL here. ZKDTSM only supports none or sasl and we never want none.\n    zkConf.set(ZK_DTSM_ZK_AUTH_TYPE, \"sasl\");\n    long sessionTimeoutMs = HiveConf.getTimeVar(\n        zkConf, ConfVars.LLAP_ZKSM_ZK_SESSION_TIMEOUT, TimeUnit.MILLISECONDS);\n    long newRetryCount =\n        (ZK_DTSM_ZK_NUM_RETRIES_DEFAULT * sessionTimeoutMs) / ZK_DTSM_ZK_SESSION_TIMEOUT_DEFAULT;\n    long connTimeoutMs = Math.max(sessionTimeoutMs, ZK_DTSM_ZK_CONNECTION_TIMEOUT_DEFAULT);\n    zkConf.set(ZK_DTSM_ZK_SESSION_TIMEOUT, Long.toString(sessionTimeoutMs));\n    zkConf.set(ZK_DTSM_ZK_CONNECTION_TIMEOUT, Long.toString(connTimeoutMs));\n    zkConf.set(ZK_DTSM_ZK_NUM_RETRIES, Long.toString(newRetryCount));\n    setZkConfIfNotSet(zkConf, ZK_DTSM_ZK_CONNECTION_STRING,\n        HiveConf.getVar(zkConf, ConfVars.LLAP_ZKSM_ZK_CONNECTION_STRING));\n\n    UserGroupInformation zkUgi = null;\n    try {\n      zkUgi = LlapUtil.loginWithKerberos(principal, keyTab);\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n    return new LlapZkConf(zkConf, zkUgi);\n  }"
        ]
    ],
    "952fe6e17e8418515caf33de96e33ff16711265f": [
        [
            "TestSchemaTool::validateMetastoreDbPropertiesTable()",
            " 690  \n 691  \n 692  \n 693  \n 694  \n 695 -\n 696 -\n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  ",
            "  private void validateMetastoreDbPropertiesTable() throws HiveMetaException, IOException {\n    boolean isValid = (boolean) schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n    // adding same property key twice should throw unique key constraint violation exception\n    String[] scripts = new String[] {\n        \"insert into METASTORE_DB_PROPERTIES values (1, 'guid', 'test-uuid-1', 'dummy uuid 1')\",\n        \"insert into METASTORE_DB_PROPERTIES values (2, 'guid', 'test-uuid-2', 'dummy uuid 2')\", };\n    File scriptFile = generateTestScript(scripts);\n    Exception ex = null;\n    try {\n      schemaTool.runBeeLine(scriptFile.getPath());\n    } catch (Exception iox) {\n      ex = iox;\n    }\n    assertTrue(ex != null && ex instanceof IOException);\n  }",
            " 690  \n 691  \n 692  \n 693  \n 694  \n 695 +\n 696 +\n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  ",
            "  private void validateMetastoreDbPropertiesTable() throws HiveMetaException, IOException {\n    boolean isValid = (boolean) schemaTool.validateSchemaTables(conn);\n    assertTrue(isValid);\n    // adding same property key twice should throw unique key constraint violation exception\n    String[] scripts = new String[] {\n        \"insert into METASTORE_DB_PROPERTIES values ('guid', 'test-uuid-1', 'dummy uuid 1')\",\n        \"insert into METASTORE_DB_PROPERTIES values ('guid', 'test-uuid-2', 'dummy uuid 2')\", };\n    File scriptFile = generateTestScript(scripts);\n    Exception ex = null;\n    try {\n      schemaTool.runBeeLine(scriptFile.getPath());\n    } catch (Exception iox) {\n      ex = iox;\n    }\n    assertTrue(ex != null && ex instanceof IOException);\n  }"
        ]
    ],
    "314fe44e5c45fdf1ea2a3edbe9bb9eedda5c12fd": [
        [
            "HiveSchemaTool::validateSchemaTables(Connection)",
            " 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747 -\n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  ",
            "  boolean validateSchemaTables(Connection conn) throws HiveMetaException {\n    String version            = null;\n    ResultSet rs              = null;\n    DatabaseMetaData metadata = null;\n    List<String> dbTables     = new ArrayList<String>();\n    List<String> schemaTables = new ArrayList<String>();\n    List<String> subScripts   = new ArrayList<String>();\n    Connection hmsConn        = getConnectionToMetastore(false);\n\n    System.out.println(\"Validating metastore schema tables\");\n    try {\n      version = metaStoreSchemaInfo.getMetaStoreSchemaVersion(getConnectionInfo(false));\n    } catch (HiveMetaException he) {\n      System.err.println(\"Failed to determine schema version from Hive Metastore DB. \" + he.getMessage());\n      System.out.println(\"Failed in schema table validation.\");\n      LOG.debug(\"Failed to determine schema version from Hive Metastore DB,\" + he.getMessage());\n      return false;\n    }\n\n    // re-open the hms connection\n    hmsConn = getConnectionToMetastore(false);\n\n    LOG.debug(\"Validating tables in the schema for version \" + version);\n    try {\n      metadata       = conn.getMetaData();\n      String[] types = {\"TABLE\"};\n      rs             = metadata.getTables(null, null, \"%\", types);\n      String table   = null;\n\n      while (rs.next()) {\n        table = rs.getString(\"TABLE_NAME\");\n        dbTables.add(table.toLowerCase());\n        LOG.debug(\"Found table \" + table + \" in HMS dbstore\");\n      }\n    } catch (SQLException e) {\n      throw new HiveMetaException(\"Failed to retrieve schema tables from Hive Metastore DB,\" + e.getMessage());\n    } finally {\n      if (rs != null) {\n        try {\n          rs.close();\n        } catch (SQLException e) {\n          throw new HiveMetaException(\"Failed to close resultset\", e);\n        }\n      }\n    }\n\n    // parse the schema file to determine the tables that are expected to exist\n    // we are using oracle schema because it is simpler to parse, no quotes or backticks etc\n    String baseDir    = new File(metaStoreSchemaInfo.getMetaStoreScriptDir()).getParent();\n    String schemaFile = new File(metaStoreSchemaInfo.getMetaStoreScriptDir(),\n        metaStoreSchemaInfo.generateInitFileName(version)).getPath();\n    try {\n      LOG.debug(\"Parsing schema script \" + schemaFile);\n      subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      while (subScripts.size() > 0) {\n        schemaFile = baseDir + \"/\" + dbType + \"/\" + subScripts.remove(0);\n        LOG.debug(\"Parsing subscript \" + schemaFile);\n        subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      }\n    } catch (Exception e) {\n      System.err.println(\"Exception in parsing schema file. Cause:\" + e.getMessage());\n      System.out.println(\"Failed in schema table validation.\");\n      return false;\n    }\n\n    LOG.debug(\"Schema tables:[ \" + Arrays.toString(schemaTables.toArray()) + \" ]\");\n    LOG.debug(\"DB tables:[ \" + Arrays.toString(dbTables.toArray()) + \" ]\");\n    // now diff the lists\n    schemaTables.removeAll(dbTables);\n    if (schemaTables.size() > 0) {\n      Collections.sort(schemaTables);\n      System.err.println(\"Table(s) [ \" + Arrays.toString(schemaTables.toArray())\n          + \" ] are missing from the metastore database schema.\");\n      System.out.println(\"Failed in schema table validation\");\n      return false;\n    } else {\n      System.out.println(\"Succeeded in schema table validation.\");\n      return true;\n    }\n  }",
            " 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747 +\n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  ",
            "  boolean validateSchemaTables(Connection conn) throws HiveMetaException {\n    String version            = null;\n    ResultSet rs              = null;\n    DatabaseMetaData metadata = null;\n    List<String> dbTables     = new ArrayList<String>();\n    List<String> schemaTables = new ArrayList<String>();\n    List<String> subScripts   = new ArrayList<String>();\n    Connection hmsConn        = getConnectionToMetastore(false);\n\n    System.out.println(\"Validating metastore schema tables\");\n    try {\n      version = metaStoreSchemaInfo.getMetaStoreSchemaVersion(getConnectionInfo(false));\n    } catch (HiveMetaException he) {\n      System.err.println(\"Failed to determine schema version from Hive Metastore DB. \" + he.getMessage());\n      System.out.println(\"Failed in schema table validation.\");\n      LOG.debug(\"Failed to determine schema version from Hive Metastore DB,\" + he.getMessage());\n      return false;\n    }\n\n    // re-open the hms connection\n    hmsConn = getConnectionToMetastore(false);\n\n    LOG.debug(\"Validating tables in the schema for version \" + version);\n    try {\n      metadata       = conn.getMetaData();\n      String[] types = {\"TABLE\"};\n      rs             = metadata.getTables(null, hmsConn.getSchema(), \"%\", types);\n      String table   = null;\n\n      while (rs.next()) {\n        table = rs.getString(\"TABLE_NAME\");\n        dbTables.add(table.toLowerCase());\n        LOG.debug(\"Found table \" + table + \" in HMS dbstore\");\n      }\n    } catch (SQLException e) {\n      throw new HiveMetaException(\"Failed to retrieve schema tables from Hive Metastore DB,\" + e.getMessage());\n    } finally {\n      if (rs != null) {\n        try {\n          rs.close();\n        } catch (SQLException e) {\n          throw new HiveMetaException(\"Failed to close resultset\", e);\n        }\n      }\n    }\n\n    // parse the schema file to determine the tables that are expected to exist\n    // we are using oracle schema because it is simpler to parse, no quotes or backticks etc\n    String baseDir    = new File(metaStoreSchemaInfo.getMetaStoreScriptDir()).getParent();\n    String schemaFile = new File(metaStoreSchemaInfo.getMetaStoreScriptDir(),\n        metaStoreSchemaInfo.generateInitFileName(version)).getPath();\n    try {\n      LOG.debug(\"Parsing schema script \" + schemaFile);\n      subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      while (subScripts.size() > 0) {\n        schemaFile = baseDir + \"/\" + dbType + \"/\" + subScripts.remove(0);\n        LOG.debug(\"Parsing subscript \" + schemaFile);\n        subScripts.addAll(findCreateTable(schemaFile, schemaTables));\n      }\n    } catch (Exception e) {\n      System.err.println(\"Exception in parsing schema file. Cause:\" + e.getMessage());\n      System.out.println(\"Failed in schema table validation.\");\n      return false;\n    }\n\n    LOG.debug(\"Schema tables:[ \" + Arrays.toString(schemaTables.toArray()) + \" ]\");\n    LOG.debug(\"DB tables:[ \" + Arrays.toString(dbTables.toArray()) + \" ]\");\n    // now diff the lists\n    schemaTables.removeAll(dbTables);\n    if (schemaTables.size() > 0) {\n      Collections.sort(schemaTables);\n      System.err.println(\"Table(s) [ \" + Arrays.toString(schemaTables.toArray())\n          + \" ] are missing from the metastore database schema.\");\n      System.out.println(\"Failed in schema table validation\");\n      return false;\n    } else {\n      System.out.println(\"Succeeded in schema table validation.\");\n      return true;\n    }\n  }"
        ]
    ],
    "aef22bf4eb4336ba25b1505dd37c92e5a20dfb83": [
        [
            "LoadSemanticAnalyzer::analyzeInternal(ASTNode)",
            " 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  ",
            "  @Override\n  public void analyzeInternal(ASTNode ast) throws SemanticException {\n    boolean isLocal = false;\n    boolean isOverWrite = false;\n    Tree fromTree = ast.getChild(0);\n    Tree tableTree = ast.getChild(1);\n\n    if (ast.getChildCount() == 4) {\n      isLocal = true;\n      isOverWrite = true;\n    }\n\n    if (ast.getChildCount() == 3) {\n      if (ast.getChild(2).getText().toLowerCase().equals(\"local\")) {\n        isLocal = true;\n      } else {\n        isOverWrite = true;\n      }\n    }\n\n    // initialize load path\n    URI fromURI;\n    try {\n      String fromPath = stripQuotes(fromTree.getText());\n      fromURI = initializeFromURI(fromPath, isLocal);\n    } catch (IOException e) {\n      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e\n          .getMessage()), e);\n    } catch (URISyntaxException e) {\n      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e\n          .getMessage()), e);\n    }\n\n    // initialize destination table/partition\n    TableSpec ts = new TableSpec(db, conf, (ASTNode) tableTree);\n\n    if (ts.tableHandle.isView() || ts.tableHandle.isMaterializedView()) {\n      throw new SemanticException(ErrorMsg.DML_AGAINST_VIEW.getMsg());\n    }\n    if (ts.tableHandle.isNonNative()) {\n      throw new SemanticException(ErrorMsg.LOAD_INTO_NON_NATIVE.getMsg());\n    }\n\n    if(ts.tableHandle.isStoredAsSubDirectories()) {\n      throw new SemanticException(ErrorMsg.LOAD_INTO_STORED_AS_DIR.getMsg());\n    }\n\n    List<FieldSchema> parts = ts.tableHandle.getPartitionKeys();\n    if ((parts != null && parts.size() > 0)\n        && (ts.partSpec == null || ts.partSpec.size() == 0)) {\n      throw new SemanticException(ErrorMsg.NEED_PARTITION_ERROR.getMsg());\n    }\n    List<String> bucketCols = ts.tableHandle.getBucketCols();\n    if (bucketCols != null && !bucketCols.isEmpty()) {\n      String error = StrictChecks.checkBucketing(conf);\n      if (error != null) throw new SemanticException(\"Please load into an intermediate table\"\n          + \" and use 'insert... select' to allow Hive to enforce bucketing. \" + error);\n    }\n\n    // make sure the arguments make sense\n    List<FileStatus> files = applyConstraintsAndGetFiles(fromURI, fromTree, isLocal);\n\n    // for managed tables, make sure the file formats match\n    if (TableType.MANAGED_TABLE.equals(ts.tableHandle.getTableType())\n        && conf.getBoolVar(HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n      ensureFileFormatsMatch(ts, files, fromURI);\n    }\n    inputs.add(toReadEntity(new Path(fromURI)));\n    Task<? extends Serializable> rTask = null;\n\n    // create final load/move work\n\n    boolean preservePartitionSpecs = false;\n\n    Map<String, String> partSpec = ts.getPartSpec();\n    if (partSpec == null) {\n      partSpec = new LinkedHashMap<String, String>();\n      outputs.add(new WriteEntity(ts.tableHandle,\n          (isOverWrite ? WriteEntity.WriteType.INSERT_OVERWRITE :\n              WriteEntity.WriteType.INSERT)));\n    } else {\n      try{\n        Partition part = Hive.get().getPartition(ts.tableHandle, partSpec, false);\n        if (part != null) {\n          if (isOverWrite){\n            outputs.add(new WriteEntity(part, WriteEntity.WriteType.INSERT_OVERWRITE));\n          } else {\n            outputs.add(new WriteEntity(part, WriteEntity.WriteType.INSERT));\n            // If partition already exists and we aren't overwriting it, then respect\n            // its current location info rather than picking it from the parent TableDesc\n            preservePartitionSpecs = true;\n          }\n        } else {\n          outputs.add(new WriteEntity(ts.tableHandle,\n          (isOverWrite ? WriteEntity.WriteType.INSERT_OVERWRITE :\n              WriteEntity.WriteType.INSERT)));\n        }\n      } catch(HiveException e) {\n        throw new SemanticException(e);\n      }\n    }\n\n\n    LoadTableDesc loadTableWork;\n    loadTableWork = new LoadTableDesc(new Path(fromURI),\n      Utilities.getTableDesc(ts.tableHandle), partSpec, isOverWrite);\n    if (preservePartitionSpecs){\n      // Note : preservePartitionSpecs=true implies inheritTableSpecs=false but\n      // but preservePartitionSpecs=false(default) here is not sufficient enough\n      // info to set inheritTableSpecs=true\n      loadTableWork.setInheritTableSpecs(false);\n    }\n\n    Task<? extends Serializable> childTask = TaskFactory.get(new MoveWork(getInputs(),\n        getOutputs(), loadTableWork, null, true, isLocal), conf);\n    if (rTask != null) {\n      rTask.addDependentTask(childTask);\n    } else {\n      rTask = childTask;\n    }\n\n    rootTasks.add(rTask);\n\n    // The user asked for stats to be collected.\n    // Some stats like number of rows require a scan of the data\n    // However, some other stats, like number of files, do not require a complete scan\n    // Update the stats which do not require a complete scan.\n    Task<? extends Serializable> statTask = null;\n    if (conf.getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n      StatsWork statDesc = new StatsWork(loadTableWork);\n      statDesc.setNoStatsAggregator(true);\n      statDesc.setClearAggregatorStats(true);\n      statDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n      statTask = TaskFactory.get(statDesc, conf);\n    }\n\n    // HIVE-3334 has been filed for load file with index auto update\n    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEINDEXAUTOUPDATE)) {\n      IndexUpdater indexUpdater = new IndexUpdater(loadTableWork, getInputs(), conf);\n      try {\n        List<Task<? extends Serializable>> indexUpdateTasks = indexUpdater.generateUpdateTasks();\n\n        for (Task<? extends Serializable> updateTask : indexUpdateTasks) {\n          //LOAD DATA will either have a copy & move or just a move,\n          // we always want the update to be dependent on the move\n          childTask.addDependentTask(updateTask);\n          if (statTask != null) {\n            updateTask.addDependentTask(statTask);\n          }\n        }\n      } catch (HiveException e) {\n        console.printInfo(\"WARNING: could not auto-update stale indexes, indexes are not out of sync\");\n      }\n    }\n    else if (statTask != null) {\n      childTask.addDependentTask(statTask);\n    }\n  }",
            " 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231 +\n 232 +\n 233 +\n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  ",
            "  @Override\n  public void analyzeInternal(ASTNode ast) throws SemanticException {\n    boolean isLocal = false;\n    boolean isOverWrite = false;\n    Tree fromTree = ast.getChild(0);\n    Tree tableTree = ast.getChild(1);\n\n    if (ast.getChildCount() == 4) {\n      isLocal = true;\n      isOverWrite = true;\n    }\n\n    if (ast.getChildCount() == 3) {\n      if (ast.getChild(2).getText().toLowerCase().equals(\"local\")) {\n        isLocal = true;\n      } else {\n        isOverWrite = true;\n      }\n    }\n\n    // initialize load path\n    URI fromURI;\n    try {\n      String fromPath = stripQuotes(fromTree.getText());\n      fromURI = initializeFromURI(fromPath, isLocal);\n    } catch (IOException e) {\n      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e\n          .getMessage()), e);\n    } catch (URISyntaxException e) {\n      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e\n          .getMessage()), e);\n    }\n\n    // initialize destination table/partition\n    TableSpec ts = new TableSpec(db, conf, (ASTNode) tableTree);\n\n    if (ts.tableHandle.isView() || ts.tableHandle.isMaterializedView()) {\n      throw new SemanticException(ErrorMsg.DML_AGAINST_VIEW.getMsg());\n    }\n    if (ts.tableHandle.isNonNative()) {\n      throw new SemanticException(ErrorMsg.LOAD_INTO_NON_NATIVE.getMsg());\n    }\n\n    if(ts.tableHandle.isStoredAsSubDirectories()) {\n      throw new SemanticException(ErrorMsg.LOAD_INTO_STORED_AS_DIR.getMsg());\n    }\n\n    List<FieldSchema> parts = ts.tableHandle.getPartitionKeys();\n    if ((parts != null && parts.size() > 0)\n        && (ts.partSpec == null || ts.partSpec.size() == 0)) {\n      throw new SemanticException(ErrorMsg.NEED_PARTITION_ERROR.getMsg());\n    }\n    List<String> bucketCols = ts.tableHandle.getBucketCols();\n    if (bucketCols != null && !bucketCols.isEmpty()) {\n      String error = StrictChecks.checkBucketing(conf);\n      if (error != null) throw new SemanticException(\"Please load into an intermediate table\"\n          + \" and use 'insert... select' to allow Hive to enforce bucketing. \" + error);\n    }\n\n    if(AcidUtils.isAcidTable(ts.tableHandle)) {\n      throw new SemanticException(ErrorMsg.LOAD_DATA_ON_ACID_TABLE, ts.tableHandle.getCompleteName());\n    }\n    // make sure the arguments make sense\n    List<FileStatus> files = applyConstraintsAndGetFiles(fromURI, fromTree, isLocal);\n\n    // for managed tables, make sure the file formats match\n    if (TableType.MANAGED_TABLE.equals(ts.tableHandle.getTableType())\n        && conf.getBoolVar(HiveConf.ConfVars.HIVECHECKFILEFORMAT)) {\n      ensureFileFormatsMatch(ts, files, fromURI);\n    }\n    inputs.add(toReadEntity(new Path(fromURI)));\n    Task<? extends Serializable> rTask = null;\n\n    // create final load/move work\n\n    boolean preservePartitionSpecs = false;\n\n    Map<String, String> partSpec = ts.getPartSpec();\n    if (partSpec == null) {\n      partSpec = new LinkedHashMap<String, String>();\n      outputs.add(new WriteEntity(ts.tableHandle,\n          (isOverWrite ? WriteEntity.WriteType.INSERT_OVERWRITE :\n              WriteEntity.WriteType.INSERT)));\n    } else {\n      try{\n        Partition part = Hive.get().getPartition(ts.tableHandle, partSpec, false);\n        if (part != null) {\n          if (isOverWrite){\n            outputs.add(new WriteEntity(part, WriteEntity.WriteType.INSERT_OVERWRITE));\n          } else {\n            outputs.add(new WriteEntity(part, WriteEntity.WriteType.INSERT));\n            // If partition already exists and we aren't overwriting it, then respect\n            // its current location info rather than picking it from the parent TableDesc\n            preservePartitionSpecs = true;\n          }\n        } else {\n          outputs.add(new WriteEntity(ts.tableHandle,\n          (isOverWrite ? WriteEntity.WriteType.INSERT_OVERWRITE :\n              WriteEntity.WriteType.INSERT)));\n        }\n      } catch(HiveException e) {\n        throw new SemanticException(e);\n      }\n    }\n\n\n    LoadTableDesc loadTableWork;\n    loadTableWork = new LoadTableDesc(new Path(fromURI),\n      Utilities.getTableDesc(ts.tableHandle), partSpec, isOverWrite);\n    if (preservePartitionSpecs){\n      // Note : preservePartitionSpecs=true implies inheritTableSpecs=false but\n      // but preservePartitionSpecs=false(default) here is not sufficient enough\n      // info to set inheritTableSpecs=true\n      loadTableWork.setInheritTableSpecs(false);\n    }\n\n    Task<? extends Serializable> childTask = TaskFactory.get(new MoveWork(getInputs(),\n        getOutputs(), loadTableWork, null, true, isLocal), conf);\n    if (rTask != null) {\n      rTask.addDependentTask(childTask);\n    } else {\n      rTask = childTask;\n    }\n\n    rootTasks.add(rTask);\n\n    // The user asked for stats to be collected.\n    // Some stats like number of rows require a scan of the data\n    // However, some other stats, like number of files, do not require a complete scan\n    // Update the stats which do not require a complete scan.\n    Task<? extends Serializable> statTask = null;\n    if (conf.getBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER)) {\n      StatsWork statDesc = new StatsWork(loadTableWork);\n      statDesc.setNoStatsAggregator(true);\n      statDesc.setClearAggregatorStats(true);\n      statDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));\n      statTask = TaskFactory.get(statDesc, conf);\n    }\n\n    // HIVE-3334 has been filed for load file with index auto update\n    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVEINDEXAUTOUPDATE)) {\n      IndexUpdater indexUpdater = new IndexUpdater(loadTableWork, getInputs(), conf);\n      try {\n        List<Task<? extends Serializable>> indexUpdateTasks = indexUpdater.generateUpdateTasks();\n\n        for (Task<? extends Serializable> updateTask : indexUpdateTasks) {\n          //LOAD DATA will either have a copy & move or just a move,\n          // we always want the update to be dependent on the move\n          childTask.addDependentTask(updateTask);\n          if (statTask != null) {\n            updateTask.addDependentTask(statTask);\n          }\n        }\n      } catch (HiveException e) {\n        console.printInfo(\"WARNING: could not auto-update stale indexes, indexes are not out of sync\");\n      }\n    }\n    else if (statTask != null) {\n      childTask.addDependentTask(statTask);\n    }\n  }"
        ]
    ],
    "fe477b59eac2a003e6091d3a8b47738056fbae27": [
        [
            "LlapTaskUmbilicalExternalClient::LlapTaskUmbilicalExternalImpl::taskKilled(TezTaskAttemptID)",
            " 488  \n 489  \n 490  \n 491 -\n 492  \n 493  \n 494 -\n 495 -\n 496 -\n 497 -\n 498  \n 499 -\n 500 -\n 501  \n 502  \n 503  \n 504  \n 505  ",
            "    @Override\n    public void taskKilled(TezTaskAttemptID taskAttemptId) throws IOException {\n      String taskAttemptIdString = taskAttemptId.toString();\n      LOG.error(\"Task killed - \" + taskAttemptIdString);\n      LlapTaskUmbilicalExternalClient client = registeredClients.get(taskAttemptIdString);\n      if (client != null) {\n        try {\n          client.unregisterClient();\n          if (client.responder != null) {\n            client.responder.taskKilled(taskAttemptId);\n          }\n        } catch (Exception err) {\n          LOG.error(\"Error during responder execution\", err);\n        }\n      } else {\n        LOG.info(\"Received task killed notification for task which is not currently being tracked: \" + taskAttemptId);\n      }\n    }",
            " 488  \n 489  \n 490  \n 491  \n 492  \n 493 +\n 494 +\n 495 +\n 496 +\n 497 +\n 498 +\n 499 +\n 500 +\n 501 +\n 502 +\n 503 +\n 504  \n 505  \n 506  \n 507  \n 508  \n 509  ",
            "    @Override\n    public void taskKilled(TezTaskAttemptID taskAttemptId) throws IOException {\n      String taskAttemptIdString = taskAttemptId.toString();\n      LlapTaskUmbilicalExternalClient client = registeredClients.get(taskAttemptIdString);\n      if (client != null) {\n        if (client.requestInfo.state == RequestState.PENDING) {\n          LOG.debug(\"Ignoring task kill for {}, request is still in pending state\", taskAttemptIdString);\n        } else {\n          try {\n            LOG.error(\"Task killed - \" + taskAttemptIdString);\n            client.unregisterClient();\n            if (client.responder != null) {\n              client.responder.taskKilled(taskAttemptId);\n            }\n          } catch (Exception err) {\n            LOG.error(\"Error during responder execution\", err);\n          }\n        }\n      } else {\n        LOG.info(\"Received task killed notification for task which is not currently being tracked: \" + taskAttemptId);\n      }\n    }"
        ]
    ],
    "aeb837727500a1dc02b1f23e083b4a29930f30ca": [
        [
            "TezCompiler::removeSemijoinsParallelToMapJoin(OptimizeTezProcContext)",
            "1058  \n1059  \n1060  \n1061 -\n1062 -\n1063  \n1064  \n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  ",
            "  private void removeSemijoinsParallelToMapJoin(OptimizeTezProcContext procCtx)\n          throws SemanticException {\n    if(!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            !procCtx.conf.getBoolVar(ConfVars.HIVECONVERTJOIN)) {\n      // Not needed without semi-join reduction\n      return;\n    }\n\n    // Get all the TS ops.\n    List<Operator<?>> topOps = new ArrayList<>();\n    topOps.addAll(procCtx.parseContext.getTopOps().values());\n\n    Map<ReduceSinkOperator, TableScanOperator> semijoins = new HashMap<>();\n    for (Operator<?> parent : topOps) {\n      // A TS can have multiple branches due to DPP Or Semijoin Opt.\n      // USe DFS to traverse all the branches until RS is hit.\n      Deque<Operator<?>> deque = new LinkedList<>();\n      deque.add(parent);\n      while (!deque.isEmpty()) {\n        Operator<?> op = deque.poll();\n        if (op instanceof ReduceSinkOperator) {\n          // Done with this branch\n          continue;\n        }\n\n        if (op instanceof MapJoinOperator) {\n          // A candidate.\n          if (!findParallelSemiJoinBranch(op, (TableScanOperator) parent,\n                  procCtx.parseContext, semijoins)) {\n            // No parallel edge was found for the given mapjoin op,\n            // no need to go down further, skip this TS operator pipeline.\n            break;\n          }\n        }\n        deque.addAll(op.getChildOperators());\n      }\n    }\n\n    if (semijoins.size() > 0) {\n      for (ReduceSinkOperator rs : semijoins.keySet()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Semijoin optimization with parallel edge to map join. Removing semijoin \"\n              + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(semijoins.get(rs)));\n        }\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(procCtx.parseContext, rs,\n                semijoins.get(rs));\n      }\n    }\n  }",
            "1058  \n1059  \n1060  \n1061 +\n1062 +\n1063 +\n1064 +\n1065  \n1066  \n1067  \n1068  \n1069  \n1070  \n1071  \n1072  \n1073  \n1074  \n1075  \n1076  \n1077  \n1078  \n1079  \n1080  \n1081  \n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  ",
            "  private void removeSemijoinsParallelToMapJoin(OptimizeTezProcContext procCtx)\n          throws SemanticException {\n    if(!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION) ||\n            !procCtx.conf.getBoolVar(ConfVars.HIVECONVERTJOIN) ||\n            procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_SEMIJOIN_REDUCTION_FOR_MAPJOIN)) {\n      // Not needed without semi-join reduction or mapjoins or when semijoins\n      // are enabled for parallel mapjoins.\n      return;\n    }\n\n    // Get all the TS ops.\n    List<Operator<?>> topOps = new ArrayList<>();\n    topOps.addAll(procCtx.parseContext.getTopOps().values());\n\n    Map<ReduceSinkOperator, TableScanOperator> semijoins = new HashMap<>();\n    for (Operator<?> parent : topOps) {\n      // A TS can have multiple branches due to DPP Or Semijoin Opt.\n      // USe DFS to traverse all the branches until RS is hit.\n      Deque<Operator<?>> deque = new LinkedList<>();\n      deque.add(parent);\n      while (!deque.isEmpty()) {\n        Operator<?> op = deque.poll();\n        if (op instanceof ReduceSinkOperator) {\n          // Done with this branch\n          continue;\n        }\n\n        if (op instanceof MapJoinOperator) {\n          // A candidate.\n          if (!findParallelSemiJoinBranch(op, (TableScanOperator) parent,\n                  procCtx.parseContext, semijoins)) {\n            // No parallel edge was found for the given mapjoin op,\n            // no need to go down further, skip this TS operator pipeline.\n            break;\n          }\n        }\n        deque.addAll(op.getChildOperators());\n      }\n    }\n\n    if (semijoins.size() > 0) {\n      for (ReduceSinkOperator rs : semijoins.keySet()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Semijoin optimization with parallel edge to map join. Removing semijoin \"\n              + OperatorUtils.getOpNamePretty(rs) + \" - \" + OperatorUtils.getOpNamePretty(semijoins.get(rs)));\n        }\n        GenTezUtils.removeBranch(rs);\n        GenTezUtils.removeSemiJoinOperator(procCtx.parseContext, rs,\n                semijoins.get(rs));\n      }\n    }\n  }"
        ]
    ],
    "afd0d9ffc4d8bc39effbb9f7324fcafa7518b141": [
        [
            "HiveMetaStore::HMSHandler::deleteParentRecursive(Path,int,boolean)",
            "3591  \n3592 -\n3593 -\n3594  \n3595  \n3596  ",
            "    private void deleteParentRecursive(Path parent, int depth, boolean mustPurge) throws IOException, MetaException {\n      if (depth > 0 && parent != null && wh.isWritable(parent) && wh.isEmpty(parent)) {\n        wh.deleteDir(parent, true, mustPurge);\n        deleteParentRecursive(parent.getParent(), depth - 1, mustPurge);\n      }\n    }",
            "3591  \n3592 +\n3593 +\n3594 +\n3595 +\n3596  \n3597  \n3598  ",
            "    private void deleteParentRecursive(Path parent, int depth, boolean mustPurge) throws IOException, MetaException {\n      if (depth > 0 && parent != null && wh.isWritable(parent)) {\n        if (wh.isDir(parent) && wh.isEmpty(parent)) {\n          wh.deleteDir(parent, true, mustPurge);\n        }\n        deleteParentRecursive(parent.getParent(), depth - 1, mustPurge);\n      }\n    }"
        ]
    ],
    "a988c159cc3e9f43211432eb035454ee6143b219": [
        [
            "JoinDesc::getColumnExprMapForExplain()",
            " 381  \n 382  \n 383 -\n 384  \n 385 -\n 386  \n 387 -\n 388  \n 389 -\n 390 -\n 391  \n 392  \n 393  ",
            "  @Override\n  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, ExprNodeDesc> getColumnExprMapForExplain() {\n    if(this.reversedExprs == null) {\n      return this.colExprMap;\n    }\n    Map<String, ExprNodeDesc> explainColMap = new HashMap<>();\n    for(String col:this.colExprMap.keySet()){\n      String taggedCol = this.reversedExprs.get(col) + \" \" + col;\n      explainColMap.put(taggedCol, this.colExprMap.get(col));\n    }\n    return explainColMap;\n  }",
            " 381  \n 382  \n 383 +\n 384  \n 385 +\n 386  \n 387 +\n 388  \n 389 +\n 390 +\n 391  \n 392  \n 393  ",
            "  @Override\n  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, String> getColumnExprMapForExplain() {\n    if(this.reversedExprs == null) {\n      return super.getColumnExprMapForExplain();\n    }\n    Map<String, String> explainColMap = new HashMap<>();\n    for(String col:this.colExprMap.keySet()){\n      String taggedCol = this.reversedExprs.get(col) + \":\" + this.colExprMap.get(col);\n      explainColMap.put(col, taggedCol);\n    }\n    return explainColMap;\n  }"
        ],
        [
            "AbstractOperatorDesc::getColumnExprMapForExplain()",
            " 144  \n 145 -\n 146 -\n 147  ",
            "  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, ExprNodeDesc> getColumnExprMapForExplain() {\n    return this.colExprMap;\n  }",
            " 145  \n 146 +\n 147 +\n 148 +\n 149 +\n 150 +\n 151 +\n 152  ",
            "  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, String> getColumnExprMapForExplain() {\n    Map<String, String> colExprMapForExplain = new HashMap<>();\n    for(String col:this.colExprMap.keySet()) {\n      colExprMapForExplain.put(col, this.colExprMap.get(col).toString());\n    }\n    return colExprMapForExplain;\n  }"
        ]
    ],
    "d62a038a1ca84037b7b17ce989954a93d1a76a25": [
        [
            "JoinDesc::getColumnExprMapForExplain()",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 -\n 390  \n 391  \n 392  \n 393  ",
            "  @Override\n  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, String> getColumnExprMapForExplain() {\n    if(this.reversedExprs == null) {\n      return super.getColumnExprMapForExplain();\n    }\n    Map<String, String> explainColMap = new HashMap<>();\n    for(String col:this.colExprMap.keySet()){\n      String taggedCol = this.reversedExprs.get(col) + \":\" + this.colExprMap.get(col);\n      explainColMap.put(col, taggedCol);\n    }\n    return explainColMap;\n  }",
            " 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389 +\n 390 +\n 391  \n 392  \n 393  \n 394  ",
            "  @Override\n  @Explain(displayName = \"columnExprMap\", jsonOnly = true)\n  public Map<String, String> getColumnExprMapForExplain() {\n    if(this.reversedExprs == null) {\n      return super.getColumnExprMapForExplain();\n    }\n    Map<String, String> explainColMap = new HashMap<>();\n    for(String col:this.colExprMap.keySet()){\n      String taggedCol = this.reversedExprs.get(col) + \":\"\n          + this.colExprMap.get(col).getExprString();\n      explainColMap.put(col, taggedCol);\n    }\n    return explainColMap;\n  }"
        ]
    ],
    "f99a6e84447874a6a2453c62c34e6756492aae12": [
        [
            "HiveOnTezCostModel::TezBucketJoinAlgorithm::getCollation(HiveJoin)",
            " 457  \n 458  \n 459 -\n 460 -\n 461 -\n 462  \n 463  \n 464  \n 465  ",
            "    @Override\n    public ImmutableList<RelCollation> getCollation(HiveJoin join) {\n      if (join.getStreamingSide() != MapJoinStreamingRelation.LEFT_RELATION\n              || join.getStreamingSide() != MapJoinStreamingRelation.RIGHT_RELATION) {\n        return null;\n      }\n      return HiveAlgorithmsUtil.getJoinCollation(join.getJoinPredicateInfo(),\n              join.getStreamingSide());\n    }",
            " 464  \n 465  \n 466 +\n 467 +\n 468 +\n 469 +\n 470 +\n 471 +\n 472  \n 473  \n 474  \n 475  ",
            "    @Override\n    public ImmutableList<RelCollation> getCollation(HiveJoin join) {\n      final MapJoinStreamingRelation streamingSide = join.getStreamingSide();\n      if (streamingSide != MapJoinStreamingRelation.LEFT_RELATION\n              && streamingSide != MapJoinStreamingRelation.RIGHT_RELATION) {\n        // Error; default value\n        LOG.warn(\"Streaming side for map join not chosen\");\n        return ImmutableList.of();\n      }\n      return HiveAlgorithmsUtil.getJoinCollation(join.getJoinPredicateInfo(),\n              join.getStreamingSide());\n    }"
        ],
        [
            "HiveOnTezCostModel::TezMapJoinAlgorithm::getDistribution(HiveJoin)",
            " 301  \n 302  \n 303 -\n 304 -\n 305 -\n 306  \n 307  \n 308  \n 309  ",
            "    @Override\n    public RelDistribution getDistribution(HiveJoin join) {\n      if (join.getStreamingSide() != MapJoinStreamingRelation.LEFT_RELATION\n              || join.getStreamingSide() != MapJoinStreamingRelation.RIGHT_RELATION) {\n        return null;\n      }\n      return HiveAlgorithmsUtil.getJoinDistribution(join.getJoinPredicateInfo(),\n              join.getStreamingSide());\n    }",
            " 305  \n 306  \n 307 +\n 308 +\n 309 +\n 310 +\n 311 +\n 312 +\n 313  \n 314  \n 315  \n 316  ",
            "    @Override\n    public RelDistribution getDistribution(HiveJoin join) {\n      final MapJoinStreamingRelation streamingSide = join.getStreamingSide();\n      if (streamingSide != MapJoinStreamingRelation.LEFT_RELATION\n              && streamingSide != MapJoinStreamingRelation.RIGHT_RELATION) {\n        // Error; default value\n        LOG.warn(\"Streaming side for map join not chosen\");\n        return RelDistributions.SINGLETON;\n      }\n      return HiveAlgorithmsUtil.getJoinDistribution(join.getJoinPredicateInfo(),\n              join.getStreamingSide());\n    }"
        ],
        [
            "HiveOnTezCostModel::TezMapJoinAlgorithm::getCollation(HiveJoin)",
            " 291  \n 292  \n 293 -\n 294 -\n 295 -\n 296  \n 297  \n 298  \n 299  ",
            "    @Override\n    public ImmutableList<RelCollation> getCollation(HiveJoin join) {\n      if (join.getStreamingSide() != MapJoinStreamingRelation.LEFT_RELATION\n              || join.getStreamingSide() != MapJoinStreamingRelation.RIGHT_RELATION) {\n        return null;\n      }\n      return HiveAlgorithmsUtil.getJoinCollation(join.getJoinPredicateInfo(),\n              join.getStreamingSide());\n    }",
            " 292  \n 293  \n 294 +\n 295 +\n 296 +\n 297 +\n 298 +\n 299 +\n 300  \n 301  \n 302  \n 303  ",
            "    @Override\n    public ImmutableList<RelCollation> getCollation(HiveJoin join) {\n      final MapJoinStreamingRelation streamingSide = join.getStreamingSide();\n      if (streamingSide != MapJoinStreamingRelation.LEFT_RELATION\n              && streamingSide != MapJoinStreamingRelation.RIGHT_RELATION) {\n        // Error; default value\n        LOG.warn(\"Streaming side for map join not chosen\");\n        return ImmutableList.of();\n      }\n      return HiveAlgorithmsUtil.getJoinCollation(join.getJoinPredicateInfo(),\n              join.getStreamingSide());\n    }"
        ]
    ],
    "f56abb4054cbc4ba8c8511596117f7823d60dbe6": [
        [
            "DruidQueryBasedInputFormat::distributeSelectQuery(Configuration,String,SelectQuery,Path)",
            " 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  ",
            "  private static HiveDruidSplit[] distributeSelectQuery(Configuration conf, String address,\n      SelectQuery query, Path dummyPath) throws IOException {\n    // If it has a limit, we use it and we do not distribute the query\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath,\n              new String[]{address} ) };\n    }\n\n    // Properties from configuration\n    final int numConnection = HiveConf.getIntVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    // Create request to obtain nodes that are holding data for the given datasource and intervals\n    final Lifecycle lifecycle = new Lifecycle();\n    final HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Lifecycle start issue\");\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    final String intervals =\n            StringUtils.join(query.getIntervals(), \",\"); // Comma-separated intervals without brackets\n    final String request = String.format(\n            \"http://%s/druid/v2/datasources/%s/candidates?intervals=%s\",\n            address, query.getDataSource().getNames().get(0), intervals);\n    final InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client, new Request(HttpMethod.GET, new URL(request)));\n    } catch (Exception e) {\n      lifecycle.stop();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    final List<LocatedSegmentDescriptor> segmentDescriptors;\n    try {\n      segmentDescriptors = DruidStorageHandlerUtils.JSON_MAPPER.readValue(response,\n              new TypeReference<List<LocatedSegmentDescriptor>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      lifecycle.stop();\n    }\n\n    // Create one input split for each segment\n    final int numSplits = segmentDescriptors.size();\n    final HiveDruidSplit[] splits = new HiveDruidSplit[segmentDescriptors.size()];\n    for (int i = 0; i < numSplits; i++) {\n      final LocatedSegmentDescriptor locatedSD = segmentDescriptors.get(i);\n      final String[] hosts = new String[locatedSD.getLocations().size()];\n      for (int j = 0; j < locatedSD.getLocations().size(); j++) {\n        hosts[j] = locatedSD.getLocations().get(j).getHost();\n      }\n      // Create partial Select query\n      final SegmentDescriptor newSD = new SegmentDescriptor(\n              locatedSD.getInterval(), locatedSD.getVersion(), locatedSD.getPartitionNumber());\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleSpecificSegmentSpec(Lists.newArrayList(newSD)));\n      splits[i] = new HiveDruidSplit(DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery),\n              dummyPath, hosts);\n    }\n    return splits;\n  }",
            " 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217 +\n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "  private static HiveDruidSplit[] distributeSelectQuery(Configuration conf, String address,\n      SelectQuery query, Path dummyPath) throws IOException {\n    // If it has a limit, we use it and we do not distribute the query\n    final boolean isFetch = query.getContextBoolean(Constants.DRUID_QUERY_FETCH, false);\n    if (isFetch) {\n      return new HiveDruidSplit[] { new HiveDruidSplit(\n              DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(query), dummyPath,\n              new String[]{address} ) };\n    }\n\n    // Properties from configuration\n    final int numConnection = HiveConf.getIntVar(conf,\n            HiveConf.ConfVars.HIVE_DRUID_NUM_HTTP_CONNECTION);\n    final Period readTimeout = new Period(\n            HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_DRUID_HTTP_READ_TIMEOUT));\n\n    // Create request to obtain nodes that are holding data for the given datasource and intervals\n    final Lifecycle lifecycle = new Lifecycle();\n    final HttpClient client = HttpClientInit.createClient(\n            HttpClientConfig.builder().withNumConnections(numConnection)\n                    .withReadTimeout(readTimeout.toStandardDuration()).build(), lifecycle);\n    try {\n      lifecycle.start();\n    } catch (Exception e) {\n      LOG.error(\"Lifecycle start issue\");\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n    final String intervals =\n            StringUtils.join(query.getIntervals(), \",\"); // Comma-separated intervals without brackets\n    final String request = String.format(\n            \"http://%s/druid/v2/datasources/%s/candidates?intervals=%s\",\n            address, query.getDataSource().getNames().get(0), URLEncoder.encode(intervals, \"UTF-8\"));\n    final InputStream response;\n    try {\n      response = DruidStorageHandlerUtils.submitRequest(client, new Request(HttpMethod.GET, new URL(request)));\n    } catch (Exception e) {\n      lifecycle.stop();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    }\n\n    // Retrieve results\n    final List<LocatedSegmentDescriptor> segmentDescriptors;\n    try {\n      segmentDescriptors = DruidStorageHandlerUtils.JSON_MAPPER.readValue(response,\n              new TypeReference<List<LocatedSegmentDescriptor>>() {});\n    } catch (Exception e) {\n      response.close();\n      throw new IOException(org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      lifecycle.stop();\n    }\n\n    // Create one input split for each segment\n    final int numSplits = segmentDescriptors.size();\n    final HiveDruidSplit[] splits = new HiveDruidSplit[segmentDescriptors.size()];\n    for (int i = 0; i < numSplits; i++) {\n      final LocatedSegmentDescriptor locatedSD = segmentDescriptors.get(i);\n      final String[] hosts = new String[locatedSD.getLocations().size()];\n      for (int j = 0; j < locatedSD.getLocations().size(); j++) {\n        hosts[j] = locatedSD.getLocations().get(j).getHost();\n      }\n      // Create partial Select query\n      final SegmentDescriptor newSD = new SegmentDescriptor(\n              locatedSD.getInterval(), locatedSD.getVersion(), locatedSD.getPartitionNumber());\n      final SelectQuery partialQuery = query.withQuerySegmentSpec(\n              new MultipleSpecificSegmentSpec(Lists.newArrayList(newSD)));\n      splits[i] = new HiveDruidSplit(DruidStorageHandlerUtils.JSON_MAPPER.writeValueAsString(partialQuery),\n              dummyPath, hosts);\n    }\n    return splits;\n  }"
        ]
    ],
    "464a3f61a0c4a1c4e44a1ce427f604295534e969": [
        [
            "TestDruidStorageHandler::testCommitMultiInsertOverwriteTable()",
            " 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377 -\n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428 -\n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458 -\n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  ",
            "  @Test\n  public void testCommitMultiInsertOverwriteTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    druidStorageHandler.preCreateTable(tableMock);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    // This create and publish the segment to be overwritten\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    // Check that there is one datasource with the published segment\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    // Sequence is the following:\n    // 1) INSERT with no segments -> Original segment still present in the datasource\n    // 2) INSERT OVERWRITE with no segments -> Datasource is empty\n    // 3) INSERT OVERWRITE with no segments -> Datasource is empty\n    // 4) INSERT with no segments -> Datasource is empty\n    // 5) INSERT with one segment -> Datasource has one segment\n    // 6) INSERT OVERWRITE with one segment -> Datasource has one segment\n    // 7) INSERT with one segment -> Datasource has two segments\n    // 8) INSERT OVERWRITE with no segments -> Datasource is empty\n\n    // We start:\n    // #1\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #2\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #3\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #4\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #5\n    DataSegment dataSegment1 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(180, 250), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath1 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment1,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment1, descriptorPath1);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #6\n    DataSegment dataSegment2 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(200, 250), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath2 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment2,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment2, descriptorPath2);\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #7\n    DataSegment dataSegment3 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 200), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath3 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment3,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment3, descriptorPath3);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(2, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #8\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n  }",
            " 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378 +\n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429 +\n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444 +\n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459 +\n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  ",
            "  @Test\n  public void testCommitMultiInsertOverwriteTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    druidStorageHandler.preCreateTable(tableMock);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    // This create and publish the segment to be overwritten\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    // Check that there is one datasource with the published segment\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    // Sequence is the following:\n    // 1) INSERT with no segments -> Original segment still present in the datasource\n    // 2) INSERT OVERWRITE with no segments -> Datasource is empty\n    // 3) INSERT OVERWRITE with no segments -> Datasource is empty\n    // 4) INSERT with no segments -> Datasource is empty\n    // 5) INSERT with one segment -> Datasource has one segment\n    // 6) INSERT OVERWRITE with one segment -> Datasource has one segment\n    // 7) INSERT with one segment -> Datasource has two segments\n    // 8) INSERT OVERWRITE with no segments -> Datasource is empty\n\n    // We start:\n    // #1\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #2\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #3\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #4\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #5\n    DataSegment dataSegment1 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(180, 250, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath1 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment1,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment1, descriptorPath1);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #6\n    DataSegment dataSegment2 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(200, 250, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath2 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment2,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment2, descriptorPath2);\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(1, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #7\n    DataSegment dataSegment3 = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 200, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath3 = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment3,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment3, descriptorPath3);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n    Assert.assertEquals(2, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n\n    // #8\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertEquals(0, getUsedSegmentsList(connector,\n            metadataStorageTablesConfig).size());\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertOverwriteTable()",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324 -\n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335 -\n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  ",
            "  @Test\n  public void testCommitInsertOverwriteTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    // This create and publish the segment to be overwritten\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // This creates and publish new segment\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(180, 250), \"v1\", new LinearShardSpec(0));\n\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(1, dataSegmentList.size());\n    DataSegment persistedSegment = Iterables.getOnlyElement(dataSegmentList);\n    Assert.assertEquals(dataSegment, persistedSegment);\n    Assert.assertEquals(dataSegment.getVersion(), persistedSegment.getVersion());\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }",
            " 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325 +\n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336 +\n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  ",
            "  @Test\n  public void testCommitInsertOverwriteTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    // This create and publish the segment to be overwritten\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // This creates and publish new segment\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(180, 250, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n\n    druidStorageHandler.commitInsertTable(tableMock, true);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(1, dataSegmentList.size());\n    DataSegment persistedSegment = Iterables.getOnlyElement(dataSegmentList);\n    Assert.assertEquals(dataSegment, persistedSegment);\n    Assert.assertEquals(dataSegment.getVersion(), persistedSegment.getVersion());\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertIntoTable()",
            " 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517 -\n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530 -\n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  ",
            "  @Test\n  public void testCommitInsertIntoTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(1)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    // Insert into appends to old version\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    Assert.assertEquals(2, persistedSegment.getShardSpec().getPartitionNum());\n\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }",
            " 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518 +\n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531 +\n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  ",
            "  @Test\n  public void testCommitInsertIntoTable() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(1)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    // Insert into appends to old version\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    Assert.assertEquals(2, persistedSegment.getShardSpec().getPartitionNum());\n\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertIntoWithConflictingIntervalSegment()",
            " 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685 -\n 686  \n 687  \n 688 -\n 689  \n 690  \n 691 -\n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706 -\n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  ",
            "  @Test(expected = IllegalStateException.class)\n  public void testCommitInsertIntoWithConflictingIntervalSegment()\n          throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays.asList(\n            createSegment(new Path(taskDirPath, \"index_old_1.zip\").toString(),\n                    new Interval(100, 150),\n                    \"v0\", new LinearShardSpec(0)),\n            createSegment(new Path(taskDirPath, \"index_old_2.zip\").toString(),\n                    new Interval(150, 200),\n                    \"v0\", new LinearShardSpec(0)),\n            createSegment(new Path(taskDirPath, \"index_old_3.zip\").toString(),\n                    new Interval(200, 300),\n                    \"v0\", new LinearShardSpec(0)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(taskDirPath.toString());\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // Try appending segment with conflicting interval\n    DataSegment conflictingSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 300), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils\n            .makeSegmentDescriptorOutputPath(conflictingSegment,\n                    new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n            );\n    DruidStorageHandlerUtils\n            .writeSegmentDescriptor(localFileSystem, conflictingSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n  }",
            " 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686 +\n 687  \n 688  \n 689 +\n 690  \n 691  \n 692 +\n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707 +\n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  ",
            "  @Test(expected = IllegalStateException.class)\n  public void testCommitInsertIntoWithConflictingIntervalSegment()\n          throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays.asList(\n            createSegment(new Path(taskDirPath, \"index_old_1.zip\").toString(),\n                    new Interval(100, 150, DateTimeZone.UTC),\n                    \"v0\", new LinearShardSpec(0)),\n            createSegment(new Path(taskDirPath, \"index_old_2.zip\").toString(),\n                    new Interval(150, 200, DateTimeZone.UTC),\n                    \"v0\", new LinearShardSpec(0)),\n            createSegment(new Path(taskDirPath, \"index_old_3.zip\").toString(),\n                    new Interval(200, 300, DateTimeZone.UTC),\n                    \"v0\", new LinearShardSpec(0)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(taskDirPath.toString());\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // Try appending segment with conflicting interval\n    DataSegment conflictingSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 300, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils\n            .makeSegmentDescriptorOutputPath(conflictingSegment,\n                    new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n            );\n    DruidStorageHandlerUtils\n            .writeSegmentDescriptor(localFileSystem, conflictingSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n  }"
        ],
        [
            "TestDruidStorageHandler::testInsertIntoAppendOneMorePartition()",
            " 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574 -\n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584 -\n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  ",
            "  @Test\n  public void testInsertIntoAppendOneMorePartition() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150), \"v0\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    Assert.assertEquals(1, persistedSegment.getShardSpec().getPartitionNum());\n\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }",
            " 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575 +\n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585 +\n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  ",
            "  @Test\n  public void testInsertIntoAppendOneMorePartition() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)));\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    Assert.assertEquals(1, persistedSegment.getShardSpec().getPartitionNum());\n\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertIntoWithNonExtendableSegment()",
            " 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726 -\n 727  \n 728 -\n 729  \n 730 -\n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744 -\n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  ",
            "  @Test(expected = IllegalStateException.class)\n  public void testCommitInsertIntoWithNonExtendableSegment() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, \"index_old_1.zip\").toString(),\n                    new Interval(100, 150), \"v0\", new NoneShardSpec()),\n                    createSegment(new Path(taskDirPath, \"index_old_2.zip\").toString(),\n                            new Interval(200, 250), \"v0\", new LinearShardSpec(0)),\n                    createSegment(new Path(taskDirPath, \"index_old_3.zip\").toString(),\n                            new Interval(250, 300), \"v0\", new LinearShardSpec(0)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(taskDirPath.toString());\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // Try appending to non extendable shard spec\n    DataSegment conflictingSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils\n            .makeSegmentDescriptorOutputPath(conflictingSegment,\n                    new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n            );\n    DruidStorageHandlerUtils\n            .writeSegmentDescriptor(localFileSystem, conflictingSegment, descriptorPath);\n\n    druidStorageHandler.commitInsertTable(tableMock, false);\n\n  }",
            " 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727 +\n 728  \n 729 +\n 730  \n 731 +\n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745 +\n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  ",
            "  @Test(expected = IllegalStateException.class)\n  public void testCommitInsertIntoWithNonExtendableSegment() throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, \"index_old_1.zip\").toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new NoneShardSpec()),\n                    createSegment(new Path(taskDirPath, \"index_old_2.zip\").toString(),\n                            new Interval(200, 250, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)),\n                    createSegment(new Path(taskDirPath, \"index_old_3.zip\").toString(),\n                            new Interval(250, 300, DateTimeZone.UTC), \"v0\", new LinearShardSpec(0)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(taskDirPath.toString());\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n\n    // Try appending to non extendable shard spec\n    DataSegment conflictingSegment = createSegment(new Path(taskDirPath, DruidStorageHandlerUtils.INDEX_ZIP).toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils\n            .makeSegmentDescriptorOutputPath(conflictingSegment,\n                    new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n            );\n    DruidStorageHandlerUtils\n            .writeSegmentDescriptor(localFileSystem, conflictingSegment, descriptorPath);\n\n    druidStorageHandler.commitInsertTable(tableMock, false);\n\n  }"
        ],
        [
            "TestDruidStorageHandler::testCommitInsertIntoWhenDestinationSegmentFileExist()",
            " 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624 -\n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636 -\n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644 -\n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  ",
            "  @Test\n  public void testCommitInsertIntoWhenDestinationSegmentFileExist()\n          throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, \"index_old.zip\").toString(),\n                    new Interval(100, 150), \"v0\", new LinearShardSpec(1)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, \"index.zip\").toString(),\n            new Interval(100, 150), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n\n    // Create segment file at the destination location with LinearShardSpec(2)\n    DataSegment segment = createSegment(new Path(taskDirPath, \"index_conflict.zip\").toString(),\n            new Interval(100, 150), \"v1\", new LinearShardSpec(1));\n    Path segmentPath = new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher.makeIndexPathName(segment, DruidStorageHandlerUtils.INDEX_ZIP));\n    FileUtils.writeStringToFile(new File(segmentPath.toUri()), \"dummy\");\n\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    // Insert into appends to old version\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    // insert into should skip and increment partition number to 3\n    Assert.assertEquals(2, persistedSegment.getShardSpec().getPartitionNum());\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }",
            " 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625 +\n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637 +\n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645 +\n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  ",
            "  @Test\n  public void testCommitInsertIntoWhenDestinationSegmentFileExist()\n          throws MetaException, IOException {\n    DerbyConnectorTestUtility connector = derbyConnectorRule.getConnector();\n    MetadataStorageTablesConfig metadataStorageTablesConfig = derbyConnectorRule\n            .metadataTablesConfigSupplier().get();\n    druidStorageHandler.preCreateTable(tableMock);\n    LocalFileSystem localFileSystem = FileSystem.getLocal(config);\n    Path taskDirPath = new Path(tableWorkingPath, druidStorageHandler.makeStagingName());\n    List<DataSegment> existingSegments = Arrays\n            .asList(createSegment(new Path(taskDirPath, \"index_old.zip\").toString(),\n                    new Interval(100, 150, DateTimeZone.UTC), \"v0\", new LinearShardSpec(1)));\n    HdfsDataSegmentPusherConfig pusherConfig = new HdfsDataSegmentPusherConfig();\n    pusherConfig.setStorageDirectory(config.get(String.valueOf(HiveConf.ConfVars.DRUID_SEGMENT_DIRECTORY)));\n    DataSegmentPusher dataSegmentPusher = new HdfsDataSegmentPusher(pusherConfig, config, DruidStorageHandlerUtils.JSON_MAPPER);\n    DruidStorageHandlerUtils\n            .publishSegmentsAndCommit(connector, metadataStorageTablesConfig, DATA_SOURCE_NAME,\n                    existingSegments,\n                    true,\n                    config,\n                    dataSegmentPusher\n            );\n    DataSegment dataSegment = createSegment(new Path(taskDirPath, \"index.zip\").toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n    Path descriptorPath = DruidStorageHandlerUtils.makeSegmentDescriptorOutputPath(dataSegment,\n            new Path(taskDirPath, DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME)\n    );\n    DruidStorageHandlerUtils.writeSegmentDescriptor(localFileSystem, dataSegment, descriptorPath);\n\n    // Create segment file at the destination location with LinearShardSpec(2)\n    DataSegment segment = createSegment(new Path(taskDirPath, \"index_conflict.zip\").toString(),\n            new Interval(100, 150, DateTimeZone.UTC), \"v1\", new LinearShardSpec(1));\n    Path segmentPath = new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher.makeIndexPathName(segment, DruidStorageHandlerUtils.INDEX_ZIP));\n    FileUtils.writeStringToFile(new File(segmentPath.toUri()), \"dummy\");\n\n    druidStorageHandler.commitInsertTable(tableMock, false);\n    Assert.assertArrayEquals(Lists.newArrayList(DATA_SOURCE_NAME).toArray(), Lists.newArrayList(\n            DruidStorageHandlerUtils.getAllDataSourceNames(connector,\n                    metadataStorageTablesConfig\n            )).toArray());\n\n    final List<DataSegment> dataSegmentList = getUsedSegmentsList(connector,\n            metadataStorageTablesConfig);\n    Assert.assertEquals(2, dataSegmentList.size());\n\n    DataSegment persistedSegment = dataSegmentList.get(1);\n    // Insert into appends to old version\n    Assert.assertEquals(\"v0\", persistedSegment.getVersion());\n    Assert.assertTrue(persistedSegment.getShardSpec() instanceof LinearShardSpec);\n    // insert into should skip and increment partition number to 3\n    Assert.assertEquals(2, persistedSegment.getShardSpec().getPartitionNum());\n    Path expectedFinalHadoopPath =  new Path(dataSegmentPusher.getPathForHadoop(), dataSegmentPusher\n            .makeIndexPathName(persistedSegment, DruidStorageHandlerUtils.INDEX_ZIP));\n\n\n    Assert.assertEquals(ImmutableMap.of(\"type\", \"hdfs\", \"path\", expectedFinalHadoopPath.toString()),\n            persistedSegment.getLoadSpec());\n    Assert.assertEquals(\"dummySegmentData\",\n            FileUtils.readFileToString(new File(expectedFinalHadoopPath.toUri())));\n  }"
        ],
        [
            "TestDruidRecordWriter::verifyRows(List,List)",
            " 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 -\n 234  ",
            "  private void verifyRows(List<ImmutableMap<String, Object>> expectedRows,\n          List<InputRow> actualRows\n  ) {\n    System.out.println(\"actualRows = \" + actualRows);\n    Assert.assertEquals(expectedRows.size(), actualRows.size());\n\n    for (int i = 0; i < expectedRows.size(); i++) {\n      Map<String, Object> expected = expectedRows.get(i);\n      InputRow actual = actualRows.get(i);\n\n      Assert.assertEquals(ImmutableList.of(\"host\"), actual.getDimensions());\n\n      Assert.assertEquals(expected.get(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN),\n              actual.getTimestamp().getMillis()\n      );\n      Assert.assertEquals(expected.get(\"host\"), actual.getDimension(\"host\"));\n      Assert.assertEquals(expected.get(\"visited_sum\"), actual.getLongMetric(\"visited_sum\"));\n      Assert.assertEquals(\n              (Double) expected.get(\"unique_hosts\"),\n              (Double) HyperUniquesAggregatorFactory\n                      .estimateCardinality(actual.getRaw(\"unique_hosts\")),\n              0.001",
            " 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233 +\n 234  ",
            "  private void verifyRows(List<ImmutableMap<String, Object>> expectedRows,\n          List<InputRow> actualRows\n  ) {\n    System.out.println(\"actualRows = \" + actualRows);\n    Assert.assertEquals(expectedRows.size(), actualRows.size());\n\n    for (int i = 0; i < expectedRows.size(); i++) {\n      Map<String, Object> expected = expectedRows.get(i);\n      InputRow actual = actualRows.get(i);\n\n      Assert.assertEquals(ImmutableList.of(\"host\"), actual.getDimensions());\n\n      Assert.assertEquals(expected.get(DruidStorageHandlerUtils.DEFAULT_TIMESTAMP_COLUMN),\n              actual.getTimestamp().getMillis()\n      );\n      Assert.assertEquals(expected.get(\"host\"), actual.getDimension(\"host\"));\n      Assert.assertEquals(expected.get(\"visited_sum\"), actual.getLongMetric(\"visited_sum\"));\n      Assert.assertEquals(\n              (Double) expected.get(\"unique_hosts\"),\n              (Double) HyperUniquesAggregatorFactory\n                      .estimateCardinality(actual.getRaw(\"unique_hosts\"), false),\n              0.001"
        ],
        [
            "TestDruidStorageHandler::createSegment(String)",
            "  90  \n  91 -\n  92  ",
            "  private DataSegment createSegment(String location) throws IOException {\n    return createSegment(location, new Interval(100, 170), \"v1\", new LinearShardSpec(0));\n  }",
            "  91  \n  92 +\n  93  ",
            "  private DataSegment createSegment(String location) throws IOException {\n    return createSegment(location, new Interval(100, 170, DateTimeZone.UTC), \"v1\", new LinearShardSpec(0));\n  }"
        ]
    ],
    "4a33ec8fcae5f7d18105ef62e33150db6e853af5": [
        [
            "HiveConf::StrictChecks::checkNoPartitionFilter(Configuration)",
            "4996  \n4997 -\n4998  \n4999  ",
            "    public static String checkNoPartitionFilter(Configuration conf) {\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY)\n          ? null : NO_PARTITIONLESS_MSG;\n    }",
            "4999  \n5000 +\n5001  \n5002  ",
            "    public static String checkNoPartitionFilter(Configuration conf) {\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_NO_PARTITION_FILTER)\n          ? null : NO_PARTITIONLESS_MSG;\n    }"
        ],
        [
            "HiveConf::StrictChecks::makeMessage(String,ConfVars)",
            "4985  \n4986  \n4987 -\n4988 -\n4989 -\n4990  ",
            "    private static String makeMessage(String what, ConfVars setting) {\n      return what + \" are disabled for safety reasons. If you know what you are doing, please set \"\n          + setting.varname + \" to false and that \" + ConfVars.HIVEMAPREDMODE.varname + \" is not\"\n          + \" set to 'strict' to proceed. Note that if you may get errors or incorrect results if\"\n          + \" you make a mistake while using some of the unsafe features.\";\n    }",
            "4988  \n4989  \n4990 +\n4991 +\n4992 +\n4993  ",
            "    private static String makeMessage(String what, ConfVars setting) {\n      return what + \" are disabled for safety reasons. If you know what you are doing, please set \"\n          + setting.varname + \" to false and make sure that \" + ConfVars.HIVEMAPREDMODE.varname +\n              \" is not set to 'strict' to proceed. Note that you may get errors or incorrect \" +\n              \"results if you make a mistake while using some of the unsafe features.\";\n    }"
        ],
        [
            "HiveConf::StrictChecks::checkNoLimit(Configuration)",
            "4992  \n4993 -\n4994  ",
            "    public static String checkNoLimit(Configuration conf) {\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_LARGE_QUERY) ? null : NO_LIMIT_MSG;\n    }",
            "4995  \n4996 +\n4997  ",
            "    public static String checkNoLimit(Configuration conf) {\n      return isAllowed(conf, ConfVars.HIVE_STRICT_CHECKS_ORDERBY_NO_LIMIT) ? null : NO_LIMIT_MSG;\n    }"
        ]
    ],
    "9e410b29628e0c517086ed2f42cf5b9f4a852397": [
        [
            "ASTConverter::convert()",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 -\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 -\n 159  \n 160  \n 161  \n 162  \n 163  \n 164 -\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176 -\n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 -\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "  private ASTNode convert() throws CalciteSemanticException {\n    /*\n     * 1. Walk RelNode Graph; note from, where, gBy.. nodes.\n     */\n    new QBVisitor().go(root);\n\n    /*\n     * 2. convert from node.\n     */\n    QueryBlockInfo qb = convertSource(from);\n    schema = qb.schema;\n    hiveAST.from = ASTBuilder.construct(HiveParser.TOK_FROM, \"TOK_FROM\").add(qb.ast).node();\n\n    /*\n     * 3. convert filterNode\n     */\n    if (where != null) {\n      ASTNode cond = where.getCondition().accept(new RexVisitor(schema));\n      hiveAST.where = ASTBuilder.where(cond);\n    }\n\n    /*\n     * 4. GBy\n     */\n    if (groupBy != null) {\n      ASTBuilder b;\n      boolean groupingSetsExpression = false;\n      Group aggregateType = groupBy.getGroupType();\n      switch (aggregateType) {\n        case SIMPLE:\n          b = ASTBuilder.construct(HiveParser.TOK_GROUPBY, \"TOK_GROUPBY\");\n          break;\n        case ROLLUP:\n          b = ASTBuilder.construct(HiveParser.TOK_ROLLUP_GROUPBY, \"TOK_ROLLUP_GROUPBY\");\n          break;\n        case CUBE:\n          b = ASTBuilder.construct(HiveParser.TOK_CUBE_GROUPBY, \"TOK_CUBE_GROUPBY\");\n          break;\n        case OTHER:\n          b = ASTBuilder.construct(HiveParser.TOK_GROUPING_SETS, \"TOK_GROUPING_SETS\");\n          groupingSetsExpression = true;\n          break;\n        default:\n          throw new CalciteSemanticException(\"Group type not recognized\");\n      }\n\n      HiveAggregate hiveAgg = (HiveAggregate) groupBy;\n      for (int pos : hiveAgg.getAggregateColumnsOrder()) {\n        RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n            groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n        b.add(iRef.accept(new RexVisitor(schema)));\n      }\n      for (int pos = 0; pos < groupBy.getGroupCount(); pos++) {\n        if (!hiveAgg.getAggregateColumnsOrder().contains(pos)) {\n          RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n              groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n          b.add(iRef.accept(new RexVisitor(schema)));\n        }\n      }\n\n      //Grouping sets expressions\n      if(groupingSetsExpression) {\n        for(ImmutableBitSet groupSet: groupBy.getGroupSets()) {\n          ASTBuilder expression = ASTBuilder.construct(\n                  HiveParser.TOK_GROUPING_SETS_EXPRESSION, \"TOK_GROUPING_SETS_EXPRESSION\");\n          for (int i : groupSet) {\n            RexInputRef iRef = new RexInputRef(i, groupBy.getCluster().getTypeFactory()\n                .createSqlType(SqlTypeName.ANY));\n            expression.add(iRef.accept(new RexVisitor(schema)));\n          }\n          b.add(expression);\n        }\n      }\n\n      if (!groupBy.getGroupSet().isEmpty()) {\n        hiveAST.groupBy = b.node();\n      }\n\n      schema = new Schema(schema, groupBy);\n    }\n\n    /*\n     * 5. Having\n     */\n    if (having != null) {\n      ASTNode cond = having.getCondition().accept(new RexVisitor(schema));\n      hiveAST.having = ASTBuilder.having(cond);\n    }\n\n    /*\n     * 6. Project\n     */\n    ASTBuilder b = ASTBuilder.construct(HiveParser.TOK_SELECT, \"TOK_SELECT\");\n\n    if (select instanceof Project) {\n      List<RexNode> childExps = ((Project) select).getChildExps();\n      if (childExps.isEmpty()) {\n        RexLiteral r = select.getCluster().getRexBuilder().makeExactLiteral(new BigDecimal(1));\n        ASTNode selectExpr = ASTBuilder.selectExpr(ASTBuilder.literal(r), \"1\");\n        b.add(selectExpr);\n      } else {\n        int i = 0;\n\n        for (RexNode r : childExps) {\n          ASTNode expr = r.accept(new RexVisitor(schema, r instanceof RexLiteral,\n              select.getCluster().getRexBuilder()));\n          String alias = select.getRowType().getFieldNames().get(i++);\n          ASTNode selectExpr = ASTBuilder.selectExpr(expr, alias);\n          b.add(selectExpr);\n        }\n      }\n      hiveAST.select = b.node();\n    } else {\n      // select is UDTF\n      HiveTableFunctionScan udtf = (HiveTableFunctionScan) select;\n      List<ASTNode> children = new ArrayList<>();\n      RexCall call = (RexCall) udtf.getCall();\n      for (RexNode r : call.getOperands()) {\n        ASTNode expr = r.accept(new RexVisitor(schema, r instanceof RexLiteral,\n            select.getCluster().getRexBuilder()));\n        children.add(expr);\n      }\n      ASTBuilder sel = ASTBuilder.construct(HiveParser.TOK_SELEXPR, \"TOK_SELEXPR\");\n      ASTNode function = buildUDTFAST(call.getOperator().getName(), children);\n      sel.add(function);\n      for (String alias : udtf.getRowType().getFieldNames()) {\n        sel.add(HiveParser.Identifier, alias);\n      }\n      b.add(sel);\n      hiveAST.select = b.node();\n    }\n\n    /*\n     * 7. Order Use in Order By from the block above. RelNode has no pointer to\n     * parent hence we need to go top down; but OB at each block really belong\n     * to its src/from. Hence the need to pass in sort for each block from\n     * its parent.\n     * 8. Limit\n     */\n    convertOrderLimitToASTNode((HiveSortLimit) orderLimit);\n\n    return hiveAST.getAST();\n  }",
            " 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158 +\n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176 +\n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193 +\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "  private ASTNode convert() throws CalciteSemanticException {\n    /*\n     * 1. Walk RelNode Graph; note from, where, gBy.. nodes.\n     */\n    new QBVisitor().go(root);\n\n    /*\n     * 2. convert from node.\n     */\n    QueryBlockInfo qb = convertSource(from);\n    schema = qb.schema;\n    hiveAST.from = ASTBuilder.construct(HiveParser.TOK_FROM, \"TOK_FROM\").add(qb.ast).node();\n\n    /*\n     * 3. convert filterNode\n     */\n    if (where != null) {\n      ASTNode cond = where.getCondition().accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder()));\n      hiveAST.where = ASTBuilder.where(cond);\n    }\n\n    /*\n     * 4. GBy\n     */\n    if (groupBy != null) {\n      ASTBuilder b;\n      boolean groupingSetsExpression = false;\n      Group aggregateType = groupBy.getGroupType();\n      switch (aggregateType) {\n        case SIMPLE:\n          b = ASTBuilder.construct(HiveParser.TOK_GROUPBY, \"TOK_GROUPBY\");\n          break;\n        case ROLLUP:\n          b = ASTBuilder.construct(HiveParser.TOK_ROLLUP_GROUPBY, \"TOK_ROLLUP_GROUPBY\");\n          break;\n        case CUBE:\n          b = ASTBuilder.construct(HiveParser.TOK_CUBE_GROUPBY, \"TOK_CUBE_GROUPBY\");\n          break;\n        case OTHER:\n          b = ASTBuilder.construct(HiveParser.TOK_GROUPING_SETS, \"TOK_GROUPING_SETS\");\n          groupingSetsExpression = true;\n          break;\n        default:\n          throw new CalciteSemanticException(\"Group type not recognized\");\n      }\n\n      HiveAggregate hiveAgg = (HiveAggregate) groupBy;\n      for (int pos : hiveAgg.getAggregateColumnsOrder()) {\n        RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n            groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n        b.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n      }\n      for (int pos = 0; pos < groupBy.getGroupCount(); pos++) {\n        if (!hiveAgg.getAggregateColumnsOrder().contains(pos)) {\n          RexInputRef iRef = new RexInputRef(groupBy.getGroupSet().nth(pos),\n              groupBy.getCluster().getTypeFactory().createSqlType(SqlTypeName.ANY));\n          b.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n        }\n      }\n\n      //Grouping sets expressions\n      if(groupingSetsExpression) {\n        for(ImmutableBitSet groupSet: groupBy.getGroupSets()) {\n          ASTBuilder expression = ASTBuilder.construct(\n                  HiveParser.TOK_GROUPING_SETS_EXPRESSION, \"TOK_GROUPING_SETS_EXPRESSION\");\n          for (int i : groupSet) {\n            RexInputRef iRef = new RexInputRef(i, groupBy.getCluster().getTypeFactory()\n                .createSqlType(SqlTypeName.ANY));\n            expression.add(iRef.accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder())));\n          }\n          b.add(expression);\n        }\n      }\n\n      if (!groupBy.getGroupSet().isEmpty()) {\n        hiveAST.groupBy = b.node();\n      }\n\n      schema = new Schema(schema, groupBy);\n    }\n\n    /*\n     * 5. Having\n     */\n    if (having != null) {\n      ASTNode cond = having.getCondition().accept(new RexVisitor(schema, false, root.getCluster().getRexBuilder()));\n      hiveAST.having = ASTBuilder.having(cond);\n    }\n\n    /*\n     * 6. Project\n     */\n    ASTBuilder b = ASTBuilder.construct(HiveParser.TOK_SELECT, \"TOK_SELECT\");\n\n    if (select instanceof Project) {\n      List<RexNode> childExps = ((Project) select).getChildExps();\n      if (childExps.isEmpty()) {\n        RexLiteral r = select.getCluster().getRexBuilder().makeExactLiteral(new BigDecimal(1));\n        ASTNode selectExpr = ASTBuilder.selectExpr(ASTBuilder.literal(r), \"1\");\n        b.add(selectExpr);\n      } else {\n        int i = 0;\n\n        for (RexNode r : childExps) {\n          ASTNode expr = r.accept(new RexVisitor(schema, r instanceof RexLiteral,\n              select.getCluster().getRexBuilder()));\n          String alias = select.getRowType().getFieldNames().get(i++);\n          ASTNode selectExpr = ASTBuilder.selectExpr(expr, alias);\n          b.add(selectExpr);\n        }\n      }\n      hiveAST.select = b.node();\n    } else {\n      // select is UDTF\n      HiveTableFunctionScan udtf = (HiveTableFunctionScan) select;\n      List<ASTNode> children = new ArrayList<>();\n      RexCall call = (RexCall) udtf.getCall();\n      for (RexNode r : call.getOperands()) {\n        ASTNode expr = r.accept(new RexVisitor(schema, r instanceof RexLiteral,\n            select.getCluster().getRexBuilder()));\n        children.add(expr);\n      }\n      ASTBuilder sel = ASTBuilder.construct(HiveParser.TOK_SELEXPR, \"TOK_SELEXPR\");\n      ASTNode function = buildUDTFAST(call.getOperator().getName(), children);\n      sel.add(function);\n      for (String alias : udtf.getRowType().getFieldNames()) {\n        sel.add(HiveParser.Identifier, alias);\n      }\n      b.add(sel);\n      hiveAST.select = b.node();\n    }\n\n    /*\n     * 7. Order Use in Order By from the block above. RelNode has no pointer to\n     * parent hence we need to go top down; but OB at each block really belong\n     * to its src/from. Hence the need to pass in sort for each block from\n     * its parent.\n     * 8. Limit\n     */\n    convertOrderLimitToASTNode((HiveSortLimit) orderLimit);\n\n    return hiveAST.getAST();\n  }"
        ],
        [
            "ASTConverter::Schema::Schema(Schema,Aggregate)",
            " 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787 -\n 788  \n 789  \n 790  \n 791  ",
            "    Schema(Schema src, Aggregate gBy) {\n      for (int i : gBy.getGroupSet()) {\n        ColumnInfo cI = src.get(i);\n        add(cI);\n      }\n      List<AggregateCall> aggs = gBy.getAggCallList();\n      for (AggregateCall agg : aggs) {\n        if (agg.getAggregation() == HiveGroupingID.INSTANCE) {\n          add(new ColumnInfo(null,VirtualColumn.GROUPINGID.getName()));\n          continue;\n        }\n        int argCount = agg.getArgList().size();\n        ASTBuilder b = agg.isDistinct() ? ASTBuilder.construct(HiveParser.TOK_FUNCTIONDI,\n            \"TOK_FUNCTIONDI\") : argCount == 0 ? ASTBuilder.construct(HiveParser.TOK_FUNCTIONSTAR,\n            \"TOK_FUNCTIONSTAR\") : ASTBuilder.construct(HiveParser.TOK_FUNCTION, \"TOK_FUNCTION\");\n        b.add(HiveParser.Identifier, agg.getAggregation().getName());\n        for (int i : agg.getArgList()) {\n          RexInputRef iRef = new RexInputRef(i, gBy.getCluster().getTypeFactory()\n              .createSqlType(SqlTypeName.ANY));\n          b.add(iRef.accept(new RexVisitor(src)));\n        }\n        add(new ColumnInfo(null, b.node()));\n      }\n    }",
            " 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811 +\n 812  \n 813  \n 814  \n 815  ",
            "    Schema(Schema src, Aggregate gBy) {\n      for (int i : gBy.getGroupSet()) {\n        ColumnInfo cI = src.get(i);\n        add(cI);\n      }\n      List<AggregateCall> aggs = gBy.getAggCallList();\n      for (AggregateCall agg : aggs) {\n        if (agg.getAggregation() == HiveGroupingID.INSTANCE) {\n          add(new ColumnInfo(null,VirtualColumn.GROUPINGID.getName()));\n          continue;\n        }\n        int argCount = agg.getArgList().size();\n        ASTBuilder b = agg.isDistinct() ? ASTBuilder.construct(HiveParser.TOK_FUNCTIONDI,\n            \"TOK_FUNCTIONDI\") : argCount == 0 ? ASTBuilder.construct(HiveParser.TOK_FUNCTIONSTAR,\n            \"TOK_FUNCTIONSTAR\") : ASTBuilder.construct(HiveParser.TOK_FUNCTION, \"TOK_FUNCTION\");\n        b.add(HiveParser.Identifier, agg.getAggregation().getName());\n        for (int i : agg.getArgList()) {\n          RexInputRef iRef = new RexInputRef(i, gBy.getCluster().getTypeFactory()\n              .createSqlType(SqlTypeName.ANY));\n          b.add(iRef.accept(new RexVisitor(src, false, gBy.getCluster().getRexBuilder())));\n        }\n        add(new ColumnInfo(null, b.node()));\n      }\n    }"
        ],
        [
            "ASTConverter::convertSource(RelNode)",
            " 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356 -\n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  ",
            "  private QueryBlockInfo convertSource(RelNode r) throws CalciteSemanticException {\n    Schema s = null;\n    ASTNode ast = null;\n\n    if (r instanceof TableScan) {\n      TableScan f = (TableScan) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof DruidQuery) {\n      DruidQuery f = (DruidQuery) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof Join) {\n      Join join = (Join) r;\n      QueryBlockInfo left = convertSource(join.getLeft());\n      QueryBlockInfo right = convertSource(join.getRight());\n      s = new Schema(left.schema, right.schema);\n      ASTNode cond = join.getCondition().accept(new RexVisitor(s));\n      boolean semiJoin = join instanceof SemiJoin;\n      if (join.getRight() instanceof Join && !semiJoin) {\n          // should not be done for semijoin since it will change the semantics\n        // Invert join inputs; this is done because otherwise the SemanticAnalyzer\n        // methods to merge joins will not kick in\n        JoinRelType type;\n        if (join.getJoinType() == JoinRelType.LEFT) {\n          type = JoinRelType.RIGHT;\n        } else if (join.getJoinType() == JoinRelType.RIGHT) {\n          type = JoinRelType.LEFT;\n        } else {\n          type = join.getJoinType();\n        }\n        ast = ASTBuilder.join(right.ast, left.ast, type, cond, semiJoin);\n      } else {\n        ast = ASTBuilder.join(left.ast, right.ast, join.getJoinType(), cond, semiJoin);\n      }\n      if (semiJoin) {\n        s = left.schema;\n      }\n    } else if (r instanceof Union) {\n      Union u = ((Union) r);\n      ASTNode left = new ASTConverter(((Union) r).getInput(0), this.derivedTableCount).convert();\n      for (int ind = 1; ind < u.getInputs().size(); ind++) {\n        left = getUnionAllAST(left, new ASTConverter(((Union) r).getInput(ind),\n            this.derivedTableCount).convert());\n        String sqAlias = nextAlias();\n        ast = ASTBuilder.subQuery(left, sqAlias);\n        s = new Schema((Union) r, sqAlias);\n      }\n    } else {\n      ASTConverter src = new ASTConverter(r, this.derivedTableCount);\n      ASTNode srcAST = src.convert();\n      String sqAlias = nextAlias();\n      s = src.getRowSchema(sqAlias);\n      ast = ASTBuilder.subQuery(srcAST, sqAlias);\n    }\n    return new QueryBlockInfo(s, ast);\n  }",
            " 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356 +\n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  ",
            "  private QueryBlockInfo convertSource(RelNode r) throws CalciteSemanticException {\n    Schema s = null;\n    ASTNode ast = null;\n\n    if (r instanceof TableScan) {\n      TableScan f = (TableScan) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof DruidQuery) {\n      DruidQuery f = (DruidQuery) r;\n      s = new Schema(f);\n      ast = ASTBuilder.table(f);\n    } else if (r instanceof Join) {\n      Join join = (Join) r;\n      QueryBlockInfo left = convertSource(join.getLeft());\n      QueryBlockInfo right = convertSource(join.getRight());\n      s = new Schema(left.schema, right.schema);\n      ASTNode cond = join.getCondition().accept(new RexVisitor(s, false, r.getCluster().getRexBuilder()));\n      boolean semiJoin = join instanceof SemiJoin;\n      if (join.getRight() instanceof Join && !semiJoin) {\n          // should not be done for semijoin since it will change the semantics\n        // Invert join inputs; this is done because otherwise the SemanticAnalyzer\n        // methods to merge joins will not kick in\n        JoinRelType type;\n        if (join.getJoinType() == JoinRelType.LEFT) {\n          type = JoinRelType.RIGHT;\n        } else if (join.getJoinType() == JoinRelType.RIGHT) {\n          type = JoinRelType.LEFT;\n        } else {\n          type = join.getJoinType();\n        }\n        ast = ASTBuilder.join(right.ast, left.ast, type, cond, semiJoin);\n      } else {\n        ast = ASTBuilder.join(left.ast, right.ast, join.getJoinType(), cond, semiJoin);\n      }\n      if (semiJoin) {\n        s = left.schema;\n      }\n    } else if (r instanceof Union) {\n      Union u = ((Union) r);\n      ASTNode left = new ASTConverter(((Union) r).getInput(0), this.derivedTableCount).convert();\n      for (int ind = 1; ind < u.getInputs().size(); ind++) {\n        left = getUnionAllAST(left, new ASTConverter(((Union) r).getInput(ind),\n            this.derivedTableCount).convert());\n        String sqAlias = nextAlias();\n        ast = ASTBuilder.subQuery(left, sqAlias);\n        s = new Schema((Union) r, sqAlias);\n      }\n    } else {\n      ASTConverter src = new ASTConverter(r, this.derivedTableCount);\n      ASTNode srcAST = src.convert();\n      String sqAlias = nextAlias();\n      s = src.getRowSchema(sqAlias);\n      ast = ASTBuilder.subQuery(srcAST, sqAlias);\n    }\n    return new QueryBlockInfo(s, ast);\n  }"
        ],
        [
            "ASTConverter::RexVisitor::visitCall(RexCall)",
            " 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673 -\n 674  \n 675  \n 676  \n 677 -\n 678  \n 679  \n 680  \n 681 -\n 682 -\n 683 -\n 684  \n 685  \n 686  \n 687  \n 688  \n 689 -\n 690 -\n 691 -\n 692 -\n 693 -\n 694 -\n 695 -\n 696 -\n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  ",
            "    @Override\n    public ASTNode visitCall(RexCall call) {\n      if (!deep) {\n        return null;\n      }\n\n      SqlOperator op = call.getOperator();\n      List<ASTNode> astNodeLst = new LinkedList<ASTNode>();\n      if (op.kind == SqlKind.CAST) {\n        HiveToken ht = TypeConverter.hiveToken(call.getType());\n        ASTBuilder astBldr = ASTBuilder.construct(ht.type, ht.text);\n        if (ht.args != null) {\n          for (String castArg : ht.args)\n            astBldr.add(HiveParser.Identifier, castArg);\n        }\n        astNodeLst.add(astBldr.node());\n      }\n\n      if (op.kind == SqlKind.EXTRACT) {\n        // Extract on date: special handling since function in Hive does\n        // include <time_unit>. Observe that <time_unit> information\n        // is implicit in the function name, thus translation will\n        // proceed correctly if we just ignore the <time_unit>\n        astNodeLst.add(call.operands.get(1).accept(this));\n      } else if (op.kind == SqlKind.FLOOR &&\n              call.operands.size() == 2) {\n        // Floor on date: special handling since function in Hive does\n        // include <time_unit>. Observe that <time_unit> information\n        // is implicit in the function name, thus translation will\n        // proceed correctly if we just ignore the <time_unit>\n        astNodeLst.add(call.operands.get(0).accept(this));\n      } else {\n        for (RexNode operand : call.operands) {\n          astNodeLst.add(operand.accept(this));\n        }\n      }\n\n      if (isFlat(call)) {\n        return SqlFunctionConverter.buildAST(op, astNodeLst, 0);\n      } else {\n        return SqlFunctionConverter.buildAST(op, astNodeLst);\n      }\n    }",
            " 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673 +\n 674 +\n 675 +\n 676 +\n 677 +\n 678 +\n 679 +\n 680 +\n 681 +\n 682 +\n 683 +\n 684 +\n 685 +\n 686 +\n 687 +\n 688 +\n 689 +\n 690 +\n 691  \n 692  \n 693  \n 694 +\n 695  \n 696 +\n 697  \n 698  \n 699 +\n 700 +\n 701 +\n 702 +\n 703 +\n 704  \n 705  \n 706  \n 707  \n 708  \n 709 +\n 710 +\n 711 +\n 712 +\n 713 +\n 714 +\n 715 +\n 716 +\n 717 +\n 718 +\n 719 +\n 720 +\n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  ",
            "    @Override\n    public ASTNode visitCall(RexCall call) {\n      if (!deep) {\n        return null;\n      }\n\n      SqlOperator op = call.getOperator();\n      List<ASTNode> astNodeLst = new LinkedList<ASTNode>();\n      switch (op.kind) {\n      case EQUALS:\n      case NOT_EQUALS:\n      case LESS_THAN:\n      case GREATER_THAN:\n      case LESS_THAN_OR_EQUAL:\n      case GREATER_THAN_OR_EQUAL:\n        if (rexBuilder != null && RexUtil.isReferenceOrAccess(call.operands.get(1), true) &&\n            RexUtil.isLiteral(call.operands.get(0), true)) {\n          // Swap to get reference on the left side\n          return visitCall((RexCall) RexUtil.invert(rexBuilder, call));\n        } else {\n          for (RexNode operand : call.operands) {\n            astNodeLst.add(operand.accept(this));\n          }\n        }\n        break;\n      case CAST:\n        HiveToken ht = TypeConverter.hiveToken(call.getType());\n        ASTBuilder astBldr = ASTBuilder.construct(ht.type, ht.text);\n        if (ht.args != null) {\n          for (String castArg : ht.args) {\n            astBldr.add(HiveParser.Identifier, castArg);\n          }\n        }\n        astNodeLst.add(astBldr.node());\n        for (RexNode operand : call.operands) {\n          astNodeLst.add(operand.accept(this));\n        }\n        break;\n      case EXTRACT:\n        // Extract on date: special handling since function in Hive does\n        // include <time_unit>. Observe that <time_unit> information\n        // is implicit in the function name, thus translation will\n        // proceed correctly if we just ignore the <time_unit>\n        astNodeLst.add(call.operands.get(1).accept(this));\n        break;\n      case FLOOR:\n        if (call.operands.size() == 2) {\n          // Floor on date: special handling since function in Hive does\n          // include <time_unit>. Observe that <time_unit> information\n          // is implicit in the function name, thus translation will\n          // proceed correctly if we just ignore the <time_unit>\n          astNodeLst.add(call.operands.get(0).accept(this));\n          break;\n        }\n        // fall-through\n      default:\n        for (RexNode operand : call.operands) {\n          astNodeLst.add(operand.accept(this));\n        }\n      }\n\n      if (isFlat(call)) {\n        return SqlFunctionConverter.buildAST(op, astNodeLst, 0);\n      } else {\n        return SqlFunctionConverter.buildAST(op, astNodeLst);\n      }\n    }"
        ],
        [
            "ASTConverter::convertOrderLimitToASTNode(HiveSortLimit)",
            " 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 -\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  ",
            "  private void convertOrderLimitToASTNode(HiveSortLimit order) {\n    if (order != null) {\n      HiveSortLimit hiveSortLimit = order;\n      if (!hiveSortLimit.getCollation().getFieldCollations().isEmpty()) {\n        // 1 Add order by token\n        ASTNode orderAst = ASTBuilder.createAST(HiveParser.TOK_ORDERBY, \"TOK_ORDERBY\");\n\n        schema = new Schema(hiveSortLimit);\n        Map<Integer, RexNode> obRefToCallMap = hiveSortLimit.getInputRefToCallMap();\n        RexNode obExpr;\n        ASTNode astCol;\n        for (RelFieldCollation c : hiveSortLimit.getCollation().getFieldCollations()) {\n\n          // 2 Add Direction token\n          ASTNode directionAST = c.getDirection() == RelFieldCollation.Direction.ASCENDING ? ASTBuilder\n              .createAST(HiveParser.TOK_TABSORTCOLNAMEASC, \"TOK_TABSORTCOLNAMEASC\") : ASTBuilder\n              .createAST(HiveParser.TOK_TABSORTCOLNAMEDESC, \"TOK_TABSORTCOLNAMEDESC\");\n          ASTNode nullDirectionAST;\n          // Null direction\n          if (c.nullDirection == RelFieldCollation.NullDirection.FIRST) {\n            nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_FIRST, \"TOK_NULLS_FIRST\");\n            directionAST.addChild(nullDirectionAST);\n          } else if (c.nullDirection == RelFieldCollation.NullDirection.LAST) {\n            nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_LAST, \"TOK_NULLS_LAST\");\n            directionAST.addChild(nullDirectionAST);\n          } else {\n            // Default\n            if (c.getDirection() == RelFieldCollation.Direction.ASCENDING) {\n              nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_FIRST, \"TOK_NULLS_FIRST\");\n              directionAST.addChild(nullDirectionAST);\n            } else {\n              nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_LAST, \"TOK_NULLS_LAST\");\n              directionAST.addChild(nullDirectionAST);\n            }\n          }\n\n          // 3 Convert OB expr (OB Expr is usually an input ref except for top\n          // level OB; top level OB will have RexCall kept in a map.)\n          obExpr = null;\n          if (obRefToCallMap != null)\n            obExpr = obRefToCallMap.get(c.getFieldIndex());\n\n          if (obExpr != null) {\n            astCol = obExpr.accept(new RexVisitor(schema));\n          } else {\n            ColumnInfo cI = schema.get(c.getFieldIndex());\n            /*\n             * The RowResolver setup for Select drops Table associations. So\n             * setup ASTNode on unqualified name.\n             */\n            astCol = ASTBuilder.unqualifiedName(cI.column);\n          }\n\n          // 4 buildup the ob expr AST\n          nullDirectionAST.addChild(astCol);\n          orderAst.addChild(directionAST);\n        }\n        hiveAST.order = orderAst;\n      }\n\n      RexNode offsetExpr = hiveSortLimit.getOffsetExpr();\n      RexNode fetchExpr = hiveSortLimit.getFetchExpr();\n      if (fetchExpr != null) {\n        Object offset = (offsetExpr == null) ?\n            new Integer(0) : ((RexLiteral) offsetExpr).getValue2();\n        Object fetch = ((RexLiteral) fetchExpr).getValue2();\n        hiveAST.limit = ASTBuilder.limit(offset, fetch);\n      }\n    }\n  }",
            " 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303 +\n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  ",
            "  private void convertOrderLimitToASTNode(HiveSortLimit order) {\n    if (order != null) {\n      HiveSortLimit hiveSortLimit = order;\n      if (!hiveSortLimit.getCollation().getFieldCollations().isEmpty()) {\n        // 1 Add order by token\n        ASTNode orderAst = ASTBuilder.createAST(HiveParser.TOK_ORDERBY, \"TOK_ORDERBY\");\n\n        schema = new Schema(hiveSortLimit);\n        Map<Integer, RexNode> obRefToCallMap = hiveSortLimit.getInputRefToCallMap();\n        RexNode obExpr;\n        ASTNode astCol;\n        for (RelFieldCollation c : hiveSortLimit.getCollation().getFieldCollations()) {\n\n          // 2 Add Direction token\n          ASTNode directionAST = c.getDirection() == RelFieldCollation.Direction.ASCENDING ? ASTBuilder\n              .createAST(HiveParser.TOK_TABSORTCOLNAMEASC, \"TOK_TABSORTCOLNAMEASC\") : ASTBuilder\n              .createAST(HiveParser.TOK_TABSORTCOLNAMEDESC, \"TOK_TABSORTCOLNAMEDESC\");\n          ASTNode nullDirectionAST;\n          // Null direction\n          if (c.nullDirection == RelFieldCollation.NullDirection.FIRST) {\n            nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_FIRST, \"TOK_NULLS_FIRST\");\n            directionAST.addChild(nullDirectionAST);\n          } else if (c.nullDirection == RelFieldCollation.NullDirection.LAST) {\n            nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_LAST, \"TOK_NULLS_LAST\");\n            directionAST.addChild(nullDirectionAST);\n          } else {\n            // Default\n            if (c.getDirection() == RelFieldCollation.Direction.ASCENDING) {\n              nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_FIRST, \"TOK_NULLS_FIRST\");\n              directionAST.addChild(nullDirectionAST);\n            } else {\n              nullDirectionAST = ASTBuilder.createAST(HiveParser.TOK_NULLS_LAST, \"TOK_NULLS_LAST\");\n              directionAST.addChild(nullDirectionAST);\n            }\n          }\n\n          // 3 Convert OB expr (OB Expr is usually an input ref except for top\n          // level OB; top level OB will have RexCall kept in a map.)\n          obExpr = null;\n          if (obRefToCallMap != null)\n            obExpr = obRefToCallMap.get(c.getFieldIndex());\n\n          if (obExpr != null) {\n            astCol = obExpr.accept(new RexVisitor(schema, false, order.getCluster().getRexBuilder()));\n          } else {\n            ColumnInfo cI = schema.get(c.getFieldIndex());\n            /*\n             * The RowResolver setup for Select drops Table associations. So\n             * setup ASTNode on unqualified name.\n             */\n            astCol = ASTBuilder.unqualifiedName(cI.column);\n          }\n\n          // 4 buildup the ob expr AST\n          nullDirectionAST.addChild(astCol);\n          orderAst.addChild(directionAST);\n        }\n        hiveAST.order = orderAst;\n      }\n\n      RexNode offsetExpr = hiveSortLimit.getOffsetExpr();\n      RexNode fetchExpr = hiveSortLimit.getFetchExpr();\n      if (fetchExpr != null) {\n        Object offset = (offsetExpr == null) ?\n            new Integer(0) : ((RexLiteral) offsetExpr).getValue2();\n        Object fetch = ((RexLiteral) fetchExpr).getValue2();\n        hiveAST.limit = ASTBuilder.limit(offset, fetch);\n      }\n    }\n  }"
        ]
    ],
    "57ae3aca05d21628df620f33a9f03966f33c8d7b": [
        [
            "SparkClientFactory::stop()",
            "  63  \n  64 -\n  65  \n  66 -\n  67 -\n  68  \n  69  ",
            "  /** Stops the SparkClient library. */\n  public static synchronized void stop() {\n    if (server != null) {\n      server.close();\n      server = null;\n    }\n  }",
            "  64  \n  65 +\n  66  \n  67 +\n  68 +\n  69 +\n  70 +\n  71 +\n  72 +\n  73  \n  74  ",
            "  /** Stops the SparkClient library. */\n  public static void stop() {\n    if (server != null) {\n      synchronized (stopLock) {\n        if (server != null) {\n          server.close();\n          server = null;\n        }\n      }\n    }\n  }"
        ]
    ],
    "0d787cbc055eb237bcccd5fdbc144fb6b1d7d4ca": [
        [
            "HiveRelMdPredicates::JoinConditionBasedPredicateInference::ExprsItr::computeNextMapping(int)",
            " 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662 -\n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  ",
            "      private void computeNextMapping(int level) {\n        int t = columnSets[level].nextSetBit(iterationIdx[level]);\n        if (t < 0) {\n          if (level == 0) {\n            nextMapping = null;\n          } else {\n            iterationIdx[level] = 0;\n            computeNextMapping(level - 1);\n          }\n        } else {\n          nextMapping.set(columns[level], t);\n          iterationIdx[level] = t + 1;\n        }\n      }",
            " 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662 +\n 663 +\n 664 +\n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  ",
            "      private void computeNextMapping(int level) {\n        int t = columnSets[level].nextSetBit(iterationIdx[level]);\n        if (t < 0) {\n          if (level == 0) {\n            nextMapping = null;\n          } else {\n            int tmp = columnSets[level].nextSetBit(0);\n            nextMapping.set(columns[level], tmp);\n            iterationIdx[level] = tmp + 1;\n            computeNextMapping(level - 1);\n          }\n        } else {\n          nextMapping.set(columns[level], t);\n          iterationIdx[level] = t + 1;\n        }\n      }"
        ]
    ],
    "1a2e378f36a1a02d68ff1647f856f1e223276da0": [
        [
            "DDLSemanticAnalyzer::buildTriggerExpression(ASTNode)",
            "1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100 -\n1101  \n1102  \n1103  \n1104  \n1105  ",
            "  private String buildTriggerExpression(ASTNode ast) throws SemanticException {\n    if (ast.getType() != HiveParser.TOK_TRIGGER_EXPRESSION || ast.getChildCount() == 0) {\n      throw new SemanticException(\"Invalid trigger expression.\");\n    }\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < ast.getChildCount(); ++i) {\n      builder.append(stripQuotes(ast.getChild(i).getText()));\n      builder.append(' ');\n    }\n    builder.deleteCharAt(builder.length() - 1);\n    return builder.toString();\n  }",
            "1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100 +\n1101  \n1102  \n1103  \n1104  \n1105  ",
            "  private String buildTriggerExpression(ASTNode ast) throws SemanticException {\n    if (ast.getType() != HiveParser.TOK_TRIGGER_EXPRESSION || ast.getChildCount() == 0) {\n      throw new SemanticException(\"Invalid trigger expression.\");\n    }\n    StringBuilder builder = new StringBuilder();\n    for (int i = 0; i < ast.getChildCount(); ++i) {\n      builder.append(ast.getChild(i).getText()); // Don't strip quotes.\n      builder.append(' ');\n    }\n    builder.deleteCharAt(builder.length() - 1);\n    return builder.toString();\n  }"
        ]
    ],
    "cc8ac97bcadd20a645e113f3193fc6b2d9db087d": [
        [
            "ObjectInspectorFactory::getUnionStructObjectInspector(List)",
            " 348  \n 349  \n 350 -\n 351 -\n 352  \n 353  \n 354 -\n 355 -\n 356 -\n 357 -\n 358 -\n 359  \n 360  \n 361  ",
            "  public static UnionStructObjectInspector getUnionStructObjectInspector(\n      List<StructObjectInspector> structObjectInspectors) {\n    UnionStructObjectInspector result = cachedUnionStructObjectInspector\n        .get(structObjectInspectors);\n    if (result == null) {\n      result = new UnionStructObjectInspector(structObjectInspectors);\n      UnionStructObjectInspector prev =\n        cachedUnionStructObjectInspector.putIfAbsent(structObjectInspectors, result);\n      if (prev != null) {\n        result = prev;\n      }\n    }\n    return result;\n  }",
            " 355  \n 356  \n 357 +\n 358  \n 359  \n 360 +\n 361  \n 362  \n 363  ",
            "  public static UnionStructObjectInspector getUnionStructObjectInspector(\n      List<StructObjectInspector> structObjectInspectors) {\n    UnionStructObjectInspector result = cachedUnionStructObjectInspector.getIfPresent(structObjectInspectors);\n    if (result == null) {\n      result = new UnionStructObjectInspector(structObjectInspectors);\n      cachedUnionStructObjectInspector.put(structObjectInspectors, result);\n    }\n    return result;\n  }"
        ]
    ],
    "57f40f71ff9275458e743aad0ca460dff2291a4a": [
        [
            "TestSchemaToolCatalogOps::initDb()",
            "  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "  @BeforeClass\n  public static void initDb() throws HiveMetaException, IOException {\n    conf = new HiveConf();\n    MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.AUTO_CREATE_ALL, false);\n    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.HMS_HANDLER_ATTEMPTS, 1);\n    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.THRIFT_CONNECTION_RETRIES, 1);\n    testMetastoreDB = System.getProperty(\"java.io.tmpdir\") +\n        File.separator + \"testschematoolcatopsdb\";\n    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.CONNECT_URL_KEY,\n        \"jdbc:derby:\" + testMetastoreDB + \";create=true\");\n    schemaTool = new MetastoreSchemaTool();\n    schemaTool.init(System.getProperty(\"test.tmp.dir\", \"target/tmp\"),\n        new String[]{\"-dbType\", \"derby\", \"--info\"}, null, conf);\n\n    String userName = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.CONNECTION_USER_NAME);\n    String passWord = MetastoreConf.getPassword(conf, MetastoreConf.ConfVars.PWD);\n    schemaTool.setUserName(userName);\n    schemaTool.setPassWord(passWord);\n\n    argsBase = \"-dbType derby -userName \" + userName + \" -passWord \" + passWord + \" \";\n    execute(new SchemaToolTaskInit(), \"-initSchema\"); // Pre-install the database so all the tables are there.\n  }",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83 +\n  84  \n  85  \n  86  \n  87  ",
            "  @BeforeClass\n  public static void initDb() throws HiveMetaException, IOException {\n    conf = new HiveConf();\n    MetastoreConf.setBoolVar(conf, MetastoreConf.ConfVars.AUTO_CREATE_ALL, false);\n    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.HMS_HANDLER_ATTEMPTS, 1);\n    MetastoreConf.setLongVar(conf, MetastoreConf.ConfVars.THRIFT_CONNECTION_RETRIES, 1);\n    testMetastoreDB = System.getProperty(\"java.io.tmpdir\") +\n        File.separator + \"testschematoolcatopsdb\";\n    MetastoreConf.setVar(conf, MetastoreConf.ConfVars.CONNECT_URL_KEY,\n        \"jdbc:derby:\" + testMetastoreDB + \";create=true\");\n    schemaTool = new MetastoreSchemaTool();\n    schemaTool.init(System.getProperty(\"test.tmp.dir\", \"target/tmp\"),\n        new String[]{\"-dbType\", \"derby\", \"--info\"}, null, conf);\n\n    String userName = MetastoreConf.getVar(conf, MetastoreConf.ConfVars.CONNECTION_USER_NAME);\n    String passWord = MetastoreConf.getPassword(conf, MetastoreConf.ConfVars.PWD);\n    schemaTool.setUserName(userName);\n    schemaTool.setPassWord(passWord);\n    errStream = System.err;\n    outStream = System.out;\n\n    argsBase = \"-dbType derby -userName \" + userName + \" -passWord \" + passWord + \" \";\n    execute(new SchemaToolTaskInit(), \"-initSchema\"); // Pre-install the database so all the tables are there.\n  }"
        ],
        [
            "TestSchemaToolCatalogOps::removeDb()",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "  @AfterClass\n  public static void removeDb() throws Exception {\n    File metaStoreDir = new File(testMetastoreDB);\n    if (metaStoreDir.exists()) {\n      FileUtils.forceDeleteOnExit(metaStoreDir);\n    }\n  }",
            "  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 +\n  96 +\n  97  ",
            "  @AfterClass\n  public static void removeDb() throws Exception {\n    File metaStoreDir = new File(testMetastoreDB);\n    if (metaStoreDir.exists()) {\n      FileUtils.forceDeleteOnExit(metaStoreDir);\n    }\n    System.setOut(outStream);\n    System.setErr(errStream);\n  }"
        ]
    ],
    "be1130d567bd3b075fa3364215bb561e221506ed": [
        [
            "DDLTask::createView(Hive,CreateViewDesc)",
            "5132  \n5133  \n5134  \n5135  \n5136  \n5137  \n5138  \n5139  \n5140  \n5141  \n5142  \n5143  \n5144  \n5145  \n5146  \n5147  \n5148  \n5149  \n5150  \n5151  \n5152  \n5153  \n5154  \n5155  \n5156  \n5157  \n5158 -\n5159  \n5160  \n5161  \n5162  \n5163  \n5164  \n5165  \n5166  \n5167  \n5168  \n5169  \n5170  \n5171  \n5172  \n5173  \n5174  \n5175  \n5176  \n5177  \n5178  \n5179  \n5180  \n5181  \n5182  \n5183  \n5184  \n5185  \n5186  \n5187  \n5188  \n5189  \n5190  \n5191  \n5192  \n5193  \n5194  \n5195  \n5196  \n5197  \n5198  \n5199  \n5200  \n5201  \n5202  \n5203  \n5204  \n5205  \n5206  ",
            "  /**\n   * Create a new view.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtView\n   *          This is the view we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createView(Hive db, CreateViewDesc crtView) throws HiveException {\n    Table oldview = db.getTable(crtView.getViewName(), false);\n    if (oldview != null) {\n      // Check whether we are replicating\n      if (crtView.getReplicationSpec().isInReplicationScope()) {\n        // if this is a replication spec, then replace-mode semantics might apply.\n        if (crtView.getReplicationSpec().allowEventReplacementInto(oldview.getParameters())){\n          crtView.setReplace(true); // we replace existing view.\n        } else {\n          LOG.debug(\"DDLTask: Create View is skipped as view {} is newer than update\",\n              crtView.getViewName()); // no replacement, the existing table state is newer than our update.\n          return 0;\n        }\n      }\n\n      if (!crtView.isReplace()) {\n        // View already exists, thus we should be replacing\n        throw new HiveException(ErrorMsg.TABLE_ALREADY_EXISTS.getMsg(crtView.getViewName()));\n      }\n\n      // It should not be a materialized view\n      assert !crtView.isMaterialized();\n\n      // replace existing view\n      // remove the existing partition columns from the field schema\n      oldview.setViewOriginalText(crtView.getViewOriginalText());\n      oldview.setViewExpandedText(crtView.getViewExpandedText());\n      oldview.setFields(crtView.getSchema());\n      if (crtView.getComment() != null) {\n        oldview.setProperty(\"comment\", crtView.getComment());\n      }\n      if (crtView.getTblProps() != null) {\n        oldview.getTTable().getParameters().putAll(crtView.getTblProps());\n      }\n      oldview.setPartCols(crtView.getPartCols());\n      if (crtView.getInputFormat() != null) {\n        oldview.setInputFormatClass(crtView.getInputFormat());\n      }\n      if (crtView.getOutputFormat() != null) {\n        oldview.setOutputFormatClass(crtView.getOutputFormat());\n      }\n      oldview.checkValidity(null);\n      db.alterTable(crtView.getViewName(), oldview, false, null, true);\n      addIfAbsentByName(new WriteEntity(oldview, WriteEntity.WriteType.DDL_NO_LOCK));\n    } else {\n      // We create new view\n      Table tbl = crtView.toTable(conf);\n      // We set the signature for the view if it is a materialized view\n      if (tbl.isMaterializedView()) {\n        CreationMetadata cm =\n            new CreationMetadata(MetaStoreUtils.getDefaultCatalog(conf), tbl.getDbName(),\n                tbl.getTableName(), ImmutableSet.copyOf(crtView.getTablesUsed()));\n        cm.setValidTxnList(conf.get(ValidTxnWriteIdList.VALID_TABLES_WRITEIDS_KEY));\n        tbl.getTTable().setCreationMetadata(cm);\n      }\n      db.createTable(tbl, crtView.getIfNotExists());\n      addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n\n      //set lineage info\n      DataContainer dc = new DataContainer(tbl.getTTable());\n      queryState.getLineageState().setLineage(new Path(crtView.getViewName()), dc, tbl.getCols());\n    }\n    return 0;\n  }",
            "5132  \n5133  \n5134  \n5135  \n5136  \n5137  \n5138  \n5139  \n5140  \n5141  \n5142  \n5143  \n5144  \n5145  \n5146  \n5147  \n5148  \n5149  \n5150  \n5151  \n5152  \n5153  \n5154  \n5155  \n5156  \n5157  \n5158 +\n5159  \n5160  \n5161  \n5162  \n5163  \n5164  \n5165  \n5166  \n5167  \n5168  \n5169  \n5170  \n5171  \n5172  \n5173  \n5174  \n5175  \n5176  \n5177  \n5178  \n5179  \n5180  \n5181  \n5182  \n5183  \n5184  \n5185  \n5186  \n5187  \n5188  \n5189  \n5190  \n5191  \n5192  \n5193  \n5194  \n5195  \n5196  \n5197  \n5198  \n5199  \n5200  \n5201  \n5202  \n5203  \n5204  \n5205  \n5206  ",
            "  /**\n   * Create a new view.\n   *\n   * @param db\n   *          The database in question.\n   * @param crtView\n   *          This is the view we're creating.\n   * @return Returns 0 when execution succeeds and above 0 if it fails.\n   * @throws HiveException\n   *           Throws this exception if an unexpected error occurs.\n   */\n  private int createView(Hive db, CreateViewDesc crtView) throws HiveException {\n    Table oldview = db.getTable(crtView.getViewName(), false);\n    if (oldview != null) {\n      // Check whether we are replicating\n      if (crtView.getReplicationSpec().isInReplicationScope()) {\n        // if this is a replication spec, then replace-mode semantics might apply.\n        if (crtView.getReplicationSpec().allowEventReplacementInto(oldview.getParameters())){\n          crtView.setReplace(true); // we replace existing view.\n        } else {\n          LOG.debug(\"DDLTask: Create View is skipped as view {} is newer than update\",\n              crtView.getViewName()); // no replacement, the existing table state is newer than our update.\n          return 0;\n        }\n      }\n\n      if (!crtView.isReplace() && !crtView.getIfNotExists()) {\n        // View already exists, thus we should be replacing\n        throw new HiveException(ErrorMsg.TABLE_ALREADY_EXISTS.getMsg(crtView.getViewName()));\n      }\n\n      // It should not be a materialized view\n      assert !crtView.isMaterialized();\n\n      // replace existing view\n      // remove the existing partition columns from the field schema\n      oldview.setViewOriginalText(crtView.getViewOriginalText());\n      oldview.setViewExpandedText(crtView.getViewExpandedText());\n      oldview.setFields(crtView.getSchema());\n      if (crtView.getComment() != null) {\n        oldview.setProperty(\"comment\", crtView.getComment());\n      }\n      if (crtView.getTblProps() != null) {\n        oldview.getTTable().getParameters().putAll(crtView.getTblProps());\n      }\n      oldview.setPartCols(crtView.getPartCols());\n      if (crtView.getInputFormat() != null) {\n        oldview.setInputFormatClass(crtView.getInputFormat());\n      }\n      if (crtView.getOutputFormat() != null) {\n        oldview.setOutputFormatClass(crtView.getOutputFormat());\n      }\n      oldview.checkValidity(null);\n      db.alterTable(crtView.getViewName(), oldview, false, null, true);\n      addIfAbsentByName(new WriteEntity(oldview, WriteEntity.WriteType.DDL_NO_LOCK));\n    } else {\n      // We create new view\n      Table tbl = crtView.toTable(conf);\n      // We set the signature for the view if it is a materialized view\n      if (tbl.isMaterializedView()) {\n        CreationMetadata cm =\n            new CreationMetadata(MetaStoreUtils.getDefaultCatalog(conf), tbl.getDbName(),\n                tbl.getTableName(), ImmutableSet.copyOf(crtView.getTablesUsed()));\n        cm.setValidTxnList(conf.get(ValidTxnWriteIdList.VALID_TABLES_WRITEIDS_KEY));\n        tbl.getTTable().setCreationMetadata(cm);\n      }\n      db.createTable(tbl, crtView.getIfNotExists());\n      addIfAbsentByName(new WriteEntity(tbl, WriteEntity.WriteType.DDL_NO_LOCK));\n\n      //set lineage info\n      DataContainer dc = new DataContainer(tbl.getTTable());\n      queryState.getLineageState().setLineage(new Path(crtView.getViewName()), dc, tbl.getCols());\n    }\n    return 0;\n  }"
        ]
    ],
    "0f7163fad61745276f5b8e1eaa797ba297810780": [
        [
            "CalcitePlanner::createPlanner(HiveConf,Set,Set)",
            " 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415 -\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  ",
            "  private static RelOptPlanner createPlanner(\n      HiveConf conf, Set<RelNode> corrScalarRexSQWithAgg, Set<RelNode> scalarAggNoGbyNoWin) {\n    final Double maxSplitSize = (double) HiveConf.getLongVar(\n            conf, HiveConf.ConfVars.MAPREDMAXSPLITSIZE);\n    final Double maxMemory = (double) HiveConf.getLongVar(\n            conf, HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);\n    HiveAlgorithmsConf algorithmsConf = new HiveAlgorithmsConf(maxSplitSize, maxMemory);\n    HiveRulesRegistry registry = new HiveRulesRegistry();\n    Properties calciteConfigProperties = new Properties();\n    calciteConfigProperties.setProperty(\n        CalciteConnectionProperty.TIME_ZONE.camelName(),\n        conf.getLocalTimeZone().getId());\n    calciteConfigProperties.setProperty(\n        CalciteConnectionProperty.MATERIALIZATIONS_ENABLED.camelName(),\n        Boolean.FALSE.toString());\n    CalciteConnectionConfig calciteConfig = new CalciteConnectionConfigImpl(calciteConfigProperties);\n    boolean isCorrelatedColumns = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_CORRELATED_MULTI_KEY_JOINS);\n    boolean heuristicMaterializationStrategy = HiveConf.getVar(conf,\n        HiveConf.ConfVars.HIVE_MATERIALIZED_VIEW_REWRITING_SELECTION_STRATEGY).equals(\"heuristic\");\n    HivePlannerContext confContext = new HivePlannerContext(algorithmsConf, registry, calciteConfig,\n        corrScalarRexSQWithAgg, scalarAggNoGbyNoWin,\n        new HiveConfPlannerContext(isCorrelatedColumns, heuristicMaterializationStrategy));\n    return HiveVolcanoPlanner.createPlanner(confContext);\n  }",
            " 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415 +\n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  ",
            "  private static RelOptPlanner createPlanner(\n      HiveConf conf, Set<RelNode> corrScalarRexSQWithAgg, Set<RelNode> scalarAggNoGbyNoWin) {\n    final Double maxSplitSize = (double) HiveConf.getLongVar(\n            conf, HiveConf.ConfVars.MAPREDMAXSPLITSIZE);\n    final Double maxMemory = (double) HiveConf.getLongVar(\n            conf, HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);\n    HiveAlgorithmsConf algorithmsConf = new HiveAlgorithmsConf(maxSplitSize, maxMemory);\n    HiveRulesRegistry registry = new HiveRulesRegistry();\n    Properties calciteConfigProperties = new Properties();\n    calciteConfigProperties.setProperty(\n        CalciteConnectionProperty.TIME_ZONE.camelName(),\n        conf.getLocalTimeZone().getId());\n    calciteConfigProperties.setProperty(\n        CalciteConnectionProperty.MATERIALIZATIONS_ENABLED.camelName(),\n        Boolean.FALSE.toString());\n    CalciteConnectionConfig calciteConfig = new CalciteConnectionConfigImpl(calciteConfigProperties);\n    boolean isCorrelatedColumns = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CBO_STATS_CORRELATED_MULTI_KEY_JOINS);\n    boolean heuristicMaterializationStrategy = HiveConf.getVar(conf,\n        HiveConf.ConfVars.HIVE_MATERIALIZED_VIEW_REWRITING_SELECTION_STRATEGY).equals(\"heuristic\");\n    HivePlannerContext confContext = new HivePlannerContext(algorithmsConf, registry, calciteConfig,\n        corrScalarRexSQWithAgg, scalarAggNoGbyNoWin,\n        new HiveConfPlannerContext(isCorrelatedColumns, heuristicMaterializationStrategy));\n    return HiveVolcanoPlanner.createPlanner(confContext);\n  }"
        ]
    ],
    "83d1fd23a22ee96dd2c464d67303eadf31194454": [
        [
            "RelOptHiveTable::isNonNullableKey(ImmutableBitSet)",
            " 223  \n 224  \n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  ",
            "  public boolean isNonNullableKey(ImmutableBitSet columns) {\n    for (ImmutableBitSet key : nonNullablekeys) {\n      if (key.contains(columns)) {\n        return true;\n      }\n    }\n    return false;\n  }",
            " 223  \n 224  \n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  ",
            "  public boolean isNonNullableKey(ImmutableBitSet columns) {\n    for (ImmutableBitSet key : nonNullablekeys) {\n      if (columns.contains(key)) {\n        return true;\n      }\n    }\n    return false;\n  }"
        ]
    ],
    "4e415609ce333fd17c1dd5d4bf44ca9a3897ec42": [
        [
            "TestHiveCredentialProviders::testHadoopCredentialProvider()",
            " 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "  @Test\n  public void testHadoopCredentialProvider() throws Exception {\n    setupConfigs(true, true, true, false);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    Assert.assertEquals(HADOOP_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n\n    // make sure MAP task environment points to HADOOP_CREDSTORE_PASSWORD\n    Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_MAP_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    // make sure REDUCE task environment points to HADOOP_CREDSTORE_PASSWORD\n    Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n  }",
            " 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129 +\n 130 +\n 131  ",
            "  @Test\n  public void testHadoopCredentialProvider() throws Exception {\n    setupConfigs(true, true, true, false);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    Assert.assertEquals(HADOOP_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n\n    // make sure MAP task environment points to HADOOP_CREDSTORE_PASSWORD\n    Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_MAP_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    // make sure REDUCE task environment points to HADOOP_CREDSTORE_PASSWORD\n    Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    Assert.assertTrue(jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)\n        .containsAll(REDACTED_PROPERTIES));\n  }"
        ],
        [
            "TestHiveCredentialProviders::testJobCredentialProviderUnset()",
            " 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  ",
            "  @Test\n  public void testJobCredentialProviderUnset() throws Exception {\n    setupConfigs(true, true, false, false);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    assertEquals(HADOOP_CREDSTORE_LOCATION, jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n\n    assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_MAP_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n  }",
            " 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225 +\n 226 +\n 227 +\n 228  ",
            "  @Test\n  public void testJobCredentialProviderUnset() throws Exception {\n    setupConfigs(true, true, false, false);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    assertEquals(HADOOP_CREDSTORE_LOCATION, jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n\n    assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_MAP_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    Assert.assertTrue(jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)\n        .containsAll(REDACTED_PROPERTIES));\n  }"
        ],
        [
            "HiveConfUtil::updateJobCredentialProviders(Configuration)",
            " 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191 -\n 192 -\n 193 -\n 194  \n 195  \n 196 -\n 197 -\n 198 -\n 199 -\n 200 -\n 201 -\n 202  \n 203  \n 204  ",
            "  /**\n   * Updates the job configuration with the job specific credential provider information available\n   * in the HiveConf.It uses the environment variables HADOOP_CREDSTORE_PASSWORD or\n   * HIVE_JOB_CREDSTORE_PASSWORD to get the custom password for all the keystores configured in the\n   * provider path. This usage of environment variables is similar in lines with Hadoop credential\n   * provider mechanism for getting the keystore passwords. The other way of communicating the\n   * password is through a file which stores the password in clear-text which needs to be readable\n   * by all the consumers and therefore is not supported.\n   *\n   *<ul>\n   * <li>If HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH is set in the hive configuration this method\n   * overrides the MR job configuration property hadoop.security.credential.provider.path with its\n   * value. If not set then it does not change the value of hadoop.security.credential.provider.path\n   * <li>In order to choose the password for the credential provider we check :\n   *\n   *   (1) if job credential provider path HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH is set we check if\n   *       HIVE_SERVER2_JOB_CREDSTORE_PASSWORD_ENVVAR is set. If it is set we use it.\n   *   (2) If password is not set using (1) above we use HADOOP_CREDSTORE_PASSWORD if it is set.\n   *   (3) If none of those are set, we do not set any password in the MR task environment. In this\n   *       case the hadoop credential provider should use the default password of \"none\" automatically\n   *</ul>\n   * @param jobConf - job specific configuration\n   */\n  public static void updateJobCredentialProviders(Configuration jobConf) {\n    if(jobConf == null) {\n      return;\n    }\n\n    String jobKeyStoreLocation = jobConf.get(HiveConf.ConfVars.HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH.varname);\n    String oldKeyStoreLocation = jobConf.get(Constants.HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG);\n    if (StringUtils.isNotBlank(jobKeyStoreLocation)) {\n      jobConf.set(Constants.HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG, jobKeyStoreLocation);\n      LOG.debug(\"Setting job conf credstore location to \" + jobKeyStoreLocation\n          + \" previous location was \" + oldKeyStoreLocation);\n    }\n\n    String credStorepassword = getJobCredentialProviderPassword(jobConf);\n    if (credStorepassword != null) {\n      // if the execution engine is MR set the map/reduce env with the credential store password\n      String execEngine = jobConf.get(ConfVars.HIVE_EXECUTION_ENGINE.varname);\n      if (\"mr\".equalsIgnoreCase(execEngine)) {\n        addKeyValuePair(jobConf, JobConf.MAPRED_MAP_TASK_ENV,\n            Constants.HADOOP_CREDENTIAL_PASSWORD_ENVVAR, credStorepassword);\n        addKeyValuePair(jobConf, JobConf.MAPRED_REDUCE_TASK_ENV,\n            Constants.HADOOP_CREDENTIAL_PASSWORD_ENVVAR, credStorepassword);\n        addKeyValuePair(jobConf, \"yarn.app.mapreduce.am.admin.user.env\",\n            Constants.HADOOP_CREDENTIAL_PASSWORD_ENVVAR, credStorepassword);\n      }\n    }\n  }",
            " 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188 +\n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195 +\n 196 +\n 197  \n 198 +\n 199  \n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214 +\n 215 +\n 216 +\n 217 +\n 218 +\n 219  \n 220  \n 221  ",
            "  /**\n   * Updates the job configuration with the job specific credential provider information available\n   * in the HiveConf.It uses the environment variables HADOOP_CREDSTORE_PASSWORD or\n   * HIVE_JOB_CREDSTORE_PASSWORD to get the custom password for all the keystores configured in the\n   * provider path. This usage of environment variables is similar in lines with Hadoop credential\n   * provider mechanism for getting the keystore passwords. The other way of communicating the\n   * password is through a file which stores the password in clear-text which needs to be readable\n   * by all the consumers and therefore is not supported.\n   *\n   *<ul>\n   * <li>If HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH is set in the hive configuration this method\n   * overrides the MR job configuration property hadoop.security.credential.provider.path with its\n   * value. If not set then it does not change the value of hadoop.security.credential.provider.path\n   * <li>In order to choose the password for the credential provider we check :\n   *\n   *   (1) if job credential provider path HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH is set we check if\n   *       HIVE_SERVER2_JOB_CREDSTORE_PASSWORD_ENVVAR is set. If it is set we use it.\n   *   (2) If password is not set using (1) above we use HADOOP_CREDSTORE_PASSWORD if it is set.\n   *   (3) If none of those are set, we do not set any password in the MR task environment. In this\n   *       case the hadoop credential provider should use the default password of \"none\" automatically\n   *</ul>\n   * @param jobConf - job specific configuration\n   */\n  public static void updateJobCredentialProviders(Configuration jobConf) {\n    if(jobConf == null) {\n      return;\n    }\n\n    String jobKeyStoreLocation = jobConf.get(HiveConf.ConfVars.HIVE_SERVER2_JOB_CREDENTIAL_PROVIDER_PATH.varname);\n    String oldKeyStoreLocation = jobConf.get(Constants.HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG);\n\n    if (StringUtils.isNotBlank(jobKeyStoreLocation)) {\n      jobConf.set(Constants.HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG, jobKeyStoreLocation);\n      LOG.debug(\"Setting job conf credstore location to \" + jobKeyStoreLocation\n          + \" previous location was \" + oldKeyStoreLocation);\n    }\n\n    String credstorePassword = getJobCredentialProviderPassword(jobConf);\n    if (credstorePassword != null) {\n      String execEngine = jobConf.get(ConfVars.HIVE_EXECUTION_ENGINE.varname);\n\n      if (\"mr\".equalsIgnoreCase(execEngine)) {\n        // if the execution engine is MR set the map/reduce env with the credential store password\n\n        Collection<String> redactedProperties =\n            jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES);\n\n        Stream.of(\n            JobConf.MAPRED_MAP_TASK_ENV,\n            JobConf.MAPRED_REDUCE_TASK_ENV,\n            \"yarn.app.mapreduce.am.admin.user.env\")\n\n            .forEach(property -> {\n              addKeyValuePair(jobConf, property,\n                  Constants.HADOOP_CREDENTIAL_PASSWORD_ENVVAR, credstorePassword);\n              redactedProperties.add(property);\n            });\n\n        // Hide sensitive configuration values from MR HistoryUI by telling MR to redact the following list.\n        jobConf.set(MRJobConfig.MR_JOB_REDACTED_PROPERTIES,\n            StringUtils.join(redactedProperties, \",\"));\n      }\n    }\n  }"
        ],
        [
            "TestHiveCredentialProviders::testCredentialProviderWithNoPasswords()",
            " 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "  @Test\n  public void testCredentialProviderWithNoPasswords() throws Exception {\n    setupConfigs(true, false, false, true);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    Assert.assertEquals(JOB_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n    Assert.assertNull(jobConf.get(JobConf.MAPRED_MAP_TASK_ENV));\n    Assert.assertNull(jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV));\n\n    resetConfig();\n    setupConfigs(true, false, false, false);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    Assert.assertEquals(HADOOP_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n    Assert.assertNull(jobConf.get(JobConf.MAPRED_MAP_TASK_ENV));\n    Assert.assertNull(jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV));\n  }",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190 +\n 191 +\n 192 +\n 193 +\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 +\n 203 +\n 204 +\n 205 +\n 206  ",
            "  @Test\n  public void testCredentialProviderWithNoPasswords() throws Exception {\n    setupConfigs(true, false, false, true);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    Assert.assertEquals(JOB_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n    Assert.assertNull(jobConf.get(JobConf.MAPRED_MAP_TASK_ENV));\n    Assert.assertNull(jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV));\n\n    REDACTED_PROPERTIES.forEach(property -> Assert.assertFalse(\n        jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)\n            .contains(property)));\n\n    resetConfig();\n    setupConfigs(true, false, false, false);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    Assert.assertEquals(HADOOP_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n    Assert.assertNull(jobConf.get(JobConf.MAPRED_MAP_TASK_ENV));\n    Assert.assertNull(jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV));\n\n    REDACTED_PROPERTIES.forEach(property -> Assert.assertFalse(\n        jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)\n            .contains(property)));\n  }"
        ],
        [
            "TestHiveCredentialProviders::testJobCredentialProviderWithDefaultPassword()",
            " 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  ",
            "  @Test\n  public void testJobCredentialProviderWithDefaultPassword() throws Exception {\n    setupConfigs(false, true, false, true);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    Assert.assertEquals(JOB_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n\n    Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_MAP_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n  }",
            " 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171 +\n 172 +\n 173 +\n 174  ",
            "  @Test\n  public void testJobCredentialProviderWithDefaultPassword() throws Exception {\n    setupConfigs(false, true, false, true);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    Assert.assertEquals(JOB_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n\n    Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_MAP_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    Assert.assertEquals(HADOOP_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    Assert.assertTrue(jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)\n        .containsAll(REDACTED_PROPERTIES));\n  }"
        ],
        [
            "TestHiveCredentialProviders::testJobCredentialProvider()",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  ",
            "  @Test\n  public void testJobCredentialProvider() throws Exception {\n    setupConfigs(true, true, true, true);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    // make sure credential provider path points to HIVE_SERVER2_JOB_CREDSTORE_LOCATION\n    Assert.assertEquals(JOB_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n\n    // make sure MAP task environment points to HIVE_JOB_CREDSTORE_PASSWORD\n    Assert.assertEquals(HIVE_JOB_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_MAP_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    // make sure REDUCE task environment points to HIVE_JOB_CREDSTORE_PASSWORD\n    Assert.assertEquals(HIVE_JOB_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n  }",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105 +\n 106 +\n 107  ",
            "  @Test\n  public void testJobCredentialProvider() throws Exception {\n    setupConfigs(true, true, true, true);\n\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n    // make sure credential provider path points to HIVE_SERVER2_JOB_CREDSTORE_LOCATION\n    Assert.assertEquals(JOB_CREDSTORE_LOCATION,\n        jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG));\n\n    // make sure MAP task environment points to HIVE_JOB_CREDSTORE_PASSWORD\n    Assert.assertEquals(HIVE_JOB_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_MAP_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    // make sure REDUCE task environment points to HIVE_JOB_CREDSTORE_PASSWORD\n    Assert.assertEquals(HIVE_JOB_CREDSTORE_PASSWORD_ENVVAR_VAL, getValueFromJobConf(\n        jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV), HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    Assert.assertTrue(jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)\n        .containsAll(REDACTED_PROPERTIES));\n  }"
        ],
        [
            "TestHiveCredentialProviders::testNoCredentialProvider()",
            " 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  ",
            "  @Test\n  public void testNoCredentialProvider() throws Exception {\n    setupConfigs(false, false, false, false);\n\n    assertTrue(StringUtils.isBlank(jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG)));\n\n    assertNull(getValueFromJobConf(jobConf.get(JobConf.MAPRED_MAP_TASK_ENV),\n        HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    assertNull(getValueFromJobConf(jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV),\n        HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n  }",
            " 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245 +\n 246 +\n 247 +\n 248 +\n 249  ",
            "  @Test\n  public void testNoCredentialProvider() throws Exception {\n    setupConfigs(false, false, false, false);\n\n    assertTrue(StringUtils.isBlank(jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG)));\n\n    assertNull(getValueFromJobConf(jobConf.get(JobConf.MAPRED_MAP_TASK_ENV),\n        HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    assertNull(getValueFromJobConf(jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV),\n        HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    REDACTED_PROPERTIES.forEach(property -> Assert.assertFalse(\n        jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)\n            .contains(property)));\n  }"
        ],
        [
            "TestHiveCredentialProviders::testNoCredentialProviderWithPassword()",
            " 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  ",
            "  @Test\n  public void testNoCredentialProviderWithPassword() throws Exception {\n    setupConfigs(false, false, true, false);\n\n    Assert.assertTrue(StringUtils.isBlank(jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG)));\n\n    Assert.assertNull(getValueFromJobConf(jobConf.get(JobConf.MAPRED_MAP_TASK_ENV),\n        HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    Assert.assertNull(getValueFromJobConf(jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV),\n        HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n  }",
            " 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148 +\n 149 +\n 150 +\n 151 +\n 152  ",
            "  @Test\n  public void testNoCredentialProviderWithPassword() throws Exception {\n    setupConfigs(false, false, true, false);\n\n    Assert.assertTrue(StringUtils.isBlank(jobConf.get(HADOOP_CREDENTIAL_PROVIDER_PATH_CONFIG)));\n\n    Assert.assertNull(getValueFromJobConf(jobConf.get(JobConf.MAPRED_MAP_TASK_ENV),\n        HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    Assert.assertNull(getValueFromJobConf(jobConf.get(JobConf.MAPRED_REDUCE_TASK_ENV),\n        HADOOP_CREDENTIAL_PASSWORD_ENVVAR));\n\n    REDACTED_PROPERTIES.forEach(property -> Assert.assertFalse(\n        jobConf.getStringCollection(MRJobConfig.MR_JOB_REDACTED_PROPERTIES)\n            .contains(property)));\n  }"
        ]
    ],
    "4271bbfb02ae81901b1b44e15e0ca6bd1407de9d": [
        [
            "ReplExternalTables::externalTableLocation(HiveConf,String)",
            "  65 -\n  66 -\n  67  \n  68 -\n  69 -\n  70 -\n  71 -\n  72  \n  73  ",
            "  public static String externalTableLocation(HiveConf hiveConf, String location) {\n    String currentPath = new Path(location).toUri().getPath();\n    String baseDir = hiveConf.get(HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname);\n    URI basePath = new Path(baseDir).toUri();\n    String dataPath = currentPath.replaceFirst(Path.SEPARATOR, basePath.getPath() + Path.SEPARATOR);\n    Path dataLocation = new Path(basePath.getScheme(), basePath.getAuthority(), dataPath);\n    LOG.debug(\"incoming location: {} , new location: {}\", location, dataLocation.toString());\n    return dataLocation.toString();\n  }",
            "  65 +\n  66  \n  67 +\n  68 +\n  69 +\n  70 +\n  71 +\n  72 +\n  73 +\n  74 +\n  75 +\n  76 +\n  77 +\n  78 +\n  79 +\n  80  \n  81  ",
            "  public static String externalTableLocation(HiveConf hiveConf, String location) throws SemanticException {\n    String baseDir = hiveConf.get(HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname);\n    Path basePath = new Path(baseDir);\n    Path currentPath = new Path(location);\n    String targetPathWithoutSchemeAndAuth = basePath.toUri().getPath() + currentPath.toUri().getPath();\n    Path dataLocation;\n    try {\n      dataLocation = PathBuilder.fullyQualifiedHDFSUri(\n              new Path(targetPathWithoutSchemeAndAuth),\n              basePath.getFileSystem(hiveConf)\n      );\n    } catch (IOException e) {\n      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(), e);\n    }\n    LOG.info(\"Incoming external table location: {} , new location: {}\", location, dataLocation.toString());\n    return dataLocation.toString();\n  }"
        ],
        [
            "TestReplicationScenariosExternalTables::externalTableReplicationWithCustomPaths()",
            " 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 -\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  ",
            "  @Test\n  public void externalTableReplicationWithCustomPaths() throws Throwable {\n    Path externalTableLocation =\n        new Path(\"/\" + testName.getMethodName() + \"/\" + primaryDbName + \"/\" + \"a/\");\n    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n    fs.mkdirs(externalTableLocation, new FsPermission(\"777\"));\n\n    List<String> loadWithClause = externalTableBasePathWithClause();\n\n    WarehouseInstance.Tuple bootstrapTuple = primary.run(\"use \" + primaryDbName)\n        .run(\"create external table a (i int, j int) \"\n            + \"row format delimited fields terminated by ',' \"\n            + \"location '\" + externalTableLocation.toUri() + \"'\")\n        .dump(primaryDbName, null);\n\n    replica.load(replicatedDbName, bootstrapTuple.dumpLocation, loadWithClause)\n        .run(\"use \" + replicatedDbName)\n        .run(\"show tables like 'a'\")\n        .verifyResults(Collections.singletonList(\"a\"))\n        .run(\"select * From a\").verifyResults(Collections.emptyList());\n\n    assertTablePartitionLocation(primaryDbName + \".a\", replicatedDbName + \".a\");\n\n    //externally add data to location\n    try (FSDataOutputStream outputStream =\n        fs.create(new Path(externalTableLocation, \"file1.txt\"))) {\n      outputStream.write(\"1,2\\n\".getBytes());\n      outputStream.write(\"13,21\\n\".getBytes());\n    }\n\n    WarehouseInstance.Tuple incrementalTuple = primary.run(\"create table b (i int)\")\n        .dump(primaryDbName, bootstrapTuple.lastReplicationId);\n\n    replica.load(replicatedDbName, incrementalTuple.dumpLocation, loadWithClause)\n        .run(\"select i From a\")\n        .verifyResults(new String[] { \"1\", \"13\" })\n        .run(\"select j from a\")\n        .verifyResults(new String[] { \"2\", \"21\" });\n\n    // alter table location to something new.\n    externalTableLocation =\n        new Path(\"/\" + testName.getMethodName() + \"/\" + primaryDbName + \"/new_location/a/\");\n    incrementalTuple = primary.run(\"use \" + primaryDbName)\n        .run(\"alter table a set location '\" + externalTableLocation + \"'\")\n        .dump(primaryDbName, incrementalTuple.lastReplicationId);\n\n    replica.load(replicatedDbName, incrementalTuple.dumpLocation, loadWithClause)\n        .run(\"use \" + replicatedDbName)\n        .run(\"select i From a\")\n        .verifyResults(Collections.emptyList());\n    assertTablePartitionLocation(primaryDbName + \".a\", replicatedDbName + \".a\");\n  }",
            " 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219 +\n 220 +\n 221 +\n 222 +\n 223 +\n 224 +\n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  ",
            "  @Test\n  public void externalTableReplicationWithCustomPaths() throws Throwable {\n    Path externalTableLocation =\n        new Path(\"/\" + testName.getMethodName() + \"/\" + primaryDbName + \"/\" + \"a/\");\n    DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n    fs.mkdirs(externalTableLocation, new FsPermission(\"777\"));\n\n    // Create base directory but use HDFS path without schema or authority details.\n    // Hive should pick up the local cluster's HDFS schema/authority.\n    externalTableBasePathWithClause();\n    List<String> loadWithClause = Collections.singletonList(\n            \"'\" + HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname + \"'='\"\n                    + REPLICA_EXTERNAL_BASE + \"'\"\n    );\n\n    WarehouseInstance.Tuple bootstrapTuple = primary.run(\"use \" + primaryDbName)\n        .run(\"create external table a (i int, j int) \"\n            + \"row format delimited fields terminated by ',' \"\n            + \"location '\" + externalTableLocation.toUri() + \"'\")\n        .dump(primaryDbName, null);\n\n    replica.load(replicatedDbName, bootstrapTuple.dumpLocation, loadWithClause)\n        .run(\"use \" + replicatedDbName)\n        .run(\"show tables like 'a'\")\n        .verifyResults(Collections.singletonList(\"a\"))\n        .run(\"select * From a\").verifyResults(Collections.emptyList());\n\n    assertTablePartitionLocation(primaryDbName + \".a\", replicatedDbName + \".a\");\n\n    //externally add data to location\n    try (FSDataOutputStream outputStream =\n        fs.create(new Path(externalTableLocation, \"file1.txt\"))) {\n      outputStream.write(\"1,2\\n\".getBytes());\n      outputStream.write(\"13,21\\n\".getBytes());\n    }\n\n    WarehouseInstance.Tuple incrementalTuple = primary.run(\"create table b (i int)\")\n        .dump(primaryDbName, bootstrapTuple.lastReplicationId);\n\n    replica.load(replicatedDbName, incrementalTuple.dumpLocation, loadWithClause)\n        .run(\"select i From a\")\n        .verifyResults(new String[] { \"1\", \"13\" })\n        .run(\"select j from a\")\n        .verifyResults(new String[] { \"2\", \"21\" });\n\n    // alter table location to something new.\n    externalTableLocation =\n        new Path(\"/\" + testName.getMethodName() + \"/\" + primaryDbName + \"/new_location/a/\");\n    incrementalTuple = primary.run(\"use \" + primaryDbName)\n        .run(\"alter table a set location '\" + externalTableLocation + \"'\")\n        .dump(primaryDbName, incrementalTuple.lastReplicationId);\n\n    replica.load(replicatedDbName, incrementalTuple.dumpLocation, loadWithClause)\n        .run(\"use \" + replicatedDbName)\n        .run(\"select i From a\")\n        .verifyResults(Collections.emptyList());\n    assertTablePartitionLocation(primaryDbName + \".a\", replicatedDbName + \".a\");\n  }"
        ]
    ],
    "6d74222521d2a1333990b9b3577ec9a7f7e619b8": [
        [
            "SemanticAnalyzer::analyzeInternal(ASTNode,PlannerContextFactory)",
            "12320  \n12321  \n12322  \n12323  \n12324  \n12325  \n12326  \n12327  \n12328  \n12329  \n12330  \n12331  \n12332  \n12333  \n12334  \n12335  \n12336  \n12337  \n12338  \n12339  \n12340  \n12341  \n12342  \n12343  \n12344  \n12345  \n12346  \n12347  \n12348  \n12349  \n12350  \n12351  \n12352  \n12353  \n12354  \n12355  \n12356  \n12357  \n12358  \n12359  \n12360  \n12361  \n12362  \n12363  \n12364  \n12365  \n12366  \n12367  \n12368  \n12369  \n12370  \n12371  \n12372  \n12373  \n12374  \n12375  \n12376  \n12377  \n12378  \n12379  \n12380  \n12381  \n12382  \n12383  \n12384  \n12385  \n12386  \n12387  \n12388  \n12389  \n12390  \n12391  \n12392  \n12393  \n12394  \n12395  \n12396  \n12397  \n12398  \n12399  \n12400  \n12401  \n12402  \n12403  \n12404  \n12405  \n12406  \n12407  \n12408  \n12409  \n12410  \n12411  \n12412  \n12413  \n12414  \n12415  \n12416  \n12417  \n12418  \n12419  \n12420  \n12421  \n12422  \n12423  \n12424  \n12425  \n12426  \n12427  \n12428  \n12429  \n12430  \n12431  \n12432  \n12433  \n12434  \n12435  \n12436  \n12437  \n12438  \n12439  \n12440  \n12441  \n12442  \n12443  \n12444  \n12445  \n12446  \n12447  \n12448  \n12449  \n12450  \n12451  \n12452  \n12453  \n12454  \n12455  \n12456  \n12457  \n12458  \n12459  \n12460  \n12461  \n12462  \n12463  \n12464  \n12465  \n12466  \n12467  \n12468  \n12469  \n12470  \n12471  \n12472  \n12473  \n12474  \n12475  \n12476  \n12477  \n12478  \n12479  \n12480  \n12481  \n12482  \n12483  \n12484  \n12485  \n12486  \n12487  \n12488  \n12489  \n12490  \n12491  \n12492  \n12493  \n12494  \n12495  \n12496  \n12497  \n12498  \n12499  \n12500  \n12501  \n12502  \n12503  \n12504  \n12505  \n12506  \n12507  \n12508  \n12509  \n12510  \n12511  \n12512  \n12513  \n12514  \n12515  \n12516  \n12517  \n12518  \n12519  \n12520  \n12521  \n12522  \n12523  \n12524  \n12525  \n12526  \n12527  \n12528  \n12529  \n12530  \n12531  \n12532  \n12533  \n12534  \n12535  \n12536  \n12537  \n12538  \n12539  \n12540  \n12541  \n12542  \n12543  \n12544  \n12545  \n12546  \n12547  \n12548  \n12549  \n12550  \n12551  \n12552  \n12553  \n12554  \n12555  \n12556  \n12557  \n12558  \n12559  \n12560  \n12561  \n12562  \n12563  \n12564  \n12565  ",
            "  void analyzeInternal(ASTNode ast, PlannerContextFactory pcf) throws SemanticException {\n    LOG.info(\"Starting Semantic Analysis\");\n    // 1. Generate Resolved Parse tree from syntax tree\n    boolean needsTransform = needsTransform();\n    //change the location of position alias process here\n    processPositionAlias(ast);\n    PlannerContext plannerCtx = pcf.create();\n    if (!genResolvedParseTree(ast, plannerCtx)) {\n      return;\n    }\n\n    if (HiveConf.getBoolVar(conf, ConfVars.HIVE_REMOVE_ORDERBY_IN_SUBQUERY)) {\n      for (String alias : qb.getSubqAliases()) {\n        removeOBInSubQuery(qb.getSubqForAlias(alias));\n      }\n    }\n\n    // Check query results cache.\n    // If no masking/filtering required, then we can check the cache now, before\n    // generating the operator tree and going through CBO.\n    // Otherwise we have to wait until after the masking/filtering step.\n    boolean isCacheEnabled = isResultsCacheEnabled();\n    QueryResultsCache.LookupInfo lookupInfo = null;\n    if (isCacheEnabled && !needsTransform && queryTypeCanUseCache()) {\n      lookupInfo = createLookupInfoForQuery(ast);\n      if (checkResultsCache(lookupInfo, false)) {\n        return;\n      }\n    }\n\n    ASTNode astForMasking;\n    if (isCBOExecuted() && needsTransform &&\n        (qb.isCTAS() || qb.isView() || qb.isMaterializedView() || qb.isMultiDestQuery())) {\n      // If we use CBO and we may apply masking/filtering policies, we create a copy of the ast.\n      // The reason is that the generation of the operator tree may modify the initial ast,\n      // but if we need to parse for a second time, we would like to parse the unmodified ast.\n      astForMasking = (ASTNode) ParseDriver.adaptor.dupTree(ast);\n    } else {\n      astForMasking = ast;\n    }\n\n    // 2. Gen OP Tree from resolved Parse Tree\n    Operator sinkOp = genOPTree(ast, plannerCtx);\n\n    boolean usesMasking = false;\n    if (!unparseTranslator.isEnabled() &&\n        (tableMask.isEnabled() && analyzeRewrite == null)) {\n      // Here we rewrite the * and also the masking table\n      ASTNode rewrittenAST = rewriteASTWithMaskAndFilter(tableMask, astForMasking, ctx.getTokenRewriteStream(),\n          ctx, db, tabNameToTabObject, ignoredTokens);\n      if (astForMasking != rewrittenAST) {\n        usesMasking = true;\n        plannerCtx = pcf.create();\n        ctx.setSkipTableMasking(true);\n        init(true);\n        //change the location of position alias process here\n        processPositionAlias(rewrittenAST);\n        genResolvedParseTree(rewrittenAST, plannerCtx);\n        if (this instanceof CalcitePlanner) {\n          ((CalcitePlanner) this).resetCalciteConfiguration();\n        }\n        sinkOp = genOPTree(rewrittenAST, plannerCtx);\n      }\n    }\n\n    // Check query results cache\n    // In the case that row or column masking/filtering was required, we do not support caching.\n    // TODO: Enable caching for queries with masking/filtering\n    if (isCacheEnabled && needsTransform && !usesMasking && queryTypeCanUseCache()) {\n      lookupInfo = createLookupInfoForQuery(ast);\n      if (checkResultsCache(lookupInfo, false)) {\n        return;\n      }\n    }\n\n    // 3. Deduce Resultset Schema\n    if (createVwDesc != null && !this.ctx.isCboSucceeded()) {\n      resultSchema = convertRowSchemaToViewSchema(opParseCtx.get(sinkOp).getRowResolver());\n    } else {\n      // resultSchema will be null if\n      // (1) cbo is disabled;\n      // (2) or cbo is enabled with AST return path (whether succeeded or not,\n      // resultSchema will be re-initialized)\n      // It will only be not null if cbo is enabled with new return path and it\n      // succeeds.\n      if (resultSchema == null) {\n        resultSchema = convertRowSchemaToResultSetSchema(opParseCtx.get(sinkOp).getRowResolver(),\n            HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES));\n      }\n    }\n\n    // 4. Generate Parse Context for Optimizer & Physical compiler\n    copyInfoToQueryProperties(queryProperties);\n    ParseContext pCtx = new ParseContext(queryState, opToPartPruner, opToPartList, topOps,\n        new HashSet<JoinOperator>(joinContext.keySet()),\n        new HashSet<SMBMapJoinOperator>(smbMapJoinContext.keySet()),\n        loadTableWork, loadFileWork, columnStatsAutoGatherContexts, ctx, idToTableNameMap, destTableId, uCtx,\n        listMapJoinOpsNoReducer, prunedPartitions, tabNameToTabObject, opToSamplePruner,\n        globalLimitCtx, nameToSplitSample, inputs, rootTasks, opToPartToSkewedPruner,\n        viewAliasToInput, reduceSinkOperatorsAddedByEnforceBucketingSorting,\n        analyzeRewrite, tableDesc, createVwDesc, materializedViewUpdateDesc,\n        queryProperties, viewProjectToTableSchema, acidFileSinks);\n\n    // Set the semijoin hints in parse context\n    pCtx.setSemiJoinHints(parseSemiJoinHint(getQB().getParseInfo().getHintList()));\n    // Set the mapjoin hint if it needs to be disabled.\n    pCtx.setDisableMapJoin(disableMapJoinWithHint(getQB().getParseInfo().getHintList()));\n\n    // 5. Take care of view creation\n    if (createVwDesc != null) {\n      if (ctx.getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return;\n      }\n\n      if (!ctx.isCboSucceeded()) {\n        saveViewDefinition();\n      }\n\n      // validate the create view statement at this point, the createVwDesc gets\n      // all the information for semanticcheck\n      validateCreateView();\n\n      if (createVwDesc.isMaterialized()) {\n        createVwDesc.setTablesUsed(getTablesUsed(pCtx));\n      } else {\n        // Since we're only creating a view (not executing it), we don't need to\n        // optimize or translate the plan (and in fact, those procedures can\n        // interfere with the view creation). So skip the rest of this method.\n        ctx.setResDir(null);\n        ctx.setResFile(null);\n\n        try {\n          PlanUtils.addInputsForView(pCtx);\n        } catch (HiveException e) {\n          throw new SemanticException(e);\n        }\n\n        // Generate lineage info for create view statements\n        // if LineageLogger hook is configured.\n        // Add the transformation that computes the lineage information.\n        Set<String> postExecHooks = Sets.newHashSet(Splitter.on(\",\").trimResults()\n            .omitEmptyStrings()\n            .split(Strings.nullToEmpty(HiveConf.getVar(conf, HiveConf.ConfVars.POSTEXECHOOKS))));\n        if (postExecHooks.contains(\"org.apache.hadoop.hive.ql.hooks.PostExecutePrinter\")\n            || postExecHooks.contains(\"org.apache.hadoop.hive.ql.hooks.LineageLogger\")\n            || postExecHooks.contains(\"org.apache.atlas.hive.hook.HiveHook\")) {\n          ArrayList<Transform> transformations = new ArrayList<Transform>();\n          transformations.add(new HiveOpConverterPostProc());\n          transformations.add(new Generator(postExecHooks));\n          for (Transform t : transformations) {\n            pCtx = t.transform(pCtx);\n          }\n          // we just use view name as location.\n          queryState.getLineageState()\n              .mapDirToOp(new Path(createVwDesc.getViewName()), sinkOp);\n        }\n        return;\n      }\n    }\n\n    // 6. Generate table access stats if required\n    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_TABLEKEYS)) {\n      TableAccessAnalyzer tableAccessAnalyzer = new TableAccessAnalyzer(pCtx);\n      setTableAccessInfo(tableAccessAnalyzer.analyzeTableAccess());\n    }\n\n    // 7. Perform Logical optimization\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Before logical optimization\\n\" + Operator.toString(pCtx.getTopOps().values()));\n    }\n    Optimizer optm = new Optimizer();\n    optm.setPctx(pCtx);\n    optm.initialize(conf);\n    pCtx = optm.optimize();\n    if (pCtx.getColumnAccessInfo() != null) {\n      // set ColumnAccessInfo for view column authorization\n      setColumnAccessInfo(pCtx.getColumnAccessInfo());\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After logical optimization\\n\" + Operator.toString(pCtx.getTopOps().values()));\n    }\n\n    // 8. Generate column access stats if required - wait until column pruning\n    // takes place during optimization\n    boolean isColumnInfoNeedForAuth = SessionState.get().isAuthorizationModeV2()\n        && HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED);\n    if (isColumnInfoNeedForAuth\n        || HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS)) {\n      ColumnAccessAnalyzer columnAccessAnalyzer = new ColumnAccessAnalyzer(pCtx);\n      // view column access info is carried by this.getColumnAccessInfo().\n      setColumnAccessInfo(columnAccessAnalyzer.analyzeColumnAccess(this.getColumnAccessInfo()));\n    }\n\n    // 9. Optimize Physical op tree & Translate to target execution engine (MR,\n    // TEZ..)\n    if (!ctx.getExplainLogical()) {\n      TaskCompiler compiler = TaskCompilerFactory.getCompiler(conf, pCtx);\n      compiler.init(queryState, console, db);\n      compiler.compile(pCtx, rootTasks, inputs, outputs);\n      fetchTask = pCtx.getFetchTask();\n    }\n    //find all Acid FileSinkOperatorS\n    QueryPlanPostProcessor qp = new QueryPlanPostProcessor(rootTasks, acidFileSinks, ctx.getExecutionId());\n\n    // 10. Attach CTAS/Insert-Commit-hooks for Storage Handlers\n    final Optional<TezTask> optionalTezTask =\n        rootTasks.stream().filter(task -> task instanceof TezTask).map(task -> (TezTask) task)\n            .findFirst();\n    if (optionalTezTask.isPresent()) {\n      final TezTask tezTask = optionalTezTask.get();\n      rootTasks.stream()\n          .filter(task -> task.getWork() instanceof DDLWork2)\n          .map(task -> (DDLWork2) task.getWork())\n          .filter(ddlWork -> ddlWork.getDDLDesc() != null)\n          .map(ddlWork -> (PreInsertTableDesc)ddlWork.getDDLDesc())\n          .map(ddlPreInsertTask -> new InsertCommitHookDesc(ddlPreInsertTask.getTable(),\n              ddlPreInsertTask.isOverwrite()))\n          .forEach(insertCommitHookDesc -> tezTask.addDependentTask(\n              TaskFactory.get(new DDLWork(getInputs(), getOutputs(), insertCommitHookDesc), conf)));\n    }\n\n    LOG.info(\"Completed plan generation\");\n\n    // 11. put accessed columns to readEntity\n    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS)) {\n      putAccessedColumnsToReadEntity(inputs, columnAccessInfo);\n    }\n\n    if (isCacheEnabled && lookupInfo != null) {\n      if (queryCanBeCached()) {\n        // Last chance - check if the query is available in the cache.\n        // Since we have already generated a query plan, using a cached query result at this point\n        // requires SemanticAnalyzer state to be reset.\n        if (checkResultsCache(lookupInfo, true)) {\n          LOG.info(\"Cached result found on second lookup\");\n          return;\n        } else {\n          QueryResultsCache.QueryInfo queryInfo = createCacheQueryInfoForQuery(lookupInfo);\n\n          // Specify that the results of this query can be cached.\n          setCacheUsage(new CacheUsage(\n              CacheUsage.CacheStatus.CAN_CACHE_QUERY_RESULTS, queryInfo));\n        }\n      }\n    }\n  }",
            "12320  \n12321  \n12322  \n12323  \n12324  \n12325  \n12326  \n12327  \n12328  \n12329  \n12330  \n12331  \n12332  \n12333  \n12334  \n12335  \n12336  \n12337 +\n12338 +\n12339 +\n12340 +\n12341 +\n12342 +\n12343 +\n12344 +\n12345 +\n12346 +\n12347 +\n12348  \n12349  \n12350  \n12351  \n12352  \n12353  \n12354  \n12355  \n12356  \n12357  \n12358  \n12359  \n12360  \n12361  \n12362  \n12363  \n12364  \n12365  \n12366  \n12367  \n12368  \n12369  \n12370  \n12371  \n12372  \n12373  \n12374  \n12375  \n12376  \n12377  \n12378  \n12379  \n12380  \n12381  \n12382  \n12383  \n12384  \n12385  \n12386  \n12387  \n12388  \n12389  \n12390  \n12391  \n12392  \n12393  \n12394  \n12395  \n12396  \n12397  \n12398  \n12399  \n12400  \n12401  \n12402  \n12403  \n12404  \n12405  \n12406  \n12407  \n12408  \n12409  \n12410  \n12411  \n12412  \n12413  \n12414  \n12415  \n12416  \n12417  \n12418  \n12419  \n12420  \n12421  \n12422  \n12423  \n12424  \n12425  \n12426  \n12427  \n12428  \n12429  \n12430  \n12431  \n12432  \n12433  \n12434  \n12435  \n12436  \n12437  \n12438  \n12439  \n12440  \n12441  \n12442  \n12443  \n12444  \n12445  \n12446  \n12447  \n12448  \n12449  \n12450  \n12451  \n12452  \n12453  \n12454  \n12455  \n12456  \n12457  \n12458  \n12459  \n12460  \n12461  \n12462  \n12463  \n12464  \n12465  \n12466  \n12467  \n12468  \n12469  \n12470  \n12471  \n12472  \n12473  \n12474  \n12475  \n12476  \n12477  \n12478  \n12479  \n12480  \n12481  \n12482  \n12483  \n12484  \n12485  \n12486  \n12487  \n12488  \n12489  \n12490  \n12491  \n12492  \n12493  \n12494  \n12495  \n12496  \n12497  \n12498  \n12499  \n12500  \n12501  \n12502  \n12503  \n12504  \n12505  \n12506  \n12507  \n12508  \n12509  \n12510  \n12511  \n12512  \n12513  \n12514  \n12515  \n12516  \n12517  \n12518  \n12519  \n12520  \n12521  \n12522  \n12523  \n12524  \n12525  \n12526  \n12527  \n12528  \n12529  \n12530  \n12531  \n12532  \n12533  \n12534  \n12535  \n12536  \n12537  \n12538  \n12539  \n12540  \n12541  \n12542  \n12543  \n12544  \n12545  \n12546  \n12547  \n12548  \n12549  \n12550  \n12551  \n12552  \n12553  \n12554  \n12555  \n12556  \n12557  \n12558  \n12559  \n12560  \n12561  \n12562  \n12563  \n12564  \n12565  \n12566  \n12567  \n12568  \n12569  \n12570  \n12571  \n12572  \n12573  \n12574  \n12575  \n12576  ",
            "  void analyzeInternal(ASTNode ast, PlannerContextFactory pcf) throws SemanticException {\n    LOG.info(\"Starting Semantic Analysis\");\n    // 1. Generate Resolved Parse tree from syntax tree\n    boolean needsTransform = needsTransform();\n    //change the location of position alias process here\n    processPositionAlias(ast);\n    PlannerContext plannerCtx = pcf.create();\n    if (!genResolvedParseTree(ast, plannerCtx)) {\n      return;\n    }\n\n    if (HiveConf.getBoolVar(conf, ConfVars.HIVE_REMOVE_ORDERBY_IN_SUBQUERY)) {\n      for (String alias : qb.getSubqAliases()) {\n        removeOBInSubQuery(qb.getSubqForAlias(alias));\n      }\n    }\n\n    final String llapIOETLSkipFormat = HiveConf.getVar(conf, ConfVars.LLAP_IO_ETL_SKIP_FORMAT);\n    if (qb.getParseInfo().hasInsertTables() || qb.isCTAS()) {\n      if (llapIOETLSkipFormat.equalsIgnoreCase(\"encode\")) {\n        conf.setBoolean(ConfVars.LLAP_IO_ENCODE_ENABLED.varname, false);\n        LOG.info(\"Disabling LLAP IO encode as ETL query is detected\");\n      } else if (llapIOETLSkipFormat.equalsIgnoreCase(\"all\")) {\n        conf.setBoolean(ConfVars.LLAP_IO_ENABLED.varname, false);\n        LOG.info(\"Disabling LLAP IO as ETL query is detected\");\n      }\n    }\n\n    // Check query results cache.\n    // If no masking/filtering required, then we can check the cache now, before\n    // generating the operator tree and going through CBO.\n    // Otherwise we have to wait until after the masking/filtering step.\n    boolean isCacheEnabled = isResultsCacheEnabled();\n    QueryResultsCache.LookupInfo lookupInfo = null;\n    if (isCacheEnabled && !needsTransform && queryTypeCanUseCache()) {\n      lookupInfo = createLookupInfoForQuery(ast);\n      if (checkResultsCache(lookupInfo, false)) {\n        return;\n      }\n    }\n\n    ASTNode astForMasking;\n    if (isCBOExecuted() && needsTransform &&\n        (qb.isCTAS() || qb.isView() || qb.isMaterializedView() || qb.isMultiDestQuery())) {\n      // If we use CBO and we may apply masking/filtering policies, we create a copy of the ast.\n      // The reason is that the generation of the operator tree may modify the initial ast,\n      // but if we need to parse for a second time, we would like to parse the unmodified ast.\n      astForMasking = (ASTNode) ParseDriver.adaptor.dupTree(ast);\n    } else {\n      astForMasking = ast;\n    }\n\n    // 2. Gen OP Tree from resolved Parse Tree\n    Operator sinkOp = genOPTree(ast, plannerCtx);\n\n    boolean usesMasking = false;\n    if (!unparseTranslator.isEnabled() &&\n        (tableMask.isEnabled() && analyzeRewrite == null)) {\n      // Here we rewrite the * and also the masking table\n      ASTNode rewrittenAST = rewriteASTWithMaskAndFilter(tableMask, astForMasking, ctx.getTokenRewriteStream(),\n          ctx, db, tabNameToTabObject, ignoredTokens);\n      if (astForMasking != rewrittenAST) {\n        usesMasking = true;\n        plannerCtx = pcf.create();\n        ctx.setSkipTableMasking(true);\n        init(true);\n        //change the location of position alias process here\n        processPositionAlias(rewrittenAST);\n        genResolvedParseTree(rewrittenAST, plannerCtx);\n        if (this instanceof CalcitePlanner) {\n          ((CalcitePlanner) this).resetCalciteConfiguration();\n        }\n        sinkOp = genOPTree(rewrittenAST, plannerCtx);\n      }\n    }\n\n    // Check query results cache\n    // In the case that row or column masking/filtering was required, we do not support caching.\n    // TODO: Enable caching for queries with masking/filtering\n    if (isCacheEnabled && needsTransform && !usesMasking && queryTypeCanUseCache()) {\n      lookupInfo = createLookupInfoForQuery(ast);\n      if (checkResultsCache(lookupInfo, false)) {\n        return;\n      }\n    }\n\n    // 3. Deduce Resultset Schema\n    if (createVwDesc != null && !this.ctx.isCboSucceeded()) {\n      resultSchema = convertRowSchemaToViewSchema(opParseCtx.get(sinkOp).getRowResolver());\n    } else {\n      // resultSchema will be null if\n      // (1) cbo is disabled;\n      // (2) or cbo is enabled with AST return path (whether succeeded or not,\n      // resultSchema will be re-initialized)\n      // It will only be not null if cbo is enabled with new return path and it\n      // succeeds.\n      if (resultSchema == null) {\n        resultSchema = convertRowSchemaToResultSetSchema(opParseCtx.get(sinkOp).getRowResolver(),\n            HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES));\n      }\n    }\n\n    // 4. Generate Parse Context for Optimizer & Physical compiler\n    copyInfoToQueryProperties(queryProperties);\n    ParseContext pCtx = new ParseContext(queryState, opToPartPruner, opToPartList, topOps,\n        new HashSet<JoinOperator>(joinContext.keySet()),\n        new HashSet<SMBMapJoinOperator>(smbMapJoinContext.keySet()),\n        loadTableWork, loadFileWork, columnStatsAutoGatherContexts, ctx, idToTableNameMap, destTableId, uCtx,\n        listMapJoinOpsNoReducer, prunedPartitions, tabNameToTabObject, opToSamplePruner,\n        globalLimitCtx, nameToSplitSample, inputs, rootTasks, opToPartToSkewedPruner,\n        viewAliasToInput, reduceSinkOperatorsAddedByEnforceBucketingSorting,\n        analyzeRewrite, tableDesc, createVwDesc, materializedViewUpdateDesc,\n        queryProperties, viewProjectToTableSchema, acidFileSinks);\n\n    // Set the semijoin hints in parse context\n    pCtx.setSemiJoinHints(parseSemiJoinHint(getQB().getParseInfo().getHintList()));\n    // Set the mapjoin hint if it needs to be disabled.\n    pCtx.setDisableMapJoin(disableMapJoinWithHint(getQB().getParseInfo().getHintList()));\n\n    // 5. Take care of view creation\n    if (createVwDesc != null) {\n      if (ctx.getExplainAnalyze() == AnalyzeState.RUNNING) {\n        return;\n      }\n\n      if (!ctx.isCboSucceeded()) {\n        saveViewDefinition();\n      }\n\n      // validate the create view statement at this point, the createVwDesc gets\n      // all the information for semanticcheck\n      validateCreateView();\n\n      if (createVwDesc.isMaterialized()) {\n        createVwDesc.setTablesUsed(getTablesUsed(pCtx));\n      } else {\n        // Since we're only creating a view (not executing it), we don't need to\n        // optimize or translate the plan (and in fact, those procedures can\n        // interfere with the view creation). So skip the rest of this method.\n        ctx.setResDir(null);\n        ctx.setResFile(null);\n\n        try {\n          PlanUtils.addInputsForView(pCtx);\n        } catch (HiveException e) {\n          throw new SemanticException(e);\n        }\n\n        // Generate lineage info for create view statements\n        // if LineageLogger hook is configured.\n        // Add the transformation that computes the lineage information.\n        Set<String> postExecHooks = Sets.newHashSet(Splitter.on(\",\").trimResults()\n            .omitEmptyStrings()\n            .split(Strings.nullToEmpty(HiveConf.getVar(conf, HiveConf.ConfVars.POSTEXECHOOKS))));\n        if (postExecHooks.contains(\"org.apache.hadoop.hive.ql.hooks.PostExecutePrinter\")\n            || postExecHooks.contains(\"org.apache.hadoop.hive.ql.hooks.LineageLogger\")\n            || postExecHooks.contains(\"org.apache.atlas.hive.hook.HiveHook\")) {\n          ArrayList<Transform> transformations = new ArrayList<Transform>();\n          transformations.add(new HiveOpConverterPostProc());\n          transformations.add(new Generator(postExecHooks));\n          for (Transform t : transformations) {\n            pCtx = t.transform(pCtx);\n          }\n          // we just use view name as location.\n          queryState.getLineageState()\n              .mapDirToOp(new Path(createVwDesc.getViewName()), sinkOp);\n        }\n        return;\n      }\n    }\n\n    // 6. Generate table access stats if required\n    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_TABLEKEYS)) {\n      TableAccessAnalyzer tableAccessAnalyzer = new TableAccessAnalyzer(pCtx);\n      setTableAccessInfo(tableAccessAnalyzer.analyzeTableAccess());\n    }\n\n    // 7. Perform Logical optimization\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Before logical optimization\\n\" + Operator.toString(pCtx.getTopOps().values()));\n    }\n    Optimizer optm = new Optimizer();\n    optm.setPctx(pCtx);\n    optm.initialize(conf);\n    pCtx = optm.optimize();\n    if (pCtx.getColumnAccessInfo() != null) {\n      // set ColumnAccessInfo for view column authorization\n      setColumnAccessInfo(pCtx.getColumnAccessInfo());\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After logical optimization\\n\" + Operator.toString(pCtx.getTopOps().values()));\n    }\n\n    // 8. Generate column access stats if required - wait until column pruning\n    // takes place during optimization\n    boolean isColumnInfoNeedForAuth = SessionState.get().isAuthorizationModeV2()\n        && HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED);\n    if (isColumnInfoNeedForAuth\n        || HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS)) {\n      ColumnAccessAnalyzer columnAccessAnalyzer = new ColumnAccessAnalyzer(pCtx);\n      // view column access info is carried by this.getColumnAccessInfo().\n      setColumnAccessInfo(columnAccessAnalyzer.analyzeColumnAccess(this.getColumnAccessInfo()));\n    }\n\n    // 9. Optimize Physical op tree & Translate to target execution engine (MR,\n    // TEZ..)\n    if (!ctx.getExplainLogical()) {\n      TaskCompiler compiler = TaskCompilerFactory.getCompiler(conf, pCtx);\n      compiler.init(queryState, console, db);\n      compiler.compile(pCtx, rootTasks, inputs, outputs);\n      fetchTask = pCtx.getFetchTask();\n    }\n    //find all Acid FileSinkOperatorS\n    QueryPlanPostProcessor qp = new QueryPlanPostProcessor(rootTasks, acidFileSinks, ctx.getExecutionId());\n\n    // 10. Attach CTAS/Insert-Commit-hooks for Storage Handlers\n    final Optional<TezTask> optionalTezTask =\n        rootTasks.stream().filter(task -> task instanceof TezTask).map(task -> (TezTask) task)\n            .findFirst();\n    if (optionalTezTask.isPresent()) {\n      final TezTask tezTask = optionalTezTask.get();\n      rootTasks.stream()\n          .filter(task -> task.getWork() instanceof DDLWork2)\n          .map(task -> (DDLWork2) task.getWork())\n          .filter(ddlWork -> ddlWork.getDDLDesc() != null)\n          .map(ddlWork -> (PreInsertTableDesc)ddlWork.getDDLDesc())\n          .map(ddlPreInsertTask -> new InsertCommitHookDesc(ddlPreInsertTask.getTable(),\n              ddlPreInsertTask.isOverwrite()))\n          .forEach(insertCommitHookDesc -> tezTask.addDependentTask(\n              TaskFactory.get(new DDLWork(getInputs(), getOutputs(), insertCommitHookDesc), conf)));\n    }\n\n    LOG.info(\"Completed plan generation\");\n\n    // 11. put accessed columns to readEntity\n    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS)) {\n      putAccessedColumnsToReadEntity(inputs, columnAccessInfo);\n    }\n\n    if (isCacheEnabled && lookupInfo != null) {\n      if (queryCanBeCached()) {\n        // Last chance - check if the query is available in the cache.\n        // Since we have already generated a query plan, using a cached query result at this point\n        // requires SemanticAnalyzer state to be reset.\n        if (checkResultsCache(lookupInfo, true)) {\n          LOG.info(\"Cached result found on second lookup\");\n          return;\n        } else {\n          QueryResultsCache.QueryInfo queryInfo = createCacheQueryInfoForQuery(lookupInfo);\n\n          // Specify that the results of this query can be cached.\n          setCacheUsage(new CacheUsage(\n              CacheUsage.CacheStatus.CAN_CACHE_QUERY_RESULTS, queryInfo));\n        }\n      }\n    }\n  }"
        ]
    ],
    "d7fbd639ff0188b0e5d22ada50795b1a0784a0be": [
        [
            "VectorReduceSinkUniformHashOperator::initializeOp(Configuration)",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 -\n  90  \n  91  \n  92  \n  93  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n\n    Preconditions.checkState(!isEmptyKey);\n    // Create all nulls key.\n    try {\n      Output nullKeyOutput = new Output();\n      keyBinarySortableSerializeWrite.set(nullKeyOutput);\n      for (int i = 0; i < reduceSinkKeyColumnMap.length; i++) {\n        keyBinarySortableSerializeWrite.writeNull();\n      }\n      int nullBytesLength = nullKeyOutput.getLength();\n      nullBytes = new byte[nullBytesLength];\n      System.arraycopy(nullKeyOutput.getData(), 0, nullBytes, 0, nullBytesLength);\n      nullKeyHashCode = HashCodeUtil.calculateBytesHashCode(nullBytes, 0, nullBytesLength);\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            "  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89 +\n  90  \n  91  \n  92  \n  93  ",
            "  @Override\n  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n\n    Preconditions.checkState(!isEmptyKey);\n    // Create all nulls key.\n    try {\n      Output nullKeyOutput = new Output();\n      keyBinarySortableSerializeWrite.set(nullKeyOutput);\n      for (int i = 0; i < reduceSinkKeyColumnMap.length; i++) {\n        keyBinarySortableSerializeWrite.writeNull();\n      }\n      int nullBytesLength = nullKeyOutput.getLength();\n      nullBytes = new byte[nullBytesLength];\n      System.arraycopy(nullKeyOutput.getData(), 0, nullBytes, 0, nullBytesLength);\n      nullKeyHashCode = Murmur3.hash32(nullBytes, 0, nullBytesLength, 0);\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }"
        ],
        [
            "HashCodeUtil::calculateBytesHashCode(byte,int,int)",
            "  50  \n  51  \n  52  ",
            "  public static int calculateBytesHashCode(byte[] keyBytes, int keyStart, int keyLength) {\n    return murmurHash(keyBytes, keyStart, keyLength);\n  }",
            "  50  \n  51  \n  52 +\n  53  \n  54  ",
            "  @Deprecated\n  public static int calculateBytesHashCode(byte[] keyBytes, int keyStart, int keyLength) {\n    // Don't use this for ReduceSinkOperators\n    return murmurHash(keyBytes, keyStart, keyLength);\n  }"
        ]
    ],
    "734cc2c6a80b99b7c692b20b0df7d7d59cbaffd5": [
        [
            "SharedWorkOptimizer::validPreConditions(ParseContext,SharedWorkOptimizerCache,SharedResult)",
            "1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377 -\n1378 -\n1379 -\n1380  \n1381  \n1382  \n1383  \n1384 -\n1385  \n1386  \n1387  \n1388  \n1389  \n1390  \n1391 -\n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399 -\n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  ",
            "  private static boolean validPreConditions(ParseContext pctx, SharedWorkOptimizerCache optimizerCache,\n          SharedResult sr) {\n\n    // We check whether merging the works would cause the size of\n    // the data in memory grow too large.\n    // TODO: Currently ignores GBY and PTF which may also buffer data in memory.\n    if (sr.dataSize > sr.maxDataSize) {\n      // Size surpasses limit, we cannot convert\n      LOG.debug(\"accumulated data size: {} / max size: {}\", sr.dataSize, sr.maxDataSize);\n      return false;\n    }\n\n    Operator<?> op1 = sr.retainableOps.get(0);\n    Operator<?> op2 = sr.discardableOps.get(0);\n\n    // 1) The set of operators in the works that we are merging need to meet\n    // some requirements. In particular:\n    // 1.1. None of the works that we are merging can contain a Union\n    // operator. This is not supported yet as we might end up with cycles in\n    // the Tez DAG.\n    // 1.2. There cannot be more than one DummyStore operator in the new resulting\n    // work when the operators are merged. This is due to an assumption in\n    // MergeJoinProc that needs to be further explored.\n    // If any of these conditions are not met, we cannot merge.\n    // TODO: Extend rule so it can be applied for these cases.\n    final Set<Operator<?>> workOps1 = findWorkOperators(optimizerCache, op1);\n    final Set<Operator<?>> workOps2 = findWorkOperators(optimizerCache, op2);\n    boolean foundDummyStoreOp = false;\n    for (Operator<?> op : workOps1) {\n      if (op instanceof UnionOperator) {\n        // We cannot merge (1.1)\n        return false;\n      }\n      if (op instanceof DummyStoreOperator) {\n        foundDummyStoreOp = true;\n      }\n    }\n    for (Operator<?> op : workOps2) {\n      if (op instanceof UnionOperator) {\n        // We cannot merge (1.1)\n        return false;\n      }\n      if (foundDummyStoreOp && op instanceof DummyStoreOperator) {\n        // We cannot merge (1.2)\n        return false;\n      }\n    }\n    // 2) We check whether output works when we merge the operators will collide.\n    //\n    //   Work1   Work2    (merge TS in W1 & W2)        Work1\n    //       \\   /                  ->                  | |       X\n    //       Work3                                     Work3\n    //\n    // If we do, we cannot merge. The reason is that Tez currently does\n    // not support parallel edges, i.e., multiple edges from same work x\n    // into same work y.\n    final Set<Operator<?>> outputWorksOps1 = findChildWorkOperators(pctx, optimizerCache, op1);\n    final Set<Operator<?>> outputWorksOps2 = findChildWorkOperators(pctx, optimizerCache, op2);\n    if (!Collections.disjoint(outputWorksOps1, outputWorksOps2)) {\n      // We cannot merge\n      return false;\n    }\n    // 3) We check whether we will end up with same operators inputing on same work.\n    //\n    //       Work1        (merge TS in W2 & W3)        Work1\n    //       /   \\                  ->                  | |       X\n    //   Work2   Work3                                 Work2\n    //\n    // If we do, we cannot merge. The reason is the same as above, currently\n    // Tez does not support parallel edges.\n    //\n    // In the check, we exclude the inputs to the root operator that we are trying\n    // to merge (only useful for extended merging as TS do not have inputs).\n    final Set<Operator<?>> excludeOps1 = sr.retainableOps.get(0).getNumParent() > 0 ?\n        ImmutableSet.copyOf(sr.retainableOps.get(0).getParentOperators()) : ImmutableSet.of();\n    final Set<Operator<?>> inputWorksOps1 =\n        findParentWorkOperators(pctx, optimizerCache, op1, excludeOps1);\n    final Set<Operator<?>> excludeOps2 = sr.discardableOps.get(0).getNumParent() > 0 ?\n        Sets.union(ImmutableSet.copyOf(sr.discardableOps.get(0).getParentOperators()), sr.discardableInputOps) :\n            sr.discardableInputOps;\n    final Set<Operator<?>> inputWorksOps2 =\n        findParentWorkOperators(pctx, optimizerCache, op2, excludeOps2);\n    if (!Collections.disjoint(inputWorksOps1, inputWorksOps2)) {\n      // We cannot merge\n      return false;\n    }\n    // 4) We check whether one of the operators is part of a work that is an input for\n    // the work of the other operator.\n    //\n    //   Work1            (merge TS in W1 & W3)        Work1\n    //     |                        ->                   |        X\n    //   Work2                                         Work2\n    //     |                                             |\n    //   Work3                                         Work1\n    //\n    // If we do, we cannot merge, as we would end up with a cycle in the DAG.\n    final Set<Operator<?>> descendantWorksOps1 =\n            findDescendantWorkOperators(pctx, optimizerCache, op1, sr.discardableInputOps);\n    final Set<Operator<?>> descendantWorksOps2 =\n            findDescendantWorkOperators(pctx, optimizerCache, op2, sr.discardableInputOps);\n    if (!Collections.disjoint(descendantWorksOps1, workOps2)\n            || !Collections.disjoint(workOps1, descendantWorksOps2)) {\n      return false;\n    }\n    return true;\n  }",
            "1357  \n1358  \n1359  \n1360  \n1361  \n1362  \n1363  \n1364  \n1365  \n1366  \n1367  \n1368  \n1369  \n1370  \n1371  \n1372  \n1373  \n1374  \n1375  \n1376  \n1377 +\n1378 +\n1379 +\n1380  \n1381  \n1382  \n1383  \n1384  \n1385  \n1386  \n1387  \n1388  \n1389  \n1390 +\n1391 +\n1392  \n1393  \n1394  \n1395  \n1396  \n1397  \n1398  \n1399 +\n1400  \n1401  \n1402  \n1403  \n1404  \n1405  \n1406  \n1407  \n1408  \n1409  \n1410  \n1411  \n1412  \n1413  \n1414  \n1415  \n1416  \n1417  \n1418  \n1419  \n1420  \n1421  \n1422  \n1423  \n1424  \n1425  \n1426  \n1427  \n1428  \n1429  \n1430  \n1431  \n1432  \n1433  \n1434  \n1435  \n1436  \n1437  \n1438  \n1439  \n1440  \n1441  \n1442  \n1443  \n1444  \n1445  \n1446  \n1447  \n1448  \n1449  \n1450  \n1451  \n1452  \n1453  \n1454  \n1455  \n1456  \n1457  \n1458  \n1459  \n1460  \n1461  \n1462  ",
            "  private static boolean validPreConditions(ParseContext pctx, SharedWorkOptimizerCache optimizerCache,\n          SharedResult sr) {\n\n    // We check whether merging the works would cause the size of\n    // the data in memory grow too large.\n    // TODO: Currently ignores GBY and PTF which may also buffer data in memory.\n    if (sr.dataSize > sr.maxDataSize) {\n      // Size surpasses limit, we cannot convert\n      LOG.debug(\"accumulated data size: {} / max size: {}\", sr.dataSize, sr.maxDataSize);\n      return false;\n    }\n\n    Operator<?> op1 = sr.retainableOps.get(0);\n    Operator<?> op2 = sr.discardableOps.get(0);\n\n    // 1) The set of operators in the works that we are merging need to meet\n    // some requirements. In particular:\n    // 1.1. None of the works that we are merging can contain a Union\n    // operator. This is not supported yet as we might end up with cycles in\n    // the Tez DAG.\n    // 1.2. There cannot be any DummyStore operator in the works being merged.\n    //  This is due to an assumption in MergeJoinProc that needs to be further explored.\n    //  This is also due to some assumption in task generation\n    // If any of these conditions are not met, we cannot merge.\n    // TODO: Extend rule so it can be applied for these cases.\n    final Set<Operator<?>> workOps1 = findWorkOperators(optimizerCache, op1);\n    final Set<Operator<?>> workOps2 = findWorkOperators(optimizerCache, op2);\n    for (Operator<?> op : workOps1) {\n      if (op instanceof UnionOperator) {\n        // We cannot merge (1.1)\n        return false;\n      }\n      if (op instanceof DummyStoreOperator) {\n        // We cannot merge (1.2)\n        return false;\n      }\n    }\n    for (Operator<?> op : workOps2) {\n      if (op instanceof UnionOperator) {\n        // We cannot merge (1.1)\n        return false;\n      }\n      if (op instanceof DummyStoreOperator) {\n        // We cannot merge (1.2)\n        return false;\n      }\n    }\n    // 2) We check whether output works when we merge the operators will collide.\n    //\n    //   Work1   Work2    (merge TS in W1 & W2)        Work1\n    //       \\   /                  ->                  | |       X\n    //       Work3                                     Work3\n    //\n    // If we do, we cannot merge. The reason is that Tez currently does\n    // not support parallel edges, i.e., multiple edges from same work x\n    // into same work y.\n    final Set<Operator<?>> outputWorksOps1 = findChildWorkOperators(pctx, optimizerCache, op1);\n    final Set<Operator<?>> outputWorksOps2 = findChildWorkOperators(pctx, optimizerCache, op2);\n    if (!Collections.disjoint(outputWorksOps1, outputWorksOps2)) {\n      // We cannot merge\n      return false;\n    }\n    // 3) We check whether we will end up with same operators inputing on same work.\n    //\n    //       Work1        (merge TS in W2 & W3)        Work1\n    //       /   \\                  ->                  | |       X\n    //   Work2   Work3                                 Work2\n    //\n    // If we do, we cannot merge. The reason is the same as above, currently\n    // Tez does not support parallel edges.\n    //\n    // In the check, we exclude the inputs to the root operator that we are trying\n    // to merge (only useful for extended merging as TS do not have inputs).\n    final Set<Operator<?>> excludeOps1 = sr.retainableOps.get(0).getNumParent() > 0 ?\n        ImmutableSet.copyOf(sr.retainableOps.get(0).getParentOperators()) : ImmutableSet.of();\n    final Set<Operator<?>> inputWorksOps1 =\n        findParentWorkOperators(pctx, optimizerCache, op1, excludeOps1);\n    final Set<Operator<?>> excludeOps2 = sr.discardableOps.get(0).getNumParent() > 0 ?\n        Sets.union(ImmutableSet.copyOf(sr.discardableOps.get(0).getParentOperators()), sr.discardableInputOps) :\n            sr.discardableInputOps;\n    final Set<Operator<?>> inputWorksOps2 =\n        findParentWorkOperators(pctx, optimizerCache, op2, excludeOps2);\n    if (!Collections.disjoint(inputWorksOps1, inputWorksOps2)) {\n      // We cannot merge\n      return false;\n    }\n    // 4) We check whether one of the operators is part of a work that is an input for\n    // the work of the other operator.\n    //\n    //   Work1            (merge TS in W1 & W3)        Work1\n    //     |                        ->                   |        X\n    //   Work2                                         Work2\n    //     |                                             |\n    //   Work3                                         Work1\n    //\n    // If we do, we cannot merge, as we would end up with a cycle in the DAG.\n    final Set<Operator<?>> descendantWorksOps1 =\n            findDescendantWorkOperators(pctx, optimizerCache, op1, sr.discardableInputOps);\n    final Set<Operator<?>> descendantWorksOps2 =\n            findDescendantWorkOperators(pctx, optimizerCache, op2, sr.discardableInputOps);\n    if (!Collections.disjoint(descendantWorksOps1, workOps2)\n            || !Collections.disjoint(workOps1, descendantWorksOps2)) {\n      return false;\n    }\n    return true;\n  }"
        ]
    ],
    "b8afcc3f7c0b47b1a83ce258da1091f1dbcdb10f": [
        [
            "TestInputOutputFormat::testACIDReaderNoFooterSerializeWithDeltas()",
            "3621  \n3622  \n3623  \n3624  \n3625  \n3626  \n3627  \n3628  \n3629  \n3630  \n3631  \n3632  \n3633  \n3634  \n3635  \n3636  \n3637  \n3638  \n3639  \n3640  \n3641  \n3642  \n3643  \n3644  \n3645  \n3646  \n3647  \n3648  \n3649  \n3650  \n3651  \n3652  \n3653  \n3654  \n3655  \n3656  \n3657  \n3658  \n3659  \n3660  \n3661  \n3662  \n3663  \n3664  \n3665  \n3666  \n3667  \n3668  \n3669  \n3670  \n3671  \n3672  \n3673  \n3674  \n3675  \n3676  \n3677  \n3678  \n3679  \n3680  \n3681  \n3682  \n3683  \n3684  \n3685  \n3686  \n3687  \n3688  \n3689  \n3690 -\n3691  \n3692  \n3693  \n3694  ",
            "  @Test\n  public void testACIDReaderNoFooterSerializeWithDeltas() throws Exception {\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    FileSystem fs = FileSystem.get(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable7\");\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, MyRow.getColumnNamesProperty());\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, MyRow.getColumnTypesProperty());\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    AcidOutputFormat.Options options = new AcidOutputFormat.Options(conf).bucket(1).minimumWriteId(1)\n        .maximumWriteId(1).inspector(inspector).finalDestination(mockPath);\n    OrcOutputFormat of = new OrcOutputFormat();\n    RecordUpdater ru = of.getRecordUpdater(mockPath, options);\n    for (int i = 0; i < 10; ++i) {\n      ru.insert(options.getMinimumWriteId(), new MyRow(i, 2 * i));\n    }\n    ru.close(false);//this deletes the side file\n\n    //set up props for read\n    conf.setBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, true);\n    AcidUtils.setAcidOperationalProperties(conf, true, null);\n    conf.set(ValidTxnList.VALID_TXNS_KEY,\n        new ValidReadTxnList(new long[0], new BitSet(), 1000, Long.MAX_VALUE).writeToString());\n\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertFalse(\"No footer serialize test for ACID reader, hasFooter is not expected in\" +\n        \" orc splits.\", ((OrcSplit) split).hasFooter());\n      orcInputFormat.getRecordReader(split, conf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    assertEquals(12, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }",
            "3609  \n3610  \n3611  \n3612  \n3613  \n3614  \n3615  \n3616  \n3617  \n3618  \n3619  \n3620  \n3621  \n3622  \n3623  \n3624  \n3625  \n3626  \n3627  \n3628  \n3629  \n3630  \n3631  \n3632  \n3633  \n3634  \n3635  \n3636  \n3637  \n3638  \n3639  \n3640  \n3641  \n3642  \n3643  \n3644  \n3645  \n3646  \n3647  \n3648  \n3649  \n3650  \n3651  \n3652  \n3653  \n3654  \n3655  \n3656  \n3657  \n3658  \n3659  \n3660  \n3661  \n3662  \n3663  \n3664  \n3665  \n3666  \n3667  \n3668  \n3669  \n3670  \n3671  \n3672  \n3673  \n3674  \n3675  \n3676  \n3677  \n3678 +\n3679  \n3680  \n3681  \n3682  ",
            "  @Test\n  public void testACIDReaderNoFooterSerializeWithDeltas() throws Exception {\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    FileSystem fs = FileSystem.get(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable7\");\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, MyRow.getColumnNamesProperty());\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, MyRow.getColumnTypesProperty());\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    AcidOutputFormat.Options options = new AcidOutputFormat.Options(conf).bucket(1).minimumWriteId(1)\n        .maximumWriteId(1).inspector(inspector).finalDestination(mockPath);\n    OrcOutputFormat of = new OrcOutputFormat();\n    RecordUpdater ru = of.getRecordUpdater(mockPath, options);\n    for (int i = 0; i < 10; ++i) {\n      ru.insert(options.getMinimumWriteId(), new MyRow(i, 2 * i));\n    }\n    ru.close(false);//this deletes the side file\n\n    //set up props for read\n    conf.setBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, true);\n    AcidUtils.setAcidOperationalProperties(conf, true, null);\n    conf.set(ValidTxnList.VALID_TXNS_KEY,\n        new ValidReadTxnList(new long[0], new BitSet(), 1000, Long.MAX_VALUE).writeToString());\n\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertFalse(\"No footer serialize test for ACID reader, hasFooter is not expected in\" +\n        \" orc splits.\", ((OrcSplit) split).hasFooter());\n      orcInputFormat.getRecordReader(split, conf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    assertEquals(10, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }"
        ],
        [
            "TestInputOutputFormat::testVectorReaderNoFooterSerialize()",
            "3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331  \n3332  \n3333  \n3334  \n3335  \n3336  \n3337  \n3338  \n3339  \n3340  \n3341  \n3342  \n3343  \n3344  \n3345  \n3346  \n3347  \n3348  \n3349  \n3350  \n3351  \n3352  \n3353  \n3354  \n3355  \n3356  \n3357  \n3358  \n3359  \n3360  \n3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376  \n3377  \n3378 -\n3379 -\n3380 -\n3381 -\n3382 -\n3383  \n3384  \n3385  \n3386  ",
            "  @Test\n  public void testVectorReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable3\");\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    JobConf jobConf = createMockExecutionEnvironment(workDir, new Path(\"mock:///\"),\n        \"mocktable3\", inspector, true, 0);\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, jobConf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read footer - split 1 => mock:/mocktable3/0_0\n    // call-2: open to read data - split 1 => mock:/mocktable3/0_0\n    // call-3: open to read footer - split 2 => mock:/mocktable3/0_1\n    // call-4: open to read data - split 2 => mock:/mocktable3/0_1\n    assertEquals(4, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }",
            "3311  \n3312  \n3313  \n3314  \n3315  \n3316  \n3317  \n3318  \n3319  \n3320  \n3321  \n3322  \n3323  \n3324  \n3325  \n3326  \n3327  \n3328  \n3329  \n3330  \n3331  \n3332  \n3333  \n3334  \n3335  \n3336  \n3337  \n3338  \n3339  \n3340  \n3341  \n3342  \n3343  \n3344  \n3345  \n3346  \n3347  \n3348  \n3349  \n3350  \n3351  \n3352  \n3353  \n3354  \n3355  \n3356  \n3357  \n3358  \n3359  \n3360  \n3361  \n3362  \n3363  \n3364  \n3365  \n3366  \n3367  \n3368  \n3369  \n3370  \n3371  \n3372  \n3373  \n3374  \n3375  \n3376 +\n3377 +\n3378 +\n3379  \n3380  \n3381  \n3382  ",
            "  @Test\n  public void testVectorReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable3\");\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    JobConf jobConf = createMockExecutionEnvironment(workDir, new Path(\"mock:///\"),\n        \"mocktable3\", inspector, true, 0);\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, jobConf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read - split 1 => mock:/mocktable3/0_0\n    // call-2: open to read - split 2 => mock:/mocktable3/0_1\n    assertEquals(2, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }"
        ],
        [
            "TestInputOutputFormat::testACIDReaderNoFooterSerialize()",
            "3461  \n3462  \n3463  \n3464  \n3465  \n3466  \n3467  \n3468  \n3469  \n3470  \n3471  \n3472  \n3473  \n3474  \n3475  \n3476  \n3477  \n3478  \n3479  \n3480  \n3481  \n3482  \n3483  \n3484  \n3485  \n3486  \n3487  \n3488  \n3489  \n3490  \n3491  \n3492  \n3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503  \n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514  \n3515  \n3516  \n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  \n3524  \n3525  \n3526  \n3527  \n3528 -\n3529 -\n3530 -\n3531 -\n3532 -\n3533 -\n3534 -\n3535 -\n3536 -\n3537  \n3538  \n3539  \n3540  ",
            "  @Test\n  public void testACIDReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable5\");\n    conf.set(ConfVars.HIVE_TRANSACTIONAL_TABLE_SCAN.varname, \"true\");\n    conf.setBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, true);\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, MyRow.getColumnNamesProperty());\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, MyRow.getColumnTypesProperty());\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      assertTrue(split.toString().contains(\"isOriginal=true\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for non-vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, conf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read footer - split 1 => mock:/mocktable5/0_0\n    // call-2: open to read data - split 1 => mock:/mocktable5/0_0\n    // call-3: getAcidState - split 1 => mock:/mocktable5 (to compute offset for original read)\n    // call-4: open to read footer - split 2 => mock:/mocktable5/0_1\n    // call-5: open to read data - split 2 => mock:/mocktable5/0_1\n    // call-6: getAcidState - split 2 => mock:/mocktable5 (to compute offset for original read)\n    // call-7: open to read footer - split 2 => mock:/mocktable5/0_0 (to get row count)\n    // call-8: file status - split 2 => mock:/mocktable5/0_0\n    assertEquals(12, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }",
            "3457  \n3458  \n3459  \n3460  \n3461  \n3462  \n3463  \n3464  \n3465  \n3466  \n3467  \n3468  \n3469  \n3470  \n3471  \n3472  \n3473  \n3474  \n3475  \n3476  \n3477  \n3478  \n3479  \n3480  \n3481  \n3482  \n3483  \n3484  \n3485  \n3486  \n3487  \n3488  \n3489  \n3490  \n3491  \n3492  \n3493  \n3494  \n3495  \n3496  \n3497  \n3498  \n3499  \n3500  \n3501  \n3502  \n3503  \n3504  \n3505  \n3506  \n3507  \n3508  \n3509  \n3510  \n3511  \n3512  \n3513  \n3514  \n3515  \n3516  \n3517  \n3518  \n3519  \n3520  \n3521  \n3522  \n3523  \n3524 +\n3525  \n3526  \n3527  \n3528  ",
            "  @Test\n  public void testACIDReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable5\");\n    conf.set(ConfVars.HIVE_TRANSACTIONAL_TABLE_SCAN.varname, \"true\");\n    conf.setBoolean(hive_metastoreConstants.TABLE_IS_TRANSACTIONAL, true);\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS, MyRow.getColumnNamesProperty());\n    conf.set(IOConstants.SCHEMA_EVOLUTION_COLUMNS_TYPES, MyRow.getColumnTypesProperty());\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      assertTrue(split.toString().contains(\"isOriginal=true\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for non-vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, conf, Reporter.NULL);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    assertEquals(10, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }"
        ],
        [
            "TestInputOutputFormat::testNonVectorReaderNoFooterSerialize()",
            "3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212  \n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233 -\n3234 -\n3235 -\n3236 -\n3237 -\n3238  \n3239  \n3240  \n3241  ",
            "  @Test\n  public void testNonVectorReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable1\");\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for non-vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, conf, null);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read footer - split 1 => mock:/mocktable1/0_0\n    // call-2: open to read data - split 1 => mock:/mocktable1/0_0\n    // call-3: open to read footer - split 2 => mock:/mocktable1/0_1\n    // call-4: open to read data - split 2 => mock:/mocktable1/0_1\n    assertEquals(4, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }",
            "3171  \n3172  \n3173  \n3174  \n3175  \n3176  \n3177  \n3178  \n3179  \n3180  \n3181  \n3182  \n3183  \n3184  \n3185  \n3186  \n3187  \n3188  \n3189  \n3190  \n3191  \n3192  \n3193  \n3194  \n3195  \n3196  \n3197  \n3198  \n3199  \n3200  \n3201  \n3202  \n3203  \n3204  \n3205  \n3206  \n3207  \n3208  \n3209  \n3210  \n3211  \n3212  \n3213  \n3214  \n3215  \n3216  \n3217  \n3218  \n3219  \n3220  \n3221  \n3222  \n3223  \n3224  \n3225  \n3226  \n3227  \n3228  \n3229  \n3230  \n3231  \n3232  \n3233 +\n3234 +\n3235 +\n3236  \n3237  \n3238  \n3239  ",
            "  @Test\n  public void testNonVectorReaderNoFooterSerialize() throws Exception {\n    MockFileSystem fs = new MockFileSystem(conf);\n    MockPath mockPath = new MockPath(fs, \"mock:///mocktable1\");\n    conf.set(\"hive.orc.splits.include.file.footer\", \"false\");\n    conf.set(\"mapred.input.dir\", mockPath.toString());\n    conf.set(\"fs.defaultFS\", \"mock:///\");\n    conf.set(\"fs.mock.impl\", MockFileSystem.class.getName());\n    StructObjectInspector inspector;\n    synchronized (TestOrcFile.class) {\n      inspector = (StructObjectInspector)\n          ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,\n              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n    }\n    Writer writer =\n        OrcFile.createWriter(new Path(mockPath + \"/0_0\"),\n            OrcFile.writerOptions(conf).blockPadding(false)\n                .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    writer = OrcFile.createWriter(new Path(mockPath + \"/0_1\"),\n        OrcFile.writerOptions(conf).blockPadding(false)\n            .bufferSize(1024).inspector(inspector));\n    for (int i = 0; i < 10; ++i) {\n      writer.addRow(new MyRow(i, 2 * i));\n    }\n    writer.close();\n\n    OrcInputFormat orcInputFormat = new OrcInputFormat();\n    InputSplit[] splits = orcInputFormat.getSplits(conf, 2);\n    assertEquals(2, splits.length);\n    int readOpsBefore = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsBefore = statistics.getReadOps();\n      }\n    }\n    assertTrue(\"MockFS has stats. Read ops not expected to be -1\", readOpsBefore != -1);\n\n    for (InputSplit split : splits) {\n      assertTrue(\"OrcSplit is expected\", split instanceof OrcSplit);\n      // ETL strategies will have start=3 (start of first stripe)\n      assertTrue(split.toString().contains(\"start=3\"));\n      assertTrue(split.toString().contains(\"hasFooter=false\"));\n      assertTrue(split.toString().contains(\"hasBase=true\"));\n      assertTrue(split.toString().contains(\"deltas=0\"));\n      if (split instanceof OrcSplit) {\n        assertFalse(\"No footer serialize test for non-vector reader, hasFooter is not expected in\" +\n            \" orc splits.\", ((OrcSplit) split).hasFooter());\n      }\n      orcInputFormat.getRecordReader(split, conf, null);\n    }\n\n    int readOpsDelta = -1;\n    for (FileSystem.Statistics statistics : FileSystem.getAllStatistics()) {\n      if (statistics.getScheme().equalsIgnoreCase(\"mock\")) {\n        readOpsDelta = statistics.getReadOps() - readOpsBefore;\n      }\n    }\n    // call-1: open to read - split 1 => mock:/mocktable1/0_0\n    // call-2: open to read - split 2 => mock:/mocktable1/0_1\n    assertEquals(2, readOpsDelta);\n\n    // revert back to local fs\n    conf.set(\"fs.defaultFS\", \"file:///\");\n  }"
        ]
    ]
}