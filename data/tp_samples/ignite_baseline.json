{
    "f5375deead7fd40787cfb2e7130ee7c2d58820af": [
        [
            "PlatformTransactions::processInLongOutLong(int,long)",
            " 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  ",
            "    /** {@inheritDoc} */\n    @Override public long processInLongOutLong(int type, long val) throws IgniteCheckedException {\n        switch (type) {\n            case OP_COMMIT:\n                tx(val).commit();\n\n                return txClose(val);\n\n            case OP_ROLLBACK:\n                tx(val).rollback();\n\n                return txClose(val);\n\n            case OP_CLOSE:\n                return txClose(val);\n\n            case OP_SET_ROLLBACK_ONLY:\n                return tx(val).setRollbackOnly() ? TRUE : FALSE;\n\n            case OP_STATE:\n                return tx(val).state().ordinal();\n\n            case OP_RESET_METRICS:\n                txs.resetMetrics();\n\n                return TRUE;\n        }\n\n        return super.processInLongOutLong(type, val);\n    }",
            " 159  \n 160  \n 161  \n 162 +\n 163 +\n 164 +\n 165 +\n 166 +\n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "    /** {@inheritDoc} */\n    @Override public long processInLongOutLong(int type, long val) throws IgniteCheckedException {\n        switch (type) {\n            case OP_PREPARE:\n                ((TransactionProxyImpl)tx(val)).tx().prepare();\n\n                return TRUE;\n\n            case OP_COMMIT:\n                tx(val).commit();\n\n                return txClose(val);\n\n            case OP_ROLLBACK:\n                tx(val).rollback();\n\n                return txClose(val);\n\n            case OP_CLOSE:\n                return txClose(val);\n\n            case OP_SET_ROLLBACK_ONLY:\n                return tx(val).setRollbackOnly() ? TRUE : FALSE;\n\n            case OP_STATE:\n                return tx(val).state().ordinal();\n\n            case OP_RESET_METRICS:\n                txs.resetMetrics();\n\n                return TRUE;\n        }\n\n        return super.processInLongOutLong(type, val);\n    }"
        ]
    ],
    "1410900f2c0e2f8819a1fb945109ca0d88f3a2f9": [
        [
            "PlatformProcessorImpl::context()",
            " 215  \n 216  \n 217 -\n 218 -\n 219 -\n 220 -\n 221 -\n 222 -\n 223 -\n 224 -\n 225 -\n 226  \n 227  ",
            "    /** {@inheritDoc} */\n    @Override public PlatformContext context() {\n        // This method is a single point of entry for all remote closures\n        // CPP platform does not currently support remote code execution\n        // Therefore, all remote execution attempts come from .NET\n        // Throw an error if current platform is not .NET\n        if (!PlatformUtils.PLATFORM_DOTNET.equals(interopCfg.platform())) {\n            throw new IgniteException(\".NET platform is not available [nodeId=\" + ctx.grid().localNode().id() + \"] \" +\n                \"(Use Apache.Ignite.Core.Ignition.Start() or Apache.Ignite.exe to start Ignite.NET nodes).\");\n        }\n\n        return platformCtx;\n    }",
            " 215  \n 216  \n 217  \n 218  ",
            "    /** {@inheritDoc} */\n    @Override public PlatformContext context() {\n        return platformCtx;\n    }"
        ],
        [
            "PlatformCacheEntryProcessorImpl::writeEntryAndProcessor(MutableEntry,BinaryRawWriter)",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 -\n 147 -\n 148 -\n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  ",
            "    /**\n     * Writes mutable entry and entry processor to the stream.\n     *\n     * @param entry Entry to process.\n     * @param writer Writer.\n     */\n    private void writeEntryAndProcessor(MutableEntry entry, BinaryRawWriter writer) {\n        writer.writeObject(entry.getKey());\n        writer.writeObject(entry.getValue());\n\n        if (ptr != 0) {\n            // Execute locally - we have a pointer to native processor.\n            writer.writeBoolean(true);\n            writer.writeLong(ptr);\n        }\n        else {\n            // We are on a remote node. Send processor holder back to native.\n            writer.writeBoolean(false);\n            writer.writeObject(proc);\n        }\n    }",
            " 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 +\n 157 +\n 158 +\n 159  ",
            "    /**\n     * Writes mutable entry and entry processor to the stream.\n     *\n     * @param entry Entry to process.\n     * @param writer Writer.\n     */\n    private void writeEntryAndProcessor(MutableEntry entry, BinaryRawWriter writer) {\n        if (ptr != 0) {\n            // Execute locally - we have a pointer to native processor.\n            writer.writeBoolean(true);\n            writer.writeLong(ptr);\n        }\n        else {\n            // We are on a remote node. Send processor holder back to native.\n            writer.writeBoolean(false);\n            writer.writeObject(proc);\n        }\n\n        writer.writeObject(entry.getKey());\n        writer.writeObject(entry.getValue());\n    }"
        ]
    ],
    "612e92ac99f0b8305ae9efe92dc32cb46eeb9358": [
        [
            "OdbcMessageParser::encode(OdbcResponse)",
            " 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 -\n 258  \n 259 -\n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  ",
            "    /**\n     * Encode OdbcResponse to byte array.\n     *\n     * @param msg Message.\n     * @return Byte array.\n     */\n    public byte[] encode(OdbcResponse msg) {\n        assert msg != null;\n\n        // Creating new binary writer\n        BinaryWriterExImpl writer = marsh.writer(new BinaryHeapOutputStream(INIT_CAP));\n\n        // Writing status.\n        writer.writeByte((byte) msg.status());\n\n        if (msg.status() != OdbcResponse.STATUS_SUCCESS) {\n            writer.writeString(msg.error());\n\n            return writer.array();\n        }\n\n        Object res0 = msg.response();\n\n        if (res0 == null)\n            return writer.array();\n        if (res0 instanceof OdbcHandshakeResult) {\n            OdbcHandshakeResult res = (OdbcHandshakeResult) res0;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Handshake result: \" + (res.accepted() ? \"accepted\" : \"rejected\"));\n\n            verConfirmed = res.accepted();\n\n            if (res.accepted()) {\n                verConfirmed = true;\n\n                writer.writeBoolean(true);\n            }\n            else {\n                writer.writeBoolean(false);\n                writer.writeString(res.protocolVersionSince());\n                writer.writeString(res.currentVersion());\n            }\n        }\n        else if (res0 instanceof OdbcQueryExecuteResult) {\n            OdbcQueryExecuteResult res = (OdbcQueryExecuteResult) res0;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Resulting query ID: \" + res.getQueryId());\n\n            writer.writeLong(res.getQueryId());\n\n            Collection<OdbcColumnMeta> metas = res.getColumnsMetadata();\n\n            assert metas != null;\n\n            writer.writeInt(metas.size());\n\n            for (OdbcColumnMeta meta : metas)\n                meta.write(writer);\n        }\n        else if (res0 instanceof OdbcQueryFetchResult) {\n            OdbcQueryFetchResult res = (OdbcQueryFetchResult) res0;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Resulting query ID: \" + res.queryId());\n\n            writer.writeLong(res.queryId());\n\n            Collection<?> items0 = res.items();\n\n            assert items0 != null;\n\n            writer.writeBoolean(res.last());\n\n            writer.writeInt(items0.size());\n\n            for (Object row0 : items0) {\n                if (row0 != null) {\n                    Collection<?> row = (Collection<?>)row0;\n\n                    writer.writeInt(row.size());\n\n                    for (Object obj : row) {\n                        if (obj instanceof java.sql.Timestamp)\n                            writer.writeTimestamp((java.sql.Timestamp)obj);\n                        else if (obj instanceof java.util.Date)\n                            writer.writeDate((java.util.Date)obj);\n                        else\n                            writer.writeObjectDetached(obj);\n                    }\n                }\n            }\n        }\n        else if (res0 instanceof OdbcQueryCloseResult) {\n            OdbcQueryCloseResult res = (OdbcQueryCloseResult) res0;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Resulting query ID: \" + res.getQueryId());\n\n            writer.writeLong(res.getQueryId());\n        }\n        else if (res0 instanceof OdbcQueryGetColumnsMetaResult) {\n            OdbcQueryGetColumnsMetaResult res = (OdbcQueryGetColumnsMetaResult) res0;\n\n            Collection<OdbcColumnMeta> columnsMeta = res.meta();\n\n            assert columnsMeta != null;\n\n            writer.writeInt(columnsMeta.size());\n\n            for (OdbcColumnMeta columnMeta : columnsMeta)\n                columnMeta.write(writer);\n        }\n        else if (res0 instanceof OdbcQueryGetTablesMetaResult) {\n            OdbcQueryGetTablesMetaResult res = (OdbcQueryGetTablesMetaResult) res0;\n\n            Collection<OdbcTableMeta> tablesMeta = res.meta();\n\n            assert tablesMeta != null;\n\n            writer.writeInt(tablesMeta.size());\n\n            for (OdbcTableMeta tableMeta : tablesMeta)\n                tableMeta.writeBinary(writer);\n        }\n        else if (res0 instanceof OdbcQueryGetParamsMetaResult) {\n            OdbcQueryGetParamsMetaResult res = (OdbcQueryGetParamsMetaResult) res0;\n\n            byte[] typeIds = res.typeIds();\n\n            writer.writeObjectDetached(typeIds);\n        }\n        else\n            assert false : \"Should not reach here.\";\n\n        return writer.array();\n    }",
            " 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258 +\n 259 +\n 260 +\n 261 +\n 262 +\n 263 +\n 264 +\n 265 +\n 266 +\n 267  \n 268 +\n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  ",
            "    /**\n     * Encode OdbcResponse to byte array.\n     *\n     * @param msg Message.\n     * @return Byte array.\n     */\n    public byte[] encode(OdbcResponse msg) {\n        assert msg != null;\n\n        // Creating new binary writer\n        BinaryWriterExImpl writer = marsh.writer(new BinaryHeapOutputStream(INIT_CAP));\n\n        // Writing status.\n        writer.writeByte((byte) msg.status());\n\n        if (msg.status() != OdbcResponse.STATUS_SUCCESS) {\n            writer.writeString(msg.error());\n\n            return writer.array();\n        }\n\n        Object res0 = msg.response();\n\n        if (res0 == null)\n            return writer.array();\n        if (res0 instanceof OdbcHandshakeResult) {\n            OdbcHandshakeResult res = (OdbcHandshakeResult) res0;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Handshake result: \" + (res.accepted() ? \"accepted\" : \"rejected\"));\n\n            verConfirmed = res.accepted();\n\n            if (res.accepted()) {\n                verConfirmed = true;\n\n                writer.writeBoolean(true);\n            }\n            else {\n                writer.writeBoolean(false);\n                writer.writeString(res.protocolVersionSince());\n                writer.writeString(res.currentVersion());\n            }\n        }\n        else if (res0 instanceof OdbcQueryExecuteResult) {\n            OdbcQueryExecuteResult res = (OdbcQueryExecuteResult) res0;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Resulting query ID: \" + res.getQueryId());\n\n            writer.writeLong(res.getQueryId());\n\n            Collection<OdbcColumnMeta> metas = res.getColumnsMetadata();\n\n            assert metas != null;\n\n            writer.writeInt(metas.size());\n\n            for (OdbcColumnMeta meta : metas)\n                meta.write(writer);\n        }\n        else if (res0 instanceof OdbcQueryFetchResult) {\n            OdbcQueryFetchResult res = (OdbcQueryFetchResult) res0;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Resulting query ID: \" + res.queryId());\n\n            writer.writeLong(res.queryId());\n\n            Collection<?> items0 = res.items();\n\n            assert items0 != null;\n\n            writer.writeBoolean(res.last());\n\n            writer.writeInt(items0.size());\n\n            for (Object row0 : items0) {\n                if (row0 != null) {\n                    Collection<?> row = (Collection<?>)row0;\n\n                    writer.writeInt(row.size());\n\n                    for (Object obj : row) {\n                        if (obj == null) {\n                            writer.writeObjectDetached(null);\n                            continue;\n                        }\n\n                        Class<?> cls = obj.getClass();\n\n                        if (cls == java.sql.Time.class)\n                            writer.writeTime((java.sql.Time)obj);\n                        else if (cls == java.sql.Timestamp.class)\n                            writer.writeTimestamp((java.sql.Timestamp)obj);\n                        else if (cls == java.sql.Date.class)\n                            writer.writeDate((java.util.Date)obj);\n                        else\n                            writer.writeObjectDetached(obj);\n                    }\n                }\n            }\n        }\n        else if (res0 instanceof OdbcQueryCloseResult) {\n            OdbcQueryCloseResult res = (OdbcQueryCloseResult) res0;\n\n            if (log.isDebugEnabled())\n                log.debug(\"Resulting query ID: \" + res.getQueryId());\n\n            writer.writeLong(res.getQueryId());\n        }\n        else if (res0 instanceof OdbcQueryGetColumnsMetaResult) {\n            OdbcQueryGetColumnsMetaResult res = (OdbcQueryGetColumnsMetaResult) res0;\n\n            Collection<OdbcColumnMeta> columnsMeta = res.meta();\n\n            assert columnsMeta != null;\n\n            writer.writeInt(columnsMeta.size());\n\n            for (OdbcColumnMeta columnMeta : columnsMeta)\n                columnMeta.write(writer);\n        }\n        else if (res0 instanceof OdbcQueryGetTablesMetaResult) {\n            OdbcQueryGetTablesMetaResult res = (OdbcQueryGetTablesMetaResult) res0;\n\n            Collection<OdbcTableMeta> tablesMeta = res.meta();\n\n            assert tablesMeta != null;\n\n            writer.writeInt(tablesMeta.size());\n\n            for (OdbcTableMeta tableMeta : tablesMeta)\n                tableMeta.writeBinary(writer);\n        }\n        else if (res0 instanceof OdbcQueryGetParamsMetaResult) {\n            OdbcQueryGetParamsMetaResult res = (OdbcQueryGetParamsMetaResult) res0;\n\n            byte[] typeIds = res.typeIds();\n\n            writer.writeObjectDetached(typeIds);\n        }\n        else\n            assert false : \"Should not reach here.\";\n\n        return writer.array();\n    }"
        ]
    ],
    "eab8334bb49ceda249e742246d26f72539f9fa4c": [
        [
            "PlatformDotNetCacheStore::loadAll(Iterable)",
            " 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204 -\n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  ",
            "    /** {@inheritDoc} */\n    @Override public Map<K, V> loadAll(final Iterable<? extends K> keys) {\n        try {\n            final Map<K, V> loaded = new HashMap<>();\n\n            final Collection keys0 = (Collection)keys;\n\n            doInvoke(new IgniteInClosureX<BinaryRawWriterEx>() {\n                @Override public void applyx(BinaryRawWriterEx writer) throws IgniteCheckedException {\n                    writer.writeByte(OP_LOAD_ALL);\n                    writer.writeLong(session());\n                    writer.writeString(ses.cacheName());\n                    writer.writeCollection(keys0);\n                }\n            }, new IgniteInClosureX<BinaryRawReaderEx>() {\n                @Override public void applyx(BinaryRawReaderEx reader) {\n                    int cnt = reader.readInt();\n\n                    for (int i = 0; i < cnt; i++)\n                        loaded.put((K) reader.readObjectDetached(), (V) reader.readObjectDetached());\n                }\n            });\n\n            return loaded;\n        }\n        catch (IgniteCheckedException e) {\n            throw new CacheLoaderException(e);\n        }\n    }",
            " 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  ",
            "    /** {@inheritDoc} */\n    @Override public Map<K, V> loadAll(final Iterable<? extends K> keys) {\n        try {\n            final Map<K, V> loaded = new HashMap<>();\n\n            final Collection keys0 = (Collection)keys;\n\n            doInvoke(new IgniteInClosureX<BinaryRawWriterEx>() {\n                @Override public void applyx(BinaryRawWriterEx writer) throws IgniteCheckedException {\n                    writer.writeByte(OP_LOAD_ALL);\n                    writer.writeLong(session());\n                    writer.writeString(ses.cacheName());\n\n                    writer.writeInt(keys0.size());\n\n                    for (Object o : keys0)\n                        writer.writeObject(o);\n                }\n            }, new IgniteInClosureX<BinaryRawReaderEx>() {\n                @Override public void applyx(BinaryRawReaderEx reader) {\n                    int cnt = reader.readInt();\n\n                    for (int i = 0; i < cnt; i++)\n                        loaded.put((K) reader.readObjectDetached(), (V) reader.readObjectDetached());\n                }\n            });\n\n            return loaded;\n        }\n        catch (IgniteCheckedException e) {\n            throw new CacheLoaderException(e);\n        }\n    }"
        ],
        [
            "PlatformDotNetCacheStore::deleteAll(Collection)",
            " 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314 -\n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  ",
            "    /** {@inheritDoc} */\n    @Override public void deleteAll(final Collection<?> keys) {\n        try {\n            doInvoke(new IgniteInClosureX<BinaryRawWriterEx>() {\n                @Override public void applyx(BinaryRawWriterEx writer) throws IgniteCheckedException {\n                    writer.writeByte(OP_RMV_ALL);\n                    writer.writeLong(session());\n                    writer.writeString(ses.cacheName());\n                    writer.writeCollection(keys);\n                }\n            }, null);\n        }\n        catch (IgniteCheckedException e) {\n            throw new CacheWriterException(U.convertExceptionNoWrap(e));\n        }\n    }",
            " 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318 +\n 319 +\n 320 +\n 321 +\n 322 +\n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  ",
            "    /** {@inheritDoc} */\n    @Override public void deleteAll(final Collection<?> keys) {\n        try {\n            doInvoke(new IgniteInClosureX<BinaryRawWriterEx>() {\n                @Override public void applyx(BinaryRawWriterEx writer) throws IgniteCheckedException {\n                    writer.writeByte(OP_RMV_ALL);\n                    writer.writeLong(session());\n                    writer.writeString(ses.cacheName());\n\n                    writer.writeInt(keys.size());\n\n                    for (Object o : keys)\n                        writer.writeObject(o);\n                }\n            }, null);\n        }\n        catch (IgniteCheckedException e) {\n            throw new CacheWriterException(U.convertExceptionNoWrap(e));\n        }\n    }"
        ]
    ],
    "d16e22c3af0411ff8bc88d0e1cd029d0da79e0b9": [
        [
            "DataStreamerImpl::addData(K,V)",
            " 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662 -\n 663 -\n 664 -\n 665  \n 666 -\n 667  \n 668  \n 669  \n 670  \n 671  ",
            "    /** {@inheritDoc} */\n    @Override public IgniteFuture<?> addData(K key, V val) {\n        A.notNull(key, \"key\");\n\n        if (val == null)\n            checkSecurityPermission(SecurityPermission.CACHE_REMOVE);\n        else\n            checkSecurityPermission(SecurityPermission.CACHE_PUT);\n\n        try {\n            KeyCacheObject key0 = cacheObjProc.toCacheKeyObject(cacheObjCtx, null, key, true);\n            CacheObject val0 = cacheObjProc.toCacheObject(cacheObjCtx, val, true);\n\n            return addDataInternal(Collections.singleton(new DataStreamerEntry(key0, val0)));\n        }\n        catch (Exception e) {\n            return new IgniteFinishedCacheFutureImpl<>(e);\n        }\n    }",
            " 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662 +\n 663 +\n 664  \n 665 +\n 666 +\n 667 +\n 668  \n 669  \n 670  \n 671  \n 672 +\n 673 +\n 674  ",
            "    /** {@inheritDoc} */\n    @Override public IgniteFuture<?> addData(K key, V val) {\n        A.notNull(key, \"key\");\n\n        if (val == null)\n            checkSecurityPermission(SecurityPermission.CACHE_REMOVE);\n        else\n            checkSecurityPermission(SecurityPermission.CACHE_PUT);\n\n        KeyCacheObject key0;\n        CacheObject val0;\n\n        try {\n            key0 = cacheObjProc.toCacheKeyObject(cacheObjCtx, null, key, true);\n            val0 = cacheObjProc.toCacheObject(cacheObjCtx, val, true);\n        }\n        catch (Exception e) {\n            return new IgniteFinishedCacheFutureImpl<>(e);\n        }\n\n        return addDataInternal(Collections.singleton(new DataStreamerEntry(key0, val0)));\n    }"
        ]
    ],
    "459dc9e455ca23968210a50ebe87d312846cc48b": [
        [
            "RestExecutor::sendRequest(boolean,String,Map,String,Map,String)",
            "  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169 -\n 170  \n 171  \n 172  \n 173 -\n 174 -\n 175  \n 176  \n 177  \n 178  ",
            "    /** */\n    private RestResult sendRequest(boolean demo, String path, Map<String, Object> params,\n        String mtd, Map<String, Object> headers, String body) throws IOException {\n        if (demo && AgentClusterDemo.getDemoUrl() == null) {\n            try {\n                AgentClusterDemo.tryStart().await();\n            }\n            catch (InterruptedException ignore) {\n                throw new IllegalStateException(\"Failed to send request because of embedded node for demo mode is not started yet.\");\n            }\n        }\n\n        String url = demo ? AgentClusterDemo.getDemoUrl() : nodeUrl;\n\n        HttpUrl httpUrl = HttpUrl.parse(url);\n\n        if (httpUrl == null)\n            throw new IllegalStateException(\"Failed to send request because of node URL is invalid: \" + url);\n\n        HttpUrl.Builder urlBuilder = httpUrl.newBuilder();\n\n        if (path != null)\n            urlBuilder.addPathSegment(path);\n\n        final Request.Builder reqBuilder = new Request.Builder();\n\n        if (headers != null) {\n            for (Map.Entry<String, Object> entry : headers.entrySet())\n                if (entry.getValue() != null)\n                    reqBuilder.addHeader(entry.getKey(), entry.getValue().toString());\n        }\n\n        if (\"GET\".equalsIgnoreCase(mtd)) {\n            if (params != null) {\n                for (Map.Entry<String, Object> entry : params.entrySet()) {\n                    if (entry.getValue() != null)\n                        urlBuilder.addQueryParameter(entry.getKey(), entry.getValue().toString());\n                }\n            }\n        }\n        else if (\"POST\".equalsIgnoreCase(mtd)) {\n            if (body != null) {\n                MediaType contentType = MediaType.parse(\"text/plain\");\n\n                reqBuilder.post(RequestBody.create(contentType, body));\n            }\n            else {\n                FormBody.Builder formBody = new FormBody.Builder();\n\n                if (params != null) {\n                    for (Map.Entry<String, Object> entry : params.entrySet()) {\n                        if (entry.getValue() != null)\n                            formBody.add(entry.getKey(), entry.getValue().toString());\n                    }\n                }\n\n                reqBuilder.post(formBody.build());\n            }\n        }\n        else\n            throw new IllegalArgumentException(\"Unknown HTTP-method: \" + mtd);\n\n        reqBuilder.url(urlBuilder.build());\n\n        try (Response resp = httpClient.newCall(reqBuilder.build()).execute()) {\n            String content = resp.body().string();\n\n            if (resp.isSuccessful()) {\n                JsonNode node = mapper.readTree(content);\n\n                int status = node.get(\"successStatus\").asInt();\n\n                switch (status) {\n                    case STATUS_SUCCESS:\n                        return RestResult.success(node.get(\"response\").toString());\n\n                    default:\n                        return RestResult.fail(status, node.get(\"error\").asText());\n                }\n            }\n\n            if (resp.code() == 401)\n                return RestResult.fail(STATUS_AUTH_FAILED, \"Failed to authenticate in grid. Please check agent\\'s login and password or node port.\");\n\n            return RestResult.fail(STATUS_FAILED, \"Failed connect to node and execute REST command.\");\n        }\n        catch (ConnectException ignore) {\n            log.warn(\"Please ensure that nodes have ignite-rest-http module in classpath (was copied from libs/optional to libs folder).\");\n\n            throw new ConnectException(\"Failed connect to node and execute REST command [url=\" + urlBuilder + \"]\");\n        }\n    }",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173 +\n 174 +\n 175  \n 176  \n 177  \n 178 +\n 179 +\n 180 +\n 181 +\n 182  \n 183  \n 184  \n 185  ",
            "    /** */\n    private RestResult sendRequest(boolean demo, String path, Map<String, Object> params,\n        String mtd, Map<String, Object> headers, String body) throws IOException {\n        if (demo && AgentClusterDemo.getDemoUrl() == null) {\n            try {\n                AgentClusterDemo.tryStart().await();\n            }\n            catch (InterruptedException ignore) {\n                throw new IllegalStateException(\"Failed to send request because of embedded node for demo mode is not started yet.\");\n            }\n        }\n\n        String url = demo ? AgentClusterDemo.getDemoUrl() : nodeUrl;\n\n        HttpUrl httpUrl = HttpUrl.parse(url);\n\n        if (httpUrl == null)\n            throw new IllegalStateException(\"Failed to send request because of node URL is invalid: \" + url);\n\n        HttpUrl.Builder urlBuilder = httpUrl.newBuilder();\n\n        if (path != null)\n            urlBuilder.addPathSegment(path);\n\n        final Request.Builder reqBuilder = new Request.Builder();\n\n        if (headers != null) {\n            for (Map.Entry<String, Object> entry : headers.entrySet())\n                if (entry.getValue() != null)\n                    reqBuilder.addHeader(entry.getKey(), entry.getValue().toString());\n        }\n\n        if (\"GET\".equalsIgnoreCase(mtd)) {\n            if (params != null) {\n                for (Map.Entry<String, Object> entry : params.entrySet()) {\n                    if (entry.getValue() != null)\n                        urlBuilder.addQueryParameter(entry.getKey(), entry.getValue().toString());\n                }\n            }\n        }\n        else if (\"POST\".equalsIgnoreCase(mtd)) {\n            if (body != null) {\n                MediaType contentType = MediaType.parse(\"text/plain\");\n\n                reqBuilder.post(RequestBody.create(contentType, body));\n            }\n            else {\n                FormBody.Builder formBody = new FormBody.Builder();\n\n                if (params != null) {\n                    for (Map.Entry<String, Object> entry : params.entrySet()) {\n                        if (entry.getValue() != null)\n                            formBody.add(entry.getKey(), entry.getValue().toString());\n                    }\n                }\n\n                reqBuilder.post(formBody.build());\n            }\n        }\n        else\n            throw new IllegalArgumentException(\"Unknown HTTP-method: \" + mtd);\n\n        reqBuilder.url(urlBuilder.build());\n\n        try (Response resp = httpClient.newCall(reqBuilder.build()).execute()) {\n            String content = resp.body().string();\n\n            if (resp.isSuccessful()) {\n                JsonNode node = mapper.readTree(content);\n\n                int status = node.get(\"successStatus\").asInt();\n\n                switch (status) {\n                    case STATUS_SUCCESS:\n                        return RestResult.success(node.get(\"response\").toString());\n\n                    default:\n                        return RestResult.fail(status, node.get(\"error\").asText());\n                }\n            }\n\n            if (resp.code() == 401)\n                return RestResult.fail(STATUS_AUTH_FAILED, \"Failed to authenticate in grid. \" +\n                    \"Please check agent\\'s login and password or node port.\");\n\n            return RestResult.fail(STATUS_FAILED, \"Failed connect to node and execute REST command.\");\n        }\n        catch (ConnectException ignored) {\n            LT.warn(log, \"Failed connect to node and execute REST command. \" +\n                \"Please ensure that nodes have [ignite-rest-http] module in classpath \" +\n                \"(was copied from libs/optional to libs folder).\");\n\n            throw new ConnectException(\"Failed connect to node and execute REST command [url=\" + urlBuilder + \"]\");\n        }\n    }"
        ],
        [
            "RestExecutor::execute(boolean,String,Map,String,Map,String)",
            " 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190 -\n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 -\n 198  \n 199  \n 200  \n 201  \n 202  ",
            "    /**\n     * @param demo Is demo node request.\n     * @param path Path segment.\n     * @param params Params.\n     * @param mtd Method.\n     * @param headers Headers.\n     * @param body Body.\n     */\n    public RestResult execute(boolean demo, String path, Map<String, Object> params,\n        String mtd, Map<String, Object> headers, String body) {\n        log.debug(\"Start execute REST command [method=\" + mtd + \", uri=/\" + (path == null ? \"\" : path) +\n                \", parameters=\" + params + \"]\");\n\n        try {\n            return sendRequest(demo, path, params, mtd, headers, body);\n        }\n        catch (Exception e) {\n            log.info(\"Failed to execute REST command [method=\" + mtd + \", uri=/\" + (path == null ? \"\" : path) +\n                \", parameters=\" + params + \"]\", e);\n\n            return RestResult.fail(404, e.getMessage());\n        }\n    }",
            " 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 +\n 198 +\n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209  \n 210  ",
            "    /**\n     * @param demo Is demo node request.\n     * @param path Path segment.\n     * @param params Params.\n     * @param mtd Method.\n     * @param headers Headers.\n     * @param body Body.\n     */\n    public RestResult execute(boolean demo, String path, Map<String, Object> params,\n        String mtd, Map<String, Object> headers, String body) {\n        if (log.isDebugEnabled())\n            log.debug(\"Start execute REST command [method=\" + mtd + \", uri=/\" + (path == null ? \"\" : path) +\n                \", parameters=\" + params + \"]\");\n\n        try {\n            return sendRequest(demo, path, params, mtd, headers, body);\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to execute REST command [method=\" + mtd + \", uri=/\" + (path == null ? \"\" : path) +\n                \", parameters=\" + params + \"]\", e);\n\n            return RestResult.fail(404, e.getMessage());\n        }\n    }"
        ]
    ],
    "7adb11109bab5d83ed4f376b0cad42b026dd0a71": [
        [
            "GridDhtPartitionTopologyImpl::beforeExchange(GridDhtPartitionsExchangeFuture,boolean)",
            " 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447 -\n 448 -\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455 -\n 456 -\n 457  \n 458  \n 459  \n 460  \n 461  \n 462 -\n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473 -\n 474 -\n 475 -\n 476 -\n 477 -\n 478  \n 479  \n 480 -\n 481 -\n 482 -\n 483 -\n 484 -\n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502 -\n 503 -\n 504 -\n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut, boolean affReady)\n        throws IgniteCheckedException {\n        DiscoveryEvent discoEvt = exchFut.discoveryEvent();\n\n        treatAllPartAsLoc = exchFut.activateCluster()\n            || (discoEvt.type() == EventType.EVT_NODE_JOINED\n            && discoEvt.eventNode().isLocal()\n            && !ctx.kernalContext().clientNode()\n        );\n\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            synchronized (ctx.exchange().interruptLock()) {\n                if (Thread.currentThread().isInterrupted())\n                    throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n                U.writeLock(lock);\n\n                try {\n                    if (stopping)\n                        return;\n\n                    GridDhtPartitionExchangeId exchId = exchFut.exchangeId();assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" +\n                        topVer + \", exchId=\" + exchId + ']';\n\n                    if (exchId.isLeft() && exchFut.serverNodeDiscoveryEvent())\n                        removeNode(exchId.nodeId());\n    \n                    ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n                    if (log.isDebugEnabled())\n                        log.debug(\"Partition map beforeExchange [exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n\n                    long updateSeq = this.updateSeq.incrementAndGet();\n\n                    cntrMap.clear();\n\n                    boolean grpStarted = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());// If this is the oldest node.\n\n                    if (oldest != null && (loc.equals(oldest) || grpStarted)) {\n                        if (node2part == null) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                                    exchId + \", fullMap=\" + fullMapString() + ']');\n                        }\n                        else if (!node2part.valid()) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId + \", fullMap=\" +\n                                    node2part + ']');\n                        }\n                        else if (!node2part.nodeId().equals(loc.id())) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq, node2part, false);\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [exchId=\" +\n                                    exchId + \", fullMap=\" + fullMapString() + ']');\n                        }\n                    }\n\n                    if (grpStarted ||\n                        exchFut.discoveryEvent().type() == EVT_DISCOVERY_CUSTOM_EVT ||\n                        exchFut.serverNodeDiscoveryEvent()) {\n                        if (affReady)\n                            initPartitions0(exchFut, updateSeq);\n                        else {\n                            List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                            createPartitions(aff, updateSeq);\n                        }\n                    }\n\n                    consistencyCheck();\n\n                    if (log.isDebugEnabled())\n                        log.debug(\"Partition map after beforeExchange [exchId=\" + exchId + \", fullMap=\" +\n                            fullMapString() + ']');\n                }\n                finally {\n                    lock.writeLock().unlock();\n                }\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }",
            " 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447 +\n 448 +\n 449 +\n 450 +\n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457 +\n 458 +\n 459 +\n 460 +\n 461  \n 462  \n 463  \n 464  \n 465  \n 466 +\n 467  \n 468 +\n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478 +\n 479 +\n 480 +\n 481 +\n 482 +\n 483 +\n 484 +\n 485 +\n 486 +\n 487 +\n 488  \n 489  \n 490 +\n 491 +\n 492 +\n 493 +\n 494 +\n 495 +\n 496 +\n 497 +\n 498 +\n 499 +\n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517 +\n 518 +\n 519 +\n 520 +\n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  ",
            "    /** {@inheritDoc} */\n    @Override public void beforeExchange(GridDhtPartitionsExchangeFuture exchFut, boolean affReady)\n        throws IgniteCheckedException {\n        DiscoveryEvent discoEvt = exchFut.discoveryEvent();\n\n        treatAllPartAsLoc = exchFut.activateCluster()\n            || (discoEvt.type() == EventType.EVT_NODE_JOINED\n            && discoEvt.eventNode().isLocal()\n            && !ctx.kernalContext().clientNode()\n        );\n\n        ClusterNode loc = ctx.localNode();\n\n        ctx.database().checkpointReadLock();\n\n        try {\n            synchronized (ctx.exchange().interruptLock()) {\n                if (Thread.currentThread().isInterrupted())\n                    throw new IgniteInterruptedCheckedException(\"Thread is interrupted: \" + Thread.currentThread());\n\n                U.writeLock(lock);\n\n                try {\n                    if (stopping)\n                        return;\n\n                    GridDhtPartitionExchangeId exchId = exchFut.exchangeId();\n\n                    assert topVer.equals(exchId.topologyVersion()) : \"Invalid topology version [topVer=\" + topVer +\n                        \", exchId=\" + exchId + ']';\n\n                    if (exchId.isLeft() && exchFut.serverNodeDiscoveryEvent())\n                        removeNode(exchId.nodeId());\n    \n                    ClusterNode oldest = discoCache.oldestAliveServerNodeWithCache();\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map beforeExchange [exchId=\" + exchId +\n                            \", fullMap=\" + fullMapString() + ']');\n                    }\n\n                    long updateSeq = this.updateSeq.incrementAndGet();\n\n                    cntrMap.clear();\n\n                    boolean grpStarted = exchFut.cacheGroupAddedOnExchange(grp.groupId(), grp.receivedFrom());\n\n                    // If this is the oldest node.\n                    if (oldest != null && (loc.equals(oldest) || grpStarted)) {\n                        if (node2part == null) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(), oldest.order(), updateSeq);\n\n                            if (log.isDebugEnabled())\n                                log.debug(\"Created brand new full topology map on oldest node [exchId=\" +\n                                    exchId + \", fullMap=\" + fullMapString() + ']');\n                        }\n                        else if (!node2part.valid()) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Created new full topology map on oldest node [exchId=\" + exchId +\n                                    \", fullMap=\" + node2part + ']');\n                            }\n                        }\n                        else if (!node2part.nodeId().equals(loc.id())) {\n                            node2part = new GridDhtPartitionFullMap(oldest.id(),\n                                oldest.order(),\n                                updateSeq,\n                                node2part,\n                                false);\n\n                            if (log.isDebugEnabled()) {\n                                log.debug(\"Copied old map into new map on oldest node (previous oldest node left) [\" +\n                                    \"exchId=\" + exchId + \", fullMap=\" + fullMapString() + ']');\n                            }\n                        }\n                    }\n\n                    if (grpStarted ||\n                        exchFut.discoveryEvent().type() == EVT_DISCOVERY_CUSTOM_EVT ||\n                        exchFut.serverNodeDiscoveryEvent()) {\n                        if (affReady)\n                            initPartitions0(exchFut, updateSeq);\n                        else {\n                            List<List<ClusterNode>> aff = grp.affinity().idealAssignment();\n\n                            createPartitions(aff, updateSeq);\n                        }\n                    }\n\n                    consistencyCheck();\n\n                    if (log.isDebugEnabled()) {\n                        log.debug(\"Partition map after beforeExchange [exchId=\" + exchId +\n                            \", fullMap=\" + fullMapString() + ']');\n                    }\n                }\n                finally {\n                    lock.writeLock().unlock();\n                }\n            }\n        }\n        finally {\n            ctx.database().checkpointReadUnlock();\n        }\n    }"
        ]
    ],
    "770efe23ce58a62461c698b42b4d58c5791bceb2": [
        [
            "FileWriteAheadLogManager::FileWriteHandle::flushOrWait(FileWALPointer,boolean)",
            "1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803  \n1804  \n1805  \n1806  \n1807  \n1808  \n1809  \n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  ",
            "        /**\n         * Flush or wait for concurrent flush completion.\n         *\n         * @param ptr Pointer.\n         * @throws IgniteCheckedException If failed.\n         */\n        private void flushOrWait(FileWALPointer ptr, boolean stop) throws IgniteCheckedException {\n            long expWritten;\n\n            if (ptr != null) {\n                // If requested obsolete file index, it must be already flushed by close.\n                if (ptr.index() != idx)\n                    return;\n\n                expWritten = ptr.fileOffset();\n            }\n            else // We read head position before the flush because otherwise we can get wrong position.\n                expWritten = recordOffset(head.get());\n\n            if (flush(ptr, stop))\n                return;\n\n            // Spin-wait for a while before acquiring the lock.\n            for (int i = 0; i < 64; i++) {\n                if (written >= expWritten)\n                    return;\n            }\n\n            // If we did not flush ourselves then await for concurrent flush to complete.\n            lock.lock();\n\n            try {\n                while (written < expWritten && envFailed == null)\n                    U.await(writeComplete);\n            }\n            finally {\n                lock.unlock();\n            }\n        }",
            "1782  \n1783  \n1784  \n1785  \n1786  \n1787  \n1788  \n1789  \n1790  \n1791  \n1792  \n1793  \n1794  \n1795  \n1796  \n1797  \n1798  \n1799  \n1800  \n1801  \n1802  \n1803 +\n1804 +\n1805 +\n1806 +\n1807 +\n1808 +\n1809 +\n1810  \n1811  \n1812  \n1813  \n1814  \n1815  \n1816  \n1817  \n1818  \n1819  \n1820  \n1821  \n1822  \n1823  \n1824  \n1825  \n1826  \n1827  ",
            "        /**\n         * Flush or wait for concurrent flush completion.\n         *\n         * @param ptr Pointer.\n         * @throws IgniteCheckedException If failed.\n         */\n        private void flushOrWait(FileWALPointer ptr, boolean stop) throws IgniteCheckedException {\n            long expWritten;\n\n            if (ptr != null) {\n                // If requested obsolete file index, it must be already flushed by close.\n                if (ptr.index() != idx)\n                    return;\n\n                expWritten = ptr.fileOffset();\n            }\n            else // We read head position before the flush because otherwise we can get wrong position.\n                expWritten = recordOffset(head.get());\n\n            if (flush(ptr, stop))\n                return;\n            else if (stop) {\n                FakeRecord fr = (FakeRecord)head.get();\n\n                assert fr.stop : \"Invalid fake record on top of the queue: \" + fr;\n\n                expWritten = recordOffset(fr);\n            }\n\n            // Spin-wait for a while before acquiring the lock.\n            for (int i = 0; i < 64; i++) {\n                if (written >= expWritten)\n                    return;\n            }\n\n            // If we did not flush ourselves then await for concurrent flush to complete.\n            lock.lock();\n\n            try {\n                while (written < expWritten && envFailed == null)\n                    U.await(writeComplete);\n            }\n            finally {\n                lock.unlock();\n            }\n        }"
        ],
        [
            "FileWriteAheadLogManager::FileWriteHandle::close(boolean)",
            "2046  \n2047  \n2048  \n2049  \n2050  \n2051  \n2052 -\n2053 -\n2054 -\n2055  \n2056  \n2057 -\n2058  \n2059 -\n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073 -\n2074  \n2075 -\n2076 -\n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091  \n2092  ",
            "        /**\n         * @return {@code true} If this thread actually closed the segment.\n         * @throws IgniteCheckedException If failed.\n         * @throws StorageException If failed.\n         */\n        private boolean close(boolean rollOver) throws IgniteCheckedException, StorageException {\n            if (mode == WALMode.DEFAULT)\n                fsync(null, true);\n            else\n                flushOrWait(null, true);\n\n            assert stopped() : \"Segment is not closed after close flush: \" + head.get();\n\n            if (stop.compareAndSet(false, true)) {\n                try {\n                    int switchSegmentRecSize = RecordV1Serializer.REC_TYPE_SIZE + RecordV1Serializer.FILE_WAL_POINTER_SIZE;\n\n                    if (rollOver && written < (maxSegmentSize - switchSegmentRecSize)) {\n                        //it is expected there is sufficient space for this record because rollover should run early\n                        final ByteBuffer buf = ByteBuffer.allocate(switchSegmentRecSize);\n                        buf.put((byte)(WALRecord.RecordType.SWITCH_SEGMENT_RECORD.ordinal() + 1));\n\n                        final FileWALPointer pointer = new FileWALPointer(idx, (int)fileIO.position(), -1);\n                        RecordV1Serializer.putPosition(buf, pointer);\n\n                        buf.rewind();\n\n                        fileIO.write(buf, written);\n\n                        if (mode == WALMode.DEFAULT)\n                            fileIO.force();\n                    }\n\n                    fileIO.close();\n                }\n                catch (IOException e) {\n                    throw new IgniteCheckedException(e);\n                }\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Closed WAL write handle [idx=\" + idx + \"]\");\n\n                return true;\n            }\n            else\n                return false;\n        }",
            "2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059 +\n2060  \n2061  \n2062 +\n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077 +\n2078 +\n2079 +\n2080 +\n2081 +\n2082 +\n2083 +\n2084 +\n2085 +\n2086 +\n2087 +\n2088 +\n2089 +\n2090 +\n2091  \n2092 +\n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100  \n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  ",
            "        /**\n         * @return {@code true} If this thread actually closed the segment.\n         * @throws IgniteCheckedException If failed.\n         * @throws StorageException If failed.\n         */\n        private boolean close(boolean rollOver) throws IgniteCheckedException, StorageException {\n            if (stop.compareAndSet(false, true)) {\n                flushOrWait(null, true);\n\n                assert stopped() : \"Segment is not closed after close flush: \" + head.get();\n\n                try {\n                    int switchSegmentRecSize = RecordV1Serializer.REC_TYPE_SIZE + RecordV1Serializer.FILE_WAL_POINTER_SIZE;\n\n                    if (rollOver && written < (maxSegmentSize - switchSegmentRecSize)) {\n                        //it is expected there is sufficient space for this record because rollover should run early\n                        final ByteBuffer buf = ByteBuffer.allocate(switchSegmentRecSize);\n                        buf.put((byte)(WALRecord.RecordType.SWITCH_SEGMENT_RECORD.ordinal() + 1));\n\n                        final FileWALPointer pointer = new FileWALPointer(idx, (int)fileIO.position(), -1);\n                        RecordV1Serializer.putPosition(buf, pointer);\n\n                        buf.rewind();\n\n                        int rem = buf.remaining();\n\n                        while (rem > 0) {\n                            int written0 = fileIO.write(buf, written);\n\n                            written += written0;\n\n                            rem -= written0;\n                        }\n                    }\n\n                    // Do the final fsync.\n                    if (mode == WALMode.DEFAULT) {\n                        fileIO.force();\n\n                        lastFsyncPos = written;\n                    }\n\n                    fileIO.close();\n                }\n                catch (IOException e) {\n                    throw new IgniteCheckedException(e);\n                }\n\n                if (log.isDebugEnabled())\n                    log.debug(\"Closed WAL write handle [idx=\" + idx + \"]\");\n\n                return true;\n            }\n            else\n                return false;\n        }"
        ]
    ],
    "2c9057a3eb060cb5beb8ae9bc4173d15d99433e4": [
        [
            "GridDhtPartitionsExchangeFuture::topologyVersion()",
            " 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 -\n 369  ",
            "    /** {@inheritDoc} */\n    @Override public AffinityTopologyVersion topologyVersion() {\n        /*\n        Should not be called before exchange is finished since result version can change in\n        case of merged exchanges.\n         */\n        assert exchangeDone() : \"Should not be called before exchange is finished\";\n\n        return exchCtx.events().topologyVersion();\n    }",
            " 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368 +\n 369  ",
            "    /** {@inheritDoc} */\n    @Override public AffinityTopologyVersion topologyVersion() {\n        /*\n        Should not be called before exchange is finished since result version can change in\n        case of merged exchanges.\n         */\n        assert exchangeDone() : \"Should not be called before exchange is finished\";\n\n        return isDone() ? result() : exchCtx.events().topologyVersion();\n    }"
        ]
    ],
    "5c8c492005868e6ee2bad1fb8daac5b202da52dd": [
        [
            "JdbcDatabaseMetadata::columnRow(String,String,String,int,String,boolean,int)",
            " 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846 -\n 847 -\n 848 -\n 849 -\n 850 -\n 851 -\n 852 -\n 853 -\n 854 -\n 855 -\n 856 -\n 857 -\n 858 -\n 859 -\n 860 -\n 861 -\n 862 -\n 863 -\n 864 -\n 865 -\n 866  \n 867  \n 868  ",
            "    /**\n     * @param schema Schema name.\n     * @param tbl Table name.\n     * @param col Column name.\n     * @param type Type.\n     * @param typeName Type name.\n     * @param nullable Nullable flag.\n     * @param pos Ordinal position.\n     * @return Column metadata row.\n     */\n    private List<Object> columnRow(String schema, String tbl, String col, int type, String typeName,\n        boolean nullable, int pos) {\n        List<Object> row = new ArrayList<>(20);\n\n        row.add(null);\n        row.add(schema);\n        row.add(tbl);\n        row.add(col);\n        row.add(type);\n        row.add(typeName);\n        row.add(null);\n        row.add(null);\n        row.add(10);\n        row.add(nullable ? columnNullable : columnNoNulls);\n        row.add(null);\n        row.add(null);\n        row.add(Integer.MAX_VALUE);\n        row.add(pos);\n        row.add(\"YES\");\n        row.add(null);\n        row.add(null);\n        row.add(null);\n        row.add(null);\n        row.add(\"NO\");\n\n        return row;\n    }",
            " 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886 +\n 887 +\n 888 +\n 889 +\n 890 +\n 891 +\n 892 +\n 893 +\n 894 +\n 895 +\n 896 +\n 897 +\n 898 +\n 899 +\n 900 +\n 901 +\n 902 +\n 903 +\n 904 +\n 905 +\n 906 +\n 907 +\n 908 +\n 909 +\n 910  \n 911  \n 912  ",
            "    /**\n     * @param schema Schema name.\n     * @param tbl Table name.\n     * @param col Column name.\n     * @param type Type.\n     * @param typeName Type name.\n     * @param nullable Nullable flag.\n     * @param pos Ordinal position.\n     * @return Column metadata row.\n     */\n    private List<Object> columnRow(String schema, String tbl, String col, int type, String typeName,\n        boolean nullable, int pos) {\n        List<Object> row = new ArrayList<>(20);\n\n        row.add(null);                  // 1. TABLE_CAT\n        row.add(schema);                // 2. TABLE_SCHEM\n        row.add(tbl);                   // 3. TABLE_NAME\n        row.add(col);                   // 4. COLUMN_NAME\n        row.add(type);                  // 5. DATA_TYPE\n        row.add(typeName);              // 6. TYPE_NAME\n        row.add(null);                  // 7. COLUMN_SIZE\n        row.add(null);                  // 8. BUFFER_LENGTH\n        row.add(null);                  // 9. DECIMAL_DIGITS\n        row.add(10);                    // 10. NUM_PREC_RADIX\n        row.add(nullable ? columnNullable : columnNoNulls); // 11. NULLABLE\n        row.add(null);                  // 12. REMARKS\n        row.add(null);                  // 13. COLUMN_DEF\n        row.add(type);                  // 14. SQL_DATA_TYPE\n        row.add(null);                  // 15. SQL_DATETIME_SUB\n        row.add(Integer.MAX_VALUE);     // 16. CHAR_OCTET_LENGTH\n        row.add(pos);                   // 17. ORDINAL_POSITION\n        row.add(nullable ? \"YES\" : \"NO\"); // 18. IS_NULLABLE\n        row.add(null);                  // 19. SCOPE_CATALOG\n        row.add(null);                  // 20. SCOPE_SCHEMA\n        row.add(null);                  // 21. SCOPE_TABLE\n        row.add(null);                  // 22. SOURCE_DATA_TYPE\n        row.add(\"NO\");                  // 23. IS_AUTOINCREMENT\n        row.add(\"NO\");                  // 24. IS_GENERATEDCOLUMN\n\n        return row;\n    }"
        ],
        [
            "JdbcThinDatabaseMetadata::getColumns(String,String,String,String)",
            " 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796 -\n 797 -\n 798 -\n 799 -\n 800 -\n 801 -\n 802 -\n 803 -\n 804 -\n 805 -\n 806 -\n 807 -\n 808 -\n 809 -\n 810 -\n 811 -\n 812 -\n 813 -\n 814 -\n 815 -\n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  ",
            "    /** {@inheritDoc} */\n    @Override public ResultSet getColumns(String catalog, String schemaPtrn, String tblNamePtrn, String colNamePtrn)\n        throws SQLException {\n        conn.ensureNotClosed();\n\n        final List<JdbcColumnMeta> meta = Arrays.asList(\n            new JdbcColumnMeta(null, null, \"TABLE_CAT\", String.class),\n            new JdbcColumnMeta(null, null, \"TABLE_SCHEM\", String.class),\n            new JdbcColumnMeta(null, null, \"TABLE_NAME\", String.class),\n            new JdbcColumnMeta(null, null, \"COLUMN_NAME\", String.class),\n            new JdbcColumnMeta(null, null, \"DATA_TYPE\", Short.class),\n            new JdbcColumnMeta(null, null, \"TYPE_NAME\", String.class),\n            new JdbcColumnMeta(null, null, \"COLUMN_SIZE\", Integer.class),\n            new JdbcColumnMeta(null, null, \"DECIMAL_DIGITS\", Integer.class),\n            new JdbcColumnMeta(null, null, \"NUM_PREC_RADIX\", Short.class),\n            new JdbcColumnMeta(null, null, \"NULLABLE\", Short.class),\n            new JdbcColumnMeta(null, null, \"REMARKS\", String.class),\n            new JdbcColumnMeta(null, null, \"COLUMN_DEF\", String.class),\n            new JdbcColumnMeta(null, null, \"CHAR_OCTET_LENGTH\", Integer.class),\n            new JdbcColumnMeta(null, null, \"ORDINAL_POSITION\", Integer.class),\n            new JdbcColumnMeta(null, null, \"IS_NULLABLE\", String.class),\n            new JdbcColumnMeta(null, null, \"SCOPE_CATLOG\", String.class),\n            new JdbcColumnMeta(null, null, \"SCOPE_SCHEMA\", String.class),\n            new JdbcColumnMeta(null, null, \"SCOPE_TABLE\", String.class),\n            new JdbcColumnMeta(null, null, \"SOURCE_DATA_TYPE\", Short.class),\n            new JdbcColumnMeta(null, null, \"IS_AUTOINCREMENT\", String.class));\n\n        if (!validCatalogPattern(catalog))\n            return new JdbcThinResultSet(Collections.<List<Object>>emptyList(), meta);\n\n        JdbcMetaColumnsResult res = conn.sendRequest(new JdbcMetaColumnsRequest(schemaPtrn, tblNamePtrn, colNamePtrn));\n\n        List<List<Object>> rows = new LinkedList<>();\n\n        for (int i = 0; i < res.meta().size(); ++i)\n            rows.add(columnRow(res.meta().get(i), i + 1));\n\n        return new JdbcThinResultSet(rows, meta);\n    }",
            " 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796 +\n 797 +\n 798 +\n 799 +\n 800 +\n 801 +\n 802 +\n 803 +\n 804 +\n 805 +\n 806 +\n 807 +\n 808 +\n 809 +\n 810 +\n 811 +\n 812 +\n 813 +\n 814 +\n 815 +\n 816 +\n 817 +\n 818 +\n 819 +\n 820 +\n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  ",
            "    /** {@inheritDoc} */\n    @Override public ResultSet getColumns(String catalog, String schemaPtrn, String tblNamePtrn, String colNamePtrn)\n        throws SQLException {\n        conn.ensureNotClosed();\n\n        final List<JdbcColumnMeta> meta = Arrays.asList(\n            new JdbcColumnMeta(null, null, \"TABLE_CAT\", String.class),      // 1\n            new JdbcColumnMeta(null, null, \"TABLE_SCHEM\", String.class),    // 2\n            new JdbcColumnMeta(null, null, \"TABLE_NAME\", String.class),     // 3\n            new JdbcColumnMeta(null, null, \"COLUMN_NAME\", String.class),    // 4\n            new JdbcColumnMeta(null, null, \"DATA_TYPE\", Short.class),       // 5\n            new JdbcColumnMeta(null, null, \"TYPE_NAME\", String.class),      // 6\n            new JdbcColumnMeta(null, null, \"COLUMN_SIZE\", Integer.class),   // 7\n            new JdbcColumnMeta(null, null, \"BUFFER_LENGTH \", Integer.class), // 8\n            new JdbcColumnMeta(null, null, \"DECIMAL_DIGITS\", Integer.class), // 9\n            new JdbcColumnMeta(null, null, \"NUM_PREC_RADIX\", Short.class),  // 10\n            new JdbcColumnMeta(null, null, \"NULLABLE\", Short.class),        // 11\n            new JdbcColumnMeta(null, null, \"REMARKS\", String.class),        // 12\n            new JdbcColumnMeta(null, null, \"COLUMN_DEF\", String.class),     // 13\n            new JdbcColumnMeta(null, null, \"SQL_DATA_TYPE\", Integer.class), // 14\n            new JdbcColumnMeta(null, null, \"SQL_DATETIME_SUB\", Integer.class), // 15\n            new JdbcColumnMeta(null, null, \"CHAR_OCTET_LENGTH\", Integer.class), // 16\n            new JdbcColumnMeta(null, null, \"ORDINAL_POSITION\", Integer.class), // 17\n            new JdbcColumnMeta(null, null, \"IS_NULLABLE\", String.class),    // 18\n            new JdbcColumnMeta(null, null, \"SCOPE_CATLOG\", String.class),   // 19\n            new JdbcColumnMeta(null, null, \"SCOPE_SCHEMA\", String.class),   // 20\n            new JdbcColumnMeta(null, null, \"SCOPE_TABLE\", String.class),    // 21\n            new JdbcColumnMeta(null, null, \"SOURCE_DATA_TYPE\", Short.class), // 22\n            new JdbcColumnMeta(null, null, \"IS_AUTOINCREMENT\", String.class), // 23\n            new JdbcColumnMeta(null, null, \"IS_GENERATEDCOLUMN \", String.class) // 24\n        );\n\n        if (!validCatalogPattern(catalog))\n            return new JdbcThinResultSet(Collections.<List<Object>>emptyList(), meta);\n\n        JdbcMetaColumnsResult res = conn.sendRequest(new JdbcMetaColumnsRequest(schemaPtrn, tblNamePtrn, colNamePtrn));\n\n        List<List<Object>> rows = new LinkedList<>();\n\n        for (int i = 0; i < res.meta().size(); ++i)\n            rows.add(columnRow(res.meta().get(i), i + 1));\n\n        return new JdbcThinResultSet(rows, meta);\n    }"
        ],
        [
            "JdbcThinDatabaseMetadata::columnRow(JdbcColumnMeta,int)",
            " 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838 -\n 839 -\n 840 -\n 841 -\n 842 -\n 843 -\n 844 -\n 845 -\n 846 -\n 847 -\n 848 -\n 849 -\n 850 -\n 851 -\n 852 -\n 853 -\n 854 -\n 855 -\n 856 -\n 857 -\n 858  \n 859  \n 860  ",
            "    /**\n     * @param colMeta Column metadata.\n     * @param pos Ordinal position.\n     * @return Column metadata row.\n     */\n    private List<Object> columnRow(JdbcColumnMeta colMeta, int pos) {\n        List<Object> row = new ArrayList<>(20);\n\n        row.add((String)null);\n        row.add(colMeta.schemaName());\n        row.add(colMeta.tableName());\n        row.add(colMeta.columnName());\n        row.add(colMeta.dataType());\n        row.add(colMeta.dataTypeName());\n        row.add((Integer)null);\n        row.add((Integer)null);\n        row.add(10);\n        row.add(colMeta.isNullable() ? 1 : 0);\n        row.add((String)null);\n        row.add((String)null);\n        row.add(Integer.MAX_VALUE);\n        row.add(pos);\n        row.add(\"YES\");\n        row.add((String)null);\n        row.add((String)null);\n        row.add((String)null);\n        row.add((Short)null);\n        row.add(\"NO\");\n\n        return row;\n    }",
            " 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843 +\n 844 +\n 845 +\n 846 +\n 847 +\n 848 +\n 849 +\n 850 +\n 851 +\n 852 +\n 853 +\n 854 +\n 855 +\n 856 +\n 857 +\n 858 +\n 859 +\n 860 +\n 861 +\n 862 +\n 863 +\n 864 +\n 865 +\n 866 +\n 867  \n 868  \n 869  ",
            "    /**\n     * @param colMeta Column metadata.\n     * @param pos Ordinal position.\n     * @return Column metadata row.\n     */\n    private List<Object> columnRow(JdbcColumnMeta colMeta, int pos) {\n        List<Object> row = new ArrayList<>(20);\n\n        row.add((String)null);                  // 1. TABLE_CAT\n        row.add(colMeta.schemaName());          // 2. TABLE_SCHEM\n        row.add(colMeta.tableName());           // 3. TABLE_NAME\n        row.add(colMeta.columnName());          // 4. COLUMN_NAME\n        row.add(colMeta.dataType());            // 5. DATA_TYPE\n        row.add(colMeta.dataTypeName());        // 6. TYPE_NAME\n        row.add((Integer)null);                 // 7. COLUMN_SIZE\n        row.add((Integer)null);                 // 8. BUFFER_LENGTH\n        row.add((Integer)null);                 // 9. DECIMAL_DIGITS\n        row.add(10);                            // 10. NUM_PREC_RADIX\n        row.add(colMeta.isNullable() ? columnNullable : columnNoNulls);  // 11. NULLABLE\n        row.add((String)null);                  // 12. REMARKS\n        row.add((String)null);                  // 13. COLUMN_DEF\n        row.add(colMeta.dataType());            // 14. SQL_DATA_TYPE\n        row.add((Integer)null);                 // 15. SQL_DATETIME_SUB\n        row.add(Integer.MAX_VALUE);             // 16. CHAR_OCTET_LENGTH\n        row.add(pos);                           // 17. ORDINAL_POSITION\n        row.add(colMeta.isNullable() ? \"YES\" : \"NO\"); // 18. IS_NULLABLE\n        row.add((String)null);                  // 19. SCOPE_CATALOG\n        row.add((String)null);                  // 20. SCOPE_SCHEMA\n        row.add((String)null);                  // 21. SCOPE_TABLE\n        row.add((Short)null);                   // 22. SOURCE_DATA_TYPE\n        row.add(\"NO\");                          // 23. IS_AUTOINCREMENT\n        row.add(\"NO\");                          // 23. IS_GENERATEDCOLUMN\n\n        return row;\n    }"
        ],
        [
            "JdbcMetadataSelfTest::testGetColumns()",
            " 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testGetColumns() throws Exception {\n        try (Connection conn = DriverManager.getConnection(BASE_URL)) {\n            DatabaseMetaData meta = conn.getMetaData();\n\n            ResultSet rs = meta.getColumns(\"\", \"pers\", \"PERSON\", \"%\");\n\n            assertNotNull(rs);\n\n            Collection<String> names = new ArrayList<>(2);\n\n            names.add(\"NAME\");\n            names.add(\"AGE\");\n            names.add(\"ORGID\");\n\n            int cnt = 0;\n\n            while (rs.next()) {\n                String name = rs.getString(\"COLUMN_NAME\");\n\n                assertTrue(names.remove(name));\n\n                if (\"NAME\".equals(name)) {\n                    assertEquals(VARCHAR, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"VARCHAR\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(0, rs.getInt(\"NULLABLE\"));\n                } else if (\"AGE\".equals(name)) {\n                    assertEquals(INTEGER, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"INTEGER\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(0, rs.getInt(\"NULLABLE\"));\n                } else if (\"ORGID\".equals(name)) {\n                    assertEquals(INTEGER, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"INTEGER\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(1, rs.getInt(\"NULLABLE\"));\n                }\n\n                cnt++;\n            }\n\n            assertTrue(names.isEmpty());\n            assertEquals(3, cnt);\n\n            rs = meta.getColumns(\"\", \"org\", \"ORGANIZATION\", \"%\");\n\n            assertNotNull(rs);\n\n            names.add(\"ID\");\n            names.add(\"NAME\");\n\n            cnt = 0;\n\n            while (rs.next()) {\n                String name = rs.getString(\"COLUMN_NAME\");\n\n                assertTrue(names.remove(name));\n\n                if (\"id\".equals(name)) {\n                    assertEquals(INTEGER, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"INTEGER\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(0, rs.getInt(\"NULLABLE\"));\n                } else if (\"name\".equals(name)) {\n                    assertEquals(VARCHAR, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"VARCHAR\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(1, rs.getInt(\"NULLABLE\"));\n                }\n\n                cnt++;\n            }\n\n            assertTrue(names.isEmpty());\n            assertEquals(2, cnt);\n        }\n    }",
            " 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223 +\n 224 +\n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242 +\n 243 +\n 244  \n 245  \n 246  \n 247  \n 248 +\n 249 +\n 250  \n 251  \n 252  \n 253  \n 254 +\n 255 +\n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282 +\n 283 +\n 284  \n 285  \n 286  \n 287  \n 288 +\n 289 +\n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testGetColumns() throws Exception {\n        try (Connection conn = DriverManager.getConnection(BASE_URL)) {\n            DatabaseMetaData meta = conn.getMetaData();\n\n            ResultSet rs = meta.getColumns(\"\", \"pers\", \"PERSON\", \"%\");\n\n            assertNotNull(rs);\n\n            assertEquals(24, rs.getMetaData().getColumnCount());\n\n            Collection<String> names = new ArrayList<>(2);\n\n            names.add(\"NAME\");\n            names.add(\"AGE\");\n            names.add(\"ORGID\");\n\n            int cnt = 0;\n\n            while (rs.next()) {\n                String name = rs.getString(\"COLUMN_NAME\");\n\n                assertTrue(names.remove(name));\n\n                if (\"NAME\".equals(name)) {\n                    assertEquals(VARCHAR, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"VARCHAR\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(0, rs.getInt(\"NULLABLE\"));\n                    assertEquals(0, rs.getInt(11)); // nullable column by index\n                    assertEquals(\"NO\", rs.getString(\"IS_NULLABLE\"));\n                } else if (\"AGE\".equals(name)) {\n                    assertEquals(INTEGER, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"INTEGER\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(0, rs.getInt(\"NULLABLE\"));\n                    assertEquals(0, rs.getInt(11)); // nullable column by index\n                    assertEquals(\"NO\", rs.getString(\"IS_NULLABLE\"));\n                } else if (\"ORGID\".equals(name)) {\n                    assertEquals(INTEGER, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"INTEGER\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(1, rs.getInt(\"NULLABLE\"));\n                    assertEquals(1, rs.getInt(11)); // nullable column by index\n                    assertEquals(\"YES\", rs.getString(\"IS_NULLABLE\"));\n                }\n\n                cnt++;\n            }\n\n            assertTrue(names.isEmpty());\n            assertEquals(3, cnt);\n\n            rs = meta.getColumns(\"\", \"org\", \"ORGANIZATION\", \"%\");\n\n            assertNotNull(rs);\n\n            names.add(\"ID\");\n            names.add(\"NAME\");\n\n            cnt = 0;\n\n            while (rs.next()) {\n                String name = rs.getString(\"COLUMN_NAME\");\n\n                assertTrue(names.remove(name));\n\n                if (\"id\".equals(name)) {\n                    assertEquals(INTEGER, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"INTEGER\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(0, rs.getInt(\"NULLABLE\"));\n                    assertEquals(0, rs.getInt(11)); // nullable column by index\n                    assertEquals(\"NO\", rs.getString(\"IS_NULLABLE\"));\n                } else if (\"name\".equals(name)) {\n                    assertEquals(VARCHAR, rs.getInt(\"DATA_TYPE\"));\n                    assertEquals(\"VARCHAR\", rs.getString(\"TYPE_NAME\"));\n                    assertEquals(1, rs.getInt(\"NULLABLE\"));\n                    assertEquals(1, rs.getInt(11)); // nullable column by index\n                    assertEquals(\"YES\", rs.getString(\"IS_NULLABLE\"));\n                }\n\n                cnt++;\n            }\n\n            assertTrue(names.isEmpty());\n            assertEquals(2, cnt);\n        }\n    }"
        ],
        [
            "JdbcDatabaseMetadata::getColumns(String,String,String,String)",
            " 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818 -\n 819 -\n 820 -\n 821 -\n 822 -\n 823 -\n 824 -\n 825 -\n 826 -\n 827 -\n 828  ",
            "    /** {@inheritDoc} */\n    @Override public ResultSet getColumns(String catalog, String schemaPtrn, String tblNamePtrn,\n        String colNamePtrn) throws SQLException {\n        updateMetaData();\n\n        List<List<?>> rows = new LinkedList<>();\n\n        int cnt = 0;\n\n        if (validCatalogPattern(catalog)) {\n            for (Map.Entry<String, Map<String, Map<String, ColumnInfo>>> schema : meta.entrySet()) {\n                if (matches(schema.getKey(), schemaPtrn)) {\n                    for (Map.Entry<String, Map<String, ColumnInfo>> tbl : schema.getValue().entrySet()) {\n                        if (matches(tbl.getKey(), tblNamePtrn)) {\n                            for (Map.Entry<String, ColumnInfo> col : tbl.getValue().entrySet()) {\n                                rows.add(columnRow(schema.getKey(), tbl.getKey(), col.getKey(),\n                                    JdbcUtils.type(col.getValue().typeName()), JdbcUtils.typeName(col.getValue().typeName()),\n                                    !col.getValue().isNotNull(), ++cnt));\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        return new JdbcResultSet(true, null,\n            conn.createStatement0(),\n            Collections.<String>emptyList(),\n            Arrays.asList(\"TABLE_CAT\", \"TABLE_SCHEM\", \"TABLE_NAME\", \"COLUMN_NAME\", \"DATA_TYPE\",\n                \"TYPE_NAME\", \"COLUMN_SIZE\", \"DECIMAL_DIGITS\", \"NUM_PREC_RADIX\", \"NULLABLE\",\n                \"REMARKS\", \"COLUMN_DEF\", \"CHAR_OCTET_LENGTH\", \"ORDINAL_POSITION\", \"IS_NULLABLE\",\n                \"SCOPE_CATLOG\", \"SCOPE_SCHEMA\", \"SCOPE_TABLE\", \"SOURCE_DATA_TYPE\", \"IS_AUTOINCREMENT\"),\n            Arrays.asList(String.class.getName(), String.class.getName(), String.class.getName(),\n                String.class.getName(), Integer.class.getName(), String.class.getName(), Integer.class.getName(),\n                Integer.class.getName(), Integer.class.getName(), Integer.class.getName(), String.class.getName(),\n                String.class.getName(), Integer.class.getName(), Integer.class.getName(), String.class.getName(),\n                String.class.getName(), String.class.getName(), String.class.getName(), Short.class.getName(),\n                String.class.getName()),\n            rows, true",
            " 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818 +\n 819 +\n 820 +\n 821 +\n 822 +\n 823 +\n 824 +\n 825 +\n 826 +\n 827 +\n 828 +\n 829 +\n 830 +\n 831 +\n 832 +\n 833 +\n 834 +\n 835 +\n 836 +\n 837 +\n 838 +\n 839 +\n 840 +\n 841 +\n 842 +\n 843 +\n 844 +\n 845 +\n 846 +\n 847 +\n 848 +\n 849 +\n 850 +\n 851 +\n 852 +\n 853 +\n 854 +\n 855 +\n 856 +\n 857 +\n 858 +\n 859 +\n 860 +\n 861 +\n 862 +\n 863 +\n 864 +\n 865 +\n 866 +\n 867 +\n 868  ",
            "    /** {@inheritDoc} */\n    @Override public ResultSet getColumns(String catalog, String schemaPtrn, String tblNamePtrn,\n        String colNamePtrn) throws SQLException {\n        updateMetaData();\n\n        List<List<?>> rows = new LinkedList<>();\n\n        int cnt = 0;\n\n        if (validCatalogPattern(catalog)) {\n            for (Map.Entry<String, Map<String, Map<String, ColumnInfo>>> schema : meta.entrySet()) {\n                if (matches(schema.getKey(), schemaPtrn)) {\n                    for (Map.Entry<String, Map<String, ColumnInfo>> tbl : schema.getValue().entrySet()) {\n                        if (matches(tbl.getKey(), tblNamePtrn)) {\n                            for (Map.Entry<String, ColumnInfo> col : tbl.getValue().entrySet()) {\n                                rows.add(columnRow(schema.getKey(), tbl.getKey(), col.getKey(),\n                                    JdbcUtils.type(col.getValue().typeName()), JdbcUtils.typeName(col.getValue().typeName()),\n                                    !col.getValue().isNotNull(), ++cnt));\n                            }\n                        }\n                    }\n                }\n            }\n        }\n\n        return new JdbcResultSet(true, null,\n            conn.createStatement0(),\n            Collections.<String>emptyList(),\n            Arrays.asList(\n                \"TABLE_CAT\",        // 1\n                \"TABLE_SCHEM\",      // 2\n                \"TABLE_NAME\",       // 3\n                \"COLUMN_NAME\",      // 4\n                \"DATA_TYPE\",        // 5\n                \"TYPE_NAME\",        // 6\n                \"COLUMN_SIZE\",      // 7\n                \"BUFFER_LENGTH\",    // 8\n                \"DECIMAL_DIGITS\",   // 9\n                \"NUM_PREC_RADIX\",   // 10\n                \"NULLABLE\",         // 11\n                \"REMARKS\",          // 12\n                \"COLUMN_DEF\",       // 13\n                \"SQL_DATA_TYPE\",    // 14\n                \"SQL_DATETIME_SUB\", // 15\n                \"CHAR_OCTET_LENGTH\", // 16\n                \"ORDINAL_POSITION\",  // 17\n                \"IS_NULLABLE\",      // 18\n                \"SCOPE_CATLOG\",     // 19\n                \"SCOPE_SCHEMA\",     // 20\n                \"SCOPE_TABLE\",      // 21\n                \"SOURCE_DATA_TYPE\", // 22\n                \"IS_AUTOINCREMENT\", // 23\n                \"IS_GENERATEDCOLUMN\"), // 23\n            Arrays.asList(\n                String.class.getName(),     // 1\n                String.class.getName(),     // 2\n                String.class.getName(),     // 3\n                String.class.getName(),     // 4\n                Integer.class.getName(),    // 5\n                String.class.getName(),     // 6\n                Integer.class.getName(),    // 7\n                Integer.class.getName(),    // 8\n                Integer.class.getName(),    // 9\n                Integer.class.getName(),    // 10\n                Integer.class.getName(),    // 11\n                String.class.getName(),     // 12\n                String.class.getName(),     // 13\n                Integer.class.getName(),    // 14\n                Integer.class.getName(),    // 15\n                Integer.class.getName(),    // 16\n                Integer.class.getName(),    // 17\n                String.class.getName(),     // 18\n                String.class.getName(),     // 19\n                String.class.getName(),     // 20\n                String.class.getName(),     // 21\n                Short.class.getName(),      // 22\n                String.class.getName(),     // 23\n                String.class.getName()),    // 24\n            rows, true"
        ],
        [
            "JdbcThinMetadataSelfTest::testGetColumns()",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testGetColumns() throws Exception {\n        try (Connection conn = DriverManager.getConnection(URL)) {\n            conn.setSchema(\"pers\");\n\n            DatabaseMetaData meta = conn.getMetaData();\n\n            ResultSet rs = meta.getColumns(\"\", \"pers\", \"PERSON\", \"%\");\n\n            assert rs != null;\n\n            Collection<String> names = new ArrayList<>(2);\n\n            names.add(\"NAME\");\n            names.add(\"AGE\");\n            names.add(\"ORGID\");\n\n            int cnt = 0;\n\n            while (rs.next()) {\n                String name = rs.getString(\"COLUMN_NAME\");\n\n                assert names.remove(name);\n\n                if (\"NAME\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == VARCHAR;\n                    assert \"VARCHAR\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                } else if (\"ORGID\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == INTEGER;\n                    assert \"INTEGER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 1;\n                } else if (\"AGE\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == INTEGER;\n                    assert \"INTEGER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                }\n                else if (\"_KEY\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == OTHER;\n                    assert \"OTHER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                }\n                else if (\"_VAL\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == OTHER;\n                    assert \"OTHER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                }\n\n                cnt++;\n            }\n\n            assert names.isEmpty();\n            assert cnt == 3;\n\n            rs = meta.getColumns(\"\", \"org\", \"ORGANIZATION\", \"%\");\n\n            assert rs != null;\n\n            names.add(\"ID\");\n            names.add(\"NAME\");\n\n            cnt = 0;\n\n            while (rs.next()) {\n                String name = rs.getString(\"COLUMN_NAME\");\n\n                assert names.remove(name);\n\n                if (\"id\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == INTEGER;\n                    assert \"INTEGER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                } else if (\"name\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == VARCHAR;\n                    assert \"VARCHAR\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 1;\n                }\n                if (\"_KEY\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == VARCHAR;\n                    assert \"VARCHAR\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                }\n                if (\"_VAL\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == OTHER;\n                    assert \"OTHER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                }\n\n                cnt++;\n            }\n\n            assert names.isEmpty();\n            assert cnt == 2;\n        }\n    }",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263 +\n 264 +\n 265 +\n 266 +\n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286 +\n 287 +\n 288  \n 289  \n 290  \n 291  \n 292 +\n 293 +\n 294  \n 295  \n 296  \n 297  \n 298 +\n 299 +\n 300  \n 301  \n 302  \n 303  \n 304  \n 305 +\n 306 +\n 307  \n 308  \n 309  \n 310  \n 311  \n 312 +\n 313 +\n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testGetColumns() throws Exception {\n        try (Connection conn = DriverManager.getConnection(URL)) {\n            conn.setSchema(\"pers\");\n\n            DatabaseMetaData meta = conn.getMetaData();\n\n            ResultSet rs = meta.getColumns(\"\", \"pers\", \"PERSON\", \"%\");\n\n            ResultSetMetaData rsMeta = rs.getMetaData();\n\n            assert rsMeta.getColumnCount() == 24 : \"Invalid columns count: \" + rsMeta.getColumnCount();\n\n            assert rs != null;\n\n            Collection<String> names = new ArrayList<>(2);\n\n            names.add(\"NAME\");\n            names.add(\"AGE\");\n            names.add(\"ORGID\");\n\n            int cnt = 0;\n\n            while (rs.next()) {\n                String name = rs.getString(\"COLUMN_NAME\");\n\n                assert names.remove(name);\n\n                if (\"NAME\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == VARCHAR;\n                    assert \"VARCHAR\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                    assert rs.getInt(11) == 0; // nullable column by index\n                    assert rs.getString(\"IS_NULLABLE\").equals(\"NO\");\n                } else if (\"ORGID\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == INTEGER;\n                    assert \"INTEGER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 1;\n                    assert rs.getInt(11) == 1;  // nullable column by index\n                    assert rs.getString(\"IS_NULLABLE\").equals(\"YES\");\n                } else if (\"AGE\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == INTEGER;\n                    assert \"INTEGER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                    assert rs.getInt(11) == 0;  // nullable column by index\n                    assert rs.getString(\"IS_NULLABLE\").equals(\"NO\");\n                }\n                else if (\"_KEY\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == OTHER;\n                    assert \"OTHER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                    assert rs.getInt(11) == 0;  // nullable column by index\n                    assert rs.getString(\"IS_NULLABLE\").equals(\"NO\");\n                }\n                else if (\"_VAL\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == OTHER;\n                    assert \"OTHER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                    assert rs.getInt(11) == 0;  // nullable column by index\n                    assert rs.getString(\"IS_NULLABLE\").equals(\"NO\");\n                }\n\n                cnt++;\n            }\n\n            assert names.isEmpty();\n            assert cnt == 3;\n\n            rs = meta.getColumns(\"\", \"org\", \"ORGANIZATION\", \"%\");\n\n            assert rs != null;\n\n            names.add(\"ID\");\n            names.add(\"NAME\");\n\n            cnt = 0;\n\n            while (rs.next()) {\n                String name = rs.getString(\"COLUMN_NAME\");\n\n                assert names.remove(name);\n\n                if (\"id\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == INTEGER;\n                    assert \"INTEGER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                } else if (\"name\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == VARCHAR;\n                    assert \"VARCHAR\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 1;\n                }\n                if (\"_KEY\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == VARCHAR;\n                    assert \"VARCHAR\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                }\n                if (\"_VAL\".equals(name)) {\n                    assert rs.getInt(\"DATA_TYPE\") == OTHER;\n                    assert \"OTHER\".equals(rs.getString(\"TYPE_NAME\"));\n                    assert rs.getInt(\"NULLABLE\") == 0;\n                }\n\n                cnt++;\n            }\n\n            assert names.isEmpty();\n            assert cnt == 2;\n        }\n    }"
        ]
    ],
    "5b3ad97b939bee6f3e272f93c75e920208cb7491": [
        [
            "PlatformConfigurationUtils::readCacheConfiguration(BinaryRawReaderEx)",
            " 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  ",
            "    /**\n     * Reads cache configuration from a stream.\n     *\n     * @param in Stream.\n     * @return Cache configuration.\n     */\n    public static CacheConfiguration readCacheConfiguration(BinaryRawReaderEx in) {\n        assert in != null;\n\n        CacheConfiguration ccfg = new CacheConfiguration();\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.fromOrdinal(in.readInt()));\n        ccfg.setBackups(in.readInt());\n        ccfg.setCacheMode(CacheMode.fromOrdinal(in.readInt()));\n        ccfg.setCopyOnRead(in.readBoolean());\n        ccfg.setEagerTtl(in.readBoolean());\n        ccfg.setInvalidate(in.readBoolean());\n        ccfg.setStoreKeepBinary(in.readBoolean());\n        ccfg.setLoadPreviousValue(in.readBoolean());\n        ccfg.setDefaultLockTimeout(in.readLong());\n        //noinspection deprecation\n        ccfg.setLongQueryWarningTimeout(in.readLong());\n        ccfg.setMaxConcurrentAsyncOperations(in.readInt());\n        ccfg.setName(in.readString());\n        ccfg.setReadFromBackup(in.readBoolean());\n        ccfg.setRebalanceBatchSize(in.readInt());\n        ccfg.setRebalanceDelay(in.readLong());\n        ccfg.setRebalanceMode(CacheRebalanceMode.fromOrdinal(in.readInt()));\n        ccfg.setRebalanceThrottle(in.readLong());\n        ccfg.setRebalanceTimeout(in.readLong());\n        ccfg.setSqlEscapeAll(in.readBoolean());\n        ccfg.setWriteBehindBatchSize(in.readInt());\n        ccfg.setWriteBehindEnabled(in.readBoolean());\n        ccfg.setWriteBehindFlushFrequency(in.readLong());\n        ccfg.setWriteBehindFlushSize(in.readInt());\n        ccfg.setWriteBehindFlushThreadCount(in.readInt());\n        ccfg.setWriteBehindCoalescing(in.readBoolean());\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.fromOrdinal(in.readInt()));\n        ccfg.setReadThrough(in.readBoolean());\n        ccfg.setWriteThrough(in.readBoolean());\n        ccfg.setStatisticsEnabled(in.readBoolean());\n\n        String dataRegionName = in.readString();\n\n        if (dataRegionName != null)\n            //noinspection deprecation\n            ccfg.setMemoryPolicyName(dataRegionName);\n\n        ccfg.setPartitionLossPolicy(PartitionLossPolicy.fromOrdinal((byte)in.readInt()));\n        ccfg.setGroupName(in.readString());\n\n        Object storeFactory = in.readObjectDetached();\n\n        if (storeFactory != null)\n            ccfg.setCacheStoreFactory(new PlatformDotNetCacheStoreFactoryNative(storeFactory));\n\n        ccfg.setSqlIndexMaxInlineSize(in.readInt());\n\n        int qryEntCnt = in.readInt();\n\n        if (qryEntCnt > 0) {\n            Collection<QueryEntity> entities = new ArrayList<>(qryEntCnt);\n\n            for (int i = 0; i < qryEntCnt; i++)\n                entities.add(readQueryEntity(in));\n\n            ccfg.setQueryEntities(entities);\n        }\n\n        if (in.readBoolean())\n            ccfg.setNearConfiguration(readNearConfiguration(in));\n\n        ccfg.setEvictionPolicy(readEvictionPolicy(in));\n        if (ccfg.getEvictionPolicy() != null)\n            ccfg.setOnheapCacheEnabled(true);\n\n        ccfg.setAffinity(readAffinityFunction(in));\n        ccfg.setExpiryPolicyFactory(readExpiryPolicyFactory(in));\n\n        int pluginCnt = in.readInt();\n\n        if (pluginCnt > 0) {\n            ArrayList<CachePluginConfiguration> plugins = new ArrayList<>();\n\n            for (int i = 0; i < pluginCnt; i++) {\n                if (in.readBoolean()) {\n                    // Java cache plugin.\n                    readCachePluginConfiguration(ccfg, in);\n                } else {\n                    // Platform cache plugin.\n                    plugins.add(new PlatformCachePluginConfiguration(in.readObjectDetached()));\n                }\n            }\n\n            if (ccfg.getPluginConfigurations() != null) {\n                Collections.addAll(plugins, ccfg.getPluginConfigurations());\n            }\n\n            ccfg.setPluginConfigurations(plugins.toArray(new CachePluginConfiguration[plugins.size()]));\n        }\n\n        return ccfg;\n    }",
            " 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227 +\n 228 +\n 229 +\n 230 +\n 231 +\n 232 +\n 233 +\n 234 +\n 235 +\n 236 +\n 237 +\n 238 +\n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  ",
            "    /**\n     * Reads cache configuration from a stream.\n     *\n     * @param in Stream.\n     * @return Cache configuration.\n     */\n    public static CacheConfiguration readCacheConfiguration(BinaryRawReaderEx in) {\n        assert in != null;\n\n        CacheConfiguration ccfg = new CacheConfiguration();\n\n        ccfg.setAtomicityMode(CacheAtomicityMode.fromOrdinal(in.readInt()));\n        ccfg.setBackups(in.readInt());\n        ccfg.setCacheMode(CacheMode.fromOrdinal(in.readInt()));\n        ccfg.setCopyOnRead(in.readBoolean());\n        ccfg.setEagerTtl(in.readBoolean());\n        ccfg.setInvalidate(in.readBoolean());\n        ccfg.setStoreKeepBinary(in.readBoolean());\n        ccfg.setLoadPreviousValue(in.readBoolean());\n        ccfg.setDefaultLockTimeout(in.readLong());\n        //noinspection deprecation\n        ccfg.setLongQueryWarningTimeout(in.readLong());\n        ccfg.setMaxConcurrentAsyncOperations(in.readInt());\n        ccfg.setName(in.readString());\n        ccfg.setReadFromBackup(in.readBoolean());\n        ccfg.setRebalanceBatchSize(in.readInt());\n        ccfg.setRebalanceDelay(in.readLong());\n        ccfg.setRebalanceMode(CacheRebalanceMode.fromOrdinal(in.readInt()));\n        ccfg.setRebalanceThrottle(in.readLong());\n        ccfg.setRebalanceTimeout(in.readLong());\n        ccfg.setSqlEscapeAll(in.readBoolean());\n        ccfg.setWriteBehindBatchSize(in.readInt());\n        ccfg.setWriteBehindEnabled(in.readBoolean());\n        ccfg.setWriteBehindFlushFrequency(in.readLong());\n        ccfg.setWriteBehindFlushSize(in.readInt());\n        ccfg.setWriteBehindFlushThreadCount(in.readInt());\n        ccfg.setWriteBehindCoalescing(in.readBoolean());\n        ccfg.setWriteSynchronizationMode(CacheWriteSynchronizationMode.fromOrdinal(in.readInt()));\n        ccfg.setReadThrough(in.readBoolean());\n        ccfg.setWriteThrough(in.readBoolean());\n        ccfg.setStatisticsEnabled(in.readBoolean());\n\n        String dataRegionName = in.readString();\n\n        if (dataRegionName != null)\n            //noinspection deprecation\n            ccfg.setMemoryPolicyName(dataRegionName);\n\n        ccfg.setPartitionLossPolicy(PartitionLossPolicy.fromOrdinal((byte)in.readInt()));\n        ccfg.setGroupName(in.readString());\n\n        Object storeFactory = in.readObjectDetached();\n\n        if (storeFactory != null)\n            ccfg.setCacheStoreFactory(new PlatformDotNetCacheStoreFactoryNative(storeFactory));\n\n        ccfg.setSqlIndexMaxInlineSize(in.readInt());\n\n        int qryEntCnt = in.readInt();\n\n        if (qryEntCnt > 0) {\n            Collection<QueryEntity> entities = new ArrayList<>(qryEntCnt);\n\n            for (int i = 0; i < qryEntCnt; i++)\n                entities.add(readQueryEntity(in));\n\n            ccfg.setQueryEntities(entities);\n        }\n\n        if (in.readBoolean())\n            ccfg.setNearConfiguration(readNearConfiguration(in));\n\n        ccfg.setEvictionPolicy(readEvictionPolicy(in));\n        if (ccfg.getEvictionPolicy() != null)\n            ccfg.setOnheapCacheEnabled(true);\n\n        ccfg.setAffinity(readAffinityFunction(in));\n        ccfg.setExpiryPolicyFactory(readExpiryPolicyFactory(in));\n\n        int keyCnt = in.readInt();\n\n        if (keyCnt > 0) {\n            CacheKeyConfiguration[] keys = new CacheKeyConfiguration[keyCnt];\n\n            for (int i = 0; i < keyCnt; i++) {\n                keys[i] = new CacheKeyConfiguration(in.readString(), in.readString());\n            }\n\n            ccfg.setKeyConfiguration(keys);\n        }\n\n        int pluginCnt = in.readInt();\n\n        if (pluginCnt > 0) {\n            ArrayList<CachePluginConfiguration> plugins = new ArrayList<>();\n\n            for (int i = 0; i < pluginCnt; i++) {\n                if (in.readBoolean()) {\n                    // Java cache plugin.\n                    readCachePluginConfiguration(ccfg, in);\n                } else {\n                    // Platform cache plugin.\n                    plugins.add(new PlatformCachePluginConfiguration(in.readObjectDetached()));\n                }\n            }\n\n            if (ccfg.getPluginConfigurations() != null) {\n                Collections.addAll(plugins, ccfg.getPluginConfigurations());\n            }\n\n            ccfg.setPluginConfigurations(plugins.toArray(new CachePluginConfiguration[plugins.size()]));\n        }\n\n        return ccfg;\n    }"
        ],
        [
            "PlatformConfigurationUtils::writeCacheConfiguration(BinaryRawWriter,CacheConfiguration)",
            " 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  ",
            "    /**\n     * Writes cache configuration.\n     *\n     * @param writer Writer.\n     * @param ccfg Configuration.\n     */\n    public static void writeCacheConfiguration(BinaryRawWriter writer, CacheConfiguration ccfg) {\n        assert writer != null;\n        assert ccfg != null;\n\n        writeEnumInt(writer, ccfg.getAtomicityMode(), CacheConfiguration.DFLT_CACHE_ATOMICITY_MODE);\n        writer.writeInt(ccfg.getBackups());\n        writeEnumInt(writer, ccfg.getCacheMode(), CacheConfiguration.DFLT_CACHE_MODE);\n        writer.writeBoolean(ccfg.isCopyOnRead());\n        writer.writeBoolean(ccfg.isEagerTtl());\n        writer.writeBoolean(ccfg.isInvalidate());\n        writer.writeBoolean(ccfg.isStoreKeepBinary());\n        writer.writeBoolean(ccfg.isLoadPreviousValue());\n        writer.writeLong(ccfg.getDefaultLockTimeout());\n        //noinspection deprecation\n        writer.writeLong(ccfg.getLongQueryWarningTimeout());\n        writer.writeInt(ccfg.getMaxConcurrentAsyncOperations());\n        writer.writeString(ccfg.getName());\n        writer.writeBoolean(ccfg.isReadFromBackup());\n        writer.writeInt(ccfg.getRebalanceBatchSize());\n        writer.writeLong(ccfg.getRebalanceDelay());\n        writeEnumInt(writer, ccfg.getRebalanceMode(), CacheConfiguration.DFLT_REBALANCE_MODE);\n        writer.writeLong(ccfg.getRebalanceThrottle());\n        writer.writeLong(ccfg.getRebalanceTimeout());\n        writer.writeBoolean(ccfg.isSqlEscapeAll());\n        writer.writeInt(ccfg.getWriteBehindBatchSize());\n        writer.writeBoolean(ccfg.isWriteBehindEnabled());\n        writer.writeLong(ccfg.getWriteBehindFlushFrequency());\n        writer.writeInt(ccfg.getWriteBehindFlushSize());\n        writer.writeInt(ccfg.getWriteBehindFlushThreadCount());\n        writer.writeBoolean(ccfg.getWriteBehindCoalescing());\n        writeEnumInt(writer, ccfg.getWriteSynchronizationMode());\n        writer.writeBoolean(ccfg.isReadThrough());\n        writer.writeBoolean(ccfg.isWriteThrough());\n        writer.writeBoolean(ccfg.isStatisticsEnabled());\n        //noinspection deprecation\n        writer.writeString(ccfg.getMemoryPolicyName());\n        writer.writeInt(ccfg.getPartitionLossPolicy().ordinal());\n        writer.writeString(ccfg.getGroupName());\n\n        if (ccfg.getCacheStoreFactory() instanceof PlatformDotNetCacheStoreFactoryNative)\n            writer.writeObject(((PlatformDotNetCacheStoreFactoryNative)ccfg.getCacheStoreFactory()).getNativeFactory());\n        else\n            writer.writeObject(null);\n\n        writer.writeInt(ccfg.getSqlIndexMaxInlineSize());\n\n        Collection<QueryEntity> qryEntities = ccfg.getQueryEntities();\n\n        if (qryEntities != null) {\n            writer.writeInt(qryEntities.size());\n\n            for (QueryEntity e : qryEntities)\n                writeQueryEntity(writer, e);\n        }\n        else\n            writer.writeInt(0);\n\n        NearCacheConfiguration nearCfg = ccfg.getNearConfiguration();\n\n        if (nearCfg != null) {\n            writer.writeBoolean(true);\n\n            writeNearConfiguration(writer, nearCfg);\n        }\n        else\n            writer.writeBoolean(false);\n\n        writeEvictionPolicy(writer, ccfg.getEvictionPolicy());\n        writeAffinityFunction(writer, ccfg.getAffinity());\n        writeExpiryPolicyFactory(writer, ccfg.getExpiryPolicyFactory());\n\n        CachePluginConfiguration[] plugins = ccfg.getPluginConfigurations();\n        if (plugins != null) {\n            int cnt = 0;\n\n            for (CachePluginConfiguration cfg : plugins) {\n                if (cfg instanceof PlatformCachePluginConfiguration)\n                    cnt++;\n            }\n\n            writer.writeInt(cnt);\n\n            for (CachePluginConfiguration cfg : plugins) {\n                if (cfg instanceof PlatformCachePluginConfiguration) {\n                    writer.writeBoolean(false);  // Pure platform plugin.\n                    writer.writeObject(((PlatformCachePluginConfiguration) cfg).nativeCfg());\n                }\n            }\n        }\n    }",
            " 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  \n 881  \n 882  \n 883  \n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932 +\n 933 +\n 934 +\n 935 +\n 936 +\n 937 +\n 938 +\n 939 +\n 940 +\n 941 +\n 942 +\n 943 +\n 944 +\n 945  \n 946  \n 947  \n 948  \n 949  \n 950  \n 951  \n 952  \n 953  \n 954  \n 955  \n 956  \n 957  \n 958  \n 959  \n 960  \n 961  \n 962  \n 963  ",
            "    /**\n     * Writes cache configuration.\n     *\n     * @param writer Writer.\n     * @param ccfg Configuration.\n     */\n    public static void writeCacheConfiguration(BinaryRawWriter writer, CacheConfiguration ccfg) {\n        assert writer != null;\n        assert ccfg != null;\n\n        writeEnumInt(writer, ccfg.getAtomicityMode(), CacheConfiguration.DFLT_CACHE_ATOMICITY_MODE);\n        writer.writeInt(ccfg.getBackups());\n        writeEnumInt(writer, ccfg.getCacheMode(), CacheConfiguration.DFLT_CACHE_MODE);\n        writer.writeBoolean(ccfg.isCopyOnRead());\n        writer.writeBoolean(ccfg.isEagerTtl());\n        writer.writeBoolean(ccfg.isInvalidate());\n        writer.writeBoolean(ccfg.isStoreKeepBinary());\n        writer.writeBoolean(ccfg.isLoadPreviousValue());\n        writer.writeLong(ccfg.getDefaultLockTimeout());\n        //noinspection deprecation\n        writer.writeLong(ccfg.getLongQueryWarningTimeout());\n        writer.writeInt(ccfg.getMaxConcurrentAsyncOperations());\n        writer.writeString(ccfg.getName());\n        writer.writeBoolean(ccfg.isReadFromBackup());\n        writer.writeInt(ccfg.getRebalanceBatchSize());\n        writer.writeLong(ccfg.getRebalanceDelay());\n        writeEnumInt(writer, ccfg.getRebalanceMode(), CacheConfiguration.DFLT_REBALANCE_MODE);\n        writer.writeLong(ccfg.getRebalanceThrottle());\n        writer.writeLong(ccfg.getRebalanceTimeout());\n        writer.writeBoolean(ccfg.isSqlEscapeAll());\n        writer.writeInt(ccfg.getWriteBehindBatchSize());\n        writer.writeBoolean(ccfg.isWriteBehindEnabled());\n        writer.writeLong(ccfg.getWriteBehindFlushFrequency());\n        writer.writeInt(ccfg.getWriteBehindFlushSize());\n        writer.writeInt(ccfg.getWriteBehindFlushThreadCount());\n        writer.writeBoolean(ccfg.getWriteBehindCoalescing());\n        writeEnumInt(writer, ccfg.getWriteSynchronizationMode());\n        writer.writeBoolean(ccfg.isReadThrough());\n        writer.writeBoolean(ccfg.isWriteThrough());\n        writer.writeBoolean(ccfg.isStatisticsEnabled());\n        //noinspection deprecation\n        writer.writeString(ccfg.getMemoryPolicyName());\n        writer.writeInt(ccfg.getPartitionLossPolicy().ordinal());\n        writer.writeString(ccfg.getGroupName());\n\n        if (ccfg.getCacheStoreFactory() instanceof PlatformDotNetCacheStoreFactoryNative)\n            writer.writeObject(((PlatformDotNetCacheStoreFactoryNative)ccfg.getCacheStoreFactory()).getNativeFactory());\n        else\n            writer.writeObject(null);\n\n        writer.writeInt(ccfg.getSqlIndexMaxInlineSize());\n\n        Collection<QueryEntity> qryEntities = ccfg.getQueryEntities();\n\n        if (qryEntities != null) {\n            writer.writeInt(qryEntities.size());\n\n            for (QueryEntity e : qryEntities)\n                writeQueryEntity(writer, e);\n        }\n        else\n            writer.writeInt(0);\n\n        NearCacheConfiguration nearCfg = ccfg.getNearConfiguration();\n\n        if (nearCfg != null) {\n            writer.writeBoolean(true);\n\n            writeNearConfiguration(writer, nearCfg);\n        }\n        else\n            writer.writeBoolean(false);\n\n        writeEvictionPolicy(writer, ccfg.getEvictionPolicy());\n        writeAffinityFunction(writer, ccfg.getAffinity());\n        writeExpiryPolicyFactory(writer, ccfg.getExpiryPolicyFactory());\n\n        CacheKeyConfiguration[] keys = ccfg.getKeyConfiguration();\n\n        if (keys != null) {\n            writer.writeInt(keys.length);\n\n            for (CacheKeyConfiguration key : keys) {\n                writer.writeString(key.getTypeName());\n                writer.writeString(key.getAffinityKeyFieldName());\n            }\n        } else {\n            writer.writeInt(0);\n        }\n\n        CachePluginConfiguration[] plugins = ccfg.getPluginConfigurations();\n        if (plugins != null) {\n            int cnt = 0;\n\n            for (CachePluginConfiguration cfg : plugins) {\n                if (cfg instanceof PlatformCachePluginConfiguration)\n                    cnt++;\n            }\n\n            writer.writeInt(cnt);\n\n            for (CachePluginConfiguration cfg : plugins) {\n                if (cfg instanceof PlatformCachePluginConfiguration) {\n                    writer.writeBoolean(false);  // Pure platform plugin.\n                    writer.writeObject(((PlatformCachePluginConfiguration) cfg).nativeCfg());\n                }\n            }\n        }\n    }"
        ]
    ],
    "325f5a9df67b32a123e9181fd8a1a610c6ef1f78": [
        [
            "OdbcRequestHandler::getColumnsMeta(OdbcQueryGetColumnsMetaRequest)",
            " 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432 -\n 433 -\n 434 -\n 435 -\n 436 -\n 437 -\n 438 -\n 439 -\n 440 -\n 441 -\n 442 -\n 443 -\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450 -\n 451 -\n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  ",
            "    /**\n     * {@link OdbcQueryGetColumnsMetaRequest} command handler.\n     *\n     * @param req Get columns metadata request.\n     * @return Response.\n     */\n    private ClientListenerResponse getColumnsMeta(OdbcQueryGetColumnsMetaRequest req) {\n        try {\n            List<OdbcColumnMeta> meta = new ArrayList<>();\n\n            String schemaPattern;\n            String tablePattern;\n\n            if (req.tablePattern().contains(\".\")) {\n                // Parsing two-part table name.\n                String[] parts = req.tablePattern().split(\"\\\\.\");\n\n                schemaPattern = OdbcUtils.removeQuotationMarksIfNeeded(parts[0]);\n\n                tablePattern = parts[1];\n            }\n            else {\n                schemaPattern = OdbcUtils.removeQuotationMarksIfNeeded(req.schemaPattern());\n\n                tablePattern = req.tablePattern();\n            }\n\n            GridQueryIndexing indexing = ctx.query().getIndexing();\n\n            for (String cacheName : ctx.cache().cacheNames()) {\n                String cacheSchema = indexing.schema(cacheName);\n\n                if (!matches(cacheSchema, schemaPattern))\n                    continue;\n\n                Collection<GridQueryTypeDescriptor> tablesMeta = ctx.query().types(cacheName);\n\n                for (GridQueryTypeDescriptor table : tablesMeta) {\n                    if (!matches(table.name(), tablePattern))\n                        continue;\n\n                    for (Map.Entry<String, Class<?>> field : table.fields().entrySet()) {\n                        if (!matches(field.getKey(), req.columnPattern()))\n                            continue;\n\n                    OdbcColumnMeta columnMeta = new OdbcColumnMeta(cacheSchema, table.name(),\n                        field.getKey(), field.getValue());\n\n                        if (!meta.contains(columnMeta))\n                            meta.add(columnMeta);\n                    }\n                }\n            }\n\n            OdbcQueryGetColumnsMetaResult res = new OdbcQueryGetColumnsMetaResult(meta);\n\n            return new OdbcResponse(res);\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to get columns metadata [reqId=\" + req.requestId() + \", req=\" + req + ']', e);\n\n            return exceptionToResult(e);\n        }\n    }",
            " 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432 +\n 433 +\n 434 +\n 435 +\n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442 +\n 443 +\n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  ",
            "    /**\n     * {@link OdbcQueryGetColumnsMetaRequest} command handler.\n     *\n     * @param req Get columns metadata request.\n     * @return Response.\n     */\n    private ClientListenerResponse getColumnsMeta(OdbcQueryGetColumnsMetaRequest req) {\n        try {\n            List<OdbcColumnMeta> meta = new ArrayList<>();\n\n            String schemaPattern;\n            String tablePattern;\n\n            if (req.tablePattern().contains(\".\")) {\n                // Parsing two-part table name.\n                String[] parts = req.tablePattern().split(\"\\\\.\");\n\n                schemaPattern = OdbcUtils.removeQuotationMarksIfNeeded(parts[0]);\n\n                tablePattern = parts[1];\n            }\n            else {\n                schemaPattern = OdbcUtils.removeQuotationMarksIfNeeded(req.schemaPattern());\n\n                tablePattern = req.tablePattern();\n            }\n\n            for (String cacheName : ctx.cache().publicCacheNames()) {\n                for (GridQueryTypeDescriptor table : ctx.query().types(cacheName)) {\n                    if (!matches(table.schemaName(), schemaPattern) ||\n                        !matches(table.tableName(), tablePattern))\n                        continue;\n\n                    for (Map.Entry<String, Class<?>> field : table.fields().entrySet()) {\n                        if (!matches(field.getKey(), req.columnPattern()))\n                            continue;\n\n                        OdbcColumnMeta columnMeta = new OdbcColumnMeta(table.schemaName(), table.tableName(),\n                            field.getKey(), field.getValue());\n\n                        if (!meta.contains(columnMeta))\n                            meta.add(columnMeta);\n                    }\n                }\n            }\n\n            OdbcQueryGetColumnsMetaResult res = new OdbcQueryGetColumnsMetaResult(meta);\n\n            return new OdbcResponse(res);\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to get columns metadata [reqId=\" + req.requestId() + \", req=\" + req + ']', e);\n\n            return exceptionToResult(e);\n        }\n    }"
        ],
        [
            "OdbcRequestHandler::getTablesMeta(OdbcQueryGetTablesMetaRequest)",
            " 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482 -\n 483 -\n 484 -\n 485 -\n 486 -\n 487 -\n 488 -\n 489 -\n 490 -\n 491 -\n 492 -\n 493 -\n 494 -\n 495 -\n 496 -\n 497 -\n 498  \n 499  \n 500 -\n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  ",
            "    /**\n     * {@link OdbcQueryGetTablesMetaRequest} command handler.\n     *\n     * @param req Get tables metadata request.\n     * @return Response.\n     */\n    private ClientListenerResponse getTablesMeta(OdbcQueryGetTablesMetaRequest req) {\n        try {\n            List<OdbcTableMeta> meta = new ArrayList<>();\n\n            String schemaPattern = OdbcUtils.removeQuotationMarksIfNeeded(req.schema());\n\n            GridQueryIndexing indexing = ctx.query().getIndexing();\n\n            for (String cacheName : ctx.cache().cacheNames())\n            {\n                String cacheSchema = indexing.schema(cacheName);\n\n                if (!matches(cacheSchema, schemaPattern))\n                    continue;\n\n                Collection<GridQueryTypeDescriptor> tablesMeta = ctx.query().types(cacheName);\n\n                for (GridQueryTypeDescriptor table : tablesMeta) {\n                    if (!matches(table.name(), req.table()))\n                        continue;\n\n                    if (!matches(\"TABLE\", req.tableType()))\n                        continue;\n\n                    OdbcTableMeta tableMeta = new OdbcTableMeta(null, cacheName, table.name(), \"TABLE\");\n\n                    if (!meta.contains(tableMeta))\n                        meta.add(tableMeta);\n                }\n            }\n\n            OdbcQueryGetTablesMetaResult res = new OdbcQueryGetTablesMetaResult(meta);\n\n            return new OdbcResponse(res);\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to get tables metadata [reqId=\" + req.requestId() + \", req=\" + req + ']', e);\n\n            return exceptionToResult(e);\n        }\n    }",
            " 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474 +\n 475 +\n 476 +\n 477 +\n 478 +\n 479  \n 480  \n 481 +\n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  ",
            "    /**\n     * {@link OdbcQueryGetTablesMetaRequest} command handler.\n     *\n     * @param req Get tables metadata request.\n     * @return Response.\n     */\n    private ClientListenerResponse getTablesMeta(OdbcQueryGetTablesMetaRequest req) {\n        try {\n            List<OdbcTableMeta> meta = new ArrayList<>();\n\n            String schemaPattern = OdbcUtils.removeQuotationMarksIfNeeded(req.schema());\n\n            for (String cacheName : ctx.cache().publicCacheNames()) {\n                for (GridQueryTypeDescriptor table : ctx.query().types(cacheName)) {\n                    if (!matches(table.schemaName(), schemaPattern) ||\n                        !matches(table.tableName(), req.table()) ||\n                        !matches(\"TABLE\", req.tableType()))\n                        continue;\n\n                    OdbcTableMeta tableMeta = new OdbcTableMeta(null, table.schemaName(), table.tableName(), \"TABLE\");\n\n                    if (!meta.contains(tableMeta))\n                        meta.add(tableMeta);\n                }\n            }\n\n            OdbcQueryGetTablesMetaResult res = new OdbcQueryGetTablesMetaResult(meta);\n\n            return new OdbcResponse(res);\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to get tables metadata [reqId=\" + req.requestId() + \", req=\" + req + ']', e);\n\n            return exceptionToResult(e);\n        }\n    }"
        ]
    ],
    "a760e6e72a716f59a0110954c1efb665215cfcaf": [
        [
            "IgniteWalHistoryReservationsSelfTest::testRemovesArePreloadedIfHistoryIsAvailable()",
            " 195  \n 196  \n 197  \n 198  \n 199 -\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testRemovesArePreloadedIfHistoryIsAvailable() throws Exception {\n        System.setProperty(IGNITE_WAL_REBALANCE_THRESHOLD, \"0\");\n\n        int entryCnt = 10_000;\n\n        Ignite ig0 = startGrids(2);\n\n        IgniteCache<Integer, Integer> cache = ig0.cache(\"cache1\");\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k);\n\n        forceCheckpoint();\n\n        stopGrid(1);\n\n        for (int k = 0; k < entryCnt; k += 2)\n            cache.remove(k);\n\n        forceCheckpoint();\n\n        Ignite ig1 = startGrid(1);\n\n        IgniteCache<Integer, Integer> cache1 = ig1.cache(\"cache1\");\n\n        assertEquals(entryCnt / 2, cache.size());\n        assertEquals(entryCnt / 2, cache1.size());\n\n        for (Integer k = 0; k < entryCnt; k++) {\n            if (k % 2 == 0) {\n                assertTrue(\"k=\" + k, !cache.containsKey(k));\n                assertTrue(\"k=\" + k, !cache1.containsKey(k));\n            }\n            else {\n                assertEquals(\"k=\" + k, k, cache.get(k));\n                assertEquals(\"k=\" + k, k, cache1.get(k));\n            }\n        }\n    }",
            " 195  \n 196  \n 197  \n 198  \n 199 +\n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testRemovesArePreloadedIfHistoryIsAvailable() throws Exception {\n        System.setProperty(IGNITE_PDS_WAL_REBALANCE_THRESHOLD, \"0\");\n\n        int entryCnt = 10_000;\n\n        Ignite ig0 = startGrids(2);\n\n        IgniteCache<Integer, Integer> cache = ig0.cache(\"cache1\");\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k);\n\n        forceCheckpoint();\n\n        stopGrid(1);\n\n        for (int k = 0; k < entryCnt; k += 2)\n            cache.remove(k);\n\n        forceCheckpoint();\n\n        Ignite ig1 = startGrid(1);\n\n        IgniteCache<Integer, Integer> cache1 = ig1.cache(\"cache1\");\n\n        assertEquals(entryCnt / 2, cache.size());\n        assertEquals(entryCnt / 2, cache1.size());\n\n        for (Integer k = 0; k < entryCnt; k++) {\n            if (k % 2 == 0) {\n                assertTrue(\"k=\" + k, !cache.containsKey(k));\n                assertTrue(\"k=\" + k, !cache1.containsKey(k));\n            }\n            else {\n                assertEquals(\"k=\" + k, k, cache.get(k));\n                assertEquals(\"k=\" + k, k, cache1.get(k));\n            }\n        }\n    }"
        ],
        [
            "IgniteWalHistoryReservationsSelfTest::testReservedOnExchange()",
            "  99  \n 100  \n 101  \n 102  \n 103 -\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testReservedOnExchange() throws Exception {\n        System.setProperty(IGNITE_WAL_REBALANCE_THRESHOLD, \"0\");\n\n        final int entryCnt = 10_000;\n        final int initGridCnt = 4;\n\n        final IgniteEx ig0 = (IgniteEx)startGrids(initGridCnt);\n\n        IgniteCache<Object, Object> cache = ig0.cache(\"cache1\");\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k);\n\n        forceCheckpoint();\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k * 2);\n\n        forceCheckpoint();\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k);\n\n        forceCheckpoint();\n\n\n        Lock lock = cache.lock(0);\n\n        lock.lock();\n\n        try {\n            GridTestUtils.runAsync(new Runnable() {\n                @Override public void run() {\n                    try {\n                        startGrid(initGridCnt);\n                    }\n                    catch (Exception e) {\n                        fail(e.getMessage());\n                    }\n                }\n            });\n\n            boolean reserved = GridTestUtils.waitForCondition(new GridAbsPredicate() {\n                @Override public boolean apply() {\n                    for (int g = 0; g < initGridCnt; g++) {\n                        IgniteEx ig = grid(g);\n\n                        FileWriteAheadLogManager wal = (FileWriteAheadLogManager)ig.context().cache().context().wal();\n\n                        Object archiver = GridTestUtils.getFieldValue(wal, \"archiver\");\n\n                        synchronized (archiver) {\n                            Map reserved = GridTestUtils.getFieldValue(archiver, \"reserved\");\n\n                            if (reserved.isEmpty())\n                                return false;\n                        }\n                    }\n\n                    return true;\n                }\n            }, 10_000);\n\n            assert reserved;\n        }\n        finally {\n            lock.unlock();\n        }\n\n        boolean released = GridTestUtils.waitForCondition(new GridAbsPredicate() {\n            @Override public boolean apply() {\n                for (int g = 0; g < initGridCnt; g++) {\n                    IgniteEx ig = grid(g);\n\n                    FileWriteAheadLogManager wal = (FileWriteAheadLogManager)ig.context().cache().context().wal();\n\n                    Object archiver = GridTestUtils.getFieldValue(wal, \"archiver\");\n\n                    synchronized (archiver) {\n                        Map reserved = GridTestUtils.getFieldValue(archiver, \"reserved\");\n\n                        if (!reserved.isEmpty())\n                            return false;\n                    }\n                }\n\n                return true;\n            }\n        }, 10_000);\n\n        assert released;\n    }",
            "  99  \n 100  \n 101  \n 102  \n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testReservedOnExchange() throws Exception {\n        System.setProperty(IGNITE_PDS_WAL_REBALANCE_THRESHOLD, \"0\");\n\n        final int entryCnt = 10_000;\n        final int initGridCnt = 4;\n\n        final IgniteEx ig0 = (IgniteEx)startGrids(initGridCnt);\n\n        IgniteCache<Object, Object> cache = ig0.cache(\"cache1\");\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k);\n\n        forceCheckpoint();\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k * 2);\n\n        forceCheckpoint();\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k);\n\n        forceCheckpoint();\n\n\n        Lock lock = cache.lock(0);\n\n        lock.lock();\n\n        try {\n            GridTestUtils.runAsync(new Runnable() {\n                @Override public void run() {\n                    try {\n                        startGrid(initGridCnt);\n                    }\n                    catch (Exception e) {\n                        fail(e.getMessage());\n                    }\n                }\n            });\n\n            boolean reserved = GridTestUtils.waitForCondition(new GridAbsPredicate() {\n                @Override public boolean apply() {\n                    for (int g = 0; g < initGridCnt; g++) {\n                        IgniteEx ig = grid(g);\n\n                        FileWriteAheadLogManager wal = (FileWriteAheadLogManager)ig.context().cache().context().wal();\n\n                        Object archiver = GridTestUtils.getFieldValue(wal, \"archiver\");\n\n                        synchronized (archiver) {\n                            Map reserved = GridTestUtils.getFieldValue(archiver, \"reserved\");\n\n                            if (reserved.isEmpty())\n                                return false;\n                        }\n                    }\n\n                    return true;\n                }\n            }, 10_000);\n\n            assert reserved;\n        }\n        finally {\n            lock.unlock();\n        }\n\n        boolean released = GridTestUtils.waitForCondition(new GridAbsPredicate() {\n            @Override public boolean apply() {\n                for (int g = 0; g < initGridCnt; g++) {\n                    IgniteEx ig = grid(g);\n\n                    FileWriteAheadLogManager wal = (FileWriteAheadLogManager)ig.context().cache().context().wal();\n\n                    Object archiver = GridTestUtils.getFieldValue(wal, \"archiver\");\n\n                    synchronized (archiver) {\n                        Map reserved = GridTestUtils.getFieldValue(archiver, \"reserved\");\n\n                        if (!reserved.isEmpty())\n                            return false;\n                    }\n                }\n\n                return true;\n            }\n        }, 10_000);\n\n        assert released;\n    }"
        ],
        [
            "IgniteWalHistoryReservationsSelfTest::afterTest()",
            "  88  \n  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "    /** {@inheritDoc} */\n    @Override protected void afterTest() throws Exception {\n        System.clearProperty(IGNITE_WAL_REBALANCE_THRESHOLD);\n\n        client = false;\n\n        stopAllGrids();\n\n        deleteWorkFiles();\n    }",
            "  88  \n  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "    /** {@inheritDoc} */\n    @Override protected void afterTest() throws Exception {\n        System.clearProperty(IGNITE_PDS_WAL_REBALANCE_THRESHOLD);\n\n        client = false;\n\n        stopAllGrids();\n\n        deleteWorkFiles();\n    }"
        ],
        [
            "IgniteWalHistoryReservationsSelfTest::testNodeLeftDuringExchange()",
            " 286  \n 287  \n 288  \n 289  \n 290 -\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testNodeLeftDuringExchange() throws Exception {\n        System.setProperty(IGNITE_WAL_REBALANCE_THRESHOLD, \"0\");\n\n        final int entryCnt = 10_000;\n        final int initGridCnt = 4;\n\n        final IgniteEx ig0 = (IgniteEx)startGrids(initGridCnt);\n\n        IgniteCache<Object, Object> cache = ig0.cache(\"cache1\");\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k);\n\n        forceCheckpoint();\n\n        Lock lock = cache.lock(0);\n\n        lock.lock();\n\n        try {\n            GridTestUtils.runAsync(new Runnable() {\n                @Override public void run() {\n                    try {\n                        startGrid(initGridCnt);\n                    }\n                    catch (Exception e) {\n                        fail(e.getMessage());\n                    }\n                }\n            });\n\n            boolean reserved = GridTestUtils.waitForCondition(new GridAbsPredicate() {\n                @Override public boolean apply() {\n                    for (int g = 0; g < initGridCnt; g++) {\n                        IgniteEx ig = grid(g);\n\n                        FileWriteAheadLogManager wal = (FileWriteAheadLogManager)ig.context().cache().context().wal();\n\n                        Object archiver = GridTestUtils.getFieldValue(wal, \"archiver\");\n\n                        synchronized (archiver) {\n                            Map reserved = GridTestUtils.getFieldValue(archiver, \"reserved\");\n\n                            if (reserved.isEmpty())\n                                return false;\n                        }\n                    }\n\n                    return true;\n                }\n            }, 10_000);\n\n            assert reserved;\n\n            stopGrid(Integer.toString(initGridCnt - 1), true, false);\n        }\n        finally {\n            lock.unlock();\n        }\n\n        boolean released = GridTestUtils.waitForCondition(new GridAbsPredicate() {\n            @Override public boolean apply() {\n                for (int g = 0; g < initGridCnt - 1; g++) {\n                    IgniteEx ig = grid(g);\n\n                    FileWriteAheadLogManager wal = (FileWriteAheadLogManager)ig.context().cache().context().wal();\n\n                    Object archiver = GridTestUtils.getFieldValue(wal, \"archiver\");\n\n                    synchronized (archiver) {\n                        Map reserved = GridTestUtils.getFieldValue(archiver, \"reserved\");\n\n                        if (!reserved.isEmpty())\n                            return false;\n                    }\n                }\n\n                return true;\n            }\n        }, 10_000);\n\n        assert released;\n\n        awaitPartitionMapExchange();\n    }",
            " 286  \n 287  \n 288  \n 289  \n 290 +\n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testNodeLeftDuringExchange() throws Exception {\n        System.setProperty(IGNITE_PDS_WAL_REBALANCE_THRESHOLD, \"0\");\n\n        final int entryCnt = 10_000;\n        final int initGridCnt = 4;\n\n        final IgniteEx ig0 = (IgniteEx)startGrids(initGridCnt);\n\n        IgniteCache<Object, Object> cache = ig0.cache(\"cache1\");\n\n        for (int k = 0; k < entryCnt; k++)\n            cache.put(k, k);\n\n        forceCheckpoint();\n\n        Lock lock = cache.lock(0);\n\n        lock.lock();\n\n        try {\n            GridTestUtils.runAsync(new Runnable() {\n                @Override public void run() {\n                    try {\n                        startGrid(initGridCnt);\n                    }\n                    catch (Exception e) {\n                        fail(e.getMessage());\n                    }\n                }\n            });\n\n            boolean reserved = GridTestUtils.waitForCondition(new GridAbsPredicate() {\n                @Override public boolean apply() {\n                    for (int g = 0; g < initGridCnt; g++) {\n                        IgniteEx ig = grid(g);\n\n                        FileWriteAheadLogManager wal = (FileWriteAheadLogManager)ig.context().cache().context().wal();\n\n                        Object archiver = GridTestUtils.getFieldValue(wal, \"archiver\");\n\n                        synchronized (archiver) {\n                            Map reserved = GridTestUtils.getFieldValue(archiver, \"reserved\");\n\n                            if (reserved.isEmpty())\n                                return false;\n                        }\n                    }\n\n                    return true;\n                }\n            }, 10_000);\n\n            assert reserved;\n\n            stopGrid(Integer.toString(initGridCnt - 1), true, false);\n        }\n        finally {\n            lock.unlock();\n        }\n\n        boolean released = GridTestUtils.waitForCondition(new GridAbsPredicate() {\n            @Override public boolean apply() {\n                for (int g = 0; g < initGridCnt - 1; g++) {\n                    IgniteEx ig = grid(g);\n\n                    FileWriteAheadLogManager wal = (FileWriteAheadLogManager)ig.context().cache().context().wal();\n\n                    Object archiver = GridTestUtils.getFieldValue(wal, \"archiver\");\n\n                    synchronized (archiver) {\n                        Map reserved = GridTestUtils.getFieldValue(archiver, \"reserved\");\n\n                        if (!reserved.isEmpty())\n                            return false;\n                    }\n                }\n\n                return true;\n            }\n        }, 10_000);\n\n        assert released;\n\n        awaitPartitionMapExchange();\n    }"
        ]
    ],
    "bf55814d2cd689ab6165c65e5a41e123234d4d54": [
        [
            "DemoComputeTask::DemoComputeJob::execute()",
            "  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93 -\n  94 -\n  95  \n  96  \n  97 -\n  98  ",
            "        /** {@inheritDoc} */\n        @Override public Object execute() throws IgniteException {\n            try {\n                Thread.sleep(rnd.nextInt(50));\n            }\n            catch (InterruptedException e) {\n                // Restore interrupt status\n                Thread.currentThread().interrupt();\n\n                throw new IgniteInterruptedException(e);\n            }\n\n            return rnd.nextInt(10000);\n        }",
            "  85  \n  86  \n  87  \n  88  \n  89 +\n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97 +\n  98  ",
            "        /** {@inheritDoc} */\n        @Override public Object execute() throws IgniteException {\n            try {\n                Thread.sleep(rnd.nextInt(50));\n\n                return rnd.nextInt(10000);\n            }\n            catch (InterruptedException e) {\n                // Restore interrupt status\n                Thread.currentThread().interrupt();\n            }\n\n            return null;\n        }"
        ]
    ],
    "2410f0792fec33725f1b7f74b5b576b353b8fe55": [
        [
            "VisorQueryTask::VisorQueryJob::run(VisorQueryTaskArg)",
            "  70  \n  71  \n  72  \n  73 -\n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86 -\n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  ",
            "        /** {@inheritDoc} */\n        @Override protected VisorEither<VisorQueryResult> run(final VisorQueryTaskArg arg) {\n            try {\n                IgniteCache<Object, Object> c = ignite.cache(arg.getCacheName());\n                UUID nid = ignite.localNode().id();\n\n                SqlFieldsQuery qry = new SqlFieldsQuery(arg.getQueryText());\n                qry.setPageSize(arg.getPageSize());\n                qry.setLocal(arg.isLocal());\n                qry.setDistributedJoins(arg.isDistributedJoins());\n                qry.setEnforceJoinOrder(arg.isEnforceJoinOrder());\n                qry.setReplicatedOnly(arg.isReplicatedOnly());\n                qry.setLazy(arg.getLazy());\n\n                long start = U.currentTimeMillis();\n\n                VisorQueryCursor<List<?>> cur = new VisorQueryCursor<>(c.withKeepBinary().query(qry));\n\n                Collection<GridQueryFieldMetadata> meta = cur.fieldsMeta();\n\n                if (meta == null)\n                    return new VisorEither<>(\n                        new VisorExceptionWrapper(new SQLException(\"Fail to execute query. No metadata available.\")));\n                else {\n                    List<VisorQueryField> names = new ArrayList<>(meta.size());\n\n                    for (GridQueryFieldMetadata col : meta)\n                        names.add(new VisorQueryField(col.schemaName(), col.typeName(),\n                            col.fieldName(), col.fieldTypeName()));\n\n                    List<Object[]> rows = fetchSqlQueryRows(cur, arg.getPageSize());\n\n                    // Query duration + fetch duration.\n                    long duration = U.currentTimeMillis() - start;\n\n                    boolean hasNext = cur.hasNext();\n\n                    // Generate query ID to store query cursor in node local storage.\n                    String qryId = SQL_QRY_NAME + \"-\" + UUID.randomUUID();\n\n                    if (hasNext) {\n                        ignite.cluster().<String, VisorQueryCursor<List<?>>>nodeLocalMap().put(qryId, cur);\n\n                        scheduleResultSetHolderRemoval(qryId, ignite);\n                    }\n                    else\n                        cur.close();\n\n                    return new VisorEither<>(new VisorQueryResult(nid, qryId, names, rows, hasNext, duration));\n                }\n            }\n            catch (Throwable e) {\n                return new VisorEither<>(new VisorExceptionWrapper(e));\n            }\n        }",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90 +\n  91 +\n  92 +\n  93 +\n  94 +\n  95 +\n  96 +\n  97 +\n  98 +\n  99 +\n 100 +\n 101 +\n 102 +\n 103 +\n 104 +\n 105 +\n 106 +\n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118 +\n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  ",
            "        /** {@inheritDoc} */\n        @Override protected VisorEither<VisorQueryResult> run(final VisorQueryTaskArg arg) {\n            try {\n                UUID nid = ignite.localNode().id();\n\n                SqlFieldsQuery qry = new SqlFieldsQuery(arg.getQueryText());\n                qry.setPageSize(arg.getPageSize());\n                qry.setLocal(arg.isLocal());\n                qry.setDistributedJoins(arg.isDistributedJoins());\n                qry.setEnforceJoinOrder(arg.isEnforceJoinOrder());\n                qry.setReplicatedOnly(arg.isReplicatedOnly());\n                qry.setLazy(arg.getLazy());\n\n                long start = U.currentTimeMillis();\n\n                FieldsQueryCursor<List<?>> qryCursor;\n\n                String cacheName = arg.getCacheName();\n\n                if (F.isEmpty(cacheName))\n                    qryCursor = ignite.context().query().querySqlFieldsNoCache(qry, true);\n                else {\n                    IgniteCache<Object, Object> c = ignite.cache(cacheName);\n\n                    if (c == null)\n                        throw new SQLException(\"Fail to execute query. Cache not found: \" + cacheName);\n\n                    try {\n                        qryCursor = c.withKeepBinary().query(qry);\n                    }\n                    catch (CacheException e) {\n                        // Work around for DDL without explicit schema name.\n                        if (X.hasCause(e, IgniteSQLException.class)\n                            && e.getMessage().contains(\"can only be executed on PUBLIC schema\")) {\n                            qry.setSchema(\"PUBLIC\");\n\n                            qryCursor = c.withKeepBinary().query(qry);\n                        }\n                        else\n                            throw e;\n                    }\n                }\n\n                VisorQueryCursor<List<?>> cur = new VisorQueryCursor<>(qryCursor);\n\n                Collection<GridQueryFieldMetadata> meta = cur.fieldsMeta();\n\n                if (meta == null)\n                    return new VisorEither<>(\n                        new VisorExceptionWrapper(new SQLException(\"Fail to execute query. No metadata available.\")));\n                else {\n                    List<VisorQueryField> names = new ArrayList<>(meta.size());\n\n                    for (GridQueryFieldMetadata col : meta)\n                        names.add(new VisorQueryField(col.schemaName(), col.typeName(),\n                            col.fieldName(), col.fieldTypeName()));\n\n                    List<Object[]> rows = fetchSqlQueryRows(cur, arg.getPageSize());\n\n                    // Query duration + fetch duration.\n                    long duration = U.currentTimeMillis() - start;\n\n                    boolean hasNext = cur.hasNext();\n\n                    // Generate query ID to store query cursor in node local storage.\n                    String qryId = SQL_QRY_NAME + \"-\" + UUID.randomUUID();\n\n                    if (hasNext) {\n                        ignite.cluster().<String, VisorQueryCursor<List<?>>>nodeLocalMap().put(qryId, cur);\n\n                        scheduleResultSetHolderRemoval(qryId, ignite);\n                    }\n                    else\n                        cur.close();\n\n                    return new VisorEither<>(new VisorQueryResult(nid, qryId, names, rows, hasNext, duration));\n                }\n            }\n            catch (Throwable e) {\n                return new VisorEither<>(new VisorExceptionWrapper(e));\n            }\n        }"
        ],
        [
            "GridCacheCommandHandler::MetadataJob::execute()",
            "1078  \n1079  \n1080  \n1081 -\n1082  \n1083  \n1084  \n1085  \n1086  \n1087  \n1088  \n1089  \n1090  \n1091 -\n1092  \n1093  \n1094  \n1095  \n1096  \n1097  \n1098  \n1099  \n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  ",
            "        /** {@inheritDoc} */\n        @Override public Collection<GridCacheSqlMetadata> execute() {\n            String cacheName = null;\n            IgniteInternalCache<?, ?> cache;\n\n            if (!F.isEmpty(arguments())) {\n                cacheName = argument(0);\n\n                cache = ignite.context().cache().publicCache(cacheName);\n\n                assert cache != null;\n            }\n            else {\n                cache = F.first(ignite.context().cache().publicCaches()).internalProxy();\n\n                if (cache == null)\n                    return Collections.emptyList();\n            }\n\n            try {\n                Collection<GridCacheSqlMetadata> metas = cache.context().queries().sqlMetadata();\n\n                if (cacheName != null) {\n                    for (GridCacheSqlMetadata meta : metas)\n                        if (meta.cacheName().equals(cacheName))\n                            return Collections.singleton(meta);\n\n                    throw new IgniteException(\"No meta data for \" + cacheName + \" can be found\");\n                }\n\n                return metas;\n            }\n            catch (IgniteCheckedException e) {\n                throw U.convertException(e);\n            }\n        }",
            "1079  \n1080  \n1081  \n1082 +\n1083 +\n1084 +\n1085 +\n1086 +\n1087  \n1088  \n1089  \n1090  \n1091  \n1092  \n1093  \n1094  \n1095  \n1096 +\n1097 +\n1098 +\n1099 +\n1100  \n1101  \n1102  \n1103  \n1104  \n1105  \n1106  \n1107  \n1108  \n1109  \n1110  \n1111  \n1112  \n1113  \n1114  \n1115  \n1116  \n1117  \n1118  \n1119  \n1120  \n1121  ",
            "        /** {@inheritDoc} */\n        @Override public Collection<GridCacheSqlMetadata> execute() {\n            String cacheName = null;\n\n            if (!ignite.active())\n                return Collections.emptyList();\n\n            IgniteInternalCache<?, ?> cache = null;\n\n            if (!F.isEmpty(arguments())) {\n                cacheName = argument(0);\n\n                cache = ignite.context().cache().publicCache(cacheName);\n\n                assert cache != null;\n            }\n            else {\n                IgniteCacheProxy<?, ?> pubCache = F.first(ignite.context().cache().publicCaches());\n\n                if (pubCache != null)\n                    cache = pubCache.internalProxy();\n\n                if (cache == null)\n                    return Collections.emptyList();\n            }\n\n            try {\n                Collection<GridCacheSqlMetadata> metas = cache.context().queries().sqlMetadata();\n\n                if (cacheName != null) {\n                    for (GridCacheSqlMetadata meta : metas)\n                        if (meta.cacheName().equals(cacheName))\n                            return Collections.singleton(meta);\n\n                    throw new IgniteException(\"No meta data for \" + cacheName + \" can be found\");\n                }\n\n                return metas;\n            }\n            catch (IgniteCheckedException e) {\n                throw U.convertException(e);\n            }\n        }"
        ]
    ],
    "bcd388157c76ec684f043fa140b24dfd3c11f5cb": [
        [
            "BinaryMetadataTransport::MetadataUpdateAcceptedListener::onCustomEvent(AffinityTopologyVersion,ClusterNode,MetadataUpdateAcceptedMessage)",
            " 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471 -\n 472  \n 473  \n 474  \n 475  ",
            "        /** {@inheritDoc} */\n        @Override public void onCustomEvent(AffinityTopologyVersion topVer, ClusterNode snd, MetadataUpdateAcceptedMessage msg) {\n            if (msg.duplicated())\n                return;\n\n            int typeId = msg.typeId();\n\n            BinaryMetadataHolder holder = metaLocCache.get(typeId);\n\n            assert holder != null : \"No metadata found for typeId \" + typeId;\n\n            int newAcceptedVer = msg.acceptedVersion();\n\n            if (clientNode) {\n                BinaryMetadataHolder newHolder = new BinaryMetadataHolder(holder.metadata(),\n                        holder.pendingVersion(), newAcceptedVer);\n\n                do {\n                    holder = metaLocCache.get(typeId);\n\n                    int oldAcceptedVer = holder.acceptedVersion();\n\n                    if (oldAcceptedVer > newAcceptedVer)\n                        break;\n                }\n                while (!metaLocCache.replace(typeId, holder, newHolder));\n            }\n            else {\n                int oldAcceptedVer = holder.acceptedVersion();\n\n                if (oldAcceptedVer >= newAcceptedVer) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Marking ack as duplicate [holder=\" + holder +\n                            \", newAcceptedVer: \" + newAcceptedVer + ']');\n\n                    //this is duplicate ack\n                    msg.duplicated(true);\n\n                    return;\n                }\n\n                metadataFileStore.writeMetadata(holder.metadata());\n\n                metaLocCache.put(typeId, new BinaryMetadataHolder(holder.metadata(), holder.pendingVersion(), newAcceptedVer));\n            }\n\n            for (BinaryMetadataUpdatedListener lsnr : binaryUpdatedLsnrs)\n                lsnr.binaryMetadataUpdated(holder.metadata());\n\n            GridFutureAdapter<MetadataUpdateResult> fut = syncMap.get(new SyncKey(typeId, newAcceptedVer));\n\n            if (log.isDebugEnabled())\n                log.debug(\"Completing future for \" + metaLocCache.get(typeId));\n\n            if (fut != null)\n                fut.onDone(MetadataUpdateResult.createSuccessfulResult());\n        }",
            " 420  \n 421  \n 422 +\n 423 +\n 424 +\n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475 +\n 476  \n 477  \n 478  \n 479  ",
            "        /** {@inheritDoc} */\n        @Override public void onCustomEvent(AffinityTopologyVersion topVer, ClusterNode snd, MetadataUpdateAcceptedMessage msg) {\n            if (log.isDebugEnabled())\n                log.debug(\"Received MetadataUpdateAcceptedMessage \" + msg);\n\n            if (msg.duplicated())\n                return;\n\n            int typeId = msg.typeId();\n\n            BinaryMetadataHolder holder = metaLocCache.get(typeId);\n\n            assert holder != null : \"No metadata found for typeId \" + typeId;\n\n            int newAcceptedVer = msg.acceptedVersion();\n\n            if (clientNode) {\n                BinaryMetadataHolder newHolder = new BinaryMetadataHolder(holder.metadata(),\n                        holder.pendingVersion(), newAcceptedVer);\n\n                do {\n                    holder = metaLocCache.get(typeId);\n\n                    int oldAcceptedVer = holder.acceptedVersion();\n\n                    if (oldAcceptedVer > newAcceptedVer)\n                        break;\n                }\n                while (!metaLocCache.replace(typeId, holder, newHolder));\n            }\n            else {\n                int oldAcceptedVer = holder.acceptedVersion();\n\n                if (oldAcceptedVer >= newAcceptedVer) {\n                    if (log.isDebugEnabled())\n                        log.debug(\"Marking ack as duplicate [holder=\" + holder +\n                            \", newAcceptedVer: \" + newAcceptedVer + ']');\n\n                    //this is duplicate ack\n                    msg.duplicated(true);\n\n                    return;\n                }\n\n                metadataFileStore.writeMetadata(holder.metadata());\n\n                metaLocCache.put(typeId, new BinaryMetadataHolder(holder.metadata(), holder.pendingVersion(), newAcceptedVer));\n            }\n\n            for (BinaryMetadataUpdatedListener lsnr : binaryUpdatedLsnrs)\n                lsnr.binaryMetadataUpdated(holder.metadata());\n\n            GridFutureAdapter<MetadataUpdateResult> fut = syncMap.get(new SyncKey(typeId, newAcceptedVer));\n\n            if (log.isDebugEnabled())\n                log.debug(\"Completing future \" + fut + \" for \" + metaLocCache.get(typeId));\n\n            if (fut != null)\n                fut.onDone(MetadataUpdateResult.createSuccessfulResult());\n        }"
        ],
        [
            "CacheObjectBinaryProcessorImpl::addMeta(int,BinaryType)",
            " 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  ",
            "    /** {@inheritDoc} */\n    @Override public void addMeta(final int typeId, final BinaryType newMeta) throws BinaryObjectException {\n        assert newMeta != null;\n        assert newMeta instanceof BinaryTypeImpl;\n\n        BinaryMetadata newMeta0 = ((BinaryTypeImpl)newMeta).metadata();\n\n        try {\n            BinaryMetadataHolder metaHolder = metadataLocCache.get(typeId);\n\n            BinaryMetadata oldMeta = metaHolder != null ? metaHolder.metadata() : null;\n\n            BinaryMetadata mergedMeta = BinaryUtils.mergeMetadata(oldMeta, newMeta0);\n\n            MetadataUpdateResult res = transport.requestMetadataUpdate(mergedMeta).get();\n\n            assert res != null;\n\n            if (res.rejected())\n                throw res.error();\n        }\n        catch (IgniteCheckedException e) {\n            throw new BinaryObjectException(\"Failed to update meta data for type: \" + newMeta.typeName(), e);\n        }\n    }",
            " 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445 +\n 446 +\n 447 +\n 448 +\n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  ",
            "    /** {@inheritDoc} */\n    @Override public void addMeta(final int typeId, final BinaryType newMeta) throws BinaryObjectException {\n        assert newMeta != null;\n        assert newMeta instanceof BinaryTypeImpl;\n\n        BinaryMetadata newMeta0 = ((BinaryTypeImpl)newMeta).metadata();\n\n        try {\n            BinaryMetadataHolder metaHolder = metadataLocCache.get(typeId);\n\n            BinaryMetadata oldMeta = metaHolder != null ? metaHolder.metadata() : null;\n\n            BinaryMetadata mergedMeta = BinaryUtils.mergeMetadata(oldMeta, newMeta0);\n\n            //metadata requested to be added is exactly the same as already presented in the cache\n            if (mergedMeta == oldMeta)\n                return;\n\n            MetadataUpdateResult res = transport.requestMetadataUpdate(mergedMeta).get();\n\n            assert res != null;\n\n            if (res.rejected())\n                throw res.error();\n        }\n        catch (IgniteCheckedException e) {\n            throw new BinaryObjectException(\"Failed to update meta data for type: \" + newMeta.typeName(), e);\n        }\n    }"
        ],
        [
            "BinaryMetadataTransport::requestMetadataUpdate(BinaryMetadata)",
            " 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  ",
            "    /**\n     * Sends request to cluster proposing update for given metadata.\n     *\n     * @param metadata Metadata proposed for update.\n     * @return Future to wait for update result on.\n     */\n    GridFutureAdapter<MetadataUpdateResult> requestMetadataUpdate(BinaryMetadata metadata) throws IgniteCheckedException {\n        MetadataUpdateResultFuture resFut = new MetadataUpdateResultFuture();\n\n        if (log.isDebugEnabled())\n            log.debug(\"Requesting metadata update for \" + metadata.typeId());\n\n        synchronized (this) {\n            unlabeledFutures.add(resFut);\n\n            if (!stopping)\n                discoMgr.sendCustomEvent(new MetadataUpdateProposedMessage(metadata, ctx.localNodeId()));\n            else\n                resFut.onDone(MetadataUpdateResult.createUpdateDisabledResult());\n        }\n\n        return resFut;\n    }",
            " 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163 +\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  ",
            "    /**\n     * Sends request to cluster proposing update for given metadata.\n     *\n     * @param metadata Metadata proposed for update.\n     * @return Future to wait for update result on.\n     */\n    GridFutureAdapter<MetadataUpdateResult> requestMetadataUpdate(BinaryMetadata metadata) throws IgniteCheckedException {\n        MetadataUpdateResultFuture resFut = new MetadataUpdateResultFuture();\n\n        if (log.isDebugEnabled())\n            log.debug(\"Requesting metadata update for \" + metadata.typeId() + \"; caller thread is blocked on future \"\n                + resFut);\n\n        synchronized (this) {\n            unlabeledFutures.add(resFut);\n\n            if (!stopping)\n                discoMgr.sendCustomEvent(new MetadataUpdateProposedMessage(metadata, ctx.localNodeId()));\n            else\n                resFut.onDone(MetadataUpdateResult.createUpdateDisabledResult());\n        }\n\n        return resFut;\n    }"
        ]
    ],
    "eeebfca0bddbf31b10a86a6725e2c27933fdb0ae": [
        [
            "GridToStringBuilderSelfTest::testToStringCollectionLimits()",
            " 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testToStringCollectionLimits() throws Exception {\n        int limit = IgniteSystemProperties.getInteger(IGNITE_TO_STRING_COLLECTION_LIMIT, 100);\n\n        Object vals[] = new Object[] {Byte.MIN_VALUE, Boolean.TRUE, Short.MIN_VALUE, Integer.MIN_VALUE, Long.MIN_VALUE,\n            Float.MIN_VALUE, Double.MIN_VALUE, Character.MIN_VALUE, new TestClass1()};\n        for (Object val : vals)\n            testArr(val, limit);\n\n        Map<String, String> strMap = new TreeMap<>();\n        List<String> strList = new ArrayList<>(limit+1);\n\n        TestClass1 testClass = new TestClass1();\n        testClass.strMap = strMap;\n        testClass.strListIncl = strList;\n\n        for (int i = 0; i < limit; i++) {\n            strMap.put(\"k\" + i, \"v\");\n            strList.add(\"e\");\n        }\n        String testClassStr = GridToStringBuilder.toString(TestClass1.class, testClass);\n\n        strMap.put(\"kz\", \"v\"); // important to add last element in TreeMap here\n        strList.add(\"e\");\n\n        String testClassStrOf = GridToStringBuilder.toString(TestClass1.class, testClass);\n\n        String testClassStrOfR = testClassStrOf.replaceAll(\"... and 1 more\",\"\");\n\n        assertTrue(\"Collection limit error in Map or List, normal: <\" + testClassStr + \">, overflowed: <\"\n            +\"testClassStrOf\", testClassStr.length() == testClassStrOfR.length());\n\n    }",
            " 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172 +\n 173 +\n 174 +\n 175 +\n 176 +\n 177 +\n 178 +\n 179 +\n 180 +\n 181 +\n 182 +\n 183 +\n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193 +\n 194 +\n 195 +\n 196 +\n 197 +\n 198 +\n 199 +\n 200 +\n 201 +\n 202 +\n 203 +\n 204 +\n 205 +\n 206 +\n 207 +\n 208 +\n 209 +\n 210 +\n 211 +\n 212 +\n 213 +\n 214 +\n 215 +\n 216 +\n 217 +\n 218 +\n 219 +\n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testToStringCollectionLimits() throws Exception {\n        int limit = IgniteSystemProperties.getInteger(IGNITE_TO_STRING_COLLECTION_LIMIT, 100);\n\n        Object vals[] = new Object[] {Byte.MIN_VALUE, Boolean.TRUE, Short.MIN_VALUE, Integer.MIN_VALUE, Long.MIN_VALUE,\n            Float.MIN_VALUE, Double.MIN_VALUE, Character.MIN_VALUE, new TestClass1()};\n        for (Object val : vals)\n            testArr(val, limit);\n\n        byte[] byteArr = new byte[1];\n        byteArr[0] = 1;\n        assertEquals(Arrays.toString(byteArr), GridToStringBuilder.arrayToString(byteArr.getClass(), byteArr));\n        byteArr = Arrays.copyOf(byteArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(byteArr.getClass(), byteArr).contains(\"... and 1 more\"));\n\n        boolean[] boolArr = new boolean[1];\n        boolArr[0] = true;\n        assertEquals(Arrays.toString(boolArr), GridToStringBuilder.arrayToString(boolArr.getClass(), boolArr));\n        boolArr = Arrays.copyOf(boolArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(boolArr.getClass(), boolArr).contains(\"... and 1 more\"));\n\n        short[] shortArr = new short[1];\n        shortArr[0] = 100;\n        assertEquals(Arrays.toString(shortArr), GridToStringBuilder.arrayToString(shortArr.getClass(), shortArr));\n        shortArr = Arrays.copyOf(shortArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(shortArr.getClass(), shortArr).contains(\"... and 1 more\"));\n\n        int[] intArr = new int[1];\n        intArr[0] = 10000;\n        assertEquals(Arrays.toString(intArr), GridToStringBuilder.arrayToString(intArr.getClass(), intArr));\n        intArr = Arrays.copyOf(intArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(intArr.getClass(), intArr).contains(\"... and 1 more\"));\n\n        long[] longArr = new long[1];\n        longArr[0] = 10000000;\n        assertEquals(Arrays.toString(longArr), GridToStringBuilder.arrayToString(longArr.getClass(), longArr));\n        longArr = Arrays.copyOf(longArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(longArr.getClass(), longArr).contains(\"... and 1 more\"));\n\n        float[] floatArr = new float[1];\n        floatArr[0] = 1.f;\n        assertEquals(Arrays.toString(floatArr), GridToStringBuilder.arrayToString(floatArr.getClass(), floatArr));\n        floatArr = Arrays.copyOf(floatArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(floatArr.getClass(), floatArr).contains(\"... and 1 more\"));\n\n        double[] doubleArr = new double[1];\n        doubleArr[0] = 1.;\n        assertEquals(Arrays.toString(doubleArr), GridToStringBuilder.arrayToString(doubleArr.getClass(), doubleArr));\n        doubleArr = Arrays.copyOf(doubleArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(doubleArr.getClass(), doubleArr).contains(\"... and 1 more\"));\n\n        char[] charArr = new char[1];\n        charArr[0] = 'a';\n        assertEquals(Arrays.toString(charArr), GridToStringBuilder.arrayToString(charArr.getClass(), charArr));\n        charArr = Arrays.copyOf(charArr, 101);\n        assertTrue(\"Can't find \\\"... and 1 more\\\" in overflowed array string!\",\n            GridToStringBuilder.arrayToString(charArr.getClass(), charArr).contains(\"... and 1 more\"));\n\n        Map<String, String> strMap = new TreeMap<>();\n        List<String> strList = new ArrayList<>(limit+1);\n\n        TestClass1 testClass = new TestClass1();\n        testClass.strMap = strMap;\n        testClass.strListIncl = strList;\n\n        for (int i = 0; i < limit; i++) {\n            strMap.put(\"k\" + i, \"v\");\n            strList.add(\"e\");\n        }\n        String testClassStr = GridToStringBuilder.toString(TestClass1.class, testClass);\n\n        strMap.put(\"kz\", \"v\"); // important to add last element in TreeMap here\n        strList.add(\"e\");\n\n        String testClassStrOf = GridToStringBuilder.toString(TestClass1.class, testClass);\n\n        String testClassStrOfR = testClassStrOf.replaceAll(\"... and 1 more\",\"\");\n\n        assertTrue(\"Collection limit error in Map or List, normal: <\" + testClassStr + \">, overflowed: <\"\n            +\"testClassStrOf\", testClassStr.length() == testClassStrOfR.length());\n\n    }"
        ],
        [
            "GridToStringBuilder::arrayToString(Class,Object)",
            "1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050 -\n1051  \n1052  \n1053  \n1054 -\n1055 -\n1056 -\n1057 -\n1058 -\n1059  \n1060  \n1061 -\n1062 -\n1063 -\n1064 -\n1065 -\n1066 -\n1067 -\n1068 -\n1069 -\n1070 -\n1071 -\n1072 -\n1073 -\n1074 -\n1075 -\n1076 -\n1077 -\n1078 -\n1079 -\n1080 -\n1081  \n1082  \n1083  \n1084 -\n1085  \n1086  \n1087  \n1088  \n1089  \n1090  ",
            "    /**\n     * @param arrType Type of the array.\n     * @param arr Array object.\n     * @return String representation of an array.\n     */\n    @SuppressWarnings({\"ConstantConditions\", \"unchecked\"})\n    public static <T> String arrayToString(Class arrType, @Nullable Object arr) {\n        if (arr == null)\n            return \"null\";\n\n        T[] array = (T[])arr;\n\n        if (array.length > COLLECTION_LIMIT)\n            arr = Arrays.copyOf(array, COLLECTION_LIMIT);\n\n        String res;\n\n        if (arrType.equals(byte[].class))\n            res = Arrays.toString((byte[])arr);\n        else if (arrType.equals(boolean[].class))\n            res = Arrays.toString((boolean[])arr);\n        else if (arrType.equals(short[].class))\n            res = Arrays.toString((short[])arr);\n        else if (arrType.equals(int[].class))\n            res = Arrays.toString((int[])arr);\n        else if (arrType.equals(long[].class))\n            res = Arrays.toString((long[])arr);\n        else if (arrType.equals(float[].class))\n            res = Arrays.toString((float[])arr);\n        else if (arrType.equals(double[].class))\n            res = Arrays.toString((double[])arr);\n        else if (arrType.equals(char[].class))\n            res = Arrays.toString((char[])arr);\n        else\n            res = Arrays.toString((Object[])arr);\n\n        if (array.length > COLLECTION_LIMIT) {\n            StringBuilder resSB = new StringBuilder(res);\n\n            resSB.deleteCharAt(resSB.length() - 1);\n            resSB.append(\"... and \").append(array.length - COLLECTION_LIMIT).append(\" more]\");\n\n            res = resSB.toString();\n        }\n\n        return res;\n    }",
            "1044  \n1045  \n1046  \n1047  \n1048  \n1049  \n1050 +\n1051  \n1052  \n1053  \n1054  \n1055 +\n1056  \n1057 +\n1058 +\n1059 +\n1060 +\n1061 +\n1062 +\n1063 +\n1064 +\n1065 +\n1066 +\n1067 +\n1068 +\n1069 +\n1070 +\n1071 +\n1072 +\n1073 +\n1074 +\n1075 +\n1076 +\n1077 +\n1078 +\n1079 +\n1080 +\n1081 +\n1082 +\n1083 +\n1084 +\n1085 +\n1086 +\n1087 +\n1088 +\n1089 +\n1090 +\n1091 +\n1092 +\n1093 +\n1094 +\n1095 +\n1096 +\n1097 +\n1098 +\n1099 +\n1100 +\n1101 +\n1102 +\n1103 +\n1104 +\n1105 +\n1106 +\n1107 +\n1108 +\n1109 +\n1110 +\n1111 +\n1112 +\n1113 +\n1114 +\n1115 +\n1116 +\n1117 +\n1118 +\n1119 +\n1120 +\n1121 +\n1122 +\n1123 +\n1124 +\n1125 +\n1126 +\n1127 +\n1128 +\n1129 +\n1130  \n1131  \n1132  \n1133 +\n1134  \n1135  \n1136  \n1137  \n1138  \n1139  ",
            "    /**\n     * @param arrType Type of the array.\n     * @param arr Array object.\n     * @return String representation of an array.\n     */\n    @SuppressWarnings({\"ConstantConditions\", \"unchecked\"})\n    public static <T> String arrayToString(Class arrType, Object arr) {\n        if (arr == null)\n            return \"null\";\n\n        String res;\n        int more = 0;\n\n        if (arrType.equals(byte[].class)) {\n            byte[] byteArr = (byte[])arr;\n            if (byteArr.length > COLLECTION_LIMIT) {\n                more = byteArr.length - COLLECTION_LIMIT;\n                byteArr = Arrays.copyOf(byteArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(byteArr);\n        }\n        else if (arrType.equals(boolean[].class)) {\n            boolean[] boolArr = (boolean[])arr;\n            if (boolArr.length > COLLECTION_LIMIT) {\n                more = boolArr.length - COLLECTION_LIMIT;\n                boolArr = Arrays.copyOf(boolArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(boolArr);\n        }\n        else if (arrType.equals(short[].class)) {\n            short[] shortArr = (short[])arr;\n            if (shortArr.length > COLLECTION_LIMIT) {\n                more = shortArr.length - COLLECTION_LIMIT;\n                shortArr = Arrays.copyOf(shortArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(shortArr);\n        }\n        else if (arrType.equals(int[].class)) {\n            int[] intArr = (int[])arr;\n            if (intArr.length > COLLECTION_LIMIT) {\n                more = intArr.length - COLLECTION_LIMIT;\n                intArr = Arrays.copyOf(intArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(intArr);\n        }\n        else if (arrType.equals(long[].class)) {\n            long[] longArr = (long[])arr;\n            if (longArr.length > COLLECTION_LIMIT) {\n                more = longArr.length - COLLECTION_LIMIT;\n                longArr = Arrays.copyOf(longArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(longArr);\n        }\n        else if (arrType.equals(float[].class)) {\n            float[] floatArr = (float[])arr;\n            if (floatArr.length > COLLECTION_LIMIT) {\n                more = floatArr.length - COLLECTION_LIMIT;\n                floatArr = Arrays.copyOf(floatArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(floatArr);\n        }\n        else if (arrType.equals(double[].class)) {\n            double[] doubleArr = (double[])arr;\n            if (doubleArr.length > COLLECTION_LIMIT) {\n                more = doubleArr.length - COLLECTION_LIMIT;\n                doubleArr = Arrays.copyOf(doubleArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(doubleArr);\n        }\n        else if (arrType.equals(char[].class)) {\n            char[] charArr = (char[])arr;\n            if (charArr.length > COLLECTION_LIMIT) {\n                more = charArr.length - COLLECTION_LIMIT;\n                charArr = Arrays.copyOf(charArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(charArr);\n        }\n        else {\n            Object[] objArr = (Object[])arr;\n            if (objArr.length > COLLECTION_LIMIT) {\n                more = objArr.length - COLLECTION_LIMIT;\n                objArr = Arrays.copyOf(objArr, COLLECTION_LIMIT);\n            }\n            res = Arrays.toString(objArr);\n        }\n        if (more > 0) {\n            StringBuilder resSB = new StringBuilder(res);\n\n            resSB.deleteCharAt(resSB.length() - 1);\n            resSB.append(\"... and \").append(more).append(\" more]\");\n\n            res = resSB.toString();\n        }\n\n        return res;\n    }"
        ]
    ],
    "e7ee88aa82b3c59eb1ae671e2d33f1307f17162e": [
        [
            "IgniteCachePartitionLossPolicySelfTest::checkLostPartition(boolean,boolean)",
            " 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  ",
            "    /**\n     * @param canWrite {@code True} if writes are allowed.\n     * @param safe {@code True} if lost partition should trigger exception.\n     * @throws Exception if failed.\n     */\n    private void checkLostPartition(boolean canWrite, boolean safe) throws Exception {\n        assert partLossPlc != null;\n\n        int part = prepareTopology();\n\n        for (Ignite ig : G.allGrids()) {\n            info(\"Checking node: \" + ig.cluster().localNode().id());\n\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            verifyCacheOps(canWrite, safe, part, ig);\n\n            // Check we can read and write to lost partition in recovery mode.\n            IgniteCache<Integer, Integer> recoverCache = cache.withPartitionRecover();\n\n            for (int lostPart : recoverCache.lostPartitions()) {\n                recoverCache.get(lostPart);\n                recoverCache.put(lostPart, lostPart);\n            }\n\n            // Check that writing in recover mode does not clear partition state.\n            verifyCacheOps(canWrite, safe, part, ig);\n        }\n\n        // Check that partition state does not change after we start a new node.\n        IgniteEx grd = startGrid(3);\n\n        info(\"Newly started node: \" + grd.cluster().localNode().id());\n\n        for (Ignite ig : G.allGrids())\n            verifyCacheOps(canWrite, safe, part, ig);\n\n        ignite(0).resetLostPartitions(Collections.singletonList(CACHE_NAME));\n\n        awaitPartitionMapExchange(true, true, null);\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            assertTrue(cache.lostPartitions().isEmpty());\n\n            int parts = ig.affinity(CACHE_NAME).partitions();\n\n            for (int i = 0; i < parts; i++) {\n                cache.get(i);\n\n                cache.put(i, i);\n            }\n        }\n    }",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164 +\n 165 +\n 166 +\n 167 +\n 168 +\n 169 +\n 170 +\n 171 +\n 172 +\n 173 +\n 174 +\n 175 +\n 176 +\n 177 +\n 178 +\n 179 +\n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  ",
            "    /**\n     * @param canWrite {@code True} if writes are allowed.\n     * @param safe {@code True} if lost partition should trigger exception.\n     * @throws Exception if failed.\n     */\n    private void checkLostPartition(boolean canWrite, boolean safe) throws Exception {\n        assert partLossPlc != null;\n\n        int part = prepareTopology();\n\n        // Wait for all grids (servers and client) have same topology version\n        // to make sure that all nodes received map with lost partition.\n        GridTestUtils.waitForCondition(() -> {\n            AffinityTopologyVersion last = null;\n            for (Ignite ig : G.allGrids()) {\n                AffinityTopologyVersion ver = ((IgniteEx) ig).context().cache().context().exchange().readyAffinityVersion();\n\n                if (last != null && !last.equals(ver))\n                    return false;\n\n                last = ver;\n            }\n\n            return true;\n        }, 10000);\n\n        for (Ignite ig : G.allGrids()) {\n            info(\"Checking node: \" + ig.cluster().localNode().id());\n\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            verifyCacheOps(canWrite, safe, part, ig);\n\n            // Check we can read and write to lost partition in recovery mode.\n            IgniteCache<Integer, Integer> recoverCache = cache.withPartitionRecover();\n\n            for (int lostPart : recoverCache.lostPartitions()) {\n                recoverCache.get(lostPart);\n                recoverCache.put(lostPart, lostPart);\n            }\n\n            // Check that writing in recover mode does not clear partition state.\n            verifyCacheOps(canWrite, safe, part, ig);\n        }\n\n        // Check that partition state does not change after we start a new node.\n        IgniteEx grd = startGrid(3);\n\n        info(\"Newly started node: \" + grd.cluster().localNode().id());\n\n        for (Ignite ig : G.allGrids())\n            verifyCacheOps(canWrite, safe, part, ig);\n\n        ignite(0).resetLostPartitions(Collections.singletonList(CACHE_NAME));\n\n        awaitPartitionMapExchange(true, true, null);\n\n        for (Ignite ig : G.allGrids()) {\n            IgniteCache<Integer, Integer> cache = ig.cache(CACHE_NAME);\n\n            assertTrue(cache.lostPartitions().isEmpty());\n\n            int parts = ig.affinity(CACHE_NAME).partitions();\n\n            for (int i = 0; i < parts; i++) {\n                cache.get(i);\n\n                cache.put(i, i);\n            }\n        }\n    }"
        ]
    ],
    "0020e417cfaf36da513c2063dc081514498a1597": [
        [
            "ClientListenerNioListener::onHandshake(GridNioSession,byte)",
            " 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "    /**\n     * Perform handshake.\n     *\n     * @param ses Session.\n     * @param msg Message bytes.\n     */\n    private void onHandshake(GridNioSession ses, byte[] msg) {\n        BinaryInputStream stream = new BinaryHeapInputStream(msg);\n\n        BinaryReaderExImpl reader = new BinaryReaderExImpl(null, stream, null, true);\n\n        byte cmd = reader.readByte();\n\n        if (cmd != ClientListenerRequest.HANDSHAKE) {\n            U.warn(log, \"Unexpected client request (will close session): \" + ses.remoteAddress());\n\n            ses.close();\n\n            return;\n        }\n\n        short verMajor = reader.readShort();\n        short verMinor = reader.readShort();\n        short verMaintenance = reader.readShort();\n\n        ClientListenerProtocolVersion ver = ClientListenerProtocolVersion.create(verMajor, verMinor, verMaintenance);\n\n        BinaryWriterExImpl writer = new BinaryWriterExImpl(null, new BinaryHeapOutputStream(8), null, null);\n\n        byte clientType = reader.readByte();\n\n        ClientListenerConnectionContext connCtx = null;\n\n        try {\n            connCtx = prepareContext(ses, clientType);\n\n            ensureClientPermissions(clientType);\n\n            if (connCtx.isVersionSupported(ver)) {\n                connCtx.initializeFromHandshake(ver, reader);\n\n                ses.addMeta(CONN_CTX_META_KEY, connCtx);\n            }\n            else\n                throw new IgniteCheckedException(\"Unsupported version.\");\n\n            connCtx.handler().writeHandshake(writer);\n        }\n        catch (IgniteAccessControlException authEx) {\n            writer.writeBoolean(false);\n\n            writer.writeShort((short)0);\n            writer.writeShort((short)0);\n            writer.writeShort((short)0);\n\n            writer.doWriteString(authEx.getMessage());\n\n            if (ver.compareTo(ClientConnectionContext.VER_1_1_0) >= 0)\n                writer.writeInt(ClientStatus.AUTH_FAILED);\n        }\n        catch (IgniteCheckedException e) {\n            U.warn(log, \"Error during handshake [rmtAddr=\" + ses.remoteAddress() + \", msg=\" + e.getMessage() + ']');\n\n            ClientListenerProtocolVersion currVer;\n\n            if (connCtx == null)\n                currVer = ClientListenerProtocolVersion.create(0, 0, 0);\n            else\n                currVer = connCtx.currentVersion();\n\n            writer.writeBoolean(false);\n\n            writer.writeShort(currVer.major());\n            writer.writeShort(currVer.minor());\n            writer.writeShort(currVer.maintenance());\n\n            writer.doWriteString(e.getMessage());\n\n            if (ver.compareTo(ClientConnectionContext.VER_1_1_0) >= 0)\n                writer.writeInt(ClientStatus.FAILED);\n        }\n\n        ses.send(writer.array());\n    }",
            " 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236 +\n 237 +\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  ",
            "    /**\n     * Perform handshake.\n     *\n     * @param ses Session.\n     * @param msg Message bytes.\n     */\n    private void onHandshake(GridNioSession ses, byte[] msg) {\n        BinaryInputStream stream = new BinaryHeapInputStream(msg);\n\n        BinaryReaderExImpl reader = new BinaryReaderExImpl(null, stream, null, true);\n\n        byte cmd = reader.readByte();\n\n        if (cmd != ClientListenerRequest.HANDSHAKE) {\n            U.warn(log, \"Unexpected client request (will close session): \" + ses.remoteAddress());\n\n            ses.close();\n\n            return;\n        }\n\n        short verMajor = reader.readShort();\n        short verMinor = reader.readShort();\n        short verMaintenance = reader.readShort();\n\n        ClientListenerProtocolVersion ver = ClientListenerProtocolVersion.create(verMajor, verMinor, verMaintenance);\n\n        BinaryWriterExImpl writer = new BinaryWriterExImpl(null, new BinaryHeapOutputStream(8), null, null);\n\n        byte clientType = reader.readByte();\n\n        ClientListenerConnectionContext connCtx = null;\n\n        try {\n            connCtx = prepareContext(ses, clientType);\n\n            ensureClientPermissions(clientType);\n\n            if (connCtx.isVersionSupported(ver)) {\n                connCtx.initializeFromHandshake(ver, reader);\n\n                ses.addMeta(CONN_CTX_META_KEY, connCtx);\n            }\n            else\n                throw new IgniteCheckedException(\"Unsupported version.\");\n\n            connCtx.handler().writeHandshake(writer);\n\n            ses.addMeta(CONN_CTX_HANDSHAKE_PASSED, true);\n        }\n        catch (IgniteAccessControlException authEx) {\n            writer.writeBoolean(false);\n\n            writer.writeShort((short)0);\n            writer.writeShort((short)0);\n            writer.writeShort((short)0);\n\n            writer.doWriteString(authEx.getMessage());\n\n            if (ver.compareTo(ClientConnectionContext.VER_1_1_0) >= 0)\n                writer.writeInt(ClientStatus.AUTH_FAILED);\n        }\n        catch (IgniteCheckedException e) {\n            U.warn(log, \"Error during handshake [rmtAddr=\" + ses.remoteAddress() + \", msg=\" + e.getMessage() + ']');\n\n            ClientListenerProtocolVersion currVer;\n\n            if (connCtx == null)\n                currVer = ClientListenerProtocolVersion.create(0, 0, 0);\n            else\n                currVer = connCtx.currentVersion();\n\n            writer.writeBoolean(false);\n\n            writer.writeShort(currVer.major());\n            writer.writeShort(currVer.minor());\n            writer.writeShort(currVer.maintenance());\n\n            writer.doWriteString(e.getMessage());\n\n            if (ver.compareTo(ClientConnectionContext.VER_1_1_0) >= 0)\n                writer.writeInt(ClientStatus.FAILED);\n        }\n\n        ses.send(writer.array());\n    }"
        ],
        [
            "ClientListenerNioListener::onMessage(GridNioSession,byte)",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131 -\n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  ",
            "    /** {@inheritDoc} */\n    @Override public void onMessage(GridNioSession ses, byte[] msg) {\n        assert msg != null;\n\n        ClientListenerConnectionContext connCtx = ses.meta(CONN_CTX_META_KEY);\n\n        if (connCtx == null) {\n            onHandshake(ses, msg);\n\n            ses.addMeta(CONN_CTX_HANDSHAKE_PASSED, true);\n\n            return;\n        }\n\n        ClientListenerMessageParser parser = connCtx.parser();\n        ClientListenerRequestHandler handler = connCtx.handler();\n\n        ClientListenerRequest req;\n\n        try {\n            req = parser.decode(msg);\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to parse client request.\", e);\n\n            ses.close();\n\n            return;\n        }\n\n        assert req != null;\n\n        try {\n            long startTime = 0;\n\n            if (log.isDebugEnabled()) {\n                startTime = System.nanoTime();\n\n                log.debug(\"Client request received [reqId=\" + req.requestId() + \", addr=\" +\n                    ses.remoteAddress() + \", req=\" + req + ']');\n            }\n\n            ClientListenerResponse resp = handler.handle(req);\n\n            if (resp != null) {\n                if (log.isDebugEnabled()) {\n                    long dur = (System.nanoTime() - startTime) / 1000;\n\n                    log.debug(\"Client request processed [reqId=\" + req.requestId() + \", dur(mcs)=\" + dur +\n                        \", resp=\" + resp.status() + ']');\n                }\n\n                byte[] outMsg = parser.encode(resp);\n\n                ses.send(outMsg);\n            }\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to process client request [req=\" + req + ']', e);\n\n            ses.send(parser.encode(handler.handleException(e, req)));\n        }\n    }",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  ",
            "    /** {@inheritDoc} */\n    @Override public void onMessage(GridNioSession ses, byte[] msg) {\n        assert msg != null;\n\n        ClientListenerConnectionContext connCtx = ses.meta(CONN_CTX_META_KEY);\n\n        if (connCtx == null) {\n            onHandshake(ses, msg);\n\n            return;\n        }\n\n        ClientListenerMessageParser parser = connCtx.parser();\n        ClientListenerRequestHandler handler = connCtx.handler();\n\n        ClientListenerRequest req;\n\n        try {\n            req = parser.decode(msg);\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to parse client request.\", e);\n\n            ses.close();\n\n            return;\n        }\n\n        assert req != null;\n\n        try {\n            long startTime = 0;\n\n            if (log.isDebugEnabled()) {\n                startTime = System.nanoTime();\n\n                log.debug(\"Client request received [reqId=\" + req.requestId() + \", addr=\" +\n                    ses.remoteAddress() + \", req=\" + req + ']');\n            }\n\n            ClientListenerResponse resp = handler.handle(req);\n\n            if (resp != null) {\n                if (log.isDebugEnabled()) {\n                    long dur = (System.nanoTime() - startTime) / 1000;\n\n                    log.debug(\"Client request processed [reqId=\" + req.requestId() + \", dur(mcs)=\" + dur +\n                        \", resp=\" + resp.status() + ']');\n                }\n\n                byte[] outMsg = parser.encode(resp);\n\n                ses.send(outMsg);\n            }\n        }\n        catch (Exception e) {\n            U.error(log, \"Failed to process client request [req=\" + req + ']', e);\n\n            ses.send(parser.encode(handler.handleException(e, req)));\n        }\n    }"
        ]
    ],
    "c2369ff70a5c19f4931039e3fd45d6db758fdb0e": [
        [
            "GridCommandHandlerTest::testCacheIdleVerify()",
            " 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590 -\n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603 -\n 604  \n 605  \n 606  \n 607  \n 608  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testCacheIdleVerify() throws Exception {\n        Ignite ignite = startGrids(2);\n\n        ignite.cluster().active(true);\n\n        IgniteCache<Object, Object> cache = ignite.createCache(new CacheConfiguration<>()\n            .setAffinity(new RendezvousAffinityFunction(false, 32))\n            .setBackups(1)\n            .setName(\"cacheIV\"));\n\n        for (int i = 0; i < 100; i++)\n            cache.put(i, i);\n\n        injectTestSystemOut();\n\n        assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"idle_verify\"));\n\n        assertTrue(testOut.toString().contains(\"no conflicts have been found\"));\n\n        HashSet<Integer> clearKeys = new HashSet<>(Arrays.asList(1, 2, 3, 4, 5, 6));\n\n        ((IgniteEx)ignite).context().cache().cache(\"cacheIV\").clearLocallyAll(clearKeys, true, true, true);\n\n        assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"idle_verify\"));\n\n        assertTrue(testOut.toString().contains(\"conflict partitions\"));\n    }",
            " 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590 +\n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603 +\n 604  \n 605  \n 606  \n 607  \n 608  ",
            "    /**\n     * @throws Exception If failed.\n     */\n    public void testCacheIdleVerify() throws Exception {\n        Ignite ignite = startGrids(2);\n\n        ignite.cluster().active(true);\n\n        IgniteCache<Object, Object> cache = ignite.createCache(new CacheConfiguration<>()\n            .setAffinity(new RendezvousAffinityFunction(false, 32))\n            .setBackups(1)\n            .setName(DEFAULT_CACHE_NAME));\n\n        for (int i = 0; i < 100; i++)\n            cache.put(i, i);\n\n        injectTestSystemOut();\n\n        assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"idle_verify\"));\n\n        assertTrue(testOut.toString().contains(\"no conflicts have been found\"));\n\n        HashSet<Integer> clearKeys = new HashSet<>(Arrays.asList(1, 2, 3, 4, 5, 6));\n\n        ((IgniteEx)ignite).context().cache().cache(DEFAULT_CACHE_NAME).clearLocallyAll(clearKeys, true, true, true);\n\n        assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"idle_verify\"));\n\n        assertTrue(testOut.toString().contains(\"conflict partitions\"));\n    }"
        ],
        [
            "GridCommandHandlerTest::testCacheAffinity()",
            " 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751 -\n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760 -\n 761  \n 762  \n 763  \n 764  ",
            "    /**\n     *\n     */\n    public void testCacheAffinity() throws Exception {\n        Ignite ignite = startGrid();\n\n        ignite.cluster().active(true);\n\n        IgniteCache<Object, Object> cache1 = ignite.createCache(new CacheConfiguration<>()\n            .setAffinity(new RendezvousAffinityFunction(false, 32))\n            .setBackups(1)\n            .setName(\"cacheAf\"));\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        injectTestSystemOut();\n\n        assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"list\", \".*\"));\n\n        assertTrue(testOut.toString().contains(\"cacheName=cacheAf\"));\n        assertTrue(testOut.toString().contains(\"prim=32\"));\n        assertTrue(testOut.toString().contains(\"mapped=32\"));\n        assertTrue(testOut.toString().contains(\"affCls=RendezvousAffinityFunction\"));\n    }",
            " 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742 +\n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751 +\n 752  \n 753  \n 754  \n 755  ",
            "    /**\n     *\n     */\n    public void testCacheAffinity() throws Exception {\n        Ignite ignite = startGrid();\n\n        ignite.cluster().active(true);\n\n        IgniteCache<Object, Object> cache1 = ignite.createCache(new CacheConfiguration<>()\n            .setAffinity(new RendezvousAffinityFunction(false, 32))\n            .setBackups(1)\n            .setName(DEFAULT_CACHE_NAME));\n\n        for (int i = 0; i < 100; i++)\n            cache1.put(i, i);\n\n        injectTestSystemOut();\n\n        assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"list\", \".*\"));\n\n        assertTrue(testOut.toString().contains(\"cacheName=\" + DEFAULT_CACHE_NAME));\n        assertTrue(testOut.toString().contains(\"prim=32\"));\n        assertTrue(testOut.toString().contains(\"mapped=32\"));\n        assertTrue(testOut.toString().contains(\"affCls=RendezvousAffinityFunction\"));\n    }"
        ],
        [
            "GridCommandHandlerTest::testCacheGroups()",
            " 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715 -\n 716 -\n 717 -\n 718 -\n 719 -\n 720 -\n 721 -\n 722  \n 723  \n 724  \n 725 -\n 726  \n 727 -\n 728 -\n 729 -\n 730 -\n 731 -\n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  ",
            "    /**\n     *\n     */\n    public void testCacheGroups() throws Exception {\n        Ignite ignite = startGrid();\n\n        ignite.cluster().active(true);\n\n        IgniteCache<Object, Object> cache1 = ignite.createCache(new CacheConfiguration<>()\n            .setAffinity(new RendezvousAffinityFunction(false, 32))\n            .setBackups(1)\n            .setGroupName(\"G100\")\n            .setName(\"cacheG1\"));\n\n        IgniteCache<Object, Object> cache2 = ignite.createCache(new CacheConfiguration<>()\n            .setAffinity(new RendezvousAffinityFunction(false, 32))\n            .setBackups(1)\n            .setGroupName(\"G100\")\n            .setName(\"cacheG2\"));\n\n        for (int i = 0; i < 100; i++) {\n            cache1.put(i, i);\n\n            cache2.put(i, i);\n        }\n\n        injectTestSystemOut();\n\n        assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"list\", \".*\", \"groups\"));\n\n        assertTrue(testOut.toString().contains(\"G100\"));\n    }",
            " 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715 +\n 716  \n 717  \n 718  \n 719 +\n 720  \n 721 +\n 722 +\n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  ",
            "    /**\n     *\n     */\n    public void testCacheGroups() throws Exception {\n        Ignite ignite = startGrid();\n\n        ignite.cluster().active(true);\n\n        IgniteCache<Object, Object> cache = ignite.createCache(new CacheConfiguration<>()\n            .setAffinity(new RendezvousAffinityFunction(false, 32))\n            .setBackups(1)\n            .setGroupName(\"G100\")\n            .setName(DEFAULT_CACHE_NAME));\n\n        for (int i = 0; i < 100; i++)\n            cache.put(i, i);\n\n        injectTestSystemOut();\n\n        assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"list\", \".*\", \"groups\"));\n\n        assertTrue(testOut.toString().contains(\"G100\"));\n    }"
        ],
        [
            "GridCommandHandlerTest::testCacheContention()",
            " 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627 -\n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  ",
            "    /**\n     *\n     */\n    public void testCacheContention() throws Exception {\n        int cnt = 10;\n\n        final ExecutorService svc = Executors.newFixedThreadPool(cnt);\n\n        try {\n            Ignite ignite = startGrids(2);\n\n            ignite.cluster().active(true);\n\n            final IgniteCache<Object, Object> cache = ignite.createCache(new CacheConfiguration<>()\n                .setAffinity(new RendezvousAffinityFunction(false, 32))\n                .setAtomicityMode(TRANSACTIONAL)\n                .setBackups(1)\n                .setName(\"cacheCont\"));\n\n            final CountDownLatch l = new CountDownLatch(1);\n\n            final CountDownLatch l2 = new CountDownLatch(1);\n\n            svc.submit(new Runnable() {\n                @Override public void run() {\n                    try (final Transaction tx = ignite.transactions().txStart()) {\n                        cache.put(0, 0);\n\n                        l.countDown();\n\n                        U.awaitQuiet(l2);\n\n                        tx.commit();\n                    }\n                }\n            });\n\n            for (int i = 0; i < cnt - 1; i++) {\n                svc.submit(new Runnable() {\n                    @Override public void run() {\n                        U.awaitQuiet(l);\n\n                        try (final Transaction tx = ignite.transactions().txStart()) {\n                            cache.get(0);\n\n                            tx.commit();\n                        }\n                    }\n                });\n            }\n\n            U.awaitQuiet(l);\n\n            Thread.sleep(300);\n\n            injectTestSystemOut();\n\n            assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"contention\", \"5\"));\n\n            l2.countDown();\n\n            assertTrue(testOut.toString().contains(\"TxEntry\"));\n            assertTrue(testOut.toString().contains(\"op=READ\"));\n            assertTrue(testOut.toString().contains(\"op=CREATE\"));\n            assertTrue(testOut.toString().contains(\"id=\" + ignite(0).cluster().localNode().id()));\n            assertTrue(testOut.toString().contains(\"id=\" + ignite(1).cluster().localNode().id()));\n        }\n        finally {\n            svc.shutdown();\n            svc.awaitTermination(100, TimeUnit.DAYS);\n        }\n    }",
            " 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627 +\n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  ",
            "    /**\n     *\n     */\n    public void testCacheContention() throws Exception {\n        int cnt = 10;\n\n        final ExecutorService svc = Executors.newFixedThreadPool(cnt);\n\n        try {\n            Ignite ignite = startGrids(2);\n\n            ignite.cluster().active(true);\n\n            final IgniteCache<Object, Object> cache = ignite.createCache(new CacheConfiguration<>()\n                .setAffinity(new RendezvousAffinityFunction(false, 32))\n                .setAtomicityMode(TRANSACTIONAL)\n                .setBackups(1)\n                .setName(DEFAULT_CACHE_NAME));\n\n            final CountDownLatch l = new CountDownLatch(1);\n\n            final CountDownLatch l2 = new CountDownLatch(1);\n\n            svc.submit(new Runnable() {\n                @Override public void run() {\n                    try (final Transaction tx = ignite.transactions().txStart()) {\n                        cache.put(0, 0);\n\n                        l.countDown();\n\n                        U.awaitQuiet(l2);\n\n                        tx.commit();\n                    }\n                }\n            });\n\n            for (int i = 0; i < cnt - 1; i++) {\n                svc.submit(new Runnable() {\n                    @Override public void run() {\n                        U.awaitQuiet(l);\n\n                        try (final Transaction tx = ignite.transactions().txStart()) {\n                            cache.get(0);\n\n                            tx.commit();\n                        }\n                    }\n                });\n            }\n\n            U.awaitQuiet(l);\n\n            Thread.sleep(300);\n\n            injectTestSystemOut();\n\n            assertEquals(EXIT_CODE_OK, execute(\"--cache\", \"contention\", \"5\"));\n\n            l2.countDown();\n\n            assertTrue(testOut.toString().contains(\"TxEntry\"));\n            assertTrue(testOut.toString().contains(\"op=READ\"));\n            assertTrue(testOut.toString().contains(\"op=CREATE\"));\n            assertTrue(testOut.toString().contains(\"id=\" + ignite(0).cluster().localNode().id()));\n            assertTrue(testOut.toString().contains(\"id=\" + ignite(1).cluster().localNode().id()));\n        }\n        finally {\n            svc.shutdown();\n            svc.awaitTermination(100, TimeUnit.DAYS);\n        }\n    }"
        ]
    ],
    "2bf48e5417816c2211402dfacf5678f6a67014bd": [
        [
            "GridAbstractTest::getDefaultTestTimeout()",
            "2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183  \n2184  \n2185  \n2186 -\n2187  ",
            "    /**\n     * @return Default test case timeout.\n     */\n    private long getDefaultTestTimeout() {\n        String timeout = GridTestProperties.getProperty(\"test.timeout\");\n\n        if (timeout != null)\n            return Long.parseLong(timeout);\n\n        return DFLT_TEST_TIMEOUT;\n    }",
            "2174  \n2175  \n2176  \n2177  \n2178  \n2179  \n2180  \n2181  \n2182  \n2183 +\n2184  ",
            "    /**\n     * @return Default test case timeout.\n     */\n    private long getDefaultTestTimeout() {\n        String timeout = GridTestProperties.getProperty(\"test.timeout\");\n\n        if (timeout != null)\n            return Long.parseLong(timeout);\n\n        return GridTestUtils.DFLT_TEST_TIMEOUT;\n    }"
        ]
    ],
    "49b835812a607a116c4dbc99ce60ed1684229b34": [
        [
            "IgniteWalReaderTest::createWalIteratorFactory(String,String)",
            " 915 -\n 916 -\n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  ",
            "    @NotNull private IgniteWalIteratorFactory createWalIteratorFactory(String subfolderName,\n        String workDir) throws IgniteCheckedException {\n        final File binaryMeta = U.resolveWorkDirectory(workDir, \"binary_meta\", false);\n        final File binaryMetaWithConsId = new File(binaryMeta, subfolderName);\n        final File marshallerMapping = U.resolveWorkDirectory(workDir, \"marshaller\", false);\n\n        return new IgniteWalIteratorFactory(log,\n            PAGE_SIZE,\n            binaryMetaWithConsId,\n            marshallerMapping);\n    }",
            " 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922 +\n 923 +\n 924 +\n 925 +\n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  ",
            "    /**\n     * @param subfolderName Subfolder name.\n     * @param workDir Work directory.\n     * @return WAL iterator factory.\n     * @throws IgniteCheckedException If failed.\n     */\n    @NotNull private IgniteWalIteratorFactory createWalIteratorFactory(\n        String subfolderName,\n        String workDir\n    ) throws IgniteCheckedException {\n        final File binaryMeta = U.resolveWorkDirectory(workDir, \"binary_meta\", false);\n        final File binaryMetaWithConsId = new File(binaryMeta, subfolderName);\n        final File marshallerMapping = U.resolveWorkDirectory(workDir, \"marshaller\", false);\n\n        return new IgniteWalIteratorFactory(log,\n            PAGE_SIZE,\n            binaryMetaWithConsId,\n            marshallerMapping);\n    }"
        ],
        [
            "DataEntry::DataEntry(int,KeyCacheObject,CacheObject,GridCacheOperation,GridCacheVersion,GridCacheVersion,long,int,long)",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81 -\n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  ",
            "    /**\n     * @param cacheId Cache ID.\n     * @param key Key.\n     * @param val Value.\n     * @param op Operation.\n     * @param nearXidVer Near transaction version.\n     * @param writeVer Write version.\n     * @param expireTime Expire time.\n     * @param partId Partition ID.\n     * @param partCnt Partition counter.\n     */\n    public DataEntry(\n        int cacheId,\n        KeyCacheObject key,\n        CacheObject val,\n        GridCacheOperation op,\n        GridCacheVersion nearXidVer,\n        GridCacheVersion writeVer,\n        long expireTime,\n        int partId,\n        long partCnt\n    ) {\n        this.cacheId = cacheId;\n        this.key = key;\n        this.val = val;\n        this.op = op;\n        this.nearXidVer = nearXidVer;\n        this.writeVer = writeVer;\n        this.expireTime = expireTime;\n        this.partId = partId;\n        this.partCnt = partCnt;\n\n        // Only CREATE, UPDATE and DELETE operations should be stored in WAL.\n        assert op == GridCacheOperation.CREATE || op == GridCacheOperation.UPDATE || op == GridCacheOperation.DELETE : op;\n    }",
            "  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82 +\n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  ",
            "    /**\n     * @param cacheId Cache ID.\n     * @param key Key.\n     * @param val Value or null for delete operation.\n     * @param op Operation.\n     * @param nearXidVer Near transaction version.\n     * @param writeVer Write version.\n     * @param expireTime Expire time.\n     * @param partId Partition ID.\n     * @param partCnt Partition counter.\n     */\n    public DataEntry(\n        int cacheId,\n        KeyCacheObject key,\n        @Nullable CacheObject val,\n        GridCacheOperation op,\n        GridCacheVersion nearXidVer,\n        GridCacheVersion writeVer,\n        long expireTime,\n        int partId,\n        long partCnt\n    ) {\n        this.cacheId = cacheId;\n        this.key = key;\n        this.val = val;\n        this.op = op;\n        this.nearXidVer = nearXidVer;\n        this.writeVer = writeVer;\n        this.expireTime = expireTime;\n        this.partId = partId;\n        this.partCnt = partCnt;\n\n        // Only CREATE, UPDATE and DELETE operations should be stored in WAL.\n        assert op == GridCacheOperation.CREATE || op == GridCacheOperation.UPDATE || op == GridCacheOperation.DELETE : op;\n    }"
        ],
        [
            "UnwrapDataEntry::unwrappedKey()",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "    /**\n     * Unwraps key value from cache key object into primitive boxed type or source class. If client classes were used\n     * in key, call of this method requires classes to be available in classpath\n     *\n     * @return Key which was placed into cache. Or null if failed\n     */\n    public Object unwrappedKey() {\n        try {\n            if (keepBinary && key instanceof BinaryObject)\n                return key;\n            Object unwrapped = key.value(cacheObjValCtx, false);\n            if (unwrapped instanceof BinaryObject) {\n                if (keepBinary)\n                    return unwrapped;\n                unwrapped = ((BinaryObject)unwrapped).deserialize();\n            }\n            return unwrapped;\n        }\n        catch (Exception e) {\n            cacheObjValCtx.kernalContext().log(UnwrapDataEntry.class)\n                .error(\"Unable to convert key [\" + key + \"]\", e);\n            return null;\n        }\n    }",
            "  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79 +\n  80  \n  81  \n  82  \n  83  \n  84  \n  85 +\n  86  \n  87  \n  88  \n  89  \n  90  \n  91 +\n  92  \n  93  \n  94  ",
            "    /**\n     * Unwraps key value from cache key object into primitive boxed type or source class. If client classes were used\n     * in key, call of this method requires classes to be available in classpath.\n     *\n     * @return Key which was placed into cache. Or null if failed to convert.\n     */\n    public Object unwrappedKey() {\n        try {\n            if (keepBinary && key instanceof BinaryObject)\n                return key;\n\n            Object unwrapped = key.value(cacheObjValCtx, false);\n\n            if (unwrapped instanceof BinaryObject) {\n                if (keepBinary)\n                    return unwrapped;\n                unwrapped = ((BinaryObject)unwrapped).deserialize();\n            }\n\n            return unwrapped;\n        }\n        catch (Exception e) {\n            cacheObjValCtx.kernalContext().log(UnwrapDataEntry.class)\n                .error(\"Unable to convert key [\" + key + \"]\", e);\n\n            return null;\n        }\n    }"
        ],
        [
            "UnwrapDataEntry::unwrappedValue()",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  ",
            "    /**\n     * Unwraps value value from cache value object into primitive boxed type or source class. If client classes were\n     * used in key, call of this method requires classes to be available in classpath\n     *\n     * @return Value which was placed into cache. Or null if failed\n     */\n    public Object unwrappedValue() {\n        try {\n            if (keepBinary && val instanceof BinaryObject)\n                return val;\n            return val.value(cacheObjValCtx, false);\n        }\n        catch (Exception e) {\n            cacheObjValCtx.kernalContext().log(UnwrapDataEntry.class)\n                .error(\"Unable to convert value [\" + value() + \"]\", e);\n            return null;\n        }\n    }",
            "  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105 +\n 106 +\n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  ",
            "    /**\n     * Unwraps value value from cache value object into primitive boxed type or source class. If client classes were\n     * used in key, call of this method requires classes to be available in classpath.\n     *\n     * @return Value which was placed into cache. Or null for delete operation or for failure.\n     */\n    public Object unwrappedValue() {\n        try {\n            if (val == null)\n                return null;\n\n            if (keepBinary && val instanceof BinaryObject)\n                return val;\n\n            return val.value(cacheObjValCtx, false);\n        }\n        catch (Exception e) {\n            cacheObjValCtx.kernalContext().log(UnwrapDataEntry.class)\n                .error(\"Unable to convert value [\" + value() + \"]\", e);\n            return null;\n        }\n    }"
        ],
        [
            "StandaloneWalRecordsIterator::postProcessDataEntry(IgniteCacheObjectProcessor,CacheObjectContext,DataEntry)",
            " 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341 -\n 342 -\n 343 -\n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  ",
            "    /**\n     * Converts entry or lazy data entry into unwrapped entry\n     * @param processor cache object processor for de-serializing objects.\n     * @param fakeCacheObjCtx cache object context for de-serializing binary and unwrapping objects.\n     * @param dataEntry entry to process\n     * @return post precessed entry\n     * @throws IgniteCheckedException if failed\n     */\n    @NotNull\n    private DataEntry postProcessDataEntry(\n        final IgniteCacheObjectProcessor processor,\n        final CacheObjectContext fakeCacheObjCtx,\n        final DataEntry dataEntry) throws IgniteCheckedException {\n\n        final KeyCacheObject key;\n        final CacheObject val;\n        final File marshallerMappingFileStoreDir =\n            fakeCacheObjCtx.kernalContext().marshallerContext().getMarshallerMappingFileStoreDir();\n\n        if (dataEntry instanceof LazyDataEntry) {\n            final LazyDataEntry lazyDataEntry = (LazyDataEntry)dataEntry;\n            key = processor.toKeyCacheObject(fakeCacheObjCtx,\n                lazyDataEntry.getKeyType(),\n                lazyDataEntry.getKeyBytes());\n            val = processor.toCacheObject(fakeCacheObjCtx,\n                lazyDataEntry.getValType(),\n                lazyDataEntry.getValBytes());\n        }\n        else {\n            key = dataEntry.key();\n            val = dataEntry.value();\n        }\n\n        return new UnwrapDataEntry(\n            dataEntry.cacheId(),\n            key,\n            val,\n            dataEntry.op(),\n            dataEntry.nearXidVersion(),\n            dataEntry.writeVersion(),\n            dataEntry.expireTime(),\n            dataEntry.partitionId(),\n            dataEntry.partitionCounter(),\n            fakeCacheObjCtx,\n            keepBinary || marshallerMappingFileStoreDir == null);\n    }",
            " 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338 +\n 339  \n 340  \n 341  \n 342 +\n 343 +\n 344 +\n 345 +\n 346 +\n 347 +\n 348 +\n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  ",
            "    /**\n     * Converts entry or lazy data entry into unwrapped entry\n     * @param processor cache object processor for de-serializing objects.\n     * @param fakeCacheObjCtx cache object context for de-serializing binary and unwrapping objects.\n     * @param dataEntry entry to process\n     * @return post precessed entry\n     * @throws IgniteCheckedException if failed\n     */\n    @NotNull\n    private DataEntry postProcessDataEntry(\n        final IgniteCacheObjectProcessor processor,\n        final CacheObjectContext fakeCacheObjCtx,\n        final DataEntry dataEntry) throws IgniteCheckedException {\n\n        final KeyCacheObject key;\n        final CacheObject val;\n        final File marshallerMappingFileStoreDir =\n            fakeCacheObjCtx.kernalContext().marshallerContext().getMarshallerMappingFileStoreDir();\n\n        if (dataEntry instanceof LazyDataEntry) {\n            final LazyDataEntry lazyDataEntry = (LazyDataEntry)dataEntry;\n\n            key = processor.toKeyCacheObject(fakeCacheObjCtx,\n                lazyDataEntry.getKeyType(),\n                lazyDataEntry.getKeyBytes());\n\n            final byte type = lazyDataEntry.getValType();\n\n            val = type == 0 ? null :\n                processor.toCacheObject(fakeCacheObjCtx,\n                    type,\n                    lazyDataEntry.getValBytes());\n        }\n        else {\n            key = dataEntry.key();\n            val = dataEntry.value();\n        }\n\n        return new UnwrapDataEntry(\n            dataEntry.cacheId(),\n            key,\n            val,\n            dataEntry.op(),\n            dataEntry.nearXidVersion(),\n            dataEntry.writeVersion(),\n            dataEntry.expireTime(),\n            dataEntry.partitionId(),\n            dataEntry.partitionCounter(),\n            fakeCacheObjCtx,\n            keepBinary || marshallerMappingFileStoreDir == null);\n    }"
        ]
    ],
    "a45677cf0b6b6ffa524fc10932c002d3b879f943": [
        [
            "VisorQueryTask::VisorQueryJob::run(VisorQueryTaskArg)",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103 -\n 104 -\n 105 -\n 106 -\n 107 -\n 108 -\n 109 -\n 110 -\n 111 -\n 112 -\n 113 -\n 114 -\n 115 -\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  ",
            "        /** {@inheritDoc} */\n        @Override protected VisorEither<VisorQueryResult> run(final VisorQueryTaskArg arg) {\n            try {\n                UUID nid = ignite.localNode().id();\n\n                SqlFieldsQuery qry = new SqlFieldsQuery(arg.getQueryText());\n                qry.setPageSize(arg.getPageSize());\n                qry.setLocal(arg.isLocal());\n                qry.setDistributedJoins(arg.isDistributedJoins());\n                qry.setEnforceJoinOrder(arg.isEnforceJoinOrder());\n                qry.setReplicatedOnly(arg.isReplicatedOnly());\n                qry.setLazy(arg.getLazy());\n\n                long start = U.currentTimeMillis();\n\n                FieldsQueryCursor<List<?>> qryCursor;\n\n                String cacheName = arg.getCacheName();\n\n                if (F.isEmpty(cacheName))\n                    qryCursor = ignite.context().query().querySqlFieldsNoCache(qry, true);\n                else {\n                    IgniteCache<Object, Object> c = ignite.cache(cacheName);\n\n                    if (c == null)\n                        throw new SQLException(\"Fail to execute query. Cache not found: \" + cacheName);\n\n                    try {\n                        qryCursor = c.withKeepBinary().query(qry);\n                    }\n                    catch (CacheException e) {\n                        // Work around for DDL without explicit schema name.\n                        if (X.hasCause(e, IgniteSQLException.class)\n                            && e.getMessage().contains(\"can only be executed on PUBLIC schema\")) {\n                            qry.setSchema(\"PUBLIC\");\n\n                            qryCursor = c.withKeepBinary().query(qry);\n                        }\n                        else\n                            throw e;\n                    }\n                }\n\n                VisorQueryCursor<List<?>> cur = new VisorQueryCursor<>(qryCursor);\n\n                Collection<GridQueryFieldMetadata> meta = cur.fieldsMeta();\n\n                if (meta == null)\n                    return new VisorEither<>(\n                        new VisorExceptionWrapper(new SQLException(\"Fail to execute query. No metadata available.\")));\n                else {\n                    List<VisorQueryField> names = new ArrayList<>(meta.size());\n\n                    for (GridQueryFieldMetadata col : meta)\n                        names.add(new VisorQueryField(col.schemaName(), col.typeName(),\n                            col.fieldName(), col.fieldTypeName()));\n\n                    List<Object[]> rows = fetchSqlQueryRows(cur, arg.getPageSize());\n\n                    // Query duration + fetch duration.\n                    long duration = U.currentTimeMillis() - start;\n\n                    boolean hasNext = cur.hasNext();\n\n                    // Generate query ID to store query cursor in node local storage.\n                    String qryId = SQL_QRY_NAME + \"-\" + UUID.randomUUID();\n\n                    if (hasNext) {\n                        ignite.cluster().<String, VisorQueryCursor<List<?>>>nodeLocalMap().put(qryId, cur);\n\n                        scheduleResultSetHolderRemoval(qryId, ignite);\n                    }\n                    else\n                        cur.close();\n\n                    return new VisorEither<>(new VisorQueryResult(nid, qryId, names, rows, hasNext, duration));\n                }\n            }\n            catch (Throwable e) {\n                return new VisorEither<>(new VisorExceptionWrapper(e));\n            }\n        }",
            "  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  ",
            "        /** {@inheritDoc} */\n        @Override protected VisorEither<VisorQueryResult> run(final VisorQueryTaskArg arg) {\n            try {\n                UUID nid = ignite.localNode().id();\n\n                SqlFieldsQuery qry = new SqlFieldsQuery(arg.getQueryText());\n                qry.setPageSize(arg.getPageSize());\n                qry.setLocal(arg.isLocal());\n                qry.setDistributedJoins(arg.isDistributedJoins());\n                qry.setEnforceJoinOrder(arg.isEnforceJoinOrder());\n                qry.setReplicatedOnly(arg.isReplicatedOnly());\n                qry.setLazy(arg.getLazy());\n\n                long start = U.currentTimeMillis();\n\n                FieldsQueryCursor<List<?>> qryCursor;\n\n                String cacheName = arg.getCacheName();\n\n                if (F.isEmpty(cacheName))\n                    qryCursor = ignite.context().query().querySqlFieldsNoCache(qry, true);\n                else {\n                    IgniteCache<Object, Object> c = ignite.cache(cacheName);\n\n                    if (c == null)\n                        throw new SQLException(\"Fail to execute query. Cache not found: \" + cacheName);\n\n                    qryCursor = c.withKeepBinary().query(qry);\n                }\n\n                VisorQueryCursor<List<?>> cur = new VisorQueryCursor<>(qryCursor);\n\n                Collection<GridQueryFieldMetadata> meta = cur.fieldsMeta();\n\n                if (meta == null)\n                    return new VisorEither<>(\n                        new VisorExceptionWrapper(new SQLException(\"Fail to execute query. No metadata available.\")));\n                else {\n                    List<VisorQueryField> names = new ArrayList<>(meta.size());\n\n                    for (GridQueryFieldMetadata col : meta)\n                        names.add(new VisorQueryField(col.schemaName(), col.typeName(),\n                            col.fieldName(), col.fieldTypeName()));\n\n                    List<Object[]> rows = fetchSqlQueryRows(cur, arg.getPageSize());\n\n                    // Query duration + fetch duration.\n                    long duration = U.currentTimeMillis() - start;\n\n                    boolean hasNext = cur.hasNext();\n\n                    // Generate query ID to store query cursor in node local storage.\n                    String qryId = SQL_QRY_NAME + \"-\" + UUID.randomUUID();\n\n                    if (hasNext) {\n                        ignite.cluster().<String, VisorQueryCursor<List<?>>>nodeLocalMap().put(qryId, cur);\n\n                        scheduleResultSetHolderRemoval(qryId, ignite);\n                    }\n                    else\n                        cur.close();\n\n                    return new VisorEither<>(new VisorQueryResult(nid, qryId, names, rows, hasNext, duration));\n                }\n            }\n            catch (Throwable e) {\n                return new VisorEither<>(new VisorExceptionWrapper(e));\n            }\n        }"
        ]
    ],
    "f3a61e4a4753b31ecdcef0864e8c095214b6a4ae": [
        [
            "FileWriteAheadLogManager::FileDecompressor::run()",
            "2066  \n2067  \n2068 -\n2069 -\n2070  \n2071  \n2072 -\n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090 -\n2091  \n2092  \n2093  \n2094  \n2095  \n2096  \n2097  \n2098  \n2099  \n2100 -\n2101 -\n2102 -\n2103 -\n2104 -\n2105  \n2106 -\n2107 -\n2108 -\n2109 -\n2110  \n2111  \n2112  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            Throwable err = null;\n\n            while (!Thread.currentThread().isInterrupted() && !stopped) {\n                try {\n                    long segmentToDecompress = segmentsQueue.take();\n\n                    if (stopped)\n                        break;\n\n                    File zip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".zip\");\n                    File unzipTmp = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".tmp\");\n                    File unzip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress));\n\n                    try (ZipInputStream zis = new ZipInputStream(new BufferedInputStream(new FileInputStream(zip)));\n                        FileIO io = ioFactory.create(unzipTmp)) {\n                        zis.getNextEntry();\n\n                        int bytesRead;\n                        while ((bytesRead = zis.read(arr)) > 0)\n                            io.write(arr, 0, bytesRead);\n                    }\n\n                    Files.move(unzipTmp.toPath(), unzip.toPath());\n\n                    synchronized (this) {\n                        decompressionFutures.remove(segmentToDecompress).onDone();\n                    }\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n                }\n                catch (Throwable t) {\n                    err = t;\n                }\n                finally {\n                    if (err == null && !stopped)\n                        err = new IllegalStateException(\"Thread \" + getName() + \" is terminated unexpectedly\");\n\n                    if (err instanceof OutOfMemoryError)\n                        cctx.kernalContext().failure().process(new FailureContext(CRITICAL_ERROR, err));\n                    else if (err != null)\n                        cctx.kernalContext().failure().process(new FailureContext(SYSTEM_WORKER_TERMINATION, err));\n                }\n            }\n        }",
            "2067  \n2068  \n2069  \n2070 +\n2071 +\n2072  \n2073 +\n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  \n2081  \n2082  \n2083  \n2084  \n2085  \n2086  \n2087  \n2088  \n2089  \n2090  \n2091 +\n2092 +\n2093 +\n2094 +\n2095 +\n2096 +\n2097 +\n2098 +\n2099 +\n2100 +\n2101  \n2102  \n2103  \n2104  \n2105  \n2106  \n2107  \n2108  \n2109  \n2110 +\n2111 +\n2112 +\n2113  \n2114 +\n2115 +\n2116  \n2117  \n2118  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            while (!Thread.currentThread().isInterrupted() && !stopped) {\n                long segmentToDecompress = -1L;\n\n                try {\n                    segmentToDecompress = segmentsQueue.take();\n\n                    if (stopped)\n                        break;\n\n                    File zip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".zip\");\n                    File unzipTmp = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".tmp\");\n                    File unzip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress));\n\n                    try (ZipInputStream zis = new ZipInputStream(new BufferedInputStream(new FileInputStream(zip)));\n                        FileIO io = ioFactory.create(unzipTmp)) {\n                        zis.getNextEntry();\n\n                        int bytesRead;\n                        while ((bytesRead = zis.read(arr)) > 0)\n                            io.write(arr, 0, bytesRead);\n                    }\n\n                    try {\n                        Files.move(unzipTmp.toPath(), unzip.toPath());\n                    }\n                    catch (FileAlreadyExistsException e) {\n                        U.error(log, \"Can't rename temporary unzipped segment: raw segment is already present \" +\n                            \"[tmp=\" + unzipTmp + \", raw=\" + unzip + \"]\", e);\n\n                        if (!unzipTmp.delete())\n                            U.error(log, \"Can't delete temporary unzipped segment [tmp=\" + unzipTmp + \"]\");\n                    }\n\n                    synchronized (this) {\n                        decompressionFutures.remove(segmentToDecompress).onDone();\n                    }\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n                }\n                catch (Throwable t) {\n                    if (!stopped && segmentToDecompress != -1L) {\n                        IgniteCheckedException e = new IgniteCheckedException(\"Error during WAL segment \" +\n                            \"decompression [segmentIdx=\" + segmentToDecompress + \"]\", t);\n\n                        decompressionFutures.remove(segmentToDecompress).onDone(e);\n                    }\n                }\n            }\n        }"
        ],
        [
            "FsyncModeFileWriteAheadLogManager::FileDecompressor::run()",
            "1874  \n1875  \n1876 -\n1877 -\n1878  \n1879  \n1880 -\n1881  \n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898 -\n1899  \n1900  \n1901  \n1902  \n1903  \n1904  \n1905  \n1906  \n1907  \n1908 -\n1909 -\n1910 -\n1911 -\n1912 -\n1913  \n1914 -\n1915 -\n1916 -\n1917 -\n1918  \n1919  \n1920  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            Throwable err = null;\n\n            while (!Thread.currentThread().isInterrupted() && !stopped) {\n                try {\n                    long segmentToDecompress = segmentsQueue.take();\n\n                    if (stopped)\n                        break;\n\n                    File zip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".zip\");\n                    File unzipTmp = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".tmp\");\n                    File unzip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress));\n\n                    try (ZipInputStream zis = new ZipInputStream(new BufferedInputStream(new FileInputStream(zip)));\n                        FileIO io = ioFactory.create(unzipTmp)) {\n                        zis.getNextEntry();\n\n                        int bytesRead;\n                        while ((bytesRead = zis.read(arr)) > 0)\n                            io.write(arr, 0, bytesRead);\n                    }\n\n                    Files.move(unzipTmp.toPath(), unzip.toPath());\n\n                    synchronized (this) {\n                        decompressionFutures.remove(segmentToDecompress).onDone();\n                    }\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n                }\n                catch (Throwable t) {\n                    err = t;\n                }\n                finally {\n                    if (err == null && !stopped)\n                        err = new IllegalStateException(\"Thread \" + getName() + \" is terminated unexpectedly\");\n\n                    if (err instanceof OutOfMemoryError)\n                        cctx.kernalContext().failure().process(new FailureContext(CRITICAL_ERROR, err));\n                    else if (err != null)\n                        cctx.kernalContext().failure().process(new FailureContext(SYSTEM_WORKER_TERMINATION, err));\n                }\n            }\n        }",
            "1875  \n1876  \n1877  \n1878 +\n1879 +\n1880  \n1881 +\n1882  \n1883  \n1884  \n1885  \n1886  \n1887  \n1888  \n1889  \n1890  \n1891  \n1892  \n1893  \n1894  \n1895  \n1896  \n1897  \n1898  \n1899 +\n1900 +\n1901 +\n1902 +\n1903 +\n1904 +\n1905 +\n1906 +\n1907 +\n1908 +\n1909  \n1910  \n1911  \n1912  \n1913  \n1914  \n1915  \n1916  \n1917  \n1918 +\n1919 +\n1920 +\n1921  \n1922 +\n1923 +\n1924  \n1925  \n1926  ",
            "        /** {@inheritDoc} */\n        @Override public void run() {\n            while (!Thread.currentThread().isInterrupted() && !stopped) {\n                long segmentToDecompress = -1L;\n\n                try {\n                    segmentToDecompress = segmentsQueue.take();\n\n                    if (stopped)\n                        break;\n\n                    File zip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".zip\");\n                    File unzipTmp = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress) + \".tmp\");\n                    File unzip = new File(walArchiveDir, FileDescriptor.fileName(segmentToDecompress));\n\n                    try (ZipInputStream zis = new ZipInputStream(new BufferedInputStream(new FileInputStream(zip)));\n                        FileIO io = ioFactory.create(unzipTmp)) {\n                        zis.getNextEntry();\n\n                        int bytesRead;\n                        while ((bytesRead = zis.read(arr)) > 0)\n                            io.write(arr, 0, bytesRead);\n                    }\n\n                    try {\n                        Files.move(unzipTmp.toPath(), unzip.toPath());\n                    }\n                    catch (FileAlreadyExistsException e) {\n                        U.error(log, \"Can't rename temporary unzipped segment: raw segment is already present \" +\n                            \"[tmp=\" + unzipTmp + \", raw=\" + unzip + ']', e);\n\n                        if (!unzipTmp.delete())\n                            U.error(log, \"Can't delete temporary unzipped segment [tmp=\" + unzipTmp + ']');\n                    }\n\n                    synchronized (this) {\n                        decompressionFutures.remove(segmentToDecompress).onDone();\n                    }\n                }\n                catch (InterruptedException ignore) {\n                    Thread.currentThread().interrupt();\n                }\n                catch (Throwable t) {\n                    if (!stopped && segmentToDecompress != -1L) {\n                        IgniteCheckedException e = new IgniteCheckedException(\"Error during WAL segment \" +\n                            \"decompression [segmentIdx=\" + segmentToDecompress + ']', t);\n\n                        decompressionFutures.remove(segmentToDecompress).onDone(e);\n                    }\n                }\n            }\n        }"
        ]
    ],
    "7076f42763dcfdb28f626d2f8072e7f34b1ff8d7": [
        [
            "RecordDataV1Serializer::RecordDataV1Serializer(GridCacheSharedContext)",
            " 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  ",
            "    /**\n     * @param cctx Cache shared context.\n     */\n    public RecordDataV1Serializer(GridCacheSharedContext cctx) {\n        this.cctx = cctx;\n        this.txRecordSerializer = new TxRecordSerializer();\n        this.co = cctx.kernalContext().cacheObjects();\n        this.pageSize = cctx.database().pageSize();\n        this.encSpi = cctx.gridConfig().getEncryptionSpi();\n\n        //This happen on offline WAL iteration(we don't have encryption keys available).\n        if (encSpi != null)\n            this.realPageSize = CU.encryptedPageSize(pageSize, encSpi);\n        else\n            this.realPageSize = pageSize;\n    }",
            " 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163 +\n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  ",
            "    /**\n     * @param cctx Cache shared context.\n     */\n    public RecordDataV1Serializer(GridCacheSharedContext cctx) {\n        this.cctx = cctx;\n        this.txRecordSerializer = new TxRecordSerializer();\n        this.co = cctx.kernalContext().cacheObjects();\n        this.pageSize = cctx.database().pageSize();\n        this.encSpi = cctx.gridConfig().getEncryptionSpi();\n        this.encMgr = cctx.kernalContext().encryption();\n\n        //This happen on offline WAL iteration(we don't have encryption keys available).\n        if (encSpi != null)\n            this.realPageSize = CU.encryptedPageSize(pageSize, encSpi);\n        else\n            this.realPageSize = pageSize;\n    }"
        ],
        [
            "RecordDataV1Serializer::writeEncryptedData(int,RecordType,ByteBuffer,ByteBuffer)",
            " 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312 -\n 313  \n 314  \n 315  \n 316  \n 317  ",
            "    /**\n     * Writes encrypted {@code clData} to {@code dst} stream.\n     *\n     * @param grpId Group id;\n     * @param plainRecType Plain record type\n     * @param clData Plain data.\n     * @param dst Destination buffer.\n     */\n    private void writeEncryptedData(int grpId, @Nullable RecordType plainRecType, ByteBuffer clData, ByteBuffer dst) {\n        int dtSz = encSpi.encryptedSize(clData.capacity());\n\n        dst.putInt(grpId);\n        dst.putInt(dtSz);\n\n        if (plainRecType != null)\n            putRecordType(dst, plainRecType);\n\n        Serializable key = cctx.kernalContext().encryption().groupKey(grpId);\n\n        assert key != null;\n\n        encSpi.encrypt(clData, key, dst);\n    }",
            " 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324 +\n 325  \n 326  \n 327  \n 328  \n 329  ",
            "    /**\n     * Writes encrypted {@code clData} to {@code dst} stream.\n     *\n     * @param grpId Group id;\n     * @param plainRecType Plain record type\n     * @param clData Plain data.\n     * @param dst Destination buffer.\n     */\n    private void writeEncryptedData(int grpId, @Nullable RecordType plainRecType, ByteBuffer clData, ByteBuffer dst) {\n        int dtSz = encSpi.encryptedSize(clData.capacity());\n\n        dst.putInt(grpId);\n        dst.putInt(dtSz);\n\n        if (plainRecType != null)\n            putRecordType(dst, plainRecType);\n\n        Serializable key = encMgr.groupKey(grpId);\n\n        assert key != null;\n\n        encSpi.encrypt(clData, key, dst);\n    }"
        ],
        [
            "IgnitePageMemReplaceDelayedWriteUnitTest::createPageMemory(IgniteConfiguration,ReplacedPageWriter,int)",
            " 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  ",
            "    /**\n     * @param cfg configuration\n     * @param pageWriter writer for page replacement.\n     * @param pageSize page size\n     * @return implementation for test\n     */\n    @NotNull\n    private PageMemoryImpl createPageMemory(IgniteConfiguration cfg, ReplacedPageWriter pageWriter, int pageSize) {\n        IgniteCacheDatabaseSharedManager db = mock(GridCacheDatabaseSharedManager.class);\n\n        when(db.checkpointLockIsHeldByThread()).thenReturn(true);\n\n        GridCacheSharedContext sctx = Mockito.mock(GridCacheSharedContext.class);\n\n        when(sctx.pageStore()).thenReturn(new NoOpPageStoreManager());\n        when(sctx.wal()).thenReturn(new NoOpWALManager());\n        when(sctx.database()).thenReturn(db);\n        when(sctx.logger(any(Class.class))).thenReturn(log);\n\n        GridKernalContext kernalCtx = mock(GridKernalContext.class);\n\n        when(kernalCtx.config()).thenReturn(cfg);\n        when(kernalCtx.log(any(Class.class))).thenReturn(log);\n        when(kernalCtx.internalSubscriptionProcessor()).thenAnswer(new Answer<Object>() {\n            @Override public Object answer(InvocationOnMock mock) throws Throwable {\n                return new GridInternalSubscriptionProcessor(kernalCtx);\n            }\n        });\n        when(kernalCtx.encryption()).thenAnswer(new Answer<Object>() {\n            @Override public Object answer(InvocationOnMock mock) throws Throwable {\n                return new GridEncryptionManager(kernalCtx);\n            }\n        });\n        when(sctx.kernalContext()).thenReturn(kernalCtx);\n\n        DataRegionConfiguration regCfg = cfg.getDataStorageConfiguration().getDefaultDataRegionConfiguration();\n\n        DataRegionMetricsImpl memMetrics = new DataRegionMetricsImpl(regCfg);\n\n        long[] sizes = prepareSegmentSizes(regCfg.getMaxSize());\n\n        DirectMemoryProvider provider = new UnsafeMemoryProvider(log);\n\n        PageMemoryImpl memory = new PageMemoryImpl(provider, sizes, sctx, pageSize,\n            pageWriter, null, () -> true, memMetrics, PageMemoryImpl.ThrottlingPolicy.DISABLED,\n            mock(CheckpointWriteProgressSupplier.class));\n\n        memory.start();\n        return memory;\n    }",
            " 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 +\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  ",
            "    /**\n     * @param cfg configuration\n     * @param pageWriter writer for page replacement.\n     * @param pageSize page size\n     * @return implementation for test\n     */\n    @NotNull\n    private PageMemoryImpl createPageMemory(IgniteConfiguration cfg, ReplacedPageWriter pageWriter, int pageSize) {\n        IgniteCacheDatabaseSharedManager db = mock(GridCacheDatabaseSharedManager.class);\n\n        when(db.checkpointLockIsHeldByThread()).thenReturn(true);\n\n        GridCacheSharedContext sctx = Mockito.mock(GridCacheSharedContext.class);\n\n        when(sctx.gridConfig()).thenReturn(cfg);\n        when(sctx.pageStore()).thenReturn(new NoOpPageStoreManager());\n        when(sctx.wal()).thenReturn(new NoOpWALManager());\n        when(sctx.database()).thenReturn(db);\n        when(sctx.logger(any(Class.class))).thenReturn(log);\n\n        GridKernalContext kernalCtx = mock(GridKernalContext.class);\n\n        when(kernalCtx.config()).thenReturn(cfg);\n        when(kernalCtx.log(any(Class.class))).thenReturn(log);\n        when(kernalCtx.internalSubscriptionProcessor()).thenAnswer(new Answer<Object>() {\n            @Override public Object answer(InvocationOnMock mock) throws Throwable {\n                return new GridInternalSubscriptionProcessor(kernalCtx);\n            }\n        });\n        when(kernalCtx.encryption()).thenAnswer(new Answer<Object>() {\n            @Override public Object answer(InvocationOnMock mock) throws Throwable {\n                return new GridEncryptionManager(kernalCtx);\n            }\n        });\n        when(sctx.kernalContext()).thenReturn(kernalCtx);\n\n        DataRegionConfiguration regCfg = cfg.getDataStorageConfiguration().getDefaultDataRegionConfiguration();\n\n        DataRegionMetricsImpl memMetrics = new DataRegionMetricsImpl(regCfg);\n\n        long[] sizes = prepareSegmentSizes(regCfg.getMaxSize());\n\n        DirectMemoryProvider provider = new UnsafeMemoryProvider(log);\n\n        PageMemoryImpl memory = new PageMemoryImpl(provider, sizes, sctx, pageSize,\n            pageWriter, null, () -> true, memMetrics, PageMemoryImpl.ThrottlingPolicy.DISABLED,\n            mock(CheckpointWriteProgressSupplier.class));\n\n        memory.start();\n        return memory;\n    }"
        ],
        [
            "RecordDataV1Serializer::needEncryption(WALRecord)",
            " 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  ",
            "    /**\n     * @param rec Record to check.\n     * @return {@code True} if this record should be encrypted.\n     */\n    private boolean needEncryption(WALRecord rec) {\n        if (!(rec instanceof WalRecordCacheGroupAware) || rec instanceof MetastoreDataRecord)\n            return false;\n\n        return needEncryption(((WalRecordCacheGroupAware)rec).groupId());\n    }",
            " 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231 +\n 232 +\n 233  \n 234  \n 235  \n 236  \n 237  ",
            "    /**\n     * @param rec Record to check.\n     * @return {@code True} if this record should be encrypted.\n     */\n    private boolean needEncryption(WALRecord rec) {\n        if (encSpi instanceof NoopEncryptionSpi)\n            return false;\n\n        if (!(rec instanceof WalRecordCacheGroupAware) || rec instanceof MetastoreDataRecord)\n            return false;\n\n        return needEncryption(((WalRecordCacheGroupAware)rec).groupId());\n    }"
        ],
        [
            "RecordDataV1Serializer::readEncryptedData(ByteBufferBackedDataInput,boolean)",
            " 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261 -\n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  ",
            "    /**\n     * Reads and decrypt data from {@code in} stream.\n     *\n     * @param in Input stream.\n     * @param readType If {@code true} plain record type will be read from {@code in}.\n     * @return Plain data stream, group id, plain record type,\n     * @throws IOException If failed.\n     * @throws IgniteCheckedException If failed.\n     */\n    private T3<ByteBufferBackedDataInput, Integer, RecordType> readEncryptedData(ByteBufferBackedDataInput in,\n        boolean readType)\n        throws IOException, IgniteCheckedException {\n        int grpId = in.readInt();\n        int encRecSz = in.readInt();\n        RecordType plainRecType = null;\n\n        if (readType)\n            plainRecType = RecordV1Serializer.readRecordType(in);\n\n        byte[] encData = new byte[encRecSz];\n\n        in.readFully(encData);\n\n        Serializable key = cctx.kernalContext().encryption().groupKey(grpId);\n\n        if (key == null)\n            return new T3<>(null, grpId, plainRecType);\n\n        byte[] clData = encSpi.decrypt(encData, key);\n\n        return new T3<>(new ByteBufferBackedDataInputImpl().buffer(ByteBuffer.wrap(clData)), grpId, plainRecType);\n    }",
            " 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273 +\n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  ",
            "    /**\n     * Reads and decrypt data from {@code in} stream.\n     *\n     * @param in Input stream.\n     * @param readType If {@code true} plain record type will be read from {@code in}.\n     * @return Plain data stream, group id, plain record type,\n     * @throws IOException If failed.\n     * @throws IgniteCheckedException If failed.\n     */\n    private T3<ByteBufferBackedDataInput, Integer, RecordType> readEncryptedData(ByteBufferBackedDataInput in,\n        boolean readType)\n        throws IOException, IgniteCheckedException {\n        int grpId = in.readInt();\n        int encRecSz = in.readInt();\n        RecordType plainRecType = null;\n\n        if (readType)\n            plainRecType = RecordV1Serializer.readRecordType(in);\n\n        byte[] encData = new byte[encRecSz];\n\n        in.readFully(encData);\n\n        Serializable key = encMgr.groupKey(grpId);\n\n        if (key == null)\n            return new T3<>(null, grpId, plainRecType);\n\n        byte[] clData = encSpi.decrypt(encData, key);\n\n        return new T3<>(new ByteBufferBackedDataInputImpl().buffer(ByteBuffer.wrap(clData)), grpId, plainRecType);\n    }"
        ],
        [
            "RecordDataV1Serializer::needEncryption(int)",
            " 230  \n 231  \n 232  \n 233  \n 234  \n 235 -\n 236  ",
            "    /**\n     * @param grpId Group id.\n     * @return {@code True} if this record should be encrypted.\n     */\n    private boolean needEncryption(int grpId) {\n        return cctx.kernalContext().encryption().groupKey(grpId) != null;\n    }",
            " 239  \n 240  \n 241  \n 242  \n 243  \n 244 +\n 245 +\n 246 +\n 247 +\n 248  ",
            "    /**\n     * @param grpId Group id.\n     * @return {@code True} if this record should be encrypted.\n     */\n    private boolean needEncryption(int grpId) {\n        if (encSpi instanceof NoopEncryptionSpi)\n            return false;\n\n        return encMgr.groupKey(grpId) != null;\n    }"
        ],
        [
            "PageMemoryImpl::PageMemoryImpl(DirectMemoryProvider,long,GridCacheSharedContext,int,ReplacedPageWriter,GridInClosure3X,CheckpointLockStateChecker,DataRegionMetricsImpl,ThrottlingPolicy,CheckpointWriteProgressSupplier)",
            " 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  ",
            "    /**\n     * @param directMemoryProvider Memory allocator to use.\n     * @param sizes segments sizes, last is checkpoint pool size.\n     * @param ctx Cache shared context.\n     * @param pageSize Page size.\n     * @param flushDirtyPage write callback invoked when a dirty page is removed for replacement.\n     * @param changeTracker Callback invoked to track changes in pages.\n     * @param stateChecker Checkpoint lock state provider. Used to ensure lock is held by thread, which modify pages.\n     * @param memMetrics Memory metrics to track dirty pages count and page replace rate.\n     * @param throttlingPlc Write throttle enabled and its type. Null equal to none.\n     * @param cpProgressProvider checkpoint progress, base for throttling. Null disables throttling.\n     */\n    public PageMemoryImpl(\n        DirectMemoryProvider directMemoryProvider,\n        long[] sizes,\n        GridCacheSharedContext<?, ?> ctx,\n        int pageSize,\n        ReplacedPageWriter flushDirtyPage,\n        @Nullable GridInClosure3X<Long, FullPageId, PageMemoryEx> changeTracker,\n        CheckpointLockStateChecker stateChecker,\n        DataRegionMetricsImpl memMetrics,\n        @Nullable ThrottlingPolicy throttlingPlc,\n        @NotNull CheckpointWriteProgressSupplier cpProgressProvider\n    ) {\n        assert ctx != null;\n        assert pageSize > 0;\n\n        log = ctx.logger(PageMemoryImpl.class);\n\n        this.ctx = ctx;\n        this.directMemoryProvider = directMemoryProvider;\n        this.sizes = sizes;\n        this.flushDirtyPage = flushDirtyPage;\n        delayedPageReplacementTracker =\n            getBoolean(IGNITE_DELAYED_REPLACED_PAGE_WRITE, true)\n                ? new DelayedPageReplacementTracker(pageSize, flushDirtyPage, log, sizes.length - 1) :\n                null;\n        this.changeTracker = changeTracker;\n        this.stateChecker = stateChecker;\n        this.throttlingPlc = throttlingPlc != null ? throttlingPlc : ThrottlingPolicy.CHECKPOINT_BUFFER_ONLY;\n        this.cpProgressProvider = cpProgressProvider;\n\n        storeMgr = ctx.pageStore();\n        walMgr = ctx.wal();\n        encMgr = ctx.kernalContext().encryption();\n\n        assert storeMgr != null;\n        assert walMgr != null;\n        assert encMgr != null;\n\n        sysPageSize = pageSize + PAGE_OVERHEAD;\n\n        encPageSize = CU.encryptedPageSize(pageSize, ctx.kernalContext().config().getEncryptionSpi());\n\n        rwLock = new OffheapReadWriteLock(128);\n\n        this.memMetrics = memMetrics;\n    }",
            " 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327 +\n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  ",
            "    /**\n     * @param directMemoryProvider Memory allocator to use.\n     * @param sizes segments sizes, last is checkpoint pool size.\n     * @param ctx Cache shared context.\n     * @param pageSize Page size.\n     * @param flushDirtyPage write callback invoked when a dirty page is removed for replacement.\n     * @param changeTracker Callback invoked to track changes in pages.\n     * @param stateChecker Checkpoint lock state provider. Used to ensure lock is held by thread, which modify pages.\n     * @param memMetrics Memory metrics to track dirty pages count and page replace rate.\n     * @param throttlingPlc Write throttle enabled and its type. Null equal to none.\n     * @param cpProgressProvider checkpoint progress, base for throttling. Null disables throttling.\n     */\n    public PageMemoryImpl(\n        DirectMemoryProvider directMemoryProvider,\n        long[] sizes,\n        GridCacheSharedContext<?, ?> ctx,\n        int pageSize,\n        ReplacedPageWriter flushDirtyPage,\n        @Nullable GridInClosure3X<Long, FullPageId, PageMemoryEx> changeTracker,\n        CheckpointLockStateChecker stateChecker,\n        DataRegionMetricsImpl memMetrics,\n        @Nullable ThrottlingPolicy throttlingPlc,\n        @NotNull CheckpointWriteProgressSupplier cpProgressProvider\n    ) {\n        assert ctx != null;\n        assert pageSize > 0;\n\n        log = ctx.logger(PageMemoryImpl.class);\n\n        this.ctx = ctx;\n        this.directMemoryProvider = directMemoryProvider;\n        this.sizes = sizes;\n        this.flushDirtyPage = flushDirtyPage;\n        delayedPageReplacementTracker =\n            getBoolean(IGNITE_DELAYED_REPLACED_PAGE_WRITE, true)\n                ? new DelayedPageReplacementTracker(pageSize, flushDirtyPage, log, sizes.length - 1) :\n                null;\n        this.changeTracker = changeTracker;\n        this.stateChecker = stateChecker;\n        this.throttlingPlc = throttlingPlc != null ? throttlingPlc : ThrottlingPolicy.CHECKPOINT_BUFFER_ONLY;\n        this.cpProgressProvider = cpProgressProvider;\n\n        storeMgr = ctx.pageStore();\n        walMgr = ctx.wal();\n        encMgr = ctx.kernalContext().encryption();\n        encSpi = ctx.gridConfig().getEncryptionSpi();\n\n        assert storeMgr != null;\n        assert walMgr != null;\n        assert encMgr != null;\n\n        sysPageSize = pageSize + PAGE_OVERHEAD;\n\n        encPageSize = CU.encryptedPageSize(pageSize, ctx.kernalContext().config().getEncryptionSpi());\n\n        rwLock = new OffheapReadWriteLock(128);\n\n        this.memMetrics = memMetrics;\n    }"
        ],
        [
            "PageMemoryImpl::realPageSize(int)",
            " 968  \n 969  \n 970 -\n 971  \n 972  \n 973  \n 974  ",
            "    /** {@inheritDoc} */\n    @Override public int realPageSize(int grpId) {\n        if (encMgr.groupKey(grpId) == null)\n            return pageSize();\n\n        return encPageSize;\n    }",
            " 974  \n 975  \n 976 +\n 977  \n 978  \n 979  \n 980  ",
            "    /** {@inheritDoc} */\n    @Override public int realPageSize(int grpId) {\n        if ((encSpi instanceof NoopEncryptionSpi) || encMgr.groupKey(grpId) == null)\n            return pageSize();\n\n        return encPageSize;\n    }"
        ]
    ],
    "0af4fdf839fb1fb58e34a81bba473ec86792062b": [
        [
            "WebConsoleConfigurationSelfTest::prepareMetadata()",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649 -\n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661 -\n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712  \n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852  \n 853  \n 854  \n 855  \n 856  \n 857  \n 858  \n 859  \n 860  \n 861  \n 862  \n 863  \n 864  \n 865  \n 866  \n 867  \n 868  \n 869  \n 870  \n 871  \n 872  \n 873  \n 874  \n 875  \n 876  \n 877  \n 878  \n 879  \n 880  ",
            "    /**\n     * Prepare metadata for properties, which are possible to configure.\n     */\n    @SuppressWarnings(\"deprecation\")\n    protected void prepareMetadata() {\n        // Cluster configuration.\n        Set<String> igniteCfgProps = new HashSet<>();\n        igniteCfgProps.add(\"cacheConfiguration\");\n        igniteCfgProps.add(\"discoverySpi\");\n        igniteCfgProps.add(\"localHost\");\n        igniteCfgProps.add(\"atomicConfiguration\");\n        igniteCfgProps.add(\"userAttributes\");\n        igniteCfgProps.add(\"binaryConfiguration\");\n        igniteCfgProps.add(\"cacheKeyConfiguration\");\n        igniteCfgProps.add(\"checkpointSpi\");\n        igniteCfgProps.add(\"collisionSpi\");\n        igniteCfgProps.add(\"communicationSpi\");\n        igniteCfgProps.add(\"networkTimeout\");\n        igniteCfgProps.add(\"networkSendRetryDelay\");\n        igniteCfgProps.add(\"networkSendRetryCount\");\n        igniteCfgProps.add(\"connectorConfiguration\");\n        igniteCfgProps.add(\"dataStorageConfiguration\");\n        igniteCfgProps.add(\"deploymentMode\");\n        igniteCfgProps.add(\"peerClassLoadingEnabled\");\n        igniteCfgProps.add(\"peerClassLoadingMissedResourcesCacheSize\");\n        igniteCfgProps.add(\"peerClassLoadingThreadPoolSize\");\n        igniteCfgProps.add(\"peerClassLoadingLocalClassPathExclude\");\n        igniteCfgProps.add(\"classLoader\");\n        igniteCfgProps.add(\"deploymentSpi\");\n        igniteCfgProps.add(\"eventStorageSpi\");\n        igniteCfgProps.add(\"includeEventTypes\");\n        igniteCfgProps.add(\"failureDetectionTimeout\");\n        igniteCfgProps.add(\"clientFailureDetectionTimeout\");\n        igniteCfgProps.add(\"failoverSpi\");\n        igniteCfgProps.add(\"hadoopConfiguration\");\n        igniteCfgProps.add(\"loadBalancingSpi\");\n        igniteCfgProps.add(\"marshalLocalJobs\");\n\n        // Removed since 2.0.\n        // igniteCfgProps.add(\"marshallerCacheKeepAliveTime\");\n        // igniteCfgProps.add(\"marshallerCacheThreadPoolSize\");\n\n        igniteCfgProps.add(\"metricsExpireTime\");\n        igniteCfgProps.add(\"metricsHistorySize\");\n        igniteCfgProps.add(\"metricsLogFrequency\");\n        igniteCfgProps.add(\"metricsUpdateFrequency\");\n        igniteCfgProps.add(\"workDirectory\");\n        igniteCfgProps.add(\"consistentId\");\n        igniteCfgProps.add(\"warmupClosure\");\n        igniteCfgProps.add(\"activeOnStart\");\n        igniteCfgProps.add(\"cacheSanityCheckEnabled\");\n        igniteCfgProps.add(\"longQueryWarningTimeout\");\n        igniteCfgProps.add(\"odbcConfiguration\");\n        igniteCfgProps.add(\"serviceConfiguration\");\n        igniteCfgProps.add(\"sqlConnectorConfiguration\");\n        igniteCfgProps.add(\"sslContextFactory\");\n\n        // Removed since 2.0.\n        // igniteCfgProps.add(\"swapSpaceSpi\");\n\n        igniteCfgProps.add(\"publicThreadPoolSize\");\n        igniteCfgProps.add(\"systemThreadPoolSize\");\n        igniteCfgProps.add(\"serviceThreadPoolSize\");\n        igniteCfgProps.add(\"managementThreadPoolSize\");\n        igniteCfgProps.add(\"igfsThreadPoolSize\");\n        igniteCfgProps.add(\"rebalanceThreadPoolSize\");\n        igniteCfgProps.add(\"utilityCacheThreadPoolSize\");\n        igniteCfgProps.add(\"utilityCacheKeepAliveTime\");\n        igniteCfgProps.add(\"asyncCallbackPoolSize\");\n        igniteCfgProps.add(\"stripedPoolSize\");\n        igniteCfgProps.add(\"dataStreamerThreadPoolSize\");\n        igniteCfgProps.add(\"queryThreadPoolSize\");\n        igniteCfgProps.add(\"executorConfiguration\");\n\n        // Removed since 2.0.\n        // igniteCfgProps.add(\"clockSyncSamples\");\n        // igniteCfgProps.add(\"clockSyncFrequency\");\n\n        igniteCfgProps.add(\"timeServerPortBase\");\n        igniteCfgProps.add(\"timeServerPortRange\");\n        igniteCfgProps.add(\"transactionConfiguration\");\n        igniteCfgProps.add(\"clientConnectorConfiguration\");\n        igniteCfgProps.add(\"fileSystemConfiguration\");\n        igniteCfgProps.add(\"gridLogger\");\n        igniteCfgProps.add(\"pluginConfigurations\");\n        igniteCfgProps.add(\"mvccVacuumFrequency\");\n        igniteCfgProps.add(\"mvccVacuumThreadCount\");\n\n        Set<String> igniteCfgPropsDep = new HashSet<>();\n        igniteCfgPropsDep.add(\"gridName\");\n        igniteCfgPropsDep.add(\"lateAffinityAssignment\");\n        igniteCfgPropsDep.add(\"persistentStoreConfiguration\");\n        igniteCfgPropsDep.add(\"memoryConfiguration\");\n        igniteCfgPropsDep.add(\"marshaller\");\n        igniteCfgPropsDep.add(\"discoveryStartupDelay\");\n\n        Set<String> igniteCfgPropsExcl = new HashSet<>();\n        // igniteCfgPropsExcl.add(\"lifecycleBeans\");\n        igniteCfgPropsExcl.add(\"daemon\");\n        igniteCfgPropsExcl.add(\"clientMode\");\n        igniteCfgPropsExcl.add(\"indexingSpi\");\n        igniteCfgPropsExcl.add(\"nodeId\");\n\n        metadata.put(IgniteConfiguration.class,\n            new MetadataInfo(igniteCfgProps, igniteCfgPropsDep, igniteCfgPropsExcl));\n\n        Set<String> atomicCfgProps = new HashSet<>();\n        atomicCfgProps.add(\"cacheMode\");\n        atomicCfgProps.add(\"atomicSequenceReserveSize\");\n        atomicCfgProps.add(\"backups\");\n        atomicCfgProps.add(\"affinity\");\n\n        metadata.put(AtomicConfiguration.class, new MetadataInfo(atomicCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> binaryCfgProps = new HashSet<>();\n        binaryCfgProps.add(\"idMapper\");\n        binaryCfgProps.add(\"nameMapper\");\n        binaryCfgProps.add(\"serializer\");\n        binaryCfgProps.add(\"typeConfigurations\");\n        binaryCfgProps.add(\"compactFooter\");\n        metadata.put(BinaryConfiguration.class, new MetadataInfo(binaryCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> binaryTypeCfgProps = new HashSet<>();\n        binaryTypeCfgProps.add(\"typeName\");\n        binaryTypeCfgProps.add(\"idMapper\");\n        binaryTypeCfgProps.add(\"nameMapper\");\n        binaryTypeCfgProps.add(\"serializer\");\n        binaryTypeCfgProps.add(\"enum\");\n        binaryTypeCfgProps.add(\"enumValues\");\n        metadata.put(BinaryTypeConfiguration.class, new MetadataInfo(binaryTypeCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> sharedFsCheckpointProps = new HashSet<>();\n        sharedFsCheckpointProps.add(\"directoryPaths\");\n        metadata.put(SharedFsCheckpointSpi.class, new MetadataInfo(sharedFsCheckpointProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> s3CheckpointProps = new HashSet<>();\n        s3CheckpointProps.add(\"bucketNameSuffix\");\n        s3CheckpointProps.add(\"bucketEndpoint\");\n        s3CheckpointProps.add(\"sSEAlgorithm\");\n        s3CheckpointProps.add(\"checkpointListener\");\n        metadata.put(S3CheckpointSpi.class, new MetadataInfo(s3CheckpointProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> cacheCheckpointProps = new HashSet<>();\n        cacheCheckpointProps.add(\"cacheName\");\n        metadata.put(CacheCheckpointSpi.class, new MetadataInfo(cacheCheckpointProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> jdbcCheckpointProps = new HashSet<>();\n        // Only setter for dataSource.\n        // jdbcCheckpointProps.add(\"dataSourceBean\");\n        // jdbcCheckpointProps.add(\"dialect\");\n        jdbcCheckpointProps.add(\"checkpointListener\");\n        jdbcCheckpointProps.add(\"user\");\n        // Only on code generation.\n        jdbcCheckpointProps.add(\"pwd\");\n        jdbcCheckpointProps.add(\"checkpointTableName\");\n        jdbcCheckpointProps.add(\"numberOfRetries\");\n        jdbcCheckpointProps.add(\"keyFieldName\");\n        jdbcCheckpointProps.add(\"keyFieldType\");\n        jdbcCheckpointProps.add(\"valueFieldName\");\n        jdbcCheckpointProps.add(\"valueFieldType\");\n        jdbcCheckpointProps.add(\"expireDateFieldName\");\n        jdbcCheckpointProps.add(\"expireDateFieldType\");\n        metadata.put(JdbcCheckpointSpi.class, new MetadataInfo(jdbcCheckpointProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> cliConProps = new HashSet<>();\n        cliConProps.add(\"host\");\n        cliConProps.add(\"port\");\n        cliConProps.add(\"portRange\");\n        cliConProps.add(\"socketSendBufferSize\");\n        cliConProps.add(\"socketReceiveBufferSize\");\n        cliConProps.add(\"maxOpenCursorsPerConnection\");\n        cliConProps.add(\"threadPoolSize\");\n        cliConProps.add(\"tcpNoDelay\");\n        cliConProps.add(\"idleTimeout\");\n        cliConProps.add(\"sslEnabled\");\n        cliConProps.add(\"sslClientAuth\");\n        cliConProps.add(\"useIgniteSslContextFactory\");\n        cliConProps.add(\"sslContextFactory\");\n        cliConProps.add(\"jdbcEnabled\");\n        cliConProps.add(\"odbcEnabled\");\n        cliConProps.add(\"thinClientEnabled\");\n        metadata.put(ClientConnectorConfiguration.class, new MetadataInfo(cliConProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> jobStealingCollisionProps = new HashSet<>();\n        jobStealingCollisionProps.add(\"activeJobsThreshold\");\n        jobStealingCollisionProps.add(\"waitJobsThreshold\");\n        jobStealingCollisionProps.add(\"messageExpireTime\");\n        jobStealingCollisionProps.add(\"maximumStealingAttempts\");\n        jobStealingCollisionProps.add(\"stealingEnabled\");\n        jobStealingCollisionProps.add(\"externalCollisionListener\");\n        jobStealingCollisionProps.add(\"stealingAttributes\");\n        metadata.put(JobStealingCollisionSpi.class,\n            new MetadataInfo(jobStealingCollisionProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> priQueueCollisionProps = new HashSet<>();\n        priQueueCollisionProps.add(\"parallelJobsNumber\");\n        priQueueCollisionProps.add(\"waitingJobsNumber\");\n        priQueueCollisionProps.add(\"priorityAttributeKey\");\n        priQueueCollisionProps.add(\"jobPriorityAttributeKey\");\n        priQueueCollisionProps.add(\"defaultPriority\");\n        priQueueCollisionProps.add(\"starvationIncrement\");\n        priQueueCollisionProps.add(\"starvationPreventionEnabled\");\n        metadata.put(PriorityQueueCollisionSpi.class, new MetadataInfo(priQueueCollisionProps, EMPTY_FIELDS,\n            SPI_EXCLUDED_FIELDS));\n\n        Set<String> fifoQueueCollisionProps = new HashSet<>();\n        fifoQueueCollisionProps.add(\"parallelJobsNumber\");\n        fifoQueueCollisionProps.add(\"waitingJobsNumber\");\n        metadata.put(FifoQueueCollisionSpi.class,\n            new MetadataInfo(fifoQueueCollisionProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> commProps = new HashSet<>();\n        commProps.add(\"listener\");\n        commProps.add(\"localAddress\");\n        commProps.add(\"localPort\");\n        commProps.add(\"localPortRange\");\n        commProps.add(\"sharedMemoryPort\");\n        commProps.add(\"idleConnectionTimeout\");\n        commProps.add(\"connectTimeout\");\n        commProps.add(\"maxConnectTimeout\");\n        commProps.add(\"reconnectCount\");\n        commProps.add(\"socketSendBuffer\");\n        commProps.add(\"socketReceiveBuffer\");\n        commProps.add(\"slowClientQueueLimit\");\n        commProps.add(\"ackSendThreshold\");\n        commProps.add(\"messageQueueLimit\");\n        commProps.add(\"unacknowledgedMessagesBufferSize\");\n        commProps.add(\"socketWriteTimeout\");\n        commProps.add(\"selectorsCount\");\n        commProps.add(\"addressResolver\");\n        commProps.add(\"directBuffer\");\n        commProps.add(\"directSendBuffer\");\n        commProps.add(\"tcpNoDelay\");\n\n        Set<String> commPropsDep = new HashSet<>();\n        commPropsDep.add(\"discoveryStartupDelay\");\n\n        // Removed from configuration since ignite 2.3\n        Set<String> commPropsExcl = new HashSet<>();\n        commPropsExcl.add(\"discoveryStartupDelay\");\n        commPropsExcl.addAll(SPI_EXCLUDED_FIELDS);\n\n        metadata.put(TcpCommunicationSpi.class,\n            new MetadataInfo(commProps, commPropsDep, commPropsExcl));\n\n        Set<String> discoverySpiProps = new HashSet<>();\n        discoverySpiProps.add(\"ipFinder\");\n        discoverySpiProps.add(\"localAddress\");\n        discoverySpiProps.add(\"localPort\");\n        discoverySpiProps.add(\"localPortRange\");\n        discoverySpiProps.add(\"addressResolver\");\n        discoverySpiProps.add(\"socketTimeout\");\n        discoverySpiProps.add(\"ackTimeout\");\n        discoverySpiProps.add(\"maxAckTimeout\");\n        discoverySpiProps.add(\"networkTimeout\");\n        discoverySpiProps.add(\"joinTimeout\");\n        discoverySpiProps.add(\"threadPriority\");\n        // Removed since 2.0.\n        // discoverySpiProps.add(\"heartbeatFrequency\");\n        // discoverySpiProps.add(\"maxMissedHeartbeats\");\n        // discoverySpiProps.add(\"maxMissedClientHeartbeats\");\n        discoverySpiProps.add(\"topHistorySize\");\n        discoverySpiProps.add(\"listener\");\n        discoverySpiProps.add(\"dataExchange\");\n        discoverySpiProps.add(\"metricsProvider\");\n        discoverySpiProps.add(\"reconnectCount\");\n        discoverySpiProps.add(\"statisticsPrintFrequency\");\n        discoverySpiProps.add(\"ipFinderCleanFrequency\");\n        discoverySpiProps.add(\"authenticator\");\n        discoverySpiProps.add(\"forceServerMode\");\n        discoverySpiProps.add(\"clientReconnectDisabled\");\n        metadata.put(TcpDiscoverySpi.class, new MetadataInfo(discoverySpiProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> connectorProps = new HashSet<>();\n        connectorProps.add(\"jettyPath\");\n        connectorProps.add(\"host\");\n        connectorProps.add(\"port\");\n        connectorProps.add(\"portRange\");\n        connectorProps.add(\"idleQueryCursorTimeout\");\n        connectorProps.add(\"idleQueryCursorCheckFrequency\");\n        connectorProps.add(\"idleTimeout\");\n        connectorProps.add(\"receiveBufferSize\");\n        connectorProps.add(\"sendBufferSize\");\n        connectorProps.add(\"sendQueueLimit\");\n        connectorProps.add(\"directBuffer\");\n        connectorProps.add(\"noDelay\");\n        connectorProps.add(\"selectorCount\");\n        connectorProps.add(\"threadPoolSize\");\n        connectorProps.add(\"messageInterceptor\");\n        connectorProps.add(\"secretKey\");\n        connectorProps.add(\"sslEnabled\");\n        connectorProps.add(\"sslClientAuth\");\n        connectorProps.add(\"sslFactory\");\n        metadata.put(ConnectorConfiguration.class, new MetadataInfo(connectorProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> dataStorageProps = new HashSet<>();\n        dataStorageProps.add(\"pageSize\");\n        dataStorageProps.add(\"concurrencyLevel\");\n        dataStorageProps.add(\"systemRegionInitialSize\");\n        dataStorageProps.add(\"systemRegionMaxSize\");\n        dataStorageProps.add(\"defaultDataRegionConfiguration\");\n        dataStorageProps.add(\"dataRegionConfigurations\");\n        dataStorageProps.add(\"storagePath\");\n        dataStorageProps.add(\"checkpointFrequency\");\n        dataStorageProps.add(\"checkpointThreads\");\n        dataStorageProps.add(\"checkpointWriteOrder\");\n        dataStorageProps.add(\"walMode\");\n        dataStorageProps.add(\"walPath\");\n        dataStorageProps.add(\"walArchivePath\");\n        dataStorageProps.add(\"walSegments\");\n        dataStorageProps.add(\"walSegmentSize\");\n        dataStorageProps.add(\"walHistorySize\");\n        dataStorageProps.add(\"walBufferSize\");\n        dataStorageProps.add(\"walFlushFrequency\");\n        dataStorageProps.add(\"walFsyncDelayNanos\");\n        dataStorageProps.add(\"walRecordIteratorBufferSize\");\n        dataStorageProps.add(\"lockWaitTime\");\n        dataStorageProps.add(\"walThreadLocalBufferSize\");\n        dataStorageProps.add(\"metricsSubIntervalCount\");\n        dataStorageProps.add(\"metricsRateTimeInterval\");\n        dataStorageProps.add(\"fileIOFactory\");\n        dataStorageProps.add(\"walAutoArchiveAfterInactivity\");\n        dataStorageProps.add(\"metricsEnabled\");\n        dataStorageProps.add(\"alwaysWriteFullPages\");\n        dataStorageProps.add(\"writeThrottlingEnabled\");\n        dataStorageProps.add(\"walCompactionEnabled\");\n        metadata.put(DataStorageConfiguration.class, new MetadataInfo(dataStorageProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> dataRegionProps = new HashSet<>();\n        dataRegionProps.add(\"name\");\n        dataRegionProps.add(\"initialSize\");\n        dataRegionProps.add(\"maxSize\");\n        dataRegionProps.add(\"swapPath\");\n        dataRegionProps.add(\"checkpointPageBufferSize\");\n        dataRegionProps.add(\"pageEvictionMode\");\n        dataRegionProps.add(\"evictionThreshold\");\n        dataRegionProps.add(\"emptyPagesPoolSize\");\n        dataRegionProps.add(\"metricsSubIntervalCount\");\n        dataRegionProps.add(\"metricsRateTimeInterval\");\n        dataRegionProps.add(\"metricsEnabled\");\n        dataRegionProps.add(\"persistenceEnabled\");\n        metadata.put(DataRegionConfiguration.class, new MetadataInfo(dataRegionProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> uriDeploymentProps = new HashSet<>();\n        uriDeploymentProps.add(\"uriList\");\n        uriDeploymentProps.add(\"temporaryDirectoryPath\");\n        uriDeploymentProps.add(\"scanners\");\n        uriDeploymentProps.add(\"listener\");\n        uriDeploymentProps.add(\"checkMd5\");\n        uriDeploymentProps.add(\"encodeUri\");\n        metadata.put(UriDeploymentSpi.class, new MetadataInfo(uriDeploymentProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> locDeploymentProps = new HashSet<>();\n        locDeploymentProps.add(\"listener\");\n        metadata.put(LocalDeploymentSpi.class, new MetadataInfo(locDeploymentProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> memoryEvtStorageProps = new HashSet<>();\n        memoryEvtStorageProps.add(\"expireAgeMs\");\n        memoryEvtStorageProps.add(\"expireCount\");\n        memoryEvtStorageProps.add(\"filter\");\n        metadata.put(MemoryEventStorageSpi.class, new MetadataInfo(memoryEvtStorageProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> alwaysFailoverProps = new HashSet<>();\n        alwaysFailoverProps.add(\"maximumFailoverAttempts\");\n        metadata.put(AlwaysFailoverSpi.class, new MetadataInfo(alwaysFailoverProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> jobStealingFailoverProps = new HashSet<>();\n        jobStealingFailoverProps.add(\"maximumFailoverAttempts\");\n        metadata.put(JobStealingFailoverSpi.class, new MetadataInfo(jobStealingFailoverProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> hadoopCfgProps = new HashSet<>();\n        hadoopCfgProps.add(\"mapReducePlanner\");\n        hadoopCfgProps.add(\"finishedJobInfoTtl\");\n        hadoopCfgProps.add(\"maxParallelTasks\");\n        hadoopCfgProps.add(\"maxTaskQueueSize\");\n        hadoopCfgProps.add(\"nativeLibraryNames\");\n        metadata.put(HadoopConfiguration.class, new MetadataInfo(hadoopCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> hadoopWeightMapReduceCfgProps = new HashSet<>();\n        hadoopWeightMapReduceCfgProps.add(\"localMapperWeight\");\n        hadoopWeightMapReduceCfgProps.add(\"remoteMapperWeight\");\n        hadoopWeightMapReduceCfgProps.add(\"localReducerWeight\");\n        hadoopWeightMapReduceCfgProps.add(\"remoteReducerWeight\");\n        hadoopWeightMapReduceCfgProps.add(\"preferLocalReducerThresholdWeight\");\n        metadata.put(IgniteHadoopWeightedMapReducePlanner.class,\n            new MetadataInfo(hadoopWeightMapReduceCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> weightedRndLoadBalancingProps = new HashSet<>();\n        weightedRndLoadBalancingProps.add(\"nodeWeight\");\n        weightedRndLoadBalancingProps.add(\"useWeights\");\n        metadata.put(WeightedRandomLoadBalancingSpi.class,\n            new MetadataInfo(weightedRndLoadBalancingProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> adaptiveLoadBalancingProps = new HashSet<>();\n        adaptiveLoadBalancingProps.add(\"loadProbe\");\n        metadata.put(AdaptiveLoadBalancingSpi.class,\n            new MetadataInfo(adaptiveLoadBalancingProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> roundRobinLoadBalancingProps = new HashSet<>();\n        roundRobinLoadBalancingProps.add(\"perTask\");\n        metadata.put(RoundRobinLoadBalancingSpi.class,\n            new MetadataInfo(roundRobinLoadBalancingProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> jobCntProbeProps = new HashSet<>();\n        jobCntProbeProps.add(\"useAverage\");\n        metadata.put(AdaptiveJobCountLoadProbe.class, new MetadataInfo(jobCntProbeProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> cpuLoadProbeProps = new HashSet<>();\n        cpuLoadProbeProps.add(\"useAverage\");\n        cpuLoadProbeProps.add(\"useProcessors\");\n        cpuLoadProbeProps.add(\"processorCoefficient\");\n        metadata.put(AdaptiveCpuLoadProbe.class, new MetadataInfo(cpuLoadProbeProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> adaptiveTimeProbeProps = new HashSet<>();\n        adaptiveTimeProbeProps.add(\"useAverage\");\n        metadata.put(AdaptiveProcessingTimeLoadProbe.class,\n            new MetadataInfo(adaptiveTimeProbeProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> optimizedMarshallerProps = new HashSet<>();\n        optimizedMarshallerProps.add(\"poolSize\");\n        optimizedMarshallerProps.add(\"requireSerializable\");\n\n        Set<String> optimizedMarshallerPropsExcl = new HashSet<>();\n        optimizedMarshallerPropsExcl.add(\"context\");\n\n        metadata.put(OptimizedMarshaller.class,\n            new MetadataInfo(optimizedMarshallerProps, EMPTY_FIELDS, optimizedMarshallerPropsExcl));\n\n        Set<String> memoryCfgProps = new HashSet<>();\n        memoryCfgProps.add(\"pageSize\");\n        memoryCfgProps.add(\"concurrencyLevel\");\n        memoryCfgProps.add(\"systemCacheInitialSize\");\n        memoryCfgProps.add(\"systemCacheMaxSize\");\n        memoryCfgProps.add(\"defaultMemoryPolicyName\");\n        memoryCfgProps.add(\"defaultMemoryPolicySize\");\n        memoryCfgProps.add(\"memoryPolicies\");\n        metadata.put(MemoryConfiguration.class, new MetadataInfo(EMPTY_FIELDS, memoryCfgProps, EMPTY_FIELDS));\n\n        Set<String> memoryPlcCfgProps = new HashSet<>();\n        memoryPlcCfgProps.add(\"name\");\n        memoryPlcCfgProps.add(\"initialSize\");\n        memoryPlcCfgProps.add(\"maxSize\");\n        memoryPlcCfgProps.add(\"swapFilePath\");\n        memoryPlcCfgProps.add(\"pageEvictionMode\");\n        memoryPlcCfgProps.add(\"evictionThreshold\");\n        memoryPlcCfgProps.add(\"emptyPagesPoolSize\");\n        memoryPlcCfgProps.add(\"subIntervals\");\n        memoryPlcCfgProps.add(\"rateTimeInterval\");\n        memoryPlcCfgProps.add(\"metricsEnabled\");\n        metadata.put(MemoryPolicyConfiguration.class, new MetadataInfo(EMPTY_FIELDS, memoryPlcCfgProps, EMPTY_FIELDS));\n\n        Set<String> odbcCfgProps = new HashSet<>();\n        odbcCfgProps.add(\"endpointAddress\");\n        odbcCfgProps.add(\"socketSendBufferSize\");\n        odbcCfgProps.add(\"socketReceiveBufferSize\");\n        odbcCfgProps.add(\"maxOpenCursors\");\n        odbcCfgProps.add(\"threadPoolSize\");\n        metadata.put(OdbcConfiguration.class, new MetadataInfo(EMPTY_FIELDS, odbcCfgProps, EMPTY_FIELDS));\n\n        Set<String> persistenceCfgProps = new HashSet<>();\n        persistenceCfgProps.add(\"persistentStorePath\");\n        persistenceCfgProps.add(\"metricsEnabled\");\n        persistenceCfgProps.add(\"alwaysWriteFullPages\");\n        persistenceCfgProps.add(\"checkpointingFrequency\");\n        persistenceCfgProps.add(\"checkpointingPageBufferSize\");\n        persistenceCfgProps.add(\"checkpointingThreads\");\n        persistenceCfgProps.add(\"walStorePath\");\n        persistenceCfgProps.add(\"walArchivePath\");\n        persistenceCfgProps.add(\"walSegments\");\n        persistenceCfgProps.add(\"walSegmentSize\");\n        persistenceCfgProps.add(\"walHistorySize\");\n        persistenceCfgProps.add(\"walFlushFrequency\");\n        persistenceCfgProps.add(\"walFsyncDelayNanos\");\n        persistenceCfgProps.add(\"walRecordIteratorBufferSize\");\n        persistenceCfgProps.add(\"lockWaitTime\");\n        persistenceCfgProps.add(\"rateTimeInterval\");\n        persistenceCfgProps.add(\"tlbSize\");\n        persistenceCfgProps.add(\"subIntervals\");\n        metadata.put(PersistentStoreConfiguration.class, new MetadataInfo(EMPTY_FIELDS, persistenceCfgProps, EMPTY_FIELDS));\n\n        Set<String> srvcCfgProps = new HashSet<>();\n        srvcCfgProps.add(\"name\");\n        srvcCfgProps.add(\"service\");\n        srvcCfgProps.add(\"maxPerNodeCount\");\n        srvcCfgProps.add(\"totalCount\");\n        // Field cache in model.\n        srvcCfgProps.add(\"cacheName\");\n        srvcCfgProps.add(\"affinityKey\");\n\n        Set<String> srvcCfgPropsExclude = new HashSet<>();\n        srvcCfgPropsExclude.add(\"nodeFilter\");\n\n        metadata.put(ServiceConfiguration.class, new MetadataInfo(srvcCfgProps, EMPTY_FIELDS, srvcCfgPropsExclude));\n\n        Set<String> sqlConnectorCfgProps = new HashSet<>();\n        sqlConnectorCfgProps.add(\"host\");\n        sqlConnectorCfgProps.add(\"port\");\n        sqlConnectorCfgProps.add(\"portRange\");\n        sqlConnectorCfgProps.add(\"socketSendBufferSize\");\n        sqlConnectorCfgProps.add(\"socketReceiveBufferSize\");\n        sqlConnectorCfgProps.add(\"maxOpenCursorsPerConnection\");\n        sqlConnectorCfgProps.add(\"threadPoolSize\");\n        sqlConnectorCfgProps.add(\"tcpNoDelay\");\n        metadata.put(SqlConnectorConfiguration.class, new MetadataInfo(EMPTY_FIELDS, sqlConnectorCfgProps, EMPTY_FIELDS));\n\n        Set<String> sslCfgProps = new HashSet<>();\n        sslCfgProps.add(\"keyAlgorithm\");\n        sslCfgProps.add(\"keyStoreFilePath\");\n        // Only on code generation.\n        sslCfgProps.add(\"keyStorePassword\");\n        sslCfgProps.add(\"keyStoreType\");\n        sslCfgProps.add(\"protocol\");\n        sslCfgProps.add(\"trustManagers\");\n        sslCfgProps.add(\"trustStoreFilePath\");\n        // Only on code generation.\n        sslCfgProps.add(\"trustStorePassword\");\n        sslCfgProps.add(\"trustStoreType\");\n        metadata.put(SslContextFactory.class, new MetadataInfo(sslCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> executorProps = new HashSet<>();\n        executorProps.add(\"name\");\n        executorProps.add(\"size\");\n        metadata.put(ExecutorConfiguration.class, new MetadataInfo(executorProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> transactionCfgProps = new HashSet<>();\n        transactionCfgProps.add(\"defaultTxConcurrency\");\n        transactionCfgProps.add(\"defaultTxIsolation\");\n        transactionCfgProps.add(\"defaultTxTimeout\");\n        transactionCfgProps.add(\"pessimisticTxLogLinger\");\n        transactionCfgProps.add(\"pessimisticTxLogSize\");\n        transactionCfgProps.add(\"txManagerFactory\");\n        metadata.put(TransactionConfiguration.class, new MetadataInfo(transactionCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        // Cache configuration.\n\n        Set<String> cacheCfgProps = new HashSet<>();\n        cacheCfgProps.add(\"name\");\n        cacheCfgProps.add(\"groupName\");\n        cacheCfgProps.add(\"cacheMode\");\n        cacheCfgProps.add(\"atomicityMode\");\n        cacheCfgProps.add(\"backups\");\n        cacheCfgProps.add(\"partitionLossPolicy\");\n        cacheCfgProps.add(\"readFromBackup\");\n        cacheCfgProps.add(\"copyOnRead\");\n        cacheCfgProps.add(\"isInvalidate\");\n        cacheCfgProps.add(\"affinityMapper\");\n        cacheCfgProps.add(\"topologyValidator\");\n        cacheCfgProps.add(\"maxConcurrentAsyncOperations\");\n        cacheCfgProps.add(\"defaultLockTimeout\");\n        cacheCfgProps.add(\"writeSynchronizationMode\");\n        cacheCfgProps.add(\"onheapCacheEnabled\");\n        cacheCfgProps.add(\"dataRegionName\");\n        // Removed since 2.0.\n        // cacheCfgProps.add(\"memoryMode\");\n        // cacheCfgProps.add(\"offHeapMode\");\n        // cacheCfgProps.add(\"offHeapMaxMemory\");\n        cacheCfgProps.add(\"evictionPolicy\");\n        cacheCfgProps.add(\"evictionFilter\");\n        // Removed since 2.0.\n        // cacheCfgProps.add(\"startSize\");\n        // cacheCfgProps.add(\"swapEnabled\");\n        cacheCfgProps.add(\"nearConfiguration\");\n        cacheCfgProps.add(\"sqlSchema\");\n        // Removed since 2.0.\n        // cacheCfgProps.add(\"sqlOnheapRowCacheSize\");\n        cacheCfgProps.add(\"queryDetailMetricsSize\");\n        cacheCfgProps.add(\"sqlFunctionClasses\");\n        // Removed since 2.0\n        // cacheCfgProps.add(\"snapshotableIndex\");\n        cacheCfgProps.add(\"sqlEscapeAll\");\n        cacheCfgProps.add(\"queryParallelism\");\n        cacheCfgProps.add(\"rebalanceMode\");\n        cacheCfgProps.add(\"rebalanceBatchSize\");\n        cacheCfgProps.add(\"rebalanceBatchesPrefetchCount\");\n        cacheCfgProps.add(\"rebalanceOrder\");\n        cacheCfgProps.add(\"rebalanceDelay\");\n        cacheCfgProps.add(\"rebalanceTimeout\");\n        cacheCfgProps.add(\"rebalanceThrottle\");\n        cacheCfgProps.add(\"statisticsEnabled\");\n        cacheCfgProps.add(\"managementEnabled\");\n        cacheCfgProps.add(\"cacheStoreFactory\");\n        cacheCfgProps.add(\"storeKeepBinary\");\n        cacheCfgProps.add(\"loadPreviousValue\");\n        cacheCfgProps.add(\"readThrough\");\n        cacheCfgProps.add(\"writeThrough\");\n        cacheCfgProps.add(\"writeBehindEnabled\");\n        cacheCfgProps.add(\"writeBehindBatchSize\");\n        cacheCfgProps.add(\"writeBehindFlushSize\");\n        cacheCfgProps.add(\"writeBehindFlushFrequency\");\n        cacheCfgProps.add(\"writeBehindFlushThreadCount\");\n        cacheCfgProps.add(\"writeBehindCoalescing\");\n        cacheCfgProps.add(\"indexedTypes\");\n        cacheCfgProps.add(\"queryEntities\");\n        cacheCfgProps.add(\"pluginConfigurations\");\n\n        Set<String> cacheCfgPropsDep = new HashSet<>();\n        // Removed since 2.0.\n        // cacheCfgPropsDep.add(\"atomicWriteOrderMode\");\n        cacheCfgPropsDep.add(\"memoryPolicyName\");\n        cacheCfgPropsDep.add(\"longQueryWarningTimeout\");\n\n        Set<String> cacheCfgPropsExcl = new HashSet<>();\n        cacheCfgPropsExcl.add(\"nodeFilter\");\n\n        metadata.put(CacheConfiguration.class, new MetadataInfo(cacheCfgProps, cacheCfgPropsDep, cacheCfgPropsExcl));\n\n        Set<String> rendezvousAffinityProps = new HashSet<>();\n        rendezvousAffinityProps.add(\"partitions\");\n        rendezvousAffinityProps.add(\"affinityBackupFilter\");\n        rendezvousAffinityProps.add(\"excludeNeighbors\");\n        metadata.put(RendezvousAffinityFunction.class, new MetadataInfo(rendezvousAffinityProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> nearCfgProps = new HashSet<>();\n        nearCfgProps.add(\"nearStartSize\");\n        nearCfgProps.add(\"nearEvictionPolicyFactory\");\n\n        Set<String> nearCfgPropsDep = new HashSet<>();\n        nearCfgPropsDep.add(\"nearEvictionPolicy\");\n\n        metadata.put(NearCacheConfiguration.class, new MetadataInfo(nearCfgProps, nearCfgPropsDep, EMPTY_FIELDS));\n\n        Set<String> jdbcPojoStoreProps = new HashSet<>();\n        // Only setter for dataSource field.\n        // jdbcPojoStoreProps.add(\"dataSourceBean\");\n        jdbcPojoStoreProps.add(\"dialect\");\n        jdbcPojoStoreProps.add(\"batchSize\");\n        jdbcPojoStoreProps.add(\"maximumPoolSize\");\n        jdbcPojoStoreProps.add(\"maximumWriteAttempts\");\n        jdbcPojoStoreProps.add(\"parallelLoadCacheMinimumThreshold\");\n        jdbcPojoStoreProps.add(\"hasher\");\n        jdbcPojoStoreProps.add(\"transformer\");\n        jdbcPojoStoreProps.add(\"sqlEscapeAll\");\n\n        // Configured via dataSource property.\n        Set<String> jdbcPojoStorePropsExcl = new HashSet<>();\n        jdbcPojoStorePropsExcl.add(\"dataSourceBean\");\n        jdbcPojoStorePropsExcl.add(\"dataSourceFactory\");\n\n        metadata.put(CacheJdbcPojoStoreFactory.class, new MetadataInfo(jdbcPojoStoreProps, EMPTY_FIELDS,\n            jdbcPojoStorePropsExcl));\n\n        Set<String> jdbcBlobStoreProps = new HashSet<>();\n        jdbcBlobStoreProps.add(\"connectionUrl\");\n        jdbcBlobStoreProps.add(\"user\");\n        // Only setter for dataSource.\n        // jdbcBlobStoreProps.add(\"dataSourceBean\");\n        // jdbcBlobStoreProps.add(\"dialect\");\n        jdbcBlobStoreProps.add(\"initSchema\");\n        jdbcBlobStoreProps.add(\"createTableQuery\");\n        jdbcBlobStoreProps.add(\"loadQuery\");\n        jdbcBlobStoreProps.add(\"insertQuery\");\n        jdbcBlobStoreProps.add(\"updateQuery\");\n        jdbcBlobStoreProps.add(\"deleteQuery\");\n        metadata.put(CacheJdbcBlobStore.class, new MetadataInfo(jdbcBlobStoreProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> hibernateBlobStoreProps = new HashSet<>();\n        hibernateBlobStoreProps.add(\"hibernateProperties\");\n        metadata.put(CacheHibernateBlobStore.class, new MetadataInfo(hibernateBlobStoreProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> igfsCfgProps = new HashSet<>();\n        igfsCfgProps.add(\"name\");\n        igfsCfgProps.add(\"defaultMode\");\n        // Removed since 2.0.\n        // igfsCfgProps.add(\"dualModeMaxPendingPutsSize\");\n        // igfsCfgProps.add(\"dualModePutExecutorService\");\n        // igfsCfgProps.add(\"dualModePutExecutorServiceShutdown\");\n        igfsCfgProps.add(\"fragmentizerEnabled\");\n        igfsCfgProps.add(\"fragmentizerConcurrentFiles\");\n        igfsCfgProps.add(\"fragmentizerThrottlingBlockLength\");\n        igfsCfgProps.add(\"fragmentizerThrottlingDelay\");\n        igfsCfgProps.add(\"ipcEndpointEnabled\");\n        igfsCfgProps.add(\"ipcEndpointConfiguration\");\n        igfsCfgProps.add(\"blockSize\");\n        // streamBufferSize field in model.\n        igfsCfgProps.add(\"bufferSize\");\n        // Removed since 2.0.\n        // igfsCfgProps.add(\"streamBufferSize\");\n        // igfsCfgProps.add(\"maxSpaceSize\");\n        igfsCfgProps.add(\"maximumTaskRangeLength\");\n        igfsCfgProps.add(\"managementPort\");\n        igfsCfgProps.add(\"perNodeBatchSize\");\n        igfsCfgProps.add(\"perNodeParallelBatchCount\");\n        igfsCfgProps.add(\"prefetchBlocks\");\n        igfsCfgProps.add(\"sequentialReadsBeforePrefetch\");\n        // Removed since 2.0.\n        // igfsCfgProps.add(\"trashPurgeTimeout\");\n        igfsCfgProps.add(\"colocateMetadata\");\n        igfsCfgProps.add(\"relaxedConsistency\");\n        igfsCfgProps.add(\"updateFileLengthOnFlush\");\n        igfsCfgProps.add(\"pathModes\");\n        igfsCfgProps.add(\"secondaryFileSystem\");\n\n        Set<String> igfsCfgPropsExclude = new HashSet<>();\n        igfsCfgPropsExclude.add(\"dataCacheConfiguration\");\n        igfsCfgPropsExclude.add(\"metaCacheConfiguration\");\n\n        metadata.put(FileSystemConfiguration.class, new MetadataInfo(igfsCfgProps, EMPTY_FIELDS, igfsCfgPropsExclude));\n\n        Set<String> igfsBlocMapperProps = new HashSet<>();\n        igfsBlocMapperProps.add(\"groupSize\");\n\n        metadata.put(IgfsGroupDataBlocksKeyMapper.class, new MetadataInfo(igfsBlocMapperProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> secHadoopIgfsCfgProps = new HashSet<>();\n        secHadoopIgfsCfgProps.add(\"defaultUserName\");\n        secHadoopIgfsCfgProps.add(\"fileSystemFactory\");\n\n        metadata.put(IgniteHadoopIgfsSecondaryFileSystem.class, new MetadataInfo(secHadoopIgfsCfgProps, EMPTY_FIELDS,\n            EMPTY_FIELDS));\n\n        Set<String> cachingIgfsCfgProps = new HashSet<>();\n        cachingIgfsCfgProps.add(\"uri\");\n        cachingIgfsCfgProps.add(\"configPaths\");\n\n        metadata.put(CachingHadoopFileSystemFactory.class, new MetadataInfo(cachingIgfsCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> ipcEndpointProps = new HashSet<>();\n        ipcEndpointProps.add(\"type\");\n        ipcEndpointProps.add(\"host\");\n        ipcEndpointProps.add(\"port\");\n        ipcEndpointProps.add(\"memorySize\");\n        ipcEndpointProps.add(\"threadCount\");\n        ipcEndpointProps.add(\"tokenDirectoryPath\");\n        metadata.put(IgfsIpcEndpointConfiguration.class, new MetadataInfo(ipcEndpointProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> qryEntityProps = new HashSet<>();\n        qryEntityProps.add(\"keyType\");\n        qryEntityProps.add(\"valueType\");\n        qryEntityProps.add(\"aliases\");\n        qryEntityProps.add(\"fields\");\n        qryEntityProps.add(\"indexes\");\n        qryEntityProps.add(\"tableName\");\n        qryEntityProps.add(\"keyFieldName\");\n        qryEntityProps.add(\"valueFieldName\");\n        qryEntityProps.add(\"keyFields\");\n        metadata.put(QueryEntity.class, new MetadataInfo(qryEntityProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> qryIdxProps = new HashSet<>();\n        qryIdxProps.add(\"name\");\n        qryIdxProps.add(\"indexType\");\n        qryIdxProps.add(\"fields\");\n\n        Set<String> qryIdxPropsExcl = new HashSet<>();\n        qryIdxPropsExcl.add(\"fieldNames\");\n\n        metadata.put(QueryIndex.class, new MetadataInfo(qryIdxProps, EMPTY_FIELDS, qryIdxPropsExcl));\n\n        Set<String> jdbcTypeProps = new HashSet<>();\n        jdbcTypeProps.add(\"cacheName\");\n        jdbcTypeProps.add(\"keyType\");\n        jdbcTypeProps.add(\"valueType\");\n        jdbcTypeProps.add(\"databaseSchema\");\n        jdbcTypeProps.add(\"databaseTable\");\n        jdbcTypeProps.add(\"keyFields\");\n        jdbcTypeProps.add(\"valueFields\");\n\n        metadata.put(JdbcType.class, new MetadataInfo(jdbcTypeProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> sorterEvictionProps = new HashSet<>();\n        sorterEvictionProps.add(\"batchSize\");\n        sorterEvictionProps.add(\"maxMemorySize\");\n        sorterEvictionProps.add(\"maxSize\");\n        metadata.put(SortedEvictionPolicy.class, new MetadataInfo(sorterEvictionProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> lruEvictionProps = new HashSet<>();\n        lruEvictionProps.add(\"batchSize\");\n        lruEvictionProps.add(\"maxMemorySize\");\n        lruEvictionProps.add(\"maxSize\");\n        metadata.put(LruEvictionPolicy.class, new MetadataInfo(lruEvictionProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> fifoEvictionProps = new HashSet<>();\n        fifoEvictionProps.add(\"batchSize\");\n        fifoEvictionProps.add(\"maxMemorySize\");\n        fifoEvictionProps.add(\"maxSize\");\n        metadata.put(FifoEvictionPolicy.class, new MetadataInfo(fifoEvictionProps, EMPTY_FIELDS, EMPTY_FIELDS));\n    }",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 +\n 216 +\n 217 +\n 218 +\n 219 +\n 220 +\n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449  \n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460  \n 461  \n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  \n 512  \n 513  \n 514  \n 515  \n 516  \n 517  \n 518  \n 519  \n 520  \n 521  \n 522  \n 523  \n 524  \n 525  \n 526  \n 527  \n 528  \n 529  \n 530  \n 531  \n 532  \n 533  \n 534  \n 535  \n 536  \n 537  \n 538  \n 539  \n 540  \n 541  \n 542  \n 543  \n 544  \n 545  \n 546  \n 547  \n 548  \n 549  \n 550  \n 551  \n 552  \n 553  \n 554  \n 555  \n 556  \n 557  \n 558  \n 559  \n 560  \n 561  \n 562  \n 563  \n 564  \n 565  \n 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632 +\n 633 +\n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661 +\n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673 +\n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711 +\n 712 +\n 713 +\n 714 +\n 715 +\n 716 +\n 717 +\n 718 +\n 719 +\n 720 +\n 721 +\n 722 +\n 723 +\n 724 +\n 725 +\n 726 +\n 727 +\n 728 +\n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735 +\n 736 +\n 737 +\n 738  \n 739  \n 740  \n 741 +\n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749  \n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757  \n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  \n 789  \n 790  \n 791  \n 792  \n 793  \n 794  \n 795  \n 796  \n 797  \n 798  \n 799  \n 800  \n 801  \n 802  \n 803  \n 804  \n 805  \n 806  \n 807  \n 808  \n 809  \n 810  \n 811  \n 812  \n 813  \n 814  \n 815  \n 816  \n 817  \n 818  \n 819  \n 820  \n 821  \n 822  \n 823  \n 824  \n 825  \n 826  \n 827  \n 828  \n 829  \n 830  \n 831  \n 832  \n 833  \n 834  \n 835  \n 836  \n 837  \n 838  \n 839  \n 840  \n 841  \n 842  \n 843  \n 844  \n 845  \n 846  \n 847  \n 848  \n 849  \n 850  \n 851  \n 852 +\n 853  \n 854  \n 855  \n 856 +\n 857 +\n 858 +\n 859 +\n 860 +\n 861 +\n 862 +\n 863 +\n 864 +\n 865 +\n 866 +\n 867 +\n 868 +\n 869 +\n 870 +\n 871 +\n 872 +\n 873 +\n 874 +\n 875 +\n 876 +\n 877 +\n 878 +\n 879 +\n 880 +\n 881 +\n 882 +\n 883 +\n 884  \n 885  \n 886  \n 887  \n 888  \n 889  \n 890  \n 891  \n 892  \n 893  \n 894  \n 895  \n 896  \n 897  \n 898  \n 899  \n 900  \n 901  \n 902  \n 903  \n 904  \n 905  \n 906  \n 907  \n 908  \n 909  \n 910  \n 911  \n 912  \n 913  \n 914  \n 915  \n 916  \n 917  \n 918  \n 919  \n 920  \n 921  \n 922  \n 923  \n 924  \n 925  \n 926  \n 927  \n 928  \n 929  \n 930  \n 931  \n 932  \n 933  \n 934  \n 935  \n 936  \n 937  \n 938  \n 939  \n 940  \n 941  \n 942  \n 943  ",
            "    /**\n     * Prepare metadata for properties, which are possible to configure.\n     */\n    @SuppressWarnings(\"deprecation\")\n    protected void prepareMetadata() {\n        // Cluster configuration.\n        Set<String> igniteCfgProps = new HashSet<>();\n        igniteCfgProps.add(\"cacheConfiguration\");\n        igniteCfgProps.add(\"discoverySpi\");\n        igniteCfgProps.add(\"localHost\");\n        igniteCfgProps.add(\"atomicConfiguration\");\n        igniteCfgProps.add(\"userAttributes\");\n        igniteCfgProps.add(\"binaryConfiguration\");\n        igniteCfgProps.add(\"cacheKeyConfiguration\");\n        igniteCfgProps.add(\"checkpointSpi\");\n        igniteCfgProps.add(\"collisionSpi\");\n        igniteCfgProps.add(\"communicationSpi\");\n        igniteCfgProps.add(\"networkTimeout\");\n        igniteCfgProps.add(\"networkSendRetryDelay\");\n        igniteCfgProps.add(\"networkSendRetryCount\");\n        igniteCfgProps.add(\"connectorConfiguration\");\n        igniteCfgProps.add(\"dataStorageConfiguration\");\n        igniteCfgProps.add(\"deploymentMode\");\n        igniteCfgProps.add(\"peerClassLoadingEnabled\");\n        igniteCfgProps.add(\"peerClassLoadingMissedResourcesCacheSize\");\n        igniteCfgProps.add(\"peerClassLoadingThreadPoolSize\");\n        igniteCfgProps.add(\"peerClassLoadingLocalClassPathExclude\");\n        igniteCfgProps.add(\"classLoader\");\n        igniteCfgProps.add(\"deploymentSpi\");\n        igniteCfgProps.add(\"eventStorageSpi\");\n        igniteCfgProps.add(\"includeEventTypes\");\n        igniteCfgProps.add(\"failureDetectionTimeout\");\n        igniteCfgProps.add(\"clientFailureDetectionTimeout\");\n        igniteCfgProps.add(\"failoverSpi\");\n        igniteCfgProps.add(\"hadoopConfiguration\");\n        igniteCfgProps.add(\"loadBalancingSpi\");\n        igniteCfgProps.add(\"marshalLocalJobs\");\n\n        // Removed since 2.0.\n        // igniteCfgProps.add(\"marshallerCacheKeepAliveTime\");\n        // igniteCfgProps.add(\"marshallerCacheThreadPoolSize\");\n\n        igniteCfgProps.add(\"metricsExpireTime\");\n        igniteCfgProps.add(\"metricsHistorySize\");\n        igniteCfgProps.add(\"metricsLogFrequency\");\n        igniteCfgProps.add(\"metricsUpdateFrequency\");\n        igniteCfgProps.add(\"workDirectory\");\n        igniteCfgProps.add(\"consistentId\");\n        igniteCfgProps.add(\"warmupClosure\");\n        igniteCfgProps.add(\"activeOnStart\");\n        igniteCfgProps.add(\"cacheSanityCheckEnabled\");\n        igniteCfgProps.add(\"longQueryWarningTimeout\");\n        igniteCfgProps.add(\"odbcConfiguration\");\n        igniteCfgProps.add(\"serviceConfiguration\");\n        igniteCfgProps.add(\"sqlConnectorConfiguration\");\n        igniteCfgProps.add(\"sslContextFactory\");\n\n        // Removed since 2.0.\n        // igniteCfgProps.add(\"swapSpaceSpi\");\n\n        igniteCfgProps.add(\"publicThreadPoolSize\");\n        igniteCfgProps.add(\"systemThreadPoolSize\");\n        igniteCfgProps.add(\"serviceThreadPoolSize\");\n        igniteCfgProps.add(\"managementThreadPoolSize\");\n        igniteCfgProps.add(\"igfsThreadPoolSize\");\n        igniteCfgProps.add(\"utilityCacheThreadPoolSize\");\n        igniteCfgProps.add(\"utilityCacheKeepAliveTime\");\n        igniteCfgProps.add(\"asyncCallbackPoolSize\");\n        igniteCfgProps.add(\"stripedPoolSize\");\n        igniteCfgProps.add(\"dataStreamerThreadPoolSize\");\n        igniteCfgProps.add(\"queryThreadPoolSize\");\n        igniteCfgProps.add(\"executorConfiguration\");\n\n        // Removed since 2.0.\n        // igniteCfgProps.add(\"clockSyncSamples\");\n        // igniteCfgProps.add(\"clockSyncFrequency\");\n\n        igniteCfgProps.add(\"timeServerPortBase\");\n        igniteCfgProps.add(\"timeServerPortRange\");\n        igniteCfgProps.add(\"transactionConfiguration\");\n        igniteCfgProps.add(\"clientConnectorConfiguration\");\n        igniteCfgProps.add(\"fileSystemConfiguration\");\n        igniteCfgProps.add(\"gridLogger\");\n        igniteCfgProps.add(\"pluginConfigurations\");\n        igniteCfgProps.add(\"mvccVacuumFrequency\");\n        igniteCfgProps.add(\"mvccVacuumThreadCount\");\n\n        Set<String> igniteCfgPropsDep = new HashSet<>();\n        igniteCfgPropsDep.add(\"gridName\");\n        igniteCfgPropsDep.add(\"lateAffinityAssignment\");\n        igniteCfgPropsDep.add(\"persistentStoreConfiguration\");\n        igniteCfgPropsDep.add(\"memoryConfiguration\");\n        igniteCfgPropsDep.add(\"marshaller\");\n        igniteCfgPropsDep.add(\"discoveryStartupDelay\");\n\n        Set<String> igniteCfgPropsExcl = new HashSet<>();\n        // igniteCfgPropsExcl.add(\"lifecycleBeans\");\n        igniteCfgPropsExcl.add(\"daemon\");\n        igniteCfgPropsExcl.add(\"clientMode\");\n        igniteCfgPropsExcl.add(\"indexingSpi\");\n        igniteCfgPropsExcl.add(\"nodeId\");\n\n        metadata.put(IgniteConfiguration.class,\n            new MetadataInfo(igniteCfgProps, igniteCfgPropsDep, igniteCfgPropsExcl));\n\n        Set<String> cacheKeyCfgProps = new HashSet<>();\n        cacheKeyCfgProps.add(\"typeName\");\n        cacheKeyCfgProps.add(\"affinityKeyFieldName\");\n\n        metadata.put(CacheKeyConfiguration.class, new MetadataInfo(cacheKeyCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> atomicCfgProps = new HashSet<>();\n        atomicCfgProps.add(\"cacheMode\");\n        atomicCfgProps.add(\"atomicSequenceReserveSize\");\n        atomicCfgProps.add(\"backups\");\n        atomicCfgProps.add(\"affinity\");\n\n        metadata.put(AtomicConfiguration.class, new MetadataInfo(atomicCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> binaryCfgProps = new HashSet<>();\n        binaryCfgProps.add(\"idMapper\");\n        binaryCfgProps.add(\"nameMapper\");\n        binaryCfgProps.add(\"serializer\");\n        binaryCfgProps.add(\"typeConfigurations\");\n        binaryCfgProps.add(\"compactFooter\");\n        metadata.put(BinaryConfiguration.class, new MetadataInfo(binaryCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> binaryTypeCfgProps = new HashSet<>();\n        binaryTypeCfgProps.add(\"typeName\");\n        binaryTypeCfgProps.add(\"idMapper\");\n        binaryTypeCfgProps.add(\"nameMapper\");\n        binaryTypeCfgProps.add(\"serializer\");\n        binaryTypeCfgProps.add(\"enum\");\n        binaryTypeCfgProps.add(\"enumValues\");\n        metadata.put(BinaryTypeConfiguration.class, new MetadataInfo(binaryTypeCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> sharedFsCheckpointProps = new HashSet<>();\n        sharedFsCheckpointProps.add(\"directoryPaths\");\n        metadata.put(SharedFsCheckpointSpi.class, new MetadataInfo(sharedFsCheckpointProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> s3CheckpointProps = new HashSet<>();\n        s3CheckpointProps.add(\"bucketNameSuffix\");\n        s3CheckpointProps.add(\"bucketEndpoint\");\n        s3CheckpointProps.add(\"sSEAlgorithm\");\n        s3CheckpointProps.add(\"checkpointListener\");\n        metadata.put(S3CheckpointSpi.class, new MetadataInfo(s3CheckpointProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> cacheCheckpointProps = new HashSet<>();\n        cacheCheckpointProps.add(\"cacheName\");\n        metadata.put(CacheCheckpointSpi.class, new MetadataInfo(cacheCheckpointProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> jdbcCheckpointProps = new HashSet<>();\n        // Only setter for dataSource.\n        // jdbcCheckpointProps.add(\"dataSourceBean\");\n        // jdbcCheckpointProps.add(\"dialect\");\n        jdbcCheckpointProps.add(\"checkpointListener\");\n        jdbcCheckpointProps.add(\"user\");\n        // Only on code generation.\n        jdbcCheckpointProps.add(\"pwd\");\n        jdbcCheckpointProps.add(\"checkpointTableName\");\n        jdbcCheckpointProps.add(\"numberOfRetries\");\n        jdbcCheckpointProps.add(\"keyFieldName\");\n        jdbcCheckpointProps.add(\"keyFieldType\");\n        jdbcCheckpointProps.add(\"valueFieldName\");\n        jdbcCheckpointProps.add(\"valueFieldType\");\n        jdbcCheckpointProps.add(\"expireDateFieldName\");\n        jdbcCheckpointProps.add(\"expireDateFieldType\");\n        metadata.put(JdbcCheckpointSpi.class, new MetadataInfo(jdbcCheckpointProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> cliConProps = new HashSet<>();\n        cliConProps.add(\"host\");\n        cliConProps.add(\"port\");\n        cliConProps.add(\"portRange\");\n        cliConProps.add(\"socketSendBufferSize\");\n        cliConProps.add(\"socketReceiveBufferSize\");\n        cliConProps.add(\"maxOpenCursorsPerConnection\");\n        cliConProps.add(\"threadPoolSize\");\n        cliConProps.add(\"tcpNoDelay\");\n        cliConProps.add(\"idleTimeout\");\n        cliConProps.add(\"sslEnabled\");\n        cliConProps.add(\"sslClientAuth\");\n        cliConProps.add(\"useIgniteSslContextFactory\");\n        cliConProps.add(\"sslContextFactory\");\n        cliConProps.add(\"jdbcEnabled\");\n        cliConProps.add(\"odbcEnabled\");\n        cliConProps.add(\"thinClientEnabled\");\n        metadata.put(ClientConnectorConfiguration.class, new MetadataInfo(cliConProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> jobStealingCollisionProps = new HashSet<>();\n        jobStealingCollisionProps.add(\"activeJobsThreshold\");\n        jobStealingCollisionProps.add(\"waitJobsThreshold\");\n        jobStealingCollisionProps.add(\"messageExpireTime\");\n        jobStealingCollisionProps.add(\"maximumStealingAttempts\");\n        jobStealingCollisionProps.add(\"stealingEnabled\");\n        jobStealingCollisionProps.add(\"externalCollisionListener\");\n        jobStealingCollisionProps.add(\"stealingAttributes\");\n        metadata.put(JobStealingCollisionSpi.class,\n            new MetadataInfo(jobStealingCollisionProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> priQueueCollisionProps = new HashSet<>();\n        priQueueCollisionProps.add(\"parallelJobsNumber\");\n        priQueueCollisionProps.add(\"waitingJobsNumber\");\n        priQueueCollisionProps.add(\"priorityAttributeKey\");\n        priQueueCollisionProps.add(\"jobPriorityAttributeKey\");\n        priQueueCollisionProps.add(\"defaultPriority\");\n        priQueueCollisionProps.add(\"starvationIncrement\");\n        priQueueCollisionProps.add(\"starvationPreventionEnabled\");\n        metadata.put(PriorityQueueCollisionSpi.class, new MetadataInfo(priQueueCollisionProps, EMPTY_FIELDS,\n            SPI_EXCLUDED_FIELDS));\n\n        Set<String> fifoQueueCollisionProps = new HashSet<>();\n        fifoQueueCollisionProps.add(\"parallelJobsNumber\");\n        fifoQueueCollisionProps.add(\"waitingJobsNumber\");\n        metadata.put(FifoQueueCollisionSpi.class,\n            new MetadataInfo(fifoQueueCollisionProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> commProps = new HashSet<>();\n        commProps.add(\"listener\");\n        commProps.add(\"localAddress\");\n        commProps.add(\"localPort\");\n        commProps.add(\"localPortRange\");\n        commProps.add(\"sharedMemoryPort\");\n        commProps.add(\"idleConnectionTimeout\");\n        commProps.add(\"connectTimeout\");\n        commProps.add(\"maxConnectTimeout\");\n        commProps.add(\"reconnectCount\");\n        commProps.add(\"socketSendBuffer\");\n        commProps.add(\"socketReceiveBuffer\");\n        commProps.add(\"slowClientQueueLimit\");\n        commProps.add(\"ackSendThreshold\");\n        commProps.add(\"messageQueueLimit\");\n        commProps.add(\"unacknowledgedMessagesBufferSize\");\n        commProps.add(\"socketWriteTimeout\");\n        commProps.add(\"selectorsCount\");\n        commProps.add(\"addressResolver\");\n        commProps.add(\"directBuffer\");\n        commProps.add(\"directSendBuffer\");\n        commProps.add(\"tcpNoDelay\");\n\n        Set<String> commPropsDep = new HashSet<>();\n        commPropsDep.add(\"discoveryStartupDelay\");\n\n        // Removed from configuration since ignite 2.3\n        Set<String> commPropsExcl = new HashSet<>();\n        commPropsExcl.add(\"discoveryStartupDelay\");\n        commPropsExcl.addAll(SPI_EXCLUDED_FIELDS);\n\n        metadata.put(TcpCommunicationSpi.class,\n            new MetadataInfo(commProps, commPropsDep, commPropsExcl));\n\n        Set<String> discoverySpiProps = new HashSet<>();\n        discoverySpiProps.add(\"ipFinder\");\n        discoverySpiProps.add(\"localAddress\");\n        discoverySpiProps.add(\"localPort\");\n        discoverySpiProps.add(\"localPortRange\");\n        discoverySpiProps.add(\"addressResolver\");\n        discoverySpiProps.add(\"socketTimeout\");\n        discoverySpiProps.add(\"ackTimeout\");\n        discoverySpiProps.add(\"maxAckTimeout\");\n        discoverySpiProps.add(\"networkTimeout\");\n        discoverySpiProps.add(\"joinTimeout\");\n        discoverySpiProps.add(\"threadPriority\");\n        // Removed since 2.0.\n        // discoverySpiProps.add(\"heartbeatFrequency\");\n        // discoverySpiProps.add(\"maxMissedHeartbeats\");\n        // discoverySpiProps.add(\"maxMissedClientHeartbeats\");\n        discoverySpiProps.add(\"topHistorySize\");\n        discoverySpiProps.add(\"listener\");\n        discoverySpiProps.add(\"dataExchange\");\n        discoverySpiProps.add(\"metricsProvider\");\n        discoverySpiProps.add(\"reconnectCount\");\n        discoverySpiProps.add(\"statisticsPrintFrequency\");\n        discoverySpiProps.add(\"ipFinderCleanFrequency\");\n        discoverySpiProps.add(\"authenticator\");\n        discoverySpiProps.add(\"forceServerMode\");\n        discoverySpiProps.add(\"clientReconnectDisabled\");\n        metadata.put(TcpDiscoverySpi.class, new MetadataInfo(discoverySpiProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> connectorProps = new HashSet<>();\n        connectorProps.add(\"jettyPath\");\n        connectorProps.add(\"host\");\n        connectorProps.add(\"port\");\n        connectorProps.add(\"portRange\");\n        connectorProps.add(\"idleQueryCursorTimeout\");\n        connectorProps.add(\"idleQueryCursorCheckFrequency\");\n        connectorProps.add(\"idleTimeout\");\n        connectorProps.add(\"receiveBufferSize\");\n        connectorProps.add(\"sendBufferSize\");\n        connectorProps.add(\"sendQueueLimit\");\n        connectorProps.add(\"directBuffer\");\n        connectorProps.add(\"noDelay\");\n        connectorProps.add(\"selectorCount\");\n        connectorProps.add(\"threadPoolSize\");\n        connectorProps.add(\"messageInterceptor\");\n        connectorProps.add(\"secretKey\");\n        connectorProps.add(\"sslEnabled\");\n        connectorProps.add(\"sslClientAuth\");\n        connectorProps.add(\"sslFactory\");\n        metadata.put(ConnectorConfiguration.class, new MetadataInfo(connectorProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> dataStorageProps = new HashSet<>();\n        dataStorageProps.add(\"pageSize\");\n        dataStorageProps.add(\"concurrencyLevel\");\n        dataStorageProps.add(\"systemRegionInitialSize\");\n        dataStorageProps.add(\"systemRegionMaxSize\");\n        dataStorageProps.add(\"defaultDataRegionConfiguration\");\n        dataStorageProps.add(\"dataRegionConfigurations\");\n        dataStorageProps.add(\"storagePath\");\n        dataStorageProps.add(\"checkpointFrequency\");\n        dataStorageProps.add(\"checkpointThreads\");\n        dataStorageProps.add(\"checkpointWriteOrder\");\n        dataStorageProps.add(\"walMode\");\n        dataStorageProps.add(\"walPath\");\n        dataStorageProps.add(\"walArchivePath\");\n        dataStorageProps.add(\"walSegments\");\n        dataStorageProps.add(\"walSegmentSize\");\n        dataStorageProps.add(\"walHistorySize\");\n        dataStorageProps.add(\"walBufferSize\");\n        dataStorageProps.add(\"walFlushFrequency\");\n        dataStorageProps.add(\"walFsyncDelayNanos\");\n        dataStorageProps.add(\"walRecordIteratorBufferSize\");\n        dataStorageProps.add(\"lockWaitTime\");\n        dataStorageProps.add(\"walThreadLocalBufferSize\");\n        dataStorageProps.add(\"metricsSubIntervalCount\");\n        dataStorageProps.add(\"metricsRateTimeInterval\");\n        dataStorageProps.add(\"fileIOFactory\");\n        dataStorageProps.add(\"walAutoArchiveAfterInactivity\");\n        dataStorageProps.add(\"metricsEnabled\");\n        dataStorageProps.add(\"alwaysWriteFullPages\");\n        dataStorageProps.add(\"writeThrottlingEnabled\");\n        dataStorageProps.add(\"walCompactionEnabled\");\n        metadata.put(DataStorageConfiguration.class, new MetadataInfo(dataStorageProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> dataRegionProps = new HashSet<>();\n        dataRegionProps.add(\"name\");\n        dataRegionProps.add(\"initialSize\");\n        dataRegionProps.add(\"maxSize\");\n        dataRegionProps.add(\"swapPath\");\n        dataRegionProps.add(\"checkpointPageBufferSize\");\n        dataRegionProps.add(\"pageEvictionMode\");\n        dataRegionProps.add(\"evictionThreshold\");\n        dataRegionProps.add(\"emptyPagesPoolSize\");\n        dataRegionProps.add(\"metricsSubIntervalCount\");\n        dataRegionProps.add(\"metricsRateTimeInterval\");\n        dataRegionProps.add(\"metricsEnabled\");\n        dataRegionProps.add(\"persistenceEnabled\");\n        metadata.put(DataRegionConfiguration.class, new MetadataInfo(dataRegionProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> uriDeploymentProps = new HashSet<>();\n        uriDeploymentProps.add(\"uriList\");\n        uriDeploymentProps.add(\"temporaryDirectoryPath\");\n        uriDeploymentProps.add(\"scanners\");\n        uriDeploymentProps.add(\"listener\");\n        uriDeploymentProps.add(\"checkMd5\");\n        uriDeploymentProps.add(\"encodeUri\");\n        metadata.put(UriDeploymentSpi.class, new MetadataInfo(uriDeploymentProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> locDeploymentProps = new HashSet<>();\n        locDeploymentProps.add(\"listener\");\n        metadata.put(LocalDeploymentSpi.class, new MetadataInfo(locDeploymentProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> memoryEvtStorageProps = new HashSet<>();\n        memoryEvtStorageProps.add(\"expireAgeMs\");\n        memoryEvtStorageProps.add(\"expireCount\");\n        memoryEvtStorageProps.add(\"filter\");\n        metadata.put(MemoryEventStorageSpi.class, new MetadataInfo(memoryEvtStorageProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> alwaysFailoverProps = new HashSet<>();\n        alwaysFailoverProps.add(\"maximumFailoverAttempts\");\n        metadata.put(AlwaysFailoverSpi.class, new MetadataInfo(alwaysFailoverProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> jobStealingFailoverProps = new HashSet<>();\n        jobStealingFailoverProps.add(\"maximumFailoverAttempts\");\n        metadata.put(JobStealingFailoverSpi.class, new MetadataInfo(jobStealingFailoverProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> hadoopCfgProps = new HashSet<>();\n        hadoopCfgProps.add(\"mapReducePlanner\");\n        hadoopCfgProps.add(\"finishedJobInfoTtl\");\n        hadoopCfgProps.add(\"maxParallelTasks\");\n        hadoopCfgProps.add(\"maxTaskQueueSize\");\n        hadoopCfgProps.add(\"nativeLibraryNames\");\n        metadata.put(HadoopConfiguration.class, new MetadataInfo(hadoopCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> hadoopWeightMapReduceCfgProps = new HashSet<>();\n        hadoopWeightMapReduceCfgProps.add(\"localMapperWeight\");\n        hadoopWeightMapReduceCfgProps.add(\"remoteMapperWeight\");\n        hadoopWeightMapReduceCfgProps.add(\"localReducerWeight\");\n        hadoopWeightMapReduceCfgProps.add(\"remoteReducerWeight\");\n        hadoopWeightMapReduceCfgProps.add(\"preferLocalReducerThresholdWeight\");\n        metadata.put(IgniteHadoopWeightedMapReducePlanner.class,\n            new MetadataInfo(hadoopWeightMapReduceCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> weightedRndLoadBalancingProps = new HashSet<>();\n        weightedRndLoadBalancingProps.add(\"nodeWeight\");\n        weightedRndLoadBalancingProps.add(\"useWeights\");\n        metadata.put(WeightedRandomLoadBalancingSpi.class,\n            new MetadataInfo(weightedRndLoadBalancingProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> adaptiveLoadBalancingProps = new HashSet<>();\n        adaptiveLoadBalancingProps.add(\"loadProbe\");\n        metadata.put(AdaptiveLoadBalancingSpi.class,\n            new MetadataInfo(adaptiveLoadBalancingProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> roundRobinLoadBalancingProps = new HashSet<>();\n        roundRobinLoadBalancingProps.add(\"perTask\");\n        metadata.put(RoundRobinLoadBalancingSpi.class,\n            new MetadataInfo(roundRobinLoadBalancingProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> jobCntProbeProps = new HashSet<>();\n        jobCntProbeProps.add(\"useAverage\");\n        metadata.put(AdaptiveJobCountLoadProbe.class, new MetadataInfo(jobCntProbeProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> cpuLoadProbeProps = new HashSet<>();\n        cpuLoadProbeProps.add(\"useAverage\");\n        cpuLoadProbeProps.add(\"useProcessors\");\n        cpuLoadProbeProps.add(\"processorCoefficient\");\n        metadata.put(AdaptiveCpuLoadProbe.class, new MetadataInfo(cpuLoadProbeProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> adaptiveTimeProbeProps = new HashSet<>();\n        adaptiveTimeProbeProps.add(\"useAverage\");\n        metadata.put(AdaptiveProcessingTimeLoadProbe.class,\n            new MetadataInfo(adaptiveTimeProbeProps, EMPTY_FIELDS, SPI_EXCLUDED_FIELDS));\n\n        Set<String> optimizedMarshallerProps = new HashSet<>();\n        optimizedMarshallerProps.add(\"poolSize\");\n        optimizedMarshallerProps.add(\"requireSerializable\");\n\n        Set<String> optimizedMarshallerPropsExcl = new HashSet<>();\n        optimizedMarshallerPropsExcl.add(\"context\");\n\n        metadata.put(OptimizedMarshaller.class,\n            new MetadataInfo(optimizedMarshallerProps, EMPTY_FIELDS, optimizedMarshallerPropsExcl));\n\n        Set<String> memoryCfgProps = new HashSet<>();\n        memoryCfgProps.add(\"pageSize\");\n        memoryCfgProps.add(\"concurrencyLevel\");\n        memoryCfgProps.add(\"systemCacheInitialSize\");\n        memoryCfgProps.add(\"systemCacheMaxSize\");\n        memoryCfgProps.add(\"defaultMemoryPolicyName\");\n        memoryCfgProps.add(\"defaultMemoryPolicySize\");\n        memoryCfgProps.add(\"memoryPolicies\");\n        metadata.put(MemoryConfiguration.class, new MetadataInfo(EMPTY_FIELDS, memoryCfgProps, EMPTY_FIELDS));\n\n        Set<String> memoryPlcCfgProps = new HashSet<>();\n        memoryPlcCfgProps.add(\"name\");\n        memoryPlcCfgProps.add(\"initialSize\");\n        memoryPlcCfgProps.add(\"maxSize\");\n        memoryPlcCfgProps.add(\"swapFilePath\");\n        memoryPlcCfgProps.add(\"pageEvictionMode\");\n        memoryPlcCfgProps.add(\"evictionThreshold\");\n        memoryPlcCfgProps.add(\"emptyPagesPoolSize\");\n        memoryPlcCfgProps.add(\"subIntervals\");\n        memoryPlcCfgProps.add(\"rateTimeInterval\");\n        memoryPlcCfgProps.add(\"metricsEnabled\");\n        metadata.put(MemoryPolicyConfiguration.class, new MetadataInfo(EMPTY_FIELDS, memoryPlcCfgProps, EMPTY_FIELDS));\n\n        Set<String> odbcCfgProps = new HashSet<>();\n        odbcCfgProps.add(\"endpointAddress\");\n        odbcCfgProps.add(\"socketSendBufferSize\");\n        odbcCfgProps.add(\"socketReceiveBufferSize\");\n        odbcCfgProps.add(\"maxOpenCursors\");\n        odbcCfgProps.add(\"threadPoolSize\");\n        metadata.put(OdbcConfiguration.class, new MetadataInfo(EMPTY_FIELDS, odbcCfgProps, EMPTY_FIELDS));\n\n        Set<String> persistenceCfgProps = new HashSet<>();\n        persistenceCfgProps.add(\"persistentStorePath\");\n        persistenceCfgProps.add(\"metricsEnabled\");\n        persistenceCfgProps.add(\"alwaysWriteFullPages\");\n        persistenceCfgProps.add(\"checkpointingFrequency\");\n        persistenceCfgProps.add(\"checkpointingPageBufferSize\");\n        persistenceCfgProps.add(\"checkpointingThreads\");\n        persistenceCfgProps.add(\"walStorePath\");\n        persistenceCfgProps.add(\"walArchivePath\");\n        persistenceCfgProps.add(\"walSegments\");\n        persistenceCfgProps.add(\"walSegmentSize\");\n        persistenceCfgProps.add(\"walHistorySize\");\n        persistenceCfgProps.add(\"walFlushFrequency\");\n        persistenceCfgProps.add(\"walFsyncDelayNanos\");\n        persistenceCfgProps.add(\"walRecordIteratorBufferSize\");\n        persistenceCfgProps.add(\"lockWaitTime\");\n        persistenceCfgProps.add(\"rateTimeInterval\");\n        persistenceCfgProps.add(\"tlbSize\");\n        persistenceCfgProps.add(\"subIntervals\");\n        metadata.put(PersistentStoreConfiguration.class, new MetadataInfo(EMPTY_FIELDS, persistenceCfgProps, EMPTY_FIELDS));\n\n        Set<String> srvcCfgProps = new HashSet<>();\n        srvcCfgProps.add(\"name\");\n        srvcCfgProps.add(\"service\");\n        srvcCfgProps.add(\"maxPerNodeCount\");\n        srvcCfgProps.add(\"totalCount\");\n        // Field cache in model.\n        srvcCfgProps.add(\"cacheName\");\n        srvcCfgProps.add(\"affinityKey\");\n\n        Set<String> srvcCfgPropsExclude = new HashSet<>();\n        srvcCfgPropsExclude.add(\"nodeFilter\");\n\n        metadata.put(ServiceConfiguration.class, new MetadataInfo(srvcCfgProps, EMPTY_FIELDS, srvcCfgPropsExclude));\n\n        Set<String> sqlConnectorCfgProps = new HashSet<>();\n        sqlConnectorCfgProps.add(\"host\");\n        sqlConnectorCfgProps.add(\"port\");\n        sqlConnectorCfgProps.add(\"portRange\");\n        sqlConnectorCfgProps.add(\"socketSendBufferSize\");\n        sqlConnectorCfgProps.add(\"socketReceiveBufferSize\");\n        sqlConnectorCfgProps.add(\"maxOpenCursorsPerConnection\");\n        sqlConnectorCfgProps.add(\"threadPoolSize\");\n        sqlConnectorCfgProps.add(\"tcpNoDelay\");\n        metadata.put(SqlConnectorConfiguration.class, new MetadataInfo(EMPTY_FIELDS, sqlConnectorCfgProps, EMPTY_FIELDS));\n\n        Set<String> sslCfgProps = new HashSet<>();\n        sslCfgProps.add(\"keyAlgorithm\");\n        sslCfgProps.add(\"keyStoreFilePath\");\n        // Only on code generation.\n        sslCfgProps.add(\"keyStorePassword\");\n        sslCfgProps.add(\"keyStoreType\");\n        sslCfgProps.add(\"protocol\");\n        sslCfgProps.add(\"trustManagers\");\n        sslCfgProps.add(\"trustStoreFilePath\");\n        // Only on code generation.\n        sslCfgProps.add(\"trustStorePassword\");\n        sslCfgProps.add(\"trustStoreType\");\n        sslCfgProps.add(\"cipherSuites\");\n        sslCfgProps.add(\"protocols\");\n        metadata.put(SslContextFactory.class, new MetadataInfo(sslCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> executorProps = new HashSet<>();\n        executorProps.add(\"name\");\n        executorProps.add(\"size\");\n        metadata.put(ExecutorConfiguration.class, new MetadataInfo(executorProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> transactionCfgProps = new HashSet<>();\n        transactionCfgProps.add(\"defaultTxConcurrency\");\n        transactionCfgProps.add(\"defaultTxIsolation\");\n        transactionCfgProps.add(\"defaultTxTimeout\");\n        transactionCfgProps.add(\"pessimisticTxLogLinger\");\n        transactionCfgProps.add(\"pessimisticTxLogSize\");\n        transactionCfgProps.add(\"txManagerFactory\");\n        metadata.put(TransactionConfiguration.class, new MetadataInfo(transactionCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        // Cache configuration.\n\n        Set<String> cacheCfgProps = new HashSet<>();\n        cacheCfgProps.add(\"name\");\n        cacheCfgProps.add(\"groupName\");\n        cacheCfgProps.add(\"cacheMode\");\n        cacheCfgProps.add(\"atomicityMode\");\n        cacheCfgProps.add(\"backups\");\n        cacheCfgProps.add(\"partitionLossPolicy\");\n        cacheCfgProps.add(\"readFromBackup\");\n        cacheCfgProps.add(\"copyOnRead\");\n        cacheCfgProps.add(\"invalidate\");\n        cacheCfgProps.add(\"affinityMapper\");\n        cacheCfgProps.add(\"topologyValidator\");\n        cacheCfgProps.add(\"maxConcurrentAsyncOperations\");\n        cacheCfgProps.add(\"defaultLockTimeout\");\n        cacheCfgProps.add(\"writeSynchronizationMode\");\n        cacheCfgProps.add(\"onheapCacheEnabled\");\n        cacheCfgProps.add(\"dataRegionName\");\n        // Removed since 2.0.\n        // cacheCfgProps.add(\"memoryMode\");\n        // cacheCfgProps.add(\"offHeapMode\");\n        // cacheCfgProps.add(\"offHeapMaxMemory\");\n        cacheCfgProps.add(\"evictionPolicyFactory\");\n        cacheCfgProps.add(\"evictionFilter\");\n        // Removed since 2.0.\n        // cacheCfgProps.add(\"startSize\");\n        // cacheCfgProps.add(\"swapEnabled\");\n        cacheCfgProps.add(\"nearConfiguration\");\n        cacheCfgProps.add(\"sqlSchema\");\n        // Removed since 2.0.\n        // cacheCfgProps.add(\"sqlOnheapRowCacheSize\");\n        cacheCfgProps.add(\"queryDetailMetricsSize\");\n        cacheCfgProps.add(\"sqlFunctionClasses\");\n        // Removed since 2.0\n        // cacheCfgProps.add(\"snapshotableIndex\");\n        cacheCfgProps.add(\"sqlEscapeAll\");\n        cacheCfgProps.add(\"queryParallelism\");\n        cacheCfgProps.add(\"rebalanceMode\");\n        cacheCfgProps.add(\"rebalanceBatchSize\");\n        cacheCfgProps.add(\"rebalanceBatchesPrefetchCount\");\n        cacheCfgProps.add(\"rebalanceOrder\");\n        cacheCfgProps.add(\"rebalanceDelay\");\n        cacheCfgProps.add(\"rebalanceTimeout\");\n        cacheCfgProps.add(\"rebalanceThrottle\");\n        cacheCfgProps.add(\"statisticsEnabled\");\n        cacheCfgProps.add(\"managementEnabled\");\n        cacheCfgProps.add(\"cacheStoreFactory\");\n        cacheCfgProps.add(\"storeKeepBinary\");\n        cacheCfgProps.add(\"loadPreviousValue\");\n        cacheCfgProps.add(\"readThrough\");\n        cacheCfgProps.add(\"writeThrough\");\n        cacheCfgProps.add(\"writeBehindEnabled\");\n        cacheCfgProps.add(\"writeBehindBatchSize\");\n        cacheCfgProps.add(\"writeBehindFlushSize\");\n        cacheCfgProps.add(\"writeBehindFlushFrequency\");\n        cacheCfgProps.add(\"writeBehindFlushThreadCount\");\n        cacheCfgProps.add(\"writeBehindCoalescing\");\n        cacheCfgProps.add(\"indexedTypes\");\n        cacheCfgProps.add(\"queryEntities\");\n        cacheCfgProps.add(\"pluginConfigurations\");\n        cacheCfgProps.add(\"cacheWriterFactory\");\n        cacheCfgProps.add(\"cacheLoaderFactory\");\n        cacheCfgProps.add(\"expiryPolicyFactory\");\n        cacheCfgProps.add(\"storeConcurrentLoadAllThreshold\");\n        cacheCfgProps.add(\"sqlIndexMaxInlineSize\");\n        cacheCfgProps.add(\"sqlOnheapCacheEnabled\");\n        cacheCfgProps.add(\"sqlOnheapCacheMaxSize\");\n        cacheCfgProps.add(\"diskPageCompression\");\n        cacheCfgProps.add(\"diskPageCompressionLevel\");\n        cacheCfgProps.add(\"interceptor\");\n        cacheCfgProps.add(\"storeByValue\");\n        cacheCfgProps.add(\"eagerTtl\");\n        cacheCfgProps.add(\"encryptionEnabled\");\n        cacheCfgProps.add(\"eventsDisabled\");\n        cacheCfgProps.add(\"maxQueryIteratorsCount\");\n        cacheCfgProps.add(\"keyConfiguration\");\n        cacheCfgProps.add(\"cacheStoreSessionListenerFactories\");\n        cacheCfgProps.add(\"affinity\");\n\n        Set<String> cacheCfgPropsDep = new HashSet<>();\n        // Removed since 2.0.\n        // cacheCfgPropsDep.add(\"atomicWriteOrderMode\");\n        cacheCfgPropsDep.add(\"memoryPolicyName\");\n        cacheCfgPropsDep.add(\"longQueryWarningTimeout\");\n        cacheCfgPropsDep.add(\"rebalanceThreadPoolSize\");\n        cacheCfgPropsDep.add(\"transactionManagerLookupClassName\");\n        cacheCfgPropsDep.add(\"evictionPolicy\");\n\n        Set<String> cacheCfgPropsExcl = new HashSet<>();\n        cacheCfgPropsExcl.add(\"nodeFilter\");\n        cacheCfgPropsExcl.add(\"types\");\n\n        metadata.put(CacheConfiguration.class, new MetadataInfo(cacheCfgProps, cacheCfgPropsDep, cacheCfgPropsExcl));\n\n        Set<String> rendezvousAffinityProps = new HashSet<>();\n        rendezvousAffinityProps.add(\"partitions\");\n        rendezvousAffinityProps.add(\"affinityBackupFilter\");\n        rendezvousAffinityProps.add(\"excludeNeighbors\");\n        metadata.put(RendezvousAffinityFunction.class, new MetadataInfo(rendezvousAffinityProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> nearCfgProps = new HashSet<>();\n        nearCfgProps.add(\"nearStartSize\");\n        nearCfgProps.add(\"nearEvictionPolicyFactory\");\n\n        Set<String> nearCfgPropsDep = new HashSet<>();\n        nearCfgPropsDep.add(\"nearEvictionPolicy\");\n\n        metadata.put(NearCacheConfiguration.class, new MetadataInfo(nearCfgProps, nearCfgPropsDep, EMPTY_FIELDS));\n\n        Set<String> jdbcPojoStoreProps = new HashSet<>();\n        // Only setter for dataSource field.\n        // jdbcPojoStoreProps.add(\"dataSourceBean\");\n        jdbcPojoStoreProps.add(\"dialect\");\n        jdbcPojoStoreProps.add(\"batchSize\");\n        jdbcPojoStoreProps.add(\"maximumPoolSize\");\n        jdbcPojoStoreProps.add(\"maximumWriteAttempts\");\n        jdbcPojoStoreProps.add(\"parallelLoadCacheMinimumThreshold\");\n        jdbcPojoStoreProps.add(\"hasher\");\n        jdbcPojoStoreProps.add(\"transformer\");\n        jdbcPojoStoreProps.add(\"sqlEscapeAll\");\n\n        // Configured via dataSource property.\n        Set<String> jdbcPojoStorePropsExcl = new HashSet<>();\n        jdbcPojoStorePropsExcl.add(\"dataSourceBean\");\n        jdbcPojoStorePropsExcl.add(\"dataSourceFactory\");\n\n        metadata.put(CacheJdbcPojoStoreFactory.class, new MetadataInfo(jdbcPojoStoreProps, EMPTY_FIELDS,\n            jdbcPojoStorePropsExcl));\n\n        Set<String> jdbcBlobStoreProps = new HashSet<>();\n        jdbcBlobStoreProps.add(\"connectionUrl\");\n        jdbcBlobStoreProps.add(\"user\");\n        // Only setter for dataSource.\n        // jdbcBlobStoreProps.add(\"dataSourceBean\");\n        // jdbcBlobStoreProps.add(\"dialect\");\n        jdbcBlobStoreProps.add(\"initSchema\");\n        jdbcBlobStoreProps.add(\"createTableQuery\");\n        jdbcBlobStoreProps.add(\"loadQuery\");\n        jdbcBlobStoreProps.add(\"insertQuery\");\n        jdbcBlobStoreProps.add(\"updateQuery\");\n        jdbcBlobStoreProps.add(\"deleteQuery\");\n        metadata.put(CacheJdbcBlobStore.class, new MetadataInfo(jdbcBlobStoreProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> hibernateBlobStoreProps = new HashSet<>();\n        hibernateBlobStoreProps.add(\"hibernateProperties\");\n        metadata.put(CacheHibernateBlobStore.class, new MetadataInfo(hibernateBlobStoreProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> igfsCfgProps = new HashSet<>();\n        igfsCfgProps.add(\"name\");\n        igfsCfgProps.add(\"defaultMode\");\n        // Removed since 2.0.\n        // igfsCfgProps.add(\"dualModeMaxPendingPutsSize\");\n        // igfsCfgProps.add(\"dualModePutExecutorService\");\n        // igfsCfgProps.add(\"dualModePutExecutorServiceShutdown\");\n        igfsCfgProps.add(\"fragmentizerEnabled\");\n        igfsCfgProps.add(\"fragmentizerConcurrentFiles\");\n        igfsCfgProps.add(\"fragmentizerThrottlingBlockLength\");\n        igfsCfgProps.add(\"fragmentizerThrottlingDelay\");\n        igfsCfgProps.add(\"ipcEndpointEnabled\");\n        igfsCfgProps.add(\"ipcEndpointConfiguration\");\n        igfsCfgProps.add(\"blockSize\");\n        // streamBufferSize field in model.\n        igfsCfgProps.add(\"bufferSize\");\n        // Removed since 2.0.\n        // igfsCfgProps.add(\"streamBufferSize\");\n        // igfsCfgProps.add(\"maxSpaceSize\");\n        igfsCfgProps.add(\"maximumTaskRangeLength\");\n        igfsCfgProps.add(\"managementPort\");\n        igfsCfgProps.add(\"perNodeBatchSize\");\n        igfsCfgProps.add(\"perNodeParallelBatchCount\");\n        igfsCfgProps.add(\"prefetchBlocks\");\n        igfsCfgProps.add(\"sequentialReadsBeforePrefetch\");\n        // Removed since 2.0.\n        // igfsCfgProps.add(\"trashPurgeTimeout\");\n        igfsCfgProps.add(\"colocateMetadata\");\n        igfsCfgProps.add(\"relaxedConsistency\");\n        igfsCfgProps.add(\"updateFileLengthOnFlush\");\n        igfsCfgProps.add(\"pathModes\");\n        igfsCfgProps.add(\"secondaryFileSystem\");\n\n        Set<String> igfsCfgPropsExclude = new HashSet<>();\n        igfsCfgPropsExclude.add(\"dataCacheConfiguration\");\n        igfsCfgPropsExclude.add(\"metaCacheConfiguration\");\n\n        metadata.put(FileSystemConfiguration.class, new MetadataInfo(igfsCfgProps, EMPTY_FIELDS, igfsCfgPropsExclude));\n\n        Set<String> igfsBlocMapperProps = new HashSet<>();\n        igfsBlocMapperProps.add(\"groupSize\");\n\n        metadata.put(IgfsGroupDataBlocksKeyMapper.class, new MetadataInfo(igfsBlocMapperProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> secHadoopIgfsCfgProps = new HashSet<>();\n        secHadoopIgfsCfgProps.add(\"defaultUserName\");\n        secHadoopIgfsCfgProps.add(\"fileSystemFactory\");\n\n        metadata.put(IgniteHadoopIgfsSecondaryFileSystem.class, new MetadataInfo(secHadoopIgfsCfgProps, EMPTY_FIELDS,\n            EMPTY_FIELDS));\n\n        Set<String> cachingIgfsCfgProps = new HashSet<>();\n        cachingIgfsCfgProps.add(\"uri\");\n        cachingIgfsCfgProps.add(\"configPaths\");\n        cachingIgfsCfgProps.add(\"userNameMapper\");\n\n        metadata.put(CachingHadoopFileSystemFactory.class, new MetadataInfo(cachingIgfsCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> kerberosIgfsCfgProps = new HashSet<>();\n        kerberosIgfsCfgProps.add(\"uri\");\n        kerberosIgfsCfgProps.add(\"configPaths\");\n        kerberosIgfsCfgProps.add(\"userNameMapper\");\n        kerberosIgfsCfgProps.add(\"keyTab\");\n        kerberosIgfsCfgProps.add(\"keyTabPrincipal\");\n        kerberosIgfsCfgProps.add(\"reloginInterval\");\n\n        metadata.put(KerberosHadoopFileSystemFactory.class, new MetadataInfo(kerberosIgfsCfgProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> chainedIgfsUsrNameMapperProps = new HashSet<>();\n        chainedIgfsUsrNameMapperProps.add(\"mappers\");\n\n        metadata.put(ChainedUserNameMapper.class, new MetadataInfo(chainedIgfsUsrNameMapperProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> basicIgfsUsrNameMapperProps = new HashSet<>();\n        basicIgfsUsrNameMapperProps.add(\"defaultUserName\");\n        basicIgfsUsrNameMapperProps.add(\"useDefaultUserName\");\n        basicIgfsUsrNameMapperProps.add(\"mappings\");\n\n        metadata.put(BasicUserNameMapper.class, new MetadataInfo(basicIgfsUsrNameMapperProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> kerberosIgfsUsrNameMapperProps = new HashSet<>();\n        kerberosIgfsUsrNameMapperProps.add(\"instance\");\n        kerberosIgfsUsrNameMapperProps.add(\"realm\");\n\n        metadata.put(KerberosUserNameMapper.class, new MetadataInfo(kerberosIgfsUsrNameMapperProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> ipcEndpointProps = new HashSet<>();\n        ipcEndpointProps.add(\"type\");\n        ipcEndpointProps.add(\"host\");\n        ipcEndpointProps.add(\"port\");\n        ipcEndpointProps.add(\"memorySize\");\n        ipcEndpointProps.add(\"threadCount\");\n        ipcEndpointProps.add(\"tokenDirectoryPath\");\n        metadata.put(IgfsIpcEndpointConfiguration.class, new MetadataInfo(ipcEndpointProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> qryEntityProps = new HashSet<>();\n        qryEntityProps.add(\"keyType\");\n        qryEntityProps.add(\"valueType\");\n        qryEntityProps.add(\"aliases\");\n        qryEntityProps.add(\"fields\");\n        qryEntityProps.add(\"indexes\");\n        qryEntityProps.add(\"tableName\");\n        qryEntityProps.add(\"keyFieldName\");\n        qryEntityProps.add(\"valueFieldName\");\n        qryEntityProps.add(\"keyFields\");\n        metadata.put(QueryEntity.class, new MetadataInfo(qryEntityProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> qryIdxProps = new HashSet<>();\n        qryIdxProps.add(\"name\");\n        qryIdxProps.add(\"indexType\");\n        qryIdxProps.add(\"fields\");\n\n        Set<String> qryIdxPropsExcl = new HashSet<>();\n        qryIdxPropsExcl.add(\"fieldNames\");\n\n        metadata.put(QueryIndex.class, new MetadataInfo(qryIdxProps, EMPTY_FIELDS, qryIdxPropsExcl));\n\n        Set<String> jdbcTypeProps = new HashSet<>();\n        jdbcTypeProps.add(\"cacheName\");\n        jdbcTypeProps.add(\"keyType\");\n        jdbcTypeProps.add(\"valueType\");\n        jdbcTypeProps.add(\"databaseSchema\");\n        jdbcTypeProps.add(\"databaseTable\");\n        jdbcTypeProps.add(\"keyFields\");\n        jdbcTypeProps.add(\"valueFields\");\n\n        metadata.put(JdbcType.class, new MetadataInfo(jdbcTypeProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> sorterEvictionProps = new HashSet<>();\n        sorterEvictionProps.add(\"batchSize\");\n        sorterEvictionProps.add(\"maxMemorySize\");\n        sorterEvictionProps.add(\"maxSize\");\n        metadata.put(SortedEvictionPolicy.class, new MetadataInfo(sorterEvictionProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> lruEvictionProps = new HashSet<>();\n        lruEvictionProps.add(\"batchSize\");\n        lruEvictionProps.add(\"maxMemorySize\");\n        lruEvictionProps.add(\"maxSize\");\n        metadata.put(LruEvictionPolicy.class, new MetadataInfo(lruEvictionProps, EMPTY_FIELDS, EMPTY_FIELDS));\n\n        Set<String> fifoEvictionProps = new HashSet<>();\n        fifoEvictionProps.add(\"batchSize\");\n        fifoEvictionProps.add(\"maxMemorySize\");\n        fifoEvictionProps.add(\"maxSize\");\n        metadata.put(FifoEvictionPolicy.class, new MetadataInfo(fifoEvictionProps, EMPTY_FIELDS, EMPTY_FIELDS));\n    }"
        ]
    ],
    "5c01c4199df3ba3a479d4566561335d57adffd0f": [
        [
            "GridDiscoveryManager::resolveDiscoCache(int,AffinityTopologyVersion)",
            "2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061  \n2062  \n2063  \n2064  \n2065  \n2066  \n2067  \n2068  \n2069  \n2070  \n2071  \n2072  ",
            "    /**\n     * Gets discovery cache for given topology version.\n     *\n     * @param grpId Cache group ID (participates in exception message).\n     * @param topVer Topology version.\n     * @return Discovery cache.\n     */\n    private DiscoCache resolveDiscoCache(int grpId, AffinityTopologyVersion topVer) {\n        Snapshot snap = topSnap.get();\n\n        DiscoCache cache = AffinityTopologyVersion.NONE.equals(topVer) || topVer.equals(snap.topVer) ?\n            snap.discoCache : discoCacheHist.get(topVer);\n\n        if (cache == null) {\n            CacheGroupDescriptor desc = ctx.cache().cacheGroupDescriptors().get(grpId);\n\n            throw new IgniteException(\"Failed to resolve nodes topology [\" +\n                \"cacheGrp=\" + (desc != null ? desc.cacheOrGroupName() : \"N/A\") +\n                \", topVer=\" + topVer +\n                \", history=\" + discoCacheHist.keySet() +\n                \", snap=\" + snap +\n                \", locNode=\" + ctx.discovery().localNode() + ']');\n        }\n\n        return cache;\n    }",
            "2047  \n2048  \n2049  \n2050  \n2051  \n2052  \n2053  \n2054  \n2055  \n2056  \n2057  \n2058  \n2059  \n2060  \n2061 +\n2062 +\n2063 +\n2064 +\n2065 +\n2066 +\n2067 +\n2068 +\n2069  \n2070  \n2071  \n2072  \n2073  \n2074  \n2075  \n2076  \n2077  \n2078  \n2079  \n2080  ",
            "    /**\n     * Gets discovery cache for given topology version.\n     *\n     * @param grpId Cache group ID (participates in exception message).\n     * @param topVer Topology version.\n     * @return Discovery cache.\n     */\n    private DiscoCache resolveDiscoCache(int grpId, AffinityTopologyVersion topVer) {\n        Snapshot snap = topSnap.get();\n\n        DiscoCache cache = AffinityTopologyVersion.NONE.equals(topVer) || topVer.equals(snap.topVer) ?\n            snap.discoCache : discoCacheHist.get(topVer);\n\n        if (cache == null) {\n            AffinityTopologyVersion lastAffChangedTopVer =\n                ctx.cache().context().exchange().lastAffinityChangedTopologyVersion(topVer);\n\n            DiscoCache lastAffChangedDiscoCache = discoCacheHist.get(lastAffChangedTopVer);\n\n            if (lastAffChangedDiscoCache != null)\n                return lastAffChangedDiscoCache;\n\n            CacheGroupDescriptor desc = ctx.cache().cacheGroupDescriptors().get(grpId);\n\n            throw new IgniteException(\"Failed to resolve nodes topology [\" +\n                \"cacheGrp=\" + (desc != null ? desc.cacheOrGroupName() : \"N/A\") +\n                \", topVer=\" + topVer +\n                \", history=\" + discoCacheHist.keySet() +\n                \", snap=\" + snap +\n                \", locNode=\" + ctx.discovery().localNode() + ']');\n        }\n\n        return cache;\n    }"
        ]
    ],
    "de3499fe773b493ca1f385473fdfad0897cb4002": [
        [
            "PageMemoryImpl::beforeReleaseWrite(FullPageId,long,boolean)",
            "1673  \n1674  \n1675  \n1676 -\n1677 -\n1678 -\n1679 -\n1680 -\n1681 -\n1682 -\n1683 -\n1684 -\n1685 -\n1686  ",
            "    /**\n     *\n     */\n    void beforeReleaseWrite(FullPageId pageId, long ptr, boolean pageWalRec) {\n        if (walMgr != null && (pageWalRec || walMgr.isAlwaysWriteFullPages()) && !walMgr.disabled(pageId.groupId())) {\n            try {\n                walMgr.log(new PageSnapshot(pageId, ptr, pageSize(), realPageSize(pageId.groupId())));\n            }\n            catch (IgniteCheckedException e) {\n                // TODO ignite-db.\n                throw new IgniteException(e);\n            }\n        }\n    }",
            "1681  \n1682  \n1683  \n1684 +\n1685 +\n1686 +\n1687 +\n1688 +\n1689 +\n1690  ",
            "    /**\n     *\n     */\n    void beforeReleaseWrite(FullPageId pageId, long ptr, boolean pageWalRec) throws IgniteCheckedException {\n        boolean walIsNotDisable = walMgr != null && !walMgr.disabled(pageId.groupId());\n        boolean pageRecOrAlwaysWriteFullPage = walMgr != null && (pageWalRec || walMgr.isAlwaysWriteFullPages());\n\n        if (pageRecOrAlwaysWriteFullPage && walIsNotDisable)\n            walMgr.log(new PageSnapshot(pageId, ptr, pageSize(), realPageSize(pageId.groupId())));\n    }"
        ],
        [
            "PageMemoryImpl::writeUnlockPage(long,FullPageId,Boolean,boolean,boolean)",
            "1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535 -\n1536 -\n1537 -\n1538 -\n1539 -\n1540  \n1541 -\n1542  \n1543 -\n1544 -\n1545  \n1546 -\n1547  \n1548 -\n1549  \n1550 -\n1551 -\n1552 -\n1553  \n1554 -\n1555 -\n1556  \n1557 -\n1558 -\n1559 -\n1560 -\n1561 -\n1562  \n1563 -\n1564  \n1565  ",
            "    /**\n     * @param page Page pointer.\n     * @param fullId full page ID.\n     * @param walPlc\n     * @param walPlc Full page WAL record policy.\n     * @param markDirty set dirty flag to page.\n     * @param restore\n     */\n    private void writeUnlockPage(\n        long page,\n        FullPageId fullId,\n        Boolean walPlc,\n        boolean markDirty,\n        boolean restore\n    ) {\n        boolean wasDirty = isDirty(page);\n\n        //if page is for restore, we shouldn't mark it as changed\n        if (!restore && markDirty && !wasDirty && changeTracker != null)\n            changeTracker.apply(page, fullId, this);\n\n        boolean pageWalRec = markDirty && walPlc != FALSE && (walPlc == TRUE || !wasDirty);\n\n        assert PageIO.getCrc(page + PAGE_OVERHEAD) == 0; //TODO GG-11480\n\n        if (markDirty)\n            setDirty(fullId, page, markDirty, false);\n\n        beforeReleaseWrite(fullId, page + PAGE_OVERHEAD, pageWalRec);\n\n        long pageId = PageIO.getPageId(page + PAGE_OVERHEAD);\n\n        assert pageId != 0 : U.hexLong(PageHeader.readPageId(page));\n        assert PageIO.getVersion(page + PAGE_OVERHEAD) != 0 : U.hexLong(pageId);\n        assert PageIO.getType(page + PAGE_OVERHEAD) != 0 : U.hexLong(pageId);\n\n        try {\n            rwLock.writeUnlock(page + PAGE_LOCK_OFFSET, PageIdUtils.tag(pageId));\n\n            if (throttlingPlc != ThrottlingPolicy.DISABLED && !restore && markDirty && !wasDirty)\n                writeThrottle.onMarkDirty(isInCheckpoint(fullId));\n        }\n        catch (AssertionError ex) {\n            U.error(log, \"Failed to unlock page [fullPageId=\" + fullId + \", binPage=\" + U.toHexString(page, systemPageSize()) + ']');\n\n            throw ex;\n        }\n    }",
            "1518  \n1519  \n1520  \n1521  \n1522  \n1523  \n1524  \n1525  \n1526  \n1527  \n1528  \n1529  \n1530  \n1531  \n1532  \n1533  \n1534  \n1535 +\n1536 +\n1537 +\n1538 +\n1539  \n1540 +\n1541  \n1542 +\n1543  \n1544 +\n1545 +\n1546  \n1547 +\n1548 +\n1549 +\n1550 +\n1551 +\n1552 +\n1553 +\n1554 +\n1555  \n1556 +\n1557 +\n1558 +\n1559  \n1560 +\n1561 +\n1562  \n1563 +\n1564 +\n1565 +\n1566 +\n1567 +\n1568 +\n1569  \n1570 +\n1571 +\n1572  \n1573  ",
            "    /**\n     * @param page Page pointer.\n     * @param fullId full page ID.\n     * @param walPlc\n     * @param walPlc Full page WAL record policy.\n     * @param markDirty set dirty flag to page.\n     * @param restore\n     */\n    private void writeUnlockPage(\n        long page,\n        FullPageId fullId,\n        Boolean walPlc,\n        boolean markDirty,\n        boolean restore\n    ) {\n        boolean wasDirty = isDirty(page);\n\n        try {\n            //if page is for restore, we shouldn't mark it as changed\n            if (!restore && markDirty && !wasDirty && changeTracker != null)\n                changeTracker.apply(page, fullId, this);\n\n            boolean pageWalRec = markDirty && walPlc != FALSE && (walPlc == TRUE || !wasDirty);\n\n            assert PageIO.getCrc(page + PAGE_OVERHEAD) == 0; //TODO GG-11480\n\n            if (markDirty)\n                setDirty(fullId, page, markDirty, false);\n\n            beforeReleaseWrite(fullId, page + PAGE_OVERHEAD, pageWalRec);\n        }\n        catch (IgniteCheckedException e) {\n            throw new IgniteException(e);\n        }\n        // Always release the lock.\n        finally {\n            long pageId = PageIO.getPageId(page + PAGE_OVERHEAD);\n\n            assert pageId != 0 : U.hexLong(PageHeader.readPageId(page));\n            assert PageIO.getVersion(page + PAGE_OVERHEAD) != 0 : U.hexLong(pageId);\n            assert PageIO.getType(page + PAGE_OVERHEAD) != 0 : U.hexLong(pageId);\n\n            try {\n                rwLock.writeUnlock(page + PAGE_LOCK_OFFSET, PageIdUtils.tag(pageId));\n\n                if (throttlingPlc != ThrottlingPolicy.DISABLED && !restore && markDirty && !wasDirty)\n                    writeThrottle.onMarkDirty(isInCheckpoint(fullId));\n            }\n            catch (AssertionError ex) {\n                U.error(log, \"Failed to unlock page [fullPageId=\" + fullId +\n                    \", binPage=\" + U.toHexString(page, systemPageSize()) + ']');\n\n                throw ex;\n            }\n        }\n    }"
        ]
    ],
    "bb56dc6d4843c426b0d5f0015abe8dc3af794276": [
        [
            "IgniteCompatibilityNodeRunner::main(String)",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103 -\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  ",
            "    /**\n     * Starts {@link Ignite} with test's default configuration.\n     *\n     * Command-line arguments specification:\n     * <pre>\n     * args[0] - required - path to closure for tuning IgniteConfiguration before node startup;\n     * args[1] - required - name of the starting node;\n     * args[2] - required - id of the starting node;\n     * args[3] - required - sync-id of a node for synchronization of startup. Must be equals\n     * to arg[2] in case of starting the first node in the Ignite cluster;\n     * args[4] - optional - path to closure for actions after node startup.\n     * </pre>\n     *\n     * @param args Command-line arguments.\n     * @throws Exception In case of an error.\n     */\n    public static void main(String[] args) throws Exception {\n        try {\n            X.println(GridJavaProcess.PID_MSG_PREFIX + U.jvmPid());\n\n            X.println(\"Starting Ignite Node... Args=\" + Arrays.toString(args));\n\n            if (args.length < 3) {\n                throw new IllegalArgumentException(\"At least four arguments expected:\" +\n                    \" [path/to/closure/file] [ignite-instance-name] [node-id] [sync-node-id] [optional/path/to/closure/file]\");\n            }\n\n            final Thread watchdog = delayedDumpClasspath();\n\n            IgniteConfiguration cfg = CompatibilityTestsFacade.getConfiguration();\n\n            IgniteInClosure<IgniteConfiguration> cfgClo = readClosureFromFileAndDelete(args[0]);\n\n            cfgClo.apply(cfg);\n\n            final UUID nodeId = UUID.fromString(args[2]);\n            final UUID syncNodeId = UUID.fromString(args[3]);\n\n            // Ignite instance name and id must be set according to arguments\n            // it's used for nodes managing: start, stop etc.\n            cfg.setIgniteInstanceName(args[1]);\n            cfg.setNodeId(nodeId);\n\n            final Ignite ignite = Ignition.start(cfg);\n\n            assert ignite.cluster().node(syncNodeId) != null : \"Node has not joined [id=\" + nodeId + \"]\";\n\n            // It needs to set private static field 'ignite' of the IgniteNodeRunner class via reflection\n            GridTestUtils.setFieldValue(new IgniteNodeRunner(), \"ignite\", ignite);\n\n            if (args.length == 5) {\n                IgniteInClosure<Ignite> clo = readClosureFromFileAndDelete(args[4]);\n\n                clo.apply(ignite);\n            }\n\n            X.println(IgniteCompatibilityAbstractTest.SYNCHRONIZATION_LOG_MESSAGE + nodeId);\n\n            watchdog.interrupt();\n        }\n        catch (Throwable e) {\n            e.printStackTrace();\n\n            X.println(\"Dumping classpath, error occurred: \" + e);\n\n            dumpClasspath();\n\n            throw e;\n        }\n    }",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78 +\n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91 +\n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103 +\n 104 +\n 105  \n 106  \n 107  \n 108 +\n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  ",
            "    /**\n     * Starts {@link Ignite} with test's default configuration.\n     *\n     * Command-line arguments specification:\n     * <pre>\n     * args[0] - required - path to closure for tuning IgniteConfiguration before node startup;\n     * args[1] - required - name of the starting node;\n     * args[2] - required - id of the starting node;\n     * args[3] - required - sync-id of a node for synchronization of startup. Must be equals\n     * to arg[2] in case of starting the first node in the Ignite cluster;\n     * args[4] - required - expected Ignite's version to check at startup;\n     * args[5] - optional - path to closure for actions after node startup.\n     * </pre>\n     *\n     * @param args Command-line arguments.\n     * @throws Exception In case of an error.\n     */\n    public static void main(String[] args) throws Exception {\n        try {\n            X.println(GridJavaProcess.PID_MSG_PREFIX + U.jvmPid());\n\n            X.println(\"Starting Ignite Node... Args=\" + Arrays.toString(args));\n\n            if (args.length < 3) {\n                throw new IllegalArgumentException(\"At least four arguments expected:\" +\n                    \" [path/to/closure/file] [ignite-instance-name] [node-id] [sync-node-id] [node-ver] [optional/path/to/closure/file]\");\n            }\n\n            final Thread watchdog = delayedDumpClasspath();\n\n            IgniteConfiguration cfg = CompatibilityTestsFacade.getConfiguration();\n\n            IgniteInClosure<IgniteConfiguration> cfgClo = readClosureFromFileAndDelete(args[0]);\n\n            cfgClo.apply(cfg);\n\n            final UUID nodeId = UUID.fromString(args[2]);\n            final UUID syncNodeId = UUID.fromString(args[3]);\n            final IgniteProductVersion expNodeVer = IgniteProductVersion.fromString(args[4]);\n\n            // Ignite instance name and id must be set according to arguments\n            // it's used for nodes managing: start, stop etc.\n            cfg.setIgniteInstanceName(args[1]);\n            cfg.setNodeId(nodeId);\n\n            final Ignite ignite = Ignition.start(cfg);\n\n            assert ignite.cluster().node(syncNodeId) != null : \"Node has not joined [id=\" + nodeId + \"]\";\n\n            assert ignite.cluster().localNode().version().compareToIgnoreTimestamp(expNodeVer) == 0 : \"Node is of unexpected \" +\n                \"version: [act=\" + ignite.cluster().localNode().version() + \", exp=\" + expNodeVer + ']';\n\n            // It needs to set private static field 'ignite' of the IgniteNodeRunner class via reflection\n            GridTestUtils.setFieldValue(new IgniteNodeRunner(), \"ignite\", ignite);\n\n            if (args.length == 6) {\n                IgniteInClosure<Ignite> clo = readClosureFromFileAndDelete(args[5]);\n\n                clo.apply(ignite);\n            }\n\n            X.println(IgniteCompatibilityAbstractTest.SYNCHRONIZATION_LOG_MESSAGE + nodeId);\n\n            watchdog.interrupt();\n        }\n        catch (Throwable e) {\n            e.printStackTrace();\n\n            X.println(\"Dumping classpath, error occurred: \" + e);\n\n            dumpClasspath();\n\n            throw e;\n        }\n    }"
        ],
        [
            "IgniteCompatibilityAbstractTest::startGrid(String,String,IgniteInClosure,IgniteInClosure)",
            " 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 -\n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  ",
            "    /**\n     * Starts new Ignite instance of given version and name <b>in separate JVM</b>.\n     *\n     * Uses an ignite-core artifact in the Maven local repository, if it isn't exists there, it will be downloaded and\n     * stored via Maven.\n     *\n     * @param igniteInstanceName Instance name.\n     * @param ver Ignite version. Dots separated, 3-digit version.\n     * @param cfgClo IgniteInClosure for post-configuration.\n     * @param clo IgniteInClosure for actions on started Ignite.\n     * @return Started grid.\n     * @throws Exception In case of an error.\n     */\n    protected IgniteEx startGrid(final String igniteInstanceName, final String ver,\n        IgniteInClosure<IgniteConfiguration> cfgClo, IgniteInClosure<Ignite> clo) throws Exception {\n        assert isMultiJvm() : \"MultiJvm mode must be switched on for the node stop properly.\";\n\n        assert !igniteInstanceName.equals(getTestIgniteInstanceName(0)) : \"Use default instance name for local nodes only.\";\n\n        final String cfgCloPath = IgniteCompatibilityNodeRunner.storeToFile(cfgClo);\n        final String cloPath = IgniteCompatibilityNodeRunner.storeToFile(clo);\n\n        final IgniteConfiguration cfg = getConfiguration(igniteInstanceName); // stub - won't be used at node startup\n\n        IgniteProcessProxy ignite = new IgniteProcessProxy(cfg, log, locJvmInstance, true) {\n            @Override protected IgniteLogger logger(IgniteLogger log, Object ctgr) {\n                return ListenedGridTestLog4jLogger.createLogger(ctgr + \"#\" + ver.replaceAll(\"\\\\.\", \"_\"));\n            }\n\n            @Override protected String igniteNodeRunnerClassName() throws Exception {\n                return IgniteCompatibilityNodeRunner.class.getCanonicalName();\n            }\n\n            @Override protected String params(IgniteConfiguration cfg, boolean resetDiscovery) throws Exception {\n                return cfgCloPath + \" \" + igniteInstanceName + \" \"\n                    + getId() + \" \"\n                    + (rmJvmInstance == null ? getId() : ((IgniteProcessProxy)rmJvmInstance).getId())\n                    + (cloPath == null ? \"\" : \" \" + cloPath);\n            }\n\n            @Override protected Collection<String> filteredJvmArgs() throws Exception {\n                Collection<String> filteredJvmArgs = new ArrayList<>();\n\n                filteredJvmArgs.add(\"-ea\");\n\n                for (String arg : U.jvmArgs()) {\n                    if (arg.startsWith(\"-Xmx\") || arg.startsWith(\"-Xms\"))\n                        filteredJvmArgs.add(arg);\n                }\n\n                final Collection<Dependency> dependencies = getDependencies(ver);\n\n                Set<String> excluded = getExcluded(ver, dependencies);\n\n                StringBuilder pathBuilder = new StringBuilder();\n\n                for (URL url : CompatibilityTestsUtils.classLoaderUrls(CLASS_LOADER)) {\n                    String path = url.getPath();\n\n                    if (excluded.stream().noneMatch(path::contains))\n                        pathBuilder.append(path).append(File.pathSeparator);\n                }\n\n                for (Dependency dependency : dependencies) {\n                    final String artifactVer = Optional.ofNullable(dependency.version()).orElse(ver);\n\n                    String pathToArtifact = MavenUtils.getPathToIgniteArtifact(dependency.groupId(),\n                        dependency.artifactId(), artifactVer, dependency.classifier());\n\n                    pathBuilder.append(pathToArtifact).append(File.pathSeparator);\n                }\n\n                filteredJvmArgs.add(\"-cp\");\n                filteredJvmArgs.add(pathBuilder.toString());\n\n                final Collection<String> jvmParms = getJvmParams();\n\n                if (jvmParms != null)\n                    filteredJvmArgs.addAll(jvmParms);\n\n                return filteredJvmArgs;\n            }\n        };\n\n        if (locJvmInstance == null) {\n            CountDownLatch nodeJoinedLatch = new CountDownLatch(1);\n\n            UUID nodeId = ignite.getId();\n\n            ListenedGridTestLog4jLogger log = (ListenedGridTestLog4jLogger)ignite.log();\n\n            log.addListener(nodeId, new LoggedJoinNodeClosure(nodeJoinedLatch, nodeId));\n\n            final long nodeJoinTimeout = getNodeJoinTimeout();\n            final boolean joined = nodeJoinedLatch.await(nodeJoinTimeout, TimeUnit.MILLISECONDS);\n\n            assertTrue(\"Node has not joined [id=\" + nodeId + \"]/\" +\n                \"or does not completed its startup during timeout: \" + nodeJoinTimeout + \" ms.\", joined);\n\n            log.removeListener(nodeId);\n        }\n\n        if (rmJvmInstance == null)\n            rmJvmInstance = ignite;\n\n        return ignite;\n    }",
            " 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156 +\n 157 +\n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "    /**\n     * Starts new Ignite instance of given version and name <b>in separate JVM</b>.\n     *\n     * Uses an ignite-core artifact in the Maven local repository, if it isn't exists there, it will be downloaded and\n     * stored via Maven.\n     *\n     * @param igniteInstanceName Instance name.\n     * @param ver Ignite version. Dots separated, 3-digit version.\n     * @param cfgClo IgniteInClosure for post-configuration.\n     * @param clo IgniteInClosure for actions on started Ignite.\n     * @return Started grid.\n     * @throws Exception In case of an error.\n     */\n    protected IgniteEx startGrid(final String igniteInstanceName, final String ver,\n        IgniteInClosure<IgniteConfiguration> cfgClo, IgniteInClosure<Ignite> clo) throws Exception {\n        assert isMultiJvm() : \"MultiJvm mode must be switched on for the node stop properly.\";\n\n        assert !igniteInstanceName.equals(getTestIgniteInstanceName(0)) : \"Use default instance name for local nodes only.\";\n\n        final String cfgCloPath = IgniteCompatibilityNodeRunner.storeToFile(cfgClo);\n        final String cloPath = IgniteCompatibilityNodeRunner.storeToFile(clo);\n\n        final IgniteConfiguration cfg = getConfiguration(igniteInstanceName); // stub - won't be used at node startup\n\n        IgniteProcessProxy ignite = new IgniteProcessProxy(cfg, log, locJvmInstance, true) {\n            @Override protected IgniteLogger logger(IgniteLogger log, Object ctgr) {\n                return ListenedGridTestLog4jLogger.createLogger(ctgr + \"#\" + ver.replaceAll(\"\\\\.\", \"_\"));\n            }\n\n            @Override protected String igniteNodeRunnerClassName() throws Exception {\n                return IgniteCompatibilityNodeRunner.class.getCanonicalName();\n            }\n\n            @Override protected String params(IgniteConfiguration cfg, boolean resetDiscovery) throws Exception {\n                return cfgCloPath + \" \" + igniteInstanceName + \" \"\n                    + getId() + \" \"\n                    + (rmJvmInstance == null ? getId() : ((IgniteProcessProxy)rmJvmInstance).getId()) + \" \"\n                    + ver\n                    + (cloPath == null ? \"\" : \" \" + cloPath);\n            }\n\n            @Override protected Collection<String> filteredJvmArgs() throws Exception {\n                Collection<String> filteredJvmArgs = new ArrayList<>();\n\n                filteredJvmArgs.add(\"-ea\");\n\n                for (String arg : U.jvmArgs()) {\n                    if (arg.startsWith(\"-Xmx\") || arg.startsWith(\"-Xms\"))\n                        filteredJvmArgs.add(arg);\n                }\n\n                final Collection<Dependency> dependencies = getDependencies(ver);\n\n                Set<String> excluded = getExcluded(ver, dependencies);\n\n                StringBuilder pathBuilder = new StringBuilder();\n\n                for (URL url : CompatibilityTestsUtils.classLoaderUrls(CLASS_LOADER)) {\n                    String path = url.getPath();\n\n                    if (excluded.stream().noneMatch(path::contains))\n                        pathBuilder.append(path).append(File.pathSeparator);\n                }\n\n                for (Dependency dependency : dependencies) {\n                    final String artifactVer = Optional.ofNullable(dependency.version()).orElse(ver);\n\n                    String pathToArtifact = MavenUtils.getPathToIgniteArtifact(dependency.groupId(),\n                        dependency.artifactId(), artifactVer, dependency.classifier());\n\n                    pathBuilder.append(pathToArtifact).append(File.pathSeparator);\n                }\n\n                filteredJvmArgs.add(\"-cp\");\n                filteredJvmArgs.add(pathBuilder.toString());\n\n                final Collection<String> jvmParms = getJvmParams();\n\n                if (jvmParms != null)\n                    filteredJvmArgs.addAll(jvmParms);\n\n                return filteredJvmArgs;\n            }\n        };\n\n        if (locJvmInstance == null) {\n            CountDownLatch nodeJoinedLatch = new CountDownLatch(1);\n\n            UUID nodeId = ignite.getId();\n\n            ListenedGridTestLog4jLogger log = (ListenedGridTestLog4jLogger)ignite.log();\n\n            log.addListener(nodeId, new LoggedJoinNodeClosure(nodeJoinedLatch, nodeId));\n\n            final long nodeJoinTimeout = getNodeJoinTimeout();\n            final boolean joined = nodeJoinedLatch.await(nodeJoinTimeout, TimeUnit.MILLISECONDS);\n\n            assertTrue(\"Node has not joined [id=\" + nodeId + \"]/\" +\n                \"or does not completed its startup during timeout: \" + nodeJoinTimeout + \" ms.\", joined);\n\n            log.removeListener(nodeId);\n        }\n\n        if (rmJvmInstance == null)\n            rmJvmInstance = ignite;\n\n        return ignite;\n    }"
        ]
    ],
    "29588c51556724940d92d6b077420f8f89baf34e": [
        [
            "GridCacheSharedTtlCleanupManager::CleanupWorker::body()",
            " 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137 -\n 138 -\n 139 -\n 140  \n 141 -\n 142 -\n 143  \n 144 -\n 145 -\n 146 -\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  ",
            "        /** {@inheritDoc} */\n        @Override protected void body() throws InterruptedException, IgniteInterruptedCheckedException {\n            Throwable err = null;\n\n            try {\n                while (!isCancelled()) {\n                    boolean expiredRemains = false;\n\n                    // TTL cleanup is allowed only when node joined to topology.\n                    if (!cctx.kernalContext().recoveryMode()) {\n                        for (GridCacheTtlManager mgr : mgrs) {\n                            updateHeartbeat();\n\n                            if (mgr.expire(CLEANUP_WORKER_ENTRIES_PROCESS_LIMIT))\n                                expiredRemains = true;\n\n                            if (isCancelled())\n                                return;\n                        }\n                    }\n\n                    updateHeartbeat();\n\n                    if (!expiredRemains)\n                        U.sleep(CLEANUP_WORKER_SLEEP_INTERVAL);\n\n                    onIdle();\n                }\n            }\n            catch (Throwable t) {\n                if (!(t instanceof IgniteInterruptedCheckedException))\n                    err = t;\n\n                throw t;\n            }\n            finally {\n                if (err == null && !isCancelled)\n                    err = new IllegalStateException(\"Thread \" + name() + \" is terminated unexpectedly\");\n\n                if (err instanceof OutOfMemoryError)\n                    cctx.kernalContext().failure().process(new FailureContext(CRITICAL_ERROR, err));\n                else if (err != null)\n                    cctx.kernalContext().failure().process(new FailureContext(SYSTEM_WORKER_TERMINATION, err));\n            }\n        }",
            " 128  \n 129  \n 130  \n 131  \n 132  \n 133 +\n 134 +\n 135 +\n 136 +\n 137  \n 138  \n 139  \n 140 +\n 141 +\n 142  \n 143 +\n 144 +\n 145  \n 146 +\n 147 +\n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  ",
            "        /** {@inheritDoc} */\n        @Override protected void body() throws InterruptedException, IgniteInterruptedCheckedException {\n            Throwable err = null;\n\n            try {\n                cctx.discovery().localJoin();\n\n                assert !cctx.kernalContext().recoveryMode();\n\n                while (!isCancelled()) {\n                    boolean expiredRemains = false;\n\n                    for (GridCacheTtlManager mgr : mgrs) {\n                        updateHeartbeat();\n\n                        if (mgr.expire(CLEANUP_WORKER_ENTRIES_PROCESS_LIMIT))\n                            expiredRemains = true;\n\n                        if (isCancelled())\n                            return;\n                    }\n\n                    updateHeartbeat();\n\n                    if (!expiredRemains)\n                        U.sleep(CLEANUP_WORKER_SLEEP_INTERVAL);\n\n                    onIdle();\n                }\n            }\n            catch (Throwable t) {\n                if (!(t instanceof IgniteInterruptedCheckedException))\n                    err = t;\n\n                throw t;\n            }\n            finally {\n                if (err == null && !isCancelled)\n                    err = new IllegalStateException(\"Thread \" + name() + \" is terminated unexpectedly\");\n\n                if (err instanceof OutOfMemoryError)\n                    cctx.kernalContext().failure().process(new FailureContext(CRITICAL_ERROR, err));\n                else if (err != null)\n                    cctx.kernalContext().failure().process(new FailureContext(SYSTEM_WORKER_TERMINATION, err));\n            }\n        }"
        ]
    ]
}