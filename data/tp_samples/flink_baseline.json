{
    "c93e04c16a7865dbbc90d89799b46f91831e0a55": [
        [
            "WebRuntimeMonitor::WebRuntimeMonitor(Configuration,LeaderRetrievalService,ActorSystem)",
            " 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300 -\n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  ",
            "\tpublic WebRuntimeMonitor(\n\t\t\tConfiguration config,\n\t\t\tLeaderRetrievalService leaderRetrievalService,\n\t\t\tActorSystem actorSystem) throws IOException, InterruptedException {\n\n\t\tthis.leaderRetrievalService = checkNotNull(leaderRetrievalService);\n\t\tthis.timeout = AkkaUtils.getTimeout(config);\n\t\tthis.retriever = new JobManagerRetriever(this, actorSystem, AkkaUtils.getTimeout(config), timeout);\n\t\t\n\t\tfinal WebMonitorConfig cfg = new WebMonitorConfig(config);\n\n\t\tfinal String configuredAddress = cfg.getWebFrontendAddress();\n\n\t\tfinal int configuredPort = cfg.getWebFrontendPort();\n\t\tif (configuredPort < 0) {\n\t\t\tthrow new IllegalArgumentException(\"Web frontend port is invalid: \" + configuredPort);\n\t\t}\n\t\t\n\t\tfinal WebMonitorUtils.LogFileLocation logFiles = WebMonitorUtils.LogFileLocation.find(config);\n\t\t\n\t\t// create an empty directory in temp for the web server\n\t\tString rootDirFileName = \"flink-web-\" + UUID.randomUUID();\n\t\twebRootDir = new File(getBaseDir(config), rootDirFileName);\n\t\tLOG.info(\"Using directory {} for the web interface files\", webRootDir);\n\n\t\tfinal boolean webSubmitAllow = cfg.isProgramSubmitEnabled();\n\t\tif (webSubmitAllow) {\n\t\t\t// create storage for uploads\n\t\t\tthis.uploadDir = getUploadDir(config);\n\t\t\t// the upload directory should either 1. exist and writable or 2. can be created and writable\n\t\t\tif (!(uploadDir.exists() && uploadDir.canWrite()) && !(uploadDir.mkdir() && uploadDir.canWrite())) {\n\t\t\t\tthrow new IOException(\n\t\t\t\t\tString.format(\"Jar upload directory %s cannot be created or is not writable.\",\n\t\t\t\t\t\tuploadDir.getAbsolutePath()));\n\t\t\t}\n\t\t\tLOG.info(\"Using directory {} for web frontend JAR file uploads\", uploadDir);\n\t\t}\n\t\telse {\n\t\t\tthis.uploadDir = null;\n\t\t}\n\n\t\tExecutionGraphHolder currentGraphs = new ExecutionGraphHolder();\n\n\t\t// - Back pressure stats ----------------------------------------------\n\n\t\tstackTraceSamples = new StackTraceSampleCoordinator(actorSystem.dispatcher(), 60000);\n\n\t\t// Back pressure stats tracker config\n\t\tint cleanUpInterval = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_BACK_PRESSURE_CLEAN_UP_INTERVAL,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_CLEAN_UP_INTERVAL);\n\n\t\tint refreshInterval = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_BACK_PRESSURE_REFRESH_INTERVAL,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_REFRESH_INTERVAL);\n\n\t\tint numSamples = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_BACK_PRESSURE_NUM_SAMPLES,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_NUM_SAMPLES);\n\n\t\tint delay = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_BACK_PRESSURE_DELAY,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_DELAY);\n\n\t\tTime delayBetweenSamples = Time.milliseconds(delay);\n\n\t\tbackPressureStatsTracker = new BackPressureStatsTracker(\n\t\t\t\tstackTraceSamples, cleanUpInterval, numSamples, delayBetweenSamples);\n\n\t\t// --------------------------------------------------------------------\n\n\t\texecutorService = new ForkJoinPool();\n\n\t\tExecutionContextExecutor context = ExecutionContext$.MODULE$.fromExecutor(executorService);\n\n\t\t// Config to enable https access to the web-ui\n\t\tboolean enableSSL = config.getBoolean(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_SSL_ENABLED,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_SSL_ENABLED) &&\n\t\t\tSSLUtils.getSSLEnabled(config);\n\n\t\tif (enableSSL) {\n\t\t\tLOG.info(\"Enabling ssl for the web frontend\");\n\t\t\ttry {\n\t\t\t\tserverSSLContext = SSLUtils.createSSLServerContext(config);\n\t\t\t} catch (Exception e) {\n\t\t\t\tthrow new IOException(\"Failed to initialize SSLContext for the web frontend\", e);\n\t\t\t}\n\t\t} else {\n\t\t\tserverSSLContext = null;\n\t\t}\n\t\tmetricFetcher = new MetricFetcher(actorSystem, retriever, context);\n\n\t\tString defaultSavepointDir = config.getString(ConfigConstants.SAVEPOINT_DIRECTORY_KEY, null);\n\n\t\tJobCancellationWithSavepointHandlers cancelWithSavepoint = new JobCancellationWithSavepointHandlers(currentGraphs, context, defaultSavepointDir);\n\t\tRuntimeMonitorHandler triggerHandler = handler(cancelWithSavepoint.getTriggerHandler());\n\t\tRuntimeMonitorHandler inProgressHandler = handler(cancelWithSavepoint.getInProgressHandler());\n\n\t\trouter = new Router()\n\t\t\t// config how to interact with this web server\n\t\t\t.GET(\"/config\", handler(new DashboardConfigHandler(cfg.getRefreshInterval())))\n\n\t\t\t// the overview - how many task managers, slots, free slots, ...\n\t\t\t.GET(\"/overview\", handler(new ClusterOverviewHandler(DEFAULT_REQUEST_TIMEOUT)))\n\n\t\t\t// job manager configuration\n\t\t\t.GET(\"/jobmanager/config\", handler(new JobManagerConfigHandler(config)))\n\n\t\t\t// overview over jobs\n\t\t\t.GET(\"/joboverview\", handler(new CurrentJobsOverviewHandler(DEFAULT_REQUEST_TIMEOUT, true, true)))\n\t\t\t.GET(\"/joboverview/running\", handler(new CurrentJobsOverviewHandler(DEFAULT_REQUEST_TIMEOUT, true, false)))\n\t\t\t.GET(\"/joboverview/completed\", handler(new CurrentJobsOverviewHandler(DEFAULT_REQUEST_TIMEOUT, false, true)))\n\n\t\t\t.GET(\"/jobs\", handler(new CurrentJobIdsHandler(DEFAULT_REQUEST_TIMEOUT)))\n\n\t\t\t.GET(\"/jobs/:jobid\", handler(new JobDetailsHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices\", handler(new JobDetailsHandler(currentGraphs, metricFetcher)))\n\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid\", handler(new JobVertexDetailsHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasktimes\", handler(new SubtasksTimesHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/taskmanagers\", handler(new JobVertexTaskManagersHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/accumulators\", handler(new JobVertexAccumulatorsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/backpressure\", handler(new JobVertexBackPressureHandler(\n\t\t\t\t\t\t\tcurrentGraphs,\n\t\t\t\t\t\t\tbackPressureStatsTracker,\n\t\t\t\t\t\t\trefreshInterval)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/metrics\", handler(new JobVertexMetricsHandler(metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasks/accumulators\", handler(new SubtasksAllAccumulatorsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasks/:subtasknum\", handler(new SubtaskCurrentAttemptDetailsHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasks/:subtasknum/attempts/:attempt\", handler(new SubtaskExecutionAttemptDetailsHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasks/:subtasknum/attempts/:attempt/accumulators\", handler(new SubtaskExecutionAttemptAccumulatorsHandler(currentGraphs)))\n\n\t\t\t.GET(\"/jobs/:jobid/plan\", handler(new JobPlanHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/config\", handler(new JobConfigHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/exceptions\", handler(new JobExceptionsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/accumulators\", handler(new JobAccumulatorsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/metrics\", handler(new JobMetricsHandler(metricFetcher)))\n\n\t\t\t.GET(\"/taskmanagers\", handler(new TaskManagersHandler(DEFAULT_REQUEST_TIMEOUT, metricFetcher)))\n\t\t\t.GET(\"/taskmanagers/:\" + TaskManagersHandler.TASK_MANAGER_ID_KEY + \"/metrics\", handler(new TaskManagersHandler(DEFAULT_REQUEST_TIMEOUT, metricFetcher)))\n\t\t\t.GET(\"/taskmanagers/:\" + TaskManagersHandler.TASK_MANAGER_ID_KEY + \"/log\", \n\t\t\t\tnew TaskManagerLogHandler(retriever, context, jobManagerAddressPromise.future(), timeout,\n\t\t\t\t\tTaskManagerLogHandler.FileMode.LOG, config, enableSSL))\n\t\t\t.GET(\"/taskmanagers/:\" + TaskManagersHandler.TASK_MANAGER_ID_KEY + \"/stdout\", \n\t\t\t\tnew TaskManagerLogHandler(retriever, context, jobManagerAddressPromise.future(), timeout,\n\t\t\t\t\tTaskManagerLogHandler.FileMode.STDOUT, config, enableSSL))\n\t\t\t.GET(\"/taskmanagers/:\" + TaskManagersHandler.TASK_MANAGER_ID_KEY + \"/metrics\", handler(new TaskManagerMetricsHandler(metricFetcher)))\n\n\t\t\t// log and stdout\n\t\t\t.GET(\"/jobmanager/log\", logFiles.logFile == null ? new ConstantTextHandler(\"(log file unavailable)\") :\n\t\t\t\tnew StaticFileServerHandler(retriever, jobManagerAddressPromise.future(), timeout, logFiles.logFile,\n\t\t\t\t\tenableSSL))\n\n\t\t\t.GET(\"/jobmanager/stdout\", logFiles.stdOutFile == null ? new ConstantTextHandler(\"(stdout file unavailable)\") :\n\t\t\t\tnew StaticFileServerHandler(retriever, jobManagerAddressPromise.future(), timeout, logFiles.stdOutFile,\n\t\t\t\t\tenableSSL))\n\n\t\t\t.GET(\"/jobmanager/metrics\", handler(new JobManagerMetricsHandler(metricFetcher)))\n\n\t\t\t// Cancel a job via GET (for proper integration with YARN this has to be performed via GET)\n\t\t\t.GET(\"/jobs/:jobid/yarn-cancel\", handler(new JobCancellationHandler()))\n\n\t\t\t// DELETE is the preferred way of canceling a job (Rest-conform)\n\t\t\t.DELETE(\"/jobs/:jobid/cancel\", handler(new JobCancellationHandler()))\n\n\t\t\t.GET(\"/jobs/:jobid/cancel-with-savepoint\", triggerHandler)\n\t\t\t.GET(\"/jobs/:jobid/cancel-with-savepoint/target-directory/:targetDirectory\", triggerHandler)\n\t\t\t.GET(JobCancellationWithSavepointHandlers.IN_PROGRESS_URL, inProgressHandler)\n\n\t\t\t// stop a job via GET (for proper integration with YARN this has to be performed via GET)\n\t\t\t.GET(\"/jobs/:jobid/yarn-stop\", handler(new JobStoppingHandler()))\n\n\t\t\t// DELETE is the preferred way of stopping a job (Rest-conform)\n\t\t\t.DELETE(\"/jobs/:jobid/stop\", handler(new JobStoppingHandler()));\n\n\t\tint maxCachedEntries = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_CHECKPOINTS_HISTORY_SIZE,\n\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_CHECKPOINTS_HISTORY_SIZE);\n\t\tCheckpointStatsCache cache = new CheckpointStatsCache(maxCachedEntries);\n\n\t\t// Register the checkpoint stats handlers\n\t\trouter\n\t\t\t.GET(\"/jobs/:jobid/checkpoints\", handler(new CheckpointStatsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/checkpoints/config\", handler(new CheckpointConfigHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/checkpoints/details/:checkpointid\", handler(new CheckpointStatsDetailsHandler(currentGraphs, cache)))\n\t\t\t.GET(\"/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid\", handler(new CheckpointStatsDetailsSubtasksHandler(currentGraphs, cache)));\n\n\t\tif (webSubmitAllow) {\n\t\t\trouter\n\t\t\t\t// fetch the list of uploaded jars.\n\t\t\t\t.GET(\"/jars\", handler(new JarListHandler(uploadDir)))\n\n\t\t\t\t// get plan for an uploaded jar\n\t\t\t\t.GET(\"/jars/:jarid/plan\", handler(new JarPlanHandler(uploadDir)))\n\n\t\t\t\t// run a jar\n\t\t\t\t.POST(\"/jars/:jarid/run\", handler(new JarRunHandler(uploadDir, timeout, config)))\n\n\t\t\t\t// upload a jar\n\t\t\t\t.POST(\"/jars/upload\", handler(new JarUploadHandler(uploadDir)))\n\n\t\t\t\t// delete an uploaded jar from submission interface\n\t\t\t\t.DELETE(\"/jars/:jarid\", handler(new JarDeleteHandler(uploadDir)));\n\t\t} else {\n\t\t\trouter\n\t\t\t\t// send an Access Denied message (sort of)\n\t\t\t\t// Every other GET request will go to the File Server, which will not provide\n\t\t\t\t// access to the jar directory anyway, because it doesn't exist in webRootDir.\n\t\t\t\t.GET(\"/jars\", handler(new JarAccessDeniedHandler()));\n\t\t}\n\n\t\t// this handler serves all the static contents\n\t\trouter.GET(\"/:*\", new StaticFileServerHandler(retriever, jobManagerAddressPromise.future(), timeout, webRootDir,\n\t\t\tenableSSL));\n\n\t\t// add shutdown hook for deleting the directories and remaining temp files on shutdown\n\t\ttry {\n\t\t\tRuntime.getRuntime().addShutdownHook(new Thread() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tcleanup();\n\t\t\t\t}\n\t\t\t});\n\t\t} catch (IllegalStateException e) {\n\t\t\t// race, JVM is in shutdown already, we can safely ignore this\n\t\t\tLOG.debug(\"Unable to add shutdown hook, shutdown already in progress\", e);\n\t\t} catch (Throwable t) {\n\t\t\t// these errors usually happen when the shutdown is already in progress\n\t\t\tLOG.warn(\"Error while adding shutdown hook\", t);\n\t\t}\n\n\t\tChannelInitializer<SocketChannel> initializer = new ChannelInitializer<SocketChannel>() {\n\n\t\t\t@Override\n\t\t\tprotected void initChannel(SocketChannel ch) {\n\t\t\t\tHandler handler = new Handler(router);\n\n\t\t\t\t// SSL should be the first handler in the pipeline\n\t\t\t\tif (serverSSLContext != null) {\n\t\t\t\t\tSSLEngine sslEngine = serverSSLContext.createSSLEngine();\n\t\t\t\t\tsslEngine.setUseClientMode(false);\n\t\t\t\t\tch.pipeline().addLast(\"ssl\", new SslHandler(sslEngine));\n\t\t\t\t}\n\n\t\t\t\tch.pipeline()\n\t\t\t\t\t\t.addLast(new HttpServerCodec())\n\t\t\t\t\t\t.addLast(new ChunkedWriteHandler())\n\t\t\t\t\t\t.addLast(new HttpRequestHandler(uploadDir))\n\t\t\t\t\t\t.addLast(handler.name(), handler)\n\t\t\t\t\t\t.addLast(new PipelineErrorHandler(LOG));\n\t\t\t}\n\t\t};\n\n\t\tNioEventLoopGroup bossGroup   = new NioEventLoopGroup(1);\n\t\tNioEventLoopGroup workerGroup = new NioEventLoopGroup();\n\n\t\tthis.bootstrap = new ServerBootstrap();\n\t\tthis.bootstrap\n\t\t\t\t.group(bossGroup, workerGroup)\n\t\t\t\t.channel(NioServerSocketChannel.class)\n\t\t\t\t.childHandler(initializer);\n\n\t\tChannelFuture ch;\n\t\tif (configuredAddress == null) {\n\t\t\tch = this.bootstrap.bind(configuredPort);\n\t\t} else {\n\t\t\tch = this.bootstrap.bind(configuredAddress, configuredPort);\n\t\t}\n\t\tthis.serverChannel = ch.sync().channel();\n\n\t\tInetSocketAddress bindAddress = (InetSocketAddress) serverChannel.localAddress();\n\t\tString address = bindAddress.getAddress().getHostAddress();\n\t\tint port = bindAddress.getPort();\n\n\t\tLOG.info(\"Web frontend listening at \" + address + ':' + port);\n\t}",
            " 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300 +\n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  \n 326  \n 327  \n 328  \n 329  \n 330  \n 331  \n 332  \n 333  \n 334  \n 335  \n 336  \n 337  \n 338  \n 339  \n 340  \n 341  \n 342  \n 343  \n 344  \n 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424  \n 425  \n 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  ",
            "\tpublic WebRuntimeMonitor(\n\t\t\tConfiguration config,\n\t\t\tLeaderRetrievalService leaderRetrievalService,\n\t\t\tActorSystem actorSystem) throws IOException, InterruptedException {\n\n\t\tthis.leaderRetrievalService = checkNotNull(leaderRetrievalService);\n\t\tthis.timeout = AkkaUtils.getTimeout(config);\n\t\tthis.retriever = new JobManagerRetriever(this, actorSystem, AkkaUtils.getTimeout(config), timeout);\n\t\t\n\t\tfinal WebMonitorConfig cfg = new WebMonitorConfig(config);\n\n\t\tfinal String configuredAddress = cfg.getWebFrontendAddress();\n\n\t\tfinal int configuredPort = cfg.getWebFrontendPort();\n\t\tif (configuredPort < 0) {\n\t\t\tthrow new IllegalArgumentException(\"Web frontend port is invalid: \" + configuredPort);\n\t\t}\n\t\t\n\t\tfinal WebMonitorUtils.LogFileLocation logFiles = WebMonitorUtils.LogFileLocation.find(config);\n\t\t\n\t\t// create an empty directory in temp for the web server\n\t\tString rootDirFileName = \"flink-web-\" + UUID.randomUUID();\n\t\twebRootDir = new File(getBaseDir(config), rootDirFileName);\n\t\tLOG.info(\"Using directory {} for the web interface files\", webRootDir);\n\n\t\tfinal boolean webSubmitAllow = cfg.isProgramSubmitEnabled();\n\t\tif (webSubmitAllow) {\n\t\t\t// create storage for uploads\n\t\t\tthis.uploadDir = getUploadDir(config);\n\t\t\t// the upload directory should either 1. exist and writable or 2. can be created and writable\n\t\t\tif (!(uploadDir.exists() && uploadDir.canWrite()) && !(uploadDir.mkdir() && uploadDir.canWrite())) {\n\t\t\t\tthrow new IOException(\n\t\t\t\t\tString.format(\"Jar upload directory %s cannot be created or is not writable.\",\n\t\t\t\t\t\tuploadDir.getAbsolutePath()));\n\t\t\t}\n\t\t\tLOG.info(\"Using directory {} for web frontend JAR file uploads\", uploadDir);\n\t\t}\n\t\telse {\n\t\t\tthis.uploadDir = null;\n\t\t}\n\n\t\tExecutionGraphHolder currentGraphs = new ExecutionGraphHolder();\n\n\t\t// - Back pressure stats ----------------------------------------------\n\n\t\tstackTraceSamples = new StackTraceSampleCoordinator(actorSystem.dispatcher(), 60000);\n\n\t\t// Back pressure stats tracker config\n\t\tint cleanUpInterval = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_BACK_PRESSURE_CLEAN_UP_INTERVAL,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_CLEAN_UP_INTERVAL);\n\n\t\tint refreshInterval = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_BACK_PRESSURE_REFRESH_INTERVAL,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_REFRESH_INTERVAL);\n\n\t\tint numSamples = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_BACK_PRESSURE_NUM_SAMPLES,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_NUM_SAMPLES);\n\n\t\tint delay = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_BACK_PRESSURE_DELAY,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_BACK_PRESSURE_DELAY);\n\n\t\tTime delayBetweenSamples = Time.milliseconds(delay);\n\n\t\tbackPressureStatsTracker = new BackPressureStatsTracker(\n\t\t\t\tstackTraceSamples, cleanUpInterval, numSamples, delayBetweenSamples);\n\n\t\t// --------------------------------------------------------------------\n\n\t\texecutorService = new ForkJoinPool();\n\n\t\tExecutionContextExecutor context = ExecutionContext$.MODULE$.fromExecutor(executorService);\n\n\t\t// Config to enable https access to the web-ui\n\t\tboolean enableSSL = config.getBoolean(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_SSL_ENABLED,\n\t\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_SSL_ENABLED) &&\n\t\t\tSSLUtils.getSSLEnabled(config);\n\n\t\tif (enableSSL) {\n\t\t\tLOG.info(\"Enabling ssl for the web frontend\");\n\t\t\ttry {\n\t\t\t\tserverSSLContext = SSLUtils.createSSLServerContext(config);\n\t\t\t} catch (Exception e) {\n\t\t\t\tthrow new IOException(\"Failed to initialize SSLContext for the web frontend\", e);\n\t\t\t}\n\t\t} else {\n\t\t\tserverSSLContext = null;\n\t\t}\n\t\tmetricFetcher = new MetricFetcher(actorSystem, retriever, context);\n\n\t\tString defaultSavepointDir = config.getString(ConfigConstants.SAVEPOINT_DIRECTORY_KEY, null);\n\n\t\tJobCancellationWithSavepointHandlers cancelWithSavepoint = new JobCancellationWithSavepointHandlers(currentGraphs, context, defaultSavepointDir);\n\t\tRuntimeMonitorHandler triggerHandler = handler(cancelWithSavepoint.getTriggerHandler());\n\t\tRuntimeMonitorHandler inProgressHandler = handler(cancelWithSavepoint.getInProgressHandler());\n\n\t\trouter = new Router()\n\t\t\t// config how to interact with this web server\n\t\t\t.GET(\"/config\", handler(new DashboardConfigHandler(cfg.getRefreshInterval())))\n\n\t\t\t// the overview - how many task managers, slots, free slots, ...\n\t\t\t.GET(\"/overview\", handler(new ClusterOverviewHandler(DEFAULT_REQUEST_TIMEOUT)))\n\n\t\t\t// job manager configuration\n\t\t\t.GET(\"/jobmanager/config\", handler(new JobManagerConfigHandler(config)))\n\n\t\t\t// overview over jobs\n\t\t\t.GET(\"/joboverview\", handler(new CurrentJobsOverviewHandler(DEFAULT_REQUEST_TIMEOUT, true, true)))\n\t\t\t.GET(\"/joboverview/running\", handler(new CurrentJobsOverviewHandler(DEFAULT_REQUEST_TIMEOUT, true, false)))\n\t\t\t.GET(\"/joboverview/completed\", handler(new CurrentJobsOverviewHandler(DEFAULT_REQUEST_TIMEOUT, false, true)))\n\n\t\t\t.GET(\"/jobs\", handler(new CurrentJobIdsHandler(DEFAULT_REQUEST_TIMEOUT)))\n\n\t\t\t.GET(\"/jobs/:jobid\", handler(new JobDetailsHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices\", handler(new JobDetailsHandler(currentGraphs, metricFetcher)))\n\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid\", handler(new JobVertexDetailsHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasktimes\", handler(new SubtasksTimesHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/taskmanagers\", handler(new JobVertexTaskManagersHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/accumulators\", handler(new JobVertexAccumulatorsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/backpressure\", handler(new JobVertexBackPressureHandler(\n\t\t\t\t\t\t\tcurrentGraphs,\n\t\t\t\t\t\t\tbackPressureStatsTracker,\n\t\t\t\t\t\t\trefreshInterval)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/metrics\", handler(new JobVertexMetricsHandler(metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasks/accumulators\", handler(new SubtasksAllAccumulatorsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasks/:subtasknum\", handler(new SubtaskCurrentAttemptDetailsHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasks/:subtasknum/attempts/:attempt\", handler(new SubtaskExecutionAttemptDetailsHandler(currentGraphs, metricFetcher)))\n\t\t\t.GET(\"/jobs/:jobid/vertices/:vertexid/subtasks/:subtasknum/attempts/:attempt/accumulators\", handler(new SubtaskExecutionAttemptAccumulatorsHandler(currentGraphs)))\n\n\t\t\t.GET(\"/jobs/:jobid/plan\", handler(new JobPlanHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/config\", handler(new JobConfigHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/exceptions\", handler(new JobExceptionsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/accumulators\", handler(new JobAccumulatorsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/metrics\", handler(new JobMetricsHandler(metricFetcher)))\n\n\t\t\t.GET(\"/taskmanagers\", handler(new TaskManagersHandler(DEFAULT_REQUEST_TIMEOUT, metricFetcher)))\n\t\t\t.GET(\"/taskmanagers/:\" + TaskManagersHandler.TASK_MANAGER_ID_KEY, handler(new TaskManagersHandler(DEFAULT_REQUEST_TIMEOUT, metricFetcher)))\n\t\t\t.GET(\"/taskmanagers/:\" + TaskManagersHandler.TASK_MANAGER_ID_KEY + \"/log\", \n\t\t\t\tnew TaskManagerLogHandler(retriever, context, jobManagerAddressPromise.future(), timeout,\n\t\t\t\t\tTaskManagerLogHandler.FileMode.LOG, config, enableSSL))\n\t\t\t.GET(\"/taskmanagers/:\" + TaskManagersHandler.TASK_MANAGER_ID_KEY + \"/stdout\", \n\t\t\t\tnew TaskManagerLogHandler(retriever, context, jobManagerAddressPromise.future(), timeout,\n\t\t\t\t\tTaskManagerLogHandler.FileMode.STDOUT, config, enableSSL))\n\t\t\t.GET(\"/taskmanagers/:\" + TaskManagersHandler.TASK_MANAGER_ID_KEY + \"/metrics\", handler(new TaskManagerMetricsHandler(metricFetcher)))\n\n\t\t\t// log and stdout\n\t\t\t.GET(\"/jobmanager/log\", logFiles.logFile == null ? new ConstantTextHandler(\"(log file unavailable)\") :\n\t\t\t\tnew StaticFileServerHandler(retriever, jobManagerAddressPromise.future(), timeout, logFiles.logFile,\n\t\t\t\t\tenableSSL))\n\n\t\t\t.GET(\"/jobmanager/stdout\", logFiles.stdOutFile == null ? new ConstantTextHandler(\"(stdout file unavailable)\") :\n\t\t\t\tnew StaticFileServerHandler(retriever, jobManagerAddressPromise.future(), timeout, logFiles.stdOutFile,\n\t\t\t\t\tenableSSL))\n\n\t\t\t.GET(\"/jobmanager/metrics\", handler(new JobManagerMetricsHandler(metricFetcher)))\n\n\t\t\t// Cancel a job via GET (for proper integration with YARN this has to be performed via GET)\n\t\t\t.GET(\"/jobs/:jobid/yarn-cancel\", handler(new JobCancellationHandler()))\n\n\t\t\t// DELETE is the preferred way of canceling a job (Rest-conform)\n\t\t\t.DELETE(\"/jobs/:jobid/cancel\", handler(new JobCancellationHandler()))\n\n\t\t\t.GET(\"/jobs/:jobid/cancel-with-savepoint\", triggerHandler)\n\t\t\t.GET(\"/jobs/:jobid/cancel-with-savepoint/target-directory/:targetDirectory\", triggerHandler)\n\t\t\t.GET(JobCancellationWithSavepointHandlers.IN_PROGRESS_URL, inProgressHandler)\n\n\t\t\t// stop a job via GET (for proper integration with YARN this has to be performed via GET)\n\t\t\t.GET(\"/jobs/:jobid/yarn-stop\", handler(new JobStoppingHandler()))\n\n\t\t\t// DELETE is the preferred way of stopping a job (Rest-conform)\n\t\t\t.DELETE(\"/jobs/:jobid/stop\", handler(new JobStoppingHandler()));\n\n\t\tint maxCachedEntries = config.getInteger(\n\t\t\t\tConfigConstants.JOB_MANAGER_WEB_CHECKPOINTS_HISTORY_SIZE,\n\t\t\tConfigConstants.DEFAULT_JOB_MANAGER_WEB_CHECKPOINTS_HISTORY_SIZE);\n\t\tCheckpointStatsCache cache = new CheckpointStatsCache(maxCachedEntries);\n\n\t\t// Register the checkpoint stats handlers\n\t\trouter\n\t\t\t.GET(\"/jobs/:jobid/checkpoints\", handler(new CheckpointStatsHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/checkpoints/config\", handler(new CheckpointConfigHandler(currentGraphs)))\n\t\t\t.GET(\"/jobs/:jobid/checkpoints/details/:checkpointid\", handler(new CheckpointStatsDetailsHandler(currentGraphs, cache)))\n\t\t\t.GET(\"/jobs/:jobid/checkpoints/details/:checkpointid/subtasks/:vertexid\", handler(new CheckpointStatsDetailsSubtasksHandler(currentGraphs, cache)));\n\n\t\tif (webSubmitAllow) {\n\t\t\trouter\n\t\t\t\t// fetch the list of uploaded jars.\n\t\t\t\t.GET(\"/jars\", handler(new JarListHandler(uploadDir)))\n\n\t\t\t\t// get plan for an uploaded jar\n\t\t\t\t.GET(\"/jars/:jarid/plan\", handler(new JarPlanHandler(uploadDir)))\n\n\t\t\t\t// run a jar\n\t\t\t\t.POST(\"/jars/:jarid/run\", handler(new JarRunHandler(uploadDir, timeout, config)))\n\n\t\t\t\t// upload a jar\n\t\t\t\t.POST(\"/jars/upload\", handler(new JarUploadHandler(uploadDir)))\n\n\t\t\t\t// delete an uploaded jar from submission interface\n\t\t\t\t.DELETE(\"/jars/:jarid\", handler(new JarDeleteHandler(uploadDir)));\n\t\t} else {\n\t\t\trouter\n\t\t\t\t// send an Access Denied message (sort of)\n\t\t\t\t// Every other GET request will go to the File Server, which will not provide\n\t\t\t\t// access to the jar directory anyway, because it doesn't exist in webRootDir.\n\t\t\t\t.GET(\"/jars\", handler(new JarAccessDeniedHandler()));\n\t\t}\n\n\t\t// this handler serves all the static contents\n\t\trouter.GET(\"/:*\", new StaticFileServerHandler(retriever, jobManagerAddressPromise.future(), timeout, webRootDir,\n\t\t\tenableSSL));\n\n\t\t// add shutdown hook for deleting the directories and remaining temp files on shutdown\n\t\ttry {\n\t\t\tRuntime.getRuntime().addShutdownHook(new Thread() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tcleanup();\n\t\t\t\t}\n\t\t\t});\n\t\t} catch (IllegalStateException e) {\n\t\t\t// race, JVM is in shutdown already, we can safely ignore this\n\t\t\tLOG.debug(\"Unable to add shutdown hook, shutdown already in progress\", e);\n\t\t} catch (Throwable t) {\n\t\t\t// these errors usually happen when the shutdown is already in progress\n\t\t\tLOG.warn(\"Error while adding shutdown hook\", t);\n\t\t}\n\n\t\tChannelInitializer<SocketChannel> initializer = new ChannelInitializer<SocketChannel>() {\n\n\t\t\t@Override\n\t\t\tprotected void initChannel(SocketChannel ch) {\n\t\t\t\tHandler handler = new Handler(router);\n\n\t\t\t\t// SSL should be the first handler in the pipeline\n\t\t\t\tif (serverSSLContext != null) {\n\t\t\t\t\tSSLEngine sslEngine = serverSSLContext.createSSLEngine();\n\t\t\t\t\tsslEngine.setUseClientMode(false);\n\t\t\t\t\tch.pipeline().addLast(\"ssl\", new SslHandler(sslEngine));\n\t\t\t\t}\n\n\t\t\t\tch.pipeline()\n\t\t\t\t\t\t.addLast(new HttpServerCodec())\n\t\t\t\t\t\t.addLast(new ChunkedWriteHandler())\n\t\t\t\t\t\t.addLast(new HttpRequestHandler(uploadDir))\n\t\t\t\t\t\t.addLast(handler.name(), handler)\n\t\t\t\t\t\t.addLast(new PipelineErrorHandler(LOG));\n\t\t\t}\n\t\t};\n\n\t\tNioEventLoopGroup bossGroup   = new NioEventLoopGroup(1);\n\t\tNioEventLoopGroup workerGroup = new NioEventLoopGroup();\n\n\t\tthis.bootstrap = new ServerBootstrap();\n\t\tthis.bootstrap\n\t\t\t\t.group(bossGroup, workerGroup)\n\t\t\t\t.channel(NioServerSocketChannel.class)\n\t\t\t\t.childHandler(initializer);\n\n\t\tChannelFuture ch;\n\t\tif (configuredAddress == null) {\n\t\t\tch = this.bootstrap.bind(configuredPort);\n\t\t} else {\n\t\t\tch = this.bootstrap.bind(configuredAddress, configuredPort);\n\t\t}\n\t\tthis.serverChannel = ch.sync().channel();\n\n\t\tInetSocketAddress bindAddress = (InetSocketAddress) serverChannel.localAddress();\n\t\tString address = bindAddress.getAddress().getHostAddress();\n\t\tint port = bindAddress.getPort();\n\n\t\tLOG.info(\"Web frontend listening at \" + address + ':' + port);\n\t}"
        ],
        [
            "TaskManagerMetricsHandler::getMapFor(Map,MetricStore)",
            "  40  \n  41  \n  42 -\n  43  \n  44  \n  45  \n  46  \n  47  \n  48  ",
            "\t@Override\n\tprotected Map<String, String> getMapFor(Map<String, String> pathParams, MetricStore metrics) {\n\t\tMetricStore.TaskManagerMetricStore taskManager = metrics.getTaskManagerMetricStore(pathParams.get(PARAMETER_TM_ID));\n\t\tif (taskManager == null) {\n\t\t\treturn null;\n\t\t} else {\n\t\t\treturn taskManager.metrics;\n\t\t}\n\t}",
            "  40  \n  41  \n  42 +\n  43  \n  44  \n  45  \n  46  \n  47  \n  48  ",
            "\t@Override\n\tprotected Map<String, String> getMapFor(Map<String, String> pathParams, MetricStore metrics) {\n\t\tMetricStore.TaskManagerMetricStore taskManager = metrics.getTaskManagerMetricStore(pathParams.get(TaskManagersHandler.TASK_MANAGER_ID_KEY));\n\t\tif (taskManager == null) {\n\t\t\treturn null;\n\t\t} else {\n\t\t\treturn taskManager.metrics;\n\t\t}\n\t}"
        ],
        [
            "TaskManagerMetricsHandlerTest::getMapFor()",
            "  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43 -\n  44  \n  45  \n  46  \n  47  \n  48  ",
            "\t@Test\n\tpublic void getMapFor() throws Exception {\n\t\tMetricFetcher fetcher = new MetricFetcher(mock(ActorSystem.class), mock(JobManagerRetriever.class), mock(ExecutionContext.class));\n\t\tMetricStore store = MetricStoreTest.setupStore(fetcher.getMetricStore());\n\n\t\tTaskManagerMetricsHandler handler = new TaskManagerMetricsHandler(fetcher);\n\n\t\tMap<String, String> pathParams = new HashMap<>();\n\t\tpathParams.put(PARAMETER_TM_ID, \"tmid\");\n\n\t\tMap<String, String> metrics = handler.getMapFor(pathParams, store);\n\n\t\tassertEquals(\"1\", metrics.get(\"abc.metric2\"));\n\t}",
            "  35  \n  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43 +\n  44  \n  45  \n  46  \n  47  \n  48  ",
            "\t@Test\n\tpublic void getMapFor() throws Exception {\n\t\tMetricFetcher fetcher = new MetricFetcher(mock(ActorSystem.class), mock(JobManagerRetriever.class), mock(ExecutionContext.class));\n\t\tMetricStore store = MetricStoreTest.setupStore(fetcher.getMetricStore());\n\n\t\tTaskManagerMetricsHandler handler = new TaskManagerMetricsHandler(fetcher);\n\n\t\tMap<String, String> pathParams = new HashMap<>();\n\t\tpathParams.put(TASK_MANAGER_ID_KEY, \"tmid\");\n\n\t\tMap<String, String> metrics = handler.getMapFor(pathParams, store);\n\n\t\tassertEquals(\"1\", metrics.get(\"abc.metric2\"));\n\t}"
        ]
    ],
    "ad21a441434b9ac5886b664871553bf57885e984": [
        [
            "NFA::extractCurrentMatches(ComputationState)",
            " 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578 -\n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  ",
            "\tMap<String, List<T>> extractCurrentMatches(final ComputationState<T> computationState) {\n\t\tif (computationState.getPreviousState() == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\n\t\tCollection<LinkedHashMultimap<String, T>> paths = stringSharedBuffer.extractPatterns(\n\t\t\t\tcomputationState.getPreviousState().getName(),\n\t\t\t\tcomputationState.getEvent(),\n\t\t\t\tcomputationState.getTimestamp(),\n\t\t\t\tcomputationState.getVersion());\n\n\t\t// for a given computation state, we cannot have more than one matching patterns.\n\t\tPreconditions.checkArgument(paths.size() <= 1);\n\n\t\tTypeSerializer<T> serializer = nonDuplicatingTypeSerializer.getTypeSerializer();\n\n\t\tMap<String, List<T>> result = new HashMap<>();\n\t\tfor (LinkedHashMultimap<String, T> path: paths) {\n\t\t\tfor (String key: path.keySet()) {\n\t\t\t\tSet<T> events = path.get(key);\n\t\t\t\tList<T> values = new ArrayList<>(events.size());\n\t\t\t\tfor (T event: events) {\n\t\t\t\t\tvalues.add(serializer.isImmutableType() ? event : serializer.copy(event));\n\t\t\t\t}\n\t\t\t\tresult.put(key, values);\n\t\t\t}\n\t\t}\n\t\treturn result;\n\t}",
            " 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578 +\n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  ",
            "\tMap<String, List<T>> extractCurrentMatches(final ComputationState<T> computationState) {\n\t\tif (computationState.getPreviousState() == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\n\t\tCollection<LinkedHashMultimap<String, T>> paths = stringSharedBuffer.extractPatterns(\n\t\t\t\tcomputationState.getPreviousState().getName(),\n\t\t\t\tcomputationState.getEvent(),\n\t\t\t\tcomputationState.getTimestamp(),\n\t\t\t\tcomputationState.getVersion());\n\n\t\t// for a given computation state, we cannot have more than one matching patterns.\n\t\tPreconditions.checkState(paths.size() <= 1);\n\n\t\tTypeSerializer<T> serializer = nonDuplicatingTypeSerializer.getTypeSerializer();\n\n\t\tMap<String, List<T>> result = new HashMap<>();\n\t\tfor (LinkedHashMultimap<String, T> path: paths) {\n\t\t\tfor (String key: path.keySet()) {\n\t\t\t\tSet<T> events = path.get(key);\n\t\t\t\tList<T> values = new ArrayList<>(events.size());\n\t\t\t\tfor (T event: events) {\n\t\t\t\t\tvalues.add(serializer.isImmutableType() ? event : serializer.copy(event));\n\t\t\t\t}\n\t\t\t\tresult.put(key, values);\n\t\t\t}\n\t\t}\n\t\treturn result;\n\t}"
        ],
        [
            "NFA::extractPatternMatches(ComputationState)",
            " 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612 -\n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  ",
            "\t/**\n\t * Extracts all the sequences of events from the start to the given computation state. An event\n\t * sequence is returned as a map which contains the events and the names of the states to which\n\t * the events were mapped.\n\t *\n\t * @param computationState The end computation state of the extracted event sequences\n\t * @return Collection of event sequences which end in the given computation state\n\t */\n\tprivate Collection<Map<String, T>> extractPatternMatches(final ComputationState<T> computationState) {\n\t\tCollection<LinkedHashMultimap<String, T>> paths = stringSharedBuffer.extractPatterns(\n\t\t\tcomputationState.getPreviousState().getName(),\n\t\t\tcomputationState.getEvent(),\n\t\t\tcomputationState.getTimestamp(),\n\t\t\tcomputationState.getVersion());\n\n\t\t// for a given computation state, we cannot have more than one matching patterns.\n\t\tPreconditions.checkArgument(paths.size() <= 1);\n\n\t\tList<Map<String, T>> result = new ArrayList<>();\n\n\t\tTypeSerializer<T> serializer = nonDuplicatingTypeSerializer.getTypeSerializer();\n\n\t\t// generate the correct names from the collection of LinkedHashMultimaps\n\t\tfor (LinkedHashMultimap<String, T> path: paths) {\n\t\t\tMap<String, T> resultPath = new HashMap<>();\n\t\t\tfor (String key: path.keySet()) {\n\t\t\t\tint counter = 0;\n\t\t\t\tSet<T> events = path.get(key);\n\n\t\t\t\t// we iterate over the elements in insertion order\n\t\t\t\tfor (T event: events) {\n\t\t\t\t\tresultPath.put(\n\t\t\t\t\t\tevents.size() > 1 ? generateStateName(key, counter): key,\n\t\t\t\t\t\t// copy the element so that the user can change it\n\t\t\t\t\t\tserializer.isImmutableType() ? event : serializer.copy(event)\n\t\t\t\t\t);\n\t\t\t\t\tcounter++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tresult.add(resultPath);\n\t\t}\n\n\t\treturn result;\n\t}",
            " 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612 +\n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  ",
            "\t/**\n\t * Extracts all the sequences of events from the start to the given computation state. An event\n\t * sequence is returned as a map which contains the events and the names of the states to which\n\t * the events were mapped.\n\t *\n\t * @param computationState The end computation state of the extracted event sequences\n\t * @return Collection of event sequences which end in the given computation state\n\t */\n\tprivate Collection<Map<String, T>> extractPatternMatches(final ComputationState<T> computationState) {\n\t\tCollection<LinkedHashMultimap<String, T>> paths = stringSharedBuffer.extractPatterns(\n\t\t\tcomputationState.getPreviousState().getName(),\n\t\t\tcomputationState.getEvent(),\n\t\t\tcomputationState.getTimestamp(),\n\t\t\tcomputationState.getVersion());\n\n\t\t// for a given computation state, we cannot have more than one matching patterns.\n\t\tPreconditions.checkState(paths.size() <= 1);\n\n\t\tList<Map<String, T>> result = new ArrayList<>();\n\n\t\tTypeSerializer<T> serializer = nonDuplicatingTypeSerializer.getTypeSerializer();\n\n\t\t// generate the correct names from the collection of LinkedHashMultimaps\n\t\tfor (LinkedHashMultimap<String, T> path: paths) {\n\t\t\tMap<String, T> resultPath = new HashMap<>();\n\t\t\tfor (String key: path.keySet()) {\n\t\t\t\tint counter = 0;\n\t\t\t\tSet<T> events = path.get(key);\n\n\t\t\t\t// we iterate over the elements in insertion order\n\t\t\t\tfor (T event: events) {\n\t\t\t\t\tresultPath.put(\n\t\t\t\t\t\tevents.size() > 1 ? generateStateName(key, counter): key,\n\t\t\t\t\t\t// copy the element so that the user can change it\n\t\t\t\t\t\tserializer.isImmutableType() ? event : serializer.copy(event)\n\t\t\t\t\t);\n\t\t\t\t\tcounter++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tresult.add(resultPath);\n\t\t}\n\n\t\treturn result;\n\t}"
        ]
    ],
    "3a65e5acbcc29636b0ce1631815861089fc21dca": [
        [
            "SqlITCase::testRowRegisterRowWithNames()",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\t@Test\n\tpublic void testRowRegisterRowWithNames() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tList<Row> data = new ArrayList<>();\n\t\tdata.add(Row.of(1, 1L, \"Hi\"));\n\t\tdata.add(Row.of(2, 2L, \"Hello\"));\n\t\tdata.add(Row.of(3, 2L, \"Hello world\"));\n\t\t\n\t\tTypeInformation<?>[] types = {\n\t\t\t\tBasicTypeInfo.INT_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.LONG_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.STRING_TYPE_INFO};\n\t\tString names[] = {\"a\",\"b\",\"c\"};\n\t\t\n\t\tRowTypeInfo typeInfo = new RowTypeInfo(types, names);\n\t\t\n\t\tDataStream<Row> ds = env.fromCollection(data).returns(typeInfo);\n\t\t\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTableRow\", in);\n\n\t\tString sqlQuery = \"SELECT a,c FROM MyTableRow\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toDataStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,Hi\");\n\t\texpected.add(\"2,Hello\");\n\t\texpected.add(\"3,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\t@Test\n\tpublic void testRowRegisterRowWithNames() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tList<Row> data = new ArrayList<>();\n\t\tdata.add(Row.of(1, 1L, \"Hi\"));\n\t\tdata.add(Row.of(2, 2L, \"Hello\"));\n\t\tdata.add(Row.of(3, 2L, \"Hello world\"));\n\t\t\n\t\tTypeInformation<?>[] types = {\n\t\t\t\tBasicTypeInfo.INT_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.LONG_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.STRING_TYPE_INFO};\n\t\tString names[] = {\"a\",\"b\",\"c\"};\n\t\t\n\t\tRowTypeInfo typeInfo = new RowTypeInfo(types, names);\n\t\t\n\t\tDataStream<Row> ds = env.fromCollection(data).returns(typeInfo);\n\t\t\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTableRow\", in);\n\n\t\tString sqlQuery = \"SELECT a,c FROM MyTableRow\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,Hi\");\n\t\texpected.add(\"2,Hello\");\n\t\texpected.add(\"3,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "SqlITCase::testFilter()",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t@Test\n\tpublic void testFilter() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds = StreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"MyTable\", ds, \"a, b, c, d, e\");\n\n\t\tString sqlQuery = \"SELECT a, b, e FROM MyTable WHERE c < 4\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toDataStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,1\");\n\t\texpected.add(\"2,2,2\");\n\t\texpected.add(\"2,3,1\");\n\t\texpected.add(\"3,4,2\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 +\n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t@Test\n\tpublic void testFilter() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds = StreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"MyTable\", ds, \"a, b, c, d, e\");\n\n\t\tString sqlQuery = \"SELECT a, b, e FROM MyTable WHERE c < 4\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,1\");\n\t\texpected.add(\"2,2,2\");\n\t\texpected.add(\"2,3,1\");\n\t\texpected.add(\"3,4,2\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "SqlITCase::testUnion()",
            " 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 -\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "\t@Test\n\tpublic void testUnion() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds1 = StreamTestData.getSmall3TupleDataSet(env);\n\t\tTable t1 = tableEnv.fromDataStream(ds1, \"a,b,c\");\n\t\ttableEnv.registerTable(\"T1\", t1);\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds2 = StreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"T2\", ds2, \"a, b, d, c, e\");\n\n\t\tString sqlQuery = \"SELECT * FROM T1 \" +\n\t\t\t\t\t\t\t\"UNION ALL \" +\n\t\t\t\t\t\t\t\"(SELECT a, b, c FROM T2 WHERE a\t< 3)\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toDataStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\t\texpected.add(\"1,1,Hallo\");\n\t\texpected.add(\"2,2,Hallo Welt\");\n\t\texpected.add(\"2,3,Hallo Welt wie\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            " 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 +\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "\t@Test\n\tpublic void testUnion() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds1 = StreamTestData.getSmall3TupleDataSet(env);\n\t\tTable t1 = tableEnv.fromDataStream(ds1, \"a,b,c\");\n\t\ttableEnv.registerTable(\"T1\", t1);\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds2 = StreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"T2\", ds2, \"a, b, d, c, e\");\n\n\t\tString sqlQuery = \"SELECT * FROM T1 \" +\n\t\t\t\t\t\t\t\"UNION ALL \" +\n\t\t\t\t\t\t\t\"(SELECT a, b, c FROM T2 WHERE a\t< 3)\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\t\texpected.add(\"1,1,Hallo\");\n\t\texpected.add(\"2,2,Hallo Welt\");\n\t\texpected.add(\"2,3,Hallo Welt wie\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "SqlITCase::testSelect()",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "\t@Test\n\tpublic void testSelect() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds = StreamTestData.getSmall3TupleDataSet(env);\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTable\", in);\n\n\t\tString sqlQuery = \"SELECT * FROM MyTable\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toDataStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "\t@Test\n\tpublic void testSelect() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds = StreamTestData.getSmall3TupleDataSet(env);\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTable\", in);\n\n\t\tString sqlQuery = \"SELECT * FROM MyTable\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ]
    ],
    "a68c15f0b3c530e7231df8507ad562b268f19f41": [
        [
            "TestRegistrationGateway::TestRegistrationGateway(RegistrationResponse)",
            "  40  \n  41  \n  42  \n  43  \n  44  \n  45 -\n  46  ",
            "\tpublic TestRegistrationGateway(RegistrationResponse... responses) {\n\t\tPreconditions.checkArgument(responses != null && responses.length > 0);\n\n\t\tthis.invocations = new LinkedBlockingQueue<>();\n\t\tthis.responses = responses;\n\t\t\n\t}",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  ",
            "\tpublic TestRegistrationGateway(RegistrationResponse... responses) {\n\t\tPreconditions.checkArgument(responses != null && responses.length > 0);\n\n\t\tthis.invocations = new LinkedBlockingQueue<>();\n\t\tthis.responses = responses;\n\t}"
        ],
        [
            "RegisteredRpcConnection::RegisteredRpcConnection(Logger,String,UUID,Executor)",
            "  72 -\n  73 -\n  74 -\n  75 -\n  76 -\n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  ",
            "\tpublic RegisteredRpcConnection(\n\t\tLogger log,\n\t\tString targetAddress,\n\t\tUUID targetLeaderId,\n\t\tExecutor executor)\n\t{\n\t\tthis.log = checkNotNull(log);\n\t\tthis.targetAddress = checkNotNull(targetAddress);\n\t\tthis.targetLeaderId = checkNotNull(targetLeaderId);\n\t\tthis.executor = checkNotNull(executor);\n\t}",
            "  72 +\n  73  \n  74  \n  75  \n  76  \n  77  ",
            "\tpublic RegisteredRpcConnection(Logger log, String targetAddress, UUID targetLeaderId, Executor executor) {\n\t\tthis.log = checkNotNull(log);\n\t\tthis.targetAddress = checkNotNull(targetAddress);\n\t\tthis.targetLeaderId = checkNotNull(targetLeaderId);\n\t\tthis.executor = checkNotNull(executor);\n\t}"
        ],
        [
            "RegisteredRpcConnectionTest::TestRpcConnection::TestRpcConnection(String,UUID,Executor,RpcService)",
            " 151 -\n 152 -\n 153 -\n 154 -\n 155 -\n 156  \n 157  \n 158  ",
            "\t\tpublic TestRpcConnection(String targetAddress,\n\t\t\t\t\t\t\t\t UUID targetLeaderId,\n\t\t\t\t\t\t\t\t Executor executor,\n\t\t\t\t\t\t\t\t RpcService rpcService)\n\t\t{\n\t\t\tsuper(LoggerFactory.getLogger(RegisteredRpcConnectionTest.class), targetAddress, targetLeaderId, executor);\n\t\t\tthis.rpcService = rpcService;\n\t\t}",
            " 152 +\n 153  \n 154  \n 155  ",
            "\t\tpublic TestRpcConnection(String targetAddress, UUID targetLeaderId, Executor executor,  RpcService rpcService) {\n\t\t\tsuper(LoggerFactory.getLogger(RegisteredRpcConnectionTest.class), targetAddress, targetLeaderId, executor);\n\t\t\tthis.rpcService = rpcService;\n\t\t}"
        ],
        [
            "RegisteredRpcConnectionTest::testRpcConnectionClose()",
            " 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120 -\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  ",
            "\t@Test\n\tpublic void testRpcConnectionClose() throws Exception {\n\t\tfinal String testRpcConnectionEndpointAddress = \"<TestRpcConnectionEndpointAddress>\";\n\t\tfinal UUID leaderId = UUID.randomUUID();\n\t\tfinal String connectionID = \"Test RPC Connection ID\";\n\n\t\tTestRegistrationGateway testGateway = new TestRegistrationGateway(new RetryingRegistrationTest.TestRegistrationSuccess(connectionID));\n\t\tTestingRpcService rpcService = new TestingRpcService();\n\n\t\ttry{\n\t\t\trpcService.registerGateway(testRpcConnectionEndpointAddress, testGateway);\n\n\t\t\tTestRpcConnection connection = new TestRpcConnection(testRpcConnectionEndpointAddress, leaderId, rpcService.getExecutor(), rpcService);\n\t\t\tconnection.start();\n\t\t\t//close the connection\n\t\t\tconnection.close();\n\n\t\t\t// validate connection is closed\n\t\t\tassertEquals(testRpcConnectionEndpointAddress, connection.getTargetAddress());\n\t\t\tassertEquals(leaderId, connection.getTargetLeaderId());\n\t\t\tassertTrue(connection.isClosed());\n\t\t}\n\t\tfinally {\n\t\t\ttestGateway.stop();\n\t\t\trpcService.stopService();\n\t\t}\n\t}",
            " 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 +\n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  ",
            "\t@Test\n\tpublic void testRpcConnectionClose() throws Exception {\n\t\tfinal String testRpcConnectionEndpointAddress = \"<TestRpcConnectionEndpointAddress>\";\n\t\tfinal UUID leaderId = UUID.randomUUID();\n\t\tfinal String connectionID = \"Test RPC Connection ID\";\n\n\t\tTestRegistrationGateway testGateway = new TestRegistrationGateway(new RetryingRegistrationTest.TestRegistrationSuccess(connectionID));\n\t\tTestingRpcService rpcService = new TestingRpcService();\n\n\t\ttry {\n\t\t\trpcService.registerGateway(testRpcConnectionEndpointAddress, testGateway);\n\n\t\t\tTestRpcConnection connection = new TestRpcConnection(testRpcConnectionEndpointAddress, leaderId, rpcService.getExecutor(), rpcService);\n\t\t\tconnection.start();\n\t\t\t//close the connection\n\t\t\tconnection.close();\n\n\t\t\t// validate connection is closed\n\t\t\tassertEquals(testRpcConnectionEndpointAddress, connection.getTargetAddress());\n\t\t\tassertEquals(leaderId, connection.getTargetLeaderId());\n\t\t\tassertTrue(connection.isClosed());\n\t\t}\n\t\tfinally {\n\t\t\ttestGateway.stop();\n\t\t\trpcService.stopService();\n\t\t}\n\t}"
        ]
    ],
    "e5afe563ad254ae0d5be300475b732f4b17a9770": [
        [
            "HistoryServerStaticFileServerHandlerTest::testRespondWithFile()",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52 -\n  53  \n  54  \n  55  \n  56 -\n  57  \n  58  \n  59  \n  60  \n  61 -\n  62  \n  63  \n  64  \n  65 -\n  66  \n  67  \n  68  \n  69  \n  70  \n  71 -\n  72  \n  73  \n  74  \n  75  \n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\t@Test\n\tpublic void testRespondWithFile() throws Exception {\n\t\tFile webDir = tmp.newFolder(\"webDir\");\n\t\tRouter router = new Router()\n\t\t\t.GET(\"/:*\", new HistoryServerStaticFileServerHandler(webDir));\n\t\tWebFrontendBootstrap webUI = new WebFrontendBootstrap(\n\t\t\trouter,\n\t\t\tLoggerFactory.getLogger(HistoryServerStaticFileServerHandlerTest.class),\n\t\t\ttmp.newFolder(\"uploadDir\"),\n\t\t\tnull,\n\t\t\t\"localhost\",\n\t\t\t8081,\n\t\t\tnew Configuration());\n\t\ttry {\n\t\t\t// verify that 404 message is returned when requesting a non-existant file\n\t\t\tString notFound404 = HistoryServerTest.getFromHTTP(\"http://localhost:8081/hello\");\n\t\t\tAssert.assertTrue(notFound404.contains(\"404 Not Found\"));\n\n\t\t\t// verify that a) a file can be loaded using the ClassLoader and b) that the HistoryServer\n\t\t\t// index_hs.html is injected\n\t\t\tString index = HistoryServerTest.getFromHTTP(\"http://localhost:8081/index.html\");\n\t\t\tAssert.assertTrue(index.contains(\"Completed Jobs\"));\n\n\t\t\t// verify that index.html is appended if the request path ends on '/'\n\t\t\tString index2 = HistoryServerTest.getFromHTTP(\"http://localhost:8081/\");\n\t\t\tAssert.assertEquals(index, index2);\n\n\t\t\t// verify that a 404 message is returned when requesting a directory\n\t\t\tFile dir = new File(webDir, \"dir.json\");\n\t\t\tdir.mkdirs();\n\t\t\tString dirNotFound404 = HistoryServerTest.getFromHTTP(\"http://localhost:8081/dir\");\n\t\t\tAssert.assertTrue(dirNotFound404.contains(\"404 Not Found\"));\n\n\t\t\t// verify that a 404 message is returned when requesting a file outside the webDir\n\t\t\ttmp.newFile(\"secret\");\n\t\t\tString x = HistoryServerTest.getFromHTTP(\"http://localhost:8081/../secret\");\n\t\t\tAssert.assertTrue(x.contains(\"404 Not Found\"));\n\t\t} finally {\n\t\t\twebUI.shutdown();\n\t\t}\n\t}",
            "  41  \n  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52 +\n  53  \n  54 +\n  55 +\n  56  \n  57  \n  58 +\n  59  \n  60  \n  61  \n  62  \n  63 +\n  64  \n  65  \n  66  \n  67 +\n  68  \n  69  \n  70  \n  71  \n  72  \n  73 +\n  74  \n  75  \n  76  \n  77  \n  78 +\n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\t@Test\n\tpublic void testRespondWithFile() throws Exception {\n\t\tFile webDir = tmp.newFolder(\"webDir\");\n\t\tRouter router = new Router()\n\t\t\t.GET(\"/:*\", new HistoryServerStaticFileServerHandler(webDir));\n\t\tWebFrontendBootstrap webUI = new WebFrontendBootstrap(\n\t\t\trouter,\n\t\t\tLoggerFactory.getLogger(HistoryServerStaticFileServerHandlerTest.class),\n\t\t\ttmp.newFolder(\"uploadDir\"),\n\t\t\tnull,\n\t\t\t\"localhost\",\n\t\t\t0,\n\t\t\tnew Configuration());\n\n\t\tint port = webUI.getServerPort();\n\t\ttry {\n\t\t\t// verify that 404 message is returned when requesting a non-existant file\n\t\t\tString notFound404 = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/hello\");\n\t\t\tAssert.assertTrue(notFound404.contains(\"404 Not Found\"));\n\n\t\t\t// verify that a) a file can be loaded using the ClassLoader and b) that the HistoryServer\n\t\t\t// index_hs.html is injected\n\t\t\tString index = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/index.html\");\n\t\t\tAssert.assertTrue(index.contains(\"Completed Jobs\"));\n\n\t\t\t// verify that index.html is appended if the request path ends on '/'\n\t\t\tString index2 = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/\");\n\t\t\tAssert.assertEquals(index, index2);\n\n\t\t\t// verify that a 404 message is returned when requesting a directory\n\t\t\tFile dir = new File(webDir, \"dir.json\");\n\t\t\tdir.mkdirs();\n\t\t\tString dirNotFound404 = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/dir\");\n\t\t\tAssert.assertTrue(dirNotFound404.contains(\"404 Not Found\"));\n\n\t\t\t// verify that a 404 message is returned when requesting a file outside the webDir\n\t\t\ttmp.newFile(\"secret\");\n\t\t\tString x = HistoryServerTest.getFromHTTP(\"http://localhost:\" + port + \"/../secret\");\n\t\t\tAssert.assertTrue(x.contains(\"404 Not Found\"));\n\t\t} finally {\n\t\t\twebUI.shutdown();\n\t\t}\n\t}"
        ]
    ],
    "1ceb89a979d560944e1099fa4700e46c09a79484": [
        [
            "NettyClient::connect(InetSocketAddress)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 -\n 201  \n 202  \n 203 -\n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  ",
            "\tChannelFuture connect(final InetSocketAddress serverSocketAddress) {\n\t\tcheckState(bootstrap != null, \"Client has not been initialized yet.\");\n\n\t\t// --------------------------------------------------------------------\n\t\t// Child channel pipeline for accepted connections\n\t\t// --------------------------------------------------------------------\n\n\t\tbootstrap.handler(new ChannelInitializer<SocketChannel>() {\n\t\t\t@Override\n\t\t\tpublic void initChannel(SocketChannel channel) throws Exception {\n\n\t\t\t\t// SSL handler should be added first in the pipeline\n\t\t\t\tif (clientSSLContext != null) {\n\t\t\t\t\tSSLEngine sslEngine = clientSSLContext.createSSLEngine(\n\t\t\t\t\t\tserverSocketAddress.getAddress().getHostAddress(),\n\t\t\t\t\t\tserverSocketAddress.getPort());\n\t\t\t\t\tsslEngine.setUseClientMode(true);\n\n\t\t\t\t\t// Enable hostname verification for remote SSL connections\n\t\t\t\t\tif (!serverSocketAddress.getAddress().isLoopbackAddress()) {\n\t\t\t\t\t\tSSLParameters newSSLParameters = sslEngine.getSSLParameters();\n\t\t\t\t\t\tconfig.setSSLVerifyHostname(newSSLParameters);\n\t\t\t\t\t\tsslEngine.setSSLParameters(newSSLParameters);\n\t\t\t\t\t}\n\n\t\t\t\t\tchannel.pipeline().addLast(\"ssl\", new SslHandler(sslEngine));\n\t\t\t\t}\n\t\t\t\tchannel.pipeline().addLast(protocol.getClientChannelHandlers());\n\t\t\t}\n\t\t});\n\n\t\ttry {\n\t\t\treturn bootstrap.connect(serverSocketAddress);\n\t\t}\n\t\tcatch (io.netty.channel.ChannelException e) {\n\t\t\tif ( (e.getCause() instanceof java.net.SocketException &&\n\t\t\t\t\te.getCause().getMessage().equals(\"Too many open files\")) ||\n\t\t\t\t(e.getCause() instanceof io.netty.channel.ChannelException &&\n\t\t\t\t\t\te.getCause().getCause() instanceof java.net.SocketException &&\n\t\t\t\t\t\te.getCause().getCause().getMessage().equals(\"Too many open files\")))\n\t\t\t{\n\t\t\t\tthrow new io.netty.channel.ChannelException(\n\t\t\t\t\t\t\"The operating system does not offer enough file handles to open the network connection. \" +\n\t\t\t\t\t\t\t\t\"Please increase the number of of available file handles.\", e.getCause());\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}",
            " 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 +\n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209 +\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  ",
            "\tChannelFuture connect(final InetSocketAddress serverSocketAddress) {\n\t\tcheckState(bootstrap != null, \"Client has not been initialized yet.\");\n\n\t\t// --------------------------------------------------------------------\n\t\t// Child channel pipeline for accepted connections\n\t\t// --------------------------------------------------------------------\n\n\t\tbootstrap.handler(new ChannelInitializer<SocketChannel>() {\n\t\t\t@Override\n\t\t\tpublic void initChannel(SocketChannel channel) throws Exception {\n\n\t\t\t\t// SSL handler should be added first in the pipeline\n\t\t\t\tif (clientSSLContext != null) {\n\t\t\t\t\tSSLEngine sslEngine = clientSSLContext.createSSLEngine(\n\t\t\t\t\t\tserverSocketAddress.getAddress().getHostAddress(),\n\t\t\t\t\t\tserverSocketAddress.getPort());\n\t\t\t\t\tsslEngine.setUseClientMode(true);\n\n\t\t\t\t\t// Enable hostname verification for remote SSL connections\n\t\t\t\t\tif (!serverSocketAddress.getAddress().isLoopbackAddress()) {\n\t\t\t\t\t\tSSLParameters newSSLParameters = sslEngine.getSSLParameters();\n\t\t\t\t\t\tconfig.setSSLVerifyHostname(newSSLParameters);\n\t\t\t\t\t\tsslEngine.setSSLParameters(newSSLParameters);\n\t\t\t\t\t}\n\n\t\t\t\t\tchannel.pipeline().addLast(\"ssl\", new SslHandler(sslEngine));\n\t\t\t\t}\n\t\t\t\tchannel.pipeline().addLast(protocol.getClientChannelHandlers());\n\t\t\t}\n\t\t});\n\n\t\ttry {\n\t\t\treturn bootstrap.connect(serverSocketAddress);\n\t\t}\n\t\tcatch (ChannelException e) {\n\t\t\tif ( (e.getCause() instanceof java.net.SocketException &&\n\t\t\t\t\te.getCause().getMessage().equals(\"Too many open files\")) ||\n\t\t\t\t(e.getCause() instanceof ChannelException &&\n\t\t\t\t\t\te.getCause().getCause() instanceof java.net.SocketException &&\n\t\t\t\t\t\te.getCause().getCause().getMessage().equals(\"Too many open files\")))\n\t\t\t{\n\t\t\t\tthrow new ChannelException(\n\t\t\t\t\t\t\"The operating system does not offer enough file handles to open the network connection. \" +\n\t\t\t\t\t\t\t\t\"Please increase the number of of available file handles.\", e.getCause());\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}"
        ]
    ],
    "9d9cdcbad6ccf353a1252866f6a56ac505bfaa95": [
        [
            "TwoPhaseCommitSinkFunction::snapshotState(FunctionSnapshotContext)",
            " 224  \n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "\t@Override\n\tpublic final void snapshotState(FunctionSnapshotContext context) throws Exception {\n\t\t// this is like the pre-commit of a 2-phase-commit transaction\n\t\t// we are ready to commit and remember the transaction\n\n\t\tcheckState(currentTransaction != null, \"bug: no transaction object when performing state snapshot\");\n\n\t\tlong checkpointId = context.getCheckpointId();\n\t\tLOG.debug(\"{} - checkpoint {} triggered, flushing transaction '{}'\", name(), context.getCheckpointId(), currentTransaction);\n\n\t\tpreCommit(currentTransaction);\n\t\tpendingCommitTransactions.put(checkpointId, currentTransaction);\n\t\tLOG.debug(\"{} - stored pending transactions {}\", name(), pendingCommitTransactions);\n\n\t\tcurrentTransaction = beginTransaction();\n\t\tLOG.debug(\"{} - started new transaction '{}'\", name(), currentTransaction);\n\n\t\tstate.clear();\n\t\tstate.add(new State<>(\n\t\t\tthis.currentTransaction,\n\t\t\tnew ArrayList<>(pendingCommitTransactions.values()),\n\t\t\tuserContext));\n\t}",
            " 224  \n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "\t@Override\n\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n\t\t// this is like the pre-commit of a 2-phase-commit transaction\n\t\t// we are ready to commit and remember the transaction\n\n\t\tcheckState(currentTransaction != null, \"bug: no transaction object when performing state snapshot\");\n\n\t\tlong checkpointId = context.getCheckpointId();\n\t\tLOG.debug(\"{} - checkpoint {} triggered, flushing transaction '{}'\", name(), context.getCheckpointId(), currentTransaction);\n\n\t\tpreCommit(currentTransaction);\n\t\tpendingCommitTransactions.put(checkpointId, currentTransaction);\n\t\tLOG.debug(\"{} - stored pending transactions {}\", name(), pendingCommitTransactions);\n\n\t\tcurrentTransaction = beginTransaction();\n\t\tLOG.debug(\"{} - started new transaction '{}'\", name(), currentTransaction);\n\n\t\tstate.clear();\n\t\tstate.add(new State<>(\n\t\t\tthis.currentTransaction,\n\t\t\tnew ArrayList<>(pendingCommitTransactions.values()),\n\t\t\tuserContext));\n\t}"
        ],
        [
            "TwoPhaseCommitSinkFunction::initializeState(FunctionInitializationContext)",
            " 248  \n 249 -\n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "\t@Override\n\tpublic final void initializeState(FunctionInitializationContext context) throws Exception {\n\t\t// when we are restoring state with pendingCommitTransactions, we don't really know whether the\n\t\t// transactions were already committed, or whether there was a failure between\n\t\t// completing the checkpoint on the master, and notifying the writer here.\n\n\t\t// (the common case is actually that is was already committed, the window\n\t\t// between the commit on the master and the notification here is very small)\n\n\t\t// it is possible to not have any transactions at all if there was a failure before\n\t\t// the first completed checkpoint, or in case of a scale-out event, where some of the\n\t\t// new task do not have and transactions assigned to check)\n\n\t\t// we can have more than one transaction to check in case of a scale-in event, or\n\t\t// for the reasons discussed in the 'notifyCheckpointComplete()' method.\n\n\t\tstate = context.getOperatorStateStore().getListState(stateDescriptor);\n\n\t\tif (context.isRestored()) {\n\t\t\tLOG.info(\"{} - restoring state\", name());\n\n\t\t\tfor (State<TXN, CONTEXT> operatorState : state.get()) {\n\t\t\t\tuserContext = operatorState.getContext();\n\t\t\t\tList<TXN> recoveredTransactions = operatorState.getPendingCommitTransactions();\n\t\t\t\tfor (TXN recoveredTransaction : recoveredTransactions) {\n\t\t\t\t\t// If this fails, there is actually a data loss\n\t\t\t\t\trecoverAndCommit(recoveredTransaction);\n\t\t\t\t\tLOG.info(\"{} committed recovered transaction {}\", name(), recoveredTransaction);\n\t\t\t\t}\n\n\t\t\t\trecoverAndAbort(operatorState.getPendingTransaction());\n\t\t\t\tLOG.info(\"{} aborted recovered transaction {}\", name(), operatorState.getPendingTransaction());\n\n\t\t\t\tif (userContext.isPresent()) {\n\t\t\t\t\tfinishRecoveringContext();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// if in restore we didn't get any userContext or we are initializing from scratch\n\t\tif (userContext == null) {\n\t\t\tLOG.info(\"{} - no state to restore {}\", name());\n\n\t\t\tuserContext = initializeUserContext();\n\t\t}\n\t\tthis.pendingCommitTransactions.clear();\n\n\t\tcurrentTransaction = beginTransaction();\n\t\tLOG.debug(\"{} - started new transaction '{}'\", name(), currentTransaction);\n\t}",
            " 248  \n 249 +\n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "\t@Override\n\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n\t\t// when we are restoring state with pendingCommitTransactions, we don't really know whether the\n\t\t// transactions were already committed, or whether there was a failure between\n\t\t// completing the checkpoint on the master, and notifying the writer here.\n\n\t\t// (the common case is actually that is was already committed, the window\n\t\t// between the commit on the master and the notification here is very small)\n\n\t\t// it is possible to not have any transactions at all if there was a failure before\n\t\t// the first completed checkpoint, or in case of a scale-out event, where some of the\n\t\t// new task do not have and transactions assigned to check)\n\n\t\t// we can have more than one transaction to check in case of a scale-in event, or\n\t\t// for the reasons discussed in the 'notifyCheckpointComplete()' method.\n\n\t\tstate = context.getOperatorStateStore().getListState(stateDescriptor);\n\n\t\tif (context.isRestored()) {\n\t\t\tLOG.info(\"{} - restoring state\", name());\n\n\t\t\tfor (State<TXN, CONTEXT> operatorState : state.get()) {\n\t\t\t\tuserContext = operatorState.getContext();\n\t\t\t\tList<TXN> recoveredTransactions = operatorState.getPendingCommitTransactions();\n\t\t\t\tfor (TXN recoveredTransaction : recoveredTransactions) {\n\t\t\t\t\t// If this fails, there is actually a data loss\n\t\t\t\t\trecoverAndCommit(recoveredTransaction);\n\t\t\t\t\tLOG.info(\"{} committed recovered transaction {}\", name(), recoveredTransaction);\n\t\t\t\t}\n\n\t\t\t\trecoverAndAbort(operatorState.getPendingTransaction());\n\t\t\t\tLOG.info(\"{} aborted recovered transaction {}\", name(), operatorState.getPendingTransaction());\n\n\t\t\t\tif (userContext.isPresent()) {\n\t\t\t\t\tfinishRecoveringContext();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// if in restore we didn't get any userContext or we are initializing from scratch\n\t\tif (userContext == null) {\n\t\t\tLOG.info(\"{} - no state to restore {}\", name());\n\n\t\t\tuserContext = initializeUserContext();\n\t\t}\n\t\tthis.pendingCommitTransactions.clear();\n\n\t\tcurrentTransaction = beginTransaction();\n\t\tLOG.debug(\"{} - started new transaction '{}'\", name(), currentTransaction);\n\t}"
        ],
        [
            "TwoPhaseCommitSinkFunctionTest::FileBasedSinkFunction::commit(FileTransaction)",
            " 155  \n 156  \n 157  \n 158 -\n 159  \n 160  \n 161  \n 162  ",
            "\t\t@Override\n\t\tprotected void commit(FileTransaction transaction) {\n\t\t\ttry {\n\t\t\t\tFiles.move(transaction.tmpFile.toPath(), new File(targetDirectory, transaction.tmpFile.getName()).toPath(), ATOMIC_MOVE);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new IllegalStateException(e);\n\t\t\t}\n\t\t}",
            " 155  \n 156  \n 157  \n 158 +\n 159 +\n 160 +\n 161 +\n 162  \n 163  \n 164  \n 165  ",
            "\t\t@Override\n\t\tprotected void commit(FileTransaction transaction) {\n\t\t\ttry {\n\t\t\t\tFiles.move(\n\t\t\t\t\ttransaction.tmpFile.toPath(),\n\t\t\t\t\tnew File(targetDirectory, transaction.tmpFile.getName()).toPath(),\n\t\t\t\t\tATOMIC_MOVE);\n\t\t\t} catch (IOException e) {\n\t\t\t\tthrow new IllegalStateException(e);\n\t\t\t}\n\t\t}"
        ]
    ],
    "867c0124e2959ea3c90dab13cc12ba43c2eb0f64": [
        [
            "AbstractStreamOperatorTestHarness::AbstractStreamOperatorTestHarness(StreamOperator,int,int,int)",
            " 120  \n 121  \n 122  \n 123 -\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137  \n 138  ",
            "\tpublic AbstractStreamOperatorTestHarness(\n\t\tStreamOperator<OUT> operator,\n\t\tint maxParallelism,\n\t\tint numSubtasks,\n\t\tint subtaskIndex) throws Exception {\n\n\t\tthis(\n\t\t\toperator,\n\t\t\tnew MockEnvironment(\n\t\t\t\t\"MockTask\",\n\t\t\t\t3 * 1024 * 1024,\n\t\t\t\tnew MockInputSplitProvider(),\n\t\t\t\t1024,\n\t\t\t\tnew Configuration(),\n\t\t\t\tnew ExecutionConfig(),\n\t\t\t\tmaxParallelism,\n\t\t\t\tnumSubtasks,\n\t\t\t\tsubtaskIndex));\n\t}",
            " 120  \n 121  \n 122  \n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 +\n 137  \n 138  ",
            "\tpublic AbstractStreamOperatorTestHarness(\n\t\tStreamOperator<OUT> operator,\n\t\tint maxParallelism,\n\t\tint parallelism,\n\t\tint subtaskIndex) throws Exception {\n\n\t\tthis(\n\t\t\toperator,\n\t\t\tnew MockEnvironment(\n\t\t\t\t\"MockTask\",\n\t\t\t\t3 * 1024 * 1024,\n\t\t\t\tnew MockInputSplitProvider(),\n\t\t\t\t1024,\n\t\t\t\tnew Configuration(),\n\t\t\t\tnew ExecutionConfig(),\n\t\t\t\tmaxParallelism,\n\t\t\t\tparallelism,\n\t\t\t\tsubtaskIndex));\n\t}"
        ],
        [
            "OneInputStreamOperatorTestHarness::OneInputStreamOperatorTestHarness(OneInputStreamOperator,int,int,int)",
            "  65  \n  66  \n  67  \n  68 -\n  69  \n  70 -\n  71  \n  72  \n  73  ",
            "\tpublic OneInputStreamOperatorTestHarness(\n\t\t\tOneInputStreamOperator<IN, OUT> operator,\n\t\t\tint maxParallelism,\n\t\t\tint numTubtasks,\n\t\t\tint subtaskIndex) throws Exception {\n\t\tsuper(operator, maxParallelism, numTubtasks, subtaskIndex);\n\n\t\tthis.oneInputOperator = operator;\n\t}",
            "  65  \n  66  \n  67  \n  68 +\n  69  \n  70 +\n  71  \n  72  \n  73  ",
            "\tpublic OneInputStreamOperatorTestHarness(\n\t\t\tOneInputStreamOperator<IN, OUT> operator,\n\t\t\tint maxParallelism,\n\t\t\tint parallelism,\n\t\t\tint subtaskIndex) throws Exception {\n\t\tsuper(operator, maxParallelism, parallelism, subtaskIndex);\n\n\t\tthis.oneInputOperator = operator;\n\t}"
        ]
    ],
    "d53a722e769e8ff6009d53208bf6702ec3e4a6f5": [
        [
            "ManualExactlyOnceTest::main(String)",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66 -\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tfinal ParameterTool pt = ParameterTool.fromArgs(args);\n\t\tLOG.info(\"Starting exactly once test\");\n\n\t\tfinal String streamName = \"flink-test-\" + UUID.randomUUID().toString();\n\t\tfinal String accessKey = pt.getRequired(\"accessKey\");\n\t\tfinal String secretKey = pt.getRequired(\"secretKey\");\n\t\tfinal String region = pt.getRequired(\"region\");\n\n\t\tProperties configProps = new Properties();\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_ACCESS_KEY_ID, accessKey);\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, secretKey);\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_REGION, region);\n\t\tAmazonKinesisClient client = AWSUtil.createKinesisClient(configProps);\n\n\t\t// create a stream for the test:\n\t\tclient.createStream(streamName, 1);\n\n\t\t// wait until stream has been created\n\t\tDescribeStreamResult status = client.describeStream(streamName);\n\t\tLOG.info(\"status {}\" , status);\n\t\twhile (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\")) {\n\t\t\tstatus = client.describeStream(streamName);\n\t\t\tLOG.info(\"Status of stream {}\", status);\n\t\t\tThread.sleep(1000);\n\t\t}\n\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tLocalFlinkMiniCluster flink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\n\t\tfinal int flinkPort = flink.getLeaderRPCPort();\n\n\t\ttry {\n\t\t\tfinal AtomicReference<Throwable> producerError = new AtomicReference<>();\n\t\t\tThread producerThread = KinesisEventsGeneratorProducerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 2,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tproducerError, flinkPort, flinkConfig);\n\t\t\tproducerThread.start();\n\n\t\t\tfinal AtomicReference<Throwable> consumerError = new AtomicReference<>();\n\t\t\tThread consumerThread = ExactlyOnceValidatingConsumerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 200, 2, 500, 500,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tconsumerError, flinkPort, flinkConfig);\n\t\t\tconsumerThread.start();\n\n\t\t\tboolean deadlinePassed = false;\n\t\t\tlong deadline = System.currentTimeMillis() + (1000 * 2 * 60); // wait at most for two minutes\n\t\t\t// wait until both producer and consumer finishes, or an unexpected error is thrown\n\t\t\twhile ((consumerThread.isAlive() || producerThread.isAlive()) &&\n\t\t\t\t(producerError.get() == null && consumerError.get() == null)) {\n\t\t\t\tThread.sleep(1000);\n\t\t\t\tif (System.currentTimeMillis() >= deadline) {\n\t\t\t\t\tLOG.warn(\"Deadline passed\");\n\t\t\t\t\tdeadlinePassed = true;\n\t\t\t\t\tbreak; // enough waiting\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (producerThread.isAlive()) {\n\t\t\t\tproducerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (consumerThread.isAlive()) {\n\t\t\t\tconsumerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (producerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Producer failed\", producerError.get());\n\t\t\t}\n\t\t\tif (consumerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Consumer failed\", consumerError.get());\n\t\t\t}\n\n\t\t\tif (!deadlinePassed) {\n\t\t\t\tLOG.info(\"+++ TEST passed! +++\");\n\t\t\t} else {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t}\n\n\t\t} finally {\n\t\t\tclient.deleteStream(streamName);\n\t\t\tclient.shutdown();\n\n\t\t\t// stopping flink\n\t\t\tflink.stop();\n\t\t}\n\t}",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tfinal ParameterTool pt = ParameterTool.fromArgs(args);\n\t\tLOG.info(\"Starting exactly once test\");\n\n\t\tfinal String streamName = \"flink-test-\" + UUID.randomUUID().toString();\n\t\tfinal String accessKey = pt.getRequired(\"accessKey\");\n\t\tfinal String secretKey = pt.getRequired(\"secretKey\");\n\t\tfinal String region = pt.getRequired(\"region\");\n\n\t\tProperties configProps = new Properties();\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_ACCESS_KEY_ID, accessKey);\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, secretKey);\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_REGION, region);\n\t\tAmazonKinesis client = AWSUtil.createKinesisClient(configProps);\n\n\t\t// create a stream for the test:\n\t\tclient.createStream(streamName, 1);\n\n\t\t// wait until stream has been created\n\t\tDescribeStreamResult status = client.describeStream(streamName);\n\t\tLOG.info(\"status {}\" , status);\n\t\twhile (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\")) {\n\t\t\tstatus = client.describeStream(streamName);\n\t\t\tLOG.info(\"Status of stream {}\", status);\n\t\t\tThread.sleep(1000);\n\t\t}\n\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tLocalFlinkMiniCluster flink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\n\t\tfinal int flinkPort = flink.getLeaderRPCPort();\n\n\t\ttry {\n\t\t\tfinal AtomicReference<Throwable> producerError = new AtomicReference<>();\n\t\t\tThread producerThread = KinesisEventsGeneratorProducerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 2,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tproducerError, flinkPort, flinkConfig);\n\t\t\tproducerThread.start();\n\n\t\t\tfinal AtomicReference<Throwable> consumerError = new AtomicReference<>();\n\t\t\tThread consumerThread = ExactlyOnceValidatingConsumerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 200, 2, 500, 500,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tconsumerError, flinkPort, flinkConfig);\n\t\t\tconsumerThread.start();\n\n\t\t\tboolean deadlinePassed = false;\n\t\t\tlong deadline = System.currentTimeMillis() + (1000 * 2 * 60); // wait at most for two minutes\n\t\t\t// wait until both producer and consumer finishes, or an unexpected error is thrown\n\t\t\twhile ((consumerThread.isAlive() || producerThread.isAlive()) &&\n\t\t\t\t(producerError.get() == null && consumerError.get() == null)) {\n\t\t\t\tThread.sleep(1000);\n\t\t\t\tif (System.currentTimeMillis() >= deadline) {\n\t\t\t\t\tLOG.warn(\"Deadline passed\");\n\t\t\t\t\tdeadlinePassed = true;\n\t\t\t\t\tbreak; // enough waiting\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (producerThread.isAlive()) {\n\t\t\t\tproducerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (consumerThread.isAlive()) {\n\t\t\t\tconsumerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (producerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Producer failed\", producerError.get());\n\t\t\t}\n\t\t\tif (consumerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Consumer failed\", consumerError.get());\n\t\t\t}\n\n\t\t\tif (!deadlinePassed) {\n\t\t\t\tLOG.info(\"+++ TEST passed! +++\");\n\t\t\t} else {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t}\n\n\t\t} finally {\n\t\t\tclient.deleteStream(streamName);\n\t\t\tclient.shutdown();\n\n\t\t\t// stopping flink\n\t\t\tflink.stop();\n\t\t}\n\t}"
        ],
        [
            "AWSUtil::createKinesisClient(Properties)",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49 -\n  50  \n  51 -\n  52 -\n  53 -\n  54  \n  55  \n  56 -\n  57 -\n  58  \n  59 -\n  60  \n  61 -\n  62  \n  63 -\n  64  ",
            "\t/**\n\t * Creates an Amazon Kinesis Client.\n\t * @param configProps configuration properties containing the access key, secret key, and region\n\t * @return a new Amazon Kinesis Client\n\t */\n\tpublic static AmazonKinesisClient createKinesisClient(Properties configProps) {\n\t\t// set a Flink-specific user agent\n\t\tClientConfiguration awsClientConfig = new ClientConfigurationFactory().getConfig();\n\t\tawsClientConfig.setUserAgent(\"Apache Flink \" + EnvironmentInformation.getVersion() +\n\t\t\t\" (\" + EnvironmentInformation.getRevisionInformation().commitId + \") Kinesis Connector\");\n\n\t\t// utilize automatic refreshment of credentials by directly passing the AWSCredentialsProvider\n\t\tAmazonKinesisClient client = new AmazonKinesisClient(\n\t\t\tAWSUtil.getCredentialsProvider(configProps), awsClientConfig);\n\n\t\tclient.setRegion(Region.getRegion(Regions.fromName(configProps.getProperty(AWSConfigConstants.AWS_REGION))));\n\t\tif (configProps.containsKey(AWSConfigConstants.AWS_ENDPOINT)) {\n\t\t\tclient.setEndpoint(configProps.getProperty(AWSConfigConstants.AWS_ENDPOINT));\n\t\t}\n\t\treturn client;\n\t}",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52 +\n  53  \n  54 +\n  55 +\n  56 +\n  57 +\n  58  \n  59  \n  60 +\n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66 +\n  67 +\n  68 +\n  69 +\n  70  \n  71 +\n  72  ",
            "\t/**\n\t * Creates an AmazonKinesis client.\n\t * @param configProps configuration properties containing the access key, secret key, and region\n\t * @return a new AmazonKinesis client\n\t */\n\tpublic static AmazonKinesis createKinesisClient(Properties configProps) {\n\t\t// set a Flink-specific user agent\n\t\tClientConfiguration awsClientConfig = new ClientConfigurationFactory().getConfig()\n\t\t\t\t.withUserAgentPrefix(String.format(USER_AGENT_FORMAT,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tEnvironmentInformation.getVersion(),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tEnvironmentInformation.getRevisionInformation().commitId));\n\n\t\t// utilize automatic refreshment of credentials by directly passing the AWSCredentialsProvider\n\t\tAmazonKinesisClientBuilder builder = AmazonKinesisClientBuilder.standard()\n\t\t\t\t.withCredentials(AWSUtil.getCredentialsProvider(configProps))\n\t\t\t\t.withClientConfiguration(awsClientConfig)\n\t\t\t\t.withRegion(Regions.fromName(configProps.getProperty(AWSConfigConstants.AWS_REGION)));\n\n\t\tif (configProps.containsKey(AWSConfigConstants.AWS_ENDPOINT)) {\n\t\t\t// Set signingRegion as null, to facilitate mocking Kinesis for local tests\n\t\t\tbuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(\n\t\t\t\t\t\t\t\t\t\t\t\t\tconfigProps.getProperty(AWSConfigConstants.AWS_ENDPOINT),\n\t\t\t\t\t\t\t\t\t\t\t\t\tnull));\n\t\t}\n\t\treturn builder.build();\n\t}"
        ],
        [
            "ManualExactlyOnceWithStreamReshardingTest::main(String)",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tfinal ParameterTool pt = ParameterTool.fromArgs(args);\n\t\tLOG.info(\"Starting exactly once with stream resharding test\");\n\n\t\tfinal String streamName = \"flink-test-\" + UUID.randomUUID().toString();\n\t\tfinal String accessKey = pt.getRequired(\"accessKey\");\n\t\tfinal String secretKey = pt.getRequired(\"secretKey\");\n\t\tfinal String region = pt.getRequired(\"region\");\n\n\t\tfinal Properties configProps = new Properties();\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_ACCESS_KEY_ID, accessKey);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_SECRET_ACCESS_KEY, secretKey);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_REGION, region);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.SHARD_DISCOVERY_INTERVAL_MILLIS, \"0\");\n\t\tfinal AmazonKinesisClient client = AWSUtil.createKinesisClient(configProps);\n\n\t\t// the stream is first created with 1 shard\n\t\tclient.createStream(streamName, 1);\n\n\t\t// wait until stream has been created\n\t\tDescribeStreamResult status = client.describeStream(streamName);\n\t\tLOG.info(\"status {}\", status);\n\t\twhile (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\")) {\n\t\t\tstatus = client.describeStream(streamName);\n\t\t\tLOG.info(\"Status of stream {}\", status);\n\t\t\tThread.sleep(1000);\n\t\t}\n\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tLocalFlinkMiniCluster flink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\n\t\tfinal int flinkPort = flink.getLeaderRPCPort();\n\n\t\ttry {\n\t\t\t// we have to use a manual generator here instead of the FlinkKinesisProducer\n\t\t\t// because the FlinkKinesisProducer currently has a problem where records will be resent to a shard\n\t\t\t// when resharding happens; this affects the consumer exactly-once validation test and will never pass\n\t\t\tfinal AtomicReference<Throwable> producerError = new AtomicReference<>();\n\t\t\tRunnable manualGenerate = new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tAmazonKinesisClient client = AWSUtil.createKinesisClient(configProps);\n\t\t\t\t\tint count = 0;\n\t\t\t\t\tfinal int batchSize = 30;\n\t\t\t\t\twhile (true) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\n\t\t\t\t\t\t\tSet<PutRecordsRequestEntry> batch = new HashSet<>();\n\t\t\t\t\t\t\tfor (int i = count; i < count + batchSize; i++) {\n\t\t\t\t\t\t\t\tif (i >= TOTAL_EVENT_COUNT) {\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tbatch.add(\n\t\t\t\t\t\t\t\t\tnew PutRecordsRequestEntry()\n\t\t\t\t\t\t\t\t\t\t.withData(ByteBuffer.wrap(((i) + \"-\" + RandomStringUtils.randomAlphabetic(12)).getBytes(ConfigConstants.DEFAULT_CHARSET)))\n\t\t\t\t\t\t\t\t\t\t.withPartitionKey(UUID.randomUUID().toString()));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcount += batchSize;\n\n\t\t\t\t\t\t\tPutRecordsResult result = client.putRecords(new PutRecordsRequest().withStreamName(streamName).withRecords(batch));\n\n\t\t\t\t\t\t\t// the putRecords() operation may have failing records; to keep this test simple\n\t\t\t\t\t\t\t// instead of retrying on failed records, we simply pass on a runtime exception\n\t\t\t\t\t\t\t// and let this test fail\n\t\t\t\t\t\t\tif (result.getFailedRecordCount() > 0) {\n\t\t\t\t\t\t\t\tproducerError.set(new RuntimeException(\"The producer has failed records in one of the put batch attempts.\"));\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (count >= TOTAL_EVENT_COUNT) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\t\tproducerError.set(e);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\tThread producerThread = new Thread(manualGenerate);\n\t\t\tproducerThread.start();\n\n\t\t\tfinal AtomicReference<Throwable> consumerError = new AtomicReference<>();\n\t\t\tThread consumerThread = ExactlyOnceValidatingConsumerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 10000, 2, 500, 500,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tconsumerError, flinkPort, flinkConfig);\n\t\t\tconsumerThread.start();\n\n\t\t\t// reshard the Kinesis stream while the producer / and consumers are running\n\t\t\tRunnable splitShard = new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// first, split shard in the middle of the hash range\n\t\t\t\t\t\tThread.sleep(5000);\n\t\t\t\t\t\tLOG.info(\"Splitting shard ...\");\n\t\t\t\t\t\tclient.splitShard(\n\t\t\t\t\t\t\tstreamName,\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(0),\n\t\t\t\t\t\t\t\"170141183460469231731687303715884105727\");\n\n\t\t\t\t\t\t// wait until the split shard operation finishes updating ...\n\t\t\t\t\t\tDescribeStreamResult status;\n\t\t\t\t\t\tRandom rand = new Random();\n\t\t\t\t\t\tdo {\n\t\t\t\t\t\t\tstatus = null;\n\t\t\t\t\t\t\twhile (status == null) {\n\t\t\t\t\t\t\t\t// retry until we get status\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tstatus = client.describeStream(streamName);\n\t\t\t\t\t\t\t\t} catch (LimitExceededException lee) {\n\t\t\t\t\t\t\t\t\tLOG.warn(\"LimitExceededException while describing stream ... retrying ...\");\n\t\t\t\t\t\t\t\t\tThread.sleep(rand.nextInt(1200));\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} while (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\"));\n\n\t\t\t\t\t\t// then merge again\n\t\t\t\t\t\tThread.sleep(7000);\n\t\t\t\t\t\tLOG.info(\"Merging shards ...\");\n\t\t\t\t\t\tclient.mergeShards(\n\t\t\t\t\t\t\tstreamName,\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(1),\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(2));\n\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t//\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\tThread splitShardThread = new Thread(splitShard);\n\t\t\tsplitShardThread.start();\n\n\t\t\tboolean deadlinePassed = false;\n\t\t\tlong deadline = System.currentTimeMillis() + (1000 * 5 * 60); // wait at most for five minutes\n\t\t\t// wait until both producer and consumer finishes, or an unexpected error is thrown\n\t\t\twhile ((consumerThread.isAlive() || producerThread.isAlive()) &&\n\t\t\t\t(producerError.get() == null && consumerError.get() == null)) {\n\t\t\t\tThread.sleep(1000);\n\t\t\t\tif (System.currentTimeMillis() >= deadline) {\n\t\t\t\t\tLOG.warn(\"Deadline passed\");\n\t\t\t\t\tdeadlinePassed = true;\n\t\t\t\t\tbreak; // enough waiting\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (producerThread.isAlive()) {\n\t\t\t\tproducerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (consumerThread.isAlive()) {\n\t\t\t\tconsumerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (producerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Producer failed\", producerError.get());\n\n\t\t\t}\n\n\t\t\tif (consumerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Consumer failed\", consumerError.get());\n\t\t\t}\n\n\t\t\tif (!deadlinePassed) {\n\t\t\t\tLOG.info(\"+++ TEST passed! +++\");\n\t\t\t} else {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t}\n\n\t\t} finally {\n\t\t\tclient.deleteStream(streamName);\n\t\t\tclient.shutdown();\n\n\t\t\t// stopping flink\n\t\t\tflink.stop();\n\t\t}\n\t}",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tfinal ParameterTool pt = ParameterTool.fromArgs(args);\n\t\tLOG.info(\"Starting exactly once with stream resharding test\");\n\n\t\tfinal String streamName = \"flink-test-\" + UUID.randomUUID().toString();\n\t\tfinal String accessKey = pt.getRequired(\"accessKey\");\n\t\tfinal String secretKey = pt.getRequired(\"secretKey\");\n\t\tfinal String region = pt.getRequired(\"region\");\n\n\t\tfinal Properties configProps = new Properties();\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_ACCESS_KEY_ID, accessKey);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_SECRET_ACCESS_KEY, secretKey);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_REGION, region);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.SHARD_DISCOVERY_INTERVAL_MILLIS, \"0\");\n\t\tfinal AmazonKinesis client = AWSUtil.createKinesisClient(configProps);\n\n\t\t// the stream is first created with 1 shard\n\t\tclient.createStream(streamName, 1);\n\n\t\t// wait until stream has been created\n\t\tDescribeStreamResult status = client.describeStream(streamName);\n\t\tLOG.info(\"status {}\", status);\n\t\twhile (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\")) {\n\t\t\tstatus = client.describeStream(streamName);\n\t\t\tLOG.info(\"Status of stream {}\", status);\n\t\t\tThread.sleep(1000);\n\t\t}\n\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tLocalFlinkMiniCluster flink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\n\t\tfinal int flinkPort = flink.getLeaderRPCPort();\n\n\t\ttry {\n\t\t\t// we have to use a manual generator here instead of the FlinkKinesisProducer\n\t\t\t// because the FlinkKinesisProducer currently has a problem where records will be resent to a shard\n\t\t\t// when resharding happens; this affects the consumer exactly-once validation test and will never pass\n\t\t\tfinal AtomicReference<Throwable> producerError = new AtomicReference<>();\n\t\t\tRunnable manualGenerate = new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tAmazonKinesis client = AWSUtil.createKinesisClient(configProps);\n\t\t\t\t\tint count = 0;\n\t\t\t\t\tfinal int batchSize = 30;\n\t\t\t\t\twhile (true) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\n\t\t\t\t\t\t\tSet<PutRecordsRequestEntry> batch = new HashSet<>();\n\t\t\t\t\t\t\tfor (int i = count; i < count + batchSize; i++) {\n\t\t\t\t\t\t\t\tif (i >= TOTAL_EVENT_COUNT) {\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tbatch.add(\n\t\t\t\t\t\t\t\t\tnew PutRecordsRequestEntry()\n\t\t\t\t\t\t\t\t\t\t.withData(ByteBuffer.wrap(((i) + \"-\" + RandomStringUtils.randomAlphabetic(12)).getBytes(ConfigConstants.DEFAULT_CHARSET)))\n\t\t\t\t\t\t\t\t\t\t.withPartitionKey(UUID.randomUUID().toString()));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcount += batchSize;\n\n\t\t\t\t\t\t\tPutRecordsResult result = client.putRecords(new PutRecordsRequest().withStreamName(streamName).withRecords(batch));\n\n\t\t\t\t\t\t\t// the putRecords() operation may have failing records; to keep this test simple\n\t\t\t\t\t\t\t// instead of retrying on failed records, we simply pass on a runtime exception\n\t\t\t\t\t\t\t// and let this test fail\n\t\t\t\t\t\t\tif (result.getFailedRecordCount() > 0) {\n\t\t\t\t\t\t\t\tproducerError.set(new RuntimeException(\"The producer has failed records in one of the put batch attempts.\"));\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (count >= TOTAL_EVENT_COUNT) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\t\tproducerError.set(e);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\tThread producerThread = new Thread(manualGenerate);\n\t\t\tproducerThread.start();\n\n\t\t\tfinal AtomicReference<Throwable> consumerError = new AtomicReference<>();\n\t\t\tThread consumerThread = ExactlyOnceValidatingConsumerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 10000, 2, 500, 500,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tconsumerError, flinkPort, flinkConfig);\n\t\t\tconsumerThread.start();\n\n\t\t\t// reshard the Kinesis stream while the producer / and consumers are running\n\t\t\tRunnable splitShard = new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// first, split shard in the middle of the hash range\n\t\t\t\t\t\tThread.sleep(5000);\n\t\t\t\t\t\tLOG.info(\"Splitting shard ...\");\n\t\t\t\t\t\tclient.splitShard(\n\t\t\t\t\t\t\tstreamName,\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(0),\n\t\t\t\t\t\t\t\"170141183460469231731687303715884105727\");\n\n\t\t\t\t\t\t// wait until the split shard operation finishes updating ...\n\t\t\t\t\t\tDescribeStreamResult status;\n\t\t\t\t\t\tRandom rand = new Random();\n\t\t\t\t\t\tdo {\n\t\t\t\t\t\t\tstatus = null;\n\t\t\t\t\t\t\twhile (status == null) {\n\t\t\t\t\t\t\t\t// retry until we get status\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tstatus = client.describeStream(streamName);\n\t\t\t\t\t\t\t\t} catch (LimitExceededException lee) {\n\t\t\t\t\t\t\t\t\tLOG.warn(\"LimitExceededException while describing stream ... retrying ...\");\n\t\t\t\t\t\t\t\t\tThread.sleep(rand.nextInt(1200));\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} while (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\"));\n\n\t\t\t\t\t\t// then merge again\n\t\t\t\t\t\tThread.sleep(7000);\n\t\t\t\t\t\tLOG.info(\"Merging shards ...\");\n\t\t\t\t\t\tclient.mergeShards(\n\t\t\t\t\t\t\tstreamName,\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(1),\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(2));\n\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t//\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\tThread splitShardThread = new Thread(splitShard);\n\t\t\tsplitShardThread.start();\n\n\t\t\tboolean deadlinePassed = false;\n\t\t\tlong deadline = System.currentTimeMillis() + (1000 * 5 * 60); // wait at most for five minutes\n\t\t\t// wait until both producer and consumer finishes, or an unexpected error is thrown\n\t\t\twhile ((consumerThread.isAlive() || producerThread.isAlive()) &&\n\t\t\t\t(producerError.get() == null && consumerError.get() == null)) {\n\t\t\t\tThread.sleep(1000);\n\t\t\t\tif (System.currentTimeMillis() >= deadline) {\n\t\t\t\t\tLOG.warn(\"Deadline passed\");\n\t\t\t\t\tdeadlinePassed = true;\n\t\t\t\t\tbreak; // enough waiting\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (producerThread.isAlive()) {\n\t\t\t\tproducerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (consumerThread.isAlive()) {\n\t\t\t\tconsumerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (producerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Producer failed\", producerError.get());\n\n\t\t\t}\n\n\t\t\tif (consumerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Consumer failed\", consumerError.get());\n\t\t\t}\n\n\t\t\tif (!deadlinePassed) {\n\t\t\t\tLOG.info(\"+++ TEST passed! +++\");\n\t\t\t} else {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t}\n\n\t\t} finally {\n\t\t\tclient.deleteStream(streamName);\n\t\t\tclient.shutdown();\n\n\t\t\t// stopping flink\n\t\t\tflink.stop();\n\t\t}\n\t}"
        ]
    ],
    "8bb9aa4ed96f8f9fd189f67db869f177f7231e84": [
        [
            "ConfigOptionsDocGeneratorTest::testCreatingMultipleGroups()",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167 -\n 168  \n 169  \n 170  \n 171  \n 172  ",
            "\t@Test\n\tpublic void testCreatingMultipleGroups() {\n\t\tfinal List<Tuple2<ConfigGroup, String>> tables = ConfigOptionsDocGenerator.generateTablesForClass(\n\t\t\tTestConfigMultipleSubGroup.class);\n\n\t\tassertEquals(tables.size(), 3);\n\t\tfinal HashMap<String, String> tablesConverted = new HashMap<>();\n\t\tfor (Tuple2<ConfigGroup, String> table : tables) {\n\t\t\ttablesConverted.put(table.f0 != null ? table.f0.name() : \"default\", table.f1);\n\t\t}\n\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>first.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the first option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"firstGroup\"));\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>second.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the second option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"secondGroup\"));\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>fourth.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the fourth option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>third.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the third option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"default\"));\n\t}",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  ",
            "\t@Test\n\tpublic void testCreatingMultipleGroups() {\n\t\tfinal List<Tuple2<ConfigGroup, String>> tables = ConfigOptionsDocGenerator.generateTablesForClass(\n\t\t\tTestConfigMultipleSubGroup.class);\n\n\t\tassertEquals(tables.size(), 3);\n\t\tfinal HashMap<String, String> tablesConverted = new HashMap<>();\n\t\tfor (Tuple2<ConfigGroup, String> table : tables) {\n\t\t\ttablesConverted.put(table.f0 != null ? table.f0.name() : \"default\", table.f1);\n\t\t}\n\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>first.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the first option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"firstGroup\"));\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>second.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the second option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"secondGroup\"));\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>fourth.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the fourth option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>third.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the third option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"default\"));\n\t}"
        ],
        [
            "ConfigOptionsDocGeneratorTest::testCreatingDescription()",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 -\n  66  \n  67  \n  68  \n  69  \n  70 -\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\t@Test\n\tpublic void testCreatingDescription() {\n\t\tfinal String expectedTable =\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>first.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the first option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>second.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the second option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\";\n\t\tfinal String htmlTable = ConfigOptionsDocGenerator.generateTablesForClass(TestConfigGroup.class).get(0).f1;\n\n\t\tassertEquals(expectedTable, htmlTable);\n\t}",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70 +\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\t@Test\n\tpublic void testCreatingDescription() {\n\t\tfinal String expectedTable =\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>first.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the first option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>second.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the second option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\";\n\t\tfinal String htmlTable = ConfigOptionsDocGenerator.generateTablesForClass(TestConfigGroup.class).get(0).f1;\n\n\t\tassertEquals(expectedTable, htmlTable);\n\t}"
        ],
        [
            "ConfigOptionsDocGenerator::toHtmlString(ConfigOption)",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185  \n 186  \n 187  ",
            "\t/**\n\t * Transforms option to table row.\n\t *\n\t * @param option option to transform\n\t * @return row with the option description\n\t */\n\tprivate static String toHtmlString(final ConfigOption<?> option) {\n\t\tObject defaultValue = option.defaultValue();\n\t\t// This is a temporary hack that should be removed once FLINK-6490 is resolved.\n\t\t// These options use System.getProperty(\"java.io.tmpdir\") as the default.\n\t\t// As a result the generated table contains an actual path as the default, which is simply wrong.\n\t\tif (option == WebOptions.TMP_DIR || option.key().equals(\"python.dc.tmp.dir\") || option == CoreOptions.TMP_DIRS) {\n\t\t\tdefaultValue = null;\n\t\t}\n\t\treturn \"\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>\" + escapeCharacters(option.key()) + \"</h5></td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(defaultValueToHtml(defaultValue)) + \"</td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(option.description()) + \"</td>\\n\" +\n\t\t\t\"        </tr>\\n\";\n\t}",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 +\n 185  \n 186  \n 187  ",
            "\t/**\n\t * Transforms option to table row.\n\t *\n\t * @param option option to transform\n\t * @return row with the option description\n\t */\n\tprivate static String toHtmlString(final ConfigOption<?> option) {\n\t\tObject defaultValue = option.defaultValue();\n\t\t// This is a temporary hack that should be removed once FLINK-6490 is resolved.\n\t\t// These options use System.getProperty(\"java.io.tmpdir\") as the default.\n\t\t// As a result the generated table contains an actual path as the default, which is simply wrong.\n\t\tif (option == WebOptions.TMP_DIR || option.key().equals(\"python.dc.tmp.dir\") || option == CoreOptions.TMP_DIRS) {\n\t\t\tdefaultValue = null;\n\t\t}\n\t\treturn \"\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>\" + escapeCharacters(option.key()) + \"</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">\" + escapeCharacters(defaultValueToHtml(defaultValue)) + \"</td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(option.description()) + \"</td>\\n\" +\n\t\t\t\"        </tr>\\n\";\n\t}"
        ]
    ],
    "c531486288caf5241cdf7f0f00f087f3ce82239f": [
        [
            "KafkaJsonTableSourceFactoryTestBase::testTableSource(FormatDescriptor)",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 -\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 -\n 139  \n 140  \n 141  \n 142  \n 143  \n 144  ",
            "\tprivate void testTableSource(FormatDescriptor format) {\n\t\t// construct table source using a builder\n\n\t\tfinal Map<String, String> tableJsonMapping = new HashMap<>();\n\t\ttableJsonMapping.put(\"fruit-name\", \"name\");\n\t\ttableJsonMapping.put(\"count\", \"count\");\n\t\ttableJsonMapping.put(\"event-time\", \"time\");\n\n\t\tfinal Properties props = new Properties();\n\t\tprops.put(\"group.id\", \"test-group\");\n\t\tprops.put(\"bootstrap.servers\", \"localhost:1234\");\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 0), 100L);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 1), 123L);\n\n\t\tfinal KafkaTableSource builderSource = builder()\n\t\t\t\t.forJsonSchema(TableSchema.fromTypeInfo(JsonSchemaConverter.convert(JSON_SCHEMA)))\n\t\t\t\t.failOnMissingField(true)\n\t\t\t\t.withTableToJsonMapping(tableJsonMapping)\n\t\t\t\t.withKafkaProperties(props)\n\t\t\t\t.forTopic(TOPIC)\n\t\t\t\t.fromSpecificOffsets(specificOffsets)\n\t\t\t\t.withSchema(\n\t\t\t\t\tTableSchema.builder()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING)\n\t\t\t\t\t\t.field(\"count\", Types.BIG_INT)\n\t\t\t\t\t\t.field(\"event-time\", Types.BIG_DEC)\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.build())\n\t\t\t\t.withProctimeAttribute(\"proc-time\")\n\t\t\t\t.build();\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(0, 100L);\n\t\toffsets.put(1, 123L);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(version())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(props)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(format)\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING).from(\"name\")\n\t\t\t\t\t\t.field(\"count\", Types.BIG_INT) // no from so it must match with the input\n\t\t\t\t\t\t.field(\"event-time\", Types.BIG_DEC).from(\"time\")\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP).proctime());\n\n\t\tfinal TableSource<?> factorySource = TableSourceFactoryService.findAndCreateTableSource(testDesc);\n\n\t\tassertEquals(builderSource, factorySource);\n\t}",
            "  93  \n  94  \n  95  \n  96  \n  97 +\n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 +\n 122  \n 123  \n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146 +\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "\tprivate void testTableSource(FormatDescriptor format) {\n\t\t// construct table source using a builder\n\n\t\tfinal Map<String, String> tableJsonMapping = new HashMap<>();\n\t\ttableJsonMapping.put(\"name\", \"name\");\n\t\ttableJsonMapping.put(\"fruit-name\", \"name\");\n\t\ttableJsonMapping.put(\"count\", \"count\");\n\t\ttableJsonMapping.put(\"time\", \"time\");\n\n\t\tfinal Properties props = new Properties();\n\t\tprops.put(\"group.id\", \"test-group\");\n\t\tprops.put(\"bootstrap.servers\", \"localhost:1234\");\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 0), 100L);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 1), 123L);\n\n\t\tfinal KafkaTableSource builderSource = builder()\n\t\t\t\t.forJsonSchema(TableSchema.fromTypeInfo(JsonSchemaConverter.convert(JSON_SCHEMA)))\n\t\t\t\t.failOnMissingField(true)\n\t\t\t\t.withTableToJsonMapping(tableJsonMapping)\n\t\t\t\t.withKafkaProperties(props)\n\t\t\t\t.forTopic(TOPIC)\n\t\t\t\t.fromSpecificOffsets(specificOffsets)\n\t\t\t\t.withSchema(\n\t\t\t\t\tTableSchema.builder()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING)\n\t\t\t\t\t\t.field(\"count\", Types.BIG_INT)\n\t\t\t\t\t\t.field(\"event-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.build())\n\t\t\t\t.withProctimeAttribute(\"proc-time\")\n\t\t\t\t.withRowtimeAttribute(\"event-time\", new ExistingField(\"time\"), PreserveWatermarks.INSTANCE())\n\t\t\t\t.build();\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(0, 100L);\n\t\toffsets.put(1, 123L);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(version())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(props)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(format)\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING).from(\"name\")\n\t\t\t\t\t\t.field(\"count\", Types.BIG_INT) // no from so it must match with the input\n\t\t\t\t\t\t.field(\"event-time\", Types.SQL_TIMESTAMP).rowtime(\n\t\t\t\t\t\t\tnew Rowtime().timestampsFromField(\"time\").watermarksFromSource())\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP).proctime());\n\n\t\tfinal TableSource<?> factorySource = TableSourceFactoryService.findAndCreateTableSource(testDesc);\n\n\t\tassertEquals(builderSource, factorySource);\n\t}"
        ]
    ],
    "a0c17d94072ec6e127ea3cb3c507a595283b9b87": [
        [
            "StateMachineExample::main(String)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 -\n 112  ",
            "\t/**\n\t * Main entry point for the program.\n\t *\n\t * @param args The command line arguments.\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// ---- print some usage help ----\n\n\t\tSystem.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]\");\n\t\tSystem.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]\");\n\t\tSystem.out.println();\n\n\t\t// ---- determine whether to use the built-in source, or read from Kafka ----\n\n\t\tfinal SourceFunction<Event> source;\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (params.has(\"kafka-topic\")) {\n\t\t\t// set up the Kafka reader\n\t\t\tString kafkaTopic = params.get(\"kafka-topic\");\n\t\t\tString brokers = params.get(\"brokers\", \"localhost:9092\");\n\n\t\t\tSystem.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers);\n\t\t\tSystem.out.println();\n\n\t\t\tProperties kafkaProps = new Properties();\n\t\t\tkafkaProps.setProperty(\"bootstrap.servers\", brokers);\n\n\t\t\tFlinkKafkaConsumer010<Event> kafka = new FlinkKafkaConsumer010<>(kafkaTopic, new EventDeSerializer(), kafkaProps);\n\t\t\tkafka.setStartFromLatest();\n\t\t\tkafka.setCommitOffsetsOnCheckpoints(false);\n\t\t\tsource = kafka;\n\t\t}\n\t\telse {\n\t\t\tdouble errorRate = params.getDouble(\"error-rate\", 0.0);\n\t\t\tint sleep = params.getInt(\"sleep\", 1);\n\n\t\t\tSystem.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep);\n\t\t\tSystem.out.println();\n\n\t\t\tsource = new EventsGeneratorSource(errorRate, sleep);\n\t\t}\n\n\t\t// ---- main program ----\n\n\t\t// create the environment to create streams and configure execution\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<Event> events = env.addSource(source);\n\n\t\tDataStream<Alert> alerts = events\n\t\t\t\t// partition on the address to make sure equal addresses\n\t\t\t\t// end up in the same state machine flatMap function\n\t\t\t\t.keyBy(Event::sourceAddress)\n\n\t\t\t\t// the function that evaluates the state machine over the sequence of events\n\t\t\t\t.flatMap(new StateMachineMapper());\n\n\t\t// output the alerts to std-out\n\t\talerts.print();\n\n\t\t// trigger program execution\n\t\tenv.execute();\n\t}",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 +\n 112  ",
            "\t/**\n\t * Main entry point for the program.\n\t *\n\t * @param args The command line arguments.\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// ---- print some usage help ----\n\n\t\tSystem.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]\");\n\t\tSystem.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]\");\n\t\tSystem.out.println();\n\n\t\t// ---- determine whether to use the built-in source, or read from Kafka ----\n\n\t\tfinal SourceFunction<Event> source;\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (params.has(\"kafka-topic\")) {\n\t\t\t// set up the Kafka reader\n\t\t\tString kafkaTopic = params.get(\"kafka-topic\");\n\t\t\tString brokers = params.get(\"brokers\", \"localhost:9092\");\n\n\t\t\tSystem.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers);\n\t\t\tSystem.out.println();\n\n\t\t\tProperties kafkaProps = new Properties();\n\t\t\tkafkaProps.setProperty(\"bootstrap.servers\", brokers);\n\n\t\t\tFlinkKafkaConsumer010<Event> kafka = new FlinkKafkaConsumer010<>(kafkaTopic, new EventDeSerializer(), kafkaProps);\n\t\t\tkafka.setStartFromLatest();\n\t\t\tkafka.setCommitOffsetsOnCheckpoints(false);\n\t\t\tsource = kafka;\n\t\t}\n\t\telse {\n\t\t\tdouble errorRate = params.getDouble(\"error-rate\", 0.0);\n\t\t\tint sleep = params.getInt(\"sleep\", 1);\n\n\t\t\tSystem.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep);\n\t\t\tSystem.out.println();\n\n\t\t\tsource = new EventsGeneratorSource(errorRate, sleep);\n\t\t}\n\n\t\t// ---- main program ----\n\n\t\t// create the environment to create streams and configure execution\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<Event> events = env.addSource(source);\n\n\t\tDataStream<Alert> alerts = events\n\t\t\t\t// partition on the address to make sure equal addresses\n\t\t\t\t// end up in the same state machine flatMap function\n\t\t\t\t.keyBy(Event::sourceAddress)\n\n\t\t\t\t// the function that evaluates the state machine over the sequence of events\n\t\t\t\t.flatMap(new StateMachineMapper());\n\n\t\t// output the alerts to std-out\n\t\talerts.print();\n\n\t\t// trigger program execution\n\t\tenv.execute(\"State machine job\");\n\t}"
        ]
    ],
    "a666455c98c269c63373e991c6ca2751e132a7c8": [
        [
            "StateMachineExample::main(String)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 -\n 109  \n 110  \n 111  \n 112  ",
            "\t/**\n\t * Main entry point for the program.\n\t *\n\t * @param args The command line arguments.\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// ---- print some usage help ----\n\n\t\tSystem.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]\");\n\t\tSystem.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]\");\n\t\tSystem.out.println();\n\n\t\t// ---- determine whether to use the built-in source, or read from Kafka ----\n\n\t\tfinal SourceFunction<Event> source;\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (params.has(\"kafka-topic\")) {\n\t\t\t// set up the Kafka reader\n\t\t\tString kafkaTopic = params.get(\"kafka-topic\");\n\t\t\tString brokers = params.get(\"brokers\", \"localhost:9092\");\n\n\t\t\tSystem.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers);\n\t\t\tSystem.out.println();\n\n\t\t\tProperties kafkaProps = new Properties();\n\t\t\tkafkaProps.setProperty(\"bootstrap.servers\", brokers);\n\n\t\t\tFlinkKafkaConsumer010<Event> kafka = new FlinkKafkaConsumer010<>(kafkaTopic, new EventDeSerializer(), kafkaProps);\n\t\t\tkafka.setStartFromLatest();\n\t\t\tkafka.setCommitOffsetsOnCheckpoints(false);\n\t\t\tsource = kafka;\n\t\t}\n\t\telse {\n\t\t\tdouble errorRate = params.getDouble(\"error-rate\", 0.0);\n\t\t\tint sleep = params.getInt(\"sleep\", 1);\n\n\t\t\tSystem.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep);\n\t\t\tSystem.out.println();\n\n\t\t\tsource = new EventsGeneratorSource(errorRate, sleep);\n\t\t}\n\n\t\t// ---- main program ----\n\n\t\t// create the environment to create streams and configure execution\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<Event> events = env.addSource(source);\n\n\t\tDataStream<Alert> alerts = events\n\t\t\t\t// partition on the address to make sure equal addresses\n\t\t\t\t// end up in the same state machine flatMap function\n\t\t\t\t.keyBy(Event::sourceAddress)\n\n\t\t\t\t// the function that evaluates the state machine over the sequence of events\n\t\t\t\t.flatMap(new StateMachineMapper());\n\n\t\t// output the alerts to std-out\n\t\talerts.print();\n\n\t\t// trigger program execution\n\t\tenv.execute(\"State machine job\");\n\t}",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62 +\n  63 +\n  64 +\n  65 +\n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105 +\n 106 +\n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133 +\n 134 +\n 135 +\n 136 +\n 137 +\n 138 +\n 139 +\n 140  \n 141  \n 142  \n 143  ",
            "\t/**\n\t * Main entry point for the program.\n\t *\n\t * @param args The command line arguments.\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// ---- print some usage help ----\n\n\t\tSystem.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]\");\n\t\tSystem.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]\");\n\t\tSystem.out.println(\"Options for both the above setups: \");\n\t\tSystem.out.println(\"\\t[--backend <file|rocks>]\");\n\t\tSystem.out.println(\"\\t[--checkpoint-dir <filepath>]\");\n\t\tSystem.out.println(\"\\t[--async-checkpoints <true|false>]\");\n\t\tSystem.out.println(\"\\t[--incremental-checkpoints <true|false>]\");\n\t\tSystem.out.println(\"\\t[--output <filepath> OR null for stdout]\");\n\t\tSystem.out.println();\n\n\t\t// ---- determine whether to use the built-in source, or read from Kafka ----\n\n\t\tfinal SourceFunction<Event> source;\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (params.has(\"kafka-topic\")) {\n\t\t\t// set up the Kafka reader\n\t\t\tString kafkaTopic = params.get(\"kafka-topic\");\n\t\t\tString brokers = params.get(\"brokers\", \"localhost:9092\");\n\n\t\t\tSystem.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers);\n\t\t\tSystem.out.println();\n\n\t\t\tProperties kafkaProps = new Properties();\n\t\t\tkafkaProps.setProperty(\"bootstrap.servers\", brokers);\n\n\t\t\tFlinkKafkaConsumer010<Event> kafka = new FlinkKafkaConsumer010<>(kafkaTopic, new EventDeSerializer(), kafkaProps);\n\t\t\tkafka.setStartFromLatest();\n\t\t\tkafka.setCommitOffsetsOnCheckpoints(false);\n\t\t\tsource = kafka;\n\t\t}\n\t\telse {\n\t\t\tdouble errorRate = params.getDouble(\"error-rate\", 0.0);\n\t\t\tint sleep = params.getInt(\"sleep\", 1);\n\n\t\t\tSystem.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep);\n\t\t\tSystem.out.println();\n\n\t\t\tsource = new EventsGeneratorSource(errorRate, sleep);\n\t\t}\n\n\t\t// ---- main program ----\n\n\t\t// create the environment to create streams and configure execution\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.enableCheckpointing(2000L);\n\n\t\tfinal String stateBackend = params.get(\"backend\", \"memory\");\n\t\tif (\"file\".equals(stateBackend)) {\n\t\t\tfinal String checkpointDir = params.get(\"checkpoint-dir\");\n\t\t\tboolean asyncCheckpoints = params.getBoolean(\"async-checkpoints\", false);\n\t\t\tenv.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints));\n\t\t} else if (\"rocks\".equals(stateBackend)) {\n\t\t\tfinal String checkpointDir = params.get(\"checkpoint-dir\");\n\t\t\tboolean incrementalCheckpoints = params.getBoolean(\"incremental-checkpoints\", false);\n\t\t\tenv.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));\n\t\t}\n\n\t\tfinal String outputFile = params.get(\"output\");\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\tDataStream<Event> events = env.addSource(source);\n\n\t\tDataStream<Alert> alerts = events\n\t\t\t\t// partition on the address to make sure equal addresses\n\t\t\t\t// end up in the same state machine flatMap function\n\t\t\t\t.keyBy(Event::sourceAddress)\n\n\t\t\t\t// the function that evaluates the state machine over the sequence of events\n\t\t\t\t.flatMap(new StateMachineMapper());\n\n\t\t// output the alerts to std-out\n\t\tif (outputFile == null) {\n\t\t\talerts.print();\n\t\t} else {\n\t\t\talerts\n\t\t\t\t.writeAsText(outputFile, FileSystem.WriteMode.OVERWRITE)\n\t\t\t\t.setParallelism(1);\n\t\t}\n\n\t\t// trigger program execution\n\t\tenv.execute(\"State machine job\");\n\t}"
        ],
        [
            "StateMachineExample::StateMachineMapper::flatMap(Event,Collector)",
            " 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143 -\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  ",
            "\t\t@Override\n\t\tpublic void flatMap(Event evt, Collector<Alert> out) throws Exception {\n\t\t\t// get the current state for the key (source address)\n\t\t\t// if no state exists, yet, the state must be the state machine's initial state\n\t\t\tState state = currentState.value();\n\t\t\tif (state == null) {\n\t\t\t\tstate = State.Initial;\n\t\t\t}\n\n\t\t\t// ask the state machine what state we should go to based on teh given event\n\t\t\tState nextState = state.transition(evt.type());\n\n\t\t\tif (nextState == State.InvalidTransition) {\n\t\t\t\t// the current event resulted in an invalid transition\n\t\t\t\t// raise an alert!\n\t\t\t\tout.collect(new Alert(evt.sourceAddress(), state, evt.type()));\n\t\t\t}\n\t\t\telse if (nextState.isTerminal()) {\n\t\t\t\t// we reached a terminal state, clean up the current state\n\t\t\t\tcurrentState.clear();\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// remember the new state\n\t\t\t\tcurrentState.update(nextState);\n\t\t\t}\n\t\t}",
            " 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174 +\n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  ",
            "\t\t@Override\n\t\tpublic void flatMap(Event evt, Collector<Alert> out) throws Exception {\n\t\t\t// get the current state for the key (source address)\n\t\t\t// if no state exists, yet, the state must be the state machine's initial state\n\t\t\tState state = currentState.value();\n\t\t\tif (state == null) {\n\t\t\t\tstate = State.Initial;\n\t\t\t}\n\n\t\t\t// ask the state machine what state we should go to based on the given event\n\t\t\tState nextState = state.transition(evt.type());\n\n\t\t\tif (nextState == State.InvalidTransition) {\n\t\t\t\t// the current event resulted in an invalid transition\n\t\t\t\t// raise an alert!\n\t\t\t\tout.collect(new Alert(evt.sourceAddress(), state, evt.type()));\n\t\t\t}\n\t\t\telse if (nextState.isTerminal()) {\n\t\t\t\t// we reached a terminal state, clean up the current state\n\t\t\t\tcurrentState.clear();\n\t\t\t}\n\t\t\telse {\n\t\t\t\t// remember the new state\n\t\t\t\tcurrentState.update(nextState);\n\t\t\t}\n\t\t}"
        ]
    ],
    "71095dcb098c5b03a656a1f3bb48634294e537bb": [
        [
            "Elasticsearch2SinkExample::main(String)",
            "  43  \n  44  \n  45  \n  46  \n  47 -\n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58 -\n  59 -\n  60 -\n  61 -\n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 2) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, 20).map(new MapFunction<Long, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\treturn \"message #\" + value;\n\t\t\t}\n\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<InetSocketAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>(){\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch2.x end to end sink test example\");\n\t}",
            "  43  \n  44  \n  45  \n  46  \n  47 +\n  48  \n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58 +\n  59 +\n  60 +\n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 3) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --numRecords --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, parameterTool.getInt(\"numRecords\") - 1)\n\t\t\t.map(new MapFunction<Long, String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\t\treturn \"message #\" + value;\n\t\t\t\t}\n\t\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<InetSocketAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>(){\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch2.x end to end sink test example\");\n\t}"
        ],
        [
            "Elasticsearch5SinkExample::main(String)",
            "  43  \n  44  \n  45  \n  46  \n  47 -\n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58 -\n  59 -\n  60 -\n  61 -\n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 2) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, 20).map(new MapFunction<Long, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\treturn \"message #\" + value;\n\t\t\t}\n\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<InetSocketAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>() {\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch5.x end to end sink test example\");\n\t}",
            "  44  \n  45  \n  46  \n  47  \n  48 +\n  49  \n  50 +\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 +\n  59 +\n  60 +\n  61 +\n  62 +\n  63 +\n  64 +\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 3) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --numRecords <numRecords> --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, parameterTool.getInt(\"numRecords\") - 1)\n\t\t\t.map(new MapFunction<Long, String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\t\treturn \"message #\" + value;\n\t\t\t\t}\n\t\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<InetSocketAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>() {\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch5.x end to end sink test example\");\n\t}"
        ],
        [
            "Elasticsearch1SinkExample::main(String)",
            "  44  \n  45  \n  46  \n  47  \n  48 -\n  49  \n  50 -\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 -\n  59 -\n  60 -\n  61 -\n  62 -\n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 2) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, 20).map(new MapFunction<Long, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\treturn \"message # \" + value;\n\t\t\t}\n\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<TransportAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketTransportAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>() {\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch1.x end to end sink test example\");\n\t}",
            "  45  \n  46  \n  47  \n  48  \n  49 +\n  50  \n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61 +\n  62 +\n  63 +\n  64 +\n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 3) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --numRecords <numRecords> --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, parameterTool.getInt(\"numRecords\") - 1)\n\t\t\t.map(new MapFunction<Long, String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\t\treturn \"message # \" + value;\n\t\t\t\t}\n\t\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<TransportAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketTransportAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>() {\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch1.x end to end sink test example\");\n\t}"
        ]
    ],
    "cfd0206b39b08691b832ea6324e02a5bd3a1533e": [
        [
            "Execution::getMinStateRetention()",
            "  77  \n  78 -\n  79  ",
            "\tpublic long getMinStateRetention() {\n\t\treturn Long.parseLong(properties.getOrDefault(PropertyStrings.EXECUTION_MIN_STATE_RETENTION, Long.toString(Long.MIN_VALUE)));\n\t}",
            "  77  \n  78 +\n  79  ",
            "\tpublic long getMinStateRetention() {\n\t\treturn Long.parseLong(properties.getOrDefault(PropertyStrings.EXECUTION_MIN_STATE_RETENTION, Long.toString(0)));\n\t}"
        ],
        [
            "Execution::getMaxStateRetention()",
            "  81  \n  82 -\n  83  ",
            "\tpublic long getMaxStateRetention() {\n\t\treturn Long.parseLong(properties.getOrDefault(PropertyStrings.EXECUTION_MAX_STATE_RETENTION, Long.toString(Long.MIN_VALUE)));\n\t}",
            "  81  \n  82 +\n  83  ",
            "\tpublic long getMaxStateRetention() {\n\t\treturn Long.parseLong(properties.getOrDefault(PropertyStrings.EXECUTION_MAX_STATE_RETENTION, Long.toString(0)));\n\t}"
        ]
    ],
    "abbb89059f2a83705f41e405da14073800fb1870": [
        [
            "JsonRowFormatFactoryTest::testSchemaDeserializationSchema(Map)",
            " 109  \n 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "\tprivate void testSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFormatFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal JsonRowDeserializationSchema expected2 = new JsonRowDeserializationSchema(SCHEMA);\n\t\texpected2.setFailOnMissingField(false);\n\t\tassertEquals(expected2, actual2);\n\t}",
            " 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "\tprivate void testSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal JsonRowDeserializationSchema expected2 = new JsonRowDeserializationSchema(SCHEMA);\n\t\texpected2.setFailOnMissingField(false);\n\t\tassertEquals(expected2, actual2);\n\t}"
        ],
        [
            "AvroRowFormatFactoryTest::testAvroSchemaSerializationSchema(Map)",
            "  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tprivate void testAvroSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFormatFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new AvroRowSerializationSchema(AVRO_SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}",
            "  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tprivate void testAvroSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new AvroRowSerializationSchema(AVRO_SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}"
        ],
        [
            "JsonRowFormatFactoryTest::testJsonSchemaSerializationSchema(Map)",
            " 135  \n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  ",
            "\tprivate void testJsonSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFormatFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new JsonRowSerializationSchema(JSON_SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}",
            " 135  \n 136 +\n 137  \n 138  \n 139  \n 140  \n 141  ",
            "\tprivate void testJsonSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new JsonRowSerializationSchema(JSON_SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}"
        ],
        [
            "Environment::setTables(List)",
            "  64  \n  65  \n  66  \n  67 -\n  68  \n  69  \n  70 -\n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\tpublic void setTables(List<Map<String, Object>> tables) {\n\t\tthis.tables = new HashMap<>(tables.size());\n\t\ttables.forEach(config -> {\n\t\t\tif (!config.containsKey(NAME)) {\n\t\t\t\tthrow new SqlClientException(\"The 'name' attribute of a table is missing.\");\n\t\t\t}\n\t\t\tfinal Object nameObject = config.get(NAME);\n\t\t\tif (nameObject == null || !(nameObject instanceof String) || ((String) nameObject).length() <= 0) {\n\t\t\t\tthrow new SqlClientException(\"Invalid table name '\" + nameObject + \"'.\");\n\t\t\t}\n\t\t\tfinal String name = (String) nameObject;\n\t\t\tfinal Map<String, Object> properties = new HashMap<>(config);\n\t\t\tproperties.remove(NAME);\n\n\t\t\tif (this.tables.containsKey(name)) {\n\t\t\t\tthrow new SqlClientException(\"Duplicate table name '\" + name + \"'.\");\n\t\t\t}\n\t\t\tthis.tables.put(name, createTableDescriptor(name, properties));\n\t\t});\n\t}",
            "  65  \n  66  \n  67  \n  68 +\n  69  \n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  ",
            "\tpublic void setTables(List<Map<String, Object>> tables) {\n\t\tthis.tables = new HashMap<>(tables.size());\n\t\ttables.forEach(config -> {\n\t\t\tif (!config.containsKey(TABLE_NAME)) {\n\t\t\t\tthrow new SqlClientException(\"The 'name' attribute of a table is missing.\");\n\t\t\t}\n\t\t\tfinal Object nameObject = config.get(TABLE_NAME);\n\t\t\tif (nameObject == null || !(nameObject instanceof String) || ((String) nameObject).length() <= 0) {\n\t\t\t\tthrow new SqlClientException(\"Invalid table name '\" + nameObject + \"'.\");\n\t\t\t}\n\t\t\tfinal String name = (String) nameObject;\n\t\t\tfinal Map<String, Object> properties = new HashMap<>(config);\n\t\t\tproperties.remove(TABLE_NAME);\n\n\t\t\tif (this.tables.containsKey(name)) {\n\t\t\t\tthrow new SqlClientException(\"Duplicate table name '\" + name + \"'.\");\n\t\t\t}\n\t\t\tthis.tables.put(name, createTableDescriptor(name, properties));\n\t\t});\n\t}"
        ],
        [
            "KafkaTableSourceFactory::createTableSource(Map)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "\t@Override\n\tpublic TableSource<Row> createTableSource(Map<String, String> properties) {\n\t\tfinal DescriptorProperties params = new DescriptorProperties(true);\n\t\tparams.putProperties(properties);\n\n\t\t// validate\n\t\t// allow Kafka timestamps to be used, watermarks can not be received from source\n\t\tnew SchemaValidator(true, supportsKafkaTimestamps(), false).validate(params);\n\t\tnew KafkaValidator().validate(params);\n\n\t\t// deserialization schema using format discovery\n\t\tfinal DeserializationSchemaFactory<?> formatFactory = TableFormatFactoryService.find(\n\t\t\tDeserializationSchemaFactory.class,\n\t\t\tproperties,\n\t\t\tthis.getClass().getClassLoader());\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tfinal DeserializationSchema<Row> deserializationSchema = (DeserializationSchema<Row>) formatFactory\n\t\t\t.createDeserializationSchema(properties);\n\n\t\t// schema\n\t\tfinal TableSchema schema = params.getTableSchema(SCHEMA());\n\n\t\t// proctime\n\t\tfinal Optional<String> proctimeAttribute = SchemaValidator.deriveProctimeAttribute(params);\n\n\t\t// rowtime\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributes = SchemaValidator.deriveRowtimeAttributes(params);\n\n\t\t// field mapping\n\t\tfinal Map<String, String> fieldMapping = SchemaValidator.deriveFieldMapping(params, Optional.of(schema));\n\n\t\t// properties\n\t\tfinal Properties kafkaProperties = new Properties();\n\t\tfinal List<Map<String, String>> propsList = params.getFixedIndexedProperties(\n\t\t\tCONNECTOR_PROPERTIES,\n\t\t\tArrays.asList(CONNECTOR_PROPERTIES_KEY, CONNECTOR_PROPERTIES_VALUE));\n\t\tpropsList.forEach(kv -> kafkaProperties.put(\n\t\t\tparams.getString(kv.get(CONNECTOR_PROPERTIES_KEY)),\n\t\t\tparams.getString(kv.get(CONNECTOR_PROPERTIES_VALUE))\n\t\t));\n\n\t\t// topic\n\t\tfinal String topic = params.getString(CONNECTOR_TOPIC);\n\n\t\t// startup mode\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tfinal StartupMode startupMode = params\n\t\t\t.getOptionalString(CONNECTOR_STARTUP_MODE)\n\t\t\t.map(modeString -> {\n\t\t\t\tswitch (modeString) {\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_EARLIEST:\n\t\t\t\t\t\treturn StartupMode.EARLIEST;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_LATEST:\n\t\t\t\t\t\treturn StartupMode.LATEST;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_GROUP_OFFSETS:\n\t\t\t\t\t\treturn StartupMode.GROUP_OFFSETS;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_SPECIFIC_OFFSETS:\n\t\t\t\t\t\tfinal List<Map<String, String>> offsetList = params.getFixedIndexedProperties(\n\t\t\t\t\t\t\tCONNECTOR_SPECIFIC_OFFSETS,\n\t\t\t\t\t\t\tArrays.asList(CONNECTOR_SPECIFIC_OFFSETS_PARTITION, CONNECTOR_SPECIFIC_OFFSETS_OFFSET));\n\t\t\t\t\t\toffsetList.forEach(kv -> {\n\t\t\t\t\t\t\tfinal int partition = params.getInt(kv.get(CONNECTOR_SPECIFIC_OFFSETS_PARTITION));\n\t\t\t\t\t\t\tfinal long offset = params.getLong(kv.get(CONNECTOR_SPECIFIC_OFFSETS_OFFSET));\n\t\t\t\t\t\t\tfinal KafkaTopicPartition topicPartition = new KafkaTopicPartition(topic, partition);\n\t\t\t\t\t\t\tspecificOffsets.put(topicPartition, offset);\n\t\t\t\t\t\t});\n\t\t\t\t\t\treturn StartupMode.SPECIFIC_OFFSETS;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tthrow new TableException(\"Unsupported startup mode. Validator should have checked that.\");\n\t\t\t\t}\n\t\t\t}).orElse(StartupMode.GROUP_OFFSETS);\n\n\t\treturn createKafkaTableSource(\n\t\t\tschema,\n\t\t\tproctimeAttribute,\n\t\t\trowtimeAttributes,\n\t\t\tfieldMapping,\n\t\t\ttopic,\n\t\t\tkafkaProperties,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\t}",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "\t@Override\n\tpublic TableSource<Row> createTableSource(Map<String, String> properties) {\n\t\tfinal DescriptorProperties params = new DescriptorProperties(true);\n\t\tparams.putProperties(properties);\n\n\t\t// validate\n\t\t// allow Kafka timestamps to be used, watermarks can not be received from source\n\t\tnew SchemaValidator(true, supportsKafkaTimestamps(), false).validate(params);\n\t\tnew KafkaValidator().validate(params);\n\n\t\t// deserialization schema using format discovery\n\t\tfinal DeserializationSchemaFactory<?> formatFactory = TableFactoryService.find(\n\t\t\tDeserializationSchemaFactory.class,\n\t\t\tproperties,\n\t\t\tthis.getClass().getClassLoader());\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tfinal DeserializationSchema<Row> deserializationSchema = (DeserializationSchema<Row>) formatFactory\n\t\t\t.createDeserializationSchema(properties);\n\n\t\t// schema\n\t\tfinal TableSchema schema = params.getTableSchema(SCHEMA());\n\n\t\t// proctime\n\t\tfinal Optional<String> proctimeAttribute = SchemaValidator.deriveProctimeAttribute(params);\n\n\t\t// rowtime\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributes = SchemaValidator.deriveRowtimeAttributes(params);\n\n\t\t// field mapping\n\t\tfinal Map<String, String> fieldMapping = SchemaValidator.deriveFieldMapping(params, Optional.of(schema));\n\n\t\t// properties\n\t\tfinal Properties kafkaProperties = new Properties();\n\t\tfinal List<Map<String, String>> propsList = params.getFixedIndexedProperties(\n\t\t\tCONNECTOR_PROPERTIES,\n\t\t\tArrays.asList(CONNECTOR_PROPERTIES_KEY, CONNECTOR_PROPERTIES_VALUE));\n\t\tpropsList.forEach(kv -> kafkaProperties.put(\n\t\t\tparams.getString(kv.get(CONNECTOR_PROPERTIES_KEY)),\n\t\t\tparams.getString(kv.get(CONNECTOR_PROPERTIES_VALUE))\n\t\t));\n\n\t\t// topic\n\t\tfinal String topic = params.getString(CONNECTOR_TOPIC);\n\n\t\t// startup mode\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tfinal StartupMode startupMode = params\n\t\t\t.getOptionalString(CONNECTOR_STARTUP_MODE)\n\t\t\t.map(modeString -> {\n\t\t\t\tswitch (modeString) {\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_EARLIEST:\n\t\t\t\t\t\treturn StartupMode.EARLIEST;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_LATEST:\n\t\t\t\t\t\treturn StartupMode.LATEST;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_GROUP_OFFSETS:\n\t\t\t\t\t\treturn StartupMode.GROUP_OFFSETS;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_SPECIFIC_OFFSETS:\n\t\t\t\t\t\tfinal List<Map<String, String>> offsetList = params.getFixedIndexedProperties(\n\t\t\t\t\t\t\tCONNECTOR_SPECIFIC_OFFSETS,\n\t\t\t\t\t\t\tArrays.asList(CONNECTOR_SPECIFIC_OFFSETS_PARTITION, CONNECTOR_SPECIFIC_OFFSETS_OFFSET));\n\t\t\t\t\t\toffsetList.forEach(kv -> {\n\t\t\t\t\t\t\tfinal int partition = params.getInt(kv.get(CONNECTOR_SPECIFIC_OFFSETS_PARTITION));\n\t\t\t\t\t\t\tfinal long offset = params.getLong(kv.get(CONNECTOR_SPECIFIC_OFFSETS_OFFSET));\n\t\t\t\t\t\t\tfinal KafkaTopicPartition topicPartition = new KafkaTopicPartition(topic, partition);\n\t\t\t\t\t\t\tspecificOffsets.put(topicPartition, offset);\n\t\t\t\t\t\t});\n\t\t\t\t\t\treturn StartupMode.SPECIFIC_OFFSETS;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tthrow new TableException(\"Unsupported startup mode. Validator should have checked that.\");\n\t\t\t\t}\n\t\t\t}).orElse(StartupMode.GROUP_OFFSETS);\n\n\t\treturn createKafkaTableSource(\n\t\t\tschema,\n\t\t\tproctimeAttribute,\n\t\t\trowtimeAttributes,\n\t\t\tfieldMapping,\n\t\t\ttopic,\n\t\t\tkafkaProperties,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\t}"
        ],
        [
            "AvroRowFormatFactoryTest::testRecordClassDeserializationSchema(Map)",
            "  73  \n  74 -\n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\tprivate void testRecordClassDeserializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFormatFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new AvroRowSerializationSchema(AVRO_SPECIFIC_RECORD);\n\t\tassertEquals(expected1, actual1);\n\t}",
            "  73  \n  74 +\n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\tprivate void testRecordClassDeserializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new AvroRowSerializationSchema(AVRO_SPECIFIC_RECORD);\n\t\tassertEquals(expected1, actual1);\n\t}"
        ],
        [
            "AvroRowFormatFactoryTest::testRecordClassSerializationSchema(Map)",
            "  65  \n  66 -\n  67  \n  68  \n  69  \n  70  \n  71  ",
            "\tprivate void testRecordClassSerializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFormatFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal AvroRowDeserializationSchema expected2 = new AvroRowDeserializationSchema(AVRO_SPECIFIC_RECORD);\n\t\tassertEquals(expected2, actual2);\n\t}",
            "  65  \n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  ",
            "\tprivate void testRecordClassSerializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal AvroRowDeserializationSchema expected2 = new AvroRowDeserializationSchema(AVRO_SPECIFIC_RECORD);\n\t\tassertEquals(expected2, actual2);\n\t}"
        ],
        [
            "Environment::createTableDescriptor(String,Map)",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 -\n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "\t/**\n\t * Creates a table descriptor from a YAML config map.\n\t *\n\t * @param name name of the table\n\t * @param config YAML config map\n\t * @return table descriptor describing a source, sink, or both\n\t */\n\tprivate static TableDescriptor createTableDescriptor(String name, Map<String, Object> config) {\n\t\tfinal Object typeObject = config.get(TableDescriptorValidator.TABLE_TYPE());\n\t\tif (typeObject == null || !(typeObject instanceof String)) {\n\t\t\tthrow new SqlClientException(\"Invalid 'type' attribute for table '\" + name + \"'.\");\n\t\t}\n\t\tfinal String type = (String) config.get(TableDescriptorValidator.TABLE_TYPE());\n\t\tfinal Map<String, String> normalizedConfig = ConfigUtil.normalizeYaml(config);\n\t\tif (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SOURCE())) {\n\t\t\treturn new Source(name, normalizedConfig);\n\t\t} else if (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SINK())) {\n\t\t\treturn new Sink(name, normalizedConfig);\n\t\t} else if (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SOURCE_SINK())) {\n\t\t\treturn new SourceSink(name, normalizedConfig);\n\t\t}\n\t\tthrow new SqlClientException(\"Invalid 'type' attribute for table '\" + name + \"'. \" +\n\t\t\t\"Only 'source', 'sink', and 'both' are supported.\");\n\t}",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217 +\n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "\t/**\n\t * Creates a table descriptor from a YAML config map.\n\t *\n\t * @param name name of the table\n\t * @param config YAML config map\n\t * @return table descriptor describing a source, sink, or both\n\t */\n\tprivate static TableDescriptor createTableDescriptor(String name, Map<String, Object> config) {\n\t\tfinal Object typeObject = config.get(TABLE_TYPE);\n\t\tif (typeObject == null || !(typeObject instanceof String)) {\n\t\t\tthrow new SqlClientException(\"Invalid 'type' attribute for table '\" + name + \"'.\");\n\t\t}\n\t\tfinal String type = (String) config.get(TABLE_TYPE);\n\t\tconfig.remove(TABLE_TYPE);\n\t\tfinal Map<String, String> normalizedConfig = ConfigUtil.normalizeYaml(config);\n\t\tif (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SOURCE())) {\n\t\t\treturn new Source(name, normalizedConfig);\n\t\t} else if (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SINK())) {\n\t\t\treturn new Sink(name, normalizedConfig);\n\t\t} else if (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SOURCE_SINK())) {\n\t\t\treturn new SourceSink(name, normalizedConfig);\n\t\t}\n\t\tthrow new SqlClientException(\"Invalid 'type' attribute for table '\" + name + \"'. \" +\n\t\t\t\"Only 'source', 'sink', and 'both' are supported.\");\n\t}"
        ],
        [
            "KafkaTableSourceFactoryTestBase::testTableSource()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "\t@Test\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void testTableSource() {\n\n\t\t// prepare parameters for Kafka table source\n\n\t\tfinal TableSchema schema = TableSchema.builder()\n\t\t\t.field(FRUIT_NAME, Types.STRING())\n\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.build();\n\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors = Collections.singletonList(\n\t\t\tnew RowtimeAttributeDescriptor(EVENT_TIME, new ExistingField(TIME), new AscendingTimestamps()));\n\n\t\tfinal Map<String, String> fieldMapping = new HashMap<>();\n\t\tfieldMapping.put(FRUIT_NAME, NAME);\n\t\tfieldMapping.put(COUNT, COUNT);\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_0), OFFSET_0);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_1), OFFSET_1);\n\n\t\tfinal TestDeserializationSchema deserializationSchema = new TestDeserializationSchema(\n\t\t\tTableSchema.builder()\n\t\t\t\t.field(NAME, Types.STRING())\n\t\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t\t.field(TIME, Types.SQL_TIMESTAMP())\n\t\t\t\t.build()\n\t\t\t\t.toRowType()\n\t\t);\n\n\t\tfinal StartupMode startupMode = StartupMode.SPECIFIC_OFFSETS;\n\n\t\tfinal KafkaTableSource expected = getExpectedKafkaTableSource(\n\t\t\tschema,\n\t\t\tOptional.of(PROC_TIME),\n\t\t\trowtimeAttributeDescriptors,\n\t\t\tfieldMapping,\n\t\t\tTOPIC,\n\t\t\tKAFKA_PROPERTIES,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(PARTITION_0, OFFSET_0);\n\t\toffsets.put(PARTITION_1, OFFSET_1);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(getKafkaVersion())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(KAFKA_PROPERTIES)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(new TestTableFormat())\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t.field(FRUIT_NAME, Types.STRING()).from(NAME)\n\t\t\t\t\t.field(COUNT, Types.DECIMAL()) // no from so it must match with the input\n\t\t\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP()).rowtime(\n\t\t\t\t\t\tnew Rowtime().timestampsFromField(TIME).watermarksPeriodicAscending())\n\t\t\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP()).proctime());\n\n\t\tfinal TableSource<?> actualSource = TableSourceFactoryService.findAndCreateTableSource(testDesc);\n\n\t\tassertEquals(expected, actualSource);\n\n\t\t// test Kafka consumer\n\t\tfinal KafkaTableSource actualKafkaSource = (KafkaTableSource) actualSource;\n\t\tfinal StreamExecutionEnvironmentMock mock = new StreamExecutionEnvironmentMock();\n\t\tactualKafkaSource.getDataStream(mock);\n\t\tassertTrue(getExpectedFlinkKafkaConsumer().isAssignableFrom(mock.function.getClass()));\n\t}",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147 +\n 148 +\n 149  \n 150 +\n 151 +\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  ",
            "\t@Test\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void testTableSource() {\n\n\t\t// prepare parameters for Kafka table source\n\n\t\tfinal TableSchema schema = TableSchema.builder()\n\t\t\t.field(FRUIT_NAME, Types.STRING())\n\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.build();\n\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors = Collections.singletonList(\n\t\t\tnew RowtimeAttributeDescriptor(EVENT_TIME, new ExistingField(TIME), new AscendingTimestamps()));\n\n\t\tfinal Map<String, String> fieldMapping = new HashMap<>();\n\t\tfieldMapping.put(FRUIT_NAME, NAME);\n\t\tfieldMapping.put(COUNT, COUNT);\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_0), OFFSET_0);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_1), OFFSET_1);\n\n\t\tfinal TestDeserializationSchema deserializationSchema = new TestDeserializationSchema(\n\t\t\tTableSchema.builder()\n\t\t\t\t.field(NAME, Types.STRING())\n\t\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t\t.field(TIME, Types.SQL_TIMESTAMP())\n\t\t\t\t.build()\n\t\t\t\t.toRowType()\n\t\t);\n\n\t\tfinal StartupMode startupMode = StartupMode.SPECIFIC_OFFSETS;\n\n\t\tfinal KafkaTableSource expected = getExpectedKafkaTableSource(\n\t\t\tschema,\n\t\t\tOptional.of(PROC_TIME),\n\t\t\trowtimeAttributeDescriptors,\n\t\t\tfieldMapping,\n\t\t\tTOPIC,\n\t\t\tKAFKA_PROPERTIES,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(PARTITION_0, OFFSET_0);\n\t\toffsets.put(PARTITION_1, OFFSET_1);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(getKafkaVersion())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(KAFKA_PROPERTIES)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(new TestTableFormat())\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t.field(FRUIT_NAME, Types.STRING()).from(NAME)\n\t\t\t\t\t.field(COUNT, Types.DECIMAL()) // no from so it must match with the input\n\t\t\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP()).rowtime(\n\t\t\t\t\t\tnew Rowtime().timestampsFromField(TIME).watermarksPeriodicAscending())\n\t\t\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP()).proctime());\n\t\tfinal DescriptorProperties descriptorProperties = new DescriptorProperties(true);\n\t\ttestDesc.addProperties(descriptorProperties);\n\t\tfinal Map<String, String> propertiesMap = descriptorProperties.asMap();\n\n\t\tfinal TableSource<?> actualSource = TableFactoryService.find(TableSourceFactory.class, testDesc)\n\t\t\t.createTableSource(propertiesMap);\n\n\t\tassertEquals(expected, actualSource);\n\n\t\t// test Kafka consumer\n\t\tfinal KafkaTableSource actualKafkaSource = (KafkaTableSource) actualSource;\n\t\tfinal StreamExecutionEnvironmentMock mock = new StreamExecutionEnvironmentMock();\n\t\tactualKafkaSource.getDataStream(mock);\n\t\tassertTrue(getExpectedFlinkKafkaConsumer().isAssignableFrom(mock.function.getClass()));\n\t}"
        ],
        [
            "JsonRowFormatFactoryTest::testSchemaSerializationSchema(Map)",
            " 118  \n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  ",
            "\tprivate void testSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFormatFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema expected1 = new JsonRowSerializationSchema(SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}",
            " 118  \n 119 +\n 120  \n 121  \n 122  \n 123  \n 124  ",
            "\tprivate void testSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema expected1 = new JsonRowSerializationSchema(SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}"
        ],
        [
            "JsonRowFormatFactoryTest::testJsonSchemaDeserializationSchema(Map)",
            " 126  \n 127 -\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  ",
            "\tprivate void testJsonSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFormatFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal JsonRowDeserializationSchema expected2 = new JsonRowDeserializationSchema(JSON_SCHEMA);\n\t\texpected2.setFailOnMissingField(true);\n\t\tassertEquals(expected2, actual2);\n\t}",
            " 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  ",
            "\tprivate void testJsonSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal JsonRowDeserializationSchema expected2 = new JsonRowDeserializationSchema(JSON_SCHEMA);\n\t\texpected2.setFailOnMissingField(true);\n\t\tassertEquals(expected2, actual2);\n\t}"
        ],
        [
            "AvroRowFormatFactoryTest::testAvroSchemaDeserializationSchema(Map)",
            "  81  \n  82 -\n  83  \n  84  \n  85  \n  86  \n  87  ",
            "\tprivate void testAvroSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFormatFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal AvroRowDeserializationSchema expected2 = new AvroRowDeserializationSchema(AVRO_SCHEMA);\n\t\tassertEquals(expected2, actual2);\n\t}",
            "  81  \n  82 +\n  83  \n  84  \n  85  \n  86  \n  87  ",
            "\tprivate void testAvroSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal AvroRowDeserializationSchema expected2 = new AvroRowDeserializationSchema(AVRO_SCHEMA);\n\t\tassertEquals(expected2, actual2);\n\t}"
        ]
    ],
    "5eae8bd423256fb372a57151e482c501c955c008": [
        [
            "Kafka08Fetcher::runFetchLoop()",
            " 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 -\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  ",
            "\t@Override\n\tpublic void runFetchLoop() throws Exception {\n\t\t// the map from broker to the thread that is connected to that broker\n\t\tfinal Map<Node, SimpleConsumerThread<T>> brokerToThread = new HashMap<>();\n\n\t\t// this holds possible the exceptions from the concurrent broker connection threads\n\t\tfinal ExceptionProxy errorHandler = new ExceptionProxy(Thread.currentThread());\n\n\t\t// the offset handler handles the communication with ZooKeeper, to commit externally visible offsets\n\t\tfinal ZookeeperOffsetHandler zookeeperOffsetHandler = new ZookeeperOffsetHandler(kafkaConfig);\n\t\tthis.zookeeperOffsetHandler = zookeeperOffsetHandler;\n\n\t\tPeriodicOffsetCommitter periodicCommitter = null;\n\t\ttry {\n\n\t\t\t// offsets in the state may still be placeholder sentinel values if we are starting fresh, or the\n\t\t\t// checkpoint / savepoint state we were restored with had not completely been replaced with actual offset\n\t\t\t// values yet; replace those with actual offsets, according to what the sentinel value represent.\n\t\t\tfor (KafkaTopicPartitionState<TopicAndPartition> partition : subscribedPartitionStates()) {\n\t\t\t\tif (partition.getOffset() == KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET) {\n\t\t\t\t\t// this will be replaced by an actual offset in SimpleConsumerThread\n\t\t\t\t\tpartition.setOffset(OffsetRequest.EarliestTime());\n\t\t\t\t} else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.LATEST_OFFSET) {\n\t\t\t\t\t// this will be replaced by an actual offset in SimpleConsumerThread\n\t\t\t\t\tpartition.setOffset(OffsetRequest.LatestTime());\n\t\t\t\t} else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.GROUP_OFFSET) {\n\t\t\t\t\tLong committedOffset = zookeeperOffsetHandler.getCommittedOffset(partition.getKafkaTopicPartition());\n\t\t\t\t\tif (committedOffset != null) {\n\t\t\t\t\t\t// the committed offset in ZK represents the next record to process,\n\t\t\t\t\t\t// so we subtract it by 1 to correctly represent internal state\n\t\t\t\t\t\tpartition.setOffset(committedOffset - 1);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// if we can't find an offset for a partition in ZK when using GROUP_OFFSETS,\n\t\t\t\t\t\t// we default to \"auto.offset.reset\" like the Kafka high-level consumer\n\t\t\t\t\t\tLOG.warn(\"No group offset can be found for partition {} in Zookeeper;\" +\n\t\t\t\t\t\t\t\" resetting starting offset to 'auto.offset.reset'\", partition);\n\n\t\t\t\t\t\tpartition.setOffset(invalidOffsetBehavior);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// the partition already has a specific start offset and is ready to be consumed\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// start the periodic offset committer thread, if necessary\n\t\t\tif (autoCommitInterval > 0) {\n\t\t\t\tLOG.info(\"Starting periodic offset committer, with commit interval of {}ms\", autoCommitInterval);\n\n\t\t\t\tperiodicCommitter = new PeriodicOffsetCommitter(\n\t\t\t\t\t\tzookeeperOffsetHandler,\n\t\t\t\t\t\tsubscribedPartitionStates(),\n\t\t\t\t\t\terrorHandler,\n\t\t\t\t\t\tautoCommitInterval);\n\t\t\t\tperiodicCommitter.setName(\"Periodic Kafka partition offset committer\");\n\t\t\t\tperiodicCommitter.setDaemon(true);\n\t\t\t\tperiodicCommitter.start();\n\t\t\t}\n\n\t\t\t// Main loop polling elements from the unassignedPartitions queue to the threads\n\t\t\twhile (running) {\n\t\t\t\t// re-throw any exception from the concurrent fetcher threads\n\t\t\t\terrorHandler.checkAndThrowException();\n\n\t\t\t\t// wait for max 5 seconds trying to get partitions to assign\n\t\t\t\t// if threads shut down, this poll returns earlier, because the threads inject the\n\t\t\t\t// special marker into the queue\n\t\t\t\tList<KafkaTopicPartitionState<TopicAndPartition>> partitionsToAssign =\n\t\t\t\t\t\tunassignedPartitionsQueue.getBatchBlocking(5000);\n\t\t\t\tpartitionsToAssign.remove(MARKER);\n\n\t\t\t\tif (!partitionsToAssign.isEmpty()) {\n\t\t\t\t\tLOG.info(\"Assigning {} partitions to broker threads\", partitionsToAssign.size());\n\t\t\t\t\tMap<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeaders =\n\t\t\t\t\t\t\tfindLeaderForPartitions(partitionsToAssign, kafkaConfig);\n\n\t\t\t\t\t// assign the partitions to the leaders (maybe start the threads)\n\t\t\t\t\tfor (Map.Entry<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeader :\n\t\t\t\t\t\t\tpartitionsWithLeaders.entrySet()) {\n\t\t\t\t\t\tfinal Node leader = partitionsWithLeader.getKey();\n\t\t\t\t\t\tfinal List<KafkaTopicPartitionState<TopicAndPartition>> partitions = partitionsWithLeader.getValue();\n\t\t\t\t\t\tSimpleConsumerThread<T> brokerThread = brokerToThread.get(leader);\n\n\t\t\t\t\t\tif (!running) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (brokerThread == null || !brokerThread.getNewPartitionsQueue().isOpen()) {\n\t\t\t\t\t\t\t// start new thread\n\t\t\t\t\t\t\tbrokerThread = createAndStartSimpleConsumerThread(partitions, leader, errorHandler);\n\t\t\t\t\t\t\tbrokerToThread.put(leader, brokerThread);\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// put elements into queue of thread\n\t\t\t\t\t\t\tClosableBlockingQueue<KafkaTopicPartitionState<TopicAndPartition>> newPartitionsQueue =\n\t\t\t\t\t\t\t\t\tbrokerThread.getNewPartitionsQueue();\n\n\t\t\t\t\t\t\tfor (KafkaTopicPartitionState<TopicAndPartition> fp : partitions) {\n\t\t\t\t\t\t\t\tif (!newPartitionsQueue.addIfOpen(fp)) {\n\t\t\t\t\t\t\t\t\t// we were unable to add the partition to the broker's queue\n\t\t\t\t\t\t\t\t\t// the broker has closed in the meantime (the thread will shut down)\n\t\t\t\t\t\t\t\t\t// create a new thread for connecting to this broker\n\t\t\t\t\t\t\t\t\tList<KafkaTopicPartitionState<TopicAndPartition>> seedPartitions = new ArrayList<>();\n\t\t\t\t\t\t\t\t\tseedPartitions.add(fp);\n\t\t\t\t\t\t\t\t\tbrokerThread = createAndStartSimpleConsumerThread(seedPartitions, leader, errorHandler);\n\t\t\t\t\t\t\t\t\tbrokerToThread.put(leader, brokerThread);\n\t\t\t\t\t\t\t\t\tnewPartitionsQueue = brokerThread.getNewPartitionsQueue(); // update queue for the subsequent partitions\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// there were no partitions to assign. Check if any broker threads shut down.\n\t\t\t\t\t// we get into this section of the code, if either the poll timed out, or the\n\t\t\t\t\t// blocking poll was woken up by the marker element\n\t\t\t\t\tIterator<SimpleConsumerThread<T>> bttIterator = brokerToThread.values().iterator();\n\t\t\t\t\twhile (bttIterator.hasNext()) {\n\t\t\t\t\t\tSimpleConsumerThread<T> thread = bttIterator.next();\n\t\t\t\t\t\tif (!thread.getNewPartitionsQueue().isOpen()) {\n\t\t\t\t\t\t\tLOG.info(\"Removing stopped consumer thread {}\", thread.getName());\n\t\t\t\t\t\t\tbttIterator.remove();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (brokerToThread.size() == 0 && unassignedPartitionsQueue.isEmpty()) {\n\t\t\t\t\tif (unassignedPartitionsQueue.close()) {\n\t\t\t\t\t\tLOG.info(\"All consumer threads are finished, there are no more unassigned partitions. Stopping fetcher\");\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t// we end up here if somebody added something to the queue in the meantime --> continue to poll queue again\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcatch (InterruptedException e) {\n\t\t\t// this may be thrown because an exception on one of the concurrent fetcher threads\n\t\t\t// woke this thread up. make sure we throw the root exception instead in that case\n\t\t\terrorHandler.checkAndThrowException();\n\n\t\t\t// no other root exception, throw the interrupted exception\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tthis.running = false;\n\t\t\tthis.zookeeperOffsetHandler = null;\n\n\t\t\t// if we run a periodic committer thread, shut that down\n\t\t\tif (periodicCommitter != null) {\n\t\t\t\tperiodicCommitter.shutdown();\n\t\t\t}\n\n\t\t\t// clear the interruption flag\n\t\t\t// this allows the joining on consumer threads (on best effort) to happen in\n\t\t\t// case the initial interrupt already\n\t\t\tThread.interrupted();\n\n\t\t\t// make sure that in any case (completion, abort, error), all spawned threads are stopped\n\t\t\ttry {\n\t\t\t\tint runningThreads;\n\t\t\t\tdo {\n\t\t\t\t\t// check whether threads are alive and cancel them\n\t\t\t\t\trunningThreads = 0;\n\t\t\t\t\tIterator<SimpleConsumerThread<T>> threads = brokerToThread.values().iterator();\n\t\t\t\t\twhile (threads.hasNext()) {\n\t\t\t\t\t\tSimpleConsumerThread<?> t = threads.next();\n\t\t\t\t\t\tif (t.isAlive()) {\n\t\t\t\t\t\t\tt.cancel();\n\t\t\t\t\t\t\trunningThreads++;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthreads.remove();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// wait for the threads to finish, before issuing a cancel call again\n\t\t\t\t\tif (runningThreads > 0) {\n\t\t\t\t\t\tfor (SimpleConsumerThread<?> t : brokerToThread.values()) {\n\t\t\t\t\t\t\tt.join(500 / runningThreads + 1);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\twhile (runningThreads > 0);\n\t\t\t}\n\t\t\tcatch (InterruptedException ignored) {\n\t\t\t\t// waiting for the thread shutdown apparently got interrupted\n\t\t\t\t// restore interrupted state and continue\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// we catch all here to preserve the original exception\n\t\t\t\tLOG.error(\"Exception while shutting down consumer threads\", t);\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tzookeeperOffsetHandler.close();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// we catch all here to preserve the original exception\n\t\t\t\tLOG.error(\"Exception while shutting down ZookeeperOffsetHandler\", t);\n\t\t\t}\n\t\t}\n\t}",
            " 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 +\n 193 +\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  ",
            "\t@Override\n\tpublic void runFetchLoop() throws Exception {\n\t\t// the map from broker to the thread that is connected to that broker\n\t\tfinal Map<Node, SimpleConsumerThread<T>> brokerToThread = new HashMap<>();\n\n\t\t// this holds possible the exceptions from the concurrent broker connection threads\n\t\tfinal ExceptionProxy errorHandler = new ExceptionProxy(Thread.currentThread());\n\n\t\t// the offset handler handles the communication with ZooKeeper, to commit externally visible offsets\n\t\tfinal ZookeeperOffsetHandler zookeeperOffsetHandler = new ZookeeperOffsetHandler(kafkaConfig);\n\t\tthis.zookeeperOffsetHandler = zookeeperOffsetHandler;\n\n\t\tPeriodicOffsetCommitter periodicCommitter = null;\n\t\ttry {\n\n\t\t\t// offsets in the state may still be placeholder sentinel values if we are starting fresh, or the\n\t\t\t// checkpoint / savepoint state we were restored with had not completely been replaced with actual offset\n\t\t\t// values yet; replace those with actual offsets, according to what the sentinel value represent.\n\t\t\tfor (KafkaTopicPartitionState<TopicAndPartition> partition : subscribedPartitionStates()) {\n\t\t\t\tif (partition.getOffset() == KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET) {\n\t\t\t\t\t// this will be replaced by an actual offset in SimpleConsumerThread\n\t\t\t\t\tpartition.setOffset(OffsetRequest.EarliestTime());\n\t\t\t\t} else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.LATEST_OFFSET) {\n\t\t\t\t\t// this will be replaced by an actual offset in SimpleConsumerThread\n\t\t\t\t\tpartition.setOffset(OffsetRequest.LatestTime());\n\t\t\t\t} else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.GROUP_OFFSET) {\n\t\t\t\t\tLong committedOffset = zookeeperOffsetHandler.getCommittedOffset(partition.getKafkaTopicPartition());\n\t\t\t\t\tif (committedOffset != null) {\n\t\t\t\t\t\t// the committed offset in ZK represents the next record to process,\n\t\t\t\t\t\t// so we subtract it by 1 to correctly represent internal state\n\t\t\t\t\t\tpartition.setOffset(committedOffset - 1);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// if we can't find an offset for a partition in ZK when using GROUP_OFFSETS,\n\t\t\t\t\t\t// we default to \"auto.offset.reset\" like the Kafka high-level consumer\n\t\t\t\t\t\tLOG.warn(\"No group offset can be found for partition {} in Zookeeper;\" +\n\t\t\t\t\t\t\t\" resetting starting offset to 'auto.offset.reset'\", partition);\n\n\t\t\t\t\t\tpartition.setOffset(invalidOffsetBehavior);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// the partition already has a specific start offset and is ready to be consumed\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// start the periodic offset committer thread, if necessary\n\t\t\tif (autoCommitInterval > 0) {\n\t\t\t\tLOG.info(\"Starting periodic offset committer, with commit interval of {}ms\", autoCommitInterval);\n\n\t\t\t\tperiodicCommitter = new PeriodicOffsetCommitter(\n\t\t\t\t\t\tzookeeperOffsetHandler,\n\t\t\t\t\t\tsubscribedPartitionStates(),\n\t\t\t\t\t\terrorHandler,\n\t\t\t\t\t\tautoCommitInterval);\n\t\t\t\tperiodicCommitter.setName(\"Periodic Kafka partition offset committer\");\n\t\t\t\tperiodicCommitter.setDaemon(true);\n\t\t\t\tperiodicCommitter.start();\n\t\t\t}\n\n\t\t\t// Main loop polling elements from the unassignedPartitions queue to the threads\n\t\t\twhile (running) {\n\t\t\t\t// re-throw any exception from the concurrent fetcher threads\n\t\t\t\terrorHandler.checkAndThrowException();\n\n\t\t\t\t// wait for max 5 seconds trying to get partitions to assign\n\t\t\t\t// if threads shut down, this poll returns earlier, because the threads inject the\n\t\t\t\t// special marker into the queue\n\t\t\t\tList<KafkaTopicPartitionState<TopicAndPartition>> partitionsToAssign =\n\t\t\t\t\t\tunassignedPartitionsQueue.getBatchBlocking(5000);\n\t\t\t\t// note: if there are more markers, remove them all\n\t\t\t\tpartitionsToAssign.removeIf(MARKER::equals);\n\n\t\t\t\tif (!partitionsToAssign.isEmpty()) {\n\t\t\t\t\tLOG.info(\"Assigning {} partitions to broker threads\", partitionsToAssign.size());\n\t\t\t\t\tMap<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeaders =\n\t\t\t\t\t\t\tfindLeaderForPartitions(partitionsToAssign, kafkaConfig);\n\n\t\t\t\t\t// assign the partitions to the leaders (maybe start the threads)\n\t\t\t\t\tfor (Map.Entry<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeader :\n\t\t\t\t\t\t\tpartitionsWithLeaders.entrySet()) {\n\t\t\t\t\t\tfinal Node leader = partitionsWithLeader.getKey();\n\t\t\t\t\t\tfinal List<KafkaTopicPartitionState<TopicAndPartition>> partitions = partitionsWithLeader.getValue();\n\t\t\t\t\t\tSimpleConsumerThread<T> brokerThread = brokerToThread.get(leader);\n\n\t\t\t\t\t\tif (!running) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (brokerThread == null || !brokerThread.getNewPartitionsQueue().isOpen()) {\n\t\t\t\t\t\t\t// start new thread\n\t\t\t\t\t\t\tbrokerThread = createAndStartSimpleConsumerThread(partitions, leader, errorHandler);\n\t\t\t\t\t\t\tbrokerToThread.put(leader, brokerThread);\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// put elements into queue of thread\n\t\t\t\t\t\t\tClosableBlockingQueue<KafkaTopicPartitionState<TopicAndPartition>> newPartitionsQueue =\n\t\t\t\t\t\t\t\t\tbrokerThread.getNewPartitionsQueue();\n\n\t\t\t\t\t\t\tfor (KafkaTopicPartitionState<TopicAndPartition> fp : partitions) {\n\t\t\t\t\t\t\t\tif (!newPartitionsQueue.addIfOpen(fp)) {\n\t\t\t\t\t\t\t\t\t// we were unable to add the partition to the broker's queue\n\t\t\t\t\t\t\t\t\t// the broker has closed in the meantime (the thread will shut down)\n\t\t\t\t\t\t\t\t\t// create a new thread for connecting to this broker\n\t\t\t\t\t\t\t\t\tList<KafkaTopicPartitionState<TopicAndPartition>> seedPartitions = new ArrayList<>();\n\t\t\t\t\t\t\t\t\tseedPartitions.add(fp);\n\t\t\t\t\t\t\t\t\tbrokerThread = createAndStartSimpleConsumerThread(seedPartitions, leader, errorHandler);\n\t\t\t\t\t\t\t\t\tbrokerToThread.put(leader, brokerThread);\n\t\t\t\t\t\t\t\t\tnewPartitionsQueue = brokerThread.getNewPartitionsQueue(); // update queue for the subsequent partitions\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// there were no partitions to assign. Check if any broker threads shut down.\n\t\t\t\t\t// we get into this section of the code, if either the poll timed out, or the\n\t\t\t\t\t// blocking poll was woken up by the marker element\n\t\t\t\t\tIterator<SimpleConsumerThread<T>> bttIterator = brokerToThread.values().iterator();\n\t\t\t\t\twhile (bttIterator.hasNext()) {\n\t\t\t\t\t\tSimpleConsumerThread<T> thread = bttIterator.next();\n\t\t\t\t\t\tif (!thread.getNewPartitionsQueue().isOpen()) {\n\t\t\t\t\t\t\tLOG.info(\"Removing stopped consumer thread {}\", thread.getName());\n\t\t\t\t\t\t\tbttIterator.remove();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (brokerToThread.size() == 0 && unassignedPartitionsQueue.isEmpty()) {\n\t\t\t\t\tif (unassignedPartitionsQueue.close()) {\n\t\t\t\t\t\tLOG.info(\"All consumer threads are finished, there are no more unassigned partitions. Stopping fetcher\");\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t// we end up here if somebody added something to the queue in the meantime --> continue to poll queue again\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcatch (InterruptedException e) {\n\t\t\t// this may be thrown because an exception on one of the concurrent fetcher threads\n\t\t\t// woke this thread up. make sure we throw the root exception instead in that case\n\t\t\terrorHandler.checkAndThrowException();\n\n\t\t\t// no other root exception, throw the interrupted exception\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tthis.running = false;\n\t\t\tthis.zookeeperOffsetHandler = null;\n\n\t\t\t// if we run a periodic committer thread, shut that down\n\t\t\tif (periodicCommitter != null) {\n\t\t\t\tperiodicCommitter.shutdown();\n\t\t\t}\n\n\t\t\t// clear the interruption flag\n\t\t\t// this allows the joining on consumer threads (on best effort) to happen in\n\t\t\t// case the initial interrupt already\n\t\t\tThread.interrupted();\n\n\t\t\t// make sure that in any case (completion, abort, error), all spawned threads are stopped\n\t\t\ttry {\n\t\t\t\tint runningThreads;\n\t\t\t\tdo {\n\t\t\t\t\t// check whether threads are alive and cancel them\n\t\t\t\t\trunningThreads = 0;\n\t\t\t\t\tIterator<SimpleConsumerThread<T>> threads = brokerToThread.values().iterator();\n\t\t\t\t\twhile (threads.hasNext()) {\n\t\t\t\t\t\tSimpleConsumerThread<?> t = threads.next();\n\t\t\t\t\t\tif (t.isAlive()) {\n\t\t\t\t\t\t\tt.cancel();\n\t\t\t\t\t\t\trunningThreads++;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthreads.remove();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// wait for the threads to finish, before issuing a cancel call again\n\t\t\t\t\tif (runningThreads > 0) {\n\t\t\t\t\t\tfor (SimpleConsumerThread<?> t : brokerToThread.values()) {\n\t\t\t\t\t\t\tt.join(500 / runningThreads + 1);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\twhile (runningThreads > 0);\n\t\t\t}\n\t\t\tcatch (InterruptedException ignored) {\n\t\t\t\t// waiting for the thread shutdown apparently got interrupted\n\t\t\t\t// restore interrupted state and continue\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// we catch all here to preserve the original exception\n\t\t\t\tLOG.error(\"Exception while shutting down consumer threads\", t);\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tzookeeperOffsetHandler.close();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// we catch all here to preserve the original exception\n\t\t\t\tLOG.error(\"Exception while shutting down ZookeeperOffsetHandler\", t);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "FlinkKafkaConsumerBase::run(SourceContext)",
            " 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712 -\n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  ",
            "\t@Override\n\tpublic void run(SourceContext<T> sourceContext) throws Exception {\n\t\tif (subscribedPartitionsToStartOffsets == null) {\n\t\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t\t}\n\n\t\t// initialize commit metrics and default offset callback method\n\t\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\t\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\t\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t\t@Override\n\t\t\tpublic void onSuccess() {\n\t\t\t\tsuccessfulCommits.inc();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void onException(Throwable cause) {\n\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\t\tfailedCommits.inc();\n\t\t\t}\n\t\t};\n\n\t\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t\t// status will automatically be triggered back to be active.\n\t\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\t\tsourceContext.markAsTemporarilyIdle();\n\t\t}\n\n\t\t// from this point forward:\n\t\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t\t//     Kafka through the fetcher, if configured to do so)\n\t\tthis.kafkaFetcher = createFetcher(\n\t\t\t\tsourceContext,\n\t\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\t\tperiodicWatermarkAssigner,\n\t\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\t\toffsetCommitMode,\n\t\t\t\tgetRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n\t\t\t\tuseMetrics);\n\n\t\tif (!running) {\n\t\t\treturn;\n\t\t}\n\n\t\t// depending on whether we were restored with the current state version (1.3),\n\t\t// remaining logic branches off into 2 paths:\n\t\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t\t//                 thread running the main fetcher loop\n\t\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\t\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\t\twhile (running) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t\t} finally {\n\t\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\t\tcancel();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tdiscoveryLoopThread.start();\n\t\t\tkafkaFetcher.runFetchLoop();\n\n\t\t\t// --------------------------------------------------------------------\n\n\t\t\t// make sure that the partition discoverer is properly closed\n\t\t\tpartitionDiscoverer.close();\n\t\t\tdiscoveryLoopThread.join();\n\n\t\t\t// rethrow any fetcher errors\n\t\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\t\tif (discoveryLoopError != null) {\n\t\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t\t}\n\t\t} else {\n\t\t\t// won't be using the discoverer\n\t\t\tpartitionDiscoverer.close();\n\n\t\t\tkafkaFetcher.runFetchLoop();\n\t\t}\n\t}",
            " 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712 +\n 713 +\n 714 +\n 715 +\n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  ",
            "\t@Override\n\tpublic void run(SourceContext<T> sourceContext) throws Exception {\n\t\tif (subscribedPartitionsToStartOffsets == null) {\n\t\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t\t}\n\n\t\t// initialize commit metrics and default offset callback method\n\t\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\t\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\t\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t\t@Override\n\t\t\tpublic void onSuccess() {\n\t\t\t\tsuccessfulCommits.inc();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void onException(Throwable cause) {\n\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\t\tfailedCommits.inc();\n\t\t\t}\n\t\t};\n\n\t\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t\t// status will automatically be triggered back to be active.\n\t\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\t\tsourceContext.markAsTemporarilyIdle();\n\t\t}\n\n\t\t// from this point forward:\n\t\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t\t//     Kafka through the fetcher, if configured to do so)\n\t\tthis.kafkaFetcher = createFetcher(\n\t\t\t\tsourceContext,\n\t\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\t\tperiodicWatermarkAssigner,\n\t\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\t\toffsetCommitMode,\n\t\t\t\tgetRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n\t\t\t\tuseMetrics);\n\n\t\tif (!running) {\n\t\t\treturn;\n\t\t}\n\n\t\t// depending on whether we were restored with the current state version (1.3),\n\t\t// remaining logic branches off into 2 paths:\n\t\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t\t//                 thread running the main fetcher loop\n\t\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\t\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\t\twhile (running) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t\t} finally {\n\t\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\t\t// (if not running, cancel() was already called)\n\t\t\t\t\t\tif (running) {\n\t\t\t\t\t\t\tcancel();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tdiscoveryLoopThread.start();\n\t\t\tkafkaFetcher.runFetchLoop();\n\n\t\t\t// --------------------------------------------------------------------\n\n\t\t\t// make sure that the partition discoverer is properly closed\n\t\t\tpartitionDiscoverer.close();\n\t\t\tdiscoveryLoopThread.join();\n\n\t\t\t// rethrow any fetcher errors\n\t\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\t\tif (discoveryLoopError != null) {\n\t\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t\t}\n\t\t} else {\n\t\t\t// won't be using the discoverer\n\t\t\tpartitionDiscoverer.close();\n\n\t\t\tkafkaFetcher.runFetchLoop();\n\t\t}\n\t}"
        ]
    ],
    "fa11015ddcdb0217da6a8b2be1e2a55efd26d7fa": [
        [
            "YarnResourceManager::onContainersCompleted(List)",
            " 331  \n 332 -\n 333  \n 334 -\n 335 -\n 336 -\n 337 -\n 338  \n 339 -\n 340  \n 341  ",
            "\t@Override\n\tpublic void onContainersCompleted(List<ContainerStatus> list) {\n\t\trunAsync(() -> {\n\t\t\t\tfor (ContainerStatus container : list) {\n\t\t\t\t\tif (container.getExitStatus() < 0) {\n\t\t\t\t\t\tcloseTaskManagerConnection(new ResourceID(\n\t\t\t\t\t\t\tcontainer.getContainerId().toString()), new Exception(container.getDiagnostics()));\n\t\t\t\t\t}\n\t\t\t\t\tworkerNodeMap.remove(new ResourceID(container.getContainerId().toString()));\n\t\t\t\t}\n\t\t\t}",
            " 326  \n 327 +\n 328  \n 329 +\n 330 +\n 331 +\n 332 +\n 333 +\n 334 +\n 335 +\n 336 +\n 337 +\n 338 +\n 339  \n 340  \n 341  ",
            "\t@Override\n\tpublic void onContainersCompleted(final List<ContainerStatus> list) {\n\t\trunAsync(() -> {\n\t\t\t\tfor (final ContainerStatus containerStatus : list) {\n\n\t\t\t\t\tfinal ResourceID resourceId = new ResourceID(containerStatus.getContainerId().toString());\n\t\t\t\t\tfinal YarnWorkerNode yarnWorkerNode = workerNodeMap.remove(resourceId);\n\n\t\t\t\t\tif (yarnWorkerNode != null) {\n\t\t\t\t\t\t// Container completed unexpectedly ~> start a new one\n\t\t\t\t\t\tfinal Container container = yarnWorkerNode.getContainer();\n\t\t\t\t\t\trequestYarnContainer(container.getResource(), yarnWorkerNode.getContainer().getPriority());\n\t\t\t\t\t\tcloseTaskManagerConnection(resourceId, new Exception(containerStatus.getDiagnostics()));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}"
        ],
        [
            "YarnResourceManager::stopWorker(YarnWorkerNode)",
            " 297  \n 298 -\n 299 -\n 300 -\n 301 -\n 302 -\n 303 -\n 304 -\n 305 -\n 306 -\n 307 -\n 308 -\n 309 -\n 310 -\n 311 -\n 312  \n 313  \n 314  ",
            "\t@Override\n\tpublic boolean stopWorker(YarnWorkerNode workerNode) {\n\t\tif (workerNode != null) {\n\t\t\tContainer container = workerNode.getContainer();\n\t\t\tlog.info(\"Stopping container {}.\", container.getId());\n\t\t\t// release the container on the node manager\n\t\t\ttry {\n\t\t\t\tnodeManagerClient.stopContainer(container.getId(), container.getNodeId());\n\t\t\t} catch (Throwable t) {\n\t\t\t\tlog.warn(\"Error while calling YARN Node Manager to stop container\", t);\n\t\t\t}\n\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\tworkerNodeMap.remove(workerNode.getResourceID());\n\t\t} else {\n\t\t\tlog.error(\"Can not find container for null workerNode.\");\n\t\t}\n\t\treturn true;\n\t}",
            " 297  \n 298 +\n 299 +\n 300 +\n 301 +\n 302 +\n 303 +\n 304 +\n 305  \n 306 +\n 307 +\n 308  \n 309  ",
            "\t@Override\n\tpublic boolean stopWorker(final YarnWorkerNode workerNode) {\n\t\tfinal Container container = workerNode.getContainer();\n\t\tlog.info(\"Stopping container {}.\", container.getId());\n\t\ttry {\n\t\t\tnodeManagerClient.stopContainer(container.getId(), container.getNodeId());\n\t\t} catch (final Exception e) {\n\t\t\tlog.warn(\"Error while calling YARN Node Manager to stop container\", e);\n\t\t}\n\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\tworkerNodeMap.remove(workerNode.getResourceID());\n\t\treturn true;\n\t}"
        ],
        [
            "YarnResourceManager::onContainersAllocated(List)",
            " 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359 -\n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  ",
            "\t@Override\n\tpublic void onContainersAllocated(List<Container> containers) {\n\t\trunAsync(() -> {\n\t\t\tfor (Container container : containers) {\n\t\t\t\tlog.info(\n\t\t\t\t\t\"Received new container: {} - Remaining pending container requests: {}\",\n\t\t\t\t\tcontainer.getId(),\n\t\t\t\t\tnumPendingContainerRequests);\n\n\t\t\t\tif (numPendingContainerRequests > 0) {\n\t\t\t\t\tnumPendingContainerRequests--;\n\n\t\t\t\t\tfinal String containerIdStr = container.getId().toString();\n\n\t\t\t\t\tworkerNodeMap.put(new ResourceID(containerIdStr), new YarnWorkerNode(container));\n\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// Context information used to start a TaskExecutor Java process\n\t\t\t\t\t\tContainerLaunchContext taskExecutorLaunchContext = createTaskExecutorLaunchContext(\n\t\t\t\t\t\t\tcontainer.getResource(),\n\t\t\t\t\t\t\tcontainerIdStr,\n\t\t\t\t\t\t\tcontainer.getNodeId().getHost());\n\n\t\t\t\t\t\tnodeManagerClient.startContainer(container, taskExecutorLaunchContext);\n\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\tlog.error(\"Could not start TaskManager in container {}.\", container.getId(), t);\n\n\t\t\t\t\t\t// release the failed container\n\t\t\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\t\t\t// and ask for a new one\n\t\t\t\t\t\trequestYarnContainer(container.getResource(), container.getPriority());\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// return the excessive containers\n\t\t\t\t\tlog.info(\"Returning excess container {}.\", container.getId());\n\t\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// if we are waiting for no further containers, we can go to the\n\t\t\t// regular heartbeat interval\n\t\t\tif (numPendingContainerRequests <= 0) {\n\t\t\t\tresourceManagerClient.setHeartbeatInterval(yarnHeartbeatIntervalMillis);\n\t\t\t}\n\t\t});\n\t}",
            " 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358 +\n 359  \n 360 +\n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374 +\n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  ",
            "\t@Override\n\tpublic void onContainersAllocated(List<Container> containers) {\n\t\trunAsync(() -> {\n\t\t\tfor (Container container : containers) {\n\t\t\t\tlog.info(\n\t\t\t\t\t\"Received new container: {} - Remaining pending container requests: {}\",\n\t\t\t\t\tcontainer.getId(),\n\t\t\t\t\tnumPendingContainerRequests);\n\n\t\t\t\tif (numPendingContainerRequests > 0) {\n\t\t\t\t\tnumPendingContainerRequests--;\n\n\t\t\t\t\tfinal String containerIdStr = container.getId().toString();\n\t\t\t\t\tfinal ResourceID resourceId = new ResourceID(containerIdStr);\n\n\t\t\t\t\tworkerNodeMap.put(resourceId, new YarnWorkerNode(container));\n\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// Context information used to start a TaskExecutor Java process\n\t\t\t\t\t\tContainerLaunchContext taskExecutorLaunchContext = createTaskExecutorLaunchContext(\n\t\t\t\t\t\t\tcontainer.getResource(),\n\t\t\t\t\t\t\tcontainerIdStr,\n\t\t\t\t\t\t\tcontainer.getNodeId().getHost());\n\n\t\t\t\t\t\tnodeManagerClient.startContainer(container, taskExecutorLaunchContext);\n\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\tlog.error(\"Could not start TaskManager in container {}.\", container.getId(), t);\n\n\t\t\t\t\t\t// release the failed container\n\t\t\t\t\t\tworkerNodeMap.remove(resourceId);\n\t\t\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\t\t\t// and ask for a new one\n\t\t\t\t\t\trequestYarnContainer(container.getResource(), container.getPriority());\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// return the excessive containers\n\t\t\t\t\tlog.info(\"Returning excess container {}.\", container.getId());\n\t\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// if we are waiting for no further containers, we can go to the\n\t\t\t// regular heartbeat interval\n\t\t\tif (numPendingContainerRequests <= 0) {\n\t\t\t\tresourceManagerClient.setHeartbeatInterval(yarnHeartbeatIntervalMillis);\n\t\t\t}\n\t\t});\n\t}"
        ]
    ],
    "d1a03dd239555298da9ac9be4ea94ccf52d9887b": [
        [
            "CassandraConnectorITCase::testCassandraTableSink()",
            " 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460 -\n 461 -\n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  ",
            "\t@Test\n\tpublic void testCassandraTableSink() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.getTableEnvironment(env);\n\n\t\tDataStreamSource<Row> source = env.fromCollection(rowCollection);\n\n\t\ttEnv.registerDataStreamInternal(\"testFlinkTable\", source);\n\n\t\ttEnv.sql(\"select * from testFlinkTable\").writeToSink(\n\t\t\tnew CassandraAppendTableSink(builder, injectTableName(INSERT_DATA_QUERY)));\n\n\t\tenv.execute();\n\t\tResultSet rs = session.execute(injectTableName(SELECT_DATA_QUERY));\n\n\t\t// validate that all input was correctly written to Cassandra\n\t\tList<Row> input = new ArrayList<>(rowCollection);\n\t\tList<com.datastax.driver.core.Row> output = rs.all();\n\t\tfor (com.datastax.driver.core.Row o : output) {\n\t\t\tRow cmp = new Row(3);\n\t\t\tcmp.setField(0, o.getString(0));\n\t\t\tcmp.setField(1, o.getInt(2));\n\t\t\tcmp.setField(2, o.getInt(1));\n\t\t\tAssert.assertTrue(\"Row \" + cmp + \" was written to Cassandra but not in input.\", input.remove(cmp));\n\t\t}\n\t\tAssert.assertTrue(\"The input data was not completely written to Cassandra\", input.isEmpty());\n\t}",
            " 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460 +\n 461 +\n 462 +\n 463 +\n 464 +\n 465  \n 466 +\n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  ",
            "\t@Test\n\tpublic void testCassandraTableSink() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.getTableEnvironment(env);\n\n\t\tDataStreamSource<Row> source = env.fromCollection(rowCollection);\n\n\t\ttEnv.registerDataStreamInternal(\"testFlinkTable\", source);\n\t\ttEnv.registerTableSink(\"cassandraTable\",\n\t\t\tnew CassandraAppendTableSink(builder, injectTableName(INSERT_DATA_QUERY))\n\t\t\t\t.configure(\n\t\t\t\t\tnew String[]{\"f0\", \"f1\", \"f2\"},\n\t\t\t\t\tnew TypeInformation[]{Types.STRING, Types.INT, Types.INT}));\n\n\t\ttEnv.sqlQuery(\"select * from testFlinkTable\").insertInto(\"cassandraTable\");\n\n\t\tenv.execute();\n\t\tResultSet rs = session.execute(injectTableName(SELECT_DATA_QUERY));\n\n\t\t// validate that all input was correctly written to Cassandra\n\t\tList<Row> input = new ArrayList<>(rowCollection);\n\t\tList<com.datastax.driver.core.Row> output = rs.all();\n\t\tfor (com.datastax.driver.core.Row o : output) {\n\t\t\tRow cmp = new Row(3);\n\t\t\tcmp.setField(0, o.getString(0));\n\t\t\tcmp.setField(1, o.getInt(2));\n\t\t\tcmp.setField(2, o.getInt(1));\n\t\t\tAssert.assertTrue(\"Row \" + cmp + \" was written to Cassandra but not in input.\", input.remove(cmp));\n\t\t}\n\t\tAssert.assertTrue(\"The input data was not completely written to Cassandra\", input.isEmpty());\n\t}"
        ]
    ],
    "39f796320fd5b3374f4d48222677257401e98286": [
        [
            "RpcCheckpointResponder::declineCheckpoint(JobID,ExecutionAttemptID,long,Throwable)",
            "  58  \n  59  \n  60  \n  61  \n  62 -\n  63  \n  64  \n  65 -\n  66  \n  67  ",
            "\t@Override\n\tpublic void declineCheckpoint(\n\t\t\tJobID jobID,\n\t\t\tExecutionAttemptID executionAttemptID,\n\t\t\tlong checkpointId, \n\t\t\tThrowable cause) {\n\n\t\tLOG.info(\"Declining checkpoint {} of job {}.\", checkpointId, jobID, cause);\n\t\tcheckpointCoordinatorGateway.declineCheckpoint(jobID, executionAttemptID, checkpointId, cause);\n\t}",
            "  53  \n  54  \n  55  \n  56  \n  57 +\n  58  \n  59  \n  60  \n  61  ",
            "\t@Override\n\tpublic void declineCheckpoint(\n\t\t\tJobID jobID,\n\t\t\tExecutionAttemptID executionAttemptID,\n\t\t\tlong checkpointId,\n\t\t\tThrowable cause) {\n\n\t\tcheckpointCoordinatorGateway.declineCheckpoint(jobID, executionAttemptID, checkpointId, cause);\n\t}"
        ],
        [
            "CheckpointCoordinator::discardCheckpoint(PendingCheckpoint,Throwable)",
            "1240  \n1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254 -\n1255 -\n1256 -\n1257  \n1258  \n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  ",
            "\t/**\n\t * Discards the given pending checkpoint because of the given cause.\n\t *\n\t * @param pendingCheckpoint to discard\n\t * @param cause for discarding the checkpoint\n\t */\n\tprivate void discardCheckpoint(PendingCheckpoint pendingCheckpoint, @Nullable Throwable cause) {\n\t\tassert(Thread.holdsLock(lock));\n\t\tPreconditions.checkNotNull(pendingCheckpoint);\n\n\t\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\n\t\tLOG.info(\"Discarding checkpoint {} of job {}.\", checkpointId, job, cause);\n\n\t\tif (cause != null) {\n\t\t\tpendingCheckpoint.abortError(cause);\n\t\t} else {\n\t\t\tpendingCheckpoint.abortDeclined();\n\t\t}\n\n\t\trememberRecentCheckpointId(checkpointId);\n\n\t\t// we don't have to schedule another \"dissolving\" checkpoint any more because the\n\t\t// cancellation barriers take care of breaking downstream alignments\n\t\t// we only need to make sure that suspended queued requests are resumed\n\n\t\tboolean haveMoreRecentPending = false;\n\t\tfor (PendingCheckpoint p : pendingCheckpoints.values()) {\n\t\t\tif (!p.isDiscarded() && p.getCheckpointId() >= pendingCheckpoint.getCheckpointId()) {\n\t\t\t\thaveMoreRecentPending = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!haveMoreRecentPending) {\n\t\t\ttriggerQueuedRequests();\n\t\t}\n\t}",
            "1241  \n1242  \n1243  \n1244  \n1245  \n1246  \n1247  \n1248  \n1249  \n1250  \n1251  \n1252  \n1253  \n1254  \n1255 +\n1256  \n1257 +\n1258 +\n1259  \n1260  \n1261  \n1262  \n1263  \n1264  \n1265  \n1266  \n1267  \n1268  \n1269  \n1270  \n1271  \n1272  \n1273  \n1274  \n1275  \n1276  \n1277  \n1278  ",
            "\t/**\n\t * Discards the given pending checkpoint because of the given cause.\n\t *\n\t * @param pendingCheckpoint to discard\n\t * @param cause for discarding the checkpoint\n\t */\n\tprivate void discardCheckpoint(PendingCheckpoint pendingCheckpoint, @Nullable Throwable cause) {\n\t\tassert(Thread.holdsLock(lock));\n\t\tPreconditions.checkNotNull(pendingCheckpoint);\n\n\t\tfinal long checkpointId = pendingCheckpoint.getCheckpointId();\n\n\t\tLOG.info(\"Discarding checkpoint {} of job {}.\", checkpointId, job, cause);\n\n\t\tif (cause == null || cause instanceof CheckpointDeclineException) {\n\t\t\tpendingCheckpoint.abortDeclined();\n\t\t} else {\n\t\t\tpendingCheckpoint.abortError(cause);\n\t\t}\n\n\t\trememberRecentCheckpointId(checkpointId);\n\n\t\t// we don't have to schedule another \"dissolving\" checkpoint any more because the\n\t\t// cancellation barriers take care of breaking downstream alignments\n\t\t// we only need to make sure that suspended queued requests are resumed\n\n\t\tboolean haveMoreRecentPending = false;\n\t\tfor (PendingCheckpoint p : pendingCheckpoints.values()) {\n\t\t\tif (!p.isDiscarded() && p.getCheckpointId() >= pendingCheckpoint.getCheckpointId()) {\n\t\t\t\thaveMoreRecentPending = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\n\t\tif (!haveMoreRecentPending) {\n\t\t\ttriggerQueuedRequests();\n\t\t}\n\t}"
        ],
        [
            "AbstractStreamOperator::snapshotState(long,long,CheckpointOptions,CheckpointStreamFactory)",
            " 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  ",
            "\t@Override\n\tpublic final OperatorSnapshotFutures snapshotState(long checkpointId, long timestamp, CheckpointOptions checkpointOptions,\n\t\t\tCheckpointStreamFactory factory) throws Exception {\n\n\t\tKeyGroupRange keyGroupRange = null != keyedStateBackend ?\n\t\t\t\tkeyedStateBackend.getKeyGroupRange() : KeyGroupRange.EMPTY_KEY_GROUP_RANGE;\n\n\t\tOperatorSnapshotFutures snapshotInProgress = new OperatorSnapshotFutures();\n\n\t\ttry (StateSnapshotContextSynchronousImpl snapshotContext = new StateSnapshotContextSynchronousImpl(\n\t\t\t\tcheckpointId,\n\t\t\t\ttimestamp,\n\t\t\t\tfactory,\n\t\t\t\tkeyGroupRange,\n\t\t\t\tgetContainingTask().getCancelables())) {\n\n\t\t\tsnapshotState(snapshotContext);\n\n\t\t\tsnapshotInProgress.setKeyedStateRawFuture(snapshotContext.getKeyedStateStreamFuture());\n\t\t\tsnapshotInProgress.setOperatorStateRawFuture(snapshotContext.getOperatorStateStreamFuture());\n\n\t\t\tif (null != operatorStateBackend) {\n\t\t\t\tsnapshotInProgress.setOperatorStateManagedFuture(\n\t\t\t\t\toperatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));\n\t\t\t}\n\n\t\t\tif (null != keyedStateBackend) {\n\t\t\t\tsnapshotInProgress.setKeyedStateManagedFuture(\n\t\t\t\t\tkeyedStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));\n\t\t\t}\n\t\t} catch (Exception snapshotException) {\n\t\t\ttry {\n\t\t\t\tsnapshotInProgress.cancel();\n\t\t\t} catch (Exception e) {\n\t\t\t\tsnapshotException.addSuppressed(e);\n\t\t\t}\n\n\t\t\tString snapshotFailMessage = \"Could not complete snapshot \" + checkpointId + \" for operator \" +\n\t\t\t\tgetOperatorName() + \".\";\n\n\t\t\tthrow new Exception(snapshotFailMessage, snapshotException);\n\t\t}\n\n\t\treturn snapshotInProgress;\n\t}",
            " 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419 +\n 420  \n 421  \n 422  \n 423  \n 424  ",
            "\t@Override\n\tpublic final OperatorSnapshotFutures snapshotState(long checkpointId, long timestamp, CheckpointOptions checkpointOptions,\n\t\t\tCheckpointStreamFactory factory) throws Exception {\n\n\t\tKeyGroupRange keyGroupRange = null != keyedStateBackend ?\n\t\t\t\tkeyedStateBackend.getKeyGroupRange() : KeyGroupRange.EMPTY_KEY_GROUP_RANGE;\n\n\t\tOperatorSnapshotFutures snapshotInProgress = new OperatorSnapshotFutures();\n\n\t\ttry (StateSnapshotContextSynchronousImpl snapshotContext = new StateSnapshotContextSynchronousImpl(\n\t\t\t\tcheckpointId,\n\t\t\t\ttimestamp,\n\t\t\t\tfactory,\n\t\t\t\tkeyGroupRange,\n\t\t\t\tgetContainingTask().getCancelables())) {\n\n\t\t\tsnapshotState(snapshotContext);\n\n\t\t\tsnapshotInProgress.setKeyedStateRawFuture(snapshotContext.getKeyedStateStreamFuture());\n\t\t\tsnapshotInProgress.setOperatorStateRawFuture(snapshotContext.getOperatorStateStreamFuture());\n\n\t\t\tif (null != operatorStateBackend) {\n\t\t\t\tsnapshotInProgress.setOperatorStateManagedFuture(\n\t\t\t\t\toperatorStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));\n\t\t\t}\n\n\t\t\tif (null != keyedStateBackend) {\n\t\t\t\tsnapshotInProgress.setKeyedStateManagedFuture(\n\t\t\t\t\tkeyedStateBackend.snapshot(checkpointId, timestamp, factory, checkpointOptions));\n\t\t\t}\n\t\t} catch (Exception snapshotException) {\n\t\t\ttry {\n\t\t\t\tsnapshotInProgress.cancel();\n\t\t\t} catch (Exception e) {\n\t\t\t\tsnapshotException.addSuppressed(e);\n\t\t\t}\n\n\t\t\tString snapshotFailMessage = \"Could not complete snapshot \" + checkpointId + \" for operator \" +\n\t\t\t\tgetOperatorName() + \".\";\n\n\t\t\tLOG.info(snapshotFailMessage, snapshotException);\n\t\t\tthrow new Exception(snapshotFailMessage, snapshotException);\n\t\t}\n\n\t\treturn snapshotInProgress;\n\t}"
        ]
    ],
    "96bb0d47b36f869e9b007fc3312e004a90779807": [
        [
            "SqlFunctionUtils::lpad(String,int,String)",
            " 217  \n 218  \n 219  \n 220  \n 221  \n 222 -\n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "\t/**\n\t * Returns the string str left-padded with the string pad to a length of len characters.\n\t * If str is longer than len, the return value is shortened to len characters.\n\t */\n\tpublic static String lpad(String base, int len, String pad) {\n\t\tif (len < 0) {\n\t\t\treturn null;\n\t\t} else if (len == 0) {\n\t\t\treturn \"\";\n\t\t}\n\n\t\tchar[] data = new char[len];\n\t\tchar[] baseChars = base.toCharArray();\n\t\tchar[] padChars = pad.toCharArray();\n\n\t\t// the length of the padding needed\n\t\tint pos = Math.max(len - base.length(), 0);\n\n\t\t// copy the padding\n\t\tfor (int i = 0; i < pos; i += pad.length()) {\n\t\t\tfor (int j = 0; j < pad.length() && j < pos - i; j++) {\n\t\t\t\tdata[i + j] = padChars[j];\n\t\t\t}\n\t\t}\n\n\t\t// copy the base\n\t\tint i = 0;\n\t\twhile (pos + i < len && i < base.length()) {\n\t\t\tdata[pos + i] = baseChars[i];\n\t\t\ti += 1;\n\t\t}\n\n\t\treturn new String(data);\n\t}",
            " 217  \n 218  \n 219  \n 220  \n 221  \n 222 +\n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "\t/**\n\t * Returns the string str left-padded with the string pad to a length of len characters.\n\t * If str is longer than len, the return value is shortened to len characters.\n\t */\n\tpublic static String lpad(String base, int len, String pad) {\n\t\tif (len < 0 || \"\".equals(pad)) {\n\t\t\treturn null;\n\t\t} else if (len == 0) {\n\t\t\treturn \"\";\n\t\t}\n\n\t\tchar[] data = new char[len];\n\t\tchar[] baseChars = base.toCharArray();\n\t\tchar[] padChars = pad.toCharArray();\n\n\t\t// the length of the padding needed\n\t\tint pos = Math.max(len - base.length(), 0);\n\n\t\t// copy the padding\n\t\tfor (int i = 0; i < pos; i += pad.length()) {\n\t\t\tfor (int j = 0; j < pad.length() && j < pos - i; j++) {\n\t\t\t\tdata[i + j] = padChars[j];\n\t\t\t}\n\t\t}\n\n\t\t// copy the base\n\t\tint i = 0;\n\t\twhile (pos + i < len && i < base.length()) {\n\t\t\tdata[pos + i] = baseChars[i];\n\t\t\ti += 1;\n\t\t}\n\n\t\treturn new String(data);\n\t}"
        ],
        [
            "SqlFunctionUtils::rpad(String,int,String)",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257 -\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "\t/**\n\t * Returns the string str right-padded with the string pad to a length of len characters.\n\t * If str is longer than len, the return value is shortened to len characters.\n\t */\n\tpublic static String rpad(String base, int len, String pad) {\n\t\tif (len < 0) {\n\t\t\treturn null;\n\t\t} else if (len == 0) {\n\t\t\treturn \"\";\n\t\t}\n\n\t\tchar[] data = new char[len];\n\t\tchar[] baseChars = base.toCharArray();\n\t\tchar[] padChars = pad.toCharArray();\n\n\t\tint pos = 0;\n\n\t\t// copy the base\n\t\twhile (pos < base.length() && pos < len) {\n\t\t\tdata[pos] = baseChars[pos];\n\t\t\tpos += 1;\n\t\t}\n\n\t\t// copy the padding\n\t\twhile (pos < len) {\n\t\t\tint i = 0;\n\t\t\twhile (i < pad.length() && i < len - pos) {\n\t\t\t\tdata[pos + i] = padChars[i];\n\t\t\t\ti += 1;\n\t\t\t}\n\t\t\tpos += pad.length();\n\t\t}\n\n\t\treturn new String(data);\n\t}",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "\t/**\n\t * Returns the string str right-padded with the string pad to a length of len characters.\n\t * If str is longer than len, the return value is shortened to len characters.\n\t */\n\tpublic static String rpad(String base, int len, String pad) {\n\t\tif (len < 0 || \"\".equals(pad)) {\n\t\t\treturn null;\n\t\t} else if (len == 0) {\n\t\t\treturn \"\";\n\t\t}\n\n\t\tchar[] data = new char[len];\n\t\tchar[] baseChars = base.toCharArray();\n\t\tchar[] padChars = pad.toCharArray();\n\n\t\tint pos = 0;\n\n\t\t// copy the base\n\t\twhile (pos < base.length() && pos < len) {\n\t\t\tdata[pos] = baseChars[pos];\n\t\t\tpos += 1;\n\t\t}\n\n\t\t// copy the padding\n\t\twhile (pos < len) {\n\t\t\tint i = 0;\n\t\t\twhile (i < pad.length() && i < len - pos) {\n\t\t\t\tdata[pos + i] = padChars[i];\n\t\t\t\ti += 1;\n\t\t\t}\n\t\t\tpos += pad.length();\n\t\t}\n\n\t\treturn new String(data);\n\t}"
        ]
    ],
    "c6ac93af99244a7d7dfd2f87032541e8420aa1ba": [
        [
            "ProctimeSqlFunction::ProctimeSqlFunction()",
            "  36  \n  37  \n  38 -\n  39 -\n  40 -\n  41 -\n  42 -\n  43 -\n  44  ",
            "\tpublic ProctimeSqlFunction() {\n\t\tsuper(\n\t\t\t\"PROCTIME\",\n\t\t\tSqlKind.OTHER_FUNCTION,\n\t\t\tReturnTypes.explicit(new ProctimeRelProtoDataType()),\n\t\t\tnull,\n\t\t\tOperandTypes.NILADIC,\n\t\t\tSqlFunctionCategory.TIMEDATE);\n\t}",
            "  36  \n  37  \n  38 +\n  39 +\n  40 +\n  41 +\n  42 +\n  43 +\n  44  ",
            "\tpublic ProctimeSqlFunction() {\n\t\tsuper(\n\t\t\t\t\"PROCTIME\",\n\t\t\t\tSqlKind.OTHER_FUNCTION,\n\t\t\t\tReturnTypes.explicit(new ProctimeRelProtoDataType()),\n\t\t\t\tnull,\n\t\t\t\tOperandTypes.NILADIC,\n\t\t\t\tSqlFunctionCategory.TIMEDATE);\n\t}"
        ]
    ],
    "cad94a0b36cdb014c071d5729212f880a7566c48": [
        [
            "PlannerContext::createFrameworkConfig(CalciteSchema,List)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  ",
            "\tprivate FrameworkConfig createFrameworkConfig(CalciteSchema rootSchema, List<RelTraitDef> traitDefs) {\n\t\treturn Frameworks.newConfigBuilder()\n\t\t\t\t.defaultSchema(rootSchema.plus())\n\t\t\t\t.parserConfig(getSqlParserConfig())\n\t\t\t\t.costFactory(new FlinkCostFactory())\n\t\t\t\t.typeSystem(typeSystem)\n\t\t\t\t.sqlToRelConverterConfig(getSqlToRelConverterConfig(getCalciteConfig(tableConfig)))\n\t\t\t\t.operatorTable(getSqlOperatorTable(getCalciteConfig(tableConfig), functionCatalog))\n\t\t\t\t// set the executor to evaluate constant expressions\n\t\t\t\t.executor(new ExpressionReducer(tableConfig, false))\n\t\t\t\t.context(new FlinkContextImpl(tableConfig))\n\t\t\t\t.traitDefs(traitDefs)\n\t\t\t\t.build();\n\t}",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  ",
            "\tprivate FrameworkConfig createFrameworkConfig(CalciteSchema rootSchema, List<RelTraitDef> traitDefs) {\n\t\treturn Frameworks.newConfigBuilder()\n\t\t\t\t.defaultSchema(rootSchema.plus())\n\t\t\t\t.parserConfig(getSqlParserConfig())\n\t\t\t\t.costFactory(new FlinkCostFactory())\n\t\t\t\t.typeSystem(typeSystem)\n\t\t\t\t.sqlToRelConverterConfig(getSqlToRelConverterConfig(getCalciteConfig(tableConfig)))\n\t\t\t\t.operatorTable(getSqlOperatorTable(getCalciteConfig(tableConfig), functionCatalog))\n\t\t\t\t// set the executor to evaluate constant expressions\n\t\t\t\t.executor(new ExpressionReducer(tableConfig, false))\n\t\t\t\t.context(new FlinkContextImpl(tableConfig, functionCatalog))\n\t\t\t\t.traitDefs(traitDefs)\n\t\t\t\t.build();\n\t}"
        ],
        [
            "RexNodeConverter::translateScalarCall(FunctionDefinition,List)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "\tprivate RexNode translateScalarCall(FunctionDefinition def, List<Expression> children) {\n\n\t\tif (def.equals(BuiltInFunctionDefinitions.CAST)) {\n\t\t\tRexNode child = children.get(0).accept(this);\n\t\t\tTypeLiteralExpression type = (TypeLiteralExpression) children.get(1);\n\t\t\treturn relBuilder.getRexBuilder().makeAbstractCast(\n\t\t\t\t\ttypeFactory.createFieldTypeFromLogicalType(\n\t\t\t\t\t\t\ttype.getOutputDataType().getLogicalType().copy(child.getType().isNullable())),\n\t\t\t\t\tchild);\n\t\t} else if (def.equals(BuiltInFunctionDefinitions.REINTERPRET_CAST)) {\n\t\t\tRexNode child = children.get(0).accept(this);\n\t\t\tTypeLiteralExpression type = (TypeLiteralExpression) children.get(1);\n\t\t\tRexNode checkOverflow = children.get(2).accept(this);\n\t\t\treturn relBuilder.getRexBuilder().makeReinterpretCast(\n\t\t\t\t\ttypeFactory.createFieldTypeFromLogicalType(\n\t\t\t\t\t\t\ttype.getOutputDataType().getLogicalType().copy(child.getType().isNullable())),\n\t\t\t\t\tchild,\n\t\t\t\t\tcheckOverflow);\n\t\t}\n\n\t\tList<RexNode> child = convertCallChildren(children);\n\t\tif (BuiltInFunctionDefinitions.IF.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.CASE, child);\n\t\t} else if (BuiltInFunctionDefinitions.IS_NULL.equals(def)) {\n\t\t\treturn relBuilder.isNull(child.get(0));\n\t\t} else if (BuiltInFunctionDefinitions.PLUS.equals(def)) {\n\t\t\tif (isCharacterString(toLogicalType(child.get(0).getType()))) {\n\t\t\t\treturn relBuilder.call(\n\t\t\t\t\t\tFlinkSqlOperatorTable.CONCAT,\n\t\t\t\t\t\tchild.get(0),\n\t\t\t\t\t\trelBuilder.cast(child.get(1), VARCHAR));\n\t\t\t} else if (isCharacterString(toLogicalType(child.get(1).getType()))) {\n\t\t\t\treturn relBuilder.call(\n\t\t\t\t\t\tFlinkSqlOperatorTable.CONCAT,\n\t\t\t\t\t\trelBuilder.cast(child.get(0), VARCHAR),\n\t\t\t\t\t\tchild.get(1));\n\t\t\t} else if (isTimeInterval(toLogicalType(child.get(0).getType())) &&\n\t\t\t\t\tchild.get(0).getType() == child.get(1).getType()) {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.PLUS, child);\n\t\t\t} else if (isTimeInterval(toLogicalType(child.get(0).getType()))\n\t\t\t\t\t&& isTemporal(toLogicalType(child.get(1).getType()))) {\n\t\t\t\t// Calcite has a bug that can't apply INTERVAL + DATETIME (INTERVAL at left)\n\t\t\t\t// we manually switch them here\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DATETIME_PLUS, child);\n\t\t\t} else if (isTemporal(toLogicalType(child.get(0).getType())) &&\n\t\t\t\t\tisTemporal(toLogicalType(child.get(1).getType()))) {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DATETIME_PLUS, child);\n\t\t\t} else {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.PLUS, child);\n\t\t\t}\n\t\t} else if (BuiltInFunctionDefinitions.MINUS.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MINUS, child);\n\t\t} else if (BuiltInFunctionDefinitions.EQUALS.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.EQUALS, child);\n\t\t} else if (BuiltInFunctionDefinitions.DIVIDE.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DIVIDE, child);\n\t\t} else if (BuiltInFunctionDefinitions.LESS_THAN.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.LESS_THAN, child);\n\t\t} else if (BuiltInFunctionDefinitions.GREATER_THAN.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.GREATER_THAN, child);\n\t\t} else if (BuiltInFunctionDefinitions.OR.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.OR, child);\n\t\t} else if (BuiltInFunctionDefinitions.CONCAT.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.CONCAT, child);\n\t\t} else if (InternalFunctionDefinitions.THROW_EXCEPTION.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.THROW_EXCEPTION, child);\n\t\t} else if (BuiltInFunctionDefinitions.AND.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.AND, child);\n\t\t} else if (BuiltInFunctionDefinitions.NOT.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.NOT, child);\n\t\t} else if (BuiltInFunctionDefinitions.TIMES.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MULTIPLY, child);\n\t\t} else if (BuiltInFunctionDefinitions.MOD.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MOD, child);\n\t\t} else {\n\t\t\tthrow new UnsupportedOperationException(def.toString());\n\t\t}\n\t}",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193  \n 194  \n 195  \n 196  ",
            "\tprivate RexNode translateScalarCall(FunctionDefinition def, List<Expression> children) {\n\n\t\tif (def.equals(BuiltInFunctionDefinitions.CAST)) {\n\t\t\tRexNode child = children.get(0).accept(this);\n\t\t\tTypeLiteralExpression type = (TypeLiteralExpression) children.get(1);\n\t\t\treturn relBuilder.getRexBuilder().makeAbstractCast(\n\t\t\t\t\ttypeFactory.createFieldTypeFromLogicalType(\n\t\t\t\t\t\t\ttype.getOutputDataType().getLogicalType().copy(child.getType().isNullable())),\n\t\t\t\t\tchild);\n\t\t} else if (def.equals(BuiltInFunctionDefinitions.REINTERPRET_CAST)) {\n\t\t\tRexNode child = children.get(0).accept(this);\n\t\t\tTypeLiteralExpression type = (TypeLiteralExpression) children.get(1);\n\t\t\tRexNode checkOverflow = children.get(2).accept(this);\n\t\t\treturn relBuilder.getRexBuilder().makeReinterpretCast(\n\t\t\t\t\ttypeFactory.createFieldTypeFromLogicalType(\n\t\t\t\t\t\t\ttype.getOutputDataType().getLogicalType().copy(child.getType().isNullable())),\n\t\t\t\t\tchild,\n\t\t\t\t\tcheckOverflow);\n\t\t}\n\n\t\tList<RexNode> child = convertCallChildren(children);\n\t\tif (BuiltInFunctionDefinitions.IF.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.CASE, child);\n\t\t} else if (BuiltInFunctionDefinitions.IS_NULL.equals(def)) {\n\t\t\treturn relBuilder.isNull(child.get(0));\n\t\t} else if (BuiltInFunctionDefinitions.PLUS.equals(def)) {\n\t\t\tif (isCharacterString(toLogicalType(child.get(0).getType()))) {\n\t\t\t\treturn relBuilder.call(\n\t\t\t\t\t\tFlinkSqlOperatorTable.CONCAT,\n\t\t\t\t\t\tchild.get(0),\n\t\t\t\t\t\trelBuilder.cast(child.get(1), VARCHAR));\n\t\t\t} else if (isCharacterString(toLogicalType(child.get(1).getType()))) {\n\t\t\t\treturn relBuilder.call(\n\t\t\t\t\t\tFlinkSqlOperatorTable.CONCAT,\n\t\t\t\t\t\trelBuilder.cast(child.get(0), VARCHAR),\n\t\t\t\t\t\tchild.get(1));\n\t\t\t} else if (isTimeInterval(toLogicalType(child.get(0).getType())) &&\n\t\t\t\t\tchild.get(0).getType() == child.get(1).getType()) {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.PLUS, child);\n\t\t\t} else if (isTimeInterval(toLogicalType(child.get(0).getType()))\n\t\t\t\t\t&& isTemporal(toLogicalType(child.get(1).getType()))) {\n\t\t\t\t// Calcite has a bug that can't apply INTERVAL + DATETIME (INTERVAL at left)\n\t\t\t\t// we manually switch them here\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DATETIME_PLUS, child);\n\t\t\t} else if (isTemporal(toLogicalType(child.get(0).getType())) &&\n\t\t\t\t\tisTemporal(toLogicalType(child.get(1).getType()))) {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DATETIME_PLUS, child);\n\t\t\t} else {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.PLUS, child);\n\t\t\t}\n\t\t} else if (BuiltInFunctionDefinitions.MINUS.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MINUS, child);\n\t\t} else if (BuiltInFunctionDefinitions.EQUALS.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.EQUALS, child);\n\t\t} else if (BuiltInFunctionDefinitions.DIVIDE.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DIVIDE, child);\n\t\t} else if (BuiltInFunctionDefinitions.LESS_THAN.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.LESS_THAN, child);\n\t\t} else if (BuiltInFunctionDefinitions.GREATER_THAN.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.GREATER_THAN, child);\n\t\t} else if (BuiltInFunctionDefinitions.OR.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.OR, child);\n\t\t} else if (BuiltInFunctionDefinitions.CONCAT.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.CONCAT, child);\n\t\t} else if (InternalFunctionDefinitions.THROW_EXCEPTION.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.THROW_EXCEPTION, child);\n\t\t} else if (BuiltInFunctionDefinitions.AND.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.AND, child);\n\t\t} else if (BuiltInFunctionDefinitions.NOT.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.NOT, child);\n\t\t} else if (BuiltInFunctionDefinitions.TIMES.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MULTIPLY, child);\n\t\t} else if (BuiltInFunctionDefinitions.MOD.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MOD, child);\n\t\t} else if (def instanceof ScalarFunctionDefinition) {\n\t\t\tScalarFunction scalarFunc = ((ScalarFunctionDefinition) def).getScalarFunction();\n\t\t\tSqlFunction sqlFunction = UserDefinedFunctionUtils.createScalarSqlFunction(\n\t\t\t\t// TODO use the name under which the function is registered\n\t\t\t\tscalarFunc.functionIdentifier(),\n\t\t\t\tscalarFunc.toString(),\n\t\t\t\tscalarFunc,\n\t\t\t\ttypeFactory);\n\t\t\treturn relBuilder.call(sqlFunction, child);\n\t\t} else {\n\t\t\tthrow new UnsupportedOperationException(def.toString());\n\t\t}\n\t}"
        ]
    ],
    "8b68ca769967f6c2035ceba6ebc25fe6b9988250": [
        [
            "HiveCatalog::retrieveFlinkProperties(Map)",
            " 581  \n 582  \n 583  \n 584  \n 585  \n 586 -\n 587  \n 588  ",
            "\t/**\n\t * Filter out Hive-created properties, and return Flink-created properties.\n\t */\n\tprivate static Map<String, String> retrieveFlinkProperties(Map<String, String> hiveTableParams) {\n\t\treturn hiveTableParams.entrySet().stream()\n\t\t\t.filter(e -> e.getKey().startsWith(FLINK_PROPERTY_PREFIX))\n\t\t\t.collect(Collectors.toMap(e -> e.getKey().replace(FLINK_PROPERTY_PREFIX, \"\"), e -> e.getValue()));\n\t}",
            " 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584 +\n 585  \n 586  ",
            "\t/**\n\t * Filter out Hive-created properties, and return Flink-created properties.\n\t * Note that 'is_generic' is a special key and this method will leave it as-is.\n\t */\n\tprivate static Map<String, String> retrieveFlinkProperties(Map<String, String> hiveTableParams) {\n\t\treturn hiveTableParams.entrySet().stream()\n\t\t\t.filter(e -> e.getKey().startsWith(FLINK_PROPERTY_PREFIX) || e.getKey().equals(CatalogConfig.IS_GENERIC))\n\t\t\t.collect(Collectors.toMap(e -> e.getKey().replace(FLINK_PROPERTY_PREFIX, \"\"), e -> e.getValue()));\n\t}"
        ],
        [
            "HiveCatalog::instantiateCatalogTable(Table)",
            " 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  ",
            "\tprivate static CatalogBaseTable instantiateCatalogTable(Table hiveTable) {\n\t\tboolean isView = TableType.valueOf(hiveTable.getTableType()) == TableType.VIRTUAL_VIEW;\n\n\t\t// Table properties\n\t\tMap<String, String> properties = hiveTable.getParameters();\n\n\t\tboolean isGeneric = Boolean.valueOf(properties.computeIfAbsent(FLINK_PROPERTY_IS_GENERIC, k -> String.valueOf(false)));\n\t\tif (isGeneric) {\n\t\t\tproperties = retrieveFlinkProperties(properties);\n\t\t}\n\t\tString comment = properties.remove(CatalogTableConfig.TABLE_COMMENT);\n\n\t\t// Table schema\n\t\tTableSchema tableSchema =\n\t\t\tHiveTableUtil.createTableSchema(hiveTable.getSd().getCols(), hiveTable.getPartitionKeys());\n\n\t\t// Partition keys\n\t\tList<String> partitionKeys = new ArrayList<>();\n\t\tif (!hiveTable.getPartitionKeys().isEmpty()) {\n\t\t\tpartitionKeys = getFieldNames(hiveTable.getPartitionKeys());\n\t\t}\n\n\t\tif (isView) {\n\t\t\treturn new CatalogViewImpl(\n\t\t\t\t\thiveTable.getViewOriginalText(),\n\t\t\t\t\thiveTable.getViewExpandedText(),\n\t\t\t\t\ttableSchema,\n\t\t\t\t\tproperties,\n\t\t\t\t\tcomment);\n\t\t} else {\n\t\t\treturn new CatalogTableImpl(tableSchema, partitionKeys, properties, comment);\n\t\t}\n\t}",
            " 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482 +\n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  ",
            "\tprivate static CatalogBaseTable instantiateCatalogTable(Table hiveTable) {\n\t\tboolean isView = TableType.valueOf(hiveTable.getTableType()) == TableType.VIRTUAL_VIEW;\n\n\t\t// Table properties\n\t\tMap<String, String> properties = hiveTable.getParameters();\n\n\t\tboolean isGeneric = Boolean.valueOf(properties.get(CatalogConfig.IS_GENERIC));\n\t\tif (isGeneric) {\n\t\t\tproperties = retrieveFlinkProperties(properties);\n\t\t}\n\t\tString comment = properties.remove(CatalogTableConfig.TABLE_COMMENT);\n\n\t\t// Table schema\n\t\tTableSchema tableSchema =\n\t\t\tHiveTableUtil.createTableSchema(hiveTable.getSd().getCols(), hiveTable.getPartitionKeys());\n\n\t\t// Partition keys\n\t\tList<String> partitionKeys = new ArrayList<>();\n\t\tif (!hiveTable.getPartitionKeys().isEmpty()) {\n\t\t\tpartitionKeys = getFieldNames(hiveTable.getPartitionKeys());\n\t\t}\n\n\t\tif (isView) {\n\t\t\treturn new CatalogViewImpl(\n\t\t\t\t\thiveTable.getViewOriginalText(),\n\t\t\t\t\thiveTable.getViewExpandedText(),\n\t\t\t\t\ttableSchema,\n\t\t\t\t\tproperties,\n\t\t\t\t\tcomment);\n\t\t} else {\n\t\t\treturn new CatalogTableImpl(tableSchema, partitionKeys, properties, comment);\n\t\t}\n\t}"
        ],
        [
            "HiveCatalog::maskFlinkProperties(Map)",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596 -\n 597  ",
            "\t/**\n\t * Add a prefix to Flink-created properties to distinguish them from Hive-created properties.\n\t */\n\tprivate static Map<String, String> maskFlinkProperties(Map<String, String> properties) {\n\t\treturn properties.entrySet().stream()\n\t\t\t.filter(e -> e.getKey() != null && e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(e -> FLINK_PROPERTY_PREFIX + e.getKey(), e -> e.getValue()));\n\t}",
            " 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595 +\n 596 +\n 597 +\n 598 +\n 599  ",
            "\t/**\n\t * Add a prefix to Flink-created properties to distinguish them from Hive-created properties.\n\t * Note that 'is_generic' is a special key and this method will leave it as-is.\n\t */\n\tprivate static Map<String, String> maskFlinkProperties(Map<String, String> properties) {\n\t\treturn properties.entrySet().stream()\n\t\t\t.filter(e -> e.getKey() != null && e.getValue() != null)\n\t\t\t.map(e -> new Tuple2<>(\n\t\t\t\te.getKey().equals(CatalogConfig.IS_GENERIC) ? e.getKey() : FLINK_PROPERTY_PREFIX + e.getKey(),\n\t\t\t\te.getValue()))\n\t\t\t.collect(Collectors.toMap(t -> t.f0, t -> t.f1));\n\t}"
        ],
        [
            "HiveCatalog::ensureTableAndPartitionMatch(Table,CatalogPartition)",
            " 771  \n 772 -\n 773  \n 774  \n 775  \n 776  \n 777  \n 778  ",
            "\tprivate static void ensureTableAndPartitionMatch(Table hiveTable, CatalogPartition catalogPartition) {\n\t\tboolean isGeneric = Boolean.valueOf(hiveTable.getParameters().get(FLINK_PROPERTY_IS_GENERIC));\n\t\tif ((isGeneric && catalogPartition instanceof HiveCatalogPartition) ||\n\t\t\t(!isGeneric && catalogPartition instanceof GenericCatalogPartition)) {\n\t\t\tthrow new CatalogException(String.format(\"Cannot handle %s partition for %s table\",\n\t\t\t\tcatalogPartition.getClass().getName(), isGeneric ? \"generic\" : \"non-generic\"));\n\t\t}\n\t}",
            " 773  \n 774 +\n 775  \n 776  \n 777  \n 778  \n 779  \n 780  ",
            "\tprivate static void ensureTableAndPartitionMatch(Table hiveTable, CatalogPartition catalogPartition) {\n\t\tboolean isGeneric = Boolean.valueOf(hiveTable.getParameters().get(CatalogConfig.IS_GENERIC));\n\t\tif ((isGeneric && catalogPartition instanceof HiveCatalogPartition) ||\n\t\t\t(!isGeneric && catalogPartition instanceof GenericCatalogPartition)) {\n\t\t\tthrow new CatalogException(String.format(\"Cannot handle %s partition for %s table\",\n\t\t\t\tcatalogPartition.getClass().getName(), isGeneric ? \"generic\" : \"non-generic\"));\n\t\t}\n\t}"
        ],
        [
            "CatalogTestUtil::checkEquals(CatalogView,CatalogView)",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\tpublic static void checkEquals(CatalogView v1, CatalogView v2) {\n\t\tassertEquals(v1.getClass(), v2.getClass());\n\t\tassertEquals(v1.getSchema(), v1.getSchema());\n\t\tassertEquals(v1.getComment(), v2.getComment());\n\t\tassertEquals(v1.getOriginalQuery(), v2.getOriginalQuery());\n\t\tassertEquals(v1.getExpandedQuery(), v2.getExpandedQuery());\n\n\t\t// Hive tables may have properties created by itself\n\t\t// thus properties of Hive table is a super set of those in its corresponding Flink table\n\t\tif (Boolean.valueOf(v1.getProperties().get(CatalogConfig.IS_GENERIC))) {\n\t\t\tassertEquals(v1.getProperties(), v2.getProperties());\n\t\t} else {\n\t\t\tassertTrue(v2.getProperties().entrySet().containsAll(v1.getProperties().entrySet()));\n\t\t}\n\t}",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78 +\n  79  \n  80  \n  81  ",
            "\tpublic static void checkEquals(CatalogView v1, CatalogView v2) {\n\t\tassertEquals(v1.getClass(), v2.getClass());\n\t\tassertEquals(v1.getSchema(), v1.getSchema());\n\t\tassertEquals(v1.getComment(), v2.getComment());\n\t\tassertEquals(v1.getOriginalQuery(), v2.getOriginalQuery());\n\t\tassertEquals(v1.getExpandedQuery(), v2.getExpandedQuery());\n\n\t\t// Hive tables may have properties created by itself\n\t\t// thus properties of Hive table is a super set of those in its corresponding Flink table\n\t\tif (Boolean.valueOf(v1.getProperties().get(CatalogConfig.IS_GENERIC))) {\n\t\t\tassertEquals(v1.getProperties(), v2.getProperties());\n\t\t} else {\n\t\t\tassertTrue(v2.getProperties().keySet().stream().noneMatch(k -> k.startsWith(FLINK_PROPERTY_PREFIX)));\n\t\t\tassertTrue(v2.getProperties().entrySet().containsAll(v1.getProperties().entrySet()));\n\t\t}\n\t}"
        ],
        [
            "CatalogTestUtil::checkEquals(CatalogTable,CatalogTable)",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  ",
            "\tpublic static void checkEquals(CatalogTable t1, CatalogTable t2) {\n\t\tassertEquals(t1.getClass(), t2.getClass());\n\t\tassertEquals(t1.getSchema(), t2.getSchema());\n\t\tassertEquals(t1.getComment(), t2.getComment());\n\t\tassertEquals(t1.getPartitionKeys(), t2.getPartitionKeys());\n\t\tassertEquals(t1.isPartitioned(), t2.isPartitioned());\n\n\t\tassertEquals(\n\t\t\tt1.getProperties().get(CatalogConfig.IS_GENERIC),\n\t\t\tt2.getProperties().get(CatalogConfig.IS_GENERIC));\n\n\t\t// Hive tables may have properties created by itself\n\t\t// thus properties of Hive table is a super set of those in its corresponding Flink table\n\t\tif (Boolean.valueOf(t1.getProperties().get(CatalogConfig.IS_GENERIC))) {\n\t\t\tassertEquals(t1.getProperties(), t2.getProperties());\n\t\t} else {\n\t\t\tassertTrue(t2.getProperties().entrySet().containsAll(t1.getProperties().entrySet()));\n\t\t}\n\t}",
            "  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62  \n  63  \n  64  ",
            "\tpublic static void checkEquals(CatalogTable t1, CatalogTable t2) {\n\t\tassertEquals(t1.getClass(), t2.getClass());\n\t\tassertEquals(t1.getSchema(), t2.getSchema());\n\t\tassertEquals(t1.getComment(), t2.getComment());\n\t\tassertEquals(t1.getPartitionKeys(), t2.getPartitionKeys());\n\t\tassertEquals(t1.isPartitioned(), t2.isPartitioned());\n\n\t\tassertEquals(\n\t\t\tt1.getProperties().get(CatalogConfig.IS_GENERIC),\n\t\t\tt2.getProperties().get(CatalogConfig.IS_GENERIC));\n\n\t\t// Hive tables may have properties created by itself\n\t\t// thus properties of Hive table is a super set of those in its corresponding Flink table\n\t\tif (Boolean.valueOf(t1.getProperties().get(CatalogConfig.IS_GENERIC))) {\n\t\t\tassertEquals(t1.getProperties(), t2.getProperties());\n\t\t} else {\n\t\t\tassertTrue(t2.getProperties().keySet().stream().noneMatch(k -> k.startsWith(FLINK_PROPERTY_PREFIX)));\n\t\t\tassertTrue(t2.getProperties().entrySet().containsAll(t1.getProperties().entrySet()));\n\t\t}\n\t}"
        ]
    ],
    "687cc91975464734ac43f4c2eb062f1f312b589e": [
        [
            "NettyShuffleEnvironment::create(NettyShuffleEnvironmentConfiguration,ResourceID,TaskEventPublisher,MetricGroup,IOManager)",
            " 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133 -\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\tpublic static NettyShuffleEnvironment create(\n\t\t\tNettyShuffleEnvironmentConfiguration config,\n\t\t\tResourceID taskExecutorLocation,\n\t\t\tTaskEventPublisher taskEventPublisher,\n\t\t\tMetricGroup metricGroup,\n\t\t\tIOManager ioManager) {\n\t\tcheckNotNull(taskExecutorLocation);\n\t\tcheckNotNull(ioManager);\n\t\tcheckNotNull(taskEventPublisher);\n\t\tcheckNotNull(config);\n\n\t\tNettyConfig nettyConfig = config.nettyConfig();\n\n\t\tResultPartitionManager resultPartitionManager = new ResultPartitionManager();\n\n\t\tConnectionManager connectionManager = nettyConfig != null ?\n\t\t\tnew NettyConnectionManager(resultPartitionManager, taskEventPublisher, nettyConfig, config.isCreditBased()) :\n\t\t\tnew LocalConnectionManager();\n\n\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(\n\t\t\tconfig.numNetworkBuffers(),\n\t\t\tconfig.networkBufferSize(),\n\t\t\tconfig.networkBuffersPerChannel());\n\n\t\tregisterNetworkMetrics(metricGroup, networkBufferPool);\n\n\t\tResultPartitionFactory resultPartitionFactory = new ResultPartitionFactory(\n\t\t\tresultPartitionManager,\n\t\t\tioManager,\n\t\t\tnetworkBufferPool,\n\t\t\tconfig.networkBuffersPerChannel(),\n\t\t\tconfig.floatingNetworkBuffersPerGate(),\n\t\t\tconfig.isForcePartitionReleaseOnConsumption());\n\n\t\tSingleInputGateFactory singleInputGateFactory = new SingleInputGateFactory(\n\t\t\ttaskExecutorLocation,\n\t\t\tconfig,\n\t\t\tconnectionManager,\n\t\t\tresultPartitionManager,\n\t\t\ttaskEventPublisher,\n\t\t\tnetworkBufferPool);\n\n\t\treturn new NettyShuffleEnvironment(\n\t\t\ttaskExecutorLocation,\n\t\t\tconfig,\n\t\t\tnetworkBufferPool,\n\t\t\tconnectionManager,\n\t\t\tresultPartitionManager,\n\t\t\tresultPartitionFactory,\n\t\t\tsingleInputGateFactory);\n\t}",
            " 127  \n 128  \n 129 +\n 130  \n 131  \n 132  \n 133 +\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 +\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\tpublic static NettyShuffleEnvironment create(\n\t\t\tNettyShuffleEnvironmentConfiguration config,\n\t\t\tResourceID taskExecutorResourceId,\n\t\t\tTaskEventPublisher taskEventPublisher,\n\t\t\tMetricGroup metricGroup,\n\t\t\tIOManager ioManager) {\n\t\tcheckNotNull(taskExecutorResourceId);\n\t\tcheckNotNull(ioManager);\n\t\tcheckNotNull(taskEventPublisher);\n\t\tcheckNotNull(config);\n\n\t\tNettyConfig nettyConfig = config.nettyConfig();\n\n\t\tResultPartitionManager resultPartitionManager = new ResultPartitionManager();\n\n\t\tConnectionManager connectionManager = nettyConfig != null ?\n\t\t\tnew NettyConnectionManager(resultPartitionManager, taskEventPublisher, nettyConfig, config.isCreditBased()) :\n\t\t\tnew LocalConnectionManager();\n\n\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(\n\t\t\tconfig.numNetworkBuffers(),\n\t\t\tconfig.networkBufferSize(),\n\t\t\tconfig.networkBuffersPerChannel());\n\n\t\tregisterNetworkMetrics(metricGroup, networkBufferPool);\n\n\t\tResultPartitionFactory resultPartitionFactory = new ResultPartitionFactory(\n\t\t\tresultPartitionManager,\n\t\t\tioManager,\n\t\t\tnetworkBufferPool,\n\t\t\tconfig.networkBuffersPerChannel(),\n\t\t\tconfig.floatingNetworkBuffersPerGate(),\n\t\t\tconfig.isForcePartitionReleaseOnConsumption());\n\n\t\tSingleInputGateFactory singleInputGateFactory = new SingleInputGateFactory(\n\t\t\ttaskExecutorResourceId,\n\t\t\tconfig,\n\t\t\tconnectionManager,\n\t\t\tresultPartitionManager,\n\t\t\ttaskEventPublisher,\n\t\t\tnetworkBufferPool);\n\n\t\treturn new NettyShuffleEnvironment(\n\t\t\ttaskExecutorResourceId,\n\t\t\tconfig,\n\t\t\tnetworkBufferPool,\n\t\t\tconnectionManager,\n\t\t\tresultPartitionManager,\n\t\t\tresultPartitionFactory,\n\t\t\tsingleInputGateFactory);\n\t}"
        ],
        [
            "NettyShuffleEnvironment::NettyShuffleEnvironment(ResourceID,NettyShuffleEnvironmentConfiguration,NetworkBufferPool,ConnectionManager,ResultPartitionManager,ResultPartitionFactory,SingleInputGateFactory)",
            " 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 -\n 117  \n 118  \n 119  \n 120  \n 121 -\n 122  \n 123  \n 124  \n 125  ",
            "\tprivate NettyShuffleEnvironment(\n\t\t\tResourceID taskExecutorLocation,\n\t\t\tNettyShuffleEnvironmentConfiguration config,\n\t\t\tNetworkBufferPool networkBufferPool,\n\t\t\tConnectionManager connectionManager,\n\t\t\tResultPartitionManager resultPartitionManager,\n\t\t\tResultPartitionFactory resultPartitionFactory,\n\t\t\tSingleInputGateFactory singleInputGateFactory) {\n\t\tthis.taskExecutorLocation = taskExecutorLocation;\n\t\tthis.config = config;\n\t\tthis.networkBufferPool = networkBufferPool;\n\t\tthis.connectionManager = connectionManager;\n\t\tthis.resultPartitionManager = resultPartitionManager;\n\t\tthis.inputGatesById = new ConcurrentHashMap<>();\n\t\tthis.resultPartitionFactory = resultPartitionFactory;\n\t\tthis.singleInputGateFactory = singleInputGateFactory;\n\t\tthis.isClosed = false;\n\t}",
            " 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117  \n 118  \n 119  \n 120  \n 121 +\n 122  \n 123  \n 124  \n 125  ",
            "\tprivate NettyShuffleEnvironment(\n\t\t\tResourceID taskExecutorResourceId,\n\t\t\tNettyShuffleEnvironmentConfiguration config,\n\t\t\tNetworkBufferPool networkBufferPool,\n\t\t\tConnectionManager connectionManager,\n\t\t\tResultPartitionManager resultPartitionManager,\n\t\t\tResultPartitionFactory resultPartitionFactory,\n\t\t\tSingleInputGateFactory singleInputGateFactory) {\n\t\tthis.taskExecutorResourceId = taskExecutorResourceId;\n\t\tthis.config = config;\n\t\tthis.networkBufferPool = networkBufferPool;\n\t\tthis.connectionManager = connectionManager;\n\t\tthis.resultPartitionManager = resultPartitionManager;\n\t\tthis.inputGatesById = new ConcurrentHashMap<>(10);\n\t\tthis.resultPartitionFactory = resultPartitionFactory;\n\t\tthis.singleInputGateFactory = singleInputGateFactory;\n\t\tthis.isClosed = false;\n\t}"
        ],
        [
            "NettyShuffleEnvironment::updatePartitionInfo(ExecutionAttemptID,PartitionInfo)",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 -\n 324  \n 325  ",
            "\t@Override\n\tpublic boolean updatePartitionInfo(\n\t\t\tExecutionAttemptID consumerID,\n\t\t\tPartitionInfo partitionInfo) throws IOException, InterruptedException {\n\t\tIntermediateDataSetID intermediateResultPartitionID = partitionInfo.getIntermediateDataSetID();\n\t\tInputGateID id = new InputGateID(intermediateResultPartitionID, consumerID);\n\t\tSingleInputGate inputGate = inputGatesById.get(id);\n\t\tif (inputGate == null) {\n\t\t\treturn false;\n\t\t}\n\t\tShuffleDescriptor shuffleDescriptor = partitionInfo.getShuffleDescriptor();\n\t\tcheckArgument(shuffleDescriptor instanceof NettyShuffleDescriptor,\n\t\t\t\"Tried to update unknown channel with unknown ShuffleDescriptor %s.\",\n\t\t\tshuffleDescriptor.getClass().getName());\n\t\tinputGate.updateInputChannel(taskExecutorLocation, (NettyShuffleDescriptor) shuffleDescriptor);\n\t\treturn true;\n\t}",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 +\n 324  \n 325  ",
            "\t@Override\n\tpublic boolean updatePartitionInfo(\n\t\t\tExecutionAttemptID consumerID,\n\t\t\tPartitionInfo partitionInfo) throws IOException, InterruptedException {\n\t\tIntermediateDataSetID intermediateResultPartitionID = partitionInfo.getIntermediateDataSetID();\n\t\tInputGateID id = new InputGateID(intermediateResultPartitionID, consumerID);\n\t\tSingleInputGate inputGate = inputGatesById.get(id);\n\t\tif (inputGate == null) {\n\t\t\treturn false;\n\t\t}\n\t\tShuffleDescriptor shuffleDescriptor = partitionInfo.getShuffleDescriptor();\n\t\tcheckArgument(shuffleDescriptor instanceof NettyShuffleDescriptor,\n\t\t\t\"Tried to update unknown channel with unknown ShuffleDescriptor %s.\",\n\t\t\tshuffleDescriptor.getClass().getName());\n\t\tinputGate.updateInputChannel(taskExecutorResourceId, (NettyShuffleDescriptor) shuffleDescriptor);\n\t\treturn true;\n\t}"
        ],
        [
            "SingleInputGateFactory::createKnownInputChannel(SingleInputGate,int,NettyShuffleDescriptor,ChannelStatistics,InputChannelMetrics)",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 -\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "\tprivate InputChannel createKnownInputChannel(\n\t\t\tSingleInputGate inputGate,\n\t\t\tint index,\n\t\t\tNettyShuffleDescriptor inputChannelDescriptor,\n\t\t\tChannelStatistics channelStatistics,\n\t\t\tInputChannelMetrics metrics) {\n\t\tResultPartitionID partitionId = inputChannelDescriptor.getResultPartitionID();\n\t\tif (inputChannelDescriptor.isLocalTo(taskExecutorLocation)) {\n\t\t\t// Consuming task is deployed to the same TaskManager as the partition => local\n\t\t\tchannelStatistics.numLocalChannels++;\n\t\t\treturn new LocalInputChannel(\n\t\t\t\tinputGate,\n\t\t\t\tindex,\n\t\t\t\tpartitionId,\n\t\t\t\tpartitionManager,\n\t\t\t\ttaskEventPublisher,\n\t\t\t\tpartitionRequestInitialBackoff,\n\t\t\t\tpartitionRequestMaxBackoff,\n\t\t\t\tmetrics);\n\t\t} else {\n\t\t\t// Different instances => remote\n\t\t\tchannelStatistics.numRemoteChannels++;\n\t\t\treturn new RemoteInputChannel(\n\t\t\t\tinputGate,\n\t\t\t\tindex,\n\t\t\t\tpartitionId,\n\t\t\t\tinputChannelDescriptor.getConnectionId(),\n\t\t\t\tconnectionManager,\n\t\t\t\tpartitionRequestInitialBackoff,\n\t\t\t\tpartitionRequestMaxBackoff,\n\t\t\t\tmetrics,\n\t\t\t\tnetworkBufferPool);\n\t\t}\n\t}",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "\tprivate InputChannel createKnownInputChannel(\n\t\t\tSingleInputGate inputGate,\n\t\t\tint index,\n\t\t\tNettyShuffleDescriptor inputChannelDescriptor,\n\t\t\tChannelStatistics channelStatistics,\n\t\t\tInputChannelMetrics metrics) {\n\t\tResultPartitionID partitionId = inputChannelDescriptor.getResultPartitionID();\n\t\tif (inputChannelDescriptor.isLocalTo(taskExecutorResourceId)) {\n\t\t\t// Consuming task is deployed to the same TaskManager as the partition => local\n\t\t\tchannelStatistics.numLocalChannels++;\n\t\t\treturn new LocalInputChannel(\n\t\t\t\tinputGate,\n\t\t\t\tindex,\n\t\t\t\tpartitionId,\n\t\t\t\tpartitionManager,\n\t\t\t\ttaskEventPublisher,\n\t\t\t\tpartitionRequestInitialBackoff,\n\t\t\t\tpartitionRequestMaxBackoff,\n\t\t\t\tmetrics);\n\t\t} else {\n\t\t\t// Different instances => remote\n\t\t\tchannelStatistics.numRemoteChannels++;\n\t\t\treturn new RemoteInputChannel(\n\t\t\t\tinputGate,\n\t\t\t\tindex,\n\t\t\t\tpartitionId,\n\t\t\t\tinputChannelDescriptor.getConnectionId(),\n\t\t\t\tconnectionManager,\n\t\t\t\tpartitionRequestInitialBackoff,\n\t\t\t\tpartitionRequestMaxBackoff,\n\t\t\t\tmetrics,\n\t\t\t\tnetworkBufferPool);\n\t\t}\n\t}"
        ],
        [
            "NettyShuffleEnvironment::close()",
            " 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  ",
            "\t/**\n\t * Tries to shut down all network I/O components.\n\t */\n\t@Override\n\tpublic void close() {\n\t\tsynchronized (lock) {\n\t\t\tif (isClosed) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tLOG.info(\"Shutting down the network environment and its components.\");\n\n\t\t\t// terminate all network connections\n\t\t\ttry {\n\t\t\t\tLOG.debug(\"Shutting down network connection manager\");\n\t\t\t\tconnectionManager.shutdown();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.warn(\"Cannot shut down the network connection manager.\", t);\n\t\t\t}\n\n\t\t\t// shutdown all intermediate results\n\t\t\ttry {\n\t\t\t\tLOG.debug(\"Shutting down intermediate result partition manager\");\n\t\t\t\tresultPartitionManager.shutdown();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.warn(\"Cannot shut down the result partition manager.\", t);\n\t\t\t}\n\n\t\t\t// make sure that the global buffer pool re-acquires all buffers\n\t\t\tnetworkBufferPool.destroyAllBufferPools();\n\n\t\t\t// destroy the buffer pool\n\t\t\ttry {\n\t\t\t\tnetworkBufferPool.destroy();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.warn(\"Network buffer pool did not shut down properly.\", t);\n\t\t\t}\n\n\t\t\tisClosed = true;\n\t\t}\n\t}",
            " 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359  \n 360  \n 361 +\n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  ",
            "\t/**\n\t * Tries to shut down all network I/O components.\n\t */\n\t@Override\n\tpublic void close() {\n\t\tsynchronized (lock) {\n\t\t\tif (isClosed) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\tLOG.info(\"Shutting down the network environment and its components.\");\n\n\t\t\t// terminate all network connections\n\t\t\t//noinspection OverlyBroadCatchBlock\n\t\t\ttry {\n\t\t\t\tLOG.debug(\"Shutting down network connection manager\");\n\t\t\t\tconnectionManager.shutdown();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.warn(\"Cannot shut down the network connection manager.\", t);\n\t\t\t}\n\n\t\t\t// shutdown all intermediate results\n\t\t\ttry {\n\t\t\t\tLOG.debug(\"Shutting down intermediate result partition manager\");\n\t\t\t\tresultPartitionManager.shutdown();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.warn(\"Cannot shut down the result partition manager.\", t);\n\t\t\t}\n\n\t\t\t// make sure that the global buffer pool re-acquires all buffers\n\t\t\tnetworkBufferPool.destroyAllBufferPools();\n\n\t\t\t// destroy the buffer pool\n\t\t\ttry {\n\t\t\t\tnetworkBufferPool.destroy();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\tLOG.warn(\"Network buffer pool did not shut down properly.\", t);\n\t\t\t}\n\n\t\t\tisClosed = true;\n\t\t}\n\t}"
        ],
        [
            "SingleInputGateFactory::SingleInputGateFactory(ResourceID,NettyShuffleEnvironmentConfiguration,ConnectionManager,ResultPartitionManager,TaskEventPublisher,NetworkBufferPool)",
            "  80  \n  81 -\n  82  \n  83  \n  84  \n  85  \n  86  \n  87 -\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "\tpublic SingleInputGateFactory(\n\t\t\t@Nonnull ResourceID taskExecutorLocation,\n\t\t\t@Nonnull NettyShuffleEnvironmentConfiguration networkConfig,\n\t\t\t@Nonnull ConnectionManager connectionManager,\n\t\t\t@Nonnull ResultPartitionManager partitionManager,\n\t\t\t@Nonnull TaskEventPublisher taskEventPublisher,\n\t\t\t@Nonnull NetworkBufferPool networkBufferPool) {\n\t\tthis.taskExecutorLocation = taskExecutorLocation;\n\t\tthis.isCreditBased = networkConfig.isCreditBased();\n\t\tthis.partitionRequestInitialBackoff = networkConfig.partitionRequestInitialBackoff();\n\t\tthis.partitionRequestMaxBackoff = networkConfig.partitionRequestMaxBackoff();\n\t\tthis.networkBuffersPerChannel = networkConfig.networkBuffersPerChannel();\n\t\tthis.floatingNetworkBuffersPerGate = networkConfig.floatingNetworkBuffersPerGate();\n\t\tthis.connectionManager = connectionManager;\n\t\tthis.partitionManager = partitionManager;\n\t\tthis.taskEventPublisher = taskEventPublisher;\n\t\tthis.networkBufferPool = networkBufferPool;\n\t}",
            "  80  \n  81 +\n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "\tpublic SingleInputGateFactory(\n\t\t\t@Nonnull ResourceID taskExecutorResourceId,\n\t\t\t@Nonnull NettyShuffleEnvironmentConfiguration networkConfig,\n\t\t\t@Nonnull ConnectionManager connectionManager,\n\t\t\t@Nonnull ResultPartitionManager partitionManager,\n\t\t\t@Nonnull TaskEventPublisher taskEventPublisher,\n\t\t\t@Nonnull NetworkBufferPool networkBufferPool) {\n\t\tthis.taskExecutorResourceId = taskExecutorResourceId;\n\t\tthis.isCreditBased = networkConfig.isCreditBased();\n\t\tthis.partitionRequestInitialBackoff = networkConfig.partitionRequestInitialBackoff();\n\t\tthis.partitionRequestMaxBackoff = networkConfig.partitionRequestMaxBackoff();\n\t\tthis.networkBuffersPerChannel = networkConfig.networkBuffersPerChannel();\n\t\tthis.floatingNetworkBuffersPerGate = networkConfig.floatingNetworkBuffersPerGate();\n\t\tthis.connectionManager = connectionManager;\n\t\tthis.partitionManager = partitionManager;\n\t\tthis.taskEventPublisher = taskEventPublisher;\n\t\tthis.networkBufferPool = networkBufferPool;\n\t}"
        ]
    ],
    "305051cc6e03d7e0ed9a9c3afe9e90b56bab0dda": [
        [
            "StreamSQLTestProgram::main(String)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110 -\n 111 -\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tParameterTool params = ParameterTool.fromArgs(args);\n\t\tString outputPath = params.getRequired(\"outputPath\");\n\n\t\tStreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsEnv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n\t\t\t3,\n\t\t\tTime.of(10, TimeUnit.SECONDS)\n\t\t));\n\t\tsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\tsEnv.enableCheckpointing(4000);\n\t\tsEnv.getConfig().setAutoWatermarkInterval(1000);\n\n\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(sEnv);\n\n\t\ttEnv.registerTableSource(\"table1\", new GeneratorTableSource(10, 100, 60, 0));\n\t\ttEnv.registerTableSource(\"table2\", new GeneratorTableSource(5, 0.2f, 60, 5));\n\n\t\tint overWindowSizeSeconds = 1;\n\t\tint tumbleWindowSizeSeconds = 10;\n\n\t\tString overQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  key, \" +\n\t\t\t\"  rowtime, \" +\n\t\t\t\"  COUNT(*) OVER (PARTITION BY key ORDER BY rowtime RANGE BETWEEN INTERVAL '%d' SECOND PRECEDING AND CURRENT ROW) AS cnt \" +\n\t\t\t\"FROM table1\",\n\t\t\toverWindowSizeSeconds);\n\n\t\tString tumbleQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  key, \" +\n\t\t\t//TODO: The \"WHEN -1 THEN NULL\" part is a temporary workaround, to make the test pass, for\n\t\t\t// https://issues.apache.org/jira/browse/FLINK-12249. We should remove it once the issue is fixed.\n\t\t\t\"  CASE SUM(cnt) / COUNT(*) WHEN 101 THEN 1 WHEN -1 THEN NULL ELSE 99 END AS correct, \" +\n\t\t\t\"  TUMBLE_START(rowtime, INTERVAL '%d' SECOND) AS wStart, \" +\n\t\t\t\"  TUMBLE_ROWTIME(rowtime, INTERVAL '%d' SECOND) AS rowtime \" +\n\t\t\t\"FROM (%s) \" +\n\t\t\t\"WHERE rowtime > TIMESTAMP '1970-01-01 00:00:01' \" +\n\t\t\t\"GROUP BY key, TUMBLE(rowtime, INTERVAL '%d' SECOND)\",\n\t\t\ttumbleWindowSizeSeconds,\n\t\t\ttumbleWindowSizeSeconds,\n\t\t\toverQuery,\n\t\t\ttumbleWindowSizeSeconds);\n\n\t\tString joinQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  t1.key, \" +\n\t\t\t\"  t2.rowtime AS rowtime, \" +\n\t\t\t\"  t2.correct,\" +\n\t\t\t\"  t2.wStart \" +\n\t\t\t\"FROM table2 t1, (%s) t2 \" +\n\t\t\t\"WHERE \" +\n\t\t\t\"  t1.key = t2.key AND \" +\n\t\t\t\"  t1.rowtime BETWEEN t2.rowtime AND t2.rowtime + INTERVAL '%d' SECOND\",\n\t\t\ttumbleQuery,\n\t\t\ttumbleWindowSizeSeconds);\n\n\t\tString finalAgg = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  SUM(correct) AS correct, \" +\n\t\t\t\"  TUMBLE_START(rowtime, INTERVAL '20' SECOND) AS rowtime \" +\n\t\t\t\"FROM (%s) \" +\n\t\t\t\"GROUP BY TUMBLE(rowtime, INTERVAL '20' SECOND)\",\n\t\t\tjoinQuery);\n\n\t\t// get Table for SQL query\n\t\tTable result = tEnv.sqlQuery(finalAgg);\n\t\t// convert Table into append-only DataStream\n\t\tDataStream<Row> resultStream =\n\t\t\ttEnv.toAppendStream(result, Types.ROW(Types.INT, Types.SQL_TIMESTAMP));\n\n\t\tfinal StreamingFileSink<Row> sink = StreamingFileSink\n\t\t\t.forRowFormat(new Path(outputPath), (Encoder<Row>) (element, stream) -> {\n\t\t\t\tPrintStream out = new PrintStream(stream);\n\t\t\t\tout.println(element.toString());\n\t\t\t})\n\t\t\t.withBucketAssigner(new KeyBucketAssigner())\n\t\t\t.withRollingPolicy(OnCheckpointRollingPolicy.build())\n\t\t\t.build();\n\n\t\tresultStream\n\t\t\t// inject a KillMapper that forwards all records but terminates the first execution attempt\n\t\t\t.map(new KillMapper()).setParallelism(1)\n\t\t\t// add sink function\n\t\t\t.addSink(sink).setParallelism(1);\n\n\t\tsEnv.execute();\n\t}",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tParameterTool params = ParameterTool.fromArgs(args);\n\t\tString outputPath = params.getRequired(\"outputPath\");\n\n\t\tStreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsEnv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n\t\t\t3,\n\t\t\tTime.of(10, TimeUnit.SECONDS)\n\t\t));\n\t\tsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\tsEnv.enableCheckpointing(4000);\n\t\tsEnv.getConfig().setAutoWatermarkInterval(1000);\n\n\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(sEnv);\n\n\t\ttEnv.registerTableSource(\"table1\", new GeneratorTableSource(10, 100, 60, 0));\n\t\ttEnv.registerTableSource(\"table2\", new GeneratorTableSource(5, 0.2f, 60, 5));\n\n\t\tint overWindowSizeSeconds = 1;\n\t\tint tumbleWindowSizeSeconds = 10;\n\n\t\tString overQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  key, \" +\n\t\t\t\"  rowtime, \" +\n\t\t\t\"  COUNT(*) OVER (PARTITION BY key ORDER BY rowtime RANGE BETWEEN INTERVAL '%d' SECOND PRECEDING AND CURRENT ROW) AS cnt \" +\n\t\t\t\"FROM table1\",\n\t\t\toverWindowSizeSeconds);\n\n\t\tString tumbleQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  key, \" +\n\t\t\t\"  CASE SUM(cnt) / COUNT(*) WHEN 101 THEN 1 ELSE 99 END AS correct, \" +\n\t\t\t\"  TUMBLE_START(rowtime, INTERVAL '%d' SECOND) AS wStart, \" +\n\t\t\t\"  TUMBLE_ROWTIME(rowtime, INTERVAL '%d' SECOND) AS rowtime \" +\n\t\t\t\"FROM (%s) \" +\n\t\t\t\"WHERE rowtime > TIMESTAMP '1970-01-01 00:00:01' \" +\n\t\t\t\"GROUP BY key, TUMBLE(rowtime, INTERVAL '%d' SECOND)\",\n\t\t\ttumbleWindowSizeSeconds,\n\t\t\ttumbleWindowSizeSeconds,\n\t\t\toverQuery,\n\t\t\ttumbleWindowSizeSeconds);\n\n\t\tString joinQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  t1.key, \" +\n\t\t\t\"  t2.rowtime AS rowtime, \" +\n\t\t\t\"  t2.correct,\" +\n\t\t\t\"  t2.wStart \" +\n\t\t\t\"FROM table2 t1, (%s) t2 \" +\n\t\t\t\"WHERE \" +\n\t\t\t\"  t1.key = t2.key AND \" +\n\t\t\t\"  t1.rowtime BETWEEN t2.rowtime AND t2.rowtime + INTERVAL '%d' SECOND\",\n\t\t\ttumbleQuery,\n\t\t\ttumbleWindowSizeSeconds);\n\n\t\tString finalAgg = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  SUM(correct) AS correct, \" +\n\t\t\t\"  TUMBLE_START(rowtime, INTERVAL '20' SECOND) AS rowtime \" +\n\t\t\t\"FROM (%s) \" +\n\t\t\t\"GROUP BY TUMBLE(rowtime, INTERVAL '20' SECOND)\",\n\t\t\tjoinQuery);\n\n\t\t// get Table for SQL query\n\t\tTable result = tEnv.sqlQuery(finalAgg);\n\t\t// convert Table into append-only DataStream\n\t\tDataStream<Row> resultStream =\n\t\t\ttEnv.toAppendStream(result, Types.ROW(Types.INT, Types.SQL_TIMESTAMP));\n\n\t\tfinal StreamingFileSink<Row> sink = StreamingFileSink\n\t\t\t.forRowFormat(new Path(outputPath), (Encoder<Row>) (element, stream) -> {\n\t\t\t\tPrintStream out = new PrintStream(stream);\n\t\t\t\tout.println(element.toString());\n\t\t\t})\n\t\t\t.withBucketAssigner(new KeyBucketAssigner())\n\t\t\t.withRollingPolicy(OnCheckpointRollingPolicy.build())\n\t\t\t.build();\n\n\t\tresultStream\n\t\t\t// inject a KillMapper that forwards all records but terminates the first execution attempt\n\t\t\t.map(new KillMapper()).setParallelism(1)\n\t\t\t// add sink function\n\t\t\t.addSink(sink).setParallelism(1);\n\n\t\tsEnv.execute();\n\t}"
        ]
    ],
    "a194b37d9b99a47174de9108a937f821816d61f5": [
        [
            "HiveCatalogUseBlinkITCase::testBlinkUdf()",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "\t@Test\n\tpublic void testBlinkUdf() throws Exception {\n\t\tTableEnvironment tEnv = TableEnvironment.create(\n\t\t\t\tEnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build());\n\n\t\ttEnv.registerCatalog(\"myhive\", hiveCatalog);\n\t\ttEnv.useCatalog(\"myhive\");\n\n\t\tTableSchema schema = TableSchema.builder()\n\t\t\t\t.field(\"name\", DataTypes.STRING())\n\t\t\t\t.field(\"age\", DataTypes.INT())\n\t\t\t\t.build();\n\n\t\tFormatDescriptor format = new OldCsv()\n\t\t\t\t.field(\"name\", Types.STRING())\n\t\t\t\t.field(\"age\", Types.INT());\n\n\t\tCatalogTable source =\n\t\t\t\tnew CatalogTableBuilder(\n\t\t\t\t\t\tnew FileSystem().path(this.getClass().getResource(\"/csv/test.csv\").getPath()),\n\t\t\t\t\t\tschema)\n\t\t\t\t\t\t.withFormat(format)\n\t\t\t\t\t\t.inAppendMode()\n\t\t\t\t\t\t.withComment(\"Comment.\")\n\t\t\t\t\t\t.build();\n\n\t\tPath p = Paths.get(tempFolder.newFolder().getAbsolutePath(), \"test.csv\");\n\n\t\tTableSchema sinkSchema = TableSchema.builder()\n\t\t\t\t.field(\"name1\", Types.STRING())\n\t\t\t\t.field(\"name2\", Types.STRING())\n\t\t\t\t.field(\"sum1\", Types.INT())\n\t\t\t\t.field(\"sum2\", Types.LONG())\n\t\t\t\t.build();\n\n\t\tFormatDescriptor sinkFormat = new OldCsv()\n\t\t\t\t.field(\"name1\", Types.STRING())\n\t\t\t\t.field(\"name2\", Types.STRING())\n\t\t\t\t.field(\"sum1\", Types.INT())\n\t\t\t\t.field(\"sum2\", Types.LONG());\n\t\tCatalogTable sink =\n\t\t\t\tnew CatalogTableBuilder(\n\t\t\t\t\t\tnew FileSystem().path(p.toAbsolutePath().toString()),\n\t\t\t\t\t\tsinkSchema)\n\t\t\t\t\t\t.withFormat(sinkFormat)\n\t\t\t\t\t\t.inAppendMode()\n\t\t\t\t\t\t.withComment(\"Comment.\")\n\t\t\t\t\t\t.build();\n\n\t\thiveCatalog.createTable(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, sourceTableName),\n\t\t\t\tsource,\n\t\t\t\tfalse\n\t\t);\n\n\t\thiveCatalog.createTable(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, sinkTableName),\n\t\t\t\tsink,\n\t\t\t\tfalse\n\t\t);\n\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveSimpleUDF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"mygenericudf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveGenericUDF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudtf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveUDTF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudaf\"),\n\t\t\t\tnew CatalogFunctionImpl(GenericUDAFSum.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\n\t\tString innerSql = format(\"select mygenericudf(myudf(name), 1) as a, mygenericudf(myudf(age), 1) as b,\" +\n\t\t\t\t\" s from %s, lateral table(myudtf(name, 1)) as T(s)\", sourceTableName);\n\n\t\ttEnv.sqlUpdate(\n\t\t\t\tformat(\"insert into %s select a, s, sum(b), myudaf(b) from (%s) group by a, s\",\n\t\t\t\t\t\tsinkTableName,\n\t\t\t\t\t\tinnerSql));\n\t\ttEnv.execute(\"myjob\");\n\n\t\t// assert written result\n\t\tStringBuilder builder = new StringBuilder();\n\t\ttry (Stream<Path> paths = Files.walk(Paths.get(p.toAbsolutePath().toString()))) {\n\t\t\tpaths.filter(Files::isRegularFile).forEach(path -> {\n\t\t\t\ttry {\n\t\t\t\t\tString content = FileUtils.readFileUtf8(path.toFile());\n\t\t\t\t\tif (content.isEmpty()) {\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tbuilder.append(content);\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tthrow new RuntimeException(e);\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t\tList<String> results = Arrays.stream(builder.toString().split(\"\\n\"))\n\t\t\t\t.filter(s -> !s.isEmpty())\n\t\t\t\t.collect(Collectors.toList());\n\t\tresults.sort(String::compareTo);\n\t\tAssert.assertEquals(Arrays.asList(\"1,1,2,2\", \"2,2,4,4\", \"3,3,6,6\"), results);\n\t}",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "\t@Test\n\tpublic void testBlinkUdf() throws Exception {\n\t\tTableEnvironment tEnv = TableEnvironment.create(\n\t\t\t\tEnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build());\n\n\t\tBatchTestBase.configForMiniCluster(tEnv.getConfig());\n\n\t\ttEnv.registerCatalog(\"myhive\", hiveCatalog);\n\t\ttEnv.useCatalog(\"myhive\");\n\n\t\tTableSchema schema = TableSchema.builder()\n\t\t\t\t.field(\"name\", DataTypes.STRING())\n\t\t\t\t.field(\"age\", DataTypes.INT())\n\t\t\t\t.build();\n\n\t\tFormatDescriptor format = new OldCsv()\n\t\t\t\t.field(\"name\", Types.STRING())\n\t\t\t\t.field(\"age\", Types.INT());\n\n\t\tCatalogTable source =\n\t\t\t\tnew CatalogTableBuilder(\n\t\t\t\t\t\tnew FileSystem().path(this.getClass().getResource(\"/csv/test.csv\").getPath()),\n\t\t\t\t\t\tschema)\n\t\t\t\t\t\t.withFormat(format)\n\t\t\t\t\t\t.inAppendMode()\n\t\t\t\t\t\t.withComment(\"Comment.\")\n\t\t\t\t\t\t.build();\n\n\t\tPath p = Paths.get(tempFolder.newFolder().getAbsolutePath(), \"test.csv\");\n\n\t\tTableSchema sinkSchema = TableSchema.builder()\n\t\t\t\t.field(\"name1\", Types.STRING())\n\t\t\t\t.field(\"name2\", Types.STRING())\n\t\t\t\t.field(\"sum1\", Types.INT())\n\t\t\t\t.field(\"sum2\", Types.LONG())\n\t\t\t\t.build();\n\n\t\tFormatDescriptor sinkFormat = new OldCsv()\n\t\t\t\t.field(\"name1\", Types.STRING())\n\t\t\t\t.field(\"name2\", Types.STRING())\n\t\t\t\t.field(\"sum1\", Types.INT())\n\t\t\t\t.field(\"sum2\", Types.LONG());\n\t\tCatalogTable sink =\n\t\t\t\tnew CatalogTableBuilder(\n\t\t\t\t\t\tnew FileSystem().path(p.toAbsolutePath().toString()),\n\t\t\t\t\t\tsinkSchema)\n\t\t\t\t\t\t.withFormat(sinkFormat)\n\t\t\t\t\t\t.inAppendMode()\n\t\t\t\t\t\t.withComment(\"Comment.\")\n\t\t\t\t\t\t.build();\n\n\t\thiveCatalog.createTable(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, sourceTableName),\n\t\t\t\tsource,\n\t\t\t\tfalse\n\t\t);\n\n\t\thiveCatalog.createTable(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, sinkTableName),\n\t\t\t\tsink,\n\t\t\t\tfalse\n\t\t);\n\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveSimpleUDF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"mygenericudf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveGenericUDF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudtf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveUDTF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudaf\"),\n\t\t\t\tnew CatalogFunctionImpl(GenericUDAFSum.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\n\t\tString innerSql = format(\"select mygenericudf(myudf(name), 1) as a, mygenericudf(myudf(age), 1) as b,\" +\n\t\t\t\t\" s from %s, lateral table(myudtf(name, 1)) as T(s)\", sourceTableName);\n\n\t\ttEnv.sqlUpdate(\n\t\t\t\tformat(\"insert into %s select a, s, sum(b), myudaf(b) from (%s) group by a, s\",\n\t\t\t\t\t\tsinkTableName,\n\t\t\t\t\t\tinnerSql));\n\t\ttEnv.execute(\"myjob\");\n\n\t\t// assert written result\n\t\tStringBuilder builder = new StringBuilder();\n\t\ttry (Stream<Path> paths = Files.walk(Paths.get(p.toAbsolutePath().toString()))) {\n\t\t\tpaths.filter(Files::isRegularFile).forEach(path -> {\n\t\t\t\ttry {\n\t\t\t\t\tString content = FileUtils.readFileUtf8(path.toFile());\n\t\t\t\t\tif (content.isEmpty()) {\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tbuilder.append(content);\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tthrow new RuntimeException(e);\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t\tList<String> results = Arrays.stream(builder.toString().split(\"\\n\"))\n\t\t\t\t.filter(s -> !s.isEmpty())\n\t\t\t\t.collect(Collectors.toList());\n\t\tresults.sort(String::compareTo);\n\t\tAssert.assertEquals(Arrays.asList(\"1,1,2,2\", \"2,2,4,4\", \"3,3,6,6\"), results);\n\t}"
        ]
    ]
}