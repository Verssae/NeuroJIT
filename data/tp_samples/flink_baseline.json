{
    "67c4be648b1f51ceadae3a9e3dd41807802a89ef": [
        [
            "SocketWindowWordCount::main(String)",
            "  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\t// the port to connect to\n\t\tfinal int port;\n\t\ttry {\n\t\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\t\tport = params.getInt(\"port\");\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"No port specified. Please run 'SocketWindowWordCount --port <port>', \" +\n\t\t\t\t\t\"where port is the address of the text server\");\n\t\t\tSystem.err.println(\"To start a simple text server, run 'netcat -l <port>' and type the input text \" +\n\t\t\t\t\t\"into the command line\");\n\t\t\treturn;\n\t\t}\n\n\t\t// get the execution environment\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// get input data by connecting to the socket\n\t\tDataStream<String> text = env.socketTextStream(\"localhost\", port, \"\\n\");\n\n\t\t// parse the data, group it, window it, and aggregate the counts\n\t\tDataStream<WordWithCount> windowCounts = text\n\n\t\t\t\t.flatMap(new FlatMapFunction<String, WordWithCount>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void flatMap(String value, Collector<WordWithCount> out) {\n\t\t\t\t\t\tfor (String word : value.split(\"\\\\s\")) {\n\t\t\t\t\t\t\tout.collect(new WordWithCount(word, 1L));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\n\t\t\t\t.keyBy(\"word\")\n\t\t\t\t.timeWindow(Time.seconds(5), Time.seconds(1))\n\n\t\t\t\t.reduce(new ReduceFunction<WordWithCount>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic WordWithCount reduce(WordWithCount a, WordWithCount b) {\n\t\t\t\t\t\treturn new WordWithCount(a.word, a.count + b.count);\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\t// print the results with a single thread, rather than in parallel\n\t\twindowCounts.print().setParallelism(1);\n\n\t\tenv.execute(\"Socket Window WordCount\");\n\t}",
            "  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\t// the port to connect to\n\t\tfinal int port;\n\t\ttry {\n\t\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\t\t\tport = params.getInt(\"port\");\n\t\t} catch (Exception e) {\n\t\t\tSystem.err.println(\"No port specified. Please run 'SocketWindowWordCount --port <port>', \" +\n\t\t\t\t\t\"where port is the address of the text server\");\n\t\t\tSystem.err.println(\"To start a simple text server, run 'netcat -l <port>' and type the input text \" +\n\t\t\t\t\t\"into the command line\");\n\t\t\treturn;\n\t\t}\n\n\t\t// get the execution environment\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\n\t\t// get input data by connecting to the socket\n\t\tDataStream<String> text = env.socketTextStream(\"localhost\", port, \"\\n\");\n\n\t\t// parse the data, group it, window it, and aggregate the counts\n\t\tDataStream<WordWithCount> windowCounts = text\n\n\t\t\t\t.flatMap(new FlatMapFunction<String, WordWithCount>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic void flatMap(String value, Collector<WordWithCount> out) {\n\t\t\t\t\t\tfor (String word : value.split(\"\\\\s\")) {\n\t\t\t\t\t\t\tout.collect(new WordWithCount(word, 1L));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t})\n\n\t\t\t\t.keyBy(\"word\")\n\t\t\t\t.timeWindow(Time.seconds(5))\n\n\t\t\t\t.reduce(new ReduceFunction<WordWithCount>() {\n\t\t\t\t\t@Override\n\t\t\t\t\tpublic WordWithCount reduce(WordWithCount a, WordWithCount b) {\n\t\t\t\t\t\treturn new WordWithCount(a.word, a.count + b.count);\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\t// print the results with a single thread, rather than in parallel\n\t\twindowCounts.print().setParallelism(1);\n\n\t\tenv.execute(\"Socket Window WordCount\");\n\t}"
        ]
    ],
    "bfdaa3821c71f9fa3a3ff85f56154995d98b18b5": [
        [
            "Emitter::output(AsyncResult)",
            " 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 -\n 140 -\n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "\tprivate void output(AsyncResult asyncResult) throws InterruptedException {\n\t\tif (asyncResult.isWatermark()) {\n\t\t\tsynchronized (checkpointLock) {\n\t\t\t\t// remove the peeked element from the async collector buffer so that it is no longer\n\t\t\t\t// checkpointed\n\t\t\t\tstreamElementQueue.poll();\n\n\t\t\t\t// notify the main thread that there is again space left in the async collector\n\t\t\t\t// buffer\n\t\t\t\tcheckpointLock.notifyAll();\n\n\t\t\t\tAsyncWatermarkResult asyncWatermarkResult = asyncResult.asWatermark();\n\n\t\t\t\tLOG.debug(\"Output async watermark.\");\n\t\t\t\toutput.emitWatermark(asyncWatermarkResult.getWatermark());\n\t\t\t}\n\t\t} else {\n\t\t\tAsyncCollectionResult<OUT> streamRecordResult = asyncResult.asResultCollection();\n\n\t\t\tif (streamRecordResult.hasTimestamp()) {\n\t\t\t\ttimestampedCollector.setAbsoluteTimestamp(streamRecordResult.getTimestamp());\n\t\t\t} else {\n\t\t\t\ttimestampedCollector.eraseTimestamp();\n\t\t\t}\n\n\t\t\tsynchronized (checkpointLock) {\n\t\t\t\t// remove the peeked element from the async collector buffer so that it is no longer\n\t\t\t\t// checkpointed\n\t\t\t\tstreamElementQueue.poll();\n\n\t\t\t\t// notify the main thread that there is again space left in the async collector\n\t\t\t\t// buffer\n\t\t\t\tcheckpointLock.notifyAll();\n\n\t\t\t\tLOG.debug(\"Output async stream element collection result.\");\n\n\t\t\t\ttry {\n\t\t\t\t\tCollection<OUT> resultCollection = streamRecordResult.get();\n\n\t\t\t\t\tfor (OUT result : resultCollection) {\n\t\t\t\t\t\ttimestampedCollector.collect(result);\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\toperatorActions.failOperator(\n\t\t\t\t\t\tnew Exception(\"An async function call terminated with an exception. \" +\n\t\t\t\t\t\t\t\"Failing the AsyncWaitOperator.\", e));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}",
            " 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139 +\n 140 +\n 141 +\n 142 +\n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "\tprivate void output(AsyncResult asyncResult) throws InterruptedException {\n\t\tif (asyncResult.isWatermark()) {\n\t\t\tsynchronized (checkpointLock) {\n\t\t\t\t// remove the peeked element from the async collector buffer so that it is no longer\n\t\t\t\t// checkpointed\n\t\t\t\tstreamElementQueue.poll();\n\n\t\t\t\t// notify the main thread that there is again space left in the async collector\n\t\t\t\t// buffer\n\t\t\t\tcheckpointLock.notifyAll();\n\n\t\t\t\tAsyncWatermarkResult asyncWatermarkResult = asyncResult.asWatermark();\n\n\t\t\t\tLOG.debug(\"Output async watermark.\");\n\t\t\t\toutput.emitWatermark(asyncWatermarkResult.getWatermark());\n\t\t\t}\n\t\t} else {\n\t\t\tAsyncCollectionResult<OUT> streamRecordResult = asyncResult.asResultCollection();\n\n\t\t\tif (streamRecordResult.hasTimestamp()) {\n\t\t\t\ttimestampedCollector.setAbsoluteTimestamp(streamRecordResult.getTimestamp());\n\t\t\t} else {\n\t\t\t\ttimestampedCollector.eraseTimestamp();\n\t\t\t}\n\n\t\t\tsynchronized (checkpointLock) {\n\t\t\t\t// remove the peeked element from the async collector buffer so that it is no longer\n\t\t\t\t// checkpointed\n\t\t\t\tstreamElementQueue.poll();\n\n\t\t\t\t// notify the main thread that there is again space left in the async collector\n\t\t\t\t// buffer\n\t\t\t\tcheckpointLock.notifyAll();\n\n\t\t\t\tLOG.debug(\"Output async stream element collection result.\");\n\n\t\t\t\ttry {\n\t\t\t\t\tCollection<OUT> resultCollection = streamRecordResult.get();\n\n\t\t\t\t\tif (resultCollection != null) {\n\t\t\t\t\t\tfor (OUT result : resultCollection) {\n\t\t\t\t\t\t\ttimestampedCollector.collect(result);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} catch (Exception e) {\n\t\t\t\t\toperatorActions.failOperator(\n\t\t\t\t\t\tnew Exception(\"An async function call terminated with an exception. \" +\n\t\t\t\t\t\t\t\"Failing the AsyncWaitOperator.\", e));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "AsyncIOExample::SimpleSource::run(SourceContext)",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 -\n  97  \n  98  ",
            "\t\t@Override\n\t\tpublic void run(SourceContext<Integer> ctx) throws Exception {\n\t\t\twhile ((start < counter || counter == -1) && isRunning) {\n\t\t\t\tsynchronized (ctx.getCheckpointLock()) {\n\t\t\t\t\tctx.collect(start);\n\t\t\t\t\t++start;\n\n\t\t\t\t\t// loop back to 0\n\t\t\t\t\tif (start == Integer.MAX_VALUE) {\n\t\t\t\t\t\tstart = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tThread.sleep(10);\n\t\t\t}\n\t\t}",
            "  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96 +\n  97  \n  98  ",
            "\t\t@Override\n\t\tpublic void run(SourceContext<Integer> ctx) throws Exception {\n\t\t\twhile ((start < counter || counter == -1) && isRunning) {\n\t\t\t\tsynchronized (ctx.getCheckpointLock()) {\n\t\t\t\t\tctx.collect(start);\n\t\t\t\t\t++start;\n\n\t\t\t\t\t// loop back to 0\n\t\t\t\t\tif (start == Integer.MAX_VALUE) {\n\t\t\t\t\t\tstart = 0;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tThread.sleep(10L);\n\t\t\t}\n\t\t}"
        ]
    ],
    "8efacf588cee45eb99a24628136ced308c4fb418": [
        [
            "TableEnvironmentITCase::testCustomCalciteConfig()",
            " 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  ",
            "\t@Test(expected = TableException.class)\n\tpublic void testCustomCalciteConfig() {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tCalciteConfig cc = new CalciteConfigBuilder().replaceRuleSet(RuleSets.ofList()).build();\n\t\ttableEnv.getConfig().setCalciteConfig(cc);\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\tTable t = tableEnv.fromDataSet(ds);\n\t\ttableEnv.toDataSet(t, Row.class);\n\t}",
            " 480  \n 481  \n 482  \n 483  \n 484  \n 485 +\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  ",
            "\t@Test(expected = TableException.class)\n\tpublic void testCustomCalciteConfig() {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tCalciteConfig cc = new CalciteConfigBuilder().replaceOptRuleSet(RuleSets.ofList()).build();\n\t\ttableEnv.getConfig().setCalciteConfig(cc);\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\tTable t = tableEnv.fromDataSet(ds);\n\t\ttableEnv.toDataSet(t, Row.class);\n\t}"
        ]
    ],
    "ad21a441434b9ac5886b664871553bf57885e984": [
        [
            "NFA::extractCurrentMatches(ComputationState)",
            " 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578 -\n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  ",
            "\tMap<String, List<T>> extractCurrentMatches(final ComputationState<T> computationState) {\n\t\tif (computationState.getPreviousState() == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\n\t\tCollection<LinkedHashMultimap<String, T>> paths = stringSharedBuffer.extractPatterns(\n\t\t\t\tcomputationState.getPreviousState().getName(),\n\t\t\t\tcomputationState.getEvent(),\n\t\t\t\tcomputationState.getTimestamp(),\n\t\t\t\tcomputationState.getVersion());\n\n\t\t// for a given computation state, we cannot have more than one matching patterns.\n\t\tPreconditions.checkArgument(paths.size() <= 1);\n\n\t\tTypeSerializer<T> serializer = nonDuplicatingTypeSerializer.getTypeSerializer();\n\n\t\tMap<String, List<T>> result = new HashMap<>();\n\t\tfor (LinkedHashMultimap<String, T> path: paths) {\n\t\t\tfor (String key: path.keySet()) {\n\t\t\t\tSet<T> events = path.get(key);\n\t\t\t\tList<T> values = new ArrayList<>(events.size());\n\t\t\t\tfor (T event: events) {\n\t\t\t\t\tvalues.add(serializer.isImmutableType() ? event : serializer.copy(event));\n\t\t\t\t}\n\t\t\t\tresult.put(key, values);\n\t\t\t}\n\t\t}\n\t\treturn result;\n\t}",
            " 566  \n 567  \n 568  \n 569  \n 570  \n 571  \n 572  \n 573  \n 574  \n 575  \n 576  \n 577  \n 578 +\n 579  \n 580  \n 581  \n 582  \n 583  \n 584  \n 585  \n 586  \n 587  \n 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  ",
            "\tMap<String, List<T>> extractCurrentMatches(final ComputationState<T> computationState) {\n\t\tif (computationState.getPreviousState() == null) {\n\t\t\treturn new HashMap<>();\n\t\t}\n\n\t\tCollection<LinkedHashMultimap<String, T>> paths = stringSharedBuffer.extractPatterns(\n\t\t\t\tcomputationState.getPreviousState().getName(),\n\t\t\t\tcomputationState.getEvent(),\n\t\t\t\tcomputationState.getTimestamp(),\n\t\t\t\tcomputationState.getVersion());\n\n\t\t// for a given computation state, we cannot have more than one matching patterns.\n\t\tPreconditions.checkState(paths.size() <= 1);\n\n\t\tTypeSerializer<T> serializer = nonDuplicatingTypeSerializer.getTypeSerializer();\n\n\t\tMap<String, List<T>> result = new HashMap<>();\n\t\tfor (LinkedHashMultimap<String, T> path: paths) {\n\t\t\tfor (String key: path.keySet()) {\n\t\t\t\tSet<T> events = path.get(key);\n\t\t\t\tList<T> values = new ArrayList<>(events.size());\n\t\t\t\tfor (T event: events) {\n\t\t\t\t\tvalues.add(serializer.isImmutableType() ? event : serializer.copy(event));\n\t\t\t\t}\n\t\t\t\tresult.put(key, values);\n\t\t\t}\n\t\t}\n\t\treturn result;\n\t}"
        ],
        [
            "NFA::extractPatternMatches(ComputationState)",
            " 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612 -\n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  ",
            "\t/**\n\t * Extracts all the sequences of events from the start to the given computation state. An event\n\t * sequence is returned as a map which contains the events and the names of the states to which\n\t * the events were mapped.\n\t *\n\t * @param computationState The end computation state of the extracted event sequences\n\t * @return Collection of event sequences which end in the given computation state\n\t */\n\tprivate Collection<Map<String, T>> extractPatternMatches(final ComputationState<T> computationState) {\n\t\tCollection<LinkedHashMultimap<String, T>> paths = stringSharedBuffer.extractPatterns(\n\t\t\tcomputationState.getPreviousState().getName(),\n\t\t\tcomputationState.getEvent(),\n\t\t\tcomputationState.getTimestamp(),\n\t\t\tcomputationState.getVersion());\n\n\t\t// for a given computation state, we cannot have more than one matching patterns.\n\t\tPreconditions.checkArgument(paths.size() <= 1);\n\n\t\tList<Map<String, T>> result = new ArrayList<>();\n\n\t\tTypeSerializer<T> serializer = nonDuplicatingTypeSerializer.getTypeSerializer();\n\n\t\t// generate the correct names from the collection of LinkedHashMultimaps\n\t\tfor (LinkedHashMultimap<String, T> path: paths) {\n\t\t\tMap<String, T> resultPath = new HashMap<>();\n\t\t\tfor (String key: path.keySet()) {\n\t\t\t\tint counter = 0;\n\t\t\t\tSet<T> events = path.get(key);\n\n\t\t\t\t// we iterate over the elements in insertion order\n\t\t\t\tfor (T event: events) {\n\t\t\t\t\tresultPath.put(\n\t\t\t\t\t\tevents.size() > 1 ? generateStateName(key, counter): key,\n\t\t\t\t\t\t// copy the element so that the user can change it\n\t\t\t\t\t\tserializer.isImmutableType() ? event : serializer.copy(event)\n\t\t\t\t\t);\n\t\t\t\t\tcounter++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tresult.add(resultPath);\n\t\t}\n\n\t\treturn result;\n\t}",
            " 596  \n 597  \n 598  \n 599  \n 600  \n 601  \n 602  \n 603  \n 604  \n 605  \n 606  \n 607  \n 608  \n 609  \n 610  \n 611  \n 612 +\n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  ",
            "\t/**\n\t * Extracts all the sequences of events from the start to the given computation state. An event\n\t * sequence is returned as a map which contains the events and the names of the states to which\n\t * the events were mapped.\n\t *\n\t * @param computationState The end computation state of the extracted event sequences\n\t * @return Collection of event sequences which end in the given computation state\n\t */\n\tprivate Collection<Map<String, T>> extractPatternMatches(final ComputationState<T> computationState) {\n\t\tCollection<LinkedHashMultimap<String, T>> paths = stringSharedBuffer.extractPatterns(\n\t\t\tcomputationState.getPreviousState().getName(),\n\t\t\tcomputationState.getEvent(),\n\t\t\tcomputationState.getTimestamp(),\n\t\t\tcomputationState.getVersion());\n\n\t\t// for a given computation state, we cannot have more than one matching patterns.\n\t\tPreconditions.checkState(paths.size() <= 1);\n\n\t\tList<Map<String, T>> result = new ArrayList<>();\n\n\t\tTypeSerializer<T> serializer = nonDuplicatingTypeSerializer.getTypeSerializer();\n\n\t\t// generate the correct names from the collection of LinkedHashMultimaps\n\t\tfor (LinkedHashMultimap<String, T> path: paths) {\n\t\t\tMap<String, T> resultPath = new HashMap<>();\n\t\t\tfor (String key: path.keySet()) {\n\t\t\t\tint counter = 0;\n\t\t\t\tSet<T> events = path.get(key);\n\n\t\t\t\t// we iterate over the elements in insertion order\n\t\t\t\tfor (T event: events) {\n\t\t\t\t\tresultPath.put(\n\t\t\t\t\t\tevents.size() > 1 ? generateStateName(key, counter): key,\n\t\t\t\t\t\t// copy the element so that the user can change it\n\t\t\t\t\t\tserializer.isImmutableType() ? event : serializer.copy(event)\n\t\t\t\t\t);\n\t\t\t\t\tcounter++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tresult.add(resultPath);\n\t\t}\n\n\t\treturn result;\n\t}"
        ]
    ],
    "6a0ada81e19a972b89556f1b4b0277508b7caa7a": [
        [
            "TableEnvironmentITCase::testCustomCalciteConfig()",
            " 441  \n 442  \n 443  \n 444  \n 445  \n 446 -\n 447  \n 448  \n 449  \n 450  \n 451  \n 452  ",
            "\t@Test(expected = TableException.class)\n\tpublic void testCustomCalciteConfig() {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tCalciteConfig cc = new CalciteConfigBuilder().replaceOptRuleSet(RuleSets.ofList()).build();\n\t\ttableEnv.getConfig().setCalciteConfig(cc);\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\tTable t = tableEnv.fromDataSet(ds);\n\t\ttableEnv.toDataSet(t, Row.class);\n\t}",
            " 441  \n 442  \n 443  \n 444  \n 445  \n 446 +\n 447 +\n 448 +\n 449 +\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  ",
            "\t@Test(expected = TableException.class)\n\tpublic void testCustomCalciteConfig() {\n\t\tExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\t\tBatchTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env, config());\n\n\t\tCalciteConfig cc = new CalciteConfigBuilder()\n\t\t\t\t.replaceLogicalOptRuleSet(RuleSets.ofList())\n\t\t\t\t.replacePhysicalOptRuleSet(RuleSets.ofList())\n\t\t\t\t.build();\n\t\ttableEnv.getConfig().setCalciteConfig(cc);\n\n\t\tDataSet<Tuple3<Integer, Long, String>> ds = CollectionDataSets.get3TupleDataSet(env);\n\t\tTable t = tableEnv.fromDataSet(ds);\n\t\ttableEnv.toDataSet(t, Row.class);\n\t}"
        ]
    ],
    "3a65e5acbcc29636b0ce1631815861089fc21dca": [
        [
            "SqlITCase::testFilter()",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 -\n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t@Test\n\tpublic void testFilter() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds = StreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"MyTable\", ds, \"a, b, c, d, e\");\n\n\t\tString sqlQuery = \"SELECT a, b, e FROM MyTable WHERE c < 4\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toDataStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,1\");\n\t\texpected.add(\"2,2,2\");\n\t\texpected.add(\"2,3,1\");\n\t\texpected.add(\"3,4,2\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118 +\n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  ",
            "\t@Test\n\tpublic void testFilter() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds = StreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"MyTable\", ds, \"a, b, c, d, e\");\n\n\t\tString sqlQuery = \"SELECT a, b, e FROM MyTable WHERE c < 4\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,1\");\n\t\texpected.add(\"2,2,2\");\n\t\texpected.add(\"2,3,1\");\n\t\texpected.add(\"3,4,2\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "SqlITCase::testSelect()",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "\t@Test\n\tpublic void testSelect() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds = StreamTestData.getSmall3TupleDataSet(env);\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTable\", in);\n\n\t\tString sqlQuery = \"SELECT * FROM MyTable\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toDataStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            "  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 +\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  ",
            "\t@Test\n\tpublic void testSelect() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds = StreamTestData.getSmall3TupleDataSet(env);\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTable\", in);\n\n\t\tString sqlQuery = \"SELECT * FROM MyTable\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "SqlITCase::testUnion()",
            " 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 -\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "\t@Test\n\tpublic void testUnion() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds1 = StreamTestData.getSmall3TupleDataSet(env);\n\t\tTable t1 = tableEnv.fromDataStream(ds1, \"a,b,c\");\n\t\ttableEnv.registerTable(\"T1\", t1);\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds2 = StreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"T2\", ds2, \"a, b, d, c, e\");\n\n\t\tString sqlQuery = \"SELECT * FROM T1 \" +\n\t\t\t\t\t\t\t\"UNION ALL \" +\n\t\t\t\t\t\t\t\"(SELECT a, b, c FROM T2 WHERE a\t< 3)\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toDataStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\t\texpected.add(\"1,1,Hallo\");\n\t\texpected.add(\"2,2,Hallo Welt\");\n\t\texpected.add(\"2,3,Hallo Welt wie\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            " 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149 +\n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  ",
            "\t@Test\n\tpublic void testUnion() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tDataStream<Tuple3<Integer, Long, String>> ds1 = StreamTestData.getSmall3TupleDataSet(env);\n\t\tTable t1 = tableEnv.fromDataStream(ds1, \"a,b,c\");\n\t\ttableEnv.registerTable(\"T1\", t1);\n\n\t\tDataStream<Tuple5<Integer, Long, Integer, String, Long>> ds2 = StreamTestData.get5TupleDataStream(env);\n\t\ttableEnv.registerDataStream(\"T2\", ds2, \"a, b, d, c, e\");\n\n\t\tString sqlQuery = \"SELECT * FROM T1 \" +\n\t\t\t\t\t\t\t\"UNION ALL \" +\n\t\t\t\t\t\t\t\"(SELECT a, b, c FROM T2 WHERE a\t< 3)\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,1,Hi\");\n\t\texpected.add(\"2,2,Hello\");\n\t\texpected.add(\"3,2,Hello world\");\n\t\texpected.add(\"1,1,Hallo\");\n\t\texpected.add(\"2,2,Hallo Welt\");\n\t\texpected.add(\"2,3,Hallo Welt wie\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ],
        [
            "SqlITCase::testRowRegisterRowWithNames()",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 -\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\t@Test\n\tpublic void testRowRegisterRowWithNames() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tList<Row> data = new ArrayList<>();\n\t\tdata.add(Row.of(1, 1L, \"Hi\"));\n\t\tdata.add(Row.of(2, 2L, \"Hello\"));\n\t\tdata.add(Row.of(3, 2L, \"Hello world\"));\n\t\t\n\t\tTypeInformation<?>[] types = {\n\t\t\t\tBasicTypeInfo.INT_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.LONG_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.STRING_TYPE_INFO};\n\t\tString names[] = {\"a\",\"b\",\"c\"};\n\t\t\n\t\tRowTypeInfo typeInfo = new RowTypeInfo(types, names);\n\t\t\n\t\tDataStream<Row> ds = env.fromCollection(data).returns(typeInfo);\n\t\t\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTableRow\", in);\n\n\t\tString sqlQuery = \"SELECT a,c FROM MyTableRow\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toDataStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,Hi\");\n\t\texpected.add(\"2,Hello\");\n\t\texpected.add(\"3,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}",
            "  42  \n  43  \n  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69 +\n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\t@Test\n\tpublic void testRowRegisterRowWithNames() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tStreamTableEnvironment tableEnv = TableEnvironment.getTableEnvironment(env);\n\t\tStreamITCase.clear();\n\n\t\tList<Row> data = new ArrayList<>();\n\t\tdata.add(Row.of(1, 1L, \"Hi\"));\n\t\tdata.add(Row.of(2, 2L, \"Hello\"));\n\t\tdata.add(Row.of(3, 2L, \"Hello world\"));\n\t\t\n\t\tTypeInformation<?>[] types = {\n\t\t\t\tBasicTypeInfo.INT_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.LONG_TYPE_INFO,\n\t\t\t\tBasicTypeInfo.STRING_TYPE_INFO};\n\t\tString names[] = {\"a\",\"b\",\"c\"};\n\t\t\n\t\tRowTypeInfo typeInfo = new RowTypeInfo(types, names);\n\t\t\n\t\tDataStream<Row> ds = env.fromCollection(data).returns(typeInfo);\n\t\t\n\t\tTable in = tableEnv.fromDataStream(ds, \"a,b,c\");\n\t\ttableEnv.registerTable(\"MyTableRow\", in);\n\n\t\tString sqlQuery = \"SELECT a,c FROM MyTableRow\";\n\t\tTable result = tableEnv.sql(sqlQuery);\n\n\t\tDataStream<Row> resultSet = tableEnv.toAppendStream(result, Row.class);\n\t\tresultSet.addSink(new StreamITCase.StringSink());\n\t\tenv.execute();\n\n\t\tList<String> expected = new ArrayList<>();\n\t\texpected.add(\"1,Hi\");\n\t\texpected.add(\"2,Hello\");\n\t\texpected.add(\"3,Hello world\");\n\n\t\tStreamITCase.compareWithList(expected);\n\t}"
        ]
    ],
    "0bca76ede8b7447014a7d7ed17633d77ecfafe18": [
        [
            "MesosConfiguration::logMesosConfig(Logger,MesosConfiguration)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126 -\n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140 -\n 141  \n 142  \n 143 -\n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  ",
            "\t/**\n\t * A utility method to log relevant Mesos connection info\n     */\n\tpublic static void logMesosConfig(Logger log, MesosConfiguration config) {\n\n\t\tMap<String,String> env = System.getenv();\n\t\tProtos.FrameworkInfo.Builder info = config.frameworkInfo();\n\n\t\tlog.info(\"--------------------------------------------------------------------------------\");\n\t\tlog.info(\" Mesos Info:\");\n\t\tlog.info(\"    Master URL: {}\", config.masterUrl());\n\n\t\tlog.info(\" Framework Info:\");\n\t\tlog.info(\"    ID: {}\", info.hasId() ? info.getId().getValue() : \"(none)\");\n\t\tlog.info(\"    Name: {}\", info.hasName() ? info.getName() : \"(none)\");\n\t\tlog.info(\"    Failover Timeout (secs): {}\", info.getFailoverTimeout());\n\t\tlog.info(\"    Role: {}\", info.hasRole() ? info.getRole() : \"(none)\");\n\t\tlog.info(\"    Principal: {}\", info.hasPrincipal() ? info.getPrincipal() : \"(none)\");\n\t\tlog.info(\"    Host: {}\", info.hasHostname() ? info.getHostname() : \"(none)\");\n\t\tif(env.containsKey(\"LIBPROCESS_IP\")) {\n\t\t\tlog.info(\"    LIBPROCESS_IP: {}\", env.get(\"LIBPROCESS_IP\"));\n\t\t}\n\t\tif(env.containsKey(\"LIBPROCESS_PORT\")) {\n\t\t\tlog.info(\"    LIBPROCESS_PORT: {}\", env.get(\"LIBPROCESS_PORT\"));\n\t\t}\n\t\tlog.info(\"    Web UI: {}\", info.hasWebuiUrl() ? info.getWebuiUrl() : \"(none)\");\n\n\t\tlog.info(\"--------------------------------------------------------------------------------\");\n\n\t}",
            " 122  \n 123  \n 124  \n 125  \n 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141 +\n 142  \n 143  \n 144 +\n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  ",
            "\t/**\n\t * A utility method to log relevant Mesos connection info.\n\t */\n\tpublic static void logMesosConfig(Logger log, MesosConfiguration config) {\n\n\t\tMap<String, String> env = System.getenv();\n\t\tProtos.FrameworkInfo.Builder info = config.frameworkInfo();\n\n\t\tlog.info(\"--------------------------------------------------------------------------------\");\n\t\tlog.info(\" Mesos Info:\");\n\t\tlog.info(\"    Master URL: {}\", config.masterUrl());\n\n\t\tlog.info(\" Framework Info:\");\n\t\tlog.info(\"    ID: {}\", info.hasId() ? info.getId().getValue() : \"(none)\");\n\t\tlog.info(\"    Name: {}\", info.hasName() ? info.getName() : \"(none)\");\n\t\tlog.info(\"    Failover Timeout (secs): {}\", info.getFailoverTimeout());\n\t\tlog.info(\"    Role: {}\", info.hasRole() ? info.getRole() : \"(none)\");\n\t\tlog.info(\"    Principal: {}\", info.hasPrincipal() ? info.getPrincipal() : \"(none)\");\n\t\tlog.info(\"    Host: {}\", info.hasHostname() ? info.getHostname() : \"(none)\");\n\t\tif (env.containsKey(\"LIBPROCESS_IP\")) {\n\t\t\tlog.info(\"    LIBPROCESS_IP: {}\", env.get(\"LIBPROCESS_IP\"));\n\t\t}\n\t\tif (env.containsKey(\"LIBPROCESS_PORT\")) {\n\t\t\tlog.info(\"    LIBPROCESS_PORT: {}\", env.get(\"LIBPROCESS_PORT\"));\n\t\t}\n\t\tlog.info(\"    Web UI: {}\", info.hasWebuiUrl() ? info.getWebuiUrl() : \"(none)\");\n\n\t\tlog.info(\"--------------------------------------------------------------------------------\");\n\n\t}"
        ],
        [
            "MesosConfiguration::createDriver(Scheduler,boolean)",
            "  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99 -\n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  ",
            "\t/**\n\t * Create the Mesos scheduler driver based on this configuration.\n\t * @param scheduler the scheduler to use.\n\t * @param implicitAcknowledgements whether to configure the driver for implicit acknowledgements.\n     * @return a scheduler driver.\n     */\n\tpublic SchedulerDriver createDriver(Scheduler scheduler, boolean implicitAcknowledgements) {\n\t\tMesosSchedulerDriver schedulerDriver;\n\t\tif(this.credential().isDefined()) {\n\t\t\tschedulerDriver =\n\t\t\t\tnew MesosSchedulerDriver(scheduler, frameworkInfo.build(), this.masterUrl(),\n\t\t\t\t\timplicitAcknowledgements, this.credential().get().build());\n\t\t}\n\t\telse {\n\t\t\tschedulerDriver =\n\t\t\t\tnew MesosSchedulerDriver(scheduler, frameworkInfo.build(), this.masterUrl(),\n\t\t\t\t\timplicitAcknowledgements);\n\t\t}\n\t\treturn schedulerDriver;\n\t}",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  ",
            "\t/**\n\t * Create the Mesos scheduler driver based on this configuration.\n\t * @param scheduler the scheduler to use.\n\t * @param implicitAcknowledgements whether to configure the driver for implicit acknowledgements.\n\t * @return a scheduler driver.\n\t */\n\tpublic SchedulerDriver createDriver(Scheduler scheduler, boolean implicitAcknowledgements) {\n\t\tMesosSchedulerDriver schedulerDriver;\n\t\tif (this.credential().isDefined()) {\n\t\t\tschedulerDriver =\n\t\t\t\tnew MesosSchedulerDriver(scheduler, frameworkInfo.build(), this.masterUrl(),\n\t\t\t\t\timplicitAcknowledgements, this.credential().get().build());\n\t\t}\n\t\telse {\n\t\t\tschedulerDriver =\n\t\t\t\tnew MesosSchedulerDriver(scheduler, frameworkInfo.build(), this.masterUrl(),\n\t\t\t\t\timplicitAcknowledgements);\n\t\t}\n\t\treturn schedulerDriver;\n\t}"
        ],
        [
            "FlinkMesosSessionCli::decodeDynamicProperties(String)",
            "  36  \n  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44 -\n  45 -\n  46 -\n  47  \n  48  \n  49  \n  50  \n  51  \n  52 -\n  53 -\n  54  \n  55  \n  56  ",
            "\t/**\n\t * Decode encoded dynamic properties.\n\t * @param dynamicPropertiesEncoded encoded properties produced by the encoding method.\n\t * @return a configuration instance to be merged with the static configuration.\n\t */\n\tpublic static Configuration decodeDynamicProperties(String dynamicPropertiesEncoded) {\n\t\ttry {\n\t\t\tConfiguration configuration = new Configuration();\n\t\t\tif(dynamicPropertiesEncoded != null) {\n\t\t\t\tTypeReference<Map<String, String>> typeRef = new TypeReference<Map<String, String>>() {};\n\t\t\t\tMap<String,String> props = mapper.readValue(dynamicPropertiesEncoded, typeRef);\n\t\t\t\tfor (Map.Entry<String, String> property : props.entrySet()) {\n\t\t\t\t\tconfiguration.setString(property.getKey(), property.getValue());\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn configuration;\n\t\t}\n\t\tcatch(IOException ex) {\n\t\t\tthrow new IllegalArgumentException(\"unreadable encoded properties\", ex);\n\t\t}\n\t}",
            "  37  \n  38  \n  39  \n  40  \n  41  \n  42  \n  43  \n  44  \n  45  \n  46 +\n  47 +\n  48 +\n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55 +\n  56  \n  57  \n  58  ",
            "\t/**\n\t * Decode encoded dynamic properties.\n\t *\n\t * @param dynamicPropertiesEncoded encoded properties produced by the encoding method.\n\t * @return a configuration instance to be merged with the static configuration.\n\t */\n\tpublic static Configuration decodeDynamicProperties(String dynamicPropertiesEncoded) {\n\t\ttry {\n\t\t\tConfiguration configuration = new Configuration();\n\t\t\tif (dynamicPropertiesEncoded != null) {\n\t\t\t\tTypeReference<Map<String, String>> typeRef = new TypeReference<Map<String, String>>() {\n\t\t\t\t};\n\t\t\t\tMap<String, String> props = mapper.readValue(dynamicPropertiesEncoded, typeRef);\n\t\t\t\tfor (Map.Entry<String, String> property : props.entrySet()) {\n\t\t\t\t\tconfiguration.setString(property.getKey(), property.getValue());\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn configuration;\n\t\t} catch (IOException ex) {\n\t\t\tthrow new IllegalArgumentException(\"unreadable encoded properties\", ex);\n\t\t}\n\t}"
        ],
        [
            "MesosTaskManagerParameters::create(Configuration)",
            " 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215 -\n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224 -\n 225  \n 226  \n 227  \n 228  \n 229  \n 230 -\n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253 -\n 254  \n 255  \n 256  \n 257  \n 258  ",
            "\t/**\n\t * Create the Mesos TaskManager parameters.\n\t * @param flinkConfig the TM configuration.\n     */\n\tpublic static MesosTaskManagerParameters create(Configuration flinkConfig) {\n\n\t\tList<ConstraintEvaluator> constraints = parseConstraints(flinkConfig.getString(MESOS_CONSTRAINTS_HARD_HOSTATTR));\n\t\t// parse the common parameters\n\t\tContaineredTaskManagerParameters containeredParameters = ContaineredTaskManagerParameters.create(\n\t\t\tflinkConfig,\n\t\t\tflinkConfig.getInteger(MESOS_RM_TASKS_MEMORY_MB),\n\t\t\tflinkConfig.getInteger(MESOS_RM_TASKS_SLOTS));\n\n\t\tdouble cpus = flinkConfig.getDouble(MESOS_RM_TASKS_CPUS);\n\t\tif(cpus <= 0.0) {\n\t\t\tcpus = Math.max(containeredParameters.numSlots(), 1.0);\n\t\t}\n\n\t\t// parse the containerization parameters\n\t\tString imageName = flinkConfig.getString(MESOS_RM_CONTAINER_IMAGE_NAME);\n\n\t\tContainerType containerType;\n\t\tString containerTypeString = flinkConfig.getString(MESOS_RM_CONTAINER_TYPE);\n\t\tswitch(containerTypeString) {\n\t\t\tcase MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE_MESOS:\n\t\t\t\tcontainerType = ContainerType.MESOS;\n\t\t\t\tbreak;\n\t\t\tcase MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE_DOCKER:\n\t\t\t\tcontainerType = ContainerType.DOCKER;\n\t\t\t\tif(imageName == null || imageName.length() == 0) {\n\t\t\t\t\tthrow new IllegalConfigurationException(MESOS_RM_CONTAINER_IMAGE_NAME.key() +\n\t\t\t\t\t\t\" must be specified for docker container type\");\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalConfigurationException(\"invalid container type: \" + containerTypeString);\n\t\t}\n\n\t\tOption<String> containerVolOpt = Option.<String>apply(flinkConfig.getString(MESOS_RM_CONTAINER_VOLUMES));\n\n\t\tList<Protos.Volume> containerVolumes = buildVolumes(containerVolOpt);\n\n\t\t//obtain Task Manager Host Name from the configuration\n\t\tOption<String> taskManagerHostname = Option.apply(flinkConfig.getString(MESOS_TM_HOSTNAME));\n\n\t\t//obtain bootstrap command from the configuration\n\t\tOption<String> tmBootstrapCommand = Option.apply(flinkConfig.getString(MESOS_TM_BOOTSTRAP_CMD));\n\n\t\treturn new MesosTaskManagerParameters(\n\t\t\tcpus,\n\t\t\tcontainerType,\n\t\t\tOption.apply(imageName),\n\t\t\tcontaineredParameters,\t\t\t\n\t\t\tcontainerVolumes,\n\t\t\tconstraints,\n\t\t\ttmBootstrapCommand,\n\t\t\ttaskManagerHostname);\n\t}",
            " 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221 +\n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230 +\n 231  \n 232  \n 233  \n 234  \n 235  \n 236 +\n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259 +\n 260  \n 261  \n 262  \n 263  \n 264  ",
            "\t/**\n\t * Create the Mesos TaskManager parameters.\n\t *\n\t * @param flinkConfig the TM configuration.\n\t */\n\tpublic static MesosTaskManagerParameters create(Configuration flinkConfig) {\n\n\t\tList<ConstraintEvaluator> constraints = parseConstraints(flinkConfig.getString(MESOS_CONSTRAINTS_HARD_HOSTATTR));\n\t\t// parse the common parameters\n\t\tContaineredTaskManagerParameters containeredParameters = ContaineredTaskManagerParameters.create(\n\t\t\tflinkConfig,\n\t\t\tflinkConfig.getInteger(MESOS_RM_TASKS_MEMORY_MB),\n\t\t\tflinkConfig.getInteger(MESOS_RM_TASKS_SLOTS));\n\n\t\tdouble cpus = flinkConfig.getDouble(MESOS_RM_TASKS_CPUS);\n\t\tif (cpus <= 0.0) {\n\t\t\tcpus = Math.max(containeredParameters.numSlots(), 1.0);\n\t\t}\n\n\t\t// parse the containerization parameters\n\t\tString imageName = flinkConfig.getString(MESOS_RM_CONTAINER_IMAGE_NAME);\n\n\t\tContainerType containerType;\n\t\tString containerTypeString = flinkConfig.getString(MESOS_RM_CONTAINER_TYPE);\n\t\tswitch (containerTypeString) {\n\t\t\tcase MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE_MESOS:\n\t\t\t\tcontainerType = ContainerType.MESOS;\n\t\t\t\tbreak;\n\t\t\tcase MESOS_RESOURCEMANAGER_TASKS_CONTAINER_TYPE_DOCKER:\n\t\t\t\tcontainerType = ContainerType.DOCKER;\n\t\t\t\tif (imageName == null || imageName.length() == 0) {\n\t\t\t\t\tthrow new IllegalConfigurationException(MESOS_RM_CONTAINER_IMAGE_NAME.key() +\n\t\t\t\t\t\t\" must be specified for docker container type\");\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalConfigurationException(\"invalid container type: \" + containerTypeString);\n\t\t}\n\n\t\tOption<String> containerVolOpt = Option.<String>apply(flinkConfig.getString(MESOS_RM_CONTAINER_VOLUMES));\n\n\t\tList<Protos.Volume> containerVolumes = buildVolumes(containerVolOpt);\n\n\t\t//obtain Task Manager Host Name from the configuration\n\t\tOption<String> taskManagerHostname = Option.apply(flinkConfig.getString(MESOS_TM_HOSTNAME));\n\n\t\t//obtain bootstrap command from the configuration\n\t\tOption<String> tmBootstrapCommand = Option.apply(flinkConfig.getString(MESOS_TM_BOOTSTRAP_CMD));\n\n\t\treturn new MesosTaskManagerParameters(\n\t\t\tcpus,\n\t\t\tcontainerType,\n\t\t\tOption.apply(imageName),\n\t\t\tcontaineredParameters,\n\t\t\tcontainerVolumes,\n\t\t\tconstraints,\n\t\t\ttmBootstrapCommand,\n\t\t\ttaskManagerHostname);\n\t}"
        ],
        [
            "MesosFlinkResourceManager::acceptOffers(AcceptOffers)",
            " 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423  \n 424 -\n 425 -\n 426  \n 427  \n 428  ",
            "\t/**\n\t * Accept offers as advised by the launch coordinator.\n\t *\n\t * Acceptance is routed through the RM to update the persistent state before\n\t * forwarding the message to Mesos.\n\t */\n\tprivate void acceptOffers(AcceptOffers msg) {\n\n\t\ttry {\n\t\t\tList<TaskMonitor.TaskGoalStateUpdated> toMonitor = new ArrayList<>(msg.operations().size());\n\n\t\t\t// transition the persistent state of some tasks to Launched\n\t\t\tfor (Protos.Offer.Operation op : msg.operations()) {\n\t\t\t\tif (op.getType() != Protos.Offer.Operation.Type.LAUNCH) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tfor (Protos.TaskInfo info : op.getLaunch().getTaskInfosList()) {\n\t\t\t\t\tMesosWorkerStore.Worker worker = workersInNew.remove(extractResourceID(info.getTaskId()));\n\t\t\t\t\tassert (worker != null);\n\n\t\t\t\t\tworker = worker.launchWorker(info.getSlaveId(), msg.hostname());\n\t\t\t\t\tworkerStore.putWorker(worker);\n\t\t\t\t\tworkersInLaunch.put(extractResourceID(worker.taskID()), worker);\n\n\t\t\t\t\tLOG.info(\"Launching Mesos task {} on host {}.\",\n\t\t\t\t\t\tworker.taskID().getValue(), worker.hostname().get());\n\n\t\t\t\t\ttoMonitor.add(new TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker)));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// tell the task router about the new plans\n\t\t\tfor (TaskMonitor.TaskGoalStateUpdated update : toMonitor) {\n\t\t\t\ttaskRouter.tell(update, self());\n\t\t\t}\n\n\t\t\t// send the acceptance message to Mesos\n\t\t\tschedulerDriver.acceptOffers(msg.offerIds(), msg.operations(), msg.filters());\n\t\t}\n\t\tcatch(Exception ex) {\n\t\t\tfatalError(\"unable to accept offers\", ex);\n\t\t}\n\t}",
            " 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  \n 393  \n 394  \n 395  \n 396  \n 397  \n 398  \n 399  \n 400  \n 401  \n 402  \n 403  \n 404  \n 405  \n 406  \n 407  \n 408  \n 409  \n 410  \n 411  \n 412  \n 413  \n 414  \n 415  \n 416  \n 417  \n 418  \n 419  \n 420  \n 421  \n 422  \n 423 +\n 424  \n 425  \n 426  ",
            "\t/**\n\t * Accept offers as advised by the launch coordinator.\n\t *\n\t * <p>Acceptance is routed through the RM to update the persistent state before\n\t * forwarding the message to Mesos.\n\t */\n\tprivate void acceptOffers(AcceptOffers msg) {\n\n\t\ttry {\n\t\t\tList<TaskMonitor.TaskGoalStateUpdated> toMonitor = new ArrayList<>(msg.operations().size());\n\n\t\t\t// transition the persistent state of some tasks to Launched\n\t\t\tfor (Protos.Offer.Operation op : msg.operations()) {\n\t\t\t\tif (op.getType() != Protos.Offer.Operation.Type.LAUNCH) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tfor (Protos.TaskInfo info : op.getLaunch().getTaskInfosList()) {\n\t\t\t\t\tMesosWorkerStore.Worker worker = workersInNew.remove(extractResourceID(info.getTaskId()));\n\t\t\t\t\tassert (worker != null);\n\n\t\t\t\t\tworker = worker.launchWorker(info.getSlaveId(), msg.hostname());\n\t\t\t\t\tworkerStore.putWorker(worker);\n\t\t\t\t\tworkersInLaunch.put(extractResourceID(worker.taskID()), worker);\n\n\t\t\t\t\tLOG.info(\"Launching Mesos task {} on host {}.\",\n\t\t\t\t\t\tworker.taskID().getValue(), worker.hostname().get());\n\n\t\t\t\t\ttoMonitor.add(new TaskMonitor.TaskGoalStateUpdated(extractGoalState(worker)));\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// tell the task router about the new plans\n\t\t\tfor (TaskMonitor.TaskGoalStateUpdated update : toMonitor) {\n\t\t\t\ttaskRouter.tell(update, self());\n\t\t\t}\n\n\t\t\t// send the acceptance message to Mesos\n\t\t\tschedulerDriver.acceptOffers(msg.offerIds(), msg.operations(), msg.filters());\n\t\t} catch (Exception ex) {\n\t\t\tfatalError(\"unable to accept offers\", ex);\n\t\t}\n\t}"
        ],
        [
            "MesosFlinkResourceManager::createActorProps(Class,Configuration,MesosConfiguration,MesosWorkerStore,LeaderRetrievalService,MesosTaskManagerParameters,ContainerSpecification,MesosArtifactResolver,Logger)",
            " 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748  \n 749 -\n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757 -\n 758 -\n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  \n 788  ",
            "\t/**\n\t * Creates the props needed to instantiate this actor.\n\t *\n\t * Rather than extracting and validating parameters in the constructor, this factory method takes\n\t * care of that. That way, errors occur synchronously, and are not swallowed simply in a\n\t * failed asynchronous attempt to start the actor.\n\n\t * @param actorClass\n\t *             The actor class, to allow overriding this actor with subclasses for testing.\n\t * @param flinkConfig\n\t *             The Flink configuration object.\n\t * @param taskManagerParameters\n\t *             The parameters for launching TaskManager containers.\n\t * @param taskManagerContainerSpec\n\t *             The container specification.\n\t * @param artifactResolver\n\t *             The artifact resolver to locate artifacts\n\t * @param log\n\t *             The logger to log to.\n\t *\n\t * @return The Props object to instantiate the MesosFlinkResourceManager actor.\n\t */\n\tpublic static Props createActorProps(Class<? extends MesosFlinkResourceManager> actorClass,\n\t\t\tConfiguration flinkConfig,\n\t\t\tMesosConfiguration mesosConfig,\n\t\t\tMesosWorkerStore workerStore,\n\t\t\tLeaderRetrievalService leaderRetrievalService,\n\t\t\tMesosTaskManagerParameters taskManagerParameters,\n\t\t\tContainerSpecification taskManagerContainerSpec,\n\t\t\tMesosArtifactResolver artifactResolver,\n\t\t\tLogger log)\n\t{\n\n\t\tfinal int numInitialTaskManagers = flinkConfig.getInteger(\n\t\t\tConfigConstants.MESOS_INITIAL_TASKS, 0);\n\t\tif (numInitialTaskManagers >= 0) {\n\t\t\tlog.info(\"Mesos framework to allocate {} initial tasks\",\n\t\t\t\tnumInitialTaskManagers);\n\t\t}\n\t\telse {\n\t\t\tthrow new IllegalConfigurationException(\"Invalid value for \" +\n\t\t\t\tConfigConstants.MESOS_INITIAL_TASKS + \", which must be at least zero.\");\n\t\t}\n\n\t\tfinal int maxFailedTasks = flinkConfig.getInteger(\n\t\t\tConfigConstants.MESOS_MAX_FAILED_TASKS, numInitialTaskManagers);\n\t\tif (maxFailedTasks >= 0) {\n\t\t\tlog.info(\"Mesos framework tolerates {} failed tasks before giving up\",\n\t\t\t\tmaxFailedTasks);\n\t\t}\n\n\t\treturn Props.create(actorClass,\n\t\t\tflinkConfig,\n\t\t\tmesosConfig,\n\t\t\tworkerStore,\n\t\t\tleaderRetrievalService,\n\t\t\ttaskManagerParameters,\n\t\t\ttaskManagerContainerSpec,\n\t\t\tartifactResolver,\n\t\t\tmaxFailedTasks,\n\t\t\tnumInitialTaskManagers);\n\t}",
            " 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  \n 741  \n 742  \n 743  \n 744  \n 745  \n 746  \n 747  \n 748 +\n 749 +\n 750  \n 751  \n 752  \n 753  \n 754  \n 755  \n 756  \n 757 +\n 758  \n 759  \n 760  \n 761  \n 762  \n 763  \n 764  \n 765  \n 766  \n 767  \n 768  \n 769  \n 770  \n 771  \n 772  \n 773  \n 774  \n 775  \n 776  \n 777  \n 778  \n 779  \n 780  \n 781  \n 782  \n 783  \n 784  \n 785  \n 786  \n 787  ",
            "\t/**\n\t * Creates the props needed to instantiate this actor.\n\t *\n\t * <p>Rather than extracting and validating parameters in the constructor, this factory method takes\n\t * care of that. That way, errors occur synchronously, and are not swallowed simply in a\n\t * failed asynchronous attempt to start the actor.\n\n\t * @param actorClass\n\t *             The actor class, to allow overriding this actor with subclasses for testing.\n\t * @param flinkConfig\n\t *             The Flink configuration object.\n\t * @param taskManagerParameters\n\t *             The parameters for launching TaskManager containers.\n\t * @param taskManagerContainerSpec\n\t *             The container specification.\n\t * @param artifactResolver\n\t *             The artifact resolver to locate artifacts\n\t * @param log\n\t *             The logger to log to.\n\t *\n\t * @return The Props object to instantiate the MesosFlinkResourceManager actor.\n\t */\n\tpublic static Props createActorProps(\n\t\t\tClass<? extends MesosFlinkResourceManager> actorClass,\n\t\t\tConfiguration flinkConfig,\n\t\t\tMesosConfiguration mesosConfig,\n\t\t\tMesosWorkerStore workerStore,\n\t\t\tLeaderRetrievalService leaderRetrievalService,\n\t\t\tMesosTaskManagerParameters taskManagerParameters,\n\t\t\tContainerSpecification taskManagerContainerSpec,\n\t\t\tMesosArtifactResolver artifactResolver,\n\t\t\tLogger log) {\n\n\t\tfinal int numInitialTaskManagers = flinkConfig.getInteger(\n\t\t\tConfigConstants.MESOS_INITIAL_TASKS, 0);\n\t\tif (numInitialTaskManagers >= 0) {\n\t\t\tlog.info(\"Mesos framework to allocate {} initial tasks\",\n\t\t\t\tnumInitialTaskManagers);\n\t\t}\n\t\telse {\n\t\t\tthrow new IllegalConfigurationException(\"Invalid value for \" +\n\t\t\t\tConfigConstants.MESOS_INITIAL_TASKS + \", which must be at least zero.\");\n\t\t}\n\n\t\tfinal int maxFailedTasks = flinkConfig.getInteger(\n\t\t\tConfigConstants.MESOS_MAX_FAILED_TASKS, numInitialTaskManagers);\n\t\tif (maxFailedTasks >= 0) {\n\t\t\tlog.info(\"Mesos framework tolerates {} failed tasks before giving up\",\n\t\t\t\tmaxFailedTasks);\n\t\t}\n\n\t\treturn Props.create(actorClass,\n\t\t\tflinkConfig,\n\t\t\tmesosConfig,\n\t\t\tworkerStore,\n\t\t\tleaderRetrievalService,\n\t\t\ttaskManagerParameters,\n\t\t\ttaskManagerContainerSpec,\n\t\t\tartifactResolver,\n\t\t\tmaxFailedTasks,\n\t\t\tnumInitialTaskManagers);\n\t}"
        ],
        [
            "MesosTaskManagerParameters::getTaskManagerHostname()",
            " 177  \n 178  \n 179  \n 180 -",
            "\t/**\n \t * Get the taskManager hostname.\n \t */\n\tpublic Option<String> getTaskManagerHostname() { return taskManagerHostname; }",
            " 178  \n 179  \n 180  \n 181 +\n 182 +\n 183 +",
            "\t/**\n\t * Get the taskManager hostname.\n\t */\n\tpublic Option<String> getTaskManagerHostname() {\n\t\treturn taskManagerHostname;\n\t}"
        ],
        [
            "MesosTaskManagerParameters::bootstrapCommand()",
            " 182  \n 183  \n 184  \n 185 -",
            "\t/**\n \t * Get the bootstrap command.\n \t */\n\tpublic Option<String> bootstrapCommand() { return bootstrapCommand;\t}\t",
            " 185  \n 186  \n 187  \n 188 +\n 189 +\n 190 +",
            "\t/**\n\t * Get the bootstrap command.\n\t */\n\tpublic Option<String> bootstrapCommand() {\n\t\treturn bootstrapCommand;\n\t}"
        ]
    ],
    "d4f73391708bdfe466a5c3c771bb02f0fc3e1d03": [
        [
            "RMQConnectionConfig::Builder::build()",
            " 426  \n 427  \n 428  \n 429  \n 430  \n 431  \n 432  \n 433  \n 434  \n 435  \n 436  \n 437 -\n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  ",
            "\t\t/**\n\t\t * The Builder method\n\t\t * If URI is NULL we use host, port, vHost, username, password combination\n\t\t * to initialize connection. using  {@link RMQConnectionConfig#RMQConnectionConfig(String, Integer, String, String, String,\n\t\t * Integer, Boolean, Boolean, Integer, Integer, Integer, Integer)}\n\t\t *\n\t\t * else URI will be used to initialize the client connection\n\t\t * {@link RMQConnectionConfig#RMQConnectionConfig(String, Integer, Boolean, Boolean, Integer, Integer, Integer, Integer)}\n\t\t * @return RMQConnectionConfig\n\t\t */\n\t\tpublic RMQConnectionConfig build(){\n\t\t\tif(this.uri != null) {\n\t\t\t\treturn new RMQConnectionConfig(this.uri, this.networkRecoveryInterval,\n\t\t\t\t\tthis.automaticRecovery, this.topologyRecovery, this.connectionTimeout, this.requestedChannelMax,\n\t\t\t\t\tthis.requestedFrameMax, this.requestedHeartbeat);\n\t\t\t} else {\n\t\t\t\treturn new RMQConnectionConfig(this.host, this.port, this.virtualHost, this.username, this.password,\n\t\t\t\t\tthis.networkRecoveryInterval, this.automaticRecovery, this.topologyRecovery,\n\t\t\t\t\tthis.connectionTimeout, this.requestedChannelMax, this.requestedFrameMax, this.requestedHeartbeat);\n\t\t\t}\n\t\t}",
            " 437  \n 438  \n 439  \n 440  \n 441  \n 442  \n 443  \n 444  \n 445  \n 446  \n 447  \n 448  \n 449 +\n 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  ",
            "\t\t/**\n\t\t * The Builder method.\n\t\t *\n\t\t * <p>If URI is NULL we use host, port, vHost, username, password combination\n\t\t * to initialize connection. using  {@link RMQConnectionConfig#RMQConnectionConfig(String, Integer, String, String, String,\n\t\t * Integer, Boolean, Boolean, Integer, Integer, Integer, Integer)}.\n\t\t *\n\t\t * <p>Otherwise the URI will be used to initialize the client connection\n\t\t * {@link RMQConnectionConfig#RMQConnectionConfig(String, Integer, Boolean, Boolean, Integer, Integer, Integer, Integer)}\n\t\t * @return RMQConnectionConfig\n\t\t */\n\t\tpublic RMQConnectionConfig build(){\n\t\t\tif (this.uri != null) {\n\t\t\t\treturn new RMQConnectionConfig(this.uri, this.networkRecoveryInterval,\n\t\t\t\t\tthis.automaticRecovery, this.topologyRecovery, this.connectionTimeout, this.requestedChannelMax,\n\t\t\t\t\tthis.requestedFrameMax, this.requestedHeartbeat);\n\t\t\t} else {\n\t\t\t\treturn new RMQConnectionConfig(this.host, this.port, this.virtualHost, this.username, this.password,\n\t\t\t\t\tthis.networkRecoveryInterval, this.automaticRecovery, this.topologyRecovery,\n\t\t\t\t\tthis.connectionTimeout, this.requestedChannelMax, this.requestedFrameMax, this.requestedHeartbeat);\n\t\t\t}\n\t\t}"
        ],
        [
            "RMQConnectionConfig::getConnectionFactory()",
            " 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237 -\n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  ",
            "\t/**\n\t *\n\t * @return Connection Factory for RMQ\n\t * @throws URISyntaxException, NoSuchAlgorithmException, KeyManagementException if Malformed URI has been passed\n\t */\n\tpublic ConnectionFactory getConnectionFactory() throws URISyntaxException,\n\t\tNoSuchAlgorithmException, KeyManagementException {\n\t\tConnectionFactory factory = new ConnectionFactory();\n\t\tif (this.uri != null && !this.uri.isEmpty()){\n\t\t\ttry {\n\t\t\t\tfactory.setUri(this.uri);\n\t\t\t} catch (URISyntaxException | NoSuchAlgorithmException | KeyManagementException e) {\n\t\t\t\tLOG.error(\"Failed to parse uri\", e);\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t} else {\n\t\t\tfactory.setHost(this.host);\n\t\t\tfactory.setPort(this.port);\n\t\t\tfactory.setVirtualHost(this.virtualHost);\n\t\t\tfactory.setUsername(this.username);\n\t\t\tfactory.setPassword(this.password);\n\t\t}\n\n\t\tif (this.automaticRecovery != null) {\n\t\t\tfactory.setAutomaticRecoveryEnabled(this.automaticRecovery);\n\t\t}\n\t\tif (this.connectionTimeout != null) {\n\t\t\tfactory.setConnectionTimeout(this.connectionTimeout);\n\t\t}\n\t\tif (this.networkRecoveryInterval != null) {\n\t\t\tfactory.setNetworkRecoveryInterval(this.networkRecoveryInterval);\n\t\t}\n\t\tif (this.requestedHeartbeat != null) {\n\t\t\tfactory.setRequestedHeartbeat(this.requestedHeartbeat);\n\t\t}\n\t\tif (this.topologyRecovery != null) {\n\t\t\tfactory.setTopologyRecoveryEnabled(this.topologyRecovery);\n\t\t}\n\t\tif (this.requestedChannelMax != null) {\n\t\t\tfactory.setRequestedChannelMax(this.requestedChannelMax);\n\t\t}\n\t\tif (this.requestedFrameMax != null) {\n\t\t\tfactory.setRequestedFrameMax(this.requestedFrameMax);\n\t\t}\n\n\t\treturn factory;\n\t}",
            " 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240 +\n 241  \n 242  \n 243 +\n 244 +\n 245 +\n 246 +\n 247 +\n 248 +\n 249 +\n 250 +\n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  ",
            "\t/**\n\t *\n\t * @return Connection Factory for RMQ\n\t * @throws URISyntaxException if Malformed URI has been passed\n\t * @throws NoSuchAlgorithmException if the ssl factory could not be created\n\t * @throws KeyManagementException if the ssl context could not be initialized\n\t */\n\tpublic ConnectionFactory getConnectionFactory() throws URISyntaxException,\n\t\tNoSuchAlgorithmException, KeyManagementException {\n\t\tConnectionFactory factory = new ConnectionFactory();\n\t\tif (this.uri != null && !this.uri.isEmpty()){\n\t\t\ttry {\n\t\t\t\tfactory.setUri(this.uri);\n\t\t\t} catch (URISyntaxException e) {\n\t\t\t\tLOG.error(\"Failed to parse uri\", e);\n\t\t\t\tthrow e;\n\t\t\t} catch (KeyManagementException e) {\n\t\t\t\t// this should never happen\n\t\t\t\tLOG.error(\"Failed to initialize ssl context.\", e);\n\t\t\t\tthrow e;\n\t\t\t} catch (NoSuchAlgorithmException e) {\n\t\t\t\t// this should never happen\n\t\t\t\tLOG.error(\"Failed to setup ssl factory.\", e);\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t} else {\n\t\t\tfactory.setHost(this.host);\n\t\t\tfactory.setPort(this.port);\n\t\t\tfactory.setVirtualHost(this.virtualHost);\n\t\t\tfactory.setUsername(this.username);\n\t\t\tfactory.setPassword(this.password);\n\t\t}\n\n\t\tif (this.automaticRecovery != null) {\n\t\t\tfactory.setAutomaticRecoveryEnabled(this.automaticRecovery);\n\t\t}\n\t\tif (this.connectionTimeout != null) {\n\t\t\tfactory.setConnectionTimeout(this.connectionTimeout);\n\t\t}\n\t\tif (this.networkRecoveryInterval != null) {\n\t\t\tfactory.setNetworkRecoveryInterval(this.networkRecoveryInterval);\n\t\t}\n\t\tif (this.requestedHeartbeat != null) {\n\t\t\tfactory.setRequestedHeartbeat(this.requestedHeartbeat);\n\t\t}\n\t\tif (this.topologyRecovery != null) {\n\t\t\tfactory.setTopologyRecoveryEnabled(this.topologyRecovery);\n\t\t}\n\t\tif (this.requestedChannelMax != null) {\n\t\t\tfactory.setRequestedChannelMax(this.requestedChannelMax);\n\t\t}\n\t\tif (this.requestedFrameMax != null) {\n\t\t\tfactory.setRequestedFrameMax(this.requestedFrameMax);\n\t\t}\n\n\t\treturn factory;\n\t}"
        ],
        [
            "RMQSource::RMQSource(RMQConnectionConfig,String,boolean,DeserializationSchema)",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "\t/**\n\t * Creates a new RabbitMQ source. For exactly-once, you must set the correlation ids of messages\n\t * at the producer. The correlation id must be unique. Otherwise the behavior of the source is\n\t * undefined. In doubt, set {@param usesCorrelationId} to false. When correlation ids are not\n\t * used, this source has at-least-once processing semantics when checkpointing is enabled.\n\t * @param rmqConnectionConfig The RabbiMQ connection configuration {@link RMQConnectionConfig}.\n\t * @param queueName The queue to receive messages from.\n\t * @param usesCorrelationId Whether the messages received are supplied with a <b>unique</b>\n\t *                          id to deduplicate messages (in case of failed acknowledgments).\n\t *                          Only used when checkpointing is enabled.\n\t * @param deserializationSchema A {@link DeserializationSchema} for turning the bytes received\n\t *                              into Java objects.\n\t */\n\tpublic RMQSource(RMQConnectionConfig rmqConnectionConfig,\n\t\t\t\t\tString queueName, boolean usesCorrelationId,DeserializationSchema<OUT> deserializationSchema) {\n\t\tsuper(String.class);\n\t\tthis.rmqConnectionConfig = rmqConnectionConfig;\n\t\tthis.queueName = queueName;\n\t\tthis.usesCorrelationId = usesCorrelationId;\n\t\tthis.schema = deserializationSchema;\n\t}",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119 +\n 120  \n 121  \n 122  \n 123  \n 124  \n 125  ",
            "\t/**\n\t * Creates a new RabbitMQ source. For exactly-once, you must set the correlation ids of messages\n\t * at the producer. The correlation id must be unique. Otherwise the behavior of the source is\n\t * undefined. If in doubt, set usesCorrelationId to false. When correlation ids are not\n\t * used, this source has at-least-once processing semantics when checkpointing is enabled.\n\t * @param rmqConnectionConfig The RabbiMQ connection configuration {@link RMQConnectionConfig}.\n\t * @param queueName The queue to receive messages from.\n\t * @param usesCorrelationId Whether the messages received are supplied with a <b>unique</b>\n\t *                          id to deduplicate messages (in case of failed acknowledgments).\n\t *                          Only used when checkpointing is enabled.\n\t * @param deserializationSchema A {@link DeserializationSchema} for turning the bytes received\n\t *                              into Java objects.\n\t */\n\tpublic RMQSource(RMQConnectionConfig rmqConnectionConfig,\n\t\t\t\t\tString queueName, boolean usesCorrelationId, DeserializationSchema<OUT> deserializationSchema) {\n\t\tsuper(String.class);\n\t\tthis.rmqConnectionConfig = rmqConnectionConfig;\n\t\tthis.queueName = queueName;\n\t\tthis.usesCorrelationId = usesCorrelationId;\n\t\tthis.schema = deserializationSchema;\n\t}"
        ]
    ],
    "1ceb89a979d560944e1099fa4700e46c09a79484": [
        [
            "NettyClient::connect(InetSocketAddress)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200 -\n 201  \n 202  \n 203 -\n 204  \n 205  \n 206  \n 207 -\n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  ",
            "\tChannelFuture connect(final InetSocketAddress serverSocketAddress) {\n\t\tcheckState(bootstrap != null, \"Client has not been initialized yet.\");\n\n\t\t// --------------------------------------------------------------------\n\t\t// Child channel pipeline for accepted connections\n\t\t// --------------------------------------------------------------------\n\n\t\tbootstrap.handler(new ChannelInitializer<SocketChannel>() {\n\t\t\t@Override\n\t\t\tpublic void initChannel(SocketChannel channel) throws Exception {\n\n\t\t\t\t// SSL handler should be added first in the pipeline\n\t\t\t\tif (clientSSLContext != null) {\n\t\t\t\t\tSSLEngine sslEngine = clientSSLContext.createSSLEngine(\n\t\t\t\t\t\tserverSocketAddress.getAddress().getHostAddress(),\n\t\t\t\t\t\tserverSocketAddress.getPort());\n\t\t\t\t\tsslEngine.setUseClientMode(true);\n\n\t\t\t\t\t// Enable hostname verification for remote SSL connections\n\t\t\t\t\tif (!serverSocketAddress.getAddress().isLoopbackAddress()) {\n\t\t\t\t\t\tSSLParameters newSSLParameters = sslEngine.getSSLParameters();\n\t\t\t\t\t\tconfig.setSSLVerifyHostname(newSSLParameters);\n\t\t\t\t\t\tsslEngine.setSSLParameters(newSSLParameters);\n\t\t\t\t\t}\n\n\t\t\t\t\tchannel.pipeline().addLast(\"ssl\", new SslHandler(sslEngine));\n\t\t\t\t}\n\t\t\t\tchannel.pipeline().addLast(protocol.getClientChannelHandlers());\n\t\t\t}\n\t\t});\n\n\t\ttry {\n\t\t\treturn bootstrap.connect(serverSocketAddress);\n\t\t}\n\t\tcatch (io.netty.channel.ChannelException e) {\n\t\t\tif ( (e.getCause() instanceof java.net.SocketException &&\n\t\t\t\t\te.getCause().getMessage().equals(\"Too many open files\")) ||\n\t\t\t\t(e.getCause() instanceof io.netty.channel.ChannelException &&\n\t\t\t\t\t\te.getCause().getCause() instanceof java.net.SocketException &&\n\t\t\t\t\t\te.getCause().getCause().getMessage().equals(\"Too many open files\")))\n\t\t\t{\n\t\t\t\tthrow new io.netty.channel.ChannelException(\n\t\t\t\t\t\t\"The operating system does not offer enough file handles to open the network connection. \" +\n\t\t\t\t\t\t\t\t\"Please increase the number of of available file handles.\", e.getCause());\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}",
            " 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202 +\n 203  \n 204  \n 205 +\n 206  \n 207  \n 208  \n 209 +\n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  ",
            "\tChannelFuture connect(final InetSocketAddress serverSocketAddress) {\n\t\tcheckState(bootstrap != null, \"Client has not been initialized yet.\");\n\n\t\t// --------------------------------------------------------------------\n\t\t// Child channel pipeline for accepted connections\n\t\t// --------------------------------------------------------------------\n\n\t\tbootstrap.handler(new ChannelInitializer<SocketChannel>() {\n\t\t\t@Override\n\t\t\tpublic void initChannel(SocketChannel channel) throws Exception {\n\n\t\t\t\t// SSL handler should be added first in the pipeline\n\t\t\t\tif (clientSSLContext != null) {\n\t\t\t\t\tSSLEngine sslEngine = clientSSLContext.createSSLEngine(\n\t\t\t\t\t\tserverSocketAddress.getAddress().getHostAddress(),\n\t\t\t\t\t\tserverSocketAddress.getPort());\n\t\t\t\t\tsslEngine.setUseClientMode(true);\n\n\t\t\t\t\t// Enable hostname verification for remote SSL connections\n\t\t\t\t\tif (!serverSocketAddress.getAddress().isLoopbackAddress()) {\n\t\t\t\t\t\tSSLParameters newSSLParameters = sslEngine.getSSLParameters();\n\t\t\t\t\t\tconfig.setSSLVerifyHostname(newSSLParameters);\n\t\t\t\t\t\tsslEngine.setSSLParameters(newSSLParameters);\n\t\t\t\t\t}\n\n\t\t\t\t\tchannel.pipeline().addLast(\"ssl\", new SslHandler(sslEngine));\n\t\t\t\t}\n\t\t\t\tchannel.pipeline().addLast(protocol.getClientChannelHandlers());\n\t\t\t}\n\t\t});\n\n\t\ttry {\n\t\t\treturn bootstrap.connect(serverSocketAddress);\n\t\t}\n\t\tcatch (ChannelException e) {\n\t\t\tif ( (e.getCause() instanceof java.net.SocketException &&\n\t\t\t\t\te.getCause().getMessage().equals(\"Too many open files\")) ||\n\t\t\t\t(e.getCause() instanceof ChannelException &&\n\t\t\t\t\t\te.getCause().getCause() instanceof java.net.SocketException &&\n\t\t\t\t\t\te.getCause().getCause().getMessage().equals(\"Too many open files\")))\n\t\t\t{\n\t\t\t\tthrow new ChannelException(\n\t\t\t\t\t\t\"The operating system does not offer enough file handles to open the network connection. \" +\n\t\t\t\t\t\t\t\t\"Please increase the number of of available file handles.\", e.getCause());\n\t\t\t}\n\t\t\telse {\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}"
        ]
    ],
    "9d9cdcbad6ccf353a1252866f6a56ac505bfaa95": [
        [
            "TwoPhaseCommitSinkFunction::initializeState(FunctionInitializationContext)",
            " 248  \n 249 -\n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "\t@Override\n\tpublic final void initializeState(FunctionInitializationContext context) throws Exception {\n\t\t// when we are restoring state with pendingCommitTransactions, we don't really know whether the\n\t\t// transactions were already committed, or whether there was a failure between\n\t\t// completing the checkpoint on the master, and notifying the writer here.\n\n\t\t// (the common case is actually that is was already committed, the window\n\t\t// between the commit on the master and the notification here is very small)\n\n\t\t// it is possible to not have any transactions at all if there was a failure before\n\t\t// the first completed checkpoint, or in case of a scale-out event, where some of the\n\t\t// new task do not have and transactions assigned to check)\n\n\t\t// we can have more than one transaction to check in case of a scale-in event, or\n\t\t// for the reasons discussed in the 'notifyCheckpointComplete()' method.\n\n\t\tstate = context.getOperatorStateStore().getListState(stateDescriptor);\n\n\t\tif (context.isRestored()) {\n\t\t\tLOG.info(\"{} - restoring state\", name());\n\n\t\t\tfor (State<TXN, CONTEXT> operatorState : state.get()) {\n\t\t\t\tuserContext = operatorState.getContext();\n\t\t\t\tList<TXN> recoveredTransactions = operatorState.getPendingCommitTransactions();\n\t\t\t\tfor (TXN recoveredTransaction : recoveredTransactions) {\n\t\t\t\t\t// If this fails, there is actually a data loss\n\t\t\t\t\trecoverAndCommit(recoveredTransaction);\n\t\t\t\t\tLOG.info(\"{} committed recovered transaction {}\", name(), recoveredTransaction);\n\t\t\t\t}\n\n\t\t\t\trecoverAndAbort(operatorState.getPendingTransaction());\n\t\t\t\tLOG.info(\"{} aborted recovered transaction {}\", name(), operatorState.getPendingTransaction());\n\n\t\t\t\tif (userContext.isPresent()) {\n\t\t\t\t\tfinishRecoveringContext();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// if in restore we didn't get any userContext or we are initializing from scratch\n\t\tif (userContext == null) {\n\t\t\tLOG.info(\"{} - no state to restore {}\", name());\n\n\t\t\tuserContext = initializeUserContext();\n\t\t}\n\t\tthis.pendingCommitTransactions.clear();\n\n\t\tcurrentTransaction = beginTransaction();\n\t\tLOG.debug(\"{} - started new transaction '{}'\", name(), currentTransaction);\n\t}",
            " 248  \n 249 +\n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  ",
            "\t@Override\n\tpublic void initializeState(FunctionInitializationContext context) throws Exception {\n\t\t// when we are restoring state with pendingCommitTransactions, we don't really know whether the\n\t\t// transactions were already committed, or whether there was a failure between\n\t\t// completing the checkpoint on the master, and notifying the writer here.\n\n\t\t// (the common case is actually that is was already committed, the window\n\t\t// between the commit on the master and the notification here is very small)\n\n\t\t// it is possible to not have any transactions at all if there was a failure before\n\t\t// the first completed checkpoint, or in case of a scale-out event, where some of the\n\t\t// new task do not have and transactions assigned to check)\n\n\t\t// we can have more than one transaction to check in case of a scale-in event, or\n\t\t// for the reasons discussed in the 'notifyCheckpointComplete()' method.\n\n\t\tstate = context.getOperatorStateStore().getListState(stateDescriptor);\n\n\t\tif (context.isRestored()) {\n\t\t\tLOG.info(\"{} - restoring state\", name());\n\n\t\t\tfor (State<TXN, CONTEXT> operatorState : state.get()) {\n\t\t\t\tuserContext = operatorState.getContext();\n\t\t\t\tList<TXN> recoveredTransactions = operatorState.getPendingCommitTransactions();\n\t\t\t\tfor (TXN recoveredTransaction : recoveredTransactions) {\n\t\t\t\t\t// If this fails, there is actually a data loss\n\t\t\t\t\trecoverAndCommit(recoveredTransaction);\n\t\t\t\t\tLOG.info(\"{} committed recovered transaction {}\", name(), recoveredTransaction);\n\t\t\t\t}\n\n\t\t\t\trecoverAndAbort(operatorState.getPendingTransaction());\n\t\t\t\tLOG.info(\"{} aborted recovered transaction {}\", name(), operatorState.getPendingTransaction());\n\n\t\t\t\tif (userContext.isPresent()) {\n\t\t\t\t\tfinishRecoveringContext();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// if in restore we didn't get any userContext or we are initializing from scratch\n\t\tif (userContext == null) {\n\t\t\tLOG.info(\"{} - no state to restore {}\", name());\n\n\t\t\tuserContext = initializeUserContext();\n\t\t}\n\t\tthis.pendingCommitTransactions.clear();\n\n\t\tcurrentTransaction = beginTransaction();\n\t\tLOG.debug(\"{} - started new transaction '{}'\", name(), currentTransaction);\n\t}"
        ],
        [
            "TwoPhaseCommitSinkFunction::snapshotState(FunctionSnapshotContext)",
            " 224  \n 225 -\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "\t@Override\n\tpublic final void snapshotState(FunctionSnapshotContext context) throws Exception {\n\t\t// this is like the pre-commit of a 2-phase-commit transaction\n\t\t// we are ready to commit and remember the transaction\n\n\t\tcheckState(currentTransaction != null, \"bug: no transaction object when performing state snapshot\");\n\n\t\tlong checkpointId = context.getCheckpointId();\n\t\tLOG.debug(\"{} - checkpoint {} triggered, flushing transaction '{}'\", name(), context.getCheckpointId(), currentTransaction);\n\n\t\tpreCommit(currentTransaction);\n\t\tpendingCommitTransactions.put(checkpointId, currentTransaction);\n\t\tLOG.debug(\"{} - stored pending transactions {}\", name(), pendingCommitTransactions);\n\n\t\tcurrentTransaction = beginTransaction();\n\t\tLOG.debug(\"{} - started new transaction '{}'\", name(), currentTransaction);\n\n\t\tstate.clear();\n\t\tstate.add(new State<>(\n\t\t\tthis.currentTransaction,\n\t\t\tnew ArrayList<>(pendingCommitTransactions.values()),\n\t\t\tuserContext));\n\t}",
            " 224  \n 225 +\n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  ",
            "\t@Override\n\tpublic void snapshotState(FunctionSnapshotContext context) throws Exception {\n\t\t// this is like the pre-commit of a 2-phase-commit transaction\n\t\t// we are ready to commit and remember the transaction\n\n\t\tcheckState(currentTransaction != null, \"bug: no transaction object when performing state snapshot\");\n\n\t\tlong checkpointId = context.getCheckpointId();\n\t\tLOG.debug(\"{} - checkpoint {} triggered, flushing transaction '{}'\", name(), context.getCheckpointId(), currentTransaction);\n\n\t\tpreCommit(currentTransaction);\n\t\tpendingCommitTransactions.put(checkpointId, currentTransaction);\n\t\tLOG.debug(\"{} - stored pending transactions {}\", name(), pendingCommitTransactions);\n\n\t\tcurrentTransaction = beginTransaction();\n\t\tLOG.debug(\"{} - started new transaction '{}'\", name(), currentTransaction);\n\n\t\tstate.clear();\n\t\tstate.add(new State<>(\n\t\t\tthis.currentTransaction,\n\t\t\tnew ArrayList<>(pendingCommitTransactions.values()),\n\t\t\tuserContext));\n\t}"
        ]
    ],
    "867c0124e2959ea3c90dab13cc12ba43c2eb0f64": [
        [
            "OneInputStreamOperatorTestHarness::OneInputStreamOperatorTestHarness(OneInputStreamOperator,int,int,int)",
            "  65  \n  66  \n  67  \n  68 -\n  69  \n  70 -\n  71  \n  72  \n  73  ",
            "\tpublic OneInputStreamOperatorTestHarness(\n\t\t\tOneInputStreamOperator<IN, OUT> operator,\n\t\t\tint maxParallelism,\n\t\t\tint numTubtasks,\n\t\t\tint subtaskIndex) throws Exception {\n\t\tsuper(operator, maxParallelism, numTubtasks, subtaskIndex);\n\n\t\tthis.oneInputOperator = operator;\n\t}",
            "  65  \n  66  \n  67  \n  68 +\n  69  \n  70 +\n  71  \n  72  \n  73  ",
            "\tpublic OneInputStreamOperatorTestHarness(\n\t\t\tOneInputStreamOperator<IN, OUT> operator,\n\t\t\tint maxParallelism,\n\t\t\tint parallelism,\n\t\t\tint subtaskIndex) throws Exception {\n\t\tsuper(operator, maxParallelism, parallelism, subtaskIndex);\n\n\t\tthis.oneInputOperator = operator;\n\t}"
        ],
        [
            "AbstractStreamOperatorTestHarness::AbstractStreamOperatorTestHarness(StreamOperator,int,int,int)",
            " 120  \n 121  \n 122  \n 123 -\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 -\n 137  \n 138  ",
            "\tpublic AbstractStreamOperatorTestHarness(\n\t\tStreamOperator<OUT> operator,\n\t\tint maxParallelism,\n\t\tint numSubtasks,\n\t\tint subtaskIndex) throws Exception {\n\n\t\tthis(\n\t\t\toperator,\n\t\t\tnew MockEnvironment(\n\t\t\t\t\"MockTask\",\n\t\t\t\t3 * 1024 * 1024,\n\t\t\t\tnew MockInputSplitProvider(),\n\t\t\t\t1024,\n\t\t\t\tnew Configuration(),\n\t\t\t\tnew ExecutionConfig(),\n\t\t\t\tmaxParallelism,\n\t\t\t\tnumSubtasks,\n\t\t\t\tsubtaskIndex));\n\t}",
            " 120  \n 121  \n 122  \n 123 +\n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136 +\n 137  \n 138  ",
            "\tpublic AbstractStreamOperatorTestHarness(\n\t\tStreamOperator<OUT> operator,\n\t\tint maxParallelism,\n\t\tint parallelism,\n\t\tint subtaskIndex) throws Exception {\n\n\t\tthis(\n\t\t\toperator,\n\t\t\tnew MockEnvironment(\n\t\t\t\t\"MockTask\",\n\t\t\t\t3 * 1024 * 1024,\n\t\t\t\tnew MockInputSplitProvider(),\n\t\t\t\t1024,\n\t\t\t\tnew Configuration(),\n\t\t\t\tnew ExecutionConfig(),\n\t\t\t\tmaxParallelism,\n\t\t\t\tparallelism,\n\t\t\t\tsubtaskIndex));\n\t}"
        ]
    ],
    "d53a722e769e8ff6009d53208bf6702ec3e4a6f5": [
        [
            "AWSUtil::createKinesisClient(Properties)",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49 -\n  50  \n  51 -\n  52 -\n  53 -\n  54  \n  55  \n  56 -\n  57 -\n  58  \n  59 -\n  60  \n  61 -\n  62  \n  63 -\n  64  ",
            "\t/**\n\t * Creates an Amazon Kinesis Client.\n\t * @param configProps configuration properties containing the access key, secret key, and region\n\t * @return a new Amazon Kinesis Client\n\t */\n\tpublic static AmazonKinesisClient createKinesisClient(Properties configProps) {\n\t\t// set a Flink-specific user agent\n\t\tClientConfiguration awsClientConfig = new ClientConfigurationFactory().getConfig();\n\t\tawsClientConfig.setUserAgent(\"Apache Flink \" + EnvironmentInformation.getVersion() +\n\t\t\t\" (\" + EnvironmentInformation.getRevisionInformation().commitId + \") Kinesis Connector\");\n\n\t\t// utilize automatic refreshment of credentials by directly passing the AWSCredentialsProvider\n\t\tAmazonKinesisClient client = new AmazonKinesisClient(\n\t\t\tAWSUtil.getCredentialsProvider(configProps), awsClientConfig);\n\n\t\tclient.setRegion(Region.getRegion(Regions.fromName(configProps.getProperty(AWSConfigConstants.AWS_REGION))));\n\t\tif (configProps.containsKey(AWSConfigConstants.AWS_ENDPOINT)) {\n\t\t\tclient.setEndpoint(configProps.getProperty(AWSConfigConstants.AWS_ENDPOINT));\n\t\t}\n\t\treturn client;\n\t}",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52 +\n  53  \n  54 +\n  55 +\n  56 +\n  57 +\n  58  \n  59  \n  60 +\n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66 +\n  67 +\n  68 +\n  69 +\n  70  \n  71 +\n  72  ",
            "\t/**\n\t * Creates an AmazonKinesis client.\n\t * @param configProps configuration properties containing the access key, secret key, and region\n\t * @return a new AmazonKinesis client\n\t */\n\tpublic static AmazonKinesis createKinesisClient(Properties configProps) {\n\t\t// set a Flink-specific user agent\n\t\tClientConfiguration awsClientConfig = new ClientConfigurationFactory().getConfig()\n\t\t\t\t.withUserAgentPrefix(String.format(USER_AGENT_FORMAT,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tEnvironmentInformation.getVersion(),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\tEnvironmentInformation.getRevisionInformation().commitId));\n\n\t\t// utilize automatic refreshment of credentials by directly passing the AWSCredentialsProvider\n\t\tAmazonKinesisClientBuilder builder = AmazonKinesisClientBuilder.standard()\n\t\t\t\t.withCredentials(AWSUtil.getCredentialsProvider(configProps))\n\t\t\t\t.withClientConfiguration(awsClientConfig)\n\t\t\t\t.withRegion(Regions.fromName(configProps.getProperty(AWSConfigConstants.AWS_REGION)));\n\n\t\tif (configProps.containsKey(AWSConfigConstants.AWS_ENDPOINT)) {\n\t\t\t// Set signingRegion as null, to facilitate mocking Kinesis for local tests\n\t\t\tbuilder.withEndpointConfiguration(new AwsClientBuilder.EndpointConfiguration(\n\t\t\t\t\t\t\t\t\t\t\t\t\tconfigProps.getProperty(AWSConfigConstants.AWS_ENDPOINT),\n\t\t\t\t\t\t\t\t\t\t\t\t\tnull));\n\t\t}\n\t\treturn builder.build();\n\t}"
        ],
        [
            "ManualExactlyOnceWithStreamReshardingTest::main(String)",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 -\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tfinal ParameterTool pt = ParameterTool.fromArgs(args);\n\t\tLOG.info(\"Starting exactly once with stream resharding test\");\n\n\t\tfinal String streamName = \"flink-test-\" + UUID.randomUUID().toString();\n\t\tfinal String accessKey = pt.getRequired(\"accessKey\");\n\t\tfinal String secretKey = pt.getRequired(\"secretKey\");\n\t\tfinal String region = pt.getRequired(\"region\");\n\n\t\tfinal Properties configProps = new Properties();\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_ACCESS_KEY_ID, accessKey);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_SECRET_ACCESS_KEY, secretKey);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_REGION, region);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.SHARD_DISCOVERY_INTERVAL_MILLIS, \"0\");\n\t\tfinal AmazonKinesisClient client = AWSUtil.createKinesisClient(configProps);\n\n\t\t// the stream is first created with 1 shard\n\t\tclient.createStream(streamName, 1);\n\n\t\t// wait until stream has been created\n\t\tDescribeStreamResult status = client.describeStream(streamName);\n\t\tLOG.info(\"status {}\", status);\n\t\twhile (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\")) {\n\t\t\tstatus = client.describeStream(streamName);\n\t\t\tLOG.info(\"Status of stream {}\", status);\n\t\t\tThread.sleep(1000);\n\t\t}\n\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tLocalFlinkMiniCluster flink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\n\t\tfinal int flinkPort = flink.getLeaderRPCPort();\n\n\t\ttry {\n\t\t\t// we have to use a manual generator here instead of the FlinkKinesisProducer\n\t\t\t// because the FlinkKinesisProducer currently has a problem where records will be resent to a shard\n\t\t\t// when resharding happens; this affects the consumer exactly-once validation test and will never pass\n\t\t\tfinal AtomicReference<Throwable> producerError = new AtomicReference<>();\n\t\t\tRunnable manualGenerate = new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tAmazonKinesisClient client = AWSUtil.createKinesisClient(configProps);\n\t\t\t\t\tint count = 0;\n\t\t\t\t\tfinal int batchSize = 30;\n\t\t\t\t\twhile (true) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\n\t\t\t\t\t\t\tSet<PutRecordsRequestEntry> batch = new HashSet<>();\n\t\t\t\t\t\t\tfor (int i = count; i < count + batchSize; i++) {\n\t\t\t\t\t\t\t\tif (i >= TOTAL_EVENT_COUNT) {\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tbatch.add(\n\t\t\t\t\t\t\t\t\tnew PutRecordsRequestEntry()\n\t\t\t\t\t\t\t\t\t\t.withData(ByteBuffer.wrap(((i) + \"-\" + RandomStringUtils.randomAlphabetic(12)).getBytes(ConfigConstants.DEFAULT_CHARSET)))\n\t\t\t\t\t\t\t\t\t\t.withPartitionKey(UUID.randomUUID().toString()));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcount += batchSize;\n\n\t\t\t\t\t\t\tPutRecordsResult result = client.putRecords(new PutRecordsRequest().withStreamName(streamName).withRecords(batch));\n\n\t\t\t\t\t\t\t// the putRecords() operation may have failing records; to keep this test simple\n\t\t\t\t\t\t\t// instead of retrying on failed records, we simply pass on a runtime exception\n\t\t\t\t\t\t\t// and let this test fail\n\t\t\t\t\t\t\tif (result.getFailedRecordCount() > 0) {\n\t\t\t\t\t\t\t\tproducerError.set(new RuntimeException(\"The producer has failed records in one of the put batch attempts.\"));\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (count >= TOTAL_EVENT_COUNT) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\t\tproducerError.set(e);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\tThread producerThread = new Thread(manualGenerate);\n\t\t\tproducerThread.start();\n\n\t\t\tfinal AtomicReference<Throwable> consumerError = new AtomicReference<>();\n\t\t\tThread consumerThread = ExactlyOnceValidatingConsumerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 10000, 2, 500, 500,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tconsumerError, flinkPort, flinkConfig);\n\t\t\tconsumerThread.start();\n\n\t\t\t// reshard the Kinesis stream while the producer / and consumers are running\n\t\t\tRunnable splitShard = new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// first, split shard in the middle of the hash range\n\t\t\t\t\t\tThread.sleep(5000);\n\t\t\t\t\t\tLOG.info(\"Splitting shard ...\");\n\t\t\t\t\t\tclient.splitShard(\n\t\t\t\t\t\t\tstreamName,\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(0),\n\t\t\t\t\t\t\t\"170141183460469231731687303715884105727\");\n\n\t\t\t\t\t\t// wait until the split shard operation finishes updating ...\n\t\t\t\t\t\tDescribeStreamResult status;\n\t\t\t\t\t\tRandom rand = new Random();\n\t\t\t\t\t\tdo {\n\t\t\t\t\t\t\tstatus = null;\n\t\t\t\t\t\t\twhile (status == null) {\n\t\t\t\t\t\t\t\t// retry until we get status\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tstatus = client.describeStream(streamName);\n\t\t\t\t\t\t\t\t} catch (LimitExceededException lee) {\n\t\t\t\t\t\t\t\t\tLOG.warn(\"LimitExceededException while describing stream ... retrying ...\");\n\t\t\t\t\t\t\t\t\tThread.sleep(rand.nextInt(1200));\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} while (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\"));\n\n\t\t\t\t\t\t// then merge again\n\t\t\t\t\t\tThread.sleep(7000);\n\t\t\t\t\t\tLOG.info(\"Merging shards ...\");\n\t\t\t\t\t\tclient.mergeShards(\n\t\t\t\t\t\t\tstreamName,\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(1),\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(2));\n\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t//\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\tThread splitShardThread = new Thread(splitShard);\n\t\t\tsplitShardThread.start();\n\n\t\t\tboolean deadlinePassed = false;\n\t\t\tlong deadline = System.currentTimeMillis() + (1000 * 5 * 60); // wait at most for five minutes\n\t\t\t// wait until both producer and consumer finishes, or an unexpected error is thrown\n\t\t\twhile ((consumerThread.isAlive() || producerThread.isAlive()) &&\n\t\t\t\t(producerError.get() == null && consumerError.get() == null)) {\n\t\t\t\tThread.sleep(1000);\n\t\t\t\tif (System.currentTimeMillis() >= deadline) {\n\t\t\t\t\tLOG.warn(\"Deadline passed\");\n\t\t\t\t\tdeadlinePassed = true;\n\t\t\t\t\tbreak; // enough waiting\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (producerThread.isAlive()) {\n\t\t\t\tproducerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (consumerThread.isAlive()) {\n\t\t\t\tconsumerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (producerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Producer failed\", producerError.get());\n\n\t\t\t}\n\n\t\t\tif (consumerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Consumer failed\", consumerError.get());\n\t\t\t}\n\n\t\t\tif (!deadlinePassed) {\n\t\t\t\tLOG.info(\"+++ TEST passed! +++\");\n\t\t\t} else {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t}\n\n\t\t} finally {\n\t\t\tclient.deleteStream(streamName);\n\t\t\tclient.shutdown();\n\n\t\t\t// stopping flink\n\t\t\tflink.stop();\n\t\t}\n\t}",
            "  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tfinal ParameterTool pt = ParameterTool.fromArgs(args);\n\t\tLOG.info(\"Starting exactly once with stream resharding test\");\n\n\t\tfinal String streamName = \"flink-test-\" + UUID.randomUUID().toString();\n\t\tfinal String accessKey = pt.getRequired(\"accessKey\");\n\t\tfinal String secretKey = pt.getRequired(\"secretKey\");\n\t\tfinal String region = pt.getRequired(\"region\");\n\n\t\tfinal Properties configProps = new Properties();\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_ACCESS_KEY_ID, accessKey);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_SECRET_ACCESS_KEY, secretKey);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.AWS_REGION, region);\n\t\tconfigProps.setProperty(ConsumerConfigConstants.SHARD_DISCOVERY_INTERVAL_MILLIS, \"0\");\n\t\tfinal AmazonKinesis client = AWSUtil.createKinesisClient(configProps);\n\n\t\t// the stream is first created with 1 shard\n\t\tclient.createStream(streamName, 1);\n\n\t\t// wait until stream has been created\n\t\tDescribeStreamResult status = client.describeStream(streamName);\n\t\tLOG.info(\"status {}\", status);\n\t\twhile (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\")) {\n\t\t\tstatus = client.describeStream(streamName);\n\t\t\tLOG.info(\"Status of stream {}\", status);\n\t\t\tThread.sleep(1000);\n\t\t}\n\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tLocalFlinkMiniCluster flink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\n\t\tfinal int flinkPort = flink.getLeaderRPCPort();\n\n\t\ttry {\n\t\t\t// we have to use a manual generator here instead of the FlinkKinesisProducer\n\t\t\t// because the FlinkKinesisProducer currently has a problem where records will be resent to a shard\n\t\t\t// when resharding happens; this affects the consumer exactly-once validation test and will never pass\n\t\t\tfinal AtomicReference<Throwable> producerError = new AtomicReference<>();\n\t\t\tRunnable manualGenerate = new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\tAmazonKinesis client = AWSUtil.createKinesisClient(configProps);\n\t\t\t\t\tint count = 0;\n\t\t\t\t\tfinal int batchSize = 30;\n\t\t\t\t\twhile (true) {\n\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\tThread.sleep(10);\n\n\t\t\t\t\t\t\tSet<PutRecordsRequestEntry> batch = new HashSet<>();\n\t\t\t\t\t\t\tfor (int i = count; i < count + batchSize; i++) {\n\t\t\t\t\t\t\t\tif (i >= TOTAL_EVENT_COUNT) {\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tbatch.add(\n\t\t\t\t\t\t\t\t\tnew PutRecordsRequestEntry()\n\t\t\t\t\t\t\t\t\t\t.withData(ByteBuffer.wrap(((i) + \"-\" + RandomStringUtils.randomAlphabetic(12)).getBytes(ConfigConstants.DEFAULT_CHARSET)))\n\t\t\t\t\t\t\t\t\t\t.withPartitionKey(UUID.randomUUID().toString()));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tcount += batchSize;\n\n\t\t\t\t\t\t\tPutRecordsResult result = client.putRecords(new PutRecordsRequest().withStreamName(streamName).withRecords(batch));\n\n\t\t\t\t\t\t\t// the putRecords() operation may have failing records; to keep this test simple\n\t\t\t\t\t\t\t// instead of retrying on failed records, we simply pass on a runtime exception\n\t\t\t\t\t\t\t// and let this test fail\n\t\t\t\t\t\t\tif (result.getFailedRecordCount() > 0) {\n\t\t\t\t\t\t\t\tproducerError.set(new RuntimeException(\"The producer has failed records in one of the put batch attempts.\"));\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\tif (count >= TOTAL_EVENT_COUNT) {\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\t\tproducerError.set(e);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\tThread producerThread = new Thread(manualGenerate);\n\t\t\tproducerThread.start();\n\n\t\t\tfinal AtomicReference<Throwable> consumerError = new AtomicReference<>();\n\t\t\tThread consumerThread = ExactlyOnceValidatingConsumerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 10000, 2, 500, 500,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tconsumerError, flinkPort, flinkConfig);\n\t\t\tconsumerThread.start();\n\n\t\t\t// reshard the Kinesis stream while the producer / and consumers are running\n\t\t\tRunnable splitShard = new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// first, split shard in the middle of the hash range\n\t\t\t\t\t\tThread.sleep(5000);\n\t\t\t\t\t\tLOG.info(\"Splitting shard ...\");\n\t\t\t\t\t\tclient.splitShard(\n\t\t\t\t\t\t\tstreamName,\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(0),\n\t\t\t\t\t\t\t\"170141183460469231731687303715884105727\");\n\n\t\t\t\t\t\t// wait until the split shard operation finishes updating ...\n\t\t\t\t\t\tDescribeStreamResult status;\n\t\t\t\t\t\tRandom rand = new Random();\n\t\t\t\t\t\tdo {\n\t\t\t\t\t\t\tstatus = null;\n\t\t\t\t\t\t\twhile (status == null) {\n\t\t\t\t\t\t\t\t// retry until we get status\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tstatus = client.describeStream(streamName);\n\t\t\t\t\t\t\t\t} catch (LimitExceededException lee) {\n\t\t\t\t\t\t\t\t\tLOG.warn(\"LimitExceededException while describing stream ... retrying ...\");\n\t\t\t\t\t\t\t\t\tThread.sleep(rand.nextInt(1200));\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t} while (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\"));\n\n\t\t\t\t\t\t// then merge again\n\t\t\t\t\t\tThread.sleep(7000);\n\t\t\t\t\t\tLOG.info(\"Merging shards ...\");\n\t\t\t\t\t\tclient.mergeShards(\n\t\t\t\t\t\t\tstreamName,\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(1),\n\t\t\t\t\t\t\tKinesisShardIdGenerator.generateFromShardOrder(2));\n\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t//\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t};\n\t\t\tThread splitShardThread = new Thread(splitShard);\n\t\t\tsplitShardThread.start();\n\n\t\t\tboolean deadlinePassed = false;\n\t\t\tlong deadline = System.currentTimeMillis() + (1000 * 5 * 60); // wait at most for five minutes\n\t\t\t// wait until both producer and consumer finishes, or an unexpected error is thrown\n\t\t\twhile ((consumerThread.isAlive() || producerThread.isAlive()) &&\n\t\t\t\t(producerError.get() == null && consumerError.get() == null)) {\n\t\t\t\tThread.sleep(1000);\n\t\t\t\tif (System.currentTimeMillis() >= deadline) {\n\t\t\t\t\tLOG.warn(\"Deadline passed\");\n\t\t\t\t\tdeadlinePassed = true;\n\t\t\t\t\tbreak; // enough waiting\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (producerThread.isAlive()) {\n\t\t\t\tproducerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (consumerThread.isAlive()) {\n\t\t\t\tconsumerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (producerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Producer failed\", producerError.get());\n\n\t\t\t}\n\n\t\t\tif (consumerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Consumer failed\", consumerError.get());\n\t\t\t}\n\n\t\t\tif (!deadlinePassed) {\n\t\t\t\tLOG.info(\"+++ TEST passed! +++\");\n\t\t\t} else {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t}\n\n\t\t} finally {\n\t\t\tclient.deleteStream(streamName);\n\t\t\tclient.shutdown();\n\n\t\t\t// stopping flink\n\t\t\tflink.stop();\n\t\t}\n\t}"
        ],
        [
            "ManualExactlyOnceTest::main(String)",
            "  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66 -\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tfinal ParameterTool pt = ParameterTool.fromArgs(args);\n\t\tLOG.info(\"Starting exactly once test\");\n\n\t\tfinal String streamName = \"flink-test-\" + UUID.randomUUID().toString();\n\t\tfinal String accessKey = pt.getRequired(\"accessKey\");\n\t\tfinal String secretKey = pt.getRequired(\"secretKey\");\n\t\tfinal String region = pt.getRequired(\"region\");\n\n\t\tProperties configProps = new Properties();\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_ACCESS_KEY_ID, accessKey);\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, secretKey);\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_REGION, region);\n\t\tAmazonKinesisClient client = AWSUtil.createKinesisClient(configProps);\n\n\t\t// create a stream for the test:\n\t\tclient.createStream(streamName, 1);\n\n\t\t// wait until stream has been created\n\t\tDescribeStreamResult status = client.describeStream(streamName);\n\t\tLOG.info(\"status {}\" , status);\n\t\twhile (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\")) {\n\t\t\tstatus = client.describeStream(streamName);\n\t\t\tLOG.info(\"Status of stream {}\", status);\n\t\t\tThread.sleep(1000);\n\t\t}\n\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tLocalFlinkMiniCluster flink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\n\t\tfinal int flinkPort = flink.getLeaderRPCPort();\n\n\t\ttry {\n\t\t\tfinal AtomicReference<Throwable> producerError = new AtomicReference<>();\n\t\t\tThread producerThread = KinesisEventsGeneratorProducerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 2,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tproducerError, flinkPort, flinkConfig);\n\t\t\tproducerThread.start();\n\n\t\t\tfinal AtomicReference<Throwable> consumerError = new AtomicReference<>();\n\t\t\tThread consumerThread = ExactlyOnceValidatingConsumerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 200, 2, 500, 500,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tconsumerError, flinkPort, flinkConfig);\n\t\t\tconsumerThread.start();\n\n\t\t\tboolean deadlinePassed = false;\n\t\t\tlong deadline = System.currentTimeMillis() + (1000 * 2 * 60); // wait at most for two minutes\n\t\t\t// wait until both producer and consumer finishes, or an unexpected error is thrown\n\t\t\twhile ((consumerThread.isAlive() || producerThread.isAlive()) &&\n\t\t\t\t(producerError.get() == null && consumerError.get() == null)) {\n\t\t\t\tThread.sleep(1000);\n\t\t\t\tif (System.currentTimeMillis() >= deadline) {\n\t\t\t\t\tLOG.warn(\"Deadline passed\");\n\t\t\t\t\tdeadlinePassed = true;\n\t\t\t\t\tbreak; // enough waiting\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (producerThread.isAlive()) {\n\t\t\t\tproducerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (consumerThread.isAlive()) {\n\t\t\t\tconsumerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (producerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Producer failed\", producerError.get());\n\t\t\t}\n\t\t\tif (consumerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Consumer failed\", consumerError.get());\n\t\t\t}\n\n\t\t\tif (!deadlinePassed) {\n\t\t\t\tLOG.info(\"+++ TEST passed! +++\");\n\t\t\t} else {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t}\n\n\t\t} finally {\n\t\t\tclient.deleteStream(streamName);\n\t\t\tclient.shutdown();\n\n\t\t\t// stopping flink\n\t\t\tflink.stop();\n\t\t}\n\t}",
            "  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  ",
            "\tpublic static void main(String[] args) throws Exception {\n\t\tfinal ParameterTool pt = ParameterTool.fromArgs(args);\n\t\tLOG.info(\"Starting exactly once test\");\n\n\t\tfinal String streamName = \"flink-test-\" + UUID.randomUUID().toString();\n\t\tfinal String accessKey = pt.getRequired(\"accessKey\");\n\t\tfinal String secretKey = pt.getRequired(\"secretKey\");\n\t\tfinal String region = pt.getRequired(\"region\");\n\n\t\tProperties configProps = new Properties();\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_ACCESS_KEY_ID, accessKey);\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_SECRET_ACCESS_KEY, secretKey);\n\t\tconfigProps.setProperty(AWSConfigConstants.AWS_REGION, region);\n\t\tAmazonKinesis client = AWSUtil.createKinesisClient(configProps);\n\n\t\t// create a stream for the test:\n\t\tclient.createStream(streamName, 1);\n\n\t\t// wait until stream has been created\n\t\tDescribeStreamResult status = client.describeStream(streamName);\n\t\tLOG.info(\"status {}\" , status);\n\t\twhile (!status.getStreamDescription().getStreamStatus().equals(\"ACTIVE\")) {\n\t\t\tstatus = client.describeStream(streamName);\n\t\t\tLOG.info(\"Status of stream {}\", status);\n\t\t\tThread.sleep(1000);\n\t\t}\n\n\t\tfinal Configuration flinkConfig = new Configuration();\n\t\tflinkConfig.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER, 1);\n\t\tflinkConfig.setInteger(ConfigConstants.TASK_MANAGER_NUM_TASK_SLOTS, 8);\n\t\tflinkConfig.setLong(TaskManagerOptions.MANAGED_MEMORY_SIZE, 16);\n\t\tflinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY, \"0 s\");\n\n\t\tLocalFlinkMiniCluster flink = new LocalFlinkMiniCluster(flinkConfig, false);\n\t\tflink.start();\n\n\t\tfinal int flinkPort = flink.getLeaderRPCPort();\n\n\t\ttry {\n\t\t\tfinal AtomicReference<Throwable> producerError = new AtomicReference<>();\n\t\t\tThread producerThread = KinesisEventsGeneratorProducerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 2,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tproducerError, flinkPort, flinkConfig);\n\t\t\tproducerThread.start();\n\n\t\t\tfinal AtomicReference<Throwable> consumerError = new AtomicReference<>();\n\t\t\tThread consumerThread = ExactlyOnceValidatingConsumerThread.create(\n\t\t\t\tTOTAL_EVENT_COUNT, 200, 2, 500, 500,\n\t\t\t\taccessKey, secretKey, region, streamName,\n\t\t\t\tconsumerError, flinkPort, flinkConfig);\n\t\t\tconsumerThread.start();\n\n\t\t\tboolean deadlinePassed = false;\n\t\t\tlong deadline = System.currentTimeMillis() + (1000 * 2 * 60); // wait at most for two minutes\n\t\t\t// wait until both producer and consumer finishes, or an unexpected error is thrown\n\t\t\twhile ((consumerThread.isAlive() || producerThread.isAlive()) &&\n\t\t\t\t(producerError.get() == null && consumerError.get() == null)) {\n\t\t\t\tThread.sleep(1000);\n\t\t\t\tif (System.currentTimeMillis() >= deadline) {\n\t\t\t\t\tLOG.warn(\"Deadline passed\");\n\t\t\t\t\tdeadlinePassed = true;\n\t\t\t\t\tbreak; // enough waiting\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tif (producerThread.isAlive()) {\n\t\t\t\tproducerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (consumerThread.isAlive()) {\n\t\t\t\tconsumerThread.interrupt();\n\t\t\t}\n\n\t\t\tif (producerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Producer failed\", producerError.get());\n\t\t\t}\n\t\t\tif (consumerError.get() != null) {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t\tthrow new RuntimeException(\"Consumer failed\", consumerError.get());\n\t\t\t}\n\n\t\t\tif (!deadlinePassed) {\n\t\t\t\tLOG.info(\"+++ TEST passed! +++\");\n\t\t\t} else {\n\t\t\t\tLOG.info(\"+++ TEST failed! +++\");\n\t\t\t}\n\n\t\t} finally {\n\t\t\tclient.deleteStream(streamName);\n\t\t\tclient.shutdown();\n\n\t\t\t// stopping flink\n\t\t\tflink.stop();\n\t\t}\n\t}"
        ]
    ],
    "07bd44b92a10229a21ebac519a936a3e520a164d": [
        [
            "ConfigOptionsDocGenerator::toHtmlString(ConfigOption)",
            " 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177 -\n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  ",
            "\t/**\n\t * Transforms option to table row.\n\t *\n\t * @param option option to transform\n\t * @return row with the option description\n\t */\n\tprivate static String toHtmlString(final ConfigOption<?> option) {\n\t\tObject defaultValue = option.defaultValue();\n\t\t// This is a temporary hack that should be removed once FLINK-6490 is resolved.\n\t\t// These options use System.getProperty(\"java.io.tmpdir\") as the default.\n\t\t// As a result the generated table contains an actual path as the default, which is simply wrong.\n\t\tif (option == WebOptions.TMP_DIR || option.key().equals(\"python.dc.tmp.dir\")) {\n\t\t\tdefaultValue = null;\n\t\t}\n\t\treturn \"\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>\" + escapeCharacters(option.key()) + \"</h5></td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(defaultValueToHtml(defaultValue)) + \"</td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(option.description()) + \"</td>\\n\" +\n\t\t\t\"        </tr>\\n\";\n\t}",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178 +\n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  ",
            "\t/**\n\t * Transforms option to table row.\n\t *\n\t * @param option option to transform\n\t * @return row with the option description\n\t */\n\tprivate static String toHtmlString(final ConfigOption<?> option) {\n\t\tObject defaultValue = option.defaultValue();\n\t\t// This is a temporary hack that should be removed once FLINK-6490 is resolved.\n\t\t// These options use System.getProperty(\"java.io.tmpdir\") as the default.\n\t\t// As a result the generated table contains an actual path as the default, which is simply wrong.\n\t\tif (option == WebOptions.TMP_DIR || option.key().equals(\"python.dc.tmp.dir\") || option == CoreOptions.TMP_DIRS) {\n\t\t\tdefaultValue = null;\n\t\t}\n\t\treturn \"\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>\" + escapeCharacters(option.key()) + \"</h5></td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(defaultValueToHtml(defaultValue)) + \"</td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(option.description()) + \"</td>\\n\" +\n\t\t\t\"        </tr>\\n\";\n\t}"
        ]
    ],
    "8bb9aa4ed96f8f9fd189f67db869f177f7231e84": [
        [
            "ConfigOptionsDocGeneratorTest::testCreatingDescription()",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 -\n  66  \n  67  \n  68  \n  69  \n  70 -\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\t@Test\n\tpublic void testCreatingDescription() {\n\t\tfinal String expectedTable =\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>first.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the first option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>second.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the second option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\";\n\t\tfinal String htmlTable = ConfigOptionsDocGenerator.generateTablesForClass(TestConfigGroup.class).get(0).f1;\n\n\t\tassertEquals(expectedTable, htmlTable);\n\t}",
            "  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65 +\n  66  \n  67  \n  68  \n  69  \n  70 +\n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\t@Test\n\tpublic void testCreatingDescription() {\n\t\tfinal String expectedTable =\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>first.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the first option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>second.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the second option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\";\n\t\tfinal String htmlTable = ConfigOptionsDocGenerator.generateTablesForClass(TestConfigGroup.class).get(0).f1;\n\n\t\tassertEquals(expectedTable, htmlTable);\n\t}"
        ],
        [
            "ConfigOptionsDocGenerator::toHtmlString(ConfigOption)",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 -\n 185  \n 186  \n 187  ",
            "\t/**\n\t * Transforms option to table row.\n\t *\n\t * @param option option to transform\n\t * @return row with the option description\n\t */\n\tprivate static String toHtmlString(final ConfigOption<?> option) {\n\t\tObject defaultValue = option.defaultValue();\n\t\t// This is a temporary hack that should be removed once FLINK-6490 is resolved.\n\t\t// These options use System.getProperty(\"java.io.tmpdir\") as the default.\n\t\t// As a result the generated table contains an actual path as the default, which is simply wrong.\n\t\tif (option == WebOptions.TMP_DIR || option.key().equals(\"python.dc.tmp.dir\") || option == CoreOptions.TMP_DIRS) {\n\t\t\tdefaultValue = null;\n\t\t}\n\t\treturn \"\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>\" + escapeCharacters(option.key()) + \"</h5></td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(defaultValueToHtml(defaultValue)) + \"</td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(option.description()) + \"</td>\\n\" +\n\t\t\t\"        </tr>\\n\";\n\t}",
            " 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 +\n 185  \n 186  \n 187  ",
            "\t/**\n\t * Transforms option to table row.\n\t *\n\t * @param option option to transform\n\t * @return row with the option description\n\t */\n\tprivate static String toHtmlString(final ConfigOption<?> option) {\n\t\tObject defaultValue = option.defaultValue();\n\t\t// This is a temporary hack that should be removed once FLINK-6490 is resolved.\n\t\t// These options use System.getProperty(\"java.io.tmpdir\") as the default.\n\t\t// As a result the generated table contains an actual path as the default, which is simply wrong.\n\t\tif (option == WebOptions.TMP_DIR || option.key().equals(\"python.dc.tmp.dir\") || option == CoreOptions.TMP_DIRS) {\n\t\t\tdefaultValue = null;\n\t\t}\n\t\treturn \"\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>\" + escapeCharacters(option.key()) + \"</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">\" + escapeCharacters(defaultValueToHtml(defaultValue)) + \"</td>\\n\" +\n\t\t\t\"            <td>\" + escapeCharacters(option.description()) + \"</td>\\n\" +\n\t\t\t\"        </tr>\\n\";\n\t}"
        ],
        [
            "ConfigOptionsDocGeneratorTest::testCreatingMultipleGroups()",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 -\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167 -\n 168  \n 169  \n 170  \n 171  \n 172  ",
            "\t@Test\n\tpublic void testCreatingMultipleGroups() {\n\t\tfinal List<Tuple2<ConfigGroup, String>> tables = ConfigOptionsDocGenerator.generateTablesForClass(\n\t\t\tTestConfigMultipleSubGroup.class);\n\n\t\tassertEquals(tables.size(), 3);\n\t\tfinal HashMap<String, String> tablesConverted = new HashMap<>();\n\t\tfor (Tuple2<ConfigGroup, String> table : tables) {\n\t\t\ttablesConverted.put(table.f0 != null ? table.f0.name() : \"default\", table.f1);\n\t\t}\n\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>first.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the first option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"firstGroup\"));\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>second.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the second option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"secondGroup\"));\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>fourth.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the fourth option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>third.option.a</h5></td>\\n\" +\n\t\t\t\"            <td>2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the third option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"default\"));\n\t}",
            " 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128 +\n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167 +\n 168  \n 169  \n 170  \n 171  \n 172  ",
            "\t@Test\n\tpublic void testCreatingMultipleGroups() {\n\t\tfinal List<Tuple2<ConfigGroup, String>> tables = ConfigOptionsDocGenerator.generateTablesForClass(\n\t\t\tTestConfigMultipleSubGroup.class);\n\n\t\tassertEquals(tables.size(), 3);\n\t\tfinal HashMap<String, String> tablesConverted = new HashMap<>();\n\t\tfor (Tuple2<ConfigGroup, String> table : tables) {\n\t\t\ttablesConverted.put(table.f0 != null ? table.f0.name() : \"default\", table.f1);\n\t\t}\n\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>first.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the first option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"firstGroup\"));\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>second.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the second option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"secondGroup\"));\n\t\tassertEquals(\n\t\t\t\"<table class=\\\"table table-bordered\\\">\\n\" +\n\t\t\t\"    <thead>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 20%\\\">Key</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 15%\\\">Default</th>\\n\" +\n\t\t\t\"            <th class=\\\"text-left\\\" style=\\\"width: 65%\\\">Description</th>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </thead>\\n\" +\n\t\t\t\"    <tbody>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>fourth.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">(none)</td>\\n\" +\n\t\t\t\"            <td>This is long example description for the fourth option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"        <tr>\\n\" +\n\t\t\t\"            <td><h5>third.option.a</h5></td>\\n\" +\n\t\t\t\"            <td style=\\\"word-wrap: break-word;\\\">2</td>\\n\" +\n\t\t\t\"            <td>This is example description for the third option.</td>\\n\" +\n\t\t\t\"        </tr>\\n\" +\n\t\t\t\"    </tbody>\\n\" +\n\t\t\t\"</table>\\n\", tablesConverted.get(\"default\"));\n\t}"
        ]
    ],
    "5eae8bd423256fb372a57151e482c501c955c008": [
        [
            "Kafka08Fetcher::runFetchLoop()",
            " 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 -\n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  ",
            "\t@Override\n\tpublic void runFetchLoop() throws Exception {\n\t\t// the map from broker to the thread that is connected to that broker\n\t\tfinal Map<Node, SimpleConsumerThread<T>> brokerToThread = new HashMap<>();\n\n\t\t// this holds possible the exceptions from the concurrent broker connection threads\n\t\tfinal ExceptionProxy errorHandler = new ExceptionProxy(Thread.currentThread());\n\n\t\t// the offset handler handles the communication with ZooKeeper, to commit externally visible offsets\n\t\tfinal ZookeeperOffsetHandler zookeeperOffsetHandler = new ZookeeperOffsetHandler(kafkaConfig);\n\t\tthis.zookeeperOffsetHandler = zookeeperOffsetHandler;\n\n\t\tPeriodicOffsetCommitter periodicCommitter = null;\n\t\ttry {\n\n\t\t\t// offsets in the state may still be placeholder sentinel values if we are starting fresh, or the\n\t\t\t// checkpoint / savepoint state we were restored with had not completely been replaced with actual offset\n\t\t\t// values yet; replace those with actual offsets, according to what the sentinel value represent.\n\t\t\tfor (KafkaTopicPartitionState<TopicAndPartition> partition : subscribedPartitionStates()) {\n\t\t\t\tif (partition.getOffset() == KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET) {\n\t\t\t\t\t// this will be replaced by an actual offset in SimpleConsumerThread\n\t\t\t\t\tpartition.setOffset(OffsetRequest.EarliestTime());\n\t\t\t\t} else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.LATEST_OFFSET) {\n\t\t\t\t\t// this will be replaced by an actual offset in SimpleConsumerThread\n\t\t\t\t\tpartition.setOffset(OffsetRequest.LatestTime());\n\t\t\t\t} else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.GROUP_OFFSET) {\n\t\t\t\t\tLong committedOffset = zookeeperOffsetHandler.getCommittedOffset(partition.getKafkaTopicPartition());\n\t\t\t\t\tif (committedOffset != null) {\n\t\t\t\t\t\t// the committed offset in ZK represents the next record to process,\n\t\t\t\t\t\t// so we subtract it by 1 to correctly represent internal state\n\t\t\t\t\t\tpartition.setOffset(committedOffset - 1);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// if we can't find an offset for a partition in ZK when using GROUP_OFFSETS,\n\t\t\t\t\t\t// we default to \"auto.offset.reset\" like the Kafka high-level consumer\n\t\t\t\t\t\tLOG.warn(\"No group offset can be found for partition {} in Zookeeper;\" +\n\t\t\t\t\t\t\t\" resetting starting offset to 'auto.offset.reset'\", partition);\n\n\t\t\t\t\t\tpartition.setOffset(invalidOffsetBehavior);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// the partition already has a specific start offset and is ready to be consumed\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// start the periodic offset committer thread, if necessary\n\t\t\tif (autoCommitInterval > 0) {\n\t\t\t\tLOG.info(\"Starting periodic offset committer, with commit interval of {}ms\", autoCommitInterval);\n\n\t\t\t\tperiodicCommitter = new PeriodicOffsetCommitter(\n\t\t\t\t\t\tzookeeperOffsetHandler,\n\t\t\t\t\t\tsubscribedPartitionStates(),\n\t\t\t\t\t\terrorHandler,\n\t\t\t\t\t\tautoCommitInterval);\n\t\t\t\tperiodicCommitter.setName(\"Periodic Kafka partition offset committer\");\n\t\t\t\tperiodicCommitter.setDaemon(true);\n\t\t\t\tperiodicCommitter.start();\n\t\t\t}\n\n\t\t\t// Main loop polling elements from the unassignedPartitions queue to the threads\n\t\t\twhile (running) {\n\t\t\t\t// re-throw any exception from the concurrent fetcher threads\n\t\t\t\terrorHandler.checkAndThrowException();\n\n\t\t\t\t// wait for max 5 seconds trying to get partitions to assign\n\t\t\t\t// if threads shut down, this poll returns earlier, because the threads inject the\n\t\t\t\t// special marker into the queue\n\t\t\t\tList<KafkaTopicPartitionState<TopicAndPartition>> partitionsToAssign =\n\t\t\t\t\t\tunassignedPartitionsQueue.getBatchBlocking(5000);\n\t\t\t\tpartitionsToAssign.remove(MARKER);\n\n\t\t\t\tif (!partitionsToAssign.isEmpty()) {\n\t\t\t\t\tLOG.info(\"Assigning {} partitions to broker threads\", partitionsToAssign.size());\n\t\t\t\t\tMap<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeaders =\n\t\t\t\t\t\t\tfindLeaderForPartitions(partitionsToAssign, kafkaConfig);\n\n\t\t\t\t\t// assign the partitions to the leaders (maybe start the threads)\n\t\t\t\t\tfor (Map.Entry<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeader :\n\t\t\t\t\t\t\tpartitionsWithLeaders.entrySet()) {\n\t\t\t\t\t\tfinal Node leader = partitionsWithLeader.getKey();\n\t\t\t\t\t\tfinal List<KafkaTopicPartitionState<TopicAndPartition>> partitions = partitionsWithLeader.getValue();\n\t\t\t\t\t\tSimpleConsumerThread<T> brokerThread = brokerToThread.get(leader);\n\n\t\t\t\t\t\tif (!running) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (brokerThread == null || !brokerThread.getNewPartitionsQueue().isOpen()) {\n\t\t\t\t\t\t\t// start new thread\n\t\t\t\t\t\t\tbrokerThread = createAndStartSimpleConsumerThread(partitions, leader, errorHandler);\n\t\t\t\t\t\t\tbrokerToThread.put(leader, brokerThread);\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// put elements into queue of thread\n\t\t\t\t\t\t\tClosableBlockingQueue<KafkaTopicPartitionState<TopicAndPartition>> newPartitionsQueue =\n\t\t\t\t\t\t\t\t\tbrokerThread.getNewPartitionsQueue();\n\n\t\t\t\t\t\t\tfor (KafkaTopicPartitionState<TopicAndPartition> fp : partitions) {\n\t\t\t\t\t\t\t\tif (!newPartitionsQueue.addIfOpen(fp)) {\n\t\t\t\t\t\t\t\t\t// we were unable to add the partition to the broker's queue\n\t\t\t\t\t\t\t\t\t// the broker has closed in the meantime (the thread will shut down)\n\t\t\t\t\t\t\t\t\t// create a new thread for connecting to this broker\n\t\t\t\t\t\t\t\t\tList<KafkaTopicPartitionState<TopicAndPartition>> seedPartitions = new ArrayList<>();\n\t\t\t\t\t\t\t\t\tseedPartitions.add(fp);\n\t\t\t\t\t\t\t\t\tbrokerThread = createAndStartSimpleConsumerThread(seedPartitions, leader, errorHandler);\n\t\t\t\t\t\t\t\t\tbrokerToThread.put(leader, brokerThread);\n\t\t\t\t\t\t\t\t\tnewPartitionsQueue = brokerThread.getNewPartitionsQueue(); // update queue for the subsequent partitions\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// there were no partitions to assign. Check if any broker threads shut down.\n\t\t\t\t\t// we get into this section of the code, if either the poll timed out, or the\n\t\t\t\t\t// blocking poll was woken up by the marker element\n\t\t\t\t\tIterator<SimpleConsumerThread<T>> bttIterator = brokerToThread.values().iterator();\n\t\t\t\t\twhile (bttIterator.hasNext()) {\n\t\t\t\t\t\tSimpleConsumerThread<T> thread = bttIterator.next();\n\t\t\t\t\t\tif (!thread.getNewPartitionsQueue().isOpen()) {\n\t\t\t\t\t\t\tLOG.info(\"Removing stopped consumer thread {}\", thread.getName());\n\t\t\t\t\t\t\tbttIterator.remove();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (brokerToThread.size() == 0 && unassignedPartitionsQueue.isEmpty()) {\n\t\t\t\t\tif (unassignedPartitionsQueue.close()) {\n\t\t\t\t\t\tLOG.info(\"All consumer threads are finished, there are no more unassigned partitions. Stopping fetcher\");\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t// we end up here if somebody added something to the queue in the meantime --> continue to poll queue again\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcatch (InterruptedException e) {\n\t\t\t// this may be thrown because an exception on one of the concurrent fetcher threads\n\t\t\t// woke this thread up. make sure we throw the root exception instead in that case\n\t\t\terrorHandler.checkAndThrowException();\n\n\t\t\t// no other root exception, throw the interrupted exception\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tthis.running = false;\n\t\t\tthis.zookeeperOffsetHandler = null;\n\n\t\t\t// if we run a periodic committer thread, shut that down\n\t\t\tif (periodicCommitter != null) {\n\t\t\t\tperiodicCommitter.shutdown();\n\t\t\t}\n\n\t\t\t// clear the interruption flag\n\t\t\t// this allows the joining on consumer threads (on best effort) to happen in\n\t\t\t// case the initial interrupt already\n\t\t\tThread.interrupted();\n\n\t\t\t// make sure that in any case (completion, abort, error), all spawned threads are stopped\n\t\t\ttry {\n\t\t\t\tint runningThreads;\n\t\t\t\tdo {\n\t\t\t\t\t// check whether threads are alive and cancel them\n\t\t\t\t\trunningThreads = 0;\n\t\t\t\t\tIterator<SimpleConsumerThread<T>> threads = brokerToThread.values().iterator();\n\t\t\t\t\twhile (threads.hasNext()) {\n\t\t\t\t\t\tSimpleConsumerThread<?> t = threads.next();\n\t\t\t\t\t\tif (t.isAlive()) {\n\t\t\t\t\t\t\tt.cancel();\n\t\t\t\t\t\t\trunningThreads++;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthreads.remove();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// wait for the threads to finish, before issuing a cancel call again\n\t\t\t\t\tif (runningThreads > 0) {\n\t\t\t\t\t\tfor (SimpleConsumerThread<?> t : brokerToThread.values()) {\n\t\t\t\t\t\t\tt.join(500 / runningThreads + 1);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\twhile (runningThreads > 0);\n\t\t\t}\n\t\t\tcatch (InterruptedException ignored) {\n\t\t\t\t// waiting for the thread shutdown apparently got interrupted\n\t\t\t\t// restore interrupted state and continue\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// we catch all here to preserve the original exception\n\t\t\t\tLOG.error(\"Exception while shutting down consumer threads\", t);\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tzookeeperOffsetHandler.close();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// we catch all here to preserve the original exception\n\t\t\t\tLOG.error(\"Exception while shutting down ZookeeperOffsetHandler\", t);\n\t\t\t}\n\t\t}\n\t}",
            " 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192 +\n 193 +\n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  \n 289  \n 290  \n 291  \n 292  \n 293  \n 294  \n 295  \n 296  \n 297  \n 298  \n 299  \n 300  \n 301  \n 302  \n 303  \n 304  \n 305  \n 306  \n 307  \n 308  \n 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323  \n 324  \n 325  ",
            "\t@Override\n\tpublic void runFetchLoop() throws Exception {\n\t\t// the map from broker to the thread that is connected to that broker\n\t\tfinal Map<Node, SimpleConsumerThread<T>> brokerToThread = new HashMap<>();\n\n\t\t// this holds possible the exceptions from the concurrent broker connection threads\n\t\tfinal ExceptionProxy errorHandler = new ExceptionProxy(Thread.currentThread());\n\n\t\t// the offset handler handles the communication with ZooKeeper, to commit externally visible offsets\n\t\tfinal ZookeeperOffsetHandler zookeeperOffsetHandler = new ZookeeperOffsetHandler(kafkaConfig);\n\t\tthis.zookeeperOffsetHandler = zookeeperOffsetHandler;\n\n\t\tPeriodicOffsetCommitter periodicCommitter = null;\n\t\ttry {\n\n\t\t\t// offsets in the state may still be placeholder sentinel values if we are starting fresh, or the\n\t\t\t// checkpoint / savepoint state we were restored with had not completely been replaced with actual offset\n\t\t\t// values yet; replace those with actual offsets, according to what the sentinel value represent.\n\t\t\tfor (KafkaTopicPartitionState<TopicAndPartition> partition : subscribedPartitionStates()) {\n\t\t\t\tif (partition.getOffset() == KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET) {\n\t\t\t\t\t// this will be replaced by an actual offset in SimpleConsumerThread\n\t\t\t\t\tpartition.setOffset(OffsetRequest.EarliestTime());\n\t\t\t\t} else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.LATEST_OFFSET) {\n\t\t\t\t\t// this will be replaced by an actual offset in SimpleConsumerThread\n\t\t\t\t\tpartition.setOffset(OffsetRequest.LatestTime());\n\t\t\t\t} else if (partition.getOffset() == KafkaTopicPartitionStateSentinel.GROUP_OFFSET) {\n\t\t\t\t\tLong committedOffset = zookeeperOffsetHandler.getCommittedOffset(partition.getKafkaTopicPartition());\n\t\t\t\t\tif (committedOffset != null) {\n\t\t\t\t\t\t// the committed offset in ZK represents the next record to process,\n\t\t\t\t\t\t// so we subtract it by 1 to correctly represent internal state\n\t\t\t\t\t\tpartition.setOffset(committedOffset - 1);\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// if we can't find an offset for a partition in ZK when using GROUP_OFFSETS,\n\t\t\t\t\t\t// we default to \"auto.offset.reset\" like the Kafka high-level consumer\n\t\t\t\t\t\tLOG.warn(\"No group offset can be found for partition {} in Zookeeper;\" +\n\t\t\t\t\t\t\t\" resetting starting offset to 'auto.offset.reset'\", partition);\n\n\t\t\t\t\t\tpartition.setOffset(invalidOffsetBehavior);\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// the partition already has a specific start offset and is ready to be consumed\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// start the periodic offset committer thread, if necessary\n\t\t\tif (autoCommitInterval > 0) {\n\t\t\t\tLOG.info(\"Starting periodic offset committer, with commit interval of {}ms\", autoCommitInterval);\n\n\t\t\t\tperiodicCommitter = new PeriodicOffsetCommitter(\n\t\t\t\t\t\tzookeeperOffsetHandler,\n\t\t\t\t\t\tsubscribedPartitionStates(),\n\t\t\t\t\t\terrorHandler,\n\t\t\t\t\t\tautoCommitInterval);\n\t\t\t\tperiodicCommitter.setName(\"Periodic Kafka partition offset committer\");\n\t\t\t\tperiodicCommitter.setDaemon(true);\n\t\t\t\tperiodicCommitter.start();\n\t\t\t}\n\n\t\t\t// Main loop polling elements from the unassignedPartitions queue to the threads\n\t\t\twhile (running) {\n\t\t\t\t// re-throw any exception from the concurrent fetcher threads\n\t\t\t\terrorHandler.checkAndThrowException();\n\n\t\t\t\t// wait for max 5 seconds trying to get partitions to assign\n\t\t\t\t// if threads shut down, this poll returns earlier, because the threads inject the\n\t\t\t\t// special marker into the queue\n\t\t\t\tList<KafkaTopicPartitionState<TopicAndPartition>> partitionsToAssign =\n\t\t\t\t\t\tunassignedPartitionsQueue.getBatchBlocking(5000);\n\t\t\t\t// note: if there are more markers, remove them all\n\t\t\t\tpartitionsToAssign.removeIf(MARKER::equals);\n\n\t\t\t\tif (!partitionsToAssign.isEmpty()) {\n\t\t\t\t\tLOG.info(\"Assigning {} partitions to broker threads\", partitionsToAssign.size());\n\t\t\t\t\tMap<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeaders =\n\t\t\t\t\t\t\tfindLeaderForPartitions(partitionsToAssign, kafkaConfig);\n\n\t\t\t\t\t// assign the partitions to the leaders (maybe start the threads)\n\t\t\t\t\tfor (Map.Entry<Node, List<KafkaTopicPartitionState<TopicAndPartition>>> partitionsWithLeader :\n\t\t\t\t\t\t\tpartitionsWithLeaders.entrySet()) {\n\t\t\t\t\t\tfinal Node leader = partitionsWithLeader.getKey();\n\t\t\t\t\t\tfinal List<KafkaTopicPartitionState<TopicAndPartition>> partitions = partitionsWithLeader.getValue();\n\t\t\t\t\t\tSimpleConsumerThread<T> brokerThread = brokerToThread.get(leader);\n\n\t\t\t\t\t\tif (!running) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tif (brokerThread == null || !brokerThread.getNewPartitionsQueue().isOpen()) {\n\t\t\t\t\t\t\t// start new thread\n\t\t\t\t\t\t\tbrokerThread = createAndStartSimpleConsumerThread(partitions, leader, errorHandler);\n\t\t\t\t\t\t\tbrokerToThread.put(leader, brokerThread);\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse {\n\t\t\t\t\t\t\t// put elements into queue of thread\n\t\t\t\t\t\t\tClosableBlockingQueue<KafkaTopicPartitionState<TopicAndPartition>> newPartitionsQueue =\n\t\t\t\t\t\t\t\t\tbrokerThread.getNewPartitionsQueue();\n\n\t\t\t\t\t\t\tfor (KafkaTopicPartitionState<TopicAndPartition> fp : partitions) {\n\t\t\t\t\t\t\t\tif (!newPartitionsQueue.addIfOpen(fp)) {\n\t\t\t\t\t\t\t\t\t// we were unable to add the partition to the broker's queue\n\t\t\t\t\t\t\t\t\t// the broker has closed in the meantime (the thread will shut down)\n\t\t\t\t\t\t\t\t\t// create a new thread for connecting to this broker\n\t\t\t\t\t\t\t\t\tList<KafkaTopicPartitionState<TopicAndPartition>> seedPartitions = new ArrayList<>();\n\t\t\t\t\t\t\t\t\tseedPartitions.add(fp);\n\t\t\t\t\t\t\t\t\tbrokerThread = createAndStartSimpleConsumerThread(seedPartitions, leader, errorHandler);\n\t\t\t\t\t\t\t\t\tbrokerToThread.put(leader, brokerThread);\n\t\t\t\t\t\t\t\t\tnewPartitionsQueue = brokerThread.getNewPartitionsQueue(); // update queue for the subsequent partitions\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\t// there were no partitions to assign. Check if any broker threads shut down.\n\t\t\t\t\t// we get into this section of the code, if either the poll timed out, or the\n\t\t\t\t\t// blocking poll was woken up by the marker element\n\t\t\t\t\tIterator<SimpleConsumerThread<T>> bttIterator = brokerToThread.values().iterator();\n\t\t\t\t\twhile (bttIterator.hasNext()) {\n\t\t\t\t\t\tSimpleConsumerThread<T> thread = bttIterator.next();\n\t\t\t\t\t\tif (!thread.getNewPartitionsQueue().isOpen()) {\n\t\t\t\t\t\t\tLOG.info(\"Removing stopped consumer thread {}\", thread.getName());\n\t\t\t\t\t\t\tbttIterator.remove();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\tif (brokerToThread.size() == 0 && unassignedPartitionsQueue.isEmpty()) {\n\t\t\t\t\tif (unassignedPartitionsQueue.close()) {\n\t\t\t\t\t\tLOG.info(\"All consumer threads are finished, there are no more unassigned partitions. Stopping fetcher\");\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\t// we end up here if somebody added something to the queue in the meantime --> continue to poll queue again\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcatch (InterruptedException e) {\n\t\t\t// this may be thrown because an exception on one of the concurrent fetcher threads\n\t\t\t// woke this thread up. make sure we throw the root exception instead in that case\n\t\t\terrorHandler.checkAndThrowException();\n\n\t\t\t// no other root exception, throw the interrupted exception\n\t\t\tthrow e;\n\t\t}\n\t\tfinally {\n\t\t\tthis.running = false;\n\t\t\tthis.zookeeperOffsetHandler = null;\n\n\t\t\t// if we run a periodic committer thread, shut that down\n\t\t\tif (periodicCommitter != null) {\n\t\t\t\tperiodicCommitter.shutdown();\n\t\t\t}\n\n\t\t\t// clear the interruption flag\n\t\t\t// this allows the joining on consumer threads (on best effort) to happen in\n\t\t\t// case the initial interrupt already\n\t\t\tThread.interrupted();\n\n\t\t\t// make sure that in any case (completion, abort, error), all spawned threads are stopped\n\t\t\ttry {\n\t\t\t\tint runningThreads;\n\t\t\t\tdo {\n\t\t\t\t\t// check whether threads are alive and cancel them\n\t\t\t\t\trunningThreads = 0;\n\t\t\t\t\tIterator<SimpleConsumerThread<T>> threads = brokerToThread.values().iterator();\n\t\t\t\t\twhile (threads.hasNext()) {\n\t\t\t\t\t\tSimpleConsumerThread<?> t = threads.next();\n\t\t\t\t\t\tif (t.isAlive()) {\n\t\t\t\t\t\t\tt.cancel();\n\t\t\t\t\t\t\trunningThreads++;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tthreads.remove();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t// wait for the threads to finish, before issuing a cancel call again\n\t\t\t\t\tif (runningThreads > 0) {\n\t\t\t\t\t\tfor (SimpleConsumerThread<?> t : brokerToThread.values()) {\n\t\t\t\t\t\t\tt.join(500 / runningThreads + 1);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\twhile (runningThreads > 0);\n\t\t\t}\n\t\t\tcatch (InterruptedException ignored) {\n\t\t\t\t// waiting for the thread shutdown apparently got interrupted\n\t\t\t\t// restore interrupted state and continue\n\t\t\t\tThread.currentThread().interrupt();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// we catch all here to preserve the original exception\n\t\t\t\tLOG.error(\"Exception while shutting down consumer threads\", t);\n\t\t\t}\n\n\t\t\ttry {\n\t\t\t\tzookeeperOffsetHandler.close();\n\t\t\t}\n\t\t\tcatch (Throwable t) {\n\t\t\t\t// we catch all here to preserve the original exception\n\t\t\t\tLOG.error(\"Exception while shutting down ZookeeperOffsetHandler\", t);\n\t\t\t}\n\t\t}\n\t}"
        ],
        [
            "FlinkKafkaConsumerBase::run(SourceContext)",
            " 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712 -\n 713  \n 714  \n 715  \n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  ",
            "\t@Override\n\tpublic void run(SourceContext<T> sourceContext) throws Exception {\n\t\tif (subscribedPartitionsToStartOffsets == null) {\n\t\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t\t}\n\n\t\t// initialize commit metrics and default offset callback method\n\t\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\t\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\t\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t\t@Override\n\t\t\tpublic void onSuccess() {\n\t\t\t\tsuccessfulCommits.inc();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void onException(Throwable cause) {\n\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\t\tfailedCommits.inc();\n\t\t\t}\n\t\t};\n\n\t\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t\t// status will automatically be triggered back to be active.\n\t\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\t\tsourceContext.markAsTemporarilyIdle();\n\t\t}\n\n\t\t// from this point forward:\n\t\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t\t//     Kafka through the fetcher, if configured to do so)\n\t\tthis.kafkaFetcher = createFetcher(\n\t\t\t\tsourceContext,\n\t\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\t\tperiodicWatermarkAssigner,\n\t\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\t\toffsetCommitMode,\n\t\t\t\tgetRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n\t\t\t\tuseMetrics);\n\n\t\tif (!running) {\n\t\t\treturn;\n\t\t}\n\n\t\t// depending on whether we were restored with the current state version (1.3),\n\t\t// remaining logic branches off into 2 paths:\n\t\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t\t//                 thread running the main fetcher loop\n\t\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\t\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\t\twhile (running) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t\t} finally {\n\t\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\t\tcancel();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tdiscoveryLoopThread.start();\n\t\t\tkafkaFetcher.runFetchLoop();\n\n\t\t\t// --------------------------------------------------------------------\n\n\t\t\t// make sure that the partition discoverer is properly closed\n\t\t\tpartitionDiscoverer.close();\n\t\t\tdiscoveryLoopThread.join();\n\n\t\t\t// rethrow any fetcher errors\n\t\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\t\tif (discoveryLoopError != null) {\n\t\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t\t}\n\t\t} else {\n\t\t\t// won't be using the discoverer\n\t\t\tpartitionDiscoverer.close();\n\n\t\t\tkafkaFetcher.runFetchLoop();\n\t\t}\n\t}",
            " 612  \n 613  \n 614  \n 615  \n 616  \n 617  \n 618  \n 619  \n 620  \n 621  \n 622  \n 623  \n 624  \n 625  \n 626  \n 627  \n 628  \n 629  \n 630  \n 631  \n 632  \n 633  \n 634  \n 635  \n 636  \n 637  \n 638  \n 639  \n 640  \n 641  \n 642  \n 643  \n 644  \n 645  \n 646  \n 647  \n 648  \n 649  \n 650  \n 651  \n 652  \n 653  \n 654  \n 655  \n 656  \n 657  \n 658  \n 659  \n 660  \n 661  \n 662  \n 663  \n 664  \n 665  \n 666  \n 667  \n 668  \n 669  \n 670  \n 671  \n 672  \n 673  \n 674  \n 675  \n 676  \n 677  \n 678  \n 679  \n 680  \n 681  \n 682  \n 683  \n 684  \n 685  \n 686  \n 687  \n 688  \n 689  \n 690  \n 691  \n 692  \n 693  \n 694  \n 695  \n 696  \n 697  \n 698  \n 699  \n 700  \n 701  \n 702  \n 703  \n 704  \n 705  \n 706  \n 707  \n 708  \n 709  \n 710  \n 711  \n 712 +\n 713 +\n 714 +\n 715 +\n 716  \n 717  \n 718  \n 719  \n 720  \n 721  \n 722  \n 723  \n 724  \n 725  \n 726  \n 727  \n 728  \n 729  \n 730  \n 731  \n 732  \n 733  \n 734  \n 735  \n 736  \n 737  \n 738  \n 739  \n 740  ",
            "\t@Override\n\tpublic void run(SourceContext<T> sourceContext) throws Exception {\n\t\tif (subscribedPartitionsToStartOffsets == null) {\n\t\t\tthrow new Exception(\"The partitions were not set for the consumer\");\n\t\t}\n\n\t\t// initialize commit metrics and default offset callback method\n\t\tthis.successfulCommits = this.getRuntimeContext().getMetricGroup().counter(COMMITS_SUCCEEDED_METRICS_COUNTER);\n\t\tthis.failedCommits =  this.getRuntimeContext().getMetricGroup().counter(COMMITS_FAILED_METRICS_COUNTER);\n\n\t\tthis.offsetCommitCallback = new KafkaCommitCallback() {\n\t\t\t@Override\n\t\t\tpublic void onSuccess() {\n\t\t\t\tsuccessfulCommits.inc();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void onException(Throwable cause) {\n\t\t\t\tLOG.warn(\"Async Kafka commit failed.\", cause);\n\t\t\t\tfailedCommits.inc();\n\t\t\t}\n\t\t};\n\n\t\t// mark the subtask as temporarily idle if there are no initial seed partitions;\n\t\t// once this subtask discovers some partitions and starts collecting records, the subtask's\n\t\t// status will automatically be triggered back to be active.\n\t\tif (subscribedPartitionsToStartOffsets.isEmpty()) {\n\t\t\tsourceContext.markAsTemporarilyIdle();\n\t\t}\n\n\t\t// from this point forward:\n\t\t//   - 'snapshotState' will draw offsets from the fetcher,\n\t\t//     instead of being built from `subscribedPartitionsToStartOffsets`\n\t\t//   - 'notifyCheckpointComplete' will start to do work (i.e. commit offsets to\n\t\t//     Kafka through the fetcher, if configured to do so)\n\t\tthis.kafkaFetcher = createFetcher(\n\t\t\t\tsourceContext,\n\t\t\t\tsubscribedPartitionsToStartOffsets,\n\t\t\t\tperiodicWatermarkAssigner,\n\t\t\t\tpunctuatedWatermarkAssigner,\n\t\t\t\t(StreamingRuntimeContext) getRuntimeContext(),\n\t\t\t\toffsetCommitMode,\n\t\t\t\tgetRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n\t\t\t\tuseMetrics);\n\n\t\tif (!running) {\n\t\t\treturn;\n\t\t}\n\n\t\t// depending on whether we were restored with the current state version (1.3),\n\t\t// remaining logic branches off into 2 paths:\n\t\t//  1) New state - partition discovery loop executed as separate thread, with this\n\t\t//                 thread running the main fetcher loop\n\t\t//  2) Old state - partition discovery is disabled and only the main fetcher loop is executed\n\n\t\tif (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {\n\t\t\tfinal AtomicReference<Exception> discoveryLoopErrorRef = new AtomicReference<>();\n\t\t\tthis.discoveryLoopThread = new Thread(new Runnable() {\n\t\t\t\t@Override\n\t\t\t\tpublic void run() {\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// --------------------- partition discovery loop ---------------------\n\n\t\t\t\t\t\tList<KafkaTopicPartition> discoveredPartitions;\n\n\t\t\t\t\t\t// throughout the loop, we always eagerly check if we are still running before\n\t\t\t\t\t\t// performing the next operation, so that we can escape the loop as soon as possible\n\n\t\t\t\t\t\twhile (running) {\n\t\t\t\t\t\t\tif (LOG.isDebugEnabled()) {\n\t\t\t\t\t\t\t\tLOG.debug(\"Consumer subtask {} is trying to discover new partitions ...\", getRuntimeContext().getIndexOfThisSubtask());\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tdiscoveredPartitions = partitionDiscoverer.discoverPartitions();\n\t\t\t\t\t\t\t} catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {\n\t\t\t\t\t\t\t\t// the partition discoverer may have been closed or woken up before or during the discovery;\n\t\t\t\t\t\t\t\t// this would only happen if the consumer was canceled; simply escape the loop\n\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// no need to add the discovered partitions if we were closed during the meantime\n\t\t\t\t\t\t\tif (running && !discoveredPartitions.isEmpty()) {\n\t\t\t\t\t\t\t\tkafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n\t\t\t\t\t\t\t}\n\n\t\t\t\t\t\t\t// do not waste any time sleeping if we're not running anymore\n\t\t\t\t\t\t\tif (running && discoveryIntervalMillis != 0) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tThread.sleep(discoveryIntervalMillis);\n\t\t\t\t\t\t\t\t} catch (InterruptedException iex) {\n\t\t\t\t\t\t\t\t\t// may be interrupted if the consumer was canceled midway; simply escape the loop\n\t\t\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t} catch (Exception e) {\n\t\t\t\t\t\tdiscoveryLoopErrorRef.set(e);\n\t\t\t\t\t} finally {\n\t\t\t\t\t\t// calling cancel will also let the fetcher loop escape\n\t\t\t\t\t\t// (if not running, cancel() was already called)\n\t\t\t\t\t\tif (running) {\n\t\t\t\t\t\t\tcancel();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\n\t\t\tdiscoveryLoopThread.start();\n\t\t\tkafkaFetcher.runFetchLoop();\n\n\t\t\t// --------------------------------------------------------------------\n\n\t\t\t// make sure that the partition discoverer is properly closed\n\t\t\tpartitionDiscoverer.close();\n\t\t\tdiscoveryLoopThread.join();\n\n\t\t\t// rethrow any fetcher errors\n\t\t\tfinal Exception discoveryLoopError = discoveryLoopErrorRef.get();\n\t\t\tif (discoveryLoopError != null) {\n\t\t\t\tthrow new RuntimeException(discoveryLoopError);\n\t\t\t}\n\t\t} else {\n\t\t\t// won't be using the discoverer\n\t\t\tpartitionDiscoverer.close();\n\n\t\t\tkafkaFetcher.runFetchLoop();\n\t\t}\n\t}"
        ]
    ],
    "c531486288caf5241cdf7f0f00f087f3ce82239f": [
        [
            "KafkaJsonTableSourceFactoryTestBase::testTableSource(FormatDescriptor)",
            "  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94 -\n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115 -\n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138 -\n 139  \n 140  \n 141  \n 142  \n 143  \n 144  ",
            "\tprivate void testTableSource(FormatDescriptor format) {\n\t\t// construct table source using a builder\n\n\t\tfinal Map<String, String> tableJsonMapping = new HashMap<>();\n\t\ttableJsonMapping.put(\"fruit-name\", \"name\");\n\t\ttableJsonMapping.put(\"count\", \"count\");\n\t\ttableJsonMapping.put(\"event-time\", \"time\");\n\n\t\tfinal Properties props = new Properties();\n\t\tprops.put(\"group.id\", \"test-group\");\n\t\tprops.put(\"bootstrap.servers\", \"localhost:1234\");\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 0), 100L);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 1), 123L);\n\n\t\tfinal KafkaTableSource builderSource = builder()\n\t\t\t\t.forJsonSchema(TableSchema.fromTypeInfo(JsonSchemaConverter.convert(JSON_SCHEMA)))\n\t\t\t\t.failOnMissingField(true)\n\t\t\t\t.withTableToJsonMapping(tableJsonMapping)\n\t\t\t\t.withKafkaProperties(props)\n\t\t\t\t.forTopic(TOPIC)\n\t\t\t\t.fromSpecificOffsets(specificOffsets)\n\t\t\t\t.withSchema(\n\t\t\t\t\tTableSchema.builder()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING)\n\t\t\t\t\t\t.field(\"count\", Types.BIG_INT)\n\t\t\t\t\t\t.field(\"event-time\", Types.BIG_DEC)\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.build())\n\t\t\t\t.withProctimeAttribute(\"proc-time\")\n\t\t\t\t.build();\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(0, 100L);\n\t\toffsets.put(1, 123L);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(version())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(props)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(format)\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING).from(\"name\")\n\t\t\t\t\t\t.field(\"count\", Types.BIG_INT) // no from so it must match with the input\n\t\t\t\t\t\t.field(\"event-time\", Types.BIG_DEC).from(\"time\")\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP).proctime());\n\n\t\tfinal TableSource<?> factorySource = TableSourceFactoryService.findAndCreateTableSource(testDesc);\n\n\t\tassertEquals(builderSource, factorySource);\n\t}",
            "  93  \n  94  \n  95  \n  96  \n  97 +\n  98  \n  99  \n 100 +\n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121 +\n 122  \n 123  \n 124  \n 125 +\n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 +\n 146 +\n 147  \n 148  \n 149  \n 150  \n 151  \n 152  ",
            "\tprivate void testTableSource(FormatDescriptor format) {\n\t\t// construct table source using a builder\n\n\t\tfinal Map<String, String> tableJsonMapping = new HashMap<>();\n\t\ttableJsonMapping.put(\"name\", \"name\");\n\t\ttableJsonMapping.put(\"fruit-name\", \"name\");\n\t\ttableJsonMapping.put(\"count\", \"count\");\n\t\ttableJsonMapping.put(\"time\", \"time\");\n\n\t\tfinal Properties props = new Properties();\n\t\tprops.put(\"group.id\", \"test-group\");\n\t\tprops.put(\"bootstrap.servers\", \"localhost:1234\");\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 0), 100L);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, 1), 123L);\n\n\t\tfinal KafkaTableSource builderSource = builder()\n\t\t\t\t.forJsonSchema(TableSchema.fromTypeInfo(JsonSchemaConverter.convert(JSON_SCHEMA)))\n\t\t\t\t.failOnMissingField(true)\n\t\t\t\t.withTableToJsonMapping(tableJsonMapping)\n\t\t\t\t.withKafkaProperties(props)\n\t\t\t\t.forTopic(TOPIC)\n\t\t\t\t.fromSpecificOffsets(specificOffsets)\n\t\t\t\t.withSchema(\n\t\t\t\t\tTableSchema.builder()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING)\n\t\t\t\t\t\t.field(\"count\", Types.BIG_INT)\n\t\t\t\t\t\t.field(\"event-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP)\n\t\t\t\t\t\t.build())\n\t\t\t\t.withProctimeAttribute(\"proc-time\")\n\t\t\t\t.withRowtimeAttribute(\"event-time\", new ExistingField(\"time\"), PreserveWatermarks.INSTANCE())\n\t\t\t\t.build();\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(0, 100L);\n\t\toffsets.put(1, 123L);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(version())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(props)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(format)\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t\t.field(\"fruit-name\", Types.STRING).from(\"name\")\n\t\t\t\t\t\t.field(\"count\", Types.BIG_INT) // no from so it must match with the input\n\t\t\t\t\t\t.field(\"event-time\", Types.SQL_TIMESTAMP).rowtime(\n\t\t\t\t\t\t\tnew Rowtime().timestampsFromField(\"time\").watermarksFromSource())\n\t\t\t\t\t\t.field(\"proc-time\", Types.SQL_TIMESTAMP).proctime());\n\n\t\tfinal TableSource<?> factorySource = TableSourceFactoryService.findAndCreateTableSource(testDesc);\n\n\t\tassertEquals(builderSource, factorySource);\n\t}"
        ]
    ],
    "a0c17d94072ec6e127ea3cb3c507a595283b9b87": [
        [
            "StateMachineExample::main(String)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 -\n 112  ",
            "\t/**\n\t * Main entry point for the program.\n\t *\n\t * @param args The command line arguments.\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// ---- print some usage help ----\n\n\t\tSystem.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]\");\n\t\tSystem.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]\");\n\t\tSystem.out.println();\n\n\t\t// ---- determine whether to use the built-in source, or read from Kafka ----\n\n\t\tfinal SourceFunction<Event> source;\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (params.has(\"kafka-topic\")) {\n\t\t\t// set up the Kafka reader\n\t\t\tString kafkaTopic = params.get(\"kafka-topic\");\n\t\t\tString brokers = params.get(\"brokers\", \"localhost:9092\");\n\n\t\t\tSystem.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers);\n\t\t\tSystem.out.println();\n\n\t\t\tProperties kafkaProps = new Properties();\n\t\t\tkafkaProps.setProperty(\"bootstrap.servers\", brokers);\n\n\t\t\tFlinkKafkaConsumer010<Event> kafka = new FlinkKafkaConsumer010<>(kafkaTopic, new EventDeSerializer(), kafkaProps);\n\t\t\tkafka.setStartFromLatest();\n\t\t\tkafka.setCommitOffsetsOnCheckpoints(false);\n\t\t\tsource = kafka;\n\t\t}\n\t\telse {\n\t\t\tdouble errorRate = params.getDouble(\"error-rate\", 0.0);\n\t\t\tint sleep = params.getInt(\"sleep\", 1);\n\n\t\t\tSystem.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep);\n\t\t\tSystem.out.println();\n\n\t\t\tsource = new EventsGeneratorSource(errorRate, sleep);\n\t\t}\n\n\t\t// ---- main program ----\n\n\t\t// create the environment to create streams and configure execution\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<Event> events = env.addSource(source);\n\n\t\tDataStream<Alert> alerts = events\n\t\t\t\t// partition on the address to make sure equal addresses\n\t\t\t\t// end up in the same state machine flatMap function\n\t\t\t\t.keyBy(Event::sourceAddress)\n\n\t\t\t\t// the function that evaluates the state machine over the sequence of events\n\t\t\t\t.flatMap(new StateMachineMapper());\n\n\t\t// output the alerts to std-out\n\t\talerts.print();\n\n\t\t// trigger program execution\n\t\tenv.execute();\n\t}",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111 +\n 112  ",
            "\t/**\n\t * Main entry point for the program.\n\t *\n\t * @param args The command line arguments.\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// ---- print some usage help ----\n\n\t\tSystem.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]\");\n\t\tSystem.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]\");\n\t\tSystem.out.println();\n\n\t\t// ---- determine whether to use the built-in source, or read from Kafka ----\n\n\t\tfinal SourceFunction<Event> source;\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (params.has(\"kafka-topic\")) {\n\t\t\t// set up the Kafka reader\n\t\t\tString kafkaTopic = params.get(\"kafka-topic\");\n\t\t\tString brokers = params.get(\"brokers\", \"localhost:9092\");\n\n\t\t\tSystem.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers);\n\t\t\tSystem.out.println();\n\n\t\t\tProperties kafkaProps = new Properties();\n\t\t\tkafkaProps.setProperty(\"bootstrap.servers\", brokers);\n\n\t\t\tFlinkKafkaConsumer010<Event> kafka = new FlinkKafkaConsumer010<>(kafkaTopic, new EventDeSerializer(), kafkaProps);\n\t\t\tkafka.setStartFromLatest();\n\t\t\tkafka.setCommitOffsetsOnCheckpoints(false);\n\t\t\tsource = kafka;\n\t\t}\n\t\telse {\n\t\t\tdouble errorRate = params.getDouble(\"error-rate\", 0.0);\n\t\t\tint sleep = params.getInt(\"sleep\", 1);\n\n\t\t\tSystem.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep);\n\t\t\tSystem.out.println();\n\n\t\t\tsource = new EventsGeneratorSource(errorRate, sleep);\n\t\t}\n\n\t\t// ---- main program ----\n\n\t\t// create the environment to create streams and configure execution\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<Event> events = env.addSource(source);\n\n\t\tDataStream<Alert> alerts = events\n\t\t\t\t// partition on the address to make sure equal addresses\n\t\t\t\t// end up in the same state machine flatMap function\n\t\t\t\t.keyBy(Event::sourceAddress)\n\n\t\t\t\t// the function that evaluates the state machine over the sequence of events\n\t\t\t\t.flatMap(new StateMachineMapper());\n\n\t\t// output the alerts to std-out\n\t\talerts.print();\n\n\t\t// trigger program execution\n\t\tenv.execute(\"State machine job\");\n\t}"
        ]
    ],
    "a666455c98c269c63373e991c6ca2751e132a7c8": [
        [
            "StateMachineExample::main(String)",
            "  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  \n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95 -\n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108 -\n 109  \n 110  \n 111  \n 112  ",
            "\t/**\n\t * Main entry point for the program.\n\t *\n\t * @param args The command line arguments.\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// ---- print some usage help ----\n\n\t\tSystem.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]\");\n\t\tSystem.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]\");\n\t\tSystem.out.println();\n\n\t\t// ---- determine whether to use the built-in source, or read from Kafka ----\n\n\t\tfinal SourceFunction<Event> source;\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (params.has(\"kafka-topic\")) {\n\t\t\t// set up the Kafka reader\n\t\t\tString kafkaTopic = params.get(\"kafka-topic\");\n\t\t\tString brokers = params.get(\"brokers\", \"localhost:9092\");\n\n\t\t\tSystem.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers);\n\t\t\tSystem.out.println();\n\n\t\t\tProperties kafkaProps = new Properties();\n\t\t\tkafkaProps.setProperty(\"bootstrap.servers\", brokers);\n\n\t\t\tFlinkKafkaConsumer010<Event> kafka = new FlinkKafkaConsumer010<>(kafkaTopic, new EventDeSerializer(), kafkaProps);\n\t\t\tkafka.setStartFromLatest();\n\t\t\tkafka.setCommitOffsetsOnCheckpoints(false);\n\t\t\tsource = kafka;\n\t\t}\n\t\telse {\n\t\t\tdouble errorRate = params.getDouble(\"error-rate\", 0.0);\n\t\t\tint sleep = params.getInt(\"sleep\", 1);\n\n\t\t\tSystem.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep);\n\t\t\tSystem.out.println();\n\n\t\t\tsource = new EventsGeneratorSource(errorRate, sleep);\n\t\t}\n\n\t\t// ---- main program ----\n\n\t\t// create the environment to create streams and configure execution\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<Event> events = env.addSource(source);\n\n\t\tDataStream<Alert> alerts = events\n\t\t\t\t// partition on the address to make sure equal addresses\n\t\t\t\t// end up in the same state machine flatMap function\n\t\t\t\t.keyBy(Event::sourceAddress)\n\n\t\t\t\t// the function that evaluates the state machine over the sequence of events\n\t\t\t\t.flatMap(new StateMachineMapper());\n\n\t\t// output the alerts to std-out\n\t\talerts.print();\n\n\t\t// trigger program execution\n\t\tenv.execute(\"State machine job\");\n\t}",
            "  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62 +\n  63 +\n  64 +\n  65 +\n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104 +\n 105 +\n 106 +\n 107 +\n 108 +\n 109 +\n 110 +\n 111 +\n 112 +\n 113 +\n 114 +\n 115 +\n 116 +\n 117 +\n 118 +\n 119 +\n 120 +\n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133 +\n 134 +\n 135 +\n 136 +\n 137 +\n 138 +\n 139 +\n 140  \n 141  \n 142  \n 143  ",
            "\t/**\n\t * Main entry point for the program.\n\t *\n\t * @param args The command line arguments.\n\t */\n\tpublic static void main(String[] args) throws Exception {\n\n\t\t// ---- print some usage help ----\n\n\t\tSystem.out.println(\"Usage with built-in data generator: StateMachineExample [--error-rate <probability-of-invalid-transition>] [--sleep <sleep-per-record-in-ms>]\");\n\t\tSystem.out.println(\"Usage with Kafka: StateMachineExample --kafka-topic <topic> [--brokers <brokers>]\");\n\t\tSystem.out.println(\"Options for both the above setups: \");\n\t\tSystem.out.println(\"\\t[--backend <file|rocks>]\");\n\t\tSystem.out.println(\"\\t[--checkpoint-dir <filepath>]\");\n\t\tSystem.out.println(\"\\t[--async-checkpoints <true|false>]\");\n\t\tSystem.out.println(\"\\t[--incremental-checkpoints <true|false>]\");\n\t\tSystem.out.println(\"\\t[--output <filepath> OR null for stdout]\");\n\t\tSystem.out.println();\n\n\t\t// ---- determine whether to use the built-in source, or read from Kafka ----\n\n\t\tfinal SourceFunction<Event> source;\n\t\tfinal ParameterTool params = ParameterTool.fromArgs(args);\n\n\t\tif (params.has(\"kafka-topic\")) {\n\t\t\t// set up the Kafka reader\n\t\t\tString kafkaTopic = params.get(\"kafka-topic\");\n\t\t\tString brokers = params.get(\"brokers\", \"localhost:9092\");\n\n\t\t\tSystem.out.printf(\"Reading from kafka topic %s @ %s\\n\", kafkaTopic, brokers);\n\t\t\tSystem.out.println();\n\n\t\t\tProperties kafkaProps = new Properties();\n\t\t\tkafkaProps.setProperty(\"bootstrap.servers\", brokers);\n\n\t\t\tFlinkKafkaConsumer010<Event> kafka = new FlinkKafkaConsumer010<>(kafkaTopic, new EventDeSerializer(), kafkaProps);\n\t\t\tkafka.setStartFromLatest();\n\t\t\tkafka.setCommitOffsetsOnCheckpoints(false);\n\t\t\tsource = kafka;\n\t\t}\n\t\telse {\n\t\t\tdouble errorRate = params.getDouble(\"error-rate\", 0.0);\n\t\t\tint sleep = params.getInt(\"sleep\", 1);\n\n\t\t\tSystem.out.printf(\"Using standalone source with error rate %f and sleep delay %s millis\\n\", errorRate, sleep);\n\t\t\tSystem.out.println();\n\n\t\t\tsource = new EventsGeneratorSource(errorRate, sleep);\n\t\t}\n\n\t\t// ---- main program ----\n\n\t\t// create the environment to create streams and configure execution\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.enableCheckpointing(2000L);\n\n\t\tfinal String stateBackend = params.get(\"backend\", \"memory\");\n\t\tif (\"file\".equals(stateBackend)) {\n\t\t\tfinal String checkpointDir = params.get(\"checkpoint-dir\");\n\t\t\tboolean asyncCheckpoints = params.getBoolean(\"async-checkpoints\", false);\n\t\t\tenv.setStateBackend(new FsStateBackend(checkpointDir, asyncCheckpoints));\n\t\t} else if (\"rocks\".equals(stateBackend)) {\n\t\t\tfinal String checkpointDir = params.get(\"checkpoint-dir\");\n\t\t\tboolean incrementalCheckpoints = params.getBoolean(\"incremental-checkpoints\", false);\n\t\t\tenv.setStateBackend(new RocksDBStateBackend(checkpointDir, incrementalCheckpoints));\n\t\t}\n\n\t\tfinal String outputFile = params.get(\"output\");\n\n\t\t// make parameters available in the web interface\n\t\tenv.getConfig().setGlobalJobParameters(params);\n\n\t\tDataStream<Event> events = env.addSource(source);\n\n\t\tDataStream<Alert> alerts = events\n\t\t\t\t// partition on the address to make sure equal addresses\n\t\t\t\t// end up in the same state machine flatMap function\n\t\t\t\t.keyBy(Event::sourceAddress)\n\n\t\t\t\t// the function that evaluates the state machine over the sequence of events\n\t\t\t\t.flatMap(new StateMachineMapper());\n\n\t\t// output the alerts to std-out\n\t\tif (outputFile == null) {\n\t\t\talerts.print();\n\t\t} else {\n\t\t\talerts\n\t\t\t\t.writeAsText(outputFile, FileSystem.WriteMode.OVERWRITE)\n\t\t\t\t.setParallelism(1);\n\t\t}\n\n\t\t// trigger program execution\n\t\tenv.execute(\"State machine job\");\n\t}"
        ]
    ],
    "71095dcb098c5b03a656a1f3bb48634294e537bb": [
        [
            "Elasticsearch5SinkExample::main(String)",
            "  43  \n  44  \n  45  \n  46  \n  47 -\n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58 -\n  59 -\n  60 -\n  61 -\n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 2) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, 20).map(new MapFunction<Long, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\treturn \"message #\" + value;\n\t\t\t}\n\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<InetSocketAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>() {\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch5.x end to end sink test example\");\n\t}",
            "  44  \n  45  \n  46  \n  47  \n  48 +\n  49  \n  50 +\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 +\n  59 +\n  60 +\n  61 +\n  62 +\n  63 +\n  64 +\n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 3) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --numRecords <numRecords> --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, parameterTool.getInt(\"numRecords\") - 1)\n\t\t\t.map(new MapFunction<Long, String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\t\treturn \"message #\" + value;\n\t\t\t\t}\n\t\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<InetSocketAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>() {\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch5.x end to end sink test example\");\n\t}"
        ],
        [
            "Elasticsearch1SinkExample::main(String)",
            "  44  \n  45  \n  46  \n  47  \n  48 -\n  49  \n  50 -\n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58 -\n  59 -\n  60 -\n  61 -\n  62 -\n  63 -\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 2) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, 20).map(new MapFunction<Long, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\treturn \"message # \" + value;\n\t\t\t}\n\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<TransportAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketTransportAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>() {\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch1.x end to end sink test example\");\n\t}",
            "  45  \n  46  \n  47  \n  48  \n  49 +\n  50  \n  51 +\n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59 +\n  60 +\n  61 +\n  62 +\n  63 +\n  64 +\n  65 +\n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 3) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --numRecords <numRecords> --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, parameterTool.getInt(\"numRecords\") - 1)\n\t\t\t.map(new MapFunction<Long, String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\t\treturn \"message # \" + value;\n\t\t\t\t}\n\t\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<TransportAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketTransportAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>() {\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch1.x end to end sink test example\");\n\t}"
        ],
        [
            "Elasticsearch2SinkExample::main(String)",
            "  43  \n  44  \n  45  \n  46  \n  47 -\n  48  \n  49 -\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 -\n  58 -\n  59 -\n  60 -\n  61 -\n  62 -\n  63  \n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 2) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, 20).map(new MapFunction<Long, String>() {\n\t\t\t@Override\n\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\treturn \"message #\" + value;\n\t\t\t}\n\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<InetSocketAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>(){\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch2.x end to end sink test example\");\n\t}",
            "  43  \n  44  \n  45  \n  46  \n  47 +\n  48  \n  49 +\n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57 +\n  58 +\n  59 +\n  60 +\n  61 +\n  62 +\n  63 +\n  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  \n  79  \n  80  \n  81  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tfinal ParameterTool parameterTool = ParameterTool.fromArgs(args);\n\n\t\tif (parameterTool.getNumberOfParameters() < 3) {\n\t\t\tSystem.out.println(\"Missing parameters!\\n\" +\n\t\t\t\t\"Usage: --numRecords --index <index> --type <type>\");\n\t\t\treturn;\n\t\t}\n\n\t\tfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.getConfig().disableSysoutLogging();\n\t\tenv.enableCheckpointing(5000);\n\n\t\tDataStream<String> source = env.generateSequence(0, parameterTool.getInt(\"numRecords\") - 1)\n\t\t\t.map(new MapFunction<Long, String>() {\n\t\t\t\t@Override\n\t\t\t\tpublic String map(Long value) throws Exception {\n\t\t\t\t\treturn \"message #\" + value;\n\t\t\t\t}\n\t\t\t});\n\n\t\tMap<String, String> userConfig = new HashMap<>();\n\t\tuserConfig.put(\"cluster.name\", \"elasticsearch\");\n\t\t// This instructs the sink to emit after every element, otherwise they would be buffered\n\t\tuserConfig.put(ElasticsearchSink.CONFIG_KEY_BULK_FLUSH_MAX_ACTIONS, \"1\");\n\n\t\tList<InetSocketAddress> transports = new ArrayList<>();\n\t\ttransports.add(new InetSocketAddress(InetAddress.getByName(\"127.0.0.1\"), 9300));\n\n\t\tsource.addSink(new ElasticsearchSink<>(userConfig, transports, new ElasticsearchSinkFunction<String>(){\n\t\t\t@Override\n\t\t\tpublic void process(String element, RuntimeContext ctx, org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer indexer) {\n\t\t\t\tindexer.add(createIndexRequest(element, parameterTool));\n\t\t\t}\n\t\t}));\n\n\t\tenv.execute(\"Elasticsearch2.x end to end sink test example\");\n\t}"
        ]
    ],
    "cfd0206b39b08691b832ea6324e02a5bd3a1533e": [
        [
            "Execution::getMinStateRetention()",
            "  77  \n  78 -\n  79  ",
            "\tpublic long getMinStateRetention() {\n\t\treturn Long.parseLong(properties.getOrDefault(PropertyStrings.EXECUTION_MIN_STATE_RETENTION, Long.toString(Long.MIN_VALUE)));\n\t}",
            "  77  \n  78 +\n  79  ",
            "\tpublic long getMinStateRetention() {\n\t\treturn Long.parseLong(properties.getOrDefault(PropertyStrings.EXECUTION_MIN_STATE_RETENTION, Long.toString(0)));\n\t}"
        ],
        [
            "Execution::getMaxStateRetention()",
            "  81  \n  82 -\n  83  ",
            "\tpublic long getMaxStateRetention() {\n\t\treturn Long.parseLong(properties.getOrDefault(PropertyStrings.EXECUTION_MAX_STATE_RETENTION, Long.toString(Long.MIN_VALUE)));\n\t}",
            "  81  \n  82 +\n  83  ",
            "\tpublic long getMaxStateRetention() {\n\t\treturn Long.parseLong(properties.getOrDefault(PropertyStrings.EXECUTION_MAX_STATE_RETENTION, Long.toString(0)));\n\t}"
        ]
    ],
    "abbb89059f2a83705f41e405da14073800fb1870": [
        [
            "Environment::setTables(List)",
            "  64  \n  65  \n  66  \n  67 -\n  68  \n  69  \n  70 -\n  71  \n  72  \n  73  \n  74  \n  75  \n  76 -\n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  ",
            "\tpublic void setTables(List<Map<String, Object>> tables) {\n\t\tthis.tables = new HashMap<>(tables.size());\n\t\ttables.forEach(config -> {\n\t\t\tif (!config.containsKey(NAME)) {\n\t\t\t\tthrow new SqlClientException(\"The 'name' attribute of a table is missing.\");\n\t\t\t}\n\t\t\tfinal Object nameObject = config.get(NAME);\n\t\t\tif (nameObject == null || !(nameObject instanceof String) || ((String) nameObject).length() <= 0) {\n\t\t\t\tthrow new SqlClientException(\"Invalid table name '\" + nameObject + \"'.\");\n\t\t\t}\n\t\t\tfinal String name = (String) nameObject;\n\t\t\tfinal Map<String, Object> properties = new HashMap<>(config);\n\t\t\tproperties.remove(NAME);\n\n\t\t\tif (this.tables.containsKey(name)) {\n\t\t\t\tthrow new SqlClientException(\"Duplicate table name '\" + name + \"'.\");\n\t\t\t}\n\t\t\tthis.tables.put(name, createTableDescriptor(name, properties));\n\t\t});\n\t}",
            "  65  \n  66  \n  67  \n  68 +\n  69  \n  70  \n  71 +\n  72  \n  73  \n  74  \n  75  \n  76  \n  77 +\n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  ",
            "\tpublic void setTables(List<Map<String, Object>> tables) {\n\t\tthis.tables = new HashMap<>(tables.size());\n\t\ttables.forEach(config -> {\n\t\t\tif (!config.containsKey(TABLE_NAME)) {\n\t\t\t\tthrow new SqlClientException(\"The 'name' attribute of a table is missing.\");\n\t\t\t}\n\t\t\tfinal Object nameObject = config.get(TABLE_NAME);\n\t\t\tif (nameObject == null || !(nameObject instanceof String) || ((String) nameObject).length() <= 0) {\n\t\t\t\tthrow new SqlClientException(\"Invalid table name '\" + nameObject + \"'.\");\n\t\t\t}\n\t\t\tfinal String name = (String) nameObject;\n\t\t\tfinal Map<String, Object> properties = new HashMap<>(config);\n\t\t\tproperties.remove(TABLE_NAME);\n\n\t\t\tif (this.tables.containsKey(name)) {\n\t\t\t\tthrow new SqlClientException(\"Duplicate table name '\" + name + \"'.\");\n\t\t\t}\n\t\t\tthis.tables.put(name, createTableDescriptor(name, properties));\n\t\t});\n\t}"
        ],
        [
            "JsonRowFormatFactoryTest::testSchemaSerializationSchema(Map)",
            " 118  \n 119 -\n 120  \n 121  \n 122  \n 123  \n 124  ",
            "\tprivate void testSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFormatFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema expected1 = new JsonRowSerializationSchema(SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}",
            " 118  \n 119 +\n 120  \n 121  \n 122  \n 123  \n 124  ",
            "\tprivate void testSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema expected1 = new JsonRowSerializationSchema(SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}"
        ],
        [
            "JsonRowFormatFactoryTest::testSchemaDeserializationSchema(Map)",
            " 109  \n 110 -\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "\tprivate void testSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFormatFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal JsonRowDeserializationSchema expected2 = new JsonRowDeserializationSchema(SCHEMA);\n\t\texpected2.setFailOnMissingField(false);\n\t\tassertEquals(expected2, actual2);\n\t}",
            " 109  \n 110 +\n 111  \n 112  \n 113  \n 114  \n 115  \n 116  ",
            "\tprivate void testSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal JsonRowDeserializationSchema expected2 = new JsonRowDeserializationSchema(SCHEMA);\n\t\texpected2.setFailOnMissingField(false);\n\t\tassertEquals(expected2, actual2);\n\t}"
        ],
        [
            "KafkaTableSourceFactoryTestBase::testTableSource()",
            "  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145 -\n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  ",
            "\t@Test\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void testTableSource() {\n\n\t\t// prepare parameters for Kafka table source\n\n\t\tfinal TableSchema schema = TableSchema.builder()\n\t\t\t.field(FRUIT_NAME, Types.STRING())\n\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.build();\n\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors = Collections.singletonList(\n\t\t\tnew RowtimeAttributeDescriptor(EVENT_TIME, new ExistingField(TIME), new AscendingTimestamps()));\n\n\t\tfinal Map<String, String> fieldMapping = new HashMap<>();\n\t\tfieldMapping.put(FRUIT_NAME, NAME);\n\t\tfieldMapping.put(COUNT, COUNT);\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_0), OFFSET_0);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_1), OFFSET_1);\n\n\t\tfinal TestDeserializationSchema deserializationSchema = new TestDeserializationSchema(\n\t\t\tTableSchema.builder()\n\t\t\t\t.field(NAME, Types.STRING())\n\t\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t\t.field(TIME, Types.SQL_TIMESTAMP())\n\t\t\t\t.build()\n\t\t\t\t.toRowType()\n\t\t);\n\n\t\tfinal StartupMode startupMode = StartupMode.SPECIFIC_OFFSETS;\n\n\t\tfinal KafkaTableSource expected = getExpectedKafkaTableSource(\n\t\t\tschema,\n\t\t\tOptional.of(PROC_TIME),\n\t\t\trowtimeAttributeDescriptors,\n\t\t\tfieldMapping,\n\t\t\tTOPIC,\n\t\t\tKAFKA_PROPERTIES,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(PARTITION_0, OFFSET_0);\n\t\toffsets.put(PARTITION_1, OFFSET_1);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(getKafkaVersion())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(KAFKA_PROPERTIES)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(new TestTableFormat())\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t.field(FRUIT_NAME, Types.STRING()).from(NAME)\n\t\t\t\t\t.field(COUNT, Types.DECIMAL()) // no from so it must match with the input\n\t\t\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP()).rowtime(\n\t\t\t\t\t\tnew Rowtime().timestampsFromField(TIME).watermarksPeriodicAscending())\n\t\t\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP()).proctime());\n\n\t\tfinal TableSource<?> actualSource = TableSourceFactoryService.findAndCreateTableSource(testDesc);\n\n\t\tassertEquals(expected, actualSource);\n\n\t\t// test Kafka consumer\n\t\tfinal KafkaTableSource actualKafkaSource = (KafkaTableSource) actualSource;\n\t\tfinal StreamExecutionEnvironmentMock mock = new StreamExecutionEnvironmentMock();\n\t\tactualKafkaSource.getDataStream(mock);\n\t\tassertTrue(getExpectedFlinkKafkaConsumer().isAssignableFrom(mock.function.getClass()));\n\t}",
            "  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146 +\n 147 +\n 148 +\n 149  \n 150 +\n 151 +\n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  ",
            "\t@Test\n\t@SuppressWarnings(\"unchecked\")\n\tpublic void testTableSource() {\n\n\t\t// prepare parameters for Kafka table source\n\n\t\tfinal TableSchema schema = TableSchema.builder()\n\t\t\t.field(FRUIT_NAME, Types.STRING())\n\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP())\n\t\t\t.build();\n\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors = Collections.singletonList(\n\t\t\tnew RowtimeAttributeDescriptor(EVENT_TIME, new ExistingField(TIME), new AscendingTimestamps()));\n\n\t\tfinal Map<String, String> fieldMapping = new HashMap<>();\n\t\tfieldMapping.put(FRUIT_NAME, NAME);\n\t\tfieldMapping.put(COUNT, COUNT);\n\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_0), OFFSET_0);\n\t\tspecificOffsets.put(new KafkaTopicPartition(TOPIC, PARTITION_1), OFFSET_1);\n\n\t\tfinal TestDeserializationSchema deserializationSchema = new TestDeserializationSchema(\n\t\t\tTableSchema.builder()\n\t\t\t\t.field(NAME, Types.STRING())\n\t\t\t\t.field(COUNT, Types.DECIMAL())\n\t\t\t\t.field(TIME, Types.SQL_TIMESTAMP())\n\t\t\t\t.build()\n\t\t\t\t.toRowType()\n\t\t);\n\n\t\tfinal StartupMode startupMode = StartupMode.SPECIFIC_OFFSETS;\n\n\t\tfinal KafkaTableSource expected = getExpectedKafkaTableSource(\n\t\t\tschema,\n\t\t\tOptional.of(PROC_TIME),\n\t\t\trowtimeAttributeDescriptors,\n\t\t\tfieldMapping,\n\t\t\tTOPIC,\n\t\t\tKAFKA_PROPERTIES,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\n\t\t// construct table source using descriptors and table source factory\n\n\t\tfinal Map<Integer, Long> offsets = new HashMap<>();\n\t\toffsets.put(PARTITION_0, OFFSET_0);\n\t\toffsets.put(PARTITION_1, OFFSET_1);\n\n\t\tfinal TestTableSourceDescriptor testDesc = new TestTableSourceDescriptor(\n\t\t\t\tnew Kafka()\n\t\t\t\t\t.version(getKafkaVersion())\n\t\t\t\t\t.topic(TOPIC)\n\t\t\t\t\t.properties(KAFKA_PROPERTIES)\n\t\t\t\t\t.startFromSpecificOffsets(offsets))\n\t\t\t.addFormat(new TestTableFormat())\n\t\t\t.addSchema(\n\t\t\t\tnew Schema()\n\t\t\t\t\t.field(FRUIT_NAME, Types.STRING()).from(NAME)\n\t\t\t\t\t.field(COUNT, Types.DECIMAL()) // no from so it must match with the input\n\t\t\t\t\t.field(EVENT_TIME, Types.SQL_TIMESTAMP()).rowtime(\n\t\t\t\t\t\tnew Rowtime().timestampsFromField(TIME).watermarksPeriodicAscending())\n\t\t\t\t\t.field(PROC_TIME, Types.SQL_TIMESTAMP()).proctime());\n\t\tfinal DescriptorProperties descriptorProperties = new DescriptorProperties(true);\n\t\ttestDesc.addProperties(descriptorProperties);\n\t\tfinal Map<String, String> propertiesMap = descriptorProperties.asMap();\n\n\t\tfinal TableSource<?> actualSource = TableFactoryService.find(TableSourceFactory.class, testDesc)\n\t\t\t.createTableSource(propertiesMap);\n\n\t\tassertEquals(expected, actualSource);\n\n\t\t// test Kafka consumer\n\t\tfinal KafkaTableSource actualKafkaSource = (KafkaTableSource) actualSource;\n\t\tfinal StreamExecutionEnvironmentMock mock = new StreamExecutionEnvironmentMock();\n\t\tactualKafkaSource.getDataStream(mock);\n\t\tassertTrue(getExpectedFlinkKafkaConsumer().isAssignableFrom(mock.function.getClass()));\n\t}"
        ],
        [
            "KafkaTableSourceFactory::createTableSource(Map)",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 -\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "\t@Override\n\tpublic TableSource<Row> createTableSource(Map<String, String> properties) {\n\t\tfinal DescriptorProperties params = new DescriptorProperties(true);\n\t\tparams.putProperties(properties);\n\n\t\t// validate\n\t\t// allow Kafka timestamps to be used, watermarks can not be received from source\n\t\tnew SchemaValidator(true, supportsKafkaTimestamps(), false).validate(params);\n\t\tnew KafkaValidator().validate(params);\n\n\t\t// deserialization schema using format discovery\n\t\tfinal DeserializationSchemaFactory<?> formatFactory = TableFormatFactoryService.find(\n\t\t\tDeserializationSchemaFactory.class,\n\t\t\tproperties,\n\t\t\tthis.getClass().getClassLoader());\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tfinal DeserializationSchema<Row> deserializationSchema = (DeserializationSchema<Row>) formatFactory\n\t\t\t.createDeserializationSchema(properties);\n\n\t\t// schema\n\t\tfinal TableSchema schema = params.getTableSchema(SCHEMA());\n\n\t\t// proctime\n\t\tfinal Optional<String> proctimeAttribute = SchemaValidator.deriveProctimeAttribute(params);\n\n\t\t// rowtime\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributes = SchemaValidator.deriveRowtimeAttributes(params);\n\n\t\t// field mapping\n\t\tfinal Map<String, String> fieldMapping = SchemaValidator.deriveFieldMapping(params, Optional.of(schema));\n\n\t\t// properties\n\t\tfinal Properties kafkaProperties = new Properties();\n\t\tfinal List<Map<String, String>> propsList = params.getFixedIndexedProperties(\n\t\t\tCONNECTOR_PROPERTIES,\n\t\t\tArrays.asList(CONNECTOR_PROPERTIES_KEY, CONNECTOR_PROPERTIES_VALUE));\n\t\tpropsList.forEach(kv -> kafkaProperties.put(\n\t\t\tparams.getString(kv.get(CONNECTOR_PROPERTIES_KEY)),\n\t\t\tparams.getString(kv.get(CONNECTOR_PROPERTIES_VALUE))\n\t\t));\n\n\t\t// topic\n\t\tfinal String topic = params.getString(CONNECTOR_TOPIC);\n\n\t\t// startup mode\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tfinal StartupMode startupMode = params\n\t\t\t.getOptionalString(CONNECTOR_STARTUP_MODE)\n\t\t\t.map(modeString -> {\n\t\t\t\tswitch (modeString) {\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_EARLIEST:\n\t\t\t\t\t\treturn StartupMode.EARLIEST;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_LATEST:\n\t\t\t\t\t\treturn StartupMode.LATEST;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_GROUP_OFFSETS:\n\t\t\t\t\t\treturn StartupMode.GROUP_OFFSETS;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_SPECIFIC_OFFSETS:\n\t\t\t\t\t\tfinal List<Map<String, String>> offsetList = params.getFixedIndexedProperties(\n\t\t\t\t\t\t\tCONNECTOR_SPECIFIC_OFFSETS,\n\t\t\t\t\t\t\tArrays.asList(CONNECTOR_SPECIFIC_OFFSETS_PARTITION, CONNECTOR_SPECIFIC_OFFSETS_OFFSET));\n\t\t\t\t\t\toffsetList.forEach(kv -> {\n\t\t\t\t\t\t\tfinal int partition = params.getInt(kv.get(CONNECTOR_SPECIFIC_OFFSETS_PARTITION));\n\t\t\t\t\t\t\tfinal long offset = params.getLong(kv.get(CONNECTOR_SPECIFIC_OFFSETS_OFFSET));\n\t\t\t\t\t\t\tfinal KafkaTopicPartition topicPartition = new KafkaTopicPartition(topic, partition);\n\t\t\t\t\t\t\tspecificOffsets.put(topicPartition, offset);\n\t\t\t\t\t\t});\n\t\t\t\t\t\treturn StartupMode.SPECIFIC_OFFSETS;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tthrow new TableException(\"Unsupported startup mode. Validator should have checked that.\");\n\t\t\t\t}\n\t\t\t}).orElse(StartupMode.GROUP_OFFSETS);\n\n\t\treturn createKafkaTableSource(\n\t\t\tschema,\n\t\t\tproctimeAttribute,\n\t\t\trowtimeAttributes,\n\t\t\tfieldMapping,\n\t\t\ttopic,\n\t\t\tkafkaProperties,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\t}",
            " 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132 +\n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "\t@Override\n\tpublic TableSource<Row> createTableSource(Map<String, String> properties) {\n\t\tfinal DescriptorProperties params = new DescriptorProperties(true);\n\t\tparams.putProperties(properties);\n\n\t\t// validate\n\t\t// allow Kafka timestamps to be used, watermarks can not be received from source\n\t\tnew SchemaValidator(true, supportsKafkaTimestamps(), false).validate(params);\n\t\tnew KafkaValidator().validate(params);\n\n\t\t// deserialization schema using format discovery\n\t\tfinal DeserializationSchemaFactory<?> formatFactory = TableFactoryService.find(\n\t\t\tDeserializationSchemaFactory.class,\n\t\t\tproperties,\n\t\t\tthis.getClass().getClassLoader());\n\t\t@SuppressWarnings(\"unchecked\")\n\t\tfinal DeserializationSchema<Row> deserializationSchema = (DeserializationSchema<Row>) formatFactory\n\t\t\t.createDeserializationSchema(properties);\n\n\t\t// schema\n\t\tfinal TableSchema schema = params.getTableSchema(SCHEMA());\n\n\t\t// proctime\n\t\tfinal Optional<String> proctimeAttribute = SchemaValidator.deriveProctimeAttribute(params);\n\n\t\t// rowtime\n\t\tfinal List<RowtimeAttributeDescriptor> rowtimeAttributes = SchemaValidator.deriveRowtimeAttributes(params);\n\n\t\t// field mapping\n\t\tfinal Map<String, String> fieldMapping = SchemaValidator.deriveFieldMapping(params, Optional.of(schema));\n\n\t\t// properties\n\t\tfinal Properties kafkaProperties = new Properties();\n\t\tfinal List<Map<String, String>> propsList = params.getFixedIndexedProperties(\n\t\t\tCONNECTOR_PROPERTIES,\n\t\t\tArrays.asList(CONNECTOR_PROPERTIES_KEY, CONNECTOR_PROPERTIES_VALUE));\n\t\tpropsList.forEach(kv -> kafkaProperties.put(\n\t\t\tparams.getString(kv.get(CONNECTOR_PROPERTIES_KEY)),\n\t\t\tparams.getString(kv.get(CONNECTOR_PROPERTIES_VALUE))\n\t\t));\n\n\t\t// topic\n\t\tfinal String topic = params.getString(CONNECTOR_TOPIC);\n\n\t\t// startup mode\n\t\tfinal Map<KafkaTopicPartition, Long> specificOffsets = new HashMap<>();\n\t\tfinal StartupMode startupMode = params\n\t\t\t.getOptionalString(CONNECTOR_STARTUP_MODE)\n\t\t\t.map(modeString -> {\n\t\t\t\tswitch (modeString) {\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_EARLIEST:\n\t\t\t\t\t\treturn StartupMode.EARLIEST;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_LATEST:\n\t\t\t\t\t\treturn StartupMode.LATEST;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_GROUP_OFFSETS:\n\t\t\t\t\t\treturn StartupMode.GROUP_OFFSETS;\n\n\t\t\t\t\tcase KafkaValidator.CONNECTOR_STARTUP_MODE_VALUE_SPECIFIC_OFFSETS:\n\t\t\t\t\t\tfinal List<Map<String, String>> offsetList = params.getFixedIndexedProperties(\n\t\t\t\t\t\t\tCONNECTOR_SPECIFIC_OFFSETS,\n\t\t\t\t\t\t\tArrays.asList(CONNECTOR_SPECIFIC_OFFSETS_PARTITION, CONNECTOR_SPECIFIC_OFFSETS_OFFSET));\n\t\t\t\t\t\toffsetList.forEach(kv -> {\n\t\t\t\t\t\t\tfinal int partition = params.getInt(kv.get(CONNECTOR_SPECIFIC_OFFSETS_PARTITION));\n\t\t\t\t\t\t\tfinal long offset = params.getLong(kv.get(CONNECTOR_SPECIFIC_OFFSETS_OFFSET));\n\t\t\t\t\t\t\tfinal KafkaTopicPartition topicPartition = new KafkaTopicPartition(topic, partition);\n\t\t\t\t\t\t\tspecificOffsets.put(topicPartition, offset);\n\t\t\t\t\t\t});\n\t\t\t\t\t\treturn StartupMode.SPECIFIC_OFFSETS;\n\t\t\t\t\tdefault:\n\t\t\t\t\t\tthrow new TableException(\"Unsupported startup mode. Validator should have checked that.\");\n\t\t\t\t}\n\t\t\t}).orElse(StartupMode.GROUP_OFFSETS);\n\n\t\treturn createKafkaTableSource(\n\t\t\tschema,\n\t\t\tproctimeAttribute,\n\t\t\trowtimeAttributes,\n\t\t\tfieldMapping,\n\t\t\ttopic,\n\t\t\tkafkaProperties,\n\t\t\tdeserializationSchema,\n\t\t\tstartupMode,\n\t\t\tspecificOffsets);\n\t}"
        ],
        [
            "AvroRowFormatFactoryTest::testAvroSchemaSerializationSchema(Map)",
            "  89  \n  90 -\n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tprivate void testAvroSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFormatFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new AvroRowSerializationSchema(AVRO_SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}",
            "  89  \n  90 +\n  91  \n  92  \n  93  \n  94  \n  95  ",
            "\tprivate void testAvroSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new AvroRowSerializationSchema(AVRO_SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}"
        ],
        [
            "AvroRowFormatFactoryTest::testRecordClassSerializationSchema(Map)",
            "  65  \n  66 -\n  67  \n  68  \n  69  \n  70  \n  71  ",
            "\tprivate void testRecordClassSerializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFormatFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal AvroRowDeserializationSchema expected2 = new AvroRowDeserializationSchema(AVRO_SPECIFIC_RECORD);\n\t\tassertEquals(expected2, actual2);\n\t}",
            "  65  \n  66 +\n  67  \n  68  \n  69  \n  70  \n  71  ",
            "\tprivate void testRecordClassSerializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal AvroRowDeserializationSchema expected2 = new AvroRowDeserializationSchema(AVRO_SPECIFIC_RECORD);\n\t\tassertEquals(expected2, actual2);\n\t}"
        ],
        [
            "AvroRowFormatFactoryTest::testRecordClassDeserializationSchema(Map)",
            "  73  \n  74 -\n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\tprivate void testRecordClassDeserializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFormatFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new AvroRowSerializationSchema(AVRO_SPECIFIC_RECORD);\n\t\tassertEquals(expected1, actual1);\n\t}",
            "  73  \n  74 +\n  75  \n  76  \n  77  \n  78  \n  79  ",
            "\tprivate void testRecordClassDeserializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new AvroRowSerializationSchema(AVRO_SPECIFIC_RECORD);\n\t\tassertEquals(expected1, actual1);\n\t}"
        ],
        [
            "Environment::createTableDescriptor(String,Map)",
            " 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212 -\n 213  \n 214  \n 215  \n 216 -\n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  ",
            "\t/**\n\t * Creates a table descriptor from a YAML config map.\n\t *\n\t * @param name name of the table\n\t * @param config YAML config map\n\t * @return table descriptor describing a source, sink, or both\n\t */\n\tprivate static TableDescriptor createTableDescriptor(String name, Map<String, Object> config) {\n\t\tfinal Object typeObject = config.get(TableDescriptorValidator.TABLE_TYPE());\n\t\tif (typeObject == null || !(typeObject instanceof String)) {\n\t\t\tthrow new SqlClientException(\"Invalid 'type' attribute for table '\" + name + \"'.\");\n\t\t}\n\t\tfinal String type = (String) config.get(TableDescriptorValidator.TABLE_TYPE());\n\t\tfinal Map<String, String> normalizedConfig = ConfigUtil.normalizeYaml(config);\n\t\tif (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SOURCE())) {\n\t\t\treturn new Source(name, normalizedConfig);\n\t\t} else if (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SINK())) {\n\t\t\treturn new Sink(name, normalizedConfig);\n\t\t} else if (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SOURCE_SINK())) {\n\t\t\treturn new SourceSink(name, normalizedConfig);\n\t\t}\n\t\tthrow new SqlClientException(\"Invalid 'type' attribute for table '\" + name + \"'. \" +\n\t\t\t\"Only 'source', 'sink', and 'both' are supported.\");\n\t}",
            " 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213 +\n 214  \n 215  \n 216  \n 217 +\n 218 +\n 219  \n 220  \n 221  \n 222  \n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  ",
            "\t/**\n\t * Creates a table descriptor from a YAML config map.\n\t *\n\t * @param name name of the table\n\t * @param config YAML config map\n\t * @return table descriptor describing a source, sink, or both\n\t */\n\tprivate static TableDescriptor createTableDescriptor(String name, Map<String, Object> config) {\n\t\tfinal Object typeObject = config.get(TABLE_TYPE);\n\t\tif (typeObject == null || !(typeObject instanceof String)) {\n\t\t\tthrow new SqlClientException(\"Invalid 'type' attribute for table '\" + name + \"'.\");\n\t\t}\n\t\tfinal String type = (String) config.get(TABLE_TYPE);\n\t\tconfig.remove(TABLE_TYPE);\n\t\tfinal Map<String, String> normalizedConfig = ConfigUtil.normalizeYaml(config);\n\t\tif (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SOURCE())) {\n\t\t\treturn new Source(name, normalizedConfig);\n\t\t} else if (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SINK())) {\n\t\t\treturn new Sink(name, normalizedConfig);\n\t\t} else if (type.equals(TableDescriptorValidator.TABLE_TYPE_VALUE_SOURCE_SINK())) {\n\t\t\treturn new SourceSink(name, normalizedConfig);\n\t\t}\n\t\tthrow new SqlClientException(\"Invalid 'type' attribute for table '\" + name + \"'. \" +\n\t\t\t\"Only 'source', 'sink', and 'both' are supported.\");\n\t}"
        ],
        [
            "JsonRowFormatFactoryTest::testJsonSchemaDeserializationSchema(Map)",
            " 126  \n 127 -\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  ",
            "\tprivate void testJsonSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFormatFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal JsonRowDeserializationSchema expected2 = new JsonRowDeserializationSchema(JSON_SCHEMA);\n\t\texpected2.setFailOnMissingField(true);\n\t\tassertEquals(expected2, actual2);\n\t}",
            " 126  \n 127 +\n 128  \n 129  \n 130  \n 131  \n 132  \n 133  ",
            "\tprivate void testJsonSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal JsonRowDeserializationSchema expected2 = new JsonRowDeserializationSchema(JSON_SCHEMA);\n\t\texpected2.setFailOnMissingField(true);\n\t\tassertEquals(expected2, actual2);\n\t}"
        ],
        [
            "JsonRowFormatFactoryTest::testJsonSchemaSerializationSchema(Map)",
            " 135  \n 136 -\n 137  \n 138  \n 139  \n 140  \n 141  ",
            "\tprivate void testJsonSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFormatFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new JsonRowSerializationSchema(JSON_SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}",
            " 135  \n 136 +\n 137  \n 138  \n 139  \n 140  \n 141  ",
            "\tprivate void testJsonSchemaSerializationSchema(Map<String, String> properties) {\n\t\tfinal SerializationSchema<?> actual1 = TableFactoryService\n\t\t\t.find(SerializationSchemaFactory.class, properties)\n\t\t\t.createSerializationSchema(properties);\n\t\tfinal SerializationSchema<?> expected1 = new JsonRowSerializationSchema(JSON_SCHEMA);\n\t\tassertEquals(expected1, actual1);\n\t}"
        ],
        [
            "AvroRowFormatFactoryTest::testAvroSchemaDeserializationSchema(Map)",
            "  81  \n  82 -\n  83  \n  84  \n  85  \n  86  \n  87  ",
            "\tprivate void testAvroSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFormatFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal AvroRowDeserializationSchema expected2 = new AvroRowDeserializationSchema(AVRO_SCHEMA);\n\t\tassertEquals(expected2, actual2);\n\t}",
            "  81  \n  82 +\n  83  \n  84  \n  85  \n  86  \n  87  ",
            "\tprivate void testAvroSchemaDeserializationSchema(Map<String, String> properties) {\n\t\tfinal DeserializationSchema<?> actual2 = TableFactoryService\n\t\t\t.find(DeserializationSchemaFactory.class, properties)\n\t\t\t.createDeserializationSchema(properties);\n\t\tfinal AvroRowDeserializationSchema expected2 = new AvroRowDeserializationSchema(AVRO_SCHEMA);\n\t\tassertEquals(expected2, actual2);\n\t}"
        ]
    ],
    "6022225a2b8ddbff7dc0bea9d5ecd55ce0031a9f": [
        [
            "CliResultView::RefreshThread::run()",
            " 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  \n 287  \n 288  ",
            "\t\t@Override\n\t\tpublic void run() {\n\t\t\twhile (isRunning) {\n\t\t\t\tfinal long interval = REFRESH_INTERVALS.get(refreshInterval).f1;\n\t\t\t\tif (interval >= 0) {\n\t\t\t\t\t// refresh according to specified interval\n\t\t\t\t\tif (interval > 0) {\n\t\t\t\t\t\tsynchronized (RefreshThread.this) {\n\t\t\t\t\t\t\tif (isRunning) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tRefreshThread.this.wait(interval);\n\t\t\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tsynchronized (CliResultView.this) {\n\t\t\t\t\t\trefresh();\n\n\t\t\t\t\t\t// do the display only every 100 ms (even in fastest mode)\n\t\t\t\t\t\tif (System.currentTimeMillis() - lastUpdatedResults > 100) {\n\t\t\t\t\t\t\tif (CliResultView.this.isRunning()) {\n\t\t\t\t\t\t\t\tdisplay();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tlastUpdatedResults = System.currentTimeMillis();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// keep the thread running but without refreshing\n\t\t\t\t\tsynchronized (RefreshThread.this) {\n\t\t\t\t\t\tif (isRunning) {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tRefreshThread.this.wait(100);\n\t\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t\t// continue\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// final display\n\t\t\tsynchronized (CliResultView.this) {\n\t\t\t\tif (CliResultView.this.isRunning()) {\n\t\t\t\t\tdisplay();\n\t\t\t\t}\n\t\t\t}\n\t\t}",
            " 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  \n 251  \n 252  \n 253  \n 254  \n 255  \n 256  \n 257  \n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280 +\n 281 +\n 282 +\n 283 +\n 284 +\n 285 +\n 286 +\n 287 +\n 288 +\n 289  ",
            "\t\t@Override\n\t\tpublic void run() {\n\t\t\twhile (isRunning) {\n\t\t\t\tfinal long interval = REFRESH_INTERVALS.get(refreshInterval).f1;\n\t\t\t\tif (interval >= 0) {\n\t\t\t\t\t// refresh according to specified interval\n\t\t\t\t\tif (interval > 0) {\n\t\t\t\t\t\tsynchronized (RefreshThread.this) {\n\t\t\t\t\t\t\tif (isRunning) {\n\t\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\t\tRefreshThread.this.wait(interval);\n\t\t\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t\t\tcontinue;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tsynchronized (CliResultView.this) {\n\t\t\t\t\t\trefresh();\n\n\t\t\t\t\t\t// do the display only every 100 ms (even in fastest mode)\n\t\t\t\t\t\tif (System.currentTimeMillis() - lastUpdatedResults > 100) {\n\t\t\t\t\t\t\tif (CliResultView.this.isRunning()) {\n\t\t\t\t\t\t\t\tdisplay();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tlastUpdatedResults = System.currentTimeMillis();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// keep the thread running but without refreshing\n\t\t\t\t\tsynchronized (RefreshThread.this) {\n\t\t\t\t\t\tif (isRunning) {\n\t\t\t\t\t\t\ttry {\n\t\t\t\t\t\t\t\tRefreshThread.this.wait(100);\n\t\t\t\t\t\t\t} catch (InterruptedException e) {\n\t\t\t\t\t\t\t\t// continue\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// final display\n\t\t\tsynchronized (CliResultView.this) {\n\t\t\t\tif (CliResultView.this.isRunning()) {\n\t\t\t\t\tdisplay();\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// cancel table program\n\t\t\ttry {\n\t\t\t\t// the cancellation happens in the refresh thread in order to keep the main thread\n\t\t\t\t// responsive at all times; esp. if the cluster is not available\n\t\t\t\tclient.getExecutor().cancelQuery(client.getContext(), resultDescriptor.getResultId());\n\t\t\t} catch (SqlExecutionException e) {\n\t\t\t\t// ignore further exceptions\n\t\t\t}\n\t\t}"
        ],
        [
            "CliResultView::cleanUp()",
            " 218  \n 219  \n 220 -\n 221  \n 222 -\n 223 -\n 224 -\n 225 -\n 226 -\n 227 -\n 228 -\n 229  ",
            "\t@Override\n\tprotected void cleanUp() {\n\t\t// stop retrieval\n\t\tstopRetrieval();\n\n\t\t// cancel table program\n\t\ttry {\n\t\t\tclient.getExecutor().cancelQuery(client.getContext(), resultDescriptor.getResultId());\n\t\t} catch (SqlExecutionException e) {\n\t\t\t// ignore further exceptions\n\t\t}\n\t}",
            " 218  \n 219  \n 220  \n 221  ",
            "\t@Override\n\tprotected void cleanUp() {\n\t\tstopRetrieval();\n\t}"
        ]
    ],
    "f3bfd22b41acd4c16f71d6d69f7983abc63a1e71": [
        [
            "CliFrontend::loadCustomCommandLines(Configuration,String)",
            "1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161 -\n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  ",
            "\tpublic static List<CustomCommandLine<?>> loadCustomCommandLines(Configuration configuration, String configurationDirectory) {\n\t\tList<CustomCommandLine<?>> customCommandLines = new ArrayList<>(2);\n\n\t\t//\tCommand line interface of the YARN session, with a special initialization here\n\t\t//\tto prefix all options with y/yarn.\n\t\t//\tTips: DefaultCLI must be added at last, because getActiveCustomCommandLine(..) will get the\n\t\t//\t      active CustomCommandLine in order and DefaultCLI isActive always return true.\n\t\tfinal String flinkYarnSessionCLI = \"org.apache.flink.yarn.cli.FlinkYarnSessionCli\";\n\t\ttry {\n\t\t\tcustomCommandLines.add(\n\t\t\t\tloadCustomCommandLine(flinkYarnSessionCLI,\n\t\t\t\t\tconfiguration,\n\t\t\t\t\tconfigurationDirectory,\n\t\t\t\t\t\"y\",\n\t\t\t\t\t\"yarn\"));\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Could not load CLI class {}.\", flinkYarnSessionCLI, e);\n\t\t}\n\n\t\tif (configuration.getString(CoreOptions.MODE).equalsIgnoreCase(CoreOptions.FLIP6_MODE)) {\n\t\t\tcustomCommandLines.add(new Flip6DefaultCLI(configuration));\n\t\t} else {\n\t\t\tcustomCommandLines.add(new DefaultCLI(configuration));\n\t\t}\n\n\t\treturn customCommandLines;\n\t}",
            "1146  \n1147  \n1148  \n1149  \n1150  \n1151  \n1152  \n1153  \n1154  \n1155  \n1156  \n1157  \n1158  \n1159  \n1160  \n1161 +\n1162  \n1163  \n1164  \n1165  \n1166  \n1167  \n1168  \n1169  \n1170  \n1171  \n1172  ",
            "\tpublic static List<CustomCommandLine<?>> loadCustomCommandLines(Configuration configuration, String configurationDirectory) {\n\t\tList<CustomCommandLine<?>> customCommandLines = new ArrayList<>(2);\n\n\t\t//\tCommand line interface of the YARN session, with a special initialization here\n\t\t//\tto prefix all options with y/yarn.\n\t\t//\tTips: DefaultCLI must be added at last, because getActiveCustomCommandLine(..) will get the\n\t\t//\t      active CustomCommandLine in order and DefaultCLI isActive always return true.\n\t\tfinal String flinkYarnSessionCLI = \"org.apache.flink.yarn.cli.FlinkYarnSessionCli\";\n\t\ttry {\n\t\t\tcustomCommandLines.add(\n\t\t\t\tloadCustomCommandLine(flinkYarnSessionCLI,\n\t\t\t\t\tconfiguration,\n\t\t\t\t\tconfigurationDirectory,\n\t\t\t\t\t\"y\",\n\t\t\t\t\t\"yarn\"));\n\t\t} catch (NoClassDefFoundError | Exception e) {\n\t\t\tLOG.warn(\"Could not load CLI class {}.\", flinkYarnSessionCLI, e);\n\t\t}\n\n\t\tif (configuration.getString(CoreOptions.MODE).equalsIgnoreCase(CoreOptions.FLIP6_MODE)) {\n\t\t\tcustomCommandLines.add(new Flip6DefaultCLI(configuration));\n\t\t} else {\n\t\t\tcustomCommandLines.add(new DefaultCLI(configuration));\n\t\t}\n\n\t\treturn customCommandLines;\n\t}"
        ]
    ],
    "fa11015ddcdb0217da6a8b2be1e2a55efd26d7fa": [
        [
            "YarnResourceManager::onContainersAllocated(List)",
            " 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358  \n 359 -\n 360  \n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374  \n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  ",
            "\t@Override\n\tpublic void onContainersAllocated(List<Container> containers) {\n\t\trunAsync(() -> {\n\t\t\tfor (Container container : containers) {\n\t\t\t\tlog.info(\n\t\t\t\t\t\"Received new container: {} - Remaining pending container requests: {}\",\n\t\t\t\t\tcontainer.getId(),\n\t\t\t\t\tnumPendingContainerRequests);\n\n\t\t\t\tif (numPendingContainerRequests > 0) {\n\t\t\t\t\tnumPendingContainerRequests--;\n\n\t\t\t\t\tfinal String containerIdStr = container.getId().toString();\n\n\t\t\t\t\tworkerNodeMap.put(new ResourceID(containerIdStr), new YarnWorkerNode(container));\n\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// Context information used to start a TaskExecutor Java process\n\t\t\t\t\t\tContainerLaunchContext taskExecutorLaunchContext = createTaskExecutorLaunchContext(\n\t\t\t\t\t\t\tcontainer.getResource(),\n\t\t\t\t\t\t\tcontainerIdStr,\n\t\t\t\t\t\t\tcontainer.getNodeId().getHost());\n\n\t\t\t\t\t\tnodeManagerClient.startContainer(container, taskExecutorLaunchContext);\n\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\tlog.error(\"Could not start TaskManager in container {}.\", container.getId(), t);\n\n\t\t\t\t\t\t// release the failed container\n\t\t\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\t\t\t// and ask for a new one\n\t\t\t\t\t\trequestYarnContainer(container.getResource(), container.getPriority());\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// return the excessive containers\n\t\t\t\t\tlog.info(\"Returning excess container {}.\", container.getId());\n\t\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// if we are waiting for no further containers, we can go to the\n\t\t\t// regular heartbeat interval\n\t\t\tif (numPendingContainerRequests <= 0) {\n\t\t\t\tresourceManagerClient.setHeartbeatInterval(yarnHeartbeatIntervalMillis);\n\t\t\t}\n\t\t});\n\t}",
            " 345  \n 346  \n 347  \n 348  \n 349  \n 350  \n 351  \n 352  \n 353  \n 354  \n 355  \n 356  \n 357  \n 358 +\n 359  \n 360 +\n 361  \n 362  \n 363  \n 364  \n 365  \n 366  \n 367  \n 368  \n 369  \n 370  \n 371  \n 372  \n 373  \n 374 +\n 375  \n 376  \n 377  \n 378  \n 379  \n 380  \n 381  \n 382  \n 383  \n 384  \n 385  \n 386  \n 387  \n 388  \n 389  \n 390  \n 391  \n 392  ",
            "\t@Override\n\tpublic void onContainersAllocated(List<Container> containers) {\n\t\trunAsync(() -> {\n\t\t\tfor (Container container : containers) {\n\t\t\t\tlog.info(\n\t\t\t\t\t\"Received new container: {} - Remaining pending container requests: {}\",\n\t\t\t\t\tcontainer.getId(),\n\t\t\t\t\tnumPendingContainerRequests);\n\n\t\t\t\tif (numPendingContainerRequests > 0) {\n\t\t\t\t\tnumPendingContainerRequests--;\n\n\t\t\t\t\tfinal String containerIdStr = container.getId().toString();\n\t\t\t\t\tfinal ResourceID resourceId = new ResourceID(containerIdStr);\n\n\t\t\t\t\tworkerNodeMap.put(resourceId, new YarnWorkerNode(container));\n\n\t\t\t\t\ttry {\n\t\t\t\t\t\t// Context information used to start a TaskExecutor Java process\n\t\t\t\t\t\tContainerLaunchContext taskExecutorLaunchContext = createTaskExecutorLaunchContext(\n\t\t\t\t\t\t\tcontainer.getResource(),\n\t\t\t\t\t\t\tcontainerIdStr,\n\t\t\t\t\t\t\tcontainer.getNodeId().getHost());\n\n\t\t\t\t\t\tnodeManagerClient.startContainer(container, taskExecutorLaunchContext);\n\t\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\t\tlog.error(\"Could not start TaskManager in container {}.\", container.getId(), t);\n\n\t\t\t\t\t\t// release the failed container\n\t\t\t\t\t\tworkerNodeMap.remove(resourceId);\n\t\t\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\t\t\t// and ask for a new one\n\t\t\t\t\t\trequestYarnContainer(container.getResource(), container.getPriority());\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\t// return the excessive containers\n\t\t\t\t\tlog.info(\"Returning excess container {}.\", container.getId());\n\t\t\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// if we are waiting for no further containers, we can go to the\n\t\t\t// regular heartbeat interval\n\t\t\tif (numPendingContainerRequests <= 0) {\n\t\t\t\tresourceManagerClient.setHeartbeatInterval(yarnHeartbeatIntervalMillis);\n\t\t\t}\n\t\t});\n\t}"
        ],
        [
            "YarnResourceManager::stopWorker(YarnWorkerNode)",
            " 297  \n 298 -\n 299 -\n 300 -\n 301 -\n 302 -\n 303 -\n 304 -\n 305 -\n 306 -\n 307 -\n 308 -\n 309 -\n 310 -\n 311 -\n 312  \n 313  \n 314  ",
            "\t@Override\n\tpublic boolean stopWorker(YarnWorkerNode workerNode) {\n\t\tif (workerNode != null) {\n\t\t\tContainer container = workerNode.getContainer();\n\t\t\tlog.info(\"Stopping container {}.\", container.getId());\n\t\t\t// release the container on the node manager\n\t\t\ttry {\n\t\t\t\tnodeManagerClient.stopContainer(container.getId(), container.getNodeId());\n\t\t\t} catch (Throwable t) {\n\t\t\t\tlog.warn(\"Error while calling YARN Node Manager to stop container\", t);\n\t\t\t}\n\t\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\t\tworkerNodeMap.remove(workerNode.getResourceID());\n\t\t} else {\n\t\t\tlog.error(\"Can not find container for null workerNode.\");\n\t\t}\n\t\treturn true;\n\t}",
            " 297  \n 298 +\n 299 +\n 300 +\n 301 +\n 302 +\n 303 +\n 304 +\n 305  \n 306 +\n 307 +\n 308  \n 309  ",
            "\t@Override\n\tpublic boolean stopWorker(final YarnWorkerNode workerNode) {\n\t\tfinal Container container = workerNode.getContainer();\n\t\tlog.info(\"Stopping container {}.\", container.getId());\n\t\ttry {\n\t\t\tnodeManagerClient.stopContainer(container.getId(), container.getNodeId());\n\t\t} catch (final Exception e) {\n\t\t\tlog.warn(\"Error while calling YARN Node Manager to stop container\", e);\n\t\t}\n\t\tresourceManagerClient.releaseAssignedContainer(container.getId());\n\t\tworkerNodeMap.remove(workerNode.getResourceID());\n\t\treturn true;\n\t}"
        ],
        [
            "YarnResourceManager::onContainersCompleted(List)",
            " 331  \n 332 -\n 333  \n 334 -\n 335 -\n 336 -\n 337 -\n 338  \n 339 -\n 340  \n 341  ",
            "\t@Override\n\tpublic void onContainersCompleted(List<ContainerStatus> list) {\n\t\trunAsync(() -> {\n\t\t\t\tfor (ContainerStatus container : list) {\n\t\t\t\t\tif (container.getExitStatus() < 0) {\n\t\t\t\t\t\tcloseTaskManagerConnection(new ResourceID(\n\t\t\t\t\t\t\tcontainer.getContainerId().toString()), new Exception(container.getDiagnostics()));\n\t\t\t\t\t}\n\t\t\t\t\tworkerNodeMap.remove(new ResourceID(container.getContainerId().toString()));\n\t\t\t\t}\n\t\t\t}",
            " 326  \n 327 +\n 328  \n 329 +\n 330 +\n 331 +\n 332 +\n 333 +\n 334 +\n 335 +\n 336 +\n 337 +\n 338 +\n 339  \n 340  \n 341  ",
            "\t@Override\n\tpublic void onContainersCompleted(final List<ContainerStatus> list) {\n\t\trunAsync(() -> {\n\t\t\t\tfor (final ContainerStatus containerStatus : list) {\n\n\t\t\t\t\tfinal ResourceID resourceId = new ResourceID(containerStatus.getContainerId().toString());\n\t\t\t\t\tfinal YarnWorkerNode yarnWorkerNode = workerNodeMap.remove(resourceId);\n\n\t\t\t\t\tif (yarnWorkerNode != null) {\n\t\t\t\t\t\t// Container completed unexpectedly ~> start a new one\n\t\t\t\t\t\tfinal Container container = yarnWorkerNode.getContainer();\n\t\t\t\t\t\trequestYarnContainer(container.getResource(), yarnWorkerNode.getContainer().getPriority());\n\t\t\t\t\t\tcloseTaskManagerConnection(resourceId, new Exception(containerStatus.getDiagnostics()));\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}"
        ]
    ],
    "d1a03dd239555298da9ac9be4ea94ccf52d9887b": [
        [
            "CassandraConnectorITCase::testCassandraTableSink()",
            " 450  \n 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460 -\n 461 -\n 462  \n 463  \n 464  \n 465  \n 466  \n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  ",
            "\t@Test\n\tpublic void testCassandraTableSink() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.getTableEnvironment(env);\n\n\t\tDataStreamSource<Row> source = env.fromCollection(rowCollection);\n\n\t\ttEnv.registerDataStreamInternal(\"testFlinkTable\", source);\n\n\t\ttEnv.sql(\"select * from testFlinkTable\").writeToSink(\n\t\t\tnew CassandraAppendTableSink(builder, injectTableName(INSERT_DATA_QUERY)));\n\n\t\tenv.execute();\n\t\tResultSet rs = session.execute(injectTableName(SELECT_DATA_QUERY));\n\n\t\t// validate that all input was correctly written to Cassandra\n\t\tList<Row> input = new ArrayList<>(rowCollection);\n\t\tList<com.datastax.driver.core.Row> output = rs.all();\n\t\tfor (com.datastax.driver.core.Row o : output) {\n\t\t\tRow cmp = new Row(3);\n\t\t\tcmp.setField(0, o.getString(0));\n\t\t\tcmp.setField(1, o.getInt(2));\n\t\t\tcmp.setField(2, o.getInt(1));\n\t\t\tAssert.assertTrue(\"Row \" + cmp + \" was written to Cassandra but not in input.\", input.remove(cmp));\n\t\t}\n\t\tAssert.assertTrue(\"The input data was not completely written to Cassandra\", input.isEmpty());\n\t}",
            " 451  \n 452  \n 453  \n 454  \n 455  \n 456  \n 457  \n 458  \n 459  \n 460 +\n 461 +\n 462 +\n 463 +\n 464 +\n 465  \n 466 +\n 467  \n 468  \n 469  \n 470  \n 471  \n 472  \n 473  \n 474  \n 475  \n 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482  ",
            "\t@Test\n\tpublic void testCassandraTableSink() throws Exception {\n\t\tStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tenv.setParallelism(4);\n\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.getTableEnvironment(env);\n\n\t\tDataStreamSource<Row> source = env.fromCollection(rowCollection);\n\n\t\ttEnv.registerDataStreamInternal(\"testFlinkTable\", source);\n\t\ttEnv.registerTableSink(\"cassandraTable\",\n\t\t\tnew CassandraAppendTableSink(builder, injectTableName(INSERT_DATA_QUERY))\n\t\t\t\t.configure(\n\t\t\t\t\tnew String[]{\"f0\", \"f1\", \"f2\"},\n\t\t\t\t\tnew TypeInformation[]{Types.STRING, Types.INT, Types.INT}));\n\n\t\ttEnv.sqlQuery(\"select * from testFlinkTable\").insertInto(\"cassandraTable\");\n\n\t\tenv.execute();\n\t\tResultSet rs = session.execute(injectTableName(SELECT_DATA_QUERY));\n\n\t\t// validate that all input was correctly written to Cassandra\n\t\tList<Row> input = new ArrayList<>(rowCollection);\n\t\tList<com.datastax.driver.core.Row> output = rs.all();\n\t\tfor (com.datastax.driver.core.Row o : output) {\n\t\t\tRow cmp = new Row(3);\n\t\t\tcmp.setField(0, o.getString(0));\n\t\t\tcmp.setField(1, o.getInt(2));\n\t\t\tcmp.setField(2, o.getInt(1));\n\t\t\tAssert.assertTrue(\"Row \" + cmp + \" was written to Cassandra but not in input.\", input.remove(cmp));\n\t\t}\n\t\tAssert.assertTrue(\"The input data was not completely written to Cassandra\", input.isEmpty());\n\t}"
        ]
    ],
    "96bb0d47b36f869e9b007fc3312e004a90779807": [
        [
            "SqlFunctionUtils::rpad(String,int,String)",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257 -\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "\t/**\n\t * Returns the string str right-padded with the string pad to a length of len characters.\n\t * If str is longer than len, the return value is shortened to len characters.\n\t */\n\tpublic static String rpad(String base, int len, String pad) {\n\t\tif (len < 0) {\n\t\t\treturn null;\n\t\t} else if (len == 0) {\n\t\t\treturn \"\";\n\t\t}\n\n\t\tchar[] data = new char[len];\n\t\tchar[] baseChars = base.toCharArray();\n\t\tchar[] padChars = pad.toCharArray();\n\n\t\tint pos = 0;\n\n\t\t// copy the base\n\t\twhile (pos < base.length() && pos < len) {\n\t\t\tdata[pos] = baseChars[pos];\n\t\t\tpos += 1;\n\t\t}\n\n\t\t// copy the padding\n\t\twhile (pos < len) {\n\t\t\tint i = 0;\n\t\t\twhile (i < pad.length() && i < len - pos) {\n\t\t\t\tdata[pos + i] = padChars[i];\n\t\t\t\ti += 1;\n\t\t\t}\n\t\t\tpos += pad.length();\n\t\t}\n\n\t\treturn new String(data);\n\t}",
            " 252  \n 253  \n 254  \n 255  \n 256  \n 257 +\n 258  \n 259  \n 260  \n 261  \n 262  \n 263  \n 264  \n 265  \n 266  \n 267  \n 268  \n 269  \n 270  \n 271  \n 272  \n 273  \n 274  \n 275  \n 276  \n 277  \n 278  \n 279  \n 280  \n 281  \n 282  \n 283  \n 284  \n 285  \n 286  ",
            "\t/**\n\t * Returns the string str right-padded with the string pad to a length of len characters.\n\t * If str is longer than len, the return value is shortened to len characters.\n\t */\n\tpublic static String rpad(String base, int len, String pad) {\n\t\tif (len < 0 || \"\".equals(pad)) {\n\t\t\treturn null;\n\t\t} else if (len == 0) {\n\t\t\treturn \"\";\n\t\t}\n\n\t\tchar[] data = new char[len];\n\t\tchar[] baseChars = base.toCharArray();\n\t\tchar[] padChars = pad.toCharArray();\n\n\t\tint pos = 0;\n\n\t\t// copy the base\n\t\twhile (pos < base.length() && pos < len) {\n\t\t\tdata[pos] = baseChars[pos];\n\t\t\tpos += 1;\n\t\t}\n\n\t\t// copy the padding\n\t\twhile (pos < len) {\n\t\t\tint i = 0;\n\t\t\twhile (i < pad.length() && i < len - pos) {\n\t\t\t\tdata[pos + i] = padChars[i];\n\t\t\t\ti += 1;\n\t\t\t}\n\t\t\tpos += pad.length();\n\t\t}\n\n\t\treturn new String(data);\n\t}"
        ],
        [
            "SqlFunctionUtils::lpad(String,int,String)",
            " 217  \n 218  \n 219  \n 220  \n 221  \n 222 -\n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "\t/**\n\t * Returns the string str left-padded with the string pad to a length of len characters.\n\t * If str is longer than len, the return value is shortened to len characters.\n\t */\n\tpublic static String lpad(String base, int len, String pad) {\n\t\tif (len < 0) {\n\t\t\treturn null;\n\t\t} else if (len == 0) {\n\t\t\treturn \"\";\n\t\t}\n\n\t\tchar[] data = new char[len];\n\t\tchar[] baseChars = base.toCharArray();\n\t\tchar[] padChars = pad.toCharArray();\n\n\t\t// the length of the padding needed\n\t\tint pos = Math.max(len - base.length(), 0);\n\n\t\t// copy the padding\n\t\tfor (int i = 0; i < pos; i += pad.length()) {\n\t\t\tfor (int j = 0; j < pad.length() && j < pos - i; j++) {\n\t\t\t\tdata[i + j] = padChars[j];\n\t\t\t}\n\t\t}\n\n\t\t// copy the base\n\t\tint i = 0;\n\t\twhile (pos + i < len && i < base.length()) {\n\t\t\tdata[pos + i] = baseChars[i];\n\t\t\ti += 1;\n\t\t}\n\n\t\treturn new String(data);\n\t}",
            " 217  \n 218  \n 219  \n 220  \n 221  \n 222 +\n 223  \n 224  \n 225  \n 226  \n 227  \n 228  \n 229  \n 230  \n 231  \n 232  \n 233  \n 234  \n 235  \n 236  \n 237  \n 238  \n 239  \n 240  \n 241  \n 242  \n 243  \n 244  \n 245  \n 246  \n 247  \n 248  \n 249  \n 250  ",
            "\t/**\n\t * Returns the string str left-padded with the string pad to a length of len characters.\n\t * If str is longer than len, the return value is shortened to len characters.\n\t */\n\tpublic static String lpad(String base, int len, String pad) {\n\t\tif (len < 0 || \"\".equals(pad)) {\n\t\t\treturn null;\n\t\t} else if (len == 0) {\n\t\t\treturn \"\";\n\t\t}\n\n\t\tchar[] data = new char[len];\n\t\tchar[] baseChars = base.toCharArray();\n\t\tchar[] padChars = pad.toCharArray();\n\n\t\t// the length of the padding needed\n\t\tint pos = Math.max(len - base.length(), 0);\n\n\t\t// copy the padding\n\t\tfor (int i = 0; i < pos; i += pad.length()) {\n\t\t\tfor (int j = 0; j < pad.length() && j < pos - i; j++) {\n\t\t\t\tdata[i + j] = padChars[j];\n\t\t\t}\n\t\t}\n\n\t\t// copy the base\n\t\tint i = 0;\n\t\twhile (pos + i < len && i < base.length()) {\n\t\t\tdata[pos + i] = baseChars[i];\n\t\t\ti += 1;\n\t\t}\n\n\t\treturn new String(data);\n\t}"
        ]
    ],
    "687cc91975464734ac43f4c2eb062f1f312b589e": [
        [
            "SingleInputGateFactory::SingleInputGateFactory(ResourceID,NettyShuffleEnvironmentConfiguration,ConnectionManager,ResultPartitionManager,TaskEventPublisher,NetworkBufferPool)",
            "  80  \n  81 -\n  82  \n  83  \n  84  \n  85  \n  86  \n  87 -\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "\tpublic SingleInputGateFactory(\n\t\t\t@Nonnull ResourceID taskExecutorLocation,\n\t\t\t@Nonnull NettyShuffleEnvironmentConfiguration networkConfig,\n\t\t\t@Nonnull ConnectionManager connectionManager,\n\t\t\t@Nonnull ResultPartitionManager partitionManager,\n\t\t\t@Nonnull TaskEventPublisher taskEventPublisher,\n\t\t\t@Nonnull NetworkBufferPool networkBufferPool) {\n\t\tthis.taskExecutorLocation = taskExecutorLocation;\n\t\tthis.isCreditBased = networkConfig.isCreditBased();\n\t\tthis.partitionRequestInitialBackoff = networkConfig.partitionRequestInitialBackoff();\n\t\tthis.partitionRequestMaxBackoff = networkConfig.partitionRequestMaxBackoff();\n\t\tthis.networkBuffersPerChannel = networkConfig.networkBuffersPerChannel();\n\t\tthis.floatingNetworkBuffersPerGate = networkConfig.floatingNetworkBuffersPerGate();\n\t\tthis.connectionManager = connectionManager;\n\t\tthis.partitionManager = partitionManager;\n\t\tthis.taskEventPublisher = taskEventPublisher;\n\t\tthis.networkBufferPool = networkBufferPool;\n\t}",
            "  80  \n  81 +\n  82  \n  83  \n  84  \n  85  \n  86  \n  87 +\n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  ",
            "\tpublic SingleInputGateFactory(\n\t\t\t@Nonnull ResourceID taskExecutorResourceId,\n\t\t\t@Nonnull NettyShuffleEnvironmentConfiguration networkConfig,\n\t\t\t@Nonnull ConnectionManager connectionManager,\n\t\t\t@Nonnull ResultPartitionManager partitionManager,\n\t\t\t@Nonnull TaskEventPublisher taskEventPublisher,\n\t\t\t@Nonnull NetworkBufferPool networkBufferPool) {\n\t\tthis.taskExecutorResourceId = taskExecutorResourceId;\n\t\tthis.isCreditBased = networkConfig.isCreditBased();\n\t\tthis.partitionRequestInitialBackoff = networkConfig.partitionRequestInitialBackoff();\n\t\tthis.partitionRequestMaxBackoff = networkConfig.partitionRequestMaxBackoff();\n\t\tthis.networkBuffersPerChannel = networkConfig.networkBuffersPerChannel();\n\t\tthis.floatingNetworkBuffersPerGate = networkConfig.floatingNetworkBuffersPerGate();\n\t\tthis.connectionManager = connectionManager;\n\t\tthis.partitionManager = partitionManager;\n\t\tthis.taskEventPublisher = taskEventPublisher;\n\t\tthis.networkBufferPool = networkBufferPool;\n\t}"
        ],
        [
            "SingleInputGateFactory::createKnownInputChannel(SingleInputGate,int,NettyShuffleDescriptor,ChannelStatistics,InputChannelMetrics)",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 -\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "\tprivate InputChannel createKnownInputChannel(\n\t\t\tSingleInputGate inputGate,\n\t\t\tint index,\n\t\t\tNettyShuffleDescriptor inputChannelDescriptor,\n\t\t\tChannelStatistics channelStatistics,\n\t\t\tInputChannelMetrics metrics) {\n\t\tResultPartitionID partitionId = inputChannelDescriptor.getResultPartitionID();\n\t\tif (inputChannelDescriptor.isLocalTo(taskExecutorLocation)) {\n\t\t\t// Consuming task is deployed to the same TaskManager as the partition => local\n\t\t\tchannelStatistics.numLocalChannels++;\n\t\t\treturn new LocalInputChannel(\n\t\t\t\tinputGate,\n\t\t\t\tindex,\n\t\t\t\tpartitionId,\n\t\t\t\tpartitionManager,\n\t\t\t\ttaskEventPublisher,\n\t\t\t\tpartitionRequestInitialBackoff,\n\t\t\t\tpartitionRequestMaxBackoff,\n\t\t\t\tmetrics);\n\t\t} else {\n\t\t\t// Different instances => remote\n\t\t\tchannelStatistics.numRemoteChannels++;\n\t\t\treturn new RemoteInputChannel(\n\t\t\t\tinputGate,\n\t\t\t\tindex,\n\t\t\t\tpartitionId,\n\t\t\t\tinputChannelDescriptor.getConnectionId(),\n\t\t\t\tconnectionManager,\n\t\t\t\tpartitionRequestInitialBackoff,\n\t\t\t\tpartitionRequestMaxBackoff,\n\t\t\t\tmetrics,\n\t\t\t\tnetworkBufferPool);\n\t\t}\n\t}",
            " 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197 +\n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  \n 207  \n 208  \n 209  \n 210  \n 211  \n 212  \n 213  \n 214  \n 215  \n 216  \n 217  \n 218  \n 219  \n 220  \n 221  \n 222  \n 223  ",
            "\tprivate InputChannel createKnownInputChannel(\n\t\t\tSingleInputGate inputGate,\n\t\t\tint index,\n\t\t\tNettyShuffleDescriptor inputChannelDescriptor,\n\t\t\tChannelStatistics channelStatistics,\n\t\t\tInputChannelMetrics metrics) {\n\t\tResultPartitionID partitionId = inputChannelDescriptor.getResultPartitionID();\n\t\tif (inputChannelDescriptor.isLocalTo(taskExecutorResourceId)) {\n\t\t\t// Consuming task is deployed to the same TaskManager as the partition => local\n\t\t\tchannelStatistics.numLocalChannels++;\n\t\t\treturn new LocalInputChannel(\n\t\t\t\tinputGate,\n\t\t\t\tindex,\n\t\t\t\tpartitionId,\n\t\t\t\tpartitionManager,\n\t\t\t\ttaskEventPublisher,\n\t\t\t\tpartitionRequestInitialBackoff,\n\t\t\t\tpartitionRequestMaxBackoff,\n\t\t\t\tmetrics);\n\t\t} else {\n\t\t\t// Different instances => remote\n\t\t\tchannelStatistics.numRemoteChannels++;\n\t\t\treturn new RemoteInputChannel(\n\t\t\t\tinputGate,\n\t\t\t\tindex,\n\t\t\t\tpartitionId,\n\t\t\t\tinputChannelDescriptor.getConnectionId(),\n\t\t\t\tconnectionManager,\n\t\t\t\tpartitionRequestInitialBackoff,\n\t\t\t\tpartitionRequestMaxBackoff,\n\t\t\t\tmetrics,\n\t\t\t\tnetworkBufferPool);\n\t\t}\n\t}"
        ],
        [
            "NettyShuffleEnvironment::create(NettyShuffleEnvironmentConfiguration,ResourceID,TaskEventPublisher,MetricGroup,IOManager)",
            " 127  \n 128  \n 129 -\n 130  \n 131  \n 132  \n 133 -\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 -\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 -\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\tpublic static NettyShuffleEnvironment create(\n\t\t\tNettyShuffleEnvironmentConfiguration config,\n\t\t\tResourceID taskExecutorLocation,\n\t\t\tTaskEventPublisher taskEventPublisher,\n\t\t\tMetricGroup metricGroup,\n\t\t\tIOManager ioManager) {\n\t\tcheckNotNull(taskExecutorLocation);\n\t\tcheckNotNull(ioManager);\n\t\tcheckNotNull(taskEventPublisher);\n\t\tcheckNotNull(config);\n\n\t\tNettyConfig nettyConfig = config.nettyConfig();\n\n\t\tResultPartitionManager resultPartitionManager = new ResultPartitionManager();\n\n\t\tConnectionManager connectionManager = nettyConfig != null ?\n\t\t\tnew NettyConnectionManager(resultPartitionManager, taskEventPublisher, nettyConfig, config.isCreditBased()) :\n\t\t\tnew LocalConnectionManager();\n\n\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(\n\t\t\tconfig.numNetworkBuffers(),\n\t\t\tconfig.networkBufferSize(),\n\t\t\tconfig.networkBuffersPerChannel());\n\n\t\tregisterNetworkMetrics(metricGroup, networkBufferPool);\n\n\t\tResultPartitionFactory resultPartitionFactory = new ResultPartitionFactory(\n\t\t\tresultPartitionManager,\n\t\t\tioManager,\n\t\t\tnetworkBufferPool,\n\t\t\tconfig.networkBuffersPerChannel(),\n\t\t\tconfig.floatingNetworkBuffersPerGate(),\n\t\t\tconfig.isForcePartitionReleaseOnConsumption());\n\n\t\tSingleInputGateFactory singleInputGateFactory = new SingleInputGateFactory(\n\t\t\ttaskExecutorLocation,\n\t\t\tconfig,\n\t\t\tconnectionManager,\n\t\t\tresultPartitionManager,\n\t\t\ttaskEventPublisher,\n\t\t\tnetworkBufferPool);\n\n\t\treturn new NettyShuffleEnvironment(\n\t\t\ttaskExecutorLocation,\n\t\t\tconfig,\n\t\t\tnetworkBufferPool,\n\t\t\tconnectionManager,\n\t\t\tresultPartitionManager,\n\t\t\tresultPartitionFactory,\n\t\t\tsingleInputGateFactory);\n\t}",
            " 127  \n 128  \n 129 +\n 130  \n 131  \n 132  \n 133 +\n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162 +\n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170 +\n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  ",
            "\tpublic static NettyShuffleEnvironment create(\n\t\t\tNettyShuffleEnvironmentConfiguration config,\n\t\t\tResourceID taskExecutorResourceId,\n\t\t\tTaskEventPublisher taskEventPublisher,\n\t\t\tMetricGroup metricGroup,\n\t\t\tIOManager ioManager) {\n\t\tcheckNotNull(taskExecutorResourceId);\n\t\tcheckNotNull(ioManager);\n\t\tcheckNotNull(taskEventPublisher);\n\t\tcheckNotNull(config);\n\n\t\tNettyConfig nettyConfig = config.nettyConfig();\n\n\t\tResultPartitionManager resultPartitionManager = new ResultPartitionManager();\n\n\t\tConnectionManager connectionManager = nettyConfig != null ?\n\t\t\tnew NettyConnectionManager(resultPartitionManager, taskEventPublisher, nettyConfig, config.isCreditBased()) :\n\t\t\tnew LocalConnectionManager();\n\n\t\tNetworkBufferPool networkBufferPool = new NetworkBufferPool(\n\t\t\tconfig.numNetworkBuffers(),\n\t\t\tconfig.networkBufferSize(),\n\t\t\tconfig.networkBuffersPerChannel());\n\n\t\tregisterNetworkMetrics(metricGroup, networkBufferPool);\n\n\t\tResultPartitionFactory resultPartitionFactory = new ResultPartitionFactory(\n\t\t\tresultPartitionManager,\n\t\t\tioManager,\n\t\t\tnetworkBufferPool,\n\t\t\tconfig.networkBuffersPerChannel(),\n\t\t\tconfig.floatingNetworkBuffersPerGate(),\n\t\t\tconfig.isForcePartitionReleaseOnConsumption());\n\n\t\tSingleInputGateFactory singleInputGateFactory = new SingleInputGateFactory(\n\t\t\ttaskExecutorResourceId,\n\t\t\tconfig,\n\t\t\tconnectionManager,\n\t\t\tresultPartitionManager,\n\t\t\ttaskEventPublisher,\n\t\t\tnetworkBufferPool);\n\n\t\treturn new NettyShuffleEnvironment(\n\t\t\ttaskExecutorResourceId,\n\t\t\tconfig,\n\t\t\tnetworkBufferPool,\n\t\t\tconnectionManager,\n\t\t\tresultPartitionManager,\n\t\t\tresultPartitionFactory,\n\t\t\tsingleInputGateFactory);\n\t}"
        ],
        [
            "NettyShuffleEnvironment::NettyShuffleEnvironment(ResourceID,NettyShuffleEnvironmentConfiguration,NetworkBufferPool,ConnectionManager,ResultPartitionManager,ResultPartitionFactory,SingleInputGateFactory)",
            " 108  \n 109 -\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 -\n 117  \n 118  \n 119  \n 120  \n 121 -\n 122  \n 123  \n 124  \n 125  ",
            "\tprivate NettyShuffleEnvironment(\n\t\t\tResourceID taskExecutorLocation,\n\t\t\tNettyShuffleEnvironmentConfiguration config,\n\t\t\tNetworkBufferPool networkBufferPool,\n\t\t\tConnectionManager connectionManager,\n\t\t\tResultPartitionManager resultPartitionManager,\n\t\t\tResultPartitionFactory resultPartitionFactory,\n\t\t\tSingleInputGateFactory singleInputGateFactory) {\n\t\tthis.taskExecutorLocation = taskExecutorLocation;\n\t\tthis.config = config;\n\t\tthis.networkBufferPool = networkBufferPool;\n\t\tthis.connectionManager = connectionManager;\n\t\tthis.resultPartitionManager = resultPartitionManager;\n\t\tthis.inputGatesById = new ConcurrentHashMap<>();\n\t\tthis.resultPartitionFactory = resultPartitionFactory;\n\t\tthis.singleInputGateFactory = singleInputGateFactory;\n\t\tthis.isClosed = false;\n\t}",
            " 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116 +\n 117  \n 118  \n 119  \n 120  \n 121 +\n 122  \n 123  \n 124  \n 125  ",
            "\tprivate NettyShuffleEnvironment(\n\t\t\tResourceID taskExecutorResourceId,\n\t\t\tNettyShuffleEnvironmentConfiguration config,\n\t\t\tNetworkBufferPool networkBufferPool,\n\t\t\tConnectionManager connectionManager,\n\t\t\tResultPartitionManager resultPartitionManager,\n\t\t\tResultPartitionFactory resultPartitionFactory,\n\t\t\tSingleInputGateFactory singleInputGateFactory) {\n\t\tthis.taskExecutorResourceId = taskExecutorResourceId;\n\t\tthis.config = config;\n\t\tthis.networkBufferPool = networkBufferPool;\n\t\tthis.connectionManager = connectionManager;\n\t\tthis.resultPartitionManager = resultPartitionManager;\n\t\tthis.inputGatesById = new ConcurrentHashMap<>(10);\n\t\tthis.resultPartitionFactory = resultPartitionFactory;\n\t\tthis.singleInputGateFactory = singleInputGateFactory;\n\t\tthis.isClosed = false;\n\t}"
        ],
        [
            "NettyShuffleEnvironment::updatePartitionInfo(ExecutionAttemptID,PartitionInfo)",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 -\n 324  \n 325  ",
            "\t@Override\n\tpublic boolean updatePartitionInfo(\n\t\t\tExecutionAttemptID consumerID,\n\t\t\tPartitionInfo partitionInfo) throws IOException, InterruptedException {\n\t\tIntermediateDataSetID intermediateResultPartitionID = partitionInfo.getIntermediateDataSetID();\n\t\tInputGateID id = new InputGateID(intermediateResultPartitionID, consumerID);\n\t\tSingleInputGate inputGate = inputGatesById.get(id);\n\t\tif (inputGate == null) {\n\t\t\treturn false;\n\t\t}\n\t\tShuffleDescriptor shuffleDescriptor = partitionInfo.getShuffleDescriptor();\n\t\tcheckArgument(shuffleDescriptor instanceof NettyShuffleDescriptor,\n\t\t\t\"Tried to update unknown channel with unknown ShuffleDescriptor %s.\",\n\t\t\tshuffleDescriptor.getClass().getName());\n\t\tinputGate.updateInputChannel(taskExecutorLocation, (NettyShuffleDescriptor) shuffleDescriptor);\n\t\treturn true;\n\t}",
            " 309  \n 310  \n 311  \n 312  \n 313  \n 314  \n 315  \n 316  \n 317  \n 318  \n 319  \n 320  \n 321  \n 322  \n 323 +\n 324  \n 325  ",
            "\t@Override\n\tpublic boolean updatePartitionInfo(\n\t\t\tExecutionAttemptID consumerID,\n\t\t\tPartitionInfo partitionInfo) throws IOException, InterruptedException {\n\t\tIntermediateDataSetID intermediateResultPartitionID = partitionInfo.getIntermediateDataSetID();\n\t\tInputGateID id = new InputGateID(intermediateResultPartitionID, consumerID);\n\t\tSingleInputGate inputGate = inputGatesById.get(id);\n\t\tif (inputGate == null) {\n\t\t\treturn false;\n\t\t}\n\t\tShuffleDescriptor shuffleDescriptor = partitionInfo.getShuffleDescriptor();\n\t\tcheckArgument(shuffleDescriptor instanceof NettyShuffleDescriptor,\n\t\t\t\"Tried to update unknown channel with unknown ShuffleDescriptor %s.\",\n\t\t\tshuffleDescriptor.getClass().getName());\n\t\tinputGate.updateInputChannel(taskExecutorResourceId, (NettyShuffleDescriptor) shuffleDescriptor);\n\t\treturn true;\n\t}"
        ]
    ],
    "cad94a0b36cdb014c071d5729212f880a7566c48": [
        [
            "RexNodeConverter::translateScalarCall(FunctionDefinition,List)",
            " 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  ",
            "\tprivate RexNode translateScalarCall(FunctionDefinition def, List<Expression> children) {\n\n\t\tif (def.equals(BuiltInFunctionDefinitions.CAST)) {\n\t\t\tRexNode child = children.get(0).accept(this);\n\t\t\tTypeLiteralExpression type = (TypeLiteralExpression) children.get(1);\n\t\t\treturn relBuilder.getRexBuilder().makeAbstractCast(\n\t\t\t\t\ttypeFactory.createFieldTypeFromLogicalType(\n\t\t\t\t\t\t\ttype.getOutputDataType().getLogicalType().copy(child.getType().isNullable())),\n\t\t\t\t\tchild);\n\t\t} else if (def.equals(BuiltInFunctionDefinitions.REINTERPRET_CAST)) {\n\t\t\tRexNode child = children.get(0).accept(this);\n\t\t\tTypeLiteralExpression type = (TypeLiteralExpression) children.get(1);\n\t\t\tRexNode checkOverflow = children.get(2).accept(this);\n\t\t\treturn relBuilder.getRexBuilder().makeReinterpretCast(\n\t\t\t\t\ttypeFactory.createFieldTypeFromLogicalType(\n\t\t\t\t\t\t\ttype.getOutputDataType().getLogicalType().copy(child.getType().isNullable())),\n\t\t\t\t\tchild,\n\t\t\t\t\tcheckOverflow);\n\t\t}\n\n\t\tList<RexNode> child = convertCallChildren(children);\n\t\tif (BuiltInFunctionDefinitions.IF.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.CASE, child);\n\t\t} else if (BuiltInFunctionDefinitions.IS_NULL.equals(def)) {\n\t\t\treturn relBuilder.isNull(child.get(0));\n\t\t} else if (BuiltInFunctionDefinitions.PLUS.equals(def)) {\n\t\t\tif (isCharacterString(toLogicalType(child.get(0).getType()))) {\n\t\t\t\treturn relBuilder.call(\n\t\t\t\t\t\tFlinkSqlOperatorTable.CONCAT,\n\t\t\t\t\t\tchild.get(0),\n\t\t\t\t\t\trelBuilder.cast(child.get(1), VARCHAR));\n\t\t\t} else if (isCharacterString(toLogicalType(child.get(1).getType()))) {\n\t\t\t\treturn relBuilder.call(\n\t\t\t\t\t\tFlinkSqlOperatorTable.CONCAT,\n\t\t\t\t\t\trelBuilder.cast(child.get(0), VARCHAR),\n\t\t\t\t\t\tchild.get(1));\n\t\t\t} else if (isTimeInterval(toLogicalType(child.get(0).getType())) &&\n\t\t\t\t\tchild.get(0).getType() == child.get(1).getType()) {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.PLUS, child);\n\t\t\t} else if (isTimeInterval(toLogicalType(child.get(0).getType()))\n\t\t\t\t\t&& isTemporal(toLogicalType(child.get(1).getType()))) {\n\t\t\t\t// Calcite has a bug that can't apply INTERVAL + DATETIME (INTERVAL at left)\n\t\t\t\t// we manually switch them here\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DATETIME_PLUS, child);\n\t\t\t} else if (isTemporal(toLogicalType(child.get(0).getType())) &&\n\t\t\t\t\tisTemporal(toLogicalType(child.get(1).getType()))) {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DATETIME_PLUS, child);\n\t\t\t} else {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.PLUS, child);\n\t\t\t}\n\t\t} else if (BuiltInFunctionDefinitions.MINUS.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MINUS, child);\n\t\t} else if (BuiltInFunctionDefinitions.EQUALS.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.EQUALS, child);\n\t\t} else if (BuiltInFunctionDefinitions.DIVIDE.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DIVIDE, child);\n\t\t} else if (BuiltInFunctionDefinitions.LESS_THAN.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.LESS_THAN, child);\n\t\t} else if (BuiltInFunctionDefinitions.GREATER_THAN.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.GREATER_THAN, child);\n\t\t} else if (BuiltInFunctionDefinitions.OR.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.OR, child);\n\t\t} else if (BuiltInFunctionDefinitions.CONCAT.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.CONCAT, child);\n\t\t} else if (InternalFunctionDefinitions.THROW_EXCEPTION.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.THROW_EXCEPTION, child);\n\t\t} else if (BuiltInFunctionDefinitions.AND.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.AND, child);\n\t\t} else if (BuiltInFunctionDefinitions.NOT.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.NOT, child);\n\t\t} else if (BuiltInFunctionDefinitions.TIMES.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MULTIPLY, child);\n\t\t} else if (BuiltInFunctionDefinitions.MOD.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MOD, child);\n\t\t} else {\n\t\t\tthrow new UnsupportedOperationException(def.toString());\n\t\t}\n\t}",
            " 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184 +\n 185 +\n 186 +\n 187 +\n 188 +\n 189 +\n 190 +\n 191 +\n 192 +\n 193  \n 194  \n 195  \n 196  ",
            "\tprivate RexNode translateScalarCall(FunctionDefinition def, List<Expression> children) {\n\n\t\tif (def.equals(BuiltInFunctionDefinitions.CAST)) {\n\t\t\tRexNode child = children.get(0).accept(this);\n\t\t\tTypeLiteralExpression type = (TypeLiteralExpression) children.get(1);\n\t\t\treturn relBuilder.getRexBuilder().makeAbstractCast(\n\t\t\t\t\ttypeFactory.createFieldTypeFromLogicalType(\n\t\t\t\t\t\t\ttype.getOutputDataType().getLogicalType().copy(child.getType().isNullable())),\n\t\t\t\t\tchild);\n\t\t} else if (def.equals(BuiltInFunctionDefinitions.REINTERPRET_CAST)) {\n\t\t\tRexNode child = children.get(0).accept(this);\n\t\t\tTypeLiteralExpression type = (TypeLiteralExpression) children.get(1);\n\t\t\tRexNode checkOverflow = children.get(2).accept(this);\n\t\t\treturn relBuilder.getRexBuilder().makeReinterpretCast(\n\t\t\t\t\ttypeFactory.createFieldTypeFromLogicalType(\n\t\t\t\t\t\t\ttype.getOutputDataType().getLogicalType().copy(child.getType().isNullable())),\n\t\t\t\t\tchild,\n\t\t\t\t\tcheckOverflow);\n\t\t}\n\n\t\tList<RexNode> child = convertCallChildren(children);\n\t\tif (BuiltInFunctionDefinitions.IF.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.CASE, child);\n\t\t} else if (BuiltInFunctionDefinitions.IS_NULL.equals(def)) {\n\t\t\treturn relBuilder.isNull(child.get(0));\n\t\t} else if (BuiltInFunctionDefinitions.PLUS.equals(def)) {\n\t\t\tif (isCharacterString(toLogicalType(child.get(0).getType()))) {\n\t\t\t\treturn relBuilder.call(\n\t\t\t\t\t\tFlinkSqlOperatorTable.CONCAT,\n\t\t\t\t\t\tchild.get(0),\n\t\t\t\t\t\trelBuilder.cast(child.get(1), VARCHAR));\n\t\t\t} else if (isCharacterString(toLogicalType(child.get(1).getType()))) {\n\t\t\t\treturn relBuilder.call(\n\t\t\t\t\t\tFlinkSqlOperatorTable.CONCAT,\n\t\t\t\t\t\trelBuilder.cast(child.get(0), VARCHAR),\n\t\t\t\t\t\tchild.get(1));\n\t\t\t} else if (isTimeInterval(toLogicalType(child.get(0).getType())) &&\n\t\t\t\t\tchild.get(0).getType() == child.get(1).getType()) {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.PLUS, child);\n\t\t\t} else if (isTimeInterval(toLogicalType(child.get(0).getType()))\n\t\t\t\t\t&& isTemporal(toLogicalType(child.get(1).getType()))) {\n\t\t\t\t// Calcite has a bug that can't apply INTERVAL + DATETIME (INTERVAL at left)\n\t\t\t\t// we manually switch them here\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DATETIME_PLUS, child);\n\t\t\t} else if (isTemporal(toLogicalType(child.get(0).getType())) &&\n\t\t\t\t\tisTemporal(toLogicalType(child.get(1).getType()))) {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DATETIME_PLUS, child);\n\t\t\t} else {\n\t\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.PLUS, child);\n\t\t\t}\n\t\t} else if (BuiltInFunctionDefinitions.MINUS.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MINUS, child);\n\t\t} else if (BuiltInFunctionDefinitions.EQUALS.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.EQUALS, child);\n\t\t} else if (BuiltInFunctionDefinitions.DIVIDE.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.DIVIDE, child);\n\t\t} else if (BuiltInFunctionDefinitions.LESS_THAN.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.LESS_THAN, child);\n\t\t} else if (BuiltInFunctionDefinitions.GREATER_THAN.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.GREATER_THAN, child);\n\t\t} else if (BuiltInFunctionDefinitions.OR.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.OR, child);\n\t\t} else if (BuiltInFunctionDefinitions.CONCAT.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.CONCAT, child);\n\t\t} else if (InternalFunctionDefinitions.THROW_EXCEPTION.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.THROW_EXCEPTION, child);\n\t\t} else if (BuiltInFunctionDefinitions.AND.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.AND, child);\n\t\t} else if (BuiltInFunctionDefinitions.NOT.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.NOT, child);\n\t\t} else if (BuiltInFunctionDefinitions.TIMES.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MULTIPLY, child);\n\t\t} else if (BuiltInFunctionDefinitions.MOD.equals(def)) {\n\t\t\treturn relBuilder.call(FlinkSqlOperatorTable.MOD, child);\n\t\t} else if (def instanceof ScalarFunctionDefinition) {\n\t\t\tScalarFunction scalarFunc = ((ScalarFunctionDefinition) def).getScalarFunction();\n\t\t\tSqlFunction sqlFunction = UserDefinedFunctionUtils.createScalarSqlFunction(\n\t\t\t\t// TODO use the name under which the function is registered\n\t\t\t\tscalarFunc.functionIdentifier(),\n\t\t\t\tscalarFunc.toString(),\n\t\t\t\tscalarFunc,\n\t\t\t\ttypeFactory);\n\t\t\treturn relBuilder.call(sqlFunction, child);\n\t\t} else {\n\t\t\tthrow new UnsupportedOperationException(def.toString());\n\t\t}\n\t}"
        ],
        [
            "PlannerContext::createFrameworkConfig(CalciteSchema,List)",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 -\n 103  \n 104  \n 105  ",
            "\tprivate FrameworkConfig createFrameworkConfig(CalciteSchema rootSchema, List<RelTraitDef> traitDefs) {\n\t\treturn Frameworks.newConfigBuilder()\n\t\t\t\t.defaultSchema(rootSchema.plus())\n\t\t\t\t.parserConfig(getSqlParserConfig())\n\t\t\t\t.costFactory(new FlinkCostFactory())\n\t\t\t\t.typeSystem(typeSystem)\n\t\t\t\t.sqlToRelConverterConfig(getSqlToRelConverterConfig(getCalciteConfig(tableConfig)))\n\t\t\t\t.operatorTable(getSqlOperatorTable(getCalciteConfig(tableConfig), functionCatalog))\n\t\t\t\t// set the executor to evaluate constant expressions\n\t\t\t\t.executor(new ExpressionReducer(tableConfig, false))\n\t\t\t\t.context(new FlinkContextImpl(tableConfig))\n\t\t\t\t.traitDefs(traitDefs)\n\t\t\t\t.build();\n\t}",
            "  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103  \n 104  \n 105  ",
            "\tprivate FrameworkConfig createFrameworkConfig(CalciteSchema rootSchema, List<RelTraitDef> traitDefs) {\n\t\treturn Frameworks.newConfigBuilder()\n\t\t\t\t.defaultSchema(rootSchema.plus())\n\t\t\t\t.parserConfig(getSqlParserConfig())\n\t\t\t\t.costFactory(new FlinkCostFactory())\n\t\t\t\t.typeSystem(typeSystem)\n\t\t\t\t.sqlToRelConverterConfig(getSqlToRelConverterConfig(getCalciteConfig(tableConfig)))\n\t\t\t\t.operatorTable(getSqlOperatorTable(getCalciteConfig(tableConfig), functionCatalog))\n\t\t\t\t// set the executor to evaluate constant expressions\n\t\t\t\t.executor(new ExpressionReducer(tableConfig, false))\n\t\t\t\t.context(new FlinkContextImpl(tableConfig, functionCatalog))\n\t\t\t\t.traitDefs(traitDefs)\n\t\t\t\t.build();\n\t}"
        ]
    ],
    "8b68ca769967f6c2035ceba6ebc25fe6b9988250": [
        [
            "HiveCatalog::maskFlinkProperties(Map)",
            " 590  \n 591  \n 592  \n 593  \n 594  \n 595  \n 596 -\n 597  ",
            "\t/**\n\t * Add a prefix to Flink-created properties to distinguish them from Hive-created properties.\n\t */\n\tprivate static Map<String, String> maskFlinkProperties(Map<String, String> properties) {\n\t\treturn properties.entrySet().stream()\n\t\t\t.filter(e -> e.getKey() != null && e.getValue() != null)\n\t\t\t.collect(Collectors.toMap(e -> FLINK_PROPERTY_PREFIX + e.getKey(), e -> e.getValue()));\n\t}",
            " 588  \n 589  \n 590  \n 591  \n 592  \n 593  \n 594  \n 595 +\n 596 +\n 597 +\n 598 +\n 599  ",
            "\t/**\n\t * Add a prefix to Flink-created properties to distinguish them from Hive-created properties.\n\t * Note that 'is_generic' is a special key and this method will leave it as-is.\n\t */\n\tprivate static Map<String, String> maskFlinkProperties(Map<String, String> properties) {\n\t\treturn properties.entrySet().stream()\n\t\t\t.filter(e -> e.getKey() != null && e.getValue() != null)\n\t\t\t.map(e -> new Tuple2<>(\n\t\t\t\te.getKey().equals(CatalogConfig.IS_GENERIC) ? e.getKey() : FLINK_PROPERTY_PREFIX + e.getKey(),\n\t\t\t\te.getValue()))\n\t\t\t.collect(Collectors.toMap(t -> t.f0, t -> t.f1));\n\t}"
        ],
        [
            "CatalogTestUtil::checkEquals(CatalogView,CatalogView)",
            "  64  \n  65  \n  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78  ",
            "\tpublic static void checkEquals(CatalogView v1, CatalogView v2) {\n\t\tassertEquals(v1.getClass(), v2.getClass());\n\t\tassertEquals(v1.getSchema(), v1.getSchema());\n\t\tassertEquals(v1.getComment(), v2.getComment());\n\t\tassertEquals(v1.getOriginalQuery(), v2.getOriginalQuery());\n\t\tassertEquals(v1.getExpandedQuery(), v2.getExpandedQuery());\n\n\t\t// Hive tables may have properties created by itself\n\t\t// thus properties of Hive table is a super set of those in its corresponding Flink table\n\t\tif (Boolean.valueOf(v1.getProperties().get(CatalogConfig.IS_GENERIC))) {\n\t\t\tassertEquals(v1.getProperties(), v2.getProperties());\n\t\t} else {\n\t\t\tassertTrue(v2.getProperties().entrySet().containsAll(v1.getProperties().entrySet()));\n\t\t}\n\t}",
            "  66  \n  67  \n  68  \n  69  \n  70  \n  71  \n  72  \n  73  \n  74  \n  75  \n  76  \n  77  \n  78 +\n  79  \n  80  \n  81  ",
            "\tpublic static void checkEquals(CatalogView v1, CatalogView v2) {\n\t\tassertEquals(v1.getClass(), v2.getClass());\n\t\tassertEquals(v1.getSchema(), v1.getSchema());\n\t\tassertEquals(v1.getComment(), v2.getComment());\n\t\tassertEquals(v1.getOriginalQuery(), v2.getOriginalQuery());\n\t\tassertEquals(v1.getExpandedQuery(), v2.getExpandedQuery());\n\n\t\t// Hive tables may have properties created by itself\n\t\t// thus properties of Hive table is a super set of those in its corresponding Flink table\n\t\tif (Boolean.valueOf(v1.getProperties().get(CatalogConfig.IS_GENERIC))) {\n\t\t\tassertEquals(v1.getProperties(), v2.getProperties());\n\t\t} else {\n\t\t\tassertTrue(v2.getProperties().keySet().stream().noneMatch(k -> k.startsWith(FLINK_PROPERTY_PREFIX)));\n\t\t\tassertTrue(v2.getProperties().entrySet().containsAll(v1.getProperties().entrySet()));\n\t\t}\n\t}"
        ],
        [
            "HiveCatalog::instantiateCatalogTable(Table)",
            " 479  \n 480  \n 481  \n 482  \n 483  \n 484  \n 485 -\n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  \n 509  \n 510  \n 511  ",
            "\tprivate static CatalogBaseTable instantiateCatalogTable(Table hiveTable) {\n\t\tboolean isView = TableType.valueOf(hiveTable.getTableType()) == TableType.VIRTUAL_VIEW;\n\n\t\t// Table properties\n\t\tMap<String, String> properties = hiveTable.getParameters();\n\n\t\tboolean isGeneric = Boolean.valueOf(properties.computeIfAbsent(FLINK_PROPERTY_IS_GENERIC, k -> String.valueOf(false)));\n\t\tif (isGeneric) {\n\t\t\tproperties = retrieveFlinkProperties(properties);\n\t\t}\n\t\tString comment = properties.remove(CatalogTableConfig.TABLE_COMMENT);\n\n\t\t// Table schema\n\t\tTableSchema tableSchema =\n\t\t\tHiveTableUtil.createTableSchema(hiveTable.getSd().getCols(), hiveTable.getPartitionKeys());\n\n\t\t// Partition keys\n\t\tList<String> partitionKeys = new ArrayList<>();\n\t\tif (!hiveTable.getPartitionKeys().isEmpty()) {\n\t\t\tpartitionKeys = getFieldNames(hiveTable.getPartitionKeys());\n\t\t}\n\n\t\tif (isView) {\n\t\t\treturn new CatalogViewImpl(\n\t\t\t\t\thiveTable.getViewOriginalText(),\n\t\t\t\t\thiveTable.getViewExpandedText(),\n\t\t\t\t\ttableSchema,\n\t\t\t\t\tproperties,\n\t\t\t\t\tcomment);\n\t\t} else {\n\t\t\treturn new CatalogTableImpl(tableSchema, partitionKeys, properties, comment);\n\t\t}\n\t}",
            " 476  \n 477  \n 478  \n 479  \n 480  \n 481  \n 482 +\n 483  \n 484  \n 485  \n 486  \n 487  \n 488  \n 489  \n 490  \n 491  \n 492  \n 493  \n 494  \n 495  \n 496  \n 497  \n 498  \n 499  \n 500  \n 501  \n 502  \n 503  \n 504  \n 505  \n 506  \n 507  \n 508  ",
            "\tprivate static CatalogBaseTable instantiateCatalogTable(Table hiveTable) {\n\t\tboolean isView = TableType.valueOf(hiveTable.getTableType()) == TableType.VIRTUAL_VIEW;\n\n\t\t// Table properties\n\t\tMap<String, String> properties = hiveTable.getParameters();\n\n\t\tboolean isGeneric = Boolean.valueOf(properties.get(CatalogConfig.IS_GENERIC));\n\t\tif (isGeneric) {\n\t\t\tproperties = retrieveFlinkProperties(properties);\n\t\t}\n\t\tString comment = properties.remove(CatalogTableConfig.TABLE_COMMENT);\n\n\t\t// Table schema\n\t\tTableSchema tableSchema =\n\t\t\tHiveTableUtil.createTableSchema(hiveTable.getSd().getCols(), hiveTable.getPartitionKeys());\n\n\t\t// Partition keys\n\t\tList<String> partitionKeys = new ArrayList<>();\n\t\tif (!hiveTable.getPartitionKeys().isEmpty()) {\n\t\t\tpartitionKeys = getFieldNames(hiveTable.getPartitionKeys());\n\t\t}\n\n\t\tif (isView) {\n\t\t\treturn new CatalogViewImpl(\n\t\t\t\t\thiveTable.getViewOriginalText(),\n\t\t\t\t\thiveTable.getViewExpandedText(),\n\t\t\t\t\ttableSchema,\n\t\t\t\t\tproperties,\n\t\t\t\t\tcomment);\n\t\t} else {\n\t\t\treturn new CatalogTableImpl(tableSchema, partitionKeys, properties, comment);\n\t\t}\n\t}"
        ],
        [
            "CatalogTestUtil::checkEquals(CatalogTable,CatalogTable)",
            "  44  \n  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61  \n  62  ",
            "\tpublic static void checkEquals(CatalogTable t1, CatalogTable t2) {\n\t\tassertEquals(t1.getClass(), t2.getClass());\n\t\tassertEquals(t1.getSchema(), t2.getSchema());\n\t\tassertEquals(t1.getComment(), t2.getComment());\n\t\tassertEquals(t1.getPartitionKeys(), t2.getPartitionKeys());\n\t\tassertEquals(t1.isPartitioned(), t2.isPartitioned());\n\n\t\tassertEquals(\n\t\t\tt1.getProperties().get(CatalogConfig.IS_GENERIC),\n\t\t\tt2.getProperties().get(CatalogConfig.IS_GENERIC));\n\n\t\t// Hive tables may have properties created by itself\n\t\t// thus properties of Hive table is a super set of those in its corresponding Flink table\n\t\tif (Boolean.valueOf(t1.getProperties().get(CatalogConfig.IS_GENERIC))) {\n\t\t\tassertEquals(t1.getProperties(), t2.getProperties());\n\t\t} else {\n\t\t\tassertTrue(t2.getProperties().entrySet().containsAll(t1.getProperties().entrySet()));\n\t\t}\n\t}",
            "  45  \n  46  \n  47  \n  48  \n  49  \n  50  \n  51  \n  52  \n  53  \n  54  \n  55  \n  56  \n  57  \n  58  \n  59  \n  60  \n  61 +\n  62  \n  63  \n  64  ",
            "\tpublic static void checkEquals(CatalogTable t1, CatalogTable t2) {\n\t\tassertEquals(t1.getClass(), t2.getClass());\n\t\tassertEquals(t1.getSchema(), t2.getSchema());\n\t\tassertEquals(t1.getComment(), t2.getComment());\n\t\tassertEquals(t1.getPartitionKeys(), t2.getPartitionKeys());\n\t\tassertEquals(t1.isPartitioned(), t2.isPartitioned());\n\n\t\tassertEquals(\n\t\t\tt1.getProperties().get(CatalogConfig.IS_GENERIC),\n\t\t\tt2.getProperties().get(CatalogConfig.IS_GENERIC));\n\n\t\t// Hive tables may have properties created by itself\n\t\t// thus properties of Hive table is a super set of those in its corresponding Flink table\n\t\tif (Boolean.valueOf(t1.getProperties().get(CatalogConfig.IS_GENERIC))) {\n\t\t\tassertEquals(t1.getProperties(), t2.getProperties());\n\t\t} else {\n\t\t\tassertTrue(t2.getProperties().keySet().stream().noneMatch(k -> k.startsWith(FLINK_PROPERTY_PREFIX)));\n\t\t\tassertTrue(t2.getProperties().entrySet().containsAll(t1.getProperties().entrySet()));\n\t\t}\n\t}"
        ],
        [
            "HiveCatalog::retrieveFlinkProperties(Map)",
            " 581  \n 582  \n 583  \n 584  \n 585  \n 586 -\n 587  \n 588  ",
            "\t/**\n\t * Filter out Hive-created properties, and return Flink-created properties.\n\t */\n\tprivate static Map<String, String> retrieveFlinkProperties(Map<String, String> hiveTableParams) {\n\t\treturn hiveTableParams.entrySet().stream()\n\t\t\t.filter(e -> e.getKey().startsWith(FLINK_PROPERTY_PREFIX))\n\t\t\t.collect(Collectors.toMap(e -> e.getKey().replace(FLINK_PROPERTY_PREFIX, \"\"), e -> e.getValue()));\n\t}",
            " 578  \n 579  \n 580  \n 581  \n 582  \n 583  \n 584 +\n 585  \n 586  ",
            "\t/**\n\t * Filter out Hive-created properties, and return Flink-created properties.\n\t * Note that 'is_generic' is a special key and this method will leave it as-is.\n\t */\n\tprivate static Map<String, String> retrieveFlinkProperties(Map<String, String> hiveTableParams) {\n\t\treturn hiveTableParams.entrySet().stream()\n\t\t\t.filter(e -> e.getKey().startsWith(FLINK_PROPERTY_PREFIX) || e.getKey().equals(CatalogConfig.IS_GENERIC))\n\t\t\t.collect(Collectors.toMap(e -> e.getKey().replace(FLINK_PROPERTY_PREFIX, \"\"), e -> e.getValue()));\n\t}"
        ],
        [
            "HiveCatalog::ensureTableAndPartitionMatch(Table,CatalogPartition)",
            " 771  \n 772 -\n 773  \n 774  \n 775  \n 776  \n 777  \n 778  ",
            "\tprivate static void ensureTableAndPartitionMatch(Table hiveTable, CatalogPartition catalogPartition) {\n\t\tboolean isGeneric = Boolean.valueOf(hiveTable.getParameters().get(FLINK_PROPERTY_IS_GENERIC));\n\t\tif ((isGeneric && catalogPartition instanceof HiveCatalogPartition) ||\n\t\t\t(!isGeneric && catalogPartition instanceof GenericCatalogPartition)) {\n\t\t\tthrow new CatalogException(String.format(\"Cannot handle %s partition for %s table\",\n\t\t\t\tcatalogPartition.getClass().getName(), isGeneric ? \"generic\" : \"non-generic\"));\n\t\t}\n\t}",
            " 773  \n 774 +\n 775  \n 776  \n 777  \n 778  \n 779  \n 780  ",
            "\tprivate static void ensureTableAndPartitionMatch(Table hiveTable, CatalogPartition catalogPartition) {\n\t\tboolean isGeneric = Boolean.valueOf(hiveTable.getParameters().get(CatalogConfig.IS_GENERIC));\n\t\tif ((isGeneric && catalogPartition instanceof HiveCatalogPartition) ||\n\t\t\t(!isGeneric && catalogPartition instanceof GenericCatalogPartition)) {\n\t\t\tthrow new CatalogException(String.format(\"Cannot handle %s partition for %s table\",\n\t\t\t\tcatalogPartition.getClass().getName(), isGeneric ? \"generic\" : \"non-generic\"));\n\t\t}\n\t}"
        ]
    ],
    "305051cc6e03d7e0ed9a9c3afe9e90b56bab0dda": [
        [
            "StreamSQLTestProgram::main(String)",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 -\n 110 -\n 111 -\n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tParameterTool params = ParameterTool.fromArgs(args);\n\t\tString outputPath = params.getRequired(\"outputPath\");\n\n\t\tStreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsEnv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n\t\t\t3,\n\t\t\tTime.of(10, TimeUnit.SECONDS)\n\t\t));\n\t\tsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\tsEnv.enableCheckpointing(4000);\n\t\tsEnv.getConfig().setAutoWatermarkInterval(1000);\n\n\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(sEnv);\n\n\t\ttEnv.registerTableSource(\"table1\", new GeneratorTableSource(10, 100, 60, 0));\n\t\ttEnv.registerTableSource(\"table2\", new GeneratorTableSource(5, 0.2f, 60, 5));\n\n\t\tint overWindowSizeSeconds = 1;\n\t\tint tumbleWindowSizeSeconds = 10;\n\n\t\tString overQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  key, \" +\n\t\t\t\"  rowtime, \" +\n\t\t\t\"  COUNT(*) OVER (PARTITION BY key ORDER BY rowtime RANGE BETWEEN INTERVAL '%d' SECOND PRECEDING AND CURRENT ROW) AS cnt \" +\n\t\t\t\"FROM table1\",\n\t\t\toverWindowSizeSeconds);\n\n\t\tString tumbleQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  key, \" +\n\t\t\t//TODO: The \"WHEN -1 THEN NULL\" part is a temporary workaround, to make the test pass, for\n\t\t\t// https://issues.apache.org/jira/browse/FLINK-12249. We should remove it once the issue is fixed.\n\t\t\t\"  CASE SUM(cnt) / COUNT(*) WHEN 101 THEN 1 WHEN -1 THEN NULL ELSE 99 END AS correct, \" +\n\t\t\t\"  TUMBLE_START(rowtime, INTERVAL '%d' SECOND) AS wStart, \" +\n\t\t\t\"  TUMBLE_ROWTIME(rowtime, INTERVAL '%d' SECOND) AS rowtime \" +\n\t\t\t\"FROM (%s) \" +\n\t\t\t\"WHERE rowtime > TIMESTAMP '1970-01-01 00:00:01' \" +\n\t\t\t\"GROUP BY key, TUMBLE(rowtime, INTERVAL '%d' SECOND)\",\n\t\t\ttumbleWindowSizeSeconds,\n\t\t\ttumbleWindowSizeSeconds,\n\t\t\toverQuery,\n\t\t\ttumbleWindowSizeSeconds);\n\n\t\tString joinQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  t1.key, \" +\n\t\t\t\"  t2.rowtime AS rowtime, \" +\n\t\t\t\"  t2.correct,\" +\n\t\t\t\"  t2.wStart \" +\n\t\t\t\"FROM table2 t1, (%s) t2 \" +\n\t\t\t\"WHERE \" +\n\t\t\t\"  t1.key = t2.key AND \" +\n\t\t\t\"  t1.rowtime BETWEEN t2.rowtime AND t2.rowtime + INTERVAL '%d' SECOND\",\n\t\t\ttumbleQuery,\n\t\t\ttumbleWindowSizeSeconds);\n\n\t\tString finalAgg = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  SUM(correct) AS correct, \" +\n\t\t\t\"  TUMBLE_START(rowtime, INTERVAL '20' SECOND) AS rowtime \" +\n\t\t\t\"FROM (%s) \" +\n\t\t\t\"GROUP BY TUMBLE(rowtime, INTERVAL '20' SECOND)\",\n\t\t\tjoinQuery);\n\n\t\t// get Table for SQL query\n\t\tTable result = tEnv.sqlQuery(finalAgg);\n\t\t// convert Table into append-only DataStream\n\t\tDataStream<Row> resultStream =\n\t\t\ttEnv.toAppendStream(result, Types.ROW(Types.INT, Types.SQL_TIMESTAMP));\n\n\t\tfinal StreamingFileSink<Row> sink = StreamingFileSink\n\t\t\t.forRowFormat(new Path(outputPath), (Encoder<Row>) (element, stream) -> {\n\t\t\t\tPrintStream out = new PrintStream(stream);\n\t\t\t\tout.println(element.toString());\n\t\t\t})\n\t\t\t.withBucketAssigner(new KeyBucketAssigner())\n\t\t\t.withRollingPolicy(OnCheckpointRollingPolicy.build())\n\t\t\t.build();\n\n\t\tresultStream\n\t\t\t// inject a KillMapper that forwards all records but terminates the first execution attempt\n\t\t\t.map(new KillMapper()).setParallelism(1)\n\t\t\t// add sink function\n\t\t\t.addSink(sink).setParallelism(1);\n\n\t\tsEnv.execute();\n\t}",
            "  76  \n  77  \n  78  \n  79  \n  80  \n  81  \n  82  \n  83  \n  84  \n  85  \n  86  \n  87  \n  88  \n  89  \n  90  \n  91  \n  92  \n  93  \n  94  \n  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109 +\n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  ",
            "\tpublic static void main(String[] args) throws Exception {\n\n\t\tParameterTool params = ParameterTool.fromArgs(args);\n\t\tString outputPath = params.getRequired(\"outputPath\");\n\n\t\tStreamExecutionEnvironment sEnv = StreamExecutionEnvironment.getExecutionEnvironment();\n\t\tsEnv.setRestartStrategy(RestartStrategies.fixedDelayRestart(\n\t\t\t3,\n\t\t\tTime.of(10, TimeUnit.SECONDS)\n\t\t));\n\t\tsEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\n\t\tsEnv.enableCheckpointing(4000);\n\t\tsEnv.getConfig().setAutoWatermarkInterval(1000);\n\n\t\tStreamTableEnvironment tEnv = StreamTableEnvironment.create(sEnv);\n\n\t\ttEnv.registerTableSource(\"table1\", new GeneratorTableSource(10, 100, 60, 0));\n\t\ttEnv.registerTableSource(\"table2\", new GeneratorTableSource(5, 0.2f, 60, 5));\n\n\t\tint overWindowSizeSeconds = 1;\n\t\tint tumbleWindowSizeSeconds = 10;\n\n\t\tString overQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  key, \" +\n\t\t\t\"  rowtime, \" +\n\t\t\t\"  COUNT(*) OVER (PARTITION BY key ORDER BY rowtime RANGE BETWEEN INTERVAL '%d' SECOND PRECEDING AND CURRENT ROW) AS cnt \" +\n\t\t\t\"FROM table1\",\n\t\t\toverWindowSizeSeconds);\n\n\t\tString tumbleQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  key, \" +\n\t\t\t\"  CASE SUM(cnt) / COUNT(*) WHEN 101 THEN 1 ELSE 99 END AS correct, \" +\n\t\t\t\"  TUMBLE_START(rowtime, INTERVAL '%d' SECOND) AS wStart, \" +\n\t\t\t\"  TUMBLE_ROWTIME(rowtime, INTERVAL '%d' SECOND) AS rowtime \" +\n\t\t\t\"FROM (%s) \" +\n\t\t\t\"WHERE rowtime > TIMESTAMP '1970-01-01 00:00:01' \" +\n\t\t\t\"GROUP BY key, TUMBLE(rowtime, INTERVAL '%d' SECOND)\",\n\t\t\ttumbleWindowSizeSeconds,\n\t\t\ttumbleWindowSizeSeconds,\n\t\t\toverQuery,\n\t\t\ttumbleWindowSizeSeconds);\n\n\t\tString joinQuery = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  t1.key, \" +\n\t\t\t\"  t2.rowtime AS rowtime, \" +\n\t\t\t\"  t2.correct,\" +\n\t\t\t\"  t2.wStart \" +\n\t\t\t\"FROM table2 t1, (%s) t2 \" +\n\t\t\t\"WHERE \" +\n\t\t\t\"  t1.key = t2.key AND \" +\n\t\t\t\"  t1.rowtime BETWEEN t2.rowtime AND t2.rowtime + INTERVAL '%d' SECOND\",\n\t\t\ttumbleQuery,\n\t\t\ttumbleWindowSizeSeconds);\n\n\t\tString finalAgg = String.format(\n\t\t\t\"SELECT \" +\n\t\t\t\"  SUM(correct) AS correct, \" +\n\t\t\t\"  TUMBLE_START(rowtime, INTERVAL '20' SECOND) AS rowtime \" +\n\t\t\t\"FROM (%s) \" +\n\t\t\t\"GROUP BY TUMBLE(rowtime, INTERVAL '20' SECOND)\",\n\t\t\tjoinQuery);\n\n\t\t// get Table for SQL query\n\t\tTable result = tEnv.sqlQuery(finalAgg);\n\t\t// convert Table into append-only DataStream\n\t\tDataStream<Row> resultStream =\n\t\t\ttEnv.toAppendStream(result, Types.ROW(Types.INT, Types.SQL_TIMESTAMP));\n\n\t\tfinal StreamingFileSink<Row> sink = StreamingFileSink\n\t\t\t.forRowFormat(new Path(outputPath), (Encoder<Row>) (element, stream) -> {\n\t\t\t\tPrintStream out = new PrintStream(stream);\n\t\t\t\tout.println(element.toString());\n\t\t\t})\n\t\t\t.withBucketAssigner(new KeyBucketAssigner())\n\t\t\t.withRollingPolicy(OnCheckpointRollingPolicy.build())\n\t\t\t.build();\n\n\t\tresultStream\n\t\t\t// inject a KillMapper that forwards all records but terminates the first execution attempt\n\t\t\t.map(new KillMapper()).setParallelism(1)\n\t\t\t// add sink function\n\t\t\t.addSink(sink).setParallelism(1);\n\n\t\tsEnv.execute();\n\t}"
        ]
    ],
    "a194b37d9b99a47174de9108a937f821816d61f5": [
        [
            "HiveCatalogUseBlinkITCase::testBlinkUdf()",
            "  95  \n  96  \n  97  \n  98  \n  99  \n 100  \n 101  \n 102  \n 103  \n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  ",
            "\t@Test\n\tpublic void testBlinkUdf() throws Exception {\n\t\tTableEnvironment tEnv = TableEnvironment.create(\n\t\t\t\tEnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build());\n\n\t\ttEnv.registerCatalog(\"myhive\", hiveCatalog);\n\t\ttEnv.useCatalog(\"myhive\");\n\n\t\tTableSchema schema = TableSchema.builder()\n\t\t\t\t.field(\"name\", DataTypes.STRING())\n\t\t\t\t.field(\"age\", DataTypes.INT())\n\t\t\t\t.build();\n\n\t\tFormatDescriptor format = new OldCsv()\n\t\t\t\t.field(\"name\", Types.STRING())\n\t\t\t\t.field(\"age\", Types.INT());\n\n\t\tCatalogTable source =\n\t\t\t\tnew CatalogTableBuilder(\n\t\t\t\t\t\tnew FileSystem().path(this.getClass().getResource(\"/csv/test.csv\").getPath()),\n\t\t\t\t\t\tschema)\n\t\t\t\t\t\t.withFormat(format)\n\t\t\t\t\t\t.inAppendMode()\n\t\t\t\t\t\t.withComment(\"Comment.\")\n\t\t\t\t\t\t.build();\n\n\t\tPath p = Paths.get(tempFolder.newFolder().getAbsolutePath(), \"test.csv\");\n\n\t\tTableSchema sinkSchema = TableSchema.builder()\n\t\t\t\t.field(\"name1\", Types.STRING())\n\t\t\t\t.field(\"name2\", Types.STRING())\n\t\t\t\t.field(\"sum1\", Types.INT())\n\t\t\t\t.field(\"sum2\", Types.LONG())\n\t\t\t\t.build();\n\n\t\tFormatDescriptor sinkFormat = new OldCsv()\n\t\t\t\t.field(\"name1\", Types.STRING())\n\t\t\t\t.field(\"name2\", Types.STRING())\n\t\t\t\t.field(\"sum1\", Types.INT())\n\t\t\t\t.field(\"sum2\", Types.LONG());\n\t\tCatalogTable sink =\n\t\t\t\tnew CatalogTableBuilder(\n\t\t\t\t\t\tnew FileSystem().path(p.toAbsolutePath().toString()),\n\t\t\t\t\t\tsinkSchema)\n\t\t\t\t\t\t.withFormat(sinkFormat)\n\t\t\t\t\t\t.inAppendMode()\n\t\t\t\t\t\t.withComment(\"Comment.\")\n\t\t\t\t\t\t.build();\n\n\t\thiveCatalog.createTable(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, sourceTableName),\n\t\t\t\tsource,\n\t\t\t\tfalse\n\t\t);\n\n\t\thiveCatalog.createTable(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, sinkTableName),\n\t\t\t\tsink,\n\t\t\t\tfalse\n\t\t);\n\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveSimpleUDF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"mygenericudf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveGenericUDF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudtf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveUDTF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudaf\"),\n\t\t\t\tnew CatalogFunctionImpl(GenericUDAFSum.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\n\t\tString innerSql = format(\"select mygenericudf(myudf(name), 1) as a, mygenericudf(myudf(age), 1) as b,\" +\n\t\t\t\t\" s from %s, lateral table(myudtf(name, 1)) as T(s)\", sourceTableName);\n\n\t\ttEnv.sqlUpdate(\n\t\t\t\tformat(\"insert into %s select a, s, sum(b), myudaf(b) from (%s) group by a, s\",\n\t\t\t\t\t\tsinkTableName,\n\t\t\t\t\t\tinnerSql));\n\t\ttEnv.execute(\"myjob\");\n\n\t\t// assert written result\n\t\tStringBuilder builder = new StringBuilder();\n\t\ttry (Stream<Path> paths = Files.walk(Paths.get(p.toAbsolutePath().toString()))) {\n\t\t\tpaths.filter(Files::isRegularFile).forEach(path -> {\n\t\t\t\ttry {\n\t\t\t\t\tString content = FileUtils.readFileUtf8(path.toFile());\n\t\t\t\t\tif (content.isEmpty()) {\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tbuilder.append(content);\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tthrow new RuntimeException(e);\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t\tList<String> results = Arrays.stream(builder.toString().split(\"\\n\"))\n\t\t\t\t.filter(s -> !s.isEmpty())\n\t\t\t\t.collect(Collectors.toList());\n\t\tresults.sort(String::compareTo);\n\t\tAssert.assertEquals(Arrays.asList(\"1,1,2,2\", \"2,2,4,4\", \"3,3,6,6\"), results);\n\t}",
            "  97  \n  98  \n  99  \n 100  \n 101  \n 102 +\n 103 +\n 104  \n 105  \n 106  \n 107  \n 108  \n 109  \n 110  \n 111  \n 112  \n 113  \n 114  \n 115  \n 116  \n 117  \n 118  \n 119  \n 120  \n 121  \n 122  \n 123  \n 124  \n 125  \n 126  \n 127  \n 128  \n 129  \n 130  \n 131  \n 132  \n 133  \n 134  \n 135  \n 136  \n 137  \n 138  \n 139  \n 140  \n 141  \n 142  \n 143  \n 144  \n 145  \n 146  \n 147  \n 148  \n 149  \n 150  \n 151  \n 152  \n 153  \n 154  \n 155  \n 156  \n 157  \n 158  \n 159  \n 160  \n 161  \n 162  \n 163  \n 164  \n 165  \n 166  \n 167  \n 168  \n 169  \n 170  \n 171  \n 172  \n 173  \n 174  \n 175  \n 176  \n 177  \n 178  \n 179  \n 180  \n 181  \n 182  \n 183  \n 184  \n 185  \n 186  \n 187  \n 188  \n 189  \n 190  \n 191  \n 192  \n 193  \n 194  \n 195  \n 196  \n 197  \n 198  \n 199  \n 200  \n 201  \n 202  \n 203  \n 204  \n 205  \n 206  ",
            "\t@Test\n\tpublic void testBlinkUdf() throws Exception {\n\t\tTableEnvironment tEnv = TableEnvironment.create(\n\t\t\t\tEnvironmentSettings.newInstance().useBlinkPlanner().inBatchMode().build());\n\n\t\tBatchTestBase.configForMiniCluster(tEnv.getConfig());\n\n\t\ttEnv.registerCatalog(\"myhive\", hiveCatalog);\n\t\ttEnv.useCatalog(\"myhive\");\n\n\t\tTableSchema schema = TableSchema.builder()\n\t\t\t\t.field(\"name\", DataTypes.STRING())\n\t\t\t\t.field(\"age\", DataTypes.INT())\n\t\t\t\t.build();\n\n\t\tFormatDescriptor format = new OldCsv()\n\t\t\t\t.field(\"name\", Types.STRING())\n\t\t\t\t.field(\"age\", Types.INT());\n\n\t\tCatalogTable source =\n\t\t\t\tnew CatalogTableBuilder(\n\t\t\t\t\t\tnew FileSystem().path(this.getClass().getResource(\"/csv/test.csv\").getPath()),\n\t\t\t\t\t\tschema)\n\t\t\t\t\t\t.withFormat(format)\n\t\t\t\t\t\t.inAppendMode()\n\t\t\t\t\t\t.withComment(\"Comment.\")\n\t\t\t\t\t\t.build();\n\n\t\tPath p = Paths.get(tempFolder.newFolder().getAbsolutePath(), \"test.csv\");\n\n\t\tTableSchema sinkSchema = TableSchema.builder()\n\t\t\t\t.field(\"name1\", Types.STRING())\n\t\t\t\t.field(\"name2\", Types.STRING())\n\t\t\t\t.field(\"sum1\", Types.INT())\n\t\t\t\t.field(\"sum2\", Types.LONG())\n\t\t\t\t.build();\n\n\t\tFormatDescriptor sinkFormat = new OldCsv()\n\t\t\t\t.field(\"name1\", Types.STRING())\n\t\t\t\t.field(\"name2\", Types.STRING())\n\t\t\t\t.field(\"sum1\", Types.INT())\n\t\t\t\t.field(\"sum2\", Types.LONG());\n\t\tCatalogTable sink =\n\t\t\t\tnew CatalogTableBuilder(\n\t\t\t\t\t\tnew FileSystem().path(p.toAbsolutePath().toString()),\n\t\t\t\t\t\tsinkSchema)\n\t\t\t\t\t\t.withFormat(sinkFormat)\n\t\t\t\t\t\t.inAppendMode()\n\t\t\t\t\t\t.withComment(\"Comment.\")\n\t\t\t\t\t\t.build();\n\n\t\thiveCatalog.createTable(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, sourceTableName),\n\t\t\t\tsource,\n\t\t\t\tfalse\n\t\t);\n\n\t\thiveCatalog.createTable(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, sinkTableName),\n\t\t\t\tsink,\n\t\t\t\tfalse\n\t\t);\n\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveSimpleUDF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"mygenericudf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveGenericUDF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudtf\"),\n\t\t\t\tnew CatalogFunctionImpl(TestHiveUDTF.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\t\thiveCatalog.createFunction(\n\t\t\t\tnew ObjectPath(HiveCatalog.DEFAULT_DB, \"myudaf\"),\n\t\t\t\tnew CatalogFunctionImpl(GenericUDAFSum.class.getCanonicalName(), new HashMap<>()),\n\t\t\t\tfalse);\n\n\t\tString innerSql = format(\"select mygenericudf(myudf(name), 1) as a, mygenericudf(myudf(age), 1) as b,\" +\n\t\t\t\t\" s from %s, lateral table(myudtf(name, 1)) as T(s)\", sourceTableName);\n\n\t\ttEnv.sqlUpdate(\n\t\t\t\tformat(\"insert into %s select a, s, sum(b), myudaf(b) from (%s) group by a, s\",\n\t\t\t\t\t\tsinkTableName,\n\t\t\t\t\t\tinnerSql));\n\t\ttEnv.execute(\"myjob\");\n\n\t\t// assert written result\n\t\tStringBuilder builder = new StringBuilder();\n\t\ttry (Stream<Path> paths = Files.walk(Paths.get(p.toAbsolutePath().toString()))) {\n\t\t\tpaths.filter(Files::isRegularFile).forEach(path -> {\n\t\t\t\ttry {\n\t\t\t\t\tString content = FileUtils.readFileUtf8(path.toFile());\n\t\t\t\t\tif (content.isEmpty()) {\n\t\t\t\t\t\treturn;\n\t\t\t\t\t}\n\t\t\t\t\tbuilder.append(content);\n\t\t\t\t} catch (IOException e) {\n\t\t\t\t\tthrow new RuntimeException(e);\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\t\tList<String> results = Arrays.stream(builder.toString().split(\"\\n\"))\n\t\t\t\t.filter(s -> !s.isEmpty())\n\t\t\t\t.collect(Collectors.toList());\n\t\tresults.sort(String::compareTo);\n\t\tAssert.assertEquals(Arrays.asList(\"1,1,2,2\", \"2,2,4,4\", \"3,3,6,6\"), results);\n\t}"
        ]
    ]
}